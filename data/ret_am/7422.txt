{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Computational Linguistics in the Netherlands</b> 30 | Accepted submissions", "url": "https://clin30.sites.uu.nl/accepted-submissions/", "isFamilyFriendly": true, "displayUrl": "https://clin30.sites.uu.nl/accepted-submissions", "snippet": "Specifically, <b>BERT</b> (<b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b>) developed by Google, has been achieving high accuracies in benchmarks for tasks such as text classification and named entity recognition (NER). However these tasks tend to be in English, while our task is Dutch NER. Google has released a multi-lingual <b>BERT</b> model with 104 languages, including Dutch, but modeling multiple languages in one model seems sub-optimal. We therefore pre-trained our own Dutch <b>BERT</b> models to ...", "dateLastCrawled": "2022-01-26T05:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Understanding the Usage of Online Media for Parenting from Infancy to ...", "url": "https://dl.acm.org/doi/fullHtml/10.1145/3411764.3445203", "isFamilyFriendly": true, "displayUrl": "https://dl.acm.org/doi/fullHtml/10.1145/3411764.3445203", "snippet": "<b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b> (<b>BERT</b>) : <b>BERT</b> is a pre-trained language representation model, and has achieved state-of-the-art results on eleven natural language processing tasks, outperforming baseline methods. We used the Pytorch implementation of <b>BERT</b> 4.", "dateLastCrawled": "2021-05-13T06:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Proceedings of the 2019 Conference on Empirical Methods in Natural ...", "url": "https://aclanthology.org/volumes/D19-1/", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/volumes/D19-1", "snippet": "While self-attention-based pre-trained language encoders <b>like</b> GPT and <b>BERT</b> have been successfully applied across a range of natural language understanding tasks, their ability to handle the nuances of procedural texts is still unknown. In this paper, we explore the use of pre-trained transformer networks for entity tracking tasks in procedural text. First, we test standard lightweight approaches for prediction with pre-trained <b>transformers</b>, and find that these approaches underperforms even ...", "dateLastCrawled": "2022-01-31T01:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Abstract - arXiv", "url": "https://arxiv.org/pdf/2006.11316.pdf", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/pdf/2006.11316.pdf", "snippet": "SqueezeBERT is much <b>like</b> <b>BERT</b>-base, but with PFC layers implemented as convolutions, and grouped convolutions for many of the layers. Recall from Section 2 that each block in the <b>BERT</b>-base <b>encoder</b> has a self-attention module with 3 PFC layers, plus 3 more PFC layers called feed-forward network layers (FFN1, FFN2, and FFN3). The FFN layers have the following dimen-sions: FFN1 has C in = C out = 768, FFN2 has C in = 768 and C out = 3072, and FFN3 has 5Note that the grouped convolution with G ...", "dateLastCrawled": "2021-10-25T05:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "EMNLP-2019-Papers/accepted_papers.md at master - <b>GitHub</b>", "url": "https://github.com/roomylee/EMNLP-2019-Papers/blob/master/accepted_papers.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/roomylee/EMNLP-2019-Papers/blob/master/accepted_papers.md", "snippet": "Aggregating <b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> Using MatchLSTM for Sequence Matching (#2244) What Does This Word Mean? Explaining Contextualized Embeddings with Natural Language Definition (#2765) Pre-Training <b>BERT</b> on Domain Resources for Short Answer Grading (#3058) WIQA: A dataset for \u201cWhat if...\u201d reasoning over procedural text (#3276)", "dateLastCrawled": "2021-08-24T22:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Lectures</b> - Computational Linguistics", "url": "http://computational-linguistics-class.org/lectures.html", "isFamilyFriendly": true, "displayUrl": "computational-linguistics-class.org/<b>lectures</b>.html", "snippet": "<b>Encoder</b>-Decoder Models and Neural Machine Translation [video] Graham Neubig, Neural Machine Translation ... <b>BERT</b>&amp;colon; Pre-training of Deep <b>Bidirectional</b> <b>Transformers</b> for Language Understanding [video] by Jacob Devlin Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, <b>BERT</b>&amp;colon Pre-training of Deep <b>Bidirectional</b> <b>Transformers</b> for Language Understanding: Wed, Apr 15, 2020: Project Milestone 2 or HW10 &quot;Neural Machine Translation&quot; due Mon, Apr 20, 2020: Text Generation with Neural ...", "dateLastCrawled": "2022-01-30T02:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Accepted Papers</b> - EMNLP-IJCNLP 2019", "url": "https://2019.emnlp.org/program/accepted/", "isFamilyFriendly": true, "displayUrl": "https://2019.emnlp.org/program/accepted", "snippet": "Aggregating <b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> Using MatchLSTM for Sequence Matching Bo Shao, Yeyun Gong, Weizhen Qi, Nan Duan and Xiaola Lin; An Attentive Fine-Grained Entity Typing Model with Latent Type Representation Ying Lin and Heng Ji; An Empirical Comparison on Imitation Learning and Reinforcement Learning for Paraphrase Generation", "dateLastCrawled": "2022-02-02T12:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Emnlp_2019", "url": "https://littlecat-lff.github.io/blog/EMNLP_2019.html", "isFamilyFriendly": true, "displayUrl": "https://littlecat-lff.github.io/blog/EMNLP_2019.html", "snippet": "Attending to Future Tokens for <b>Bidirectional</b> Sequence Generation (#1443) [arXiv] Attention is Not Not Explanation (#526) ... Adaptively Sparse <b>Transformers</b> (#2900) Show Your Work: Improved Reporting of Experimental Results (#3277) A Deep Factorization of Style and Structure in Fonts (#3999) [arXiv] Back to Top . Lexical Semantics. Knowledge Enhanced Contextual Word <b>Representations</b> (#3403) [arXiv] How Contextual are Contextualized Word <b>Representations</b>? (#208) Room to Glo: A Systematic ...", "dateLastCrawled": "2022-01-24T12:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) HONEST: Measuring Hurtful Sentence Completion ... - researchgate.net", "url": "https://www.researchgate.net/publication/352365780_HONEST_Measuring_Hurtful_Sentence_Completion_in_Language_Models", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/352365780_HONEST_Measuring_Hurtful_Sentence...", "snippet": "PDF | On Jan 1, 2021, Debora Nozza and others published HONEST: Measuring Hurtful Sentence Completion in Language Models | Find, read and cite all the research you need on ResearchGate", "dateLastCrawled": "2022-02-01T09:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Asians Asians are bad drivers arXiv:2004.09456v1 [cs.CL] 20 Apr 2020", "url": "https://arxiv.org/pdf/2004.09456.pdf", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/pdf/2004.09456.pdf", "snippet": "pretrained <b>representations</b> are obtained from learn-ing on massive text corpora, there is a danger that stereotypical biases in the real world are re\ufb02ected in these models. For example, GPT2 (Radford et al.,2019), a pretrained language model, has shown to generate unpleasant stereotypical text when prompted with context containing certain races such as African-Americans (Sheng et al., 2019). In this work, we assess the stereotypical arXiv:2004.09456v1 [cs.CL] 20 Apr 2020. biases of popular ...", "dateLastCrawled": "2021-10-23T23:52:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Computational Linguistics in the Netherlands</b> 30 | Accepted submissions", "url": "https://clin30.sites.uu.nl/accepted-submissions/", "isFamilyFriendly": true, "displayUrl": "https://clin30.sites.uu.nl/accepted-submissions", "snippet": "Specifically, <b>BERT</b> (<b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b>) developed by Google, has been achieving high accuracies in benchmarks for tasks such as text classification and named entity recognition (NER). However these tasks tend to be in English, while our task is Dutch NER. Google has released a multi-lingual <b>BERT</b> model with 104 languages, including Dutch, but modeling multiple languages in one model seems sub-optimal. We therefore pre-trained our own Dutch <b>BERT</b> models to ...", "dateLastCrawled": "2022-01-26T05:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Understanding the Usage of Online Media for Parenting from Infancy to ...", "url": "https://dl.acm.org/doi/fullHtml/10.1145/3411764.3445203", "isFamilyFriendly": true, "displayUrl": "https://dl.acm.org/doi/fullHtml/10.1145/3411764.3445203", "snippet": "<b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b> (<b>BERT</b>) : <b>BERT</b> is a pre-trained language representation model, and has achieved state-of-the-art results on eleven natural language processing tasks, outperforming baseline methods. We used the Pytorch implementation of <b>BERT</b> 4.", "dateLastCrawled": "2021-05-13T06:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Proceedings of the 2019 Conference on Empirical Methods in Natural ...", "url": "https://aclanthology.org/volumes/D19-1/", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/volumes/D19-1", "snippet": "This paper proposes Tree Transformer, which adds an extra constraint to attention heads of the <b>bidirectional</b> Transformer <b>encoder</b> in order to encourage the attention heads to follow tree structures. The tree structures can be automatically induced from raw texts by our proposed \u201cConstituent Attention\u201d module, which is simply implemented by self-attention between two adjacent words. With the same training procedure identical to <b>BERT</b>, the experiments demonstrate the effectiveness of Tree ...", "dateLastCrawled": "2022-01-31T01:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Social Media - Worldcrunch", "url": "https://worldcrunch.com/tag/social-media", "isFamilyFriendly": true, "displayUrl": "https://worldcrunch.com/tag/social-media", "snippet": "The system, named &quot;<b>BERT</b>&quot; (<b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b>), &quot;learns&quot; to fill in blank sentences, then proves to be excellent in grammar exercises, questions and answers. It inspired Facebook&#39;s system, named &quot;Roberta;&quot; then OpenAI&#39;s GPT-3, with its 15 billion parameters and 500 billion words ingested (100 times the English version of Wikipedia); and the Chinese system Wu Dao 2.0, which is already 10 times larger.", "dateLastCrawled": "2022-01-13T22:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Exploring Strategies for Generalizable Commonsense Reasoning with Pre ...", "url": "https://deepai.org/publication/exploring-strategies-for-generalizable-commonsense-reasoning-with-pre-trained-models", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/exploring-strategies-for-generalizable-commonsense...", "snippet": "The logical commonsense probes in RICA Zhou et al. show that LMs perform <b>similar</b> to random guessing in the zero-shot setting, they are ... J. Devlin, M. Chang, K. Lee, and K. Toutanova (2019) <b>BERT</b>: pre-training of deep <b>bidirectional</b> <b>transformers</b> for language understanding. In Proc. of NAACL, pp. 4171\u20134186. Cited by: \u00a71 . Y. Elazar, H. Zhang, Y. Goldberg, and D. Roth (2021) Back to square one: bias detection, training and commonsense disentanglement in the winograd schema. arXiv preprint ...", "dateLastCrawled": "2022-02-01T20:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Pretrained <b>Transformers</b> as <b>Universal Computation Engines</b> \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/2103.05247/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2103.05247", "snippet": "We hypothesize that <b>transformers</b>, namely the self-attention layers, can be pretrained on a data-rich modality (i.e. where data is plentiful, such as a natural language corpus) and identify feature <b>representations</b> that are useful for arbitrary data sequences, enabling effective downstream transfer to different modalities without expensive finetuning of the self-attention layers. In particular, we seek to investigate what pretrained language models (LMs) are capable of in terms of generalizing ...", "dateLastCrawled": "2022-01-30T16:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Modeling With <b>Bert</b> Topic [4WA6RO]", "url": "https://biodinamicacraniosacrale.tn.it/Topic_Modeling_With_Bert.html", "isFamilyFriendly": true, "displayUrl": "https://biodinamicacraniosacrale.tn.it/Topic_Modeling_With_<b>Bert</b>.html", "snippet": "We introduce a new language representation model called <b>BERT</b>, which stands for <b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b>. (different. Pre-training is a hot topic in NLP research and models like <b>BERT</b> and GPT have definitely delivered exciting breakthroughs. Corpus was downloaded from two main sources: Bengali commoncrawl copurs downloaded from. 0 challenge (&quot;Default Project&quot;). Word embeddings. Rather than training models from scratch, the new paradigm in natural language ...", "dateLastCrawled": "2022-01-20T16:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Dictionary-based Debiasing of Pre-trained Word Embeddings", "url": "https://www.readkong.com/page/dictionary-based-debiasing-of-pre-trained-word-embeddings-4107311", "isFamilyFriendly": true, "displayUrl": "https://www.readkong.com/page/dictionary-based-debiasing-of-pre-trained-word...", "snippet": "<b>BERT</b>: Pre-training of deep <b>bidirectional</b> <b>transformers</b> for language under- Hila Gonen and Yoav Goldberg. 2019. Lipstick on a standing. In Proceedings of the 2019 Conference pig: Debiasing methods cover up systematic gender of the North American Chapter of the Association biases in word embeddings but do not remove them. for Computational Linguistics: Human Language In Proceedings of the 2019 Workshop on Widening Technologies, Volume 1 (Long and Short Papers), NLP, pages 60\u201363, Florence ...", "dateLastCrawled": "2022-01-18T08:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) HONEST: Measuring Hurtful Sentence Completion ... - researchgate.net", "url": "https://www.researchgate.net/publication/352365780_HONEST_Measuring_Hurtful_Sentence_Completion_in_Language_Models", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/352365780_HONEST_Measuring_Hurtful_Sentence...", "snippet": "PDF | On Jan 1, 2021, Debora Nozza and others published HONEST: Measuring Hurtful Sentence Completion in Language Models | Find, read and cite all the research you need on ResearchGate", "dateLastCrawled": "2022-02-01T09:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Nurse is Closer to Woman than Surgeon? Mitigating Gender-Biased ...", "url": "https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00327/96488/Nurse-is-Closer-to-Woman-than-Surgeon-Mitigating", "isFamilyFriendly": true, "displayUrl": "https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00327/96488/Nurse-is-Closer-to...", "snippet": "A set of seed words is then used for each of the categories to train an embedding using an <b>encoder</b> in a denoising autoencoder, such that gender-related biases from stereotypical words are removed, while preserving feminine information for non-discriminative female-biased words, masculine information for non-discriminative male-biased words, and neutrality of the gender-neutral words. The use of the correct set of seed words is critical for the approach. Moreover, inappropriate associations ...", "dateLastCrawled": "2021-12-26T06:03:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Computational Linguistics in the Netherlands</b> 30 | Accepted submissions", "url": "https://clin30.sites.uu.nl/accepted-submissions/", "isFamilyFriendly": true, "displayUrl": "https://clin30.sites.uu.nl/accepted-submissions", "snippet": "Specifically, <b>BERT</b> (<b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b>) developed by Google, has been achieving high accuracies in benchmarks for tasks such as text classification and named entity recognition (NER). However these tasks tend to be in English, while our task is Dutch NER. Google has released a multi-lingual <b>BERT</b> model with 104 languages, including Dutch, but modeling multiple languages in one model seems sub-optimal. We therefore pre-trained our own Dutch <b>BERT</b> models to ...", "dateLastCrawled": "2022-01-26T05:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Understanding the Usage of Online Media for Parenting from Infancy to ...", "url": "https://dl.acm.org/doi/fullHtml/10.1145/3411764.3445203", "isFamilyFriendly": true, "displayUrl": "https://dl.acm.org/doi/fullHtml/10.1145/3411764.3445203", "snippet": "<b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b> (<b>BERT</b>) : <b>BERT</b> is a pre-trained language representation model, and has achieved state-of-the-art results on eleven natural language processing tasks, outperforming baseline methods. We used the Pytorch implementation of <b>BERT</b> 4.", "dateLastCrawled": "2021-05-13T06:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Social Media - Worldcrunch", "url": "https://worldcrunch.com/tag/social-media", "isFamilyFriendly": true, "displayUrl": "https://worldcrunch.com/tag/social-media", "snippet": "The system, named &quot;<b>BERT</b>&quot; (<b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b>), &quot;learns&quot; to fill in blank sentences, then proves to be excellent in grammar exercises, questions and answers. It inspired Facebook&#39;s system, named &quot;Roberta;&quot; then OpenAI&#39;s GPT-3, with its 15 billion parameters and 500 billion words ingested (100 times the English version of Wikipedia); and the Chinese system Wu Dao 2.0, which is already 10 times larger.", "dateLastCrawled": "2022-01-13T22:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Modeling With <b>Bert</b> Topic [4WA6RO]", "url": "https://biodinamicacraniosacrale.tn.it/Topic_Modeling_With_Bert.html", "isFamilyFriendly": true, "displayUrl": "https://biodinamicacraniosacrale.tn.it/Topic_Modeling_With_<b>Bert</b>.html", "snippet": "<b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b> (<b>BERT</b>) is an extremely powerful general-purpose model that <b>can</b> be leveraged for nearly every text-based machine learning task. Photo by Gery. Contextualized Topic Models (CTM) are a family of topic models that use pre-trained <b>representations</b> of language (e.", "dateLastCrawled": "2022-01-20T16:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) Metaphor Generation with Conceptual Mappings", "url": "https://www.researchgate.net/publication/352081319_Metaphor_Generation_with_Conceptual_Mappings", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/352081319_Metaphor_Generation_with_Conceptual...", "snippet": "with 50, 100, and 300 dimensional <b>representations</b>; ... deep <b>bidirectional</b> <b>transformers</b> for language under- standing. In Proceedings of the 2019 Conference. of the North American Chapter of the ...", "dateLastCrawled": "2021-12-24T04:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "RuleBert: Teaching Soft Rules to Pre-trained Language Models | Request PDF", "url": "https://www.researchgate.net/publication/354889670_RuleBert_Teaching_Soft_Rules_to_Pre-trained_Language_Models", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/354889670_Rule<b>Bert</b>_Teaching_Soft_Rules_to_Pre...", "snippet": "Request PDF | RuleBert: Teaching Soft Rules to Pre-trained Language Models | While pre-trained language models (PLMs) are the go-to solution to tackle many natural language processing problems ...", "dateLastCrawled": "2022-01-25T05:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Anticipating Safety Issues in E2E Conversational AI: Framework and ...", "url": "https://www.arxiv-vanity.com/papers/2107.03451/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2107.03451", "snippet": "Neural machine learning methods (Nasr et al., 2019; Shokri et al., 2017), and language models in particular (Carlini et al., 2019, 2020) <b>can</b> be susceptible to training data leakage, where sensitive information <b>can</b> be extracted from the models. E2E conversational AI systems built on these methods are therefore also vulnerable to such privacy breaches. A recent commercial example of this is Lee-Luda, a chatbot which has been accused of exposing its users\u2019 personal information", "dateLastCrawled": "2022-01-06T13:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "\u7ffb\u8a33\u7d50\u679c: HypoGen: Hyperbole Generation with Commonsense and Counterfactual ...", "url": "https://fugumt.com/fugumt/paper/translated/2109.05097.pdf.html", "isFamilyFriendly": true, "displayUrl": "https://fugumt.com/fugumt/paper/translated/2109.05097.pdf.html", "snippet": "Comet: Commonsense <b>transformers</b> for automatic knowledge graph construction. 2019. Comet: \u77e5\u8b58\u30b0\u30e9\u30d5\u306e\u81ea\u52d5\u69cb\u7bc9\u306e\u305f\u3081\u306eCommonsense Transformer\u3002 0.59: arXiv preprint arXiv:1906.05317. arXiv preprint arXiv:1906.05317: 0.36: Tuhin Chakrabarty, Debanjan Ghosh, Smaranda Muresan, and Nanyun Peng. Tuhin Chakrabarty, Debanjan Ghosh, Smaranda Muresan, Nanyun Peng: 0.30: 2020a. R3: Reverse, retrieve, and rank for sarcasm generation with commonsense knowledge. 2020\u5e74\u3002 R3: \u5e38\u8b58\u77e5\u8b58 ...", "dateLastCrawled": "2021-10-22T23:32:00.0000000Z", "language": "ja", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Diff - 490f13c9b6b672ed8d14fbd036c0399a688375d6^! - asterixdb - Gitiles", "url": "https://asterix-gerrit.ics.uci.edu/plugins/gitiles/asterixdb/%2B/490f13c9b6b672ed8d14fbd036c0399a688375d6%255E%2521/", "isFamilyFriendly": true, "displayUrl": "https://asterix-gerrit.ics.uci.edu/plugins/gitiles/asterixdb...", "snippet": "[ASTERIXDB-2785][TEST] Convert RecoveryIT to SQL++ - user model changes: no - storage format changes: no - interface changes: no Details: - Convert RecoveryIT test files to SQL++.", "dateLastCrawled": "2022-01-31T22:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "[PAPER]@Telematika", "url": "https://paper.telematika.org/2019/index.xml", "isFamilyFriendly": true, "displayUrl": "https://paper.telematika.org/2019/index.xml", "snippet": "<b>BERT</b> has a Mouth, and It Must Speak: <b>BERT</b> as a Mark \u2026 Paper Group AWR 106. MIMII Dataset: Sound Dataset for Malfunctioning Industrial Machine Investigation and Inspection. YOLO Nano: a Highly Compact You Only Look Once Convolutional Neural Network for Object Detection. Gesture-to-Gesture Translation in the Wild via Category-Independent Conditional Maps. Bag of Tricks and A Strong Baseline for Deep Person Re-identification \u2026 Paper Group AWR 107. Universal Deep Beamformer for Variable Rate ...", "dateLastCrawled": "2022-01-21T06:57:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Computational Linguistics in the Netherlands</b> 30 | Accepted submissions", "url": "https://clin30.sites.uu.nl/accepted-submissions/", "isFamilyFriendly": true, "displayUrl": "https://clin30.sites.uu.nl/accepted-submissions", "snippet": "Specifically, <b>BERT</b> (<b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b>) developed by Google, has been achieving high accuracies in benchmarks for tasks such as text classification and named entity recognition (NER). However these tasks tend to be in English, while our task is Dutch NER. Google has released a multi-lingual <b>BERT</b> model with 104 languages, including Dutch, but modeling multiple languages in one model seems sub-optimal. We therefore pre-trained our own Dutch <b>BERT</b> models to ...", "dateLastCrawled": "2022-01-26T05:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Proceedings of the 2019 Conference on Empirical Methods in Natural ...", "url": "https://aclanthology.org/volumes/D19-1/", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/volumes/D19-1", "snippet": "This paper proposes Tree Transformer, which adds an extra constraint to attention heads of the <b>bidirectional</b> Transformer <b>encoder</b> in order to encourage the attention heads to follow tree structures. The tree structures <b>can</b> be automatically induced from raw texts by our proposed \u201cConstituent Attention\u201d module, which is simply implemented by self-attention between two adjacent words. With the same training procedure identical to <b>BERT</b>, the experiments demonstrate the effectiveness of Tree ...", "dateLastCrawled": "2022-01-31T01:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Social Media - Worldcrunch", "url": "https://worldcrunch.com/tag/social-media", "isFamilyFriendly": true, "displayUrl": "https://worldcrunch.com/tag/social-media", "snippet": "The system, named &quot;<b>BERT</b>&quot; (<b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b>), &quot;learns&quot; to fill in blank sentences, then proves to be excellent in grammar exercises, questions and answers. It inspired Facebook&#39;s system, named &quot;Roberta;&quot; then OpenAI&#39;s GPT-3, with its 15 billion parameters and 500 billion words ingested (100 times the English version of Wikipedia); and the Chinese system Wu Dao 2.0, which is already 10 times larger.", "dateLastCrawled": "2022-01-13T22:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) Unintended Bias in Language Model-drivenConversational Recommendation", "url": "https://www.researchgate.net/publication/357926190_Unintended_Bias_in_Language_Model-drivenConversational_Recommendation", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/357926190_Unintended_Bias_in_Language_Model...", "snippet": "ure 1 and relies on <b>BERT</b> as a conversational language <b>encoder</b> with. an AutoRec-style 41] recommendation decoder head to select a. restaurant venue given a textual statement of preference as input ...", "dateLastCrawled": "2022-01-24T02:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Abstract - arXiv", "url": "https://arxiv.org/pdf/2006.11316.pdf", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/pdf/2006.11316.pdf", "snippet": "<b>BERT</b>-base <b>encoder</b> has a self-attention module with 3 PFC layers, plus 3 more PFC layers called feed-forward network layers (FFN1, FFN2, and FFN3). The FFN layers have the following dimen-sions: FFN1 has C in = C out = 768, FFN2 has C in = 768 and C out = 3072, and FFN3 has 5Note that the grouped convolution with G =1is identical to an ordinary ...", "dateLastCrawled": "2021-10-25T05:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) <b>Pretrained Transformers as Universal Computation Engines</b>", "url": "https://www.researchgate.net/publication/349943389_Pretrained_Transformers_as_Universal_Computation_Engines", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/349943389_Pretrained_<b>Transformers</b>_as...", "snippet": "W e hypothesize that <b>transformers</b>, namely the self-attention layers, <b>can</b> be pretrained on a data-. rich modality (i.e. where data is plentiful, such as a natural language corpus) and identify ...", "dateLastCrawled": "2021-11-13T17:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Pretrained <b>Transformers</b> as <b>Universal Computation Engines</b> \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/2103.05247/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2103.05247", "snippet": "We hypothesize that <b>transformers</b>, namely the self-attention layers, <b>can</b> be pretrained on a data-rich modality (i.e. where data is plentiful, such as a natural language corpus) and identify feature <b>representations</b> that are useful for arbitrary data sequences, enabling effective downstream transfer to different modalities without expensive finetuning of the self-attention layers. In particular, we seek to investigate what pretrained language models (LMs) are capable of in terms of generalizing ...", "dateLastCrawled": "2022-01-30T16:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Modeling With <b>Bert</b> Topic [4WA6RO]", "url": "https://biodinamicacraniosacrale.tn.it/Topic_Modeling_With_Bert.html", "isFamilyFriendly": true, "displayUrl": "https://biodinamicacraniosacrale.tn.it/Topic_Modeling_With_<b>Bert</b>.html", "snippet": "<b>BERT</b> stands for <b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b> , open-sourced by Google in 2018., 2018; Liu et al. Pre-training is a hot topic in NLP research and models like <b>BERT</b> and GPT have definitely delivered exciting breakthroughs. Topic Modeling is a technique to understand and extract the hidden topics from large volumes of text.", "dateLastCrawled": "2022-01-20T16:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "SqueezeBERT: What <b>can</b> computer vision teach NLP about efficient neural ...", "url": "https://www.arxiv-vanity.com/papers/2006.11316/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2006.11316", "snippet": "Humans read and write hundreds of billions of messages every day. Further, due to the availability of large datasets, large computing systems, and better neural network models, natural language processing (NLP) technology has made significant strides in understanding, proofreading, and organizing these messages. Thus, there is a significant opportunity to deploy NLP in myriad applications to help web users, social networks, and businesses. In particular, we consider smartphones and other ...", "dateLastCrawled": "2021-09-24T23:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Paper Digest: EMNLP 2019 Highlights</b> \u2013 Paper Digest", "url": "https://www.paperdigest.org/2019/11/emnlp-2019-highlights/", "isFamilyFriendly": true, "displayUrl": "https://www.paperdigest.org/2019/11/emnlp-2019-highlights", "snippet": "Download EMNLP-2019-Paper-Digests.pdf \u2013 highlights of all ~680 EMNLP-2019 papers. The Conference on Empirical Methods in Natural Language Processing (EMNLP) is one of the top natural language processing conferences in the world. In 2019, it is to be held in Hong Kong, China. There were 1,813 long paper submissions, of which 465 were accepted ...", "dateLastCrawled": "2022-01-25T18:33:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "14.8. <b>Bidirectional Encoder</b> <b>Representations</b> <b>from Transformers</b> (<b>BERT</b> ...", "url": "https://d2l.ai/chapter_natural-language-processing-pretraining/bert.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_natural-language-processing-pretraining/<b>bert</b>.html", "snippet": "Combining the best of both worlds, <b>BERT</b> (<b>Bidirectional Encoder</b> <b>Representations</b> <b>from Transformers</b>) encodes context bidirectionally and requires minimal architecture changes for a wide range of natural language processing tasks [Devlin et al., 2018]. Using a pretrained transformer <b>encoder</b>, <b>BERT</b> is able to represent any token based on its <b>bidirectional</b> context. During supervised <b>learning</b> of downstream tasks, <b>BERT</b> is similar to GPT in two aspects. First, <b>BERT</b> <b>representations</b> will be fed into an ...", "dateLastCrawled": "2022-02-02T10:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Intuitive Introduction to BERT \u2013 MachineCurve</b>", "url": "https://www.machinecurve.com/index.php/2021/01/04/intuitive-introduction-to-bert/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/index.php/2021/01/04/<b>intuitive-introduction-to-bert</b>", "snippet": "This hampers <b>learning</b> unnecessarily, they argue, and they proposed a <b>bidirectional</b> variant instead: <b>BERT</b>, or <b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b>. It is covered in this article. Firstly, we\u2019ll briefly take a look at finetuning-based approaches in NLP, which is followed by <b>BERT</b> as well.", "dateLastCrawled": "2022-01-30T22:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Med-BERT: pretrained contextualized embeddings on large</b>-scale ...", "url": "https://www.nature.com/articles/s41746-021-00455-y", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41746-021-00455-y", "snippet": "Recently, <b>bidirectional</b> <b>encoder</b> <b>representations</b> <b>from transformers</b> (<b>BERT</b>) and related models have achieved tremendous successes in the natural language processing domain. The pretraining of <b>BERT</b> on ...", "dateLastCrawled": "2022-01-28T20:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "DNABERT: <b>pre-trained Bidirectional Encoder Representations from</b> ...", "url": "https://www.researchgate.net/publication/349060790_DNABERT_pre-trained_Bidirectional_Encoder_Representations_from_Transformers_model_for_DNA-language_in_genome", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/349060790_DNA<b>BERT</b>_pre-trained_<b>Bidirectional</b>...", "snippet": "<b>Bidirectional</b> <b>encoder</b> <b>representations</b> from Transformer (<b>BERT</b>) is a language-based deep <b>learning</b> model that is highly interpretable. Therefore, a model based on <b>BERT</b> architecture can potentially ...", "dateLastCrawled": "2022-01-29T03:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "How to Optimize Your Content for Search Questions using Deep <b>Learning</b> ...", "url": "https://blogs.bing.com/webmaster/july-2020/How-to-Optimize-Your-Content-for-Search-Questions-using-Deep-Learning", "isFamilyFriendly": true, "displayUrl": "https://blogs.bing.com/webmaster/july-2020/How-to-Optimize-Your-Content-for-Search...", "snippet": "<b>BERT</b> - <b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> of <b>Transformers</b>. One of the several fundamental system that Bing and other major search engines use to answer questions is called <b>BERT</b> (<b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> of <b>Transformers</b>.) As stated by Jeffrey Zhu, Program Manager of the Bing Platform in the article Bing delivers its largest improvement in search experience using Azure GPUs: \u201cRecently, there was a breakthrough in natural language understanding with a type of model called ...", "dateLastCrawled": "2022-01-18T01:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "The Ultimate Guide To Different Word Embedding Techniques In NLP ...", "url": "https://www.kdnuggets.com/2021/11/guide-word-embedding-techniques-nlp.html", "isFamilyFriendly": true, "displayUrl": "https://www.kdnuggets.com/2021/11/guide-word-embedding-techniques-nlp.html", "snippet": "<b>BERT</b> \u2014 <b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b>. Introduced by Google in 2019, <b>BERT</b> belongs to a class of NLP-based language algorithms known as <b>transformers</b>. <b>BERT</b> is a massive pre-trained deeply <b>bidirectional</b> <b>encoder</b>-based transformer model that comes in two variants. <b>BERT</b>-Base has 110 million parameters, and <b>BERT</b>-Large has ...", "dateLastCrawled": "2022-02-03T07:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "How to perform Text Summarization with Python, HuggingFace <b>Transformers</b> ...", "url": "https://www.machinecurve.com/index.php/2020/12/21/easy-text-summarization-with-huggingface-transformers-and-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/index.php/2020/12/21/easy-text-summarization-with-hugging...", "snippet": "The <b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b> by Devlin et al. (2018) takes the <b>encoder</b> segment from the classic (or vanilla) Transformer, slightly changes how the inputs are generated (by means of WordPiece rather than learned embeddings) and changes the <b>learning</b> task into a Masked Language Model plus Next Sentence Prediction (NSP) rather than training a simple language model. They also follow the argument for pretraining and subsequent fine-tuning: by taking the <b>encoder</b> ...", "dateLastCrawled": "2022-02-02T23:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Text Classification: <b>BERT</b> vs <b>DNN</b>. (Deep neural network (<b>DNN</b>) with\u2026 | by ...", "url": "https://eng.zemosolabs.com/text-classification-bert-vs-dnn-b226497c9de7", "isFamilyFriendly": true, "displayUrl": "https://eng.zemosolabs.com/text-classification-<b>bert</b>-vs-<b>dnn</b>-b226497c9de7", "snippet": "Reference Multiple layer neural network, <b>DNN</b> Architecture()2. <b>BERT</b>. <b>BERT</b> (<b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b>) is an open-sourced NLP pre-training model developed by researchers at Google in 2018. It\u2019s built on pre-training contextual <b>representations</b> \u2014 including Semi-supervised Sequence <b>Learning</b> (by Andrew Dai and Quoc Le), Elmo (by Matthew Peters and researchers from AI2 and UW CSE), ULMFiT (by fast.ai founder Jeremy Howard and Sebastian Ruder), the OpenAI ...", "dateLastCrawled": "2022-01-20T10:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Text mining-based word <b>representations</b> for biomedical data analysis and ...", "url": "https://www.biorxiv.org/content/10.1101/2020.12.09.417733v1.full.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.biorxiv.org/content/10.1101/2020.12.09.417733v1.full.pdf", "snippet": "46 Several studies employed supervised <b>machine</b> <b>learning</b> algorithms to identify and extract available under aCC-BY 4.0 ... 110 models such as <b>BERT</b> (<b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b>) [19] and ELMO (Embeddings from Language Models) [20]111 that create contextualized word 112 <b>representations</b>. Such models support fine-tuning on specific tasks and have shown effective 113 performance improvements in diverse NLP tasks such as question answering and text 114 classification ...", "dateLastCrawled": "2021-11-14T21:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "The Ultimate Guide To Different Word Embedding Techniques In NLP ...", "url": "https://machinelearningmastery.in/2021/11/10/the-ultimate-guide-to-different-word-embedding-techniques-in-nlp/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.in/2021/11/10/the-ultimate-guide-to-different-word...", "snippet": "Let\u2019s have a look at some of the most promising word embedding techniques in NLP. 1. TF-IDF \u2014 Term Frequency-Inverse Document Frequency. TF-IDF is a <b>machine</b> <b>learning</b> (ML) algorithm based on a statistical measure of finding the relevance of words in the text.", "dateLastCrawled": "2022-01-09T14:18:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(bert (bidirectional encoder representations from transformers))  is like +(babysitter)", "+(bert (bidirectional encoder representations from transformers)) is similar to +(babysitter)", "+(bert (bidirectional encoder representations from transformers)) can be thought of as +(babysitter)", "+(bert (bidirectional encoder representations from transformers)) can be compared to +(babysitter)", "machine learning +(bert (bidirectional encoder representations from transformers) AND analogy)", "machine learning +(\"bert (bidirectional encoder representations from transformers) is like\")", "machine learning +(\"bert (bidirectional encoder representations from transformers) is similar\")", "machine learning +(\"just as bert (bidirectional encoder representations from transformers)\")", "machine learning +(\"bert (bidirectional encoder representations from transformers) can be thought of as\")", "machine learning +(\"bert (bidirectional encoder representations from transformers) can be compared to\")"]}