{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) The zoo <b>of Fairness</b> <b>metrics</b> in Machine Learning", "url": "https://www.researchgate.net/publication/352054128_The_zoo_of_Fairness_metrics_in_Machine_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/352054128_The_zoo_<b>of_Fairness</b>_<b>metrics</b>_in...", "snippet": "PDF | In the recent years, the problem of addressing <b>fairness</b> in Machine Learning (ML) and automatic decision-making has attracted a lot of attention in... | Find, read and cite all the research ...", "dateLastCrawled": "2021-12-28T22:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "On <b>the (im)possibility of fairness</b> | Request PDF", "url": "https://www.researchgate.net/publication/308610093_On_the_impossibility_of_fairness", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/308610093_On_<b>the_impossibility_of_fairness</b>", "snippet": "A section of AI <b>fairness</b> research worked on ensuring <b>fairness</b> through the preprocessing stage of ML pipeline by optimizing for <b>fairness</b> in <b>data</b> used to train the ML algorithm [6,9,10,11,12,13, 14 ...", "dateLastCrawled": "2022-01-22T10:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Fairness in Machine Learning: A Survey</b> | DeepAI", "url": "https://deepai.org/publication/fairness-in-machine-learning-a-survey", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>fairness-in-machine-learning-a-survey</b>", "snippet": "These include a <b>set</b> of ml inspired group-based <b>fairness</b> <b>metrics</b> that address different forms of unfairness to address potential biases in collaborative filtering recommender systems stemming from a population imbalance or observation bias [Yao2017], <b>fairness</b> goals for recommender systems as overcoming algorithmic bias and making neutral recommendations independent of group membership (e.g., based on gender or age) [zhu2018fairness], recommendation calibration, i.e., the proportional ...", "dateLastCrawled": "2022-01-18T13:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "DS-GA 3001.009: Responsible <b>Data</b> Science", "url": "https://dataresponsibly.github.io/courses/documents/spring20/Lecture2.pdf", "isFamilyFriendly": true, "displayUrl": "https://<b>data</b>responsibly.github.io/courses/documents/spring20/Lecture2.pdf", "snippet": "\u2022 also in less-obvious <b>set</b>-ups, <b>like</b> online advertising ... Kleinberg et al. [29] establish the <b>incompatibility</b> of di\ufffferent <b>fairness</b> measures when the overall risk Pr(Y = 1 | (X ) = i) dif-fers between groups i. However, the tension we identify between maximizing public safety and satisfying various notions of algorith- mic <b>fairness</b> typically persists even if groups have the <b>same</b> overall risk. To demonstrate this phenomenon, Figure 1 shows risk score distributions for <b>two</b> hypothetical ...", "dateLastCrawled": "2022-01-29T05:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Why <b>fairness</b> cannot be automated: Bridging the gap between EU non ...", "url": "https://www.sciencedirect.com/science/article/pii/S0267364921000406", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0267364921000406", "snippet": "Adoption of CDD will help ensure discriminatory thresholds and <b>fairness</b> <b>metrics</b> are not arbitrarily chosen and \u2018frozen\u2019 in code, 8 which would unjustifiably subvert case-specific judicial interpretation of non-discrimination law and implicitly shift this power to system developers. 9. Our aim is to increase dialogue between the legal and technology communities in order to create legally sound and scalable solutions for <b>fairness</b> and non-discrimination in automated systems in Europe. We ...", "dateLastCrawled": "2021-12-27T00:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Review of Mathematical frameworks for Fairness in</b> Machine Learning | DeepAI", "url": "https://deepai.org/publication/review-of-mathematical-frameworks-for-fairness-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>review-of-mathematical-frameworks-for-fairness-in</b>...", "snippet": "In this work, we tackle only these <b>two</b> main notions <b>of fairness</b> developed among the machine learning community. There are other definitions such as avoiding disparate treatment or predictive parity, defined respectively as or .A decision making system suffers from disparate treatment if it provides different outcomes for different groups of <b>people</b> with the <b>same</b> (or similar) values of non-sensitive features but different values of sensitive features [].In other words, (partly) basing the ...", "dateLastCrawled": "2021-12-10T16:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Mirror Mirror", "url": "https://shiraamitchell.github.io/fairness/", "isFamilyFriendly": true, "displayUrl": "https://shiraamitchell.github.io/<b>fairness</b>", "snippet": "The first <b>set</b> <b>of fairness</b> definitions we will consider are those that assert equal <b>metrics</b> of the classifier across groups. Different domains might emphasize different <b>metrics</b>. For example, in a medical test we might be highly concerned with false negative diagnosis (in which case a dangerous condition could go untreated), and with false positive diagnosis (which could result in an unnecessary and dangerous medical intervention). The plurality of <b>metrics</b> give rise to a plurality <b>of fairness</b> ...", "dateLastCrawled": "2022-01-06T19:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "The Landscape and Gaps in Open Source <b>Fairness</b> Toolkits", "url": "https://dl.acm.org/doi/fullHtml/10.1145/3411764.3445261", "isFamilyFriendly": true, "displayUrl": "https://dl.acm.org/doi/fullHtml/10.1145/3411764.3445261", "snippet": "To test one&#39;s familiarity with issues <b>of fairness</b>, we asked which <b>two</b> <b>fairness</b> definitions are generally incompatible, and 44% selected the correct answer: equal odds and positive predictive parity, whose <b>incompatibility</b> was a well-cited example in the U.S. criminal recidivism scoring model and academically proven . 36% gave the incorrect answer of equal opportunity and equal odds; equal opportunity is a subset of equal odds, meaning that if equal odds is satisfied, it implies equal ...", "dateLastCrawled": "2022-01-16T20:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Who\u2019s the fairest of them all</b>? \u2013 Feminist Philosophers", "url": "https://feministphilosophers.wordpress.com/2011/10/30/whos-the-fairest-of-them-all/", "isFamilyFriendly": true, "displayUrl": "https://feministphilosophers.wordpress.com/2011/10/30/<b>whos-the-fairest-of-them-all</b>", "snippet": "First, while the <b>data</b> themselves may be empirical, the choice of <b>metrics</b> and the perceived relationship between those <b>metrics</b> and such abstract concepts as \u201csocial justice\u201d (or even \u201cquality of life\u201d) are subjective, and I think that there are some differences in the way that that the US approaches this versus a number of OECD countries. Accordingly, one might expect that if the assumptions behind the study reflect, say, a Northern European approach to social welfare, these ...", "dateLastCrawled": "2021-12-07T12:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Types of Conflict in Organisations (with Causes and Remedies</b>)", "url": "https://www.economicsdiscussion.net/management/conflict/types-of-conflict-in-organisations-with-causes-and-remedies/31472", "isFamilyFriendly": true, "displayUrl": "https://www.economicsdiscussion.net/management/conflict/types-of-conflict-in...", "snippet": "It may be between <b>two</b> persons of the <b>same</b> group and it may be called as intra group conflict. If conflict is between <b>two</b> members of the different groups it is called as inter-individual conflict but if it is blown up, it may develop as inter-group conflict. There are a number of causes why individual conflicts arises within organization and they are as follows: 1. Mismatching of individual needs with organizational reward. 2. The second cause of conflict may be the situation of widespread ...", "dateLastCrawled": "2022-02-02T19:10:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Fairness in Machine Learning: A Survey</b> | DeepAI", "url": "https://deepai.org/publication/fairness-in-machine-learning-a-survey", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>fairness-in-machine-learning-a-survey</b>", "snippet": "These include a <b>set</b> of ml inspired group-based <b>fairness</b> <b>metrics</b> that address different forms of unfairness to address potential biases in collaborative filtering recommender systems stemming from a population imbalance or observation bias [Yao2017], <b>fairness</b> goals for recommender systems as overcoming algorithmic bias and making neutral recommendations independent of group membership (e.g., based on gender or age) [zhu2018fairness], recommendation calibration, i.e., the proportional ...", "dateLastCrawled": "2022-01-18T13:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) The zoo <b>of Fairness</b> <b>metrics</b> in Machine Learning", "url": "https://www.researchgate.net/publication/352054128_The_zoo_of_Fairness_metrics_in_Machine_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/352054128_The_zoo_<b>of_Fairness</b>_<b>metrics</b>_in...", "snippet": "PDF | In the recent years, the problem of addressing <b>fairness</b> in Machine Learning (ML) and automatic decision-making has attracted a lot of attention in... | Find, read and cite all the research ...", "dateLastCrawled": "2021-12-28T22:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "On <b>the (im)possibility of fairness</b> | Request PDF", "url": "https://www.researchgate.net/publication/308610093_On_the_impossibility_of_fairness", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/308610093_On_<b>the_impossibility_of_fairness</b>", "snippet": "A section of AI <b>fairness</b> research worked on ensuring <b>fairness</b> through the preprocessing stage of ML pipeline by optimizing for <b>fairness</b> in <b>data</b> used to train the ML algorithm [6,9,10,11,12,13, 14 ...", "dateLastCrawled": "2022-01-22T10:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Review of Mathematical frameworks for Fairness in</b> Machine Learning | DeepAI", "url": "https://deepai.org/publication/review-of-mathematical-frameworks-for-fairness-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>review-of-mathematical-frameworks-for-fairness-in</b>...", "snippet": "In this work, we tackle only these <b>two</b> main notions <b>of fairness</b> developed among the machine learning community. There are other definitions such as avoiding disparate treatment or predictive parity, defined respectively as or .A decision making system suffers from disparate treatment if it provides different outcomes for different groups of <b>people</b> with the <b>same</b> (or <b>similar</b>) values of non-sensitive features but different values of sensitive features [].In other words, (partly) basing the ...", "dateLastCrawled": "2021-12-10T16:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Mirror Mirror", "url": "https://shiraamitchell.github.io/fairness/", "isFamilyFriendly": true, "displayUrl": "https://shiraamitchell.github.io/<b>fairness</b>", "snippet": "The first <b>set</b> <b>of fairness</b> definitions we will consider are those that assert equal <b>metrics</b> of the classifier across groups. Different domains might emphasize different <b>metrics</b>. For example, in a medical test we might be highly concerned with false negative diagnosis (in which case a dangerous condition could go untreated), and with false positive diagnosis (which could result in an unnecessary and dangerous medical intervention). The plurality of <b>metrics</b> give rise to a plurality <b>of fairness</b> ...", "dateLastCrawled": "2022-01-06T19:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Inconsistent allocations of harms versus benefits may exacerbate ... - PNAS", "url": "https://www.pnas.org/content/117/16/8820", "isFamilyFriendly": true, "displayUrl": "https://<b>www.pnas.org</b>/content/117/16/8820", "snippet": "Now consider an alternative scenario involving the <b>same</b> <b>two</b> communities. Imagine, however, that instead of adding new flights, O\u2019Hare was canceling flights, and officials now needed to decide whether to reduce traffic in the nosier Wood Dale, or the less noisy Elmhurst. When a different group of 100 participants read about this scenario, a large majority (86%) thought the decrease in air traffic should be allocated to the worse-off Wood Dale, while only 14% thought it should be allocated ...", "dateLastCrawled": "2021-10-13T15:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine learning fairness notions: Bridging the</b> gap with real-world ...", "url": "https://www.sciencedirect.com/science/article/pii/S0306457321001321", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0306457321001321", "snippet": "Our attempt to answer this question consists in (1) identifying the <b>set</b> <b>of fairness</b>-related characteristics of the real-world scenario at hand, (2) analyzing the behavior of each <b>fairness</b> notion, and then (3) fitting these <b>two</b> elements to recommend the most suitable <b>fairness</b> notion in every specific setup. The results are summarized in a decision diagram that can be used by practitioners and policy makers to navigate the relatively large catalog of ML <b>fairness</b> notions.", "dateLastCrawled": "2022-02-03T07:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "The Landscape and Gaps in Open Source <b>Fairness</b> Toolkits", "url": "https://dl.acm.org/doi/fullHtml/10.1145/3411764.3445261", "isFamilyFriendly": true, "displayUrl": "https://dl.acm.org/doi/fullHtml/10.1145/3411764.3445261", "snippet": "Test against standard <b>set</b> <b>of fairness</b> <b>metrics</b>, such as demographic parity, equal opportunity, etc.; Identify the most \u201c<b>similar</b>\u201d cases to compare predictions (only available in the Google What-if tool); Experiment with single or multiple decision thresholds for classification problems; Provide explanations for a prediction, e.g. feature importance, through integration with explanation toolkits; Produce visualisations on <b>fairness</b> and accuracy <b>metrics</b>; and; Augment model-building process ...", "dateLastCrawled": "2022-01-16T20:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "The efficacy of tournament designs", "url": "https://www.readkong.com/page/the-efficacy-of-tournament-designs-4171157", "isFamilyFriendly": true, "displayUrl": "https://www.readkong.com/page/the-efficacy-of-tournament-designs-4171157", "snippet": "2.3 Tournament <b>metrics</b> Efficacy is quantified by <b>looking</b> at the differences between the real and observed rankings. <b>Two</b> types of indicators are used for this purpose, the average rank of the top players and the (weighted) number of inversions. The former has been applied by Scarf et al. (2009). It compares the top players in the observed ranking to the strongest players by dividing the sum of the ranks of the players finishing in the top position with the theoretical minimum of 1 + 2 ...", "dateLastCrawled": "2022-01-21T09:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Types of Conflict in Organisations (with Causes and Remedies</b>)", "url": "https://www.economicsdiscussion.net/management/conflict/types-of-conflict-in-organisations-with-causes-and-remedies/31472", "isFamilyFriendly": true, "displayUrl": "https://www.economicsdiscussion.net/management/conflict/types-of-conflict-in...", "snippet": "It may be between <b>two</b> persons of the <b>same</b> group and it may be called as intra group conflict. If conflict is between <b>two</b> members of the different groups it is called as inter-individual conflict but if it is blown up, it may develop as inter-group conflict. There are a number of causes why individual conflicts arises within organization and they are as follows: 1. Mismatching of individual needs with organizational reward. 2. The second cause of conflict may be the situation of widespread ...", "dateLastCrawled": "2022-02-02T19:10:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Fairness in Machine Learning: A Survey</b> | DeepAI", "url": "https://deepai.org/publication/fairness-in-machine-learning-a-survey", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>fairness-in-machine-learning-a-survey</b>", "snippet": "These include a <b>set</b> of ml inspired group-based <b>fairness</b> <b>metrics</b> that address different forms of unfairness to address potential biases in collaborative filtering recommender systems stemming from a population imbalance or observation bias [Yao2017], <b>fairness</b> goals for recommender systems as overcoming algorithmic bias and making neutral recommendations independent of group membership (e.g., based on gender or age) [zhu2018fairness], recommendation calibration, i.e., the proportional ...", "dateLastCrawled": "2022-01-18T13:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Counterfactual Fairness</b> - ResearchGate", "url": "https://www.researchgate.net/publication/315454664_Counterfactual_Fairness", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/315454664_<b>Counterfactual_Fairness</b>", "snippet": "protected attribute and the <b>data</b>, certain de\ufb01nitions <b>of fairness</b> <b>can</b> actually ... treating equally <b>two</b> individuals with the <b>same</b>. W. in a way that is also counterfactually fair. Relation to ...", "dateLastCrawled": "2022-01-26T14:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>Counterfactual Fairness</b>", "url": "https://www.researchgate.net/publication/324600593_Counterfactual_Fairness", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/324600593_<b>Counterfactual_Fairness</b>", "snippet": "The counterfactual definition <b>of fairness</b> considers a model fair if upon intervention and change of the race, its output does not change (Kusner et al., 2017). The intervention is usually ...", "dateLastCrawled": "2021-12-11T00:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Why <b>fairness</b> cannot be automated: Bridging the gap between EU non ...", "url": "https://www.sciencedirect.com/science/article/pii/S0267364921000406", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0267364921000406", "snippet": "<b>Looking</b> at the <b>two</b> formulas it is immediately obvious that negative dominance strictly dominates the test of demographic disparity, and is a much harder test to satisfy. As a result, whenever a test shows that negative dominance exists, demographic disparity by definition is also present. The key insight into understanding these <b>two</b> tests and the interplay between them is <b>two</b>-fold: 1. If the group with a protected attribute occupies fifty percent of the population the <b>two</b> tests are ...", "dateLastCrawled": "2021-12-27T00:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>JUSTICE</b> AND <b>FAIRNESS</b> | Annual Review of Political Science", "url": "https://www.annualreviews.org/doi/full/10.1146/annurev.polisci.1.1.425", "isFamilyFriendly": true, "displayUrl": "https://www.annualreviews.org/doi/full/10.1146/annurev.polisci.1.1.425", "snippet": "Stability involves <b>two</b> questions: the first is whether <b>people</b> who grow up under just institutions (as the political conception defines them) acquire a normally sufficient sense of <b>justice</b> so that they generally comply with these institutions. The second question is whether in view of the general facts that characterize a democracy&#39;s public political culture, and in particular the fact of reasonable pluralism, the political conception <b>can</b> be the focus of an overlapping consensus. I assume ...", "dateLastCrawled": "2022-02-02T13:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Mirror Mirror", "url": "https://shiraamitchell.github.io/fairness/", "isFamilyFriendly": true, "displayUrl": "https://shiraamitchell.github.io/<b>fairness</b>", "snippet": "The first <b>set</b> <b>of fairness</b> definitions we will consider are those that assert equal <b>metrics</b> of the classifier across groups. Different domains might emphasize different <b>metrics</b>. For example, in a medical test we might be highly concerned with false negative diagnosis (in which case a dangerous condition could go untreated), and with false positive diagnosis (which could result in an unnecessary and dangerous medical intervention). The plurality of <b>metrics</b> give rise to a plurality <b>of fairness</b> ...", "dateLastCrawled": "2022-01-06T19:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Who\u2019s the fairest of them all</b>? - News feminist philosophers <b>can</b> use", "url": "https://feministphilosophers.wordpress.com/2011/10/30/whos-the-fairest-of-them-all/", "isFamilyFriendly": true, "displayUrl": "https://feministphilosophers.wordpress.com/2011/10/30/<b>whos-the-fairest-of-them-all</b>", "snippet": "First, while the <b>data</b> themselves may be empirical, the choice of <b>metrics</b> and the perceived relationship between those <b>metrics</b> and such abstract concepts as \u201csocial justice\u201d (or even \u201cquality of life\u201d) are subjective, and I think that there are some differences in the way that that the US approaches this versus a number of OECD countries. Accordingly, one might expect that if the assumptions behind the study reflect, say, a Northern European approach to social welfare, these ...", "dateLastCrawled": "2021-12-07T12:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Types of Conflict in Organisations (with Causes and Remedies</b>)", "url": "https://www.economicsdiscussion.net/management/conflict/types-of-conflict-in-organisations-with-causes-and-remedies/31472", "isFamilyFriendly": true, "displayUrl": "https://www.economicsdiscussion.net/management/conflict/types-of-conflict-in...", "snippet": "The conflict may be with an individual when there is an <b>incompatibility</b> between his or her own goal and event; may be between <b>two</b> individual or between <b>two</b> groups of organisation. On the basis of direction, the <b>types of conflict in organisations</b> are:- 1. Vertical Conflict 2. Horizontal Conflict 3. Line and Staff Conflict. On the basis of level, the <b>types of conflict in organisations</b> <b>can</b> be studied under:-1. Individual Level 2. Group Level 3. Organizational Level. ADVERTISEMENTS: Conflict at ...", "dateLastCrawled": "2022-02-02T19:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(DOC) <b>NMIMS Solved Assignment Answer Sheet April 2020</b> Available ...", "url": "https://www.academia.edu/42021671/NMIMS_Solved_Assignment_Answer_Sheet_April_2020_Available", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/42021671/<b>NMIMS_Solved_Assignment_Answer_Sheet_April_2020</b>...", "snippet": "Mail us at Projectreports94@gmail.com CALL NOW 09773820734 (WHATSAPP ) We help students in preparing their MBA Case Study Answers MBA Assignment Solutions Project Report Thesis. SEMESTER -I 1. Business Communication &amp; Etiquette 2. Business", "dateLastCrawled": "2022-02-03T03:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Collaborating in the Absence of Trust? What Collaborative Governance ...", "url": "https://journals.sagepub.com/doi/10.1177/0275074018773089", "isFamilyFriendly": true, "displayUrl": "https://journals.sagepub.com/doi/10.1177/0275074018773089", "snippet": "For instance, Morgan\u2019s (2014) review of the General Social Survey <b>data</b> reveals that the percentage of American respondents who agreed that \u201cmost <b>people</b> <b>can</b> be trusted\u201d dropped from 46% at the survey\u2019s start in 1972 to approximately 32% four decades later. These findings support the contemporary interest in understanding and building trust, particularly when trust may not be assumed from the start.", "dateLastCrawled": "2022-01-04T08:09:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "On <b>the (im)possibility of fairness</b> | Request PDF", "url": "https://www.researchgate.net/publication/308610093_On_the_impossibility_of_fairness", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/308610093_On_<b>the_impossibility_of_fairness</b>", "snippet": "They <b>can</b> be broadly classified into <b>two</b> categories. A group of <b>metrics</b> for individual <b>fairness</b> (or equality or equality of treatment) focuses on providing equal treatment to equal <b>people</b> [10 ...", "dateLastCrawled": "2022-01-22T10:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Fairness in Machine Learning: A Survey</b> | DeepAI", "url": "https://deepai.org/publication/fairness-in-machine-learning-a-survey", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>fairness-in-machine-learning-a-survey</b>", "snippet": "These include a <b>set</b> of ml inspired group-based <b>fairness</b> <b>metrics</b> that address different forms of unfairness to address potential biases in collaborative filtering recommender systems stemming from a population imbalance or observation bias [Yao2017], <b>fairness</b> goals for recommender systems as overcoming algorithmic bias and making neutral recommendations independent of group membership (e.g., based on gender or age) [zhu2018fairness], recommendation calibration, i.e., the proportional ...", "dateLastCrawled": "2022-01-18T13:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A Survey on Bias and <b>Fairness</b> in Machine Learning \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/1908.09635/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1908.09635", "snippet": "With the widespread use of AI systems and applications in our everyday lives, it is important to take <b>fairness</b> issues into consideration while designing and engineering these types of systems. Such systems <b>can</b> be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that the decisions do not reflect discriminatory behavior toward certain groups or populations. We have recently seen work in machine learning, natural language ...", "dateLastCrawled": "2021-11-15T17:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) Why <b>Fairness</b> Cannot Be Automated: Bridging the Gap Between EU Non ...", "url": "https://www.researchgate.net/publication/340257159_Why_Fairness_Cannot_Be_Automated_Bridging_the_Gap_Between_EU_Non-Discrimination_Law_and_AI", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/340257159_Why_<b>Fairness</b>_<b>Can</b>not_Be_Automated...", "snippet": "<b>Data</b> Protection in the Big <b>Data</b> Era, in G ROUP P RIVACY 139 \u2013 158 (2017); L EE A. B YGRAVE, <b>D ATA</b> PROTECTION LAW : A PPROACHING ITS RAT IONALE , LOGIC AND LIMITS (2002). W ACHTER , M ITTELSTADT ...", "dateLastCrawled": "2022-01-27T14:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine learning fairness notions: Bridging the</b> gap with real-world ...", "url": "https://www.sciencedirect.com/science/article/pii/S0306457321001321", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0306457321001321", "snippet": "Our attempt to answer this question consists in (1) identifying the <b>set</b> <b>of fairness</b>-related characteristics of the real-world scenario at hand, (2) analyzing the behavior of each <b>fairness</b> notion, and then (3) fitting these <b>two</b> elements to recommend the most suitable <b>fairness</b> notion in every specific setup. The results are summarized in a decision diagram that <b>can</b> be used by practitioners and policy makers to navigate the relatively large catalog of ML <b>fairness</b> notions.", "dateLastCrawled": "2022-02-03T07:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Review of Mathematical frameworks for Fairness in</b> Machine Learning | DeepAI", "url": "https://deepai.org/publication/review-of-mathematical-frameworks-for-fairness-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>review-of-mathematical-frameworks-for-fairness-in</b>...", "snippet": "In this work, we tackle only these <b>two</b> main notions <b>of fairness</b> developed among the machine learning community. There are other definitions such as avoiding disparate treatment or predictive parity, defined respectively as or .A decision making system suffers from disparate treatment if it provides different outcomes for different groups of <b>people</b> with the <b>same</b> (or similar) values of non-sensitive features but different values of sensitive features [].In other words, (partly) basing the ...", "dateLastCrawled": "2021-12-10T16:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Mirror Mirror", "url": "https://shiraamitchell.github.io/fairness/", "isFamilyFriendly": true, "displayUrl": "https://shiraamitchell.github.io/<b>fairness</b>", "snippet": "The first <b>set</b> <b>of fairness</b> definitions we will consider are those that assert equal <b>metrics</b> of the classifier across groups. Different domains might emphasize different <b>metrics</b>. For example, in a medical test we might be highly concerned with false negative diagnosis (in which case a dangerous condition could go untreated), and with false positive diagnosis (which could result in an unnecessary and dangerous medical intervention). The plurality of <b>metrics</b> give rise to a plurality <b>of fairness</b> ...", "dateLastCrawled": "2022-01-06T19:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "The Landscape and Gaps in Open Source <b>Fairness</b> Toolkits", "url": "https://dl.acm.org/doi/fullHtml/10.1145/3411764.3445261", "isFamilyFriendly": true, "displayUrl": "https://dl.acm.org/doi/fullHtml/10.1145/3411764.3445261", "snippet": "To test one&#39;s familiarity with issues <b>of fairness</b>, we asked which <b>two</b> <b>fairness</b> definitions are generally incompatible, and 44% selected the correct answer: equal odds and positive predictive parity, whose <b>incompatibility</b> was a well-cited example in the U.S. criminal recidivism scoring model and academically proven . 36% gave the incorrect answer of equal opportunity and equal odds; equal opportunity is a subset of equal odds, meaning that if equal odds is satisfied, it implies equal ...", "dateLastCrawled": "2022-01-16T20:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>JUSTICE</b> AND <b>FAIRNESS</b> | Annual Review of Political Science", "url": "https://www.annualreviews.org/doi/full/10.1146/annurev.polisci.1.1.425", "isFamilyFriendly": true, "displayUrl": "https://www.annualreviews.org/doi/full/10.1146/annurev.polisci.1.1.425", "snippet": "Stability involves <b>two</b> questions: the first is whether <b>people</b> who grow up under just institutions (as the political conception defines them) acquire a normally sufficient sense of <b>justice</b> so that they generally comply with these institutions. The second question is whether in view of the general facts that characterize a democracy&#39;s public political culture, and in particular the fact of reasonable pluralism, the political conception <b>can</b> be the focus of an overlapping consensus. I assume ...", "dateLastCrawled": "2022-02-02T13:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Inconsistent allocations of harms versus benefits may exacerbate ... - PNAS", "url": "https://www.pnas.org/content/117/16/8820", "isFamilyFriendly": true, "displayUrl": "https://<b>www.pnas.org</b>/content/117/16/8820", "snippet": "Now consider an alternative scenario involving the <b>same</b> <b>two</b> communities. Imagine, however, that instead of adding new flights, O\u2019Hare was canceling flights, and officials now needed to decide whether to reduce traffic in the nosier Wood Dale, or the less noisy Elmhurst. When a different group of 100 participants read about this scenario, a large majority (86%) thought the decrease in air traffic should be allocated to the worse-off Wood Dale, while only 14% thought it should be allocated ...", "dateLastCrawled": "2021-10-13T15:18:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Fairness in Machine Learning: Lessons from Political Philosophy</b> | DeepAI", "url": "https://deepai.org/publication/fairness-in-machine-learning-lessons-from-political-philosophy", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>fairness-in-machine-learning-lessons-from-political</b>...", "snippet": "This discussion suggests that \u2018<b>fairness</b>\u2019 as used in the fair <b>machine</b> <b>learning</b> community is best understood as a placeholder term for a variety of normative egalitarian considerations. Notably, while egalitarianism is a widely held principle, exactly what it requires is the subject of much debate. I provide an overview of some of this debate and finish with implications for the incorporation <b>of \u2018fairness</b>\u2019 into algorithmic decision-making systems.", "dateLastCrawled": "2021-12-26T22:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) How do the Existing <b>Fairness</b> <b>Metrics</b> and Unfairness Mitigation ...", "url": "https://www.researchgate.net/publication/356914252_How_do_the_Existing_Fairness_Metrics_and_Unfairness_Mitigation_Algorithms_contribute_to_Ethical_Learning_Analytics", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/356914252_How_do_the_Existing_<b>Fairness</b>...", "snippet": "<b>of fairness</b> <b>metrics</b> Narayanan ... Kearns, and R oth (2018) demonstrated the <b>incompatibility</b> between six. <b>fairness</b> measures and the impossibility to simultaneously maximize accuracy and <b>fairness</b> ...", "dateLastCrawled": "2021-12-20T23:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "50 Years of Test (Un)<b>fairness</b>: Lessons for <b>Machine</b> <b>Learning</b>", "url": "https://www.researchgate.net/publication/329207000_50_Years_of_Test_Unfairness_Lessons_for_Machine_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/329207000_50_Years_of_Test_Un<b>fairness</b>_Lessons...", "snippet": "Though bias, <b>fairness</b>, and ethics have recently become hot topics in affective computing and multimodal <b>machine</b> <b>learning</b>, there is a long history of research on enhancing the <b>fairness</b> and reducing ...", "dateLastCrawled": "2022-01-11T02:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Survey on Bias and <b>Fairness</b> in <b>Machine</b> <b>Learning</b> \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/1908.09635/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1908.09635", "snippet": "With the widespread use of AI systems and applications in our everyday lives, it is important to take <b>fairness</b> issues into consideration while designing and engineering these types of systems. Such systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that the decisions do not reflect discriminatory behavior toward certain groups or populations. We have recently seen work in <b>machine</b> <b>learning</b>, natural language ...", "dateLastCrawled": "2021-11-15T17:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Bias, <b>Fairness</b> in ML | <b>Machine</b> <b>Learning</b> | Bias", "url": "https://www.scribd.com/document/490478581/Bias-Fairness-in-ML", "isFamilyFriendly": true, "displayUrl": "https://www.scribd.com/document/490478581/Bias-<b>Fairness</b>-in-ML", "snippet": "Bias, <b>Fairness</b> in ML - Read online for free. tech research paper", "dateLastCrawled": "2021-07-24T10:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Democratizing Algorithmic Fairness</b> | SpringerLink", "url": "https://link.springer.com/article/10.1007/s13347-019-00355-w", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s13347-019-00355-w", "snippet": "<b>Metrics</b> details. Abstract. <b>Machine</b> <b>learning</b> algorithms can now identify patterns and correlations in (big) datasets and predict outcomes based on the identified patterns and correlations. They can then generate decisions in accordance with the outcomes predicted, and decision-making processes can thereby be automated. Algorithms can inherit questionable values from datasets and acquire biases in the course of (<b>machine</b>) <b>learning</b>. While researchers and developers have taken the problem of ...", "dateLastCrawled": "2022-02-03T18:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Measurement and <b>Fairness</b> | DeepAI", "url": "https://deepai.org/publication/measurement-and-fairness", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/measurement-and-<b>fairness</b>", "snippet": "Measurement and <b>Fairness</b>. We introduce the language of measurement modeling from the quantitative social sciences as a framework for understanding <b>fairness</b> in computational systems. Computational systems often involve unobservable theoretical constructs, such as &quot;creditworthiness,&quot; &quot;teacher quality,&quot; or &quot;risk to society,&quot; that cannot be ...", "dateLastCrawled": "2021-11-29T01:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Poster session 5</b> \u2013 Aies Conference", "url": "https://www.aies-conference.com/2021/schedule/poster-session-5/", "isFamilyFriendly": true, "displayUrl": "https://www.aies-conference.com/2021/schedule/<b>poster-session-5</b>", "snippet": "Much of the previous work on unfairness in <b>machine</b> <b>learning</b> has focused on the <b>fairness</b> of outcomes rather than process. We propose a feature selection method inspired by fair process (procedural <b>fairness</b>) in addition to fair outcome. Specifically, we introduce the notion of unfairness weight, which indicates how heavily to weight unfairness versus accuracy when measuring the marginal benefit of adding a new feature to a model. Our goal is to maintain accuracy while reducing unfairness, as ...", "dateLastCrawled": "2021-12-07T20:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Tuning EU equality law <b>to algorithmic discrimination: Three pathways to</b> ...", "url": "https://journals.sagepub.com/doi/full/10.1177/1023263X20982173", "isFamilyFriendly": true, "displayUrl": "https://journals.sagepub.com/doi/full/10.1177/1023263X20982173", "snippet": "A further <b>incompatibility</b> pertains to the causal nature of the link between discrimination and given protected categories compared to the reliance of <b>machine</b> <b>learning</b> algorithms on statistical inferences and correlations. Finally, the exhaustive list of protected grounds featured in EU non-discrimination law poses a further compatibility issue in light of the dynamic nature of algorithmic classifications and the risk that new patterns of discrimination emerge. The grammar of EU non ...", "dateLastCrawled": "2022-01-29T21:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "The Cultural Life of <b>Machine</b> <b>Learning</b>: An Incursion into Critical AI ...", "url": "https://www.bookzz.ren/bookzz/9385305/b71052", "isFamilyFriendly": true, "displayUrl": "https://www.bookzz.ren/bookzz/9385305/b71052", "snippet": "How can we disentangle the history of <b>machine</b> <b>learning</b> from conventional histories of artificial intelligence? How can machinic agents\u2019 capacity for novelty be theorized? Can reform initiatives for <b>fairness</b> and equity in AI and <b>machine</b> <b>learning</b> be realized, or are they doomed to cooptation and failure? And just what kind of \u201c<b>learning</b>\u201d does <b>machine</b> <b>learning</b> truly represent? We empirically address these questions and more to provide a baseline for future research. Categories: Society ...", "dateLastCrawled": "2021-12-22T14:44:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(incompatibility of fairness metrics)  is like +(two people looking at the same set of data)", "+(incompatibility of fairness metrics) is similar to +(two people looking at the same set of data)", "+(incompatibility of fairness metrics) can be thought of as +(two people looking at the same set of data)", "+(incompatibility of fairness metrics) can be compared to +(two people looking at the same set of data)", "machine learning +(incompatibility of fairness metrics AND analogy)", "machine learning +(\"incompatibility of fairness metrics is like\")", "machine learning +(\"incompatibility of fairness metrics is similar\")", "machine learning +(\"just as incompatibility of fairness metrics\")", "machine learning +(\"incompatibility of fairness metrics can be thought of as\")", "machine learning +(\"incompatibility of fairness metrics can be compared to\")"]}