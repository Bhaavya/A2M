{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine Learning</b> Glossary | <b>Google Developers</b>", "url": "https://developers.google.com/machine-learning/glossary/", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine-learning</b>/glossary", "snippet": "<b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) Choosing the function that minimizes loss on the training set. Contrast with structural <b>risk</b> <b>minimization</b>. encoder. #language . In general, any ML system that converts from a raw, sparse, or external representation into a more processed, denser, or more internal representation. Encoders are often a component of a larger model, where they are frequently paired with a decoder. Some Transformers pair encoders with decoders, though other Transformers use only ...", "dateLastCrawled": "2022-02-03T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Adversarial</b> Learning in the Cyber Security Domain | DeepAI", "url": "https://deepai.org/publication/adversarial-learning-in-the-cyber-security-domain", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>adversarial</b>-learning-in-the-cyber-security-domain", "snippet": "The model\u2019s <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) ... <b>Adversarial</b> learning in the cyber security domain is the modeling of non-stationary <b>adversarial</b> settings <b>like</b> spam <b>filtering</b> or malware detection, where a malicious adversary can carefully manipulate (or perturb) the input data, exploiting specific vulnerabilities of learning algorithms in order to compromise the (targeted) machine learning system\u2019s security. A taxonomy for the <b>adversarial</b> domain in general exists (e.g., Barreno et al ...", "dateLastCrawled": "2022-01-09T14:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "New submissions for Tue, 20 Jul 21 \u00b7 Issue #105 \u00b7 zoq/arxiv-updates ...", "url": "https://github.com/zoq/arxiv-updates/issues/105", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/zoq/arxiv-updates/issues/105", "snippet": "Standard training via <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) can produce models that achieve high accuracy on average but low accuracy on certain groups, especially in the presence of spurious correlations between the input and label. Prior approaches that achieve high worst-group accuracy, <b>like</b> group distributionally robust optimization (group DRO) require expensive group annotations for each training point, whereas approaches that do not use such group annotations typically achieve ...", "dateLastCrawled": "2021-09-08T09:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Data Science Glossary", "url": "https://aboutds.com/en/content/data-science-glossary", "isFamilyFriendly": true, "displayUrl": "https://aboutds.com/en/content/data-science-glossary", "snippet": "<b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) Choosing the function that minimizes loss on the training set. Contrast with structural <b>risk</b> <b>minimization</b>. ensemble . A merger of the predictions of multiple models. You can create an ensemble via one or more of the following: different initializations; different hyperparameters; different overall structure; Deep and wide models are a kind of ensemble. epoch. A full training pass over the entire data set such that each example has been seen once. Thus, an ...", "dateLastCrawled": "2021-12-01T03:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Statistical Learning Theory: Models, Concepts</b>, and Results", "url": "https://www.sciencedirect.com/science/article/pii/B9780444529367500161", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/B9780444529367500161", "snippet": "Now we can use this expression to draw conclusions about consistency of <b>empirical</b> <b>risk</b> <b>minimization</b>. Namely, <b>ERM</b> is consistent for function class F if the right hand side of this expression converges to 0 as n \u2192 \u221e. Let us look at a few examples. First of all, consider a case where the shattering coefficient N (F, 2 n) is considerably smaller than 2 2n, say N (F, 2 n) \u2264 (2 n) k for some constant k (this means that the shattering coefficient grows polynomially in n). Plugging this in the ...", "dateLastCrawled": "2021-10-16T22:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Analysis <b>of wideband forward looking synthetic aperture radar</b> for ...", "url": "https://www.researchgate.net/publication/245268347_Analysis_of_wideband_forward_looking_synthetic_aperture_radar_for_sensing_land_mines", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/245268347_Analysis_of_wideband_forward...", "snippet": "In addition, the neural network is based on the <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) criterion [10]; therefore, it requires a large training sample set, which is also a problem for landmine detection.", "dateLastCrawled": "2021-12-22T17:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Spam Classification using new kernel function in Support Vector Machine", "url": "https://www.researchgate.net/publication/49966400_Spam_Classification_using_new_kernel_function_in_Support_Vector_Machine/fulltext/0f316b173829de2215f6033c/Spam-Classification-using-new-kernel-function-in-Support-Vector-Machine.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/49966400_Spam_Classification_using_new_kernel...", "snippet": "The formulation uses the Structural <b>Risk</b> <b>Minimization</b> (SRM) principle, which has been shown to be superior, to traditional <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) principle, used by", "dateLastCrawled": "2021-12-21T12:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Google Machine Learning Glossary</b> Flashcards | <b>Quizlet</b>", "url": "https://quizlet.com/256349161/google-machine-learning-glossary-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/256349161/<b>google-machine-learning-glossary</b>-flash-cards", "snippet": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) Choosing the model function that minimizes loss on the training set. Contrast with structural <b>risk</b> <b>minimization</b>. Ensemble . A merger of the predictions of multiple models. You can create an ensemble via one or more of the following: different initializations different hyperparameters different overall structure Deep and wide models are a kind of ensemble. Epoch. A full training pass over the entire data set such that each example has been seen once. Thus, an ...", "dateLastCrawled": "2018-10-18T12:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Neural Network FAQ, part 4 of 7: Books, data, etc.", "url": "http://130.243.105.49/~tdt/ann/faq/FAQ4.html", "isFamilyFriendly": true, "displayUrl": "130.243.105.49/~tdt/ann/faq/FAQ4.html", "snippet": "Conditions for Consistency of <b>Empirical</b> <b>Risk</b> <b>Minimization</b> Principle; 4. Bounds on the <b>Risk</b> for Indicator Loss Functions; Appendix: Lower Bounds on the <b>Risk</b> of the <b>ERM</b> Principle; 5. Bounds on the <b>Risk</b> for Real-Valued Loss Functions; 6. The Structural <b>Risk</b> <b>Minimization</b> Principle; Appendix: Estimating Functions on the Basis of Indirect Measurements; 7. Stochastic Ill-Posed Problems; 8. Estimating the Values of Functions at Given Points; 9. Perceptrons and Their Generalizations; 10. The Support ...", "dateLastCrawled": "2021-09-03T13:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "New submissions for Fri, 30 Oct 20 \u00b7 Issue #4 \u00b7 dajinstory/daily-arxiv ...", "url": "https://github.com/dajinstory/daily-arxiv-noti/issues/4", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/dajinstory/daily-arxiv-noti/issues/4", "snippet": "For linear models with confounders, we prove that Nash equilibria of these games are closer to the ideal OOD solutions than the standard <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) and we also provide learning algorithms that provably converge to these Nash Equilibria. <b>Empirical</b> comparisons of the proposed approach with the state-of-the-art show consistent gains in achieving OOD solutions in several settings involving anti-causal variables and confounders.", "dateLastCrawled": "2022-01-03T07:22:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "New submissions for Tue, 20 Jul 21 \u00b7 Issue #105 \u00b7 zoq/arxiv-updates ...", "url": "https://github.com/zoq/arxiv-updates/issues/105", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/zoq/arxiv-updates/issues/105", "snippet": "In this paper, we investigate the excess <b>risk</b> performance and towards improved learning rates for two popular approaches of stochastic optimization: <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) and stochastic gradient descent (SGD). Although there exists plentiful generalization analysis of <b>ERM</b> and SGD for supervised learning, current theoretical understandings of <b>ERM</b> and SGD are either have stronger assumptions in convex learning, e.g., strong convexity condition, or show slow rates and less studied ...", "dateLastCrawled": "2021-09-08T09:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Statistical Learning Theory: Models, Concepts</b>, and Results", "url": "https://www.sciencedirect.com/science/article/pii/B9780444529367500161", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/B9780444529367500161", "snippet": "Now we can use this expression to draw conclusions about consistency of <b>empirical</b> <b>risk</b> <b>minimization</b>. Namely, <b>ERM</b> is consistent for function class F if the right hand side of this expression converges to 0 as n \u2192 \u221e. Let us look at a few examples. First of all, consider a case where the shattering coefficient N (F, 2 n) is considerably smaller than 2 2n, say N (F, 2 n) \u2264 (2 n) k for some constant k (this means that the shattering coefficient grows polynomially in n). Plugging this in the ...", "dateLastCrawled": "2021-10-16T22:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Adversarial</b> Learning in the Cyber Security Domain | DeepAI", "url": "https://deepai.org/publication/adversarial-learning-in-the-cyber-security-domain", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>adversarial</b>-learning-in-the-cyber-security-domain", "snippet": "The model\u2019s <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) is defined as E (x, y) \u223c D [L o s s (x, y, \u03b8)], where x is the original sample, and y is the original label. By modifying the <b>ERM</b> definition by allowing the adversary to perturb the input x by the scalar value S, <b>ERM</b> is represented by min \u03b8 \u03c1 ( \u03b8 ) : \u03c1 ( \u03b8 ) = E ( x , y ) \u223c D [ m a x \u03b4 \u2208 S L o s s ( x + r , y , \u03b8 ) ] , where \u03c1 ( \u03b8 ) denotes the objective function.", "dateLastCrawled": "2022-01-09T14:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Excess Capacity and Backdoor Poisoning | DeepAI", "url": "https://deepai.org/publication/excess-capacity-and-backdoor-poisoning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/excess-capacity-and-backdoor-poisoning", "snippet": "With probability at least 1 \u2212 \u03b4, <b>empirical</b> <b>risk</b> <b>minimization</b> on the training set S \\coloneqq S c l e a n \u222a S a d v yields a classifier \u02c6 h satisfying the success conditions for Problem 2. Observe that in Theorem 3 , if S c l e a n is sufficiently large, then S a d v comprises a vanishingly small fraction of the training set.", "dateLastCrawled": "2021-12-03T10:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Towards the Science of <b>Security and Privacy in Machine</b> ... - arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/1611.03814/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1611.03814", "snippet": "Thus, all supervised learning algorithm perform this <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) with the loss function varying across different algorithms. The PAC guarantee states that: P (| r (h \u2217) \u2212 r (^ h) | \u2264 \u03f5) \u2265 1 \u2212 \u03b4 (1) where the probability is over samples \u2192 z used to learn ^ h. This guarantee holds when two pre-conditions are met: [Condition 1: Uniform bound] given enough samples (called the sample complexity, which depends on \u03f5, \u03b4 above) that enable a uniform bound of the ...", "dateLastCrawled": "2022-01-11T14:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Excess Capacity and Backdoor Poisoning | PDF | Function (Mathematics ...", "url": "https://www.scribd.com/document/553905108/Excess-Capacity-and-Backdoor-Poisoning", "isFamilyFriendly": true, "displayUrl": "https://www.scribd.com/document/553905108/Excess-Capacity-and-Backdoor-Poisoning", "snippet": "In other words, assuming there exists a margin \u03b3 and a 0-loss classifier, <b>empirical</b> <b>risk</b> <b>minimization</b> of margin-loss with a norm constraint fails to find a 0-loss classifier on a sufficiently contaminated training set. 2.3 Memorization Capacity and Backdoor Attacks The key takeaway from the previous section is that the adversary can force an <b>ERM</b> learner to recover the union of a function that looks <b>similar</b> to the true classifier on in-distribution inputs and another function of the ...", "dateLastCrawled": "2022-01-23T17:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Analysis <b>of wideband forward looking synthetic aperture radar</b> for ...", "url": "https://www.researchgate.net/publication/245268347_Analysis_of_wideband_forward_looking_synthetic_aperture_radar_for_sensing_land_mines", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/245268347_Analysis_of_wideband_forward...", "snippet": "In addition, the neural network is based on the <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) criterion [10]; therefore, it requires a large training sample set, which is also a problem for landmine detection.", "dateLastCrawled": "2021-12-22T17:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "New submissions for Fri, 30 Oct 20 \u00b7 Issue #4 \u00b7 dajinstory/daily-arxiv ...", "url": "https://github.com/dajinstory/daily-arxiv-noti/issues/4", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/dajinstory/daily-arxiv-noti/issues/4", "snippet": "For linear models with confounders, we prove that Nash equilibria of these games are closer to the ideal OOD solutions than the standard <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) and we also provide learning algorithms that provably converge to these Nash Equilibria. <b>Empirical</b> comparisons of the proposed approach with the state-of-the-art show consistent gains in achieving OOD solutions in several settings involving anti-causal variables and confounders.", "dateLastCrawled": "2022-01-03T07:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Google Machine Learning Glossary</b> Flashcards | <b>Quizlet</b>", "url": "https://quizlet.com/256349161/google-machine-learning-glossary-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/256349161/<b>google-machine-learning-glossary</b>-flash-cards", "snippet": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) Choosing the model function that minimizes loss on the training set. Contrast with structural <b>risk</b> <b>minimization</b>. Ensemble . A merger of the predictions of multiple models. You can create an ensemble via one or more of the following: different initializations different hyperparameters different overall structure Deep and wide models are a kind of ensemble. Epoch. A full training pass over the entire data set such that each example has been seen once. Thus, an ...", "dateLastCrawled": "2018-10-18T12:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Machine Learning Google Course Flashcards | Quizlet", "url": "https://quizlet.com/300254930/machine-learning-google-course-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/300254930/machine-learning-google-course-flash-cards", "snippet": "In our spam detector example, the labeled examples would be individual <b>emails</b> that users have explicitly marked as &quot;spam&quot; or &quot;not spam.&quot; Unlabeled example. Contains features but not the label. early stopping. A method for regularization that involves ending model training before training loss finishes decreasing. In early stopping, you end model training when the loss on a validation data set starts to increase, that is, when generalization performance worsens. Training. means creating or ...", "dateLastCrawled": "2022-01-22T15:16:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "New submissions for Fri, 22 Oct 21 \u00b7 Issue #139 \u00b7 LeeKyungwook/get ...", "url": "https://github.com/LeeKyungwook/get-arxiv-noti/issues/139", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/LeeKyungwook/get-arxiv-noti/issues/139", "snippet": "While deep learning through <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) has succeeded at achieving human-level performance at a variety of complex tasks, <b>ERM</b> generalizes poorly to distribution shift. This is partly explained by overfitting to spurious features such as background in images or named entities in natural language. Synthetic data augmentation followed by <b>empirical</b> <b>risk</b> <b>minimization</b> (DA-<b>ERM</b>) is a simple yet powerful solution to remedy this problem. In this paper, we propose data augmented ...", "dateLastCrawled": "2021-12-11T07:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Causality for Machine Learning", "url": "https://ff13.fastforwardlabs.com/", "isFamilyFriendly": true, "displayUrl": "https://ff13.fastforwardlabs.com", "snippet": "When we treated the problem with <b>empirical</b> <b>risk</b> <b>minimization</b> (minimizing the cross-entropy between classes), we found good performance in the train environments, but very poor performance in the test environment. We report the metrics over 120 epochs of training in the table below. The best test accuracy is achieved at epoch 40, after which <b>ERM</b> (<b>empirical</b> <b>risk</b> <b>minimization</b>) begins to overfit. In the case of IRM (invariant <b>risk</b> <b>minimization</b>), we paid a small price in train set accuracy, but ...", "dateLastCrawled": "2022-02-01T19:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Accepted Papers | ECML/PKDD 2014", "url": "http://ecmlpkdd2014.loria.fr/index.html%3Fp=1780.html", "isFamilyFriendly": true, "displayUrl": "ecmlpkdd2014.loria.fr/index.html?p=1780.html", "snippet": "In this work, we introduce a novel <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) framework for supervised learning, neutralized <b>ERM</b> (NERM) that ensures that any classifiers obtained <b>can</b> be guaranteed to be neutral with respect to a viewpoint hypothesis. More specifically, given a viewpoint hypothesis, NERM works to find a target hypothesis that minimizes the <b>empirical</b> <b>risk</b> while simultaneously identifying a target hypothesis that is neutral to the viewpoint hypothesis. Within the NERM framework, we ...", "dateLastCrawled": "2021-08-19T15:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Peter Richtarik", "url": "https://richtarik.org/i_oldnews.html", "isFamilyFriendly": true, "displayUrl": "https://richtarik.org/i_oldnews.html", "snippet": "Abstract: We propose a remarkably general variance-reduced method suitable for solving regularized <b>empirical</b> <b>risk</b> <b>minimization</b> problems with either a large number of training examples, or a large model dimension, or both. In special cases, our method reduces to several known and previously <b>thought</b> to be unrelated methods, such as SAGA, LSVRG, JacSketch, SEGA and ISEGA, and their arbitrary sampling and proximal generalizations. However, we also highlight a large number of new specific ...", "dateLastCrawled": "2022-01-30T21:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine Learning Handouts</b> | PDF | Machine Learning | Expected ... - Scribd", "url": "https://www.scribd.com/document/54995937/Machine-Learning-Handouts", "isFamilyFriendly": true, "displayUrl": "https://www.scribd.com/document/54995937/<b>Machine-Learning-Handouts</b>", "snippet": "4 <b>Empirical</b> <b>Risk</b> <b>Minimization</b> Recall that a learning algorithm receives as input a training set S, sampled i.i.d. from an unknown distribution D, and should output a predictor hS : X \u2192 Y, where the subscript S emphasizes the fact that the output predictor depends on S.", "dateLastCrawled": "2021-11-04T13:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A Brief Introduction to Machine Learning for Engineers ... - DOKUMEN.PUB", "url": "https://dokumen.pub/a-brief-introduction-to-machine-learning-for-engineers-168083472x-9781680834727.html", "isFamilyFriendly": true, "displayUrl": "https://dokumen.pub/a-brief-introduction-to-<b>machine-learning-for-engineers-168083472x</b>...", "snippet": "\u2022 Direct inference via <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>): Learn directly an approximation t\u02c6D (\u00b7) of the optimal decision rule by minimizing an <b>empirical</b> estimate of the generalization loss (2.2) obtained from the data set as t\u02c6D (\u00b7) = arg min LD (t\u02c6), t\u02c6 (2.7) where the <b>empirical</b> <b>risk</b>, or <b>empirical</b> loss, is LD (t\u02c6) = N 1 X \u2113(tn , t\u02c6(xn )). N n=1 (2.8) The notation LD (t\u02c6) highlights the dependence of the <b>empirical</b> loss on the predictor t\u02c6(\u00b7) and on the training set D. In ...", "dateLastCrawled": "2022-01-01T04:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Neural Network FAQ, part 4 of 7: Books, data, etc.", "url": "http://130.243.105.49/~tdt/ann/faq/FAQ4.html", "isFamilyFriendly": true, "displayUrl": "130.243.105.49/~tdt/ann/faq/FAQ4.html", "snippet": "Conditions for Consistency of <b>Empirical</b> <b>Risk</b> <b>Minimization</b> Principle; 4. Bounds on the <b>Risk</b> for Indicator Loss Functions; Appendix: Lower Bounds on the <b>Risk</b> of the <b>ERM</b> Principle; 5. Bounds on the <b>Risk</b> for Real-Valued Loss Functions; 6. The Structural <b>Risk</b> <b>Minimization</b> Principle; Appendix: Estimating Functions on the Basis of Indirect Measurements; 7. Stochastic Ill-Posed Problems; 8. Estimating the Values of Functions at Given Points; 9. Perceptrons and Their Generalizations; 10. The Support ...", "dateLastCrawled": "2021-09-03T13:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Talks | <b>Machine Learning @ Johns Hopkins University</b>", "url": "https://ml.jhu.edu/talks/", "isFamilyFriendly": true, "displayUrl": "https://ml.jhu.edu/talks", "snippet": "Click on a talk title for details. To receive talk announcements by email, sign up for our mailing list. In return, please forward announcements of ML-related talks to announce (at) ml.jhu.edu. Exp\u2026", "dateLastCrawled": "2022-01-30T23:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Peer Reviewed</b> Journal - IJERA", "url": "https://www.ijera.com/pages/v2no6.html", "isFamilyFriendly": true, "displayUrl": "https://www.ijera.com/pages/v2no6.html", "snippet": "Anybody <b>can</b> submit their paper by mailing at ijera.editor@gmail.com IJERA MENU. CALL FOR PAPER. PAPER SUBMISSION. WHY CHOOSE IJERA. AUTHOR INSTRUCTIONS. STATISTICS. UNIVERSITY AFFILIATES. CHECK PAPER STATUS. FAQ. IJERA CONTENTS. CURRENT ISSUE. IJERA ARCHIVE. SPECIAL ISSUE. CALL FOR CONFERENCE. UPCOMING CONFERENCE. SPECIAL ISSUE ARCHIVE. DOWNLOADS. MODEL PAPER. COPY RIGHT FORM. COPYRIGHT INFRINGEMENT. JOURNAL ETHICS. OPEN ACCESS. OPEN ACCESS . IJERA : Volume 2 Issue 6, November-December 2012 ...", "dateLastCrawled": "2022-01-29T10:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Selected Papers of the Photoelectronic Technology Committee ... - SPIE", "url": "https://spie.org/Publications/Proceedings/Volume/9795", "isFamilyFriendly": true, "displayUrl": "https://spie.org/Publications/<b>Proceedings/Volume/9795</b>", "snippet": "<b>PROCEEDINGS VOLUME 9795</b> Selected Papers of the Photoelectronic Technology Committee Conferences held June\u2013July 2015", "dateLastCrawled": "2021-03-24T12:08:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "New submissions for Tue, 20 Jul 21 \u00b7 Issue #105 \u00b7 zoq/arxiv-updates ...", "url": "https://github.com/zoq/arxiv-updates/issues/105", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/zoq/arxiv-updates/issues/105", "snippet": "Standard training via <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) <b>can</b> produce models that achieve high accuracy on average but low accuracy on certain groups, especially in the presence of spurious correlations between the input and label. Prior approaches that achieve high worst-group accuracy, like group distributionally robust optimization (group DRO) require expensive group annotations for each training point, whereas approaches that do not use such group annotations typically achieve ...", "dateLastCrawled": "2021-09-08T09:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Sequential event prediction</b> | SpringerLink", "url": "https://link.springer.com/article/10.1007/s10994-013-5356-5", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10994-013-5356-5", "snippet": "<b>Empirical</b> <b>risk</b> <b>minimization</b> for <b>sequential event prediction</b>. We present a general framework for using <b>ERM</b> in <b>sequential event prediction</b>, and then show how the framework <b>can</b> be specified to specific applications by presenting email recipient recommendation, the online grocery store recommender system, and medical condition prediction as case studies. The core of our <b>ERM</b>-based approach to <b>sequential event prediction</b> is a ranking model of the relationship between items in the observed part of ...", "dateLastCrawled": "2021-12-07T15:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Spam Classification using new kernel function in Support Vector Machine", "url": "https://www.researchgate.net/publication/49966400_Spam_Classification_using_new_kernel_function_in_Support_Vector_Machine/fulltext/0f316b173829de2215f6033c/Spam-Classification-using-new-kernel-function-in-Support-Vector-Machine.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/49966400_Spam_Classification_using_new_kernel...", "snippet": "The formulation uses the Structural <b>Risk</b> <b>Minimization</b> (SRM) principle, which has been shown to be superior, to traditional <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) principle, used by", "dateLastCrawled": "2021-12-21T12:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Adversarial</b> Learning in the Cyber Security Domain | DeepAI", "url": "https://deepai.org/publication/adversarial-learning-in-the-cyber-security-domain", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>adversarial</b>-learning-in-the-cyber-security-domain", "snippet": "The model\u2019s <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) ... <b>Adversarial</b> learning in the cyber security domain is the modeling of non-stationary <b>adversarial</b> settings like spam <b>filtering</b> or malware detection, where a malicious adversary <b>can</b> carefully manipulate (or perturb) the input data, exploiting specific vulnerabilities of learning algorithms in order to compromise the (targeted) machine learning system\u2019s security. A taxonomy for the <b>adversarial</b> domain in general exists (e.g., Barreno et al ...", "dateLastCrawled": "2022-01-09T14:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Feature-Level Domain Adaptation | DeepAI", "url": "https://deepai.org/publication/feature-level-domain-adaptation", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/feature-level-domain-adaptation", "snippet": "We adopt an <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) framework for constructing our domain-adapted classifier. The <b>ERM</b> framework proposes a classification function h: R m \u2192 R and assesses the quality of the hypothesis by comparing its predictions with the true labels on the <b>empirical</b> data using a loss function L: Y \u00d7 R \u2192 R + 0. The <b>empirical</b> loss is an estimate of the <b>risk</b>, which is the expected value of the loss function under the data distribution. Below, we show that if the target domain ...", "dateLastCrawled": "2021-12-14T17:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Towards the Science of <b>Security and Privacy in Machine</b> ... - arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/1611.03814/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1611.03814", "snippet": "Thus, all supervised learning algorithm perform this <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) with the loss function varying across different algorithms. The PAC guarantee states that: P (| r (h \u2217) \u2212 r (^ h) | \u2264 \u03f5) \u2265 1 \u2212 \u03b4 (1) where the probability is over samples \u2192 z used to learn ^ h. This guarantee holds when two pre-conditions are met: [Condition 1: Uniform bound] given enough samples (called the sample complexity, which depends on \u03f5, \u03b4 above) that enable a uniform bound of the ...", "dateLastCrawled": "2022-01-11T14:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "New submissions for Fri, 30 Oct 20 \u00b7 Issue #4 \u00b7 dajinstory/daily-arxiv ...", "url": "https://github.com/dajinstory/daily-arxiv-noti/issues/4", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/dajinstory/daily-arxiv-noti/issues/4", "snippet": "For linear models with confounders, we prove that Nash equilibria of these games are closer to the ideal OOD solutions than the standard <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) and we also provide learning algorithms that provably converge to these Nash Equilibria. <b>Empirical</b> comparisons of the proposed approach with the state-of-the-art show consistent gains in achieving OOD solutions in several settings involving anti-causal variables and confounders.", "dateLastCrawled": "2022-01-03T07:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Analysis <b>of wideband forward looking synthetic aperture radar</b> for ...", "url": "https://www.researchgate.net/publication/245268347_Analysis_of_wideband_forward_looking_synthetic_aperture_radar_for_sensing_land_mines", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/245268347_Analysis_of_wideband_forward...", "snippet": "In addition, the neural network is based on the <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) criterion [10]; therefore, it requires a large training sample set, which is also a problem for landmine detection.", "dateLastCrawled": "2021-12-22T17:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Ultimate Data Science Flashcards | Quizlet", "url": "https://quizlet.com/474731310/ultimate-data-science-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/474731310/ultimate-data-science-flash-cards", "snippet": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) Choosing the function that minimizes loss on the training set. Contrast with structural <b>risk</b> <b>minimization</b>. Ensemble. A merger of the predictions of multiple models. You <b>can</b> create an ensemble via one or more of the following:-different initializations-different hyperparameters-different overall structure Deep and wide models are a kind of ensemble. Environment. In reinforcement learning, the world that contains the agent and allows the agent to observe that ...", "dateLastCrawled": "2021-06-24T10:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Slides25 - Cornell University", "url": "https://www.cs.cornell.edu/courses/cs4787/2021sp/notebooks/Slides25.html", "isFamilyFriendly": true, "displayUrl": "https://www.cs.cornell.edu/courses/cs4787/2021sp/notebooks/Slides25.html", "snippet": "Of course, for a spam detection system to be useful for spam <b>filtering</b>, it needs to make predictions about what <b>emails</b> are spam, and it needs to make those predictions in real time. But it also needs to learn from new spam <b>emails</b>, so that it <b>can</b> quickly adapt to new patterns in spam.", "dateLastCrawled": "2022-01-27T13:02:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> and Stochastic Gradient Descent for ...", "url": "http://proceedings.mlr.press/v89/veitch19a/veitch19a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v89/veitch19a/veitch19a.pdf", "snippet": "where F(Sn) is the <b>empirical</b> distribution.2 The <b>ERM</b> dogma is to select the predictor \u03c0\u02c6\u03b8 n given by \u02c6\u03b8 n = argmin\u03b8 R\u02c6(\u03b8,Sn). That is, the objective function that de\ufb01nes <b>learning</b> is the <b>empirical</b> <b>risk</b>. <b>ERM</b> has two useful properties. (1) It provides a prin-cipled framework for de\ufb01ning new <b>machine</b> <b>learning</b> methods. In particular, when ...", "dateLastCrawled": "2021-09-18T06:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Statistical <b>Learning</b> Theory and the C-Loss cost function", "url": "http://www.cnel.ufl.edu/courses/EEL6814/closs.pdf", "isFamilyFriendly": true, "displayUrl": "www.cnel.ufl.edu/courses/EEL6814/closs.pdf", "snippet": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) principle Let us consider a <b>learning</b> <b>machine</b> x,d are real r.v. with joint distribution P(x,y). F(x) is a function of some parameters w, i.e. f(x,w). d d. <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) principle How can we find the possible best <b>learning</b> <b>machine</b> that generalizes for unseen data from the same distribution? Define the <b>Risk</b> functional as L(.) is called the Loss function, and minimize it w.r.t. w achieving the best possible loss. But we can not do this ...", "dateLastCrawled": "2022-01-28T20:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Computational and Statistical <b>Learning</b> Theory", "url": "https://home.ttic.edu/~nati/Teaching/TTIC31120/2015/Lecture17.pdf", "isFamilyFriendly": true, "displayUrl": "https://home.ttic.edu/~nati/Teaching/TTIC31120/2015/Lecture17.pdf", "snippet": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) / Sample Average Approximation (SAA): Collect sample z1UYU zm ... SGD for <b>Machine</b> <b>Learning</b> Initialize S 4 L r At iteration t: Draw T \u00e7\u00e1U \u00e71\u00de If U \u00e7 S \u00e7 \u00e1\u00f6 T \u00e7 O s\u00e1 S \u00e7 &gt; 5 Z S \u00e7 E\u00df \u00e7U \u00e7\u00f6 T \u00e7 else: S \u00e7 &gt; 5 Z S \u00e7 Return S % \u00cd L 5 \u00cd \u00c3 \u00cd S \u00e7 \u00e7 @ 5 Draw T 5\u00e1U 5 \u00e1\u00e5\u00e1 T \u00e0 \u00e1U \u00e0 1\u00de Initialize S 4 L r At iteration t: Pick E \u00d0 s\u00e5I at random If U \u00dc S \u00e7 \u00e1\u00f6 T \u00dc O s\u00e1 S \u00e7 &gt; 5 Z S \u00e7 E\u00df \u00e7U \u00dc\u00f6 T \u00dc else: S \u00e7 &gt; 5 Z S \u00e7 S \u00e7 &gt; 5 Z ...", "dateLastCrawled": "2022-01-26T03:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Basics of <b>Machine</b> <b>Learning</b>", "url": "https://courses.cs.duke.edu/spring21/compsci527/slides/s_04_deep_learning.pdf", "isFamilyFriendly": true, "displayUrl": "https://courses.cs.duke.edu/spring21/compsci527/slides/s_04_deep_<b>learning</b>.pdf", "snippet": "This is called <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) COMPSCI 527 \u2014 Computer Vision Basics of <b>Machine</b> <b>Learning</b> 15/26. Loss and <b>Risk</b> <b>Machine</b> <b>Learning</b> and the Statistical <b>Risk</b> <b>ERM</b>: w^ 2argmin w2R m L T(w) In <b>machine</b> <b>learning</b>, we go much farther: We also want h to do well on previously unseen inputs To relate past and future data, assume that all data comes from the same joint probability distribution p(x;y) p is called the generative data model or just model The goal of <b>machine</b> <b>learning</b> is to ...", "dateLastCrawled": "2021-11-06T09:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Topics in <b>Machine</b> <b>Learning</b> (TIML-09)", "url": "https://www.cse.iitb.ac.in/~saketh/teaching/cs689.html", "isFamilyFriendly": true, "displayUrl": "https://www.cse.iitb.ac.in/~saketh/teaching/cs689.html", "snippet": "Introduction to Statistical <b>Learning</b> Theory (SLT): Definitions of loss function, <b>risk</b>, <b>empirical</b> <b>risk</b>, motivation for <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) Further Reading, Supplementary: Jan 12: Consistency of <b>ERM</b>, Sufficient condition for <b>ERM</b> as one-sided uniform convergence, Analysis for finite sets of functions and extensions to general case using Symmetrization trick, Shattering Coeff. Further Reading, Supplementary: Jan 15: Shattering coeff., growth function, VC dimension, Annealed Entropy ...", "dateLastCrawled": "2022-01-11T09:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Lecture 1: Reinforcement <b>Learning</b>: What and Why?", "url": "https://imada.sdu.dk/~kandemir/rl-lecture1.pdf", "isFamilyFriendly": true, "displayUrl": "https://imada.sdu.dk/~kandemir/rl-lecture1.pdf", "snippet": "<b>machine</b> <b>learning</b> and is referred to as <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>). 3 Challenges of reinforcement <b>learning</b> Consider the cart pole balancing problem, where a cart carrying an unactuated pole \ufb02oats on a straight horizontal track. The cart is actuated by a torque applied either to the right or the left direction. Seeherefor a real cart ...", "dateLastCrawled": "2021-09-30T12:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>GitHub</b> - gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "Loss function 3.1 Categories 3.2 <b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) and Structural <b>risk</b> <b>minimization</b> (SRM) 3.2.1 MLE 3.2.2 MAP 4. Evaluation metrics 4.1 Accuracy, precision, recall and F1 4.2 ROC, AUC 4.3 BLEU 5. Sample projects 6. Classical questions 6.1 Why is logistic regression a generalized linear model ?", "dateLastCrawled": "2021-09-12T01:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and-sample-codes ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/README.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "Loss function 3.1 Categories 3.2 <b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) and Structural <b>risk</b> <b>minimization</b> (SRM) 3.2.1 MLE 3.2.2 MAP 4. Evaluation metrics 4.1 Accuracy, precision, recall and F1 4.2 ROC, AUC 4.3 BLEU 5. Sample projects 6. Classical questions 6.1 Why is logistic regression a generalized linear model ?", "dateLastCrawled": "2021-09-22T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "[2006.09461] Robust <b>Compressed Sensing using Generative Models</b> - arXiv", "url": "https://arxiv.org/abs/2006.09461", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/abs/2006.09461", "snippet": "Classical recovery approaches such as <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) are guaranteed to succeed when the measurement matrix is sub-Gaussian. However, when the measurement matrix and measurements are heavy-tailed or have outliers, recovery may fail dramatically. In this paper we propose an algorithm inspired by the Median-of-Means (MOM). Our algorithm guarantees recovery for heavy-tailed data, even in the presence of outliers. Theoretically, our results show our novel MOM-based algorithm ...", "dateLastCrawled": "2021-06-27T11:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(PDF) <b>Support Vector Machines: Theory and Applications</b>", "url": "https://www.researchgate.net/publication/221621494_Support_Vector_Machines_Theory_and_Applications", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221621494_Support_Vector_<b>Machine</b>s", "snippet": "The principle used is <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) over a set of possible functions, called hypothesis space. Formally this can be written as minimizing the <b>empirical</b> . error: \u2211 = l. 1 i. x ...", "dateLastCrawled": "2022-02-02T02:47:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "ToyotaTechnologicalInstituteatChicago UniversityofTexasatAustin surbhi ...", "url": "https://arxiv.org/pdf/2005.07652", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/pdf/2005.07652", "snippet": "<b>Just as empirical risk minimization (ERM</b>) is central for non-robust PAC <b>learning</b>, a core component of adversarially robust <b>learning</b> is minimizing the robust empirical risk on a dataset S, \u02c6h \u2208 RERM U(S) ,argmin h\u2208H 1 m Xm i=1 sup z\u2208U(x) 1 [h(z) 6= y]. In this paper, we provide necessary and su\ufb03cient conditions on perturbation sets U, under which the robust empirical risk minimization (RERM) problem is e\ufb03ciently solvable in the realizable setting. We show that an e\ufb03cient ...", "dateLastCrawled": "2021-10-06T13:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Efficiently Learning Adversarially Robust Halfspaces with</b> Noise | DeepAI", "url": "https://deepai.org/publication/efficiently-learning-adversarially-robust-halfspaces-with-noise", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>efficiently-learning-adversarially-robust-halfspaces</b>...", "snippet": "<b>Just as empirical risk minimization (ERM</b>) is central for non-robust PAC <b>learning</b>, a core component of adversarially robust <b>learning</b> is minimizing the robust empirical risk on a dataset S, ^ h \u2208 R E R M U ( S ) \u225c argmin h \u2208 H 1 m m \u2211 i = 1 sup z \u2208 U ( x ) 1 [ h ( z ) \u2260 y ] .", "dateLastCrawled": "2021-12-05T23:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Efficiently <b>Learning</b> Adversarially Robust Halfspaces with Noise", "url": "http://proceedings.mlr.press/v119/montasser20a/montasser20a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v119/montasser20a/montasser20a.pdf", "snippet": "remains a major challenge in <b>machine</b> <b>learning</b>. A line of work has shown that predictors learned by deep neural networks are not robust to adversarial examples (Szegedy et al.,2014;Biggio et al.,2013;Goodfellow et al.,2015). This has led to a long line of research studying different aspects of robustness to adversarial examples. In this paper, we consider the problem of distribution-independent <b>learning</b> of halfspaces that are robust to ad-versarial examples at test time, also referred to as ...", "dateLastCrawled": "2021-11-21T12:03:00.0000000Z", "language": "en", "isNavigational": false}], [], []], "all_bing_queries": ["+(empirical risk minimization (erm))  is like +(filtering emails)", "+(empirical risk minimization (erm)) is similar to +(filtering emails)", "+(empirical risk minimization (erm)) can be thought of as +(filtering emails)", "+(empirical risk minimization (erm)) can be compared to +(filtering emails)", "machine learning +(empirical risk minimization (erm) AND analogy)", "machine learning +(\"empirical risk minimization (erm) is like\")", "machine learning +(\"empirical risk minimization (erm) is similar\")", "machine learning +(\"just as empirical risk minimization (erm)\")", "machine learning +(\"empirical risk minimization (erm) can be thought of as\")", "machine learning +(\"empirical risk minimization (erm) can be compared to\")"]}