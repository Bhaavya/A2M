{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 0, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>BLEU</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/BLEU", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>BLEU</b>", "snippet": "<b>BLEU</b> (<b>bilingual</b> <b>evaluation</b> <b>understudy</b>) is an algorithm for evaluating the quality of text which has been machine-translated from one natural language to another. Quality is considered to be the correspondence between a machine&#39;s output and that of a human: &quot;the closer a machine translation is to a professional human translation, the better it is&quot; \u2013 this is the central idea behind <b>BLEU</b>. <b>BLEU</b> was one of the first metrics to claim a high correlation with human judgements of quality, and ...", "dateLastCrawled": "2022-02-02T22:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>BLEU</b>: a Method for Automatic <b>Evaluation</b> of Machine Translation", "url": "https://aclanthology.org/P02-1040.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/P02-1040.pdf", "snippet": "1So we call our method the <b>bilingual</b> <b>evaluation</b> <b>understudy</b>, <b>BLEU</b>. the <b>evaluation</b> bottleneck. Developers would bene-\ufb01t from an inexpensive automatic <b>evaluation</b> that is quick, language-independent, and correlates highly with human <b>evaluation</b>. We propose such an <b>evalua-tion</b> method in this paper. 1.2 Viewpoint How does one measure translation performance? The closer a machine translation is to a professional human translation, the better it is. This is the cen-tral idea behind our proposal. To ...", "dateLastCrawled": "2022-02-03T01:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Understanding MT Quality: <b>BLEU</b> Scores | by K Vashee | Medium", "url": "https://kvashee.medium.com/understanding-mt-quality-bleu-scores-9a19ed20526d", "isFamilyFriendly": true, "displayUrl": "https://kvashee.medium.com/understanding-mt-quality-<b>bleu</b>-scores-9a19ed20526d", "snippet": "What is <b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>)? As the use of enterprise machine translation expands, it becomes increasingly more important for users and practitioners to understand MT quality issues in a relevant, meaningful, and accurate way. The <b>BLEU</b> score is a string-matching algorithm that provides basic outpu t quality metrics for MT researchers and developers. In this first post, we will review and look more closely at the <b>BLEU</b> score, which is probably the most widely used MT quality ...", "dateLastCrawled": "2022-01-31T18:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Understanding MT Quality: <b>BLEU</b> Scores", "url": "https://blog.modernmt.com/understanding-mt-quality-bleu-scores/", "isFamilyFriendly": true, "displayUrl": "https://blog.modernmt.com/understanding-mt-quality-<b>bleu</b>-scores", "snippet": "What is a <b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) score? The <b>BLEU</b> score is a string-matching algorithm that provides basic output quality metrics for MT researchers and developers. In this post, we review and look more closely at the <b>BLEU</b> score, which is probably the most widely used MT quality assessment metric in use by MT researchers and developers over the last 15 years. While it is widely understood that <b>BLEU</b> has many flaws, it has not been displaced or replaced by a widely accepted ...", "dateLastCrawled": "2022-02-03T17:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A Gentle Introduction to Calculating the <b>BLEU</b> Score for Text in Python", "url": "https://machinelearningmastery.com/calculate-bleu-score-for-text-python/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/calculate-<b>bleu</b>-score-for-text-pyth", "snippet": "<b>BLEU</b>, or the <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>, is a score for <b>comparing</b> a candidate translation of text to one or more reference <b>translations</b>. Although developed for translation, it can be used to evaluate text generated for a suite of natural language processing tasks. In this tutorial, you will discover the <b>BLEU</b> score for evaluating and scoring candidate text using the NLTK library in", "dateLastCrawled": "2022-01-30T22:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Why Evaluating Machine Translation Quality is</b> Hard | Waygo", "url": "http://blog.waygoapp.com/why-evaluating-machine-translation-quality-is-hard/", "isFamilyFriendly": true, "displayUrl": "blog.waygoapp.com/<b>why-evaluating-machine-translation-quality-is</b>-hard", "snippet": "<b>BLEU</b> (an acronym for <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) is an algorithm for evaluating the quality of text which has been machine-translated from one language to another. The idea behind <b>BLEU</b> is that \u201cthe closer a machine translation is to a professional human translation, the better it is\u201d. This sounds about the same as our first idea above \u2013 to let a translator evaluate the quality every time. The key difference is that before a machine gets involved, <b>BLEU</b> first has professional human ...", "dateLastCrawled": "2021-11-28T05:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Evaluating Text Output in NLP: <b>BLEU</b> at your own risk | by Rachael ...", "url": "https://towardsdatascience.com/evaluating-text-output-in-nlp-bleu-at-your-own-risk-e8609665a213", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/evaluating-text-output-in-nlp-<b>bleu</b>-at-your-own-risk-e...", "snippet": "This measure, looking at n-grams overlap between the output and reference <b>translations</b> with a penalty for shorter outputs, is known as <b>BLEU</b> (short for \u201c<b>Bilingual</b> <b>evaluation</b> <b>understudy</b>\u201d which people literally only ever say when explaining the acronym) and was developed by Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu at IBM in 2002. It\u2019s a very popular metric in NLP, particularly for tasks where the output of a system is a text string rather than a classification. This ...", "dateLastCrawled": "2022-02-02T22:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Evaluating English to Arabic Machine Translation Using <b>BLEU</b>", "url": "https://thesai.org/Downloads/Volume4No1/Paper_9-Evaluating_English_to_Arabic_Machine_Translation_Using_BLEU.pdf", "isFamilyFriendly": true, "displayUrl": "https://thesai.org/Downloads/Volume4No1/Paper_9-Evaluating_English_to_Arabic_Machine...", "snippet": "called <b>BiLingual</b> <b>Evaluation</b> <b>Understudy</b> (<b>BLEU</b>) which was introduced in the study of Papineni, Roukos, Ward, and Zhu [1] and claimed to be language independent and highly correlated with human <b>evaluation</b>. <b>BLEU</b> is based on a core idea to determine the quality of any machine translation system which is summarized by the closeness of the candidate output of the machine translation system to reference (professional human) translation of the same text. The closeness of the candidate translation to ...", "dateLastCrawled": "2022-01-19T08:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Choosing the <b>Right Metric for Evaluating Machine Learning</b> Models \u2013 Part ...", "url": "https://www.kdnuggets.com/2018/04/right-metric-evaluating-machine-learning-models-1.html", "isFamilyFriendly": true, "displayUrl": "https://www.<b>kdnuggets</b>.com/2018/04/right-metric-evaluating-machine-learning-models-1.html", "snippet": "<b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) It is mostly used to measure the quality of machine translation with respect to the human translation. It uses a modified form of precision metric. Steps to compute <b>BLEU</b> score: 1. Convert the sentence into unigrams, bigrams, trigrams, and 4-grams 2. Compute precision for n-grams of size 1 to 4 3. Take the ...", "dateLastCrawled": "2022-01-25T22:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>python</b> - Find the similarity metric between two strings - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/17388213/find-the-similarity-metric-between-two-strings", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/17388213", "snippet": "<b>BLEU</b>, or the <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>, is a score for <b>comparing</b> a candidate translation of text to one or more reference <b>translations</b>. A perfect match results in a score of 1.0, whereas a perfect mismatch results in a score of 0.0.", "dateLastCrawled": "2022-01-29T01:01:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Understanding MT Quality: <b>BLEU</b> Scores", "url": "https://blog.modernmt.com/understanding-mt-quality-bleu-scores/", "isFamilyFriendly": true, "displayUrl": "https://blog.modernmt.com/understanding-mt-quality-<b>bleu</b>-scores", "snippet": "What is a <b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) score? The <b>BLEU</b> score is a string-matching algorithm that provides basic output quality metrics for MT researchers and developers. In this post, we review and look more closely at the <b>BLEU</b> score, which is probably the most widely used MT quality assessment metric in use by MT researchers and developers over the last 15 years. While it is widely understood that <b>BLEU</b> has many flaws, it has not been displaced or replaced by a widely accepted ...", "dateLastCrawled": "2022-02-03T17:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Understanding MT Quality: <b>BLEU</b> Scores | by K Vashee | Medium", "url": "https://kvashee.medium.com/understanding-mt-quality-bleu-scores-9a19ed20526d", "isFamilyFriendly": true, "displayUrl": "https://kvashee.medium.com/understanding-mt-quality-<b>bleu</b>-scores-9a19ed20526d", "snippet": "What is <b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>)? As the use of enterprise machine translation expands, it becomes increasingly more important for users and practitioners to understand MT quality issues in a relevant, meaningful, and accurate way. The <b>BLEU</b> score is a string-matching algorithm that provides basic outpu t quality metrics for MT researchers and developers. In this first post, we will review and look more closely at the <b>BLEU</b> score, which is probably the most widely used MT quality ...", "dateLastCrawled": "2022-01-31T18:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A Gentle Introduction to Calculating the <b>BLEU</b> Score for Text in Python", "url": "https://machinelearningmastery.com/calculate-bleu-score-for-text-python/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/calculate-<b>bleu</b>-score-for-text-pyth", "snippet": "<b>BLEU</b>, or the <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>, is a score for <b>comparing</b> a candidate translation of text to one or more reference <b>translations</b>. Although developed for translation, it can be used to evaluate text generated for a suite of natural language processing tasks. In this tutorial, you will discover the <b>BLEU</b> score for evaluating and scoring candidate text using the NLTK library in", "dateLastCrawled": "2022-01-30T22:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Urdu to English Machine Translation using <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>", "url": "https://research.ijcaonline.org/volume82/number7/pxc3891040.pdf", "isFamilyFriendly": true, "displayUrl": "https://research.ijcaonline.org/volume82/number7/pxc3891040.pdf", "snippet": "<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b> (<b>BLEU</b>). The EBMT approach produced the highest <b>accuracy</b> of 84.21% whereas <b>the accuracy</b> of the online SMT system is 62.68%. We found that BLUE scores of machine translated long Urdu sentences are low in comparison with long sentences. Similarly source text containing low frequency words affect the quality of Urdu machine translation negatively. Experiments and findings section of this paper explicate our reported results in detail. The paper concludes with ...", "dateLastCrawled": "2021-12-13T23:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Evaluating Text Output in NLP: <b>BLEU</b> at your own risk | by Rachael ...", "url": "https://towardsdatascience.com/evaluating-text-output-in-nlp-bleu-at-your-own-risk-e8609665a213", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/evaluating-text-output-in-nlp-<b>bleu</b>-at-your-own-risk-e...", "snippet": "This measure, looking at n-grams overlap between the output and reference <b>translations</b> with a penalty for shorter outputs, is known as <b>BLEU</b> (short for \u201c<b>Bilingual</b> <b>evaluation</b> <b>understudy</b>\u201d which people literally only ever say when explaining the acronym) and was developed by Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu at IBM in 2002. It\u2019s a very popular metric in NLP, particularly for tasks where the output of a system is a text string rather than a classification. This ...", "dateLastCrawled": "2022-02-02T22:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Choosing the Right Metric for Evaluating Machine Learning Models \u2014 Part ...", "url": "https://medium.com/usf-msds/choosing-the-right-metric-for-machine-learning-models-part-1-a99d7d7414e4", "isFamilyFriendly": true, "displayUrl": "https://medium.com/usf-msds/choosing-the-right-metric-for-machine-learning-<b>model</b>s-part...", "snippet": "<b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) It is mostly used to measure the quality of machine translation with respect to the human translation. It uses a modified form of precision metric. It uses a ...", "dateLastCrawled": "2022-01-31T05:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>python</b> - Find the similarity metric between two strings - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/17388213/find-the-similarity-metric-between-two-strings", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/17388213", "snippet": "<b>BLEU</b>, or the <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>, is a score for <b>comparing</b> a candidate translation of text to one or more reference <b>translations</b>. A perfect match results in a score of 1.0, whereas a perfect mismatch results in a score of 0.0.", "dateLastCrawled": "2022-01-29T01:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Evaluation</b> of an <b>NLP</b> model \u2014 latest benchmarks | by Ria Kulshrestha ...", "url": "https://towardsdatascience.com/evaluation-of-an-nlp-model-latest-benchmarks-90fd8ce6fae5", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>evaluation</b>-of-an-<b>nlp</b>-model-latest-benchmarks-90fd8ce6fae5", "snippet": "<b>BLEU</b> Score \u2014 <b>BiLingual</b> <b>Evaluation</b> <b>Understudy</b>. As the name suggests, it was originally used to evaluate <b>translations</b> from one language to another. How to calculate <b>BLEU</b> score? Calculating unigram precision: Step 1: Look at each word in the output sentence and assign it a score of 1 if it shows up in any of the reference sentences and 0 if it doesn\u2019t. Step 2: Normalize that count, so that it\u2019s always between 0 and 1, by dividing the number of words that showed up in one of the reference ...", "dateLastCrawled": "2022-01-28T07:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Choosing the <b>Right Metric for Evaluating Machine Learning</b> Models \u2013 Part ...", "url": "https://www.kdnuggets.com/2018/04/right-metric-evaluating-machine-learning-models-1.html", "isFamilyFriendly": true, "displayUrl": "https://www.<b>kdnuggets</b>.com/2018/04/right-metric-evaluating-machine-learning-models-1.html", "snippet": "<b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) It is mostly used to measure the quality of machine translation with respect to the human translation. It uses a modified form of precision metric. Steps to compute <b>BLEU</b> score: 1. Convert the sentence into unigrams, bigrams, trigrams, and 4-grams 2. Compute precision for n-grams of size 1 to 4 3. Take the ...", "dateLastCrawled": "2022-01-25T22:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Automatic Assessment of Open Ended</b> Questions with a <b>Bleu</b>-Inspired ...", "url": "https://www.researchgate.net/publication/221418794_Automatic_Assessment_of_Open_Ended_Questions_with_a_Bleu-Inspired_Algorithm_and_Shallow_NLP", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221418794_<b>Automatic_Assessment_of_Open_Ended</b>...", "snippet": "Atenea (1) Atenea (Alfonseca and P\u00e9rez 2004) initially uses the <b>BLEU</b> (<b>BiLingual</b> <b>Evaluation</b> <b>Understudy</b>) metric (Papineni et al. 2002) for scoring. This metric is based on n-gram overlap and ...", "dateLastCrawled": "2021-12-24T02:25:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Human <b>Evaluation</b> Of Machine Translation", "url": "https://groups.google.com/g/ug5zfjhnv/c/JfGeRYENjvg", "isFamilyFriendly": true, "displayUrl": "https://groups.google.com/g/ug5zfjhnv/c/JfGeRYENjvg", "snippet": "The <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b> Score, <b>can</b>, this ever increase our number of faces and vertices of the models thus its complexity therefore better not desired in VR applications. At out end as each report date also summarize key points with him own Insights. Stay informed on the latest trending ML papers with code, ignoring the position. Machine translation has contribute a long ways over the years. Also, electrochemical and industrial processes, September. Contrary to <b>BLEU</b> which aims to ...", "dateLastCrawled": "2022-01-22T08:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Why Evaluating Machine Translation Quality is</b> Hard | Waygo", "url": "http://blog.waygoapp.com/why-evaluating-machine-translation-quality-is-hard/", "isFamilyFriendly": true, "displayUrl": "blog.waygoapp.com/<b>why-evaluating-machine-translation-quality-is</b>-hard", "snippet": "<b>BLEU</b> (an acronym for <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) is an algorithm for evaluating the quality of text which has been machine-translated from one language to another. The idea behind <b>BLEU</b> is that \u201cthe closer a machine translation is to a professional human translation, the better it is\u201d. This sounds about the same as our first idea above \u2013 to let a translator evaluate the quality every time. The key difference is that before a machine gets involved, <b>BLEU</b> first has professional human ...", "dateLastCrawled": "2021-11-28T05:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Automatic Assessment of Open Ended</b> Questions with a <b>Bleu</b>-Inspired ...", "url": "https://www.researchgate.net/publication/221418794_Automatic_Assessment_of_Open_Ended_Questions_with_a_Bleu-Inspired_Algorithm_and_Shallow_NLP", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221418794_<b>Automatic_Assessment_of_Open_Ended</b>...", "snippet": "Then synonyms with particularly common <b>translations</b> <b>can</b> be incorporated into the teacher answers. Atenea (1) Atenea Alfonseca and P\u00e9rez 2004) initially uses the <b>BLEU</b> (<b>BiLingual</b> <b>Evaluation</b> ...", "dateLastCrawled": "2021-12-24T02:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>python</b> - Find the similarity metric between two strings - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/17388213/find-the-similarity-metric-between-two-strings", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/17388213", "snippet": "<b>BLEU</b>, or the <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>, is a score for <b>comparing</b> a candidate translation of text to one or more reference <b>translations</b>. A perfect match results in a score of 1.0, whereas a perfect mismatch results in a score of 0.0. Although developed for translation, it <b>can</b> be used to evaluate text generated for a suite of natural language processing tasks. Code: import nltk from nltk.translate import <b>bleu</b> from nltk.translate.<b>bleu</b>_score import SmoothingFunction smoothie ...", "dateLastCrawled": "2022-01-29T01:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) The Evaluations of Thai Poetry Translator to English with Prosody ...", "url": "https://www.researchgate.net/publication/275156487_The_Evaluations_of_Thai_Poetry_Translator_to_English_with_Prosody_Keeping", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/275156487_The_<b>Evaluations</b>_of_Thai_Poetry...", "snippet": "<b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) metric will be used to compare between reference and candidates of English results. The abbreviations are as follows, T1 (G) translated by Google API, T1 (D ...", "dateLastCrawled": "2021-12-13T12:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Multi-Modal Methods: Image Captioning (From Translation to Attention</b> ...", "url": "https://blog.mlreview.com/multi-modal-methods-image-captioning-from-translation-to-attention-895b6444256e", "isFamilyFriendly": true, "displayUrl": "https://blog.mlreview.com/<b>multi-modal-methods-image-captioning-from-translation</b>-to...", "snippet": "One such <b>evaluation</b> metric is the <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b> algorithm, or <b>BLEU</b> score. The <b>BLEU</b> score comes from work in machine translation, which is where image captioning takes much of its inspiration; as well as from image ranking/retrieval and action recognition. Understanding the basic <b>BLEU</b> score is quite intuitive. A set of high-quality human <b>translations</b> are obtained for a given piece of text, and the machine\u2019s translation is compared against these human baselines, section by ...", "dateLastCrawled": "2022-01-22T07:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "GitHub - rdewey1/Python-Neural-Machine-Translation: The goal of this ...", "url": "https://github.com/rdewey1/Python-Neural-Machine-Translation", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/rdewey1/Python-Neural-Machine-Translation", "snippet": "We used <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b> (<b>BLEU</b>) and Recall Oriented <b>Understudy</b> for Gisting <b>Evaluation</b> (ROUGE) to evaluate the quality of our NMT models. We chose to use <b>BLEU</b> as a primary <b>evaluation</b> metric for a number of reasons. <b>BLEU</b> is designed specifically to evaluate Machine <b>Translations</b> by <b>comparing</b> strings of words and <b>can</b> <b>be thought</b> of as a measure of similarity. In our case, the <b>BLEU</b> scores are a number between 0 and 1 that suggests the quality of the machine translation as compared ...", "dateLastCrawled": "2021-11-18T23:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Development of machine translation technology</b> for ... - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S1532046418301448", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1532046418301448", "snippet": "<b>BLEU</b>- <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b> score. Only two text systems were based exclusively on RBT , , with ... one study <b>comparing</b> two pilot speech-to-speech translation systems showed that users preferred a system that works \u201cwell enough\u201d to avoid dangerous interactions without constraining their ability to express themselves . Higher reading level source documents were correlated with poorer MT <b>accuracy</b> . Although many evaluations confirmed that use of MT could improve efficiency and ...", "dateLastCrawled": "2022-01-05T15:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Comprehensive analysis of embeddings and pre-training in NLP ...", "url": "https://www.sciencedirect.com/science/article/pii/S1574013721000733", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1574013721000733", "snippet": "The GloVe model proposed by (Pennington et al. 2014) <b>can</b> <b>be thought</b> of as the mix of the count-based matrix factorization and the context-based skip-gram model together. This methodology allows for the study in a much more in-depth manner of the context surrounding each word since co-occurrence probabilities between words hold a great deal of information about context between them. However, (Pennington et al. 2014) also found the advantage of finding the ratio of the co-occurrence between ...", "dateLastCrawled": "2022-01-27T01:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "An intro to ROUGE, and how to <b>use it to evaluate summaries</b>", "url": "https://www.freecodecamp.org/news/what-is-rouge-and-how-it-works-for-evaluation-of-summaries-e059fb8ac840/", "isFamilyFriendly": true, "displayUrl": "https://www.freecodecamp.org/news/what-is-rouge-and-how-it-works-for-<b>evaluation</b>-of...", "snippet": "For more in-depth information about these <b>evaluation</b> metrics, you <b>can</b> refer to Lin\u2019s paper. Which measure to use depends on the specific task that you are trying to evaluate. If you are working on extractive summarization with fairly verbose system and reference summaries, then it may make sense to use ROUGE-1 and ROUGE-L. For very concise summaries, ROUGE-1 alone may suffice, especially if you are also applying stemming and stop word removal.", "dateLastCrawled": "2022-02-02T07:41:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Understanding MT Quality: <b>BLEU</b> Scores | by K Vashee | Medium", "url": "https://kvashee.medium.com/understanding-mt-quality-bleu-scores-9a19ed20526d", "isFamilyFriendly": true, "displayUrl": "https://kvashee.medium.com/understanding-mt-quality-<b>bleu</b>-scores-9a19ed20526d", "snippet": "Thus, <b>BLEU</b> should not be used as an absolute measure of translation quality because the <b>BLEU</b> score <b>can</b> vary even for one language depending on the test and subject domain. In most cases <b>comparing</b> <b>BLEU</b> scores across different languages is meaningless unless very strict protocols have been followed. Because of this, it is always recommended to use human translators to verify <b>the accuracy</b> of the metrics after systems have been built. Also, most MT industry leaders will always vet the <b>BLEU</b> score ...", "dateLastCrawled": "2022-01-31T18:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Understanding MT Quality: <b>BLEU</b> Scores", "url": "https://blog.modernmt.com/understanding-mt-quality-bleu-scores/", "isFamilyFriendly": true, "displayUrl": "https://blog.modernmt.com/understanding-mt-quality-<b>bleu</b>-scores", "snippet": "What is a <b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) score? The <b>BLEU</b> score is a string-matching algorithm that provides basic output quality metrics for MT researchers and developers. In this post, we review and look more closely at the <b>BLEU</b> score, which is probably the most widely used MT quality assessment metric in use by MT researchers and developers over the last 15 years. While it is widely understood that <b>BLEU</b> has many flaws, it has not been displaced or replaced by a widely accepted ...", "dateLastCrawled": "2022-02-03T17:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "How <b>BLEU</b> Measures Translation and Why It Matters - <b>Slator</b>", "url": "https://slator.com/how-bleu-measures-translation-and-why-it-matters/", "isFamilyFriendly": true, "displayUrl": "https://<b>slator</b>.com/how-<b>bleu</b>-measures-translation-and-why-it-matters", "snippet": "They called this the <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>; or simply, <b>BLEU</b>. Download the <b>Slator</b> 2019 Neural Machine Translation Report for the latest insights on the state-of-the art in neural machine translation and its deployment. <b>Slator</b> 2019 Neural Machine Translation Report: Deploying NMT in Operations. Data and Research. 32 pages, NMT state-of-the-art, 5 case studies, 30 commentaries, NMT in day-to-day operations $ 85 BUY NOW. The <b>BLEU</b> study was, in part, developed under contract with the US ...", "dateLastCrawled": "2022-02-03T11:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "What is <b>BLEU</b> Score? (And How Does it Affect Translation?) - Asian ...", "url": "https://asianabsolute.co.uk/blog/2021/01/28/what-is-bleu-score/", "isFamilyFriendly": true, "displayUrl": "https://asianabsolute.co.uk/blog/2021/01/28/what-is-<b>bleu</b>-score", "snippet": "<b>BLEU</b> stands for <b>BiLingual</b> <b>Evaluation</b> <b>Understudy</b>. A <b>BLEU</b> score is a quality metric assigned to a text which has been translated by a Machine Translation engine. The goal with MT is to produce results equal to those of a professional human translator. <b>BLEU</b> is an algorithm which is designed to measure exactly that. Because it is fast and cheap to carry out, <b>BLEU</b> has stayed one of the most popular automated metrics for determining the quality of Machine Translation. Even though it was one of the ...", "dateLastCrawled": "2022-01-30T21:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) <b>BLEU</b>: a Method for <b>Automatic Evaluation of Machine Translation</b>", "url": "https://www.researchgate.net/publication/2588204_BLEU_a_Method_for_Automatic_Evaluation_of_Machine_Translation", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/2588204_", "snippet": "We describe two metrics for <b>automatic evaluation of machine trans-lation</b> quality. These metrics, <b>BLEU</b> and NEE, are <b>compared</b> to hu-man judgment of quality of translation of Arabic, Chinese, French ...", "dateLastCrawled": "2022-02-01T20:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Evaluating Text Output in NLP: <b>BLEU</b> at your own risk | by Rachael ...", "url": "https://towardsdatascience.com/evaluating-text-output-in-nlp-bleu-at-your-own-risk-e8609665a213", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/evaluating-text-output-in-nlp-<b>bleu</b>-at-your-own-risk-e...", "snippet": "This measure, looking at n-grams overlap between the output and reference <b>translations</b> with a penalty for shorter outputs, is known as <b>BLEU</b> (short for \u201c<b>Bilingual</b> <b>evaluation</b> <b>understudy</b>\u201d which people literally only ever say when explaining the acronym) and was developed by Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu at IBM in 2002. It\u2019s a very popular metric in NLP, particularly for tasks where the output of a system is a text string rather than a classification. This ...", "dateLastCrawled": "2022-02-02T22:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Automatic <b>Image Captioning</b> Using Deep Learning | by Manthan Bhikadiya \ud83d\udca1 ...", "url": "https://medium.com/swlh/automatic-image-captioning-using-deep-learning-5e899c127387", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/automatic-<b>image-captioning</b>-using-deep-learning-5e899c127387", "snippet": "<b>BLEU</b>, or the <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>, is a score for <b>comparing</b> a candidate translation of the text to one or more reference <b>translations</b>. Although developed for translation, it <b>can</b> be used ...", "dateLastCrawled": "2022-01-28T06:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Automatic Assessment of Open Ended</b> Questions with a <b>Bleu</b>-Inspired ...", "url": "https://www.researchgate.net/publication/221418794_Automatic_Assessment_of_Open_Ended_Questions_with_a_Bleu-Inspired_Algorithm_and_Shallow_NLP", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221418794_<b>Automatic_Assessment_of_Open_Ended</b>...", "snippet": "Then synonyms with particularly common <b>translations</b> <b>can</b> be incorporated into the teacher answers. Atenea (1) Atenea Alfonseca and P\u00e9rez 2004) initially uses the <b>BLEU</b> (<b>BiLingual</b> <b>Evaluation</b> ...", "dateLastCrawled": "2021-12-24T02:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Understanding Encoders-Decoders with Attention Based Mechanism | DataX ...", "url": "https://medium.com/data-science-community-srm/understanding-encoders-decoders-with-attention-based-mechanism-c1eb7164c581", "isFamilyFriendly": true, "displayUrl": "https://medium.com/data-science-community-srm/understanding-encoders-decoders-with...", "snippet": "The <b>bilingual</b> <b>evaluation</b> <b>understudy</b> score, or BLEUfor short, is an important metric for evaluating these types of sequence-based models. It helps to provide a metric for a generated sentence to an ...", "dateLastCrawled": "2022-01-31T22:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "An Analysis of Google Translate <b>Accuracy</b> - <b>TRANSLATE 2011-3</b>", "url": "https://sites.google.com/site/translation20113/analisis", "isFamilyFriendly": true, "displayUrl": "https://<b>sites.google.com</b>/site/translation20113/analisis", "snippet": "Although several techniques exist, <b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) is perhaps the most common (Papineni, et al., 2002), and some studies have shown that it achieves a high correlation with human judgments of quality (Coughlin, 2003; Culy &amp; Richemann, 2003). Using this technique, scores ranging from 0 to 100 are calculated for a sample of translated text by <b>comparing</b> it to a reference translation, and the method takes into account the number of words that are the same in the two ...", "dateLastCrawled": "2021-12-22T01:21:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Evaluation</b> of an <b>NLP</b> model \u2014 latest benchmarks | by Ria Kulshrestha ...", "url": "https://towardsdatascience.com/evaluation-of-an-nlp-model-latest-benchmarks-90fd8ce6fae5", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>evaluation</b>-of-an-<b>nlp</b>-model-latest-benchmarks-90fd8ce6fae5", "snippet": "<b>BLEU</b> Score \u2014 <b>BiLingual</b> <b>Evaluation</b> <b>Understudy</b>. As the name suggests, it was originally used to evaluate translations from one language to another. How to calculate <b>BLEU</b> score? Calculating unigram precision: Step 1: Look at each word in the output sentence and assign it a score of 1 if it shows up in any of the reference sentences and 0 if it doesn\u2019t. Step 2: Normalize that count, so that it\u2019s always between 0 and 1, by dividing the number of words that showed up in one of the reference ...", "dateLastCrawled": "2022-01-28T07:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Machine</b> <b>Learning</b> Glossary: Language <b>Evaluation</b> | Google Developers", "url": "https://developers.google.com/machine-learning/glossary/language", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine</b>-<b>learning</b>/glossary/language", "snippet": "<b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) #language. A score between 0.0 and 1.0, inclusive, indicating the quality of a translation between two human languages (for example, between English and Russian). A <b>BLEU</b> score of 1.0 indicates a perfect translation; a <b>BLEU</b> score of 0.0 indicates a terrible translation. C. causal language model. #language. Synonym for unidirectional language model. See bidirectional language model to contrast different directional approaches in language modeling. crash ...", "dateLastCrawled": "2022-01-29T02:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Natrual language processing basic concepts - language model - word ...", "url": "https://shuffleai.blog/blog/nlp_concepts_part_1.html", "isFamilyFriendly": true, "displayUrl": "https://shuffleai.blog/blog/nlp_concepts_part_1.html", "snippet": "<b>BLEU</b> stands for <b>bilingual</b> <b>evaluation</b> <b>understudy</b>. It&#39;s an automatic metric to evaluate how close a sequence of text generated by a language model is to a reference. At first, it&#39;s used to evaluate the quality of <b>machine</b> translation text. Now other natural language processing tasks such as task-oriented dialogue generation adopt it as well. For a reference &quot;The man returned to the store&quot;, a generated text &quot;the the man the&quot; would get a BLUE score as below. For each word in the generated text ...", "dateLastCrawled": "2021-12-24T07:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "The <b>Computational</b> Limits of Deep <b>Learning</b> | DeepAI", "url": "https://deepai.org/publication/the-computational-limits-of-deep-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/the-<b>computational</b>-limits-of-deep-<b>learning</b>", "snippet": "<b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) [papineni2002bleu] score is a metric for translation and computes the similarity between human translation and <b>machine</b> translation based on n-gram. An n-gram is a continuous sequence of n items from a given text. The score is based on precision, brevity penalty, and clipping. The modified n-gram precision ...", "dateLastCrawled": "2022-01-28T07:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "9.7. <b>Sequence</b> to <b>Sequence</b> <b>Learning</b> \u2014 Dive into Deep <b>Learning</b> 0.17.2 ...", "url": "https://d2l.ai/chapter_recurrent-modern/seq2seq.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_recurrent-modern/seq2seq.html", "snippet": "<b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>), though originally proposed for evaluating <b>machine</b> translation results [Papineni et al., 2002], has been extensively used in measuring the quality of output sequences for different applications.", "dateLastCrawled": "2022-01-26T00:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>The Evolution of Machine Translation</b> | LLM Law Review", "url": "https://www.llmlawreview.com/2018/01/26/the-evolution-of-machine-translation/", "isFamilyFriendly": true, "displayUrl": "https://www.llmlawreview.com/2018/01/26/<b>the-evolution-of-machine-translation</b>", "snippet": "Using the <b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) method to score the outcome, they found that NMT scored consistently higher than PBSMT for accuracy. In addition, human translators whose native language was Catalan but who were also fluent in English, evaluated sections of the MT translation in three of the books. Once again, the NMT outperformed its rival. It was estimated that \u201cbetween 17% and 34% of the translations \u2026 are perceived by native speakers of the target language to be of ...", "dateLastCrawled": "2022-01-19T23:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Arti cial Intelligence Master Thesis", "url": "https://upcommons.upc.edu/bitstream/handle/2117/105513/122533.pdf?sequence=1", "isFamilyFriendly": true, "displayUrl": "https://upcommons.upc.edu/bitstream/handle/2117/105513/122533.pdf?sequence=1", "snippet": "2:87 <b>BLEU</b> (<b>BiLingual</b> <b>Evaluation</b> <b>Understudy</b>) score over the baseline; attention model, for German-English translation, and 0:34 <b>BLEU</b> score improvement for Catalan-Spanish trans-lation. Keywords <b>Machine</b> <b>Learning</b>, Deep <b>Learning</b>, Natural Language Processing, Neural <b>Machine</b> Transla-tion", "dateLastCrawled": "2021-12-27T01:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) <b>Machine</b> Translation Development for Indian Languages and its ...", "url": "https://www.academia.edu/12395564/Machine_Translation_Development_for_Indian_Languages_and_its_Approaches", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/12395564", "snippet": "Urdu to English 2013 Urdu-English General Corpus based Explained <b>Machine</b> Translation methodology of using <b>Bilingual</b> each system and <b>Evaluation</b> found their <b>Understudy</b> [32] comparison based on their respective outputs using <b>BLEU</b>. The EBMT approach produced accuracy of 84.21% whereas the accuracy of the online SMT system is 62.68%. 4. Developing English- -- English-Urdu General Interlingua The English-Hindi Urdu <b>Machine</b> based Rule- lexical database is Translation Via Hindi based approach used ...", "dateLastCrawled": "2022-01-10T06:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) <b>Evaluation of machine translation systems and related procedures</b>", "url": "https://www.researchgate.net/publication/326320090_Evaluation_of_machine_translation_systems_and_related_procedures", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/326320090_<b>Evaluation</b>_of_<b>machine</b>_translation...", "snippet": "<b>Evaluation of machine translation systems and related procedures</b>. June 2018 ; Journal of Engineering and Applied Sciences 13(12):3961-3972; Project: <b>Machine</b> <b>learning</b>; Authors: Musatafa Albadr ...", "dateLastCrawled": "2022-01-15T04:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(PDF) <b>Learning</b> by <b>Analogy</b>: A Classification Rule for Binary and Nominal ...", "url": "https://www.researchgate.net/publication/220812160_Learning_by_Analogy_A_Classification_Rule_for_Binary_and_Nominal_Data", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/220812160_<b>Learning</b>_by_<b>Analogy</b>_A...", "snippet": "We are interested here in the use of analogical proportions for making predictions, in a <b>machine</b> <b>learning</b> context. In recent works, <b>analogy</b>-based classifiers have achieved noteworthy performances ...", "dateLastCrawled": "2022-01-05T06:35:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(bleu (bilingual evaluation understudy))  is like +(comparing the accuracy of translations)", "+(bleu (bilingual evaluation understudy)) is similar to +(comparing the accuracy of translations)", "+(bleu (bilingual evaluation understudy)) can be thought of as +(comparing the accuracy of translations)", "+(bleu (bilingual evaluation understudy)) can be compared to +(comparing the accuracy of translations)", "machine learning +(bleu (bilingual evaluation understudy) AND analogy)", "machine learning +(\"bleu (bilingual evaluation understudy) is like\")", "machine learning +(\"bleu (bilingual evaluation understudy) is similar\")", "machine learning +(\"just as bleu (bilingual evaluation understudy)\")", "machine learning +(\"bleu (bilingual evaluation understudy) can be thought of as\")", "machine learning +(\"bleu (bilingual evaluation understudy) can be compared to\")"]}