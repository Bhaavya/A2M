{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Structural</b> <b>risk</b> <b>minimization</b> and minimum description length", "url": "https://andrewcharlesjones.github.io/journal/mdl.html", "isFamilyFriendly": true, "displayUrl": "https://andrewcharlesjones.github.io/journal/mdl.html", "snippet": "<b>Structural</b> <b>risk</b> <b>minimization</b> is a paradigm wherein we can assign priority values to various hypothesis classes under consideration, and use these assignments to guide the learning process. Specifically, again let $\\mathcal{H}$ be the overall hypothesis class, but now it\u2019s made up of a number of smaller hypothesis classes. We can write $\\mathcal{H} = \\bigcup\\limits_{n \\in \\mathbb{N}} \\mathcal{H}_n$ (it will become important later that we can write $\\mathcal{H}$ as a countable union of sub ...", "dateLastCrawled": "2022-01-02T01:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Structural Risk Minimization for Character Recognition</b>", "url": "https://proceedings.neurips.cc/paper/1991/file/10a7cdd970fe135cf4f7bb55c0e3b59f-Paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/1991/file/10a7cdd970fe135cf4f7bb55c0e3b59f-Paper.pdf", "snippet": "<b>Structural Risk Minimization for Character Recognition</b> I. Guyon, V. Vapnik, B. Boser, L. Bottou, and S. A. Solla AT&amp;T Bell Laboratories Holmdel, NJ 07733, USA Abstract The method of <b>Structural</b> <b>Risk</b> <b>Minimization</b> refers to tuning the capacity of the classifier to the available amount of training data. This capac\u00ad", "dateLastCrawled": "2022-02-01T13:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Lecture 8: Model selection and <b>structural</b> <b>risk minimization</b>", "url": "https://people.cs.umass.edu/~akshay/courses/cs690m/files/lec8.pdf", "isFamilyFriendly": true, "displayUrl": "https://people.cs.umass.edu/~akshay/courses/cs690m/files/lec8.pdf", "snippet": "Remember that we usually get a bound <b>like</b> O(q dlog(1= ) n) for the ERM of a class with VC-dimension d. Thus the <b>risk</b> on the validation set is a more precise estimate of the true <b>risk</b> than what we can even hope for, so there is essentially no loss. In cross-validation we typically train a bunch of models, all on the training data, and then look at their validation <b>risk</b>, choosing the model with the lowest validation <b>risk</b>. This method is justi ed by the above proposition and the union bound ...", "dateLastCrawled": "2021-12-31T02:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "What is the difference between sieve estimation and <b>structural</b> <b>risk</b> ...", "url": "https://stats.stackexchange.com/questions/505559/what-is-the-difference-between-sieve-estimation-and-structural-risk-minimization", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/505559/what-is-the-difference-between-sieve...", "snippet": "<b>SRM</b> is essentially a <b>regularization</b> procedure to avoid overfitting. Think about how we add a <b>regularization</b> term in the SVM estimator. Basically, we are trying to find a reasonable trade-off between how well the model fits the data and how complicated the model is. Interestingly, there is a relationship here. In both cases, we are essentially looking to approximate a complicated model in a less complicated space.", "dateLastCrawled": "2022-01-17T07:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Dyadic Classification Trees via <b>Structural</b> <b>Risk</b> <b>Minimization</b>", "url": "https://proceedings.neurips.cc/paper/2002/file/6832a7b24bc06775d02b7406880b93fc-Paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/2002/file/6832a7b24bc06775d02b7406880b93fc-Paper.pdf", "snippet": "<b>Structural</b> <b>risk</b> <b>minimization</b> (<b>SRM</b>) is an inductive principle for selecting a classi\ufb01er from a sequence of sets of classi\ufb01ers based on complexity <b>regularization</b>. It was introduced by Vapnik and Chervonenkis (see [2]), and later analyzed by Lugosi and Zeger [3], [4, Ch. 18]. We formulate <b>structural</b> <b>risk</b> <b>minimization</b> for dyadic classi\ufb01cation trees by applying results from [4, Ch. 18]. <b>SRM</b> is formulated in terms of the VC dimension, which we brie\ufb02y review. Let collection of classi\ufb01ers ...", "dateLastCrawled": "2021-09-19T13:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "STAT 535 Lecture 5 November, 2020 Brief overview of Model Selection and ...", "url": "https://sites.stat.washington.edu/mmp/courses/535/fall20/Handouts/l5-model-sel-regularization.pdf", "isFamilyFriendly": true, "displayUrl": "https://sites.stat.washington.edu/.../535/fall20/Handouts/l5-model-sel-<b>regularization</b>.pdf", "snippet": "3.2 <b>Structural</b> <b>risk</b> <b>minimization</b> (<b>SRM</b>) The importance of the VC-dimension comes from theorems such as this one. Theorem 1 Let Fbe a model class of VC-dimension hand f a classi er in F. Then, with probability w.p. &gt;1 over training sets L 01(f) L^ 01(f) + r h[1 + log(2N=h)] + log(4= ) N: (6) In other words, for a small VC-dimension, the empirical loss L^ 01(f) is a good predictor of the true loss L 01(f). The gure below displays graphically the last term from equation (6) for values of = 0:1 ...", "dateLastCrawled": "2022-01-13T14:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Overfittingand Model Selection</b>", "url": "https://www.cs.cmu.edu/~epxing/Class/10701/slides/lecture10-fit.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.cmu.edu/~epxing/Class/10701/slides/lecture10-fit.pdf", "snippet": "Generalization Theory and <b>Structural</b> <b>Risk</b> <b>Minimization</b> The battle against overfitting: each learning algorithm has some &quot;free knobs&quot; that one can &quot;tune&quot; (i.e., heck) to make the algorithm generalizes better to test data. But is there a more principled way? Cross validation <b>Regularization</b> Feature selection", "dateLastCrawled": "2022-02-03T03:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Model Selection in Machine Learning | by ANUSHKA BAJPAI | Medium", "url": "https://medium.com/@anushkhabajpai/model-selection-in-machine-learning-c568e5a42dcc", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@anushkhabajpai/model-selection-in-machine-learning-c568e5a42dcc", "snippet": "<b>Structural</b> <b>Risk</b> <b>Minimization</b> (<b>SRM</b>) Machine learning models face the inevitable problem of defining a generalized theory from a set of finite data. This leads to cases of overfitting where the ...", "dateLastCrawled": "2022-01-29T03:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Machine Learning algorithms based on &quot;<b>structural</b> <b>risk</b> <b>minimization</b>&quot;?", "url": "https://cs.stackexchange.com/questions/2006/machine-learning-algorithms-based-on-structural-risk-minimization", "isFamilyFriendly": true, "displayUrl": "https://cs.stackexchange.com/questions/2006/machine-learning-algorithms-based-on...", "snippet": "$\\begingroup$ As far as I can tell <b>SRM</b> is nothing but good old <b>regularization</b>, which is used absolutely everywhere. $\\endgroup$ \u2013 Emre. May 23 &#39;12 at 19:52 . Add a comment | 1 Answer Active Oldest Votes. 9 $\\begingroup$ The <b>structural</b> <b>risk</b> <b>minimization</b> principle is a principle that is at least partly &#39;used&#39; in all machine learning methods, since overfitting is often to be taken into account: reducing the complexity of the model is (supposedly and in practice) a good way to limit ...", "dateLastCrawled": "2022-01-19T13:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(PDF) <b>Support Vector Machines: Theory and Applications</b>", "url": "https://www.researchgate.net/publication/221621494_Support_Vector_Machines_Theory_and_Applications", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221621494_Support_Vector_Machines", "snippet": "hypothesis spaces is known as <b>Structural</b> <b>Risk</b> <b>Minimization</b> (<b>SRM</b>) (Vapnik, 1998). An important question that arises in SLT is that of meas uring the &quot;complexity&quot; of a hypothesis space - which, as ...", "dateLastCrawled": "2022-02-02T02:47:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Structural Risk Minimization</b> - AI Alignment Forum", "url": "https://www.alignmentforum.org/posts/5bd75cc58225bf0670374f81/structural-risk-minimization", "isFamilyFriendly": true, "displayUrl": "https://www.alignmentforum.org/posts/5bd75cc58225bf0670374f81/<b>structural-risk-minimization</b>", "snippet": "<b>Structural risk minimization</b>, a concept from computational learning theory, is proposed as a satisficing framework. ----- The goal of this post <b>is similar</b> to Creating a Satisficer and Minimax as an approach to reduced-impact AI: constructing agents which are safe even with a utility function which isn&#39;t quite what is actually wanted. The approach <b>is similar</b> to Paul Christiano&#39;s Model-Free Decisions. The philosophy behind this solution is to treat it as an example of the optimizer&#39;s curse ...", "dateLastCrawled": "2022-01-15T11:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Lecture 8: Model selection and <b>structural</b> <b>risk minimization</b>", "url": "https://people.cs.umass.edu/~akshay/courses/cs690m/files/lec8.pdf", "isFamilyFriendly": true, "displayUrl": "https://people.cs.umass.edu/~akshay/courses/cs690m/files/lec8.pdf", "snippet": "<b>Structural</b> <b>risk minimization</b> is a way to basically do this for free, and get the best of both worlds. The rst observation is that if we weight the classes appropriately by taking a union bound, we can get a convergence result that is fairly <b>similar</b> to the uniform convergence results we have used in the past. This is a form of non-uniform ...", "dateLastCrawled": "2021-12-31T02:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Structural</b> <b>Risk</b> <b>Minimization</b> (<b>SRM</b>) Task - GM-RKB", "url": "https://www.gabormelli.com/RKB/Structural_Risk_Minimization_(SRM)_Task", "isFamilyFriendly": true, "displayUrl": "https://www.gabormelli.com/RKB/<b>Structural</b>_<b>Risk</b>_<b>Minimization</b>_(<b>SRM</b>)_Task", "snippet": "<b>Structural</b> <b>risk</b> <b>minimization</b> (<b>SRM</b>) is an inductive principle of use in machine learning. Commonly in machine learning, a generalized model must be selected from a finite data set, with the consequent problem of overfitting \u2013 the model becoming too strongly tailored to the particularities of the training set and generalizing poorly to new data. The <b>SRM</b> principle addresses this problem by balancing the model&#39;s complexity against its success at fitting the training data. In practical terms ...", "dateLastCrawled": "2021-12-09T18:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Structural</b> <b>risk</b> <b>minimization</b> and minimum description length", "url": "https://andrewcharlesjones.github.io/journal/mdl.html", "isFamilyFriendly": true, "displayUrl": "https://andrewcharlesjones.github.io/journal/mdl.html", "snippet": "<b>Structural</b> <b>risk</b> <b>minimization</b> is a paradigm wherein we can assign priority values to various hypothesis classes under consideration, and use these assignments to guide the learning process. Specifically, again let $\\mathcal{H}$ be the overall hypothesis class, but now it\u2019s made up of a number of smaller hypothesis classes. We can write $\\mathcal{H} = \\bigcup\\limits_{n \\in \\mathbb{N}} \\mathcal{H}_n$ (it will become important later that we can write $\\mathcal{H}$ as a countable union of sub ...", "dateLastCrawled": "2022-01-02T01:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Model Selection in Machine Learning | by ANUSHKA BAJPAI | Medium", "url": "https://medium.com/@anushkhabajpai/model-selection-in-machine-learning-c568e5a42dcc", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@anushkhabajpai/model-selection-in-machine-learning-c568e5a42dcc", "snippet": "<b>Structural</b> <b>Risk</b> <b>Minimization</b> (<b>SRM</b>) Machine learning models face the inevitable problem of defining a generalized theory from a set of finite data. This leads to cases of overfitting where the ...", "dateLastCrawled": "2022-01-29T03:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Structural risk minimization of rough</b> set-based classifier | SpringerLink", "url": "https://link.springer.com/article/10.1007/s00500-019-04038-8", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00500-019-04038-8", "snippet": "The <b>structural</b> <b>risk</b> <b>minimization</b> (<b>SRM</b>) inductive principle is one of the most effective theories to control the generalization ability, which suggests a trade-off between errors in seen objects and complexity. Therefore, this paper introduces the <b>SRM</b> principle into rough set-based classifier and proposes <b>SRM</b> algorithm of rough set-based classifier called <b>SRM</b>-R algorithm. <b>SRM</b>-R algorithm uses the number of rules to characterize the actual complexity of rough set-based classifier and obtains ...", "dateLastCrawled": "2021-11-28T12:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "STAT 535 Lecture 5 November, 2021 Brief overview of Model Selection and ...", "url": "https://sites.stat.washington.edu/mmp/courses/535/fall21/Handouts/l5-model-sel-regularization.pdf", "isFamilyFriendly": true, "displayUrl": "https://sites.stat.washington.edu/.../535/fall21/Handouts/l5-model-sel-<b>regularization</b>.pdf", "snippet": "3.2 <b>Structural</b> <b>risk</b> <b>minimization</b> (<b>SRM</b>) The importance of the VC-dimension comes from theorems such as this one. Theorem 1 Let Fbe a model class of VC-dimension hand f a classi er in F. Then, with probability w.p. &gt;1 over training sets L 01(f) L^ 01(f) + r h[1 + log(2n=h)] + log(4= ) n: (6) In other words, for a small VC-dimension, the empirical ...", "dateLastCrawled": "2022-01-10T16:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Regularization</b> and Capacity Control", "url": "http://apiacoa.org/publications/teaching/machine-learning/regularization.pdf", "isFamilyFriendly": true, "displayUrl": "apiacoa.org/publications/teaching/machine-learning/<b>regularization</b>.pdf", "snippet": "<b>Structural</b> <b>Risk</b> <b>Minimization</b> Central idea Optimize a compromise between the empirical <b>risk</b> and the complexity of the class <b>SRM</b> I <b>similar</b> hypotheses as before: binary case, in\ufb01nite data set and asymptotically perfect series of classes I global capacity control: P 1 j=1 e VCdim(Gj) &lt;1 I capacity penalty: r(j;n) = q 8VCdim(Gj)log(en) n I j(g) = inf k jg 2Gk I de\ufb01ne g <b>SRM</b>;n = arg min g2 S j G j Rb l b (g;D n) + r(j(g);n) I then R l b (g <b>SRM</b>;n)!a:s: n!1 R l b 11. Links to other frameworks AIC ...", "dateLastCrawled": "2021-10-19T22:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Regularization</b> and <b>statistical learning</b> theory for data analysis ...", "url": "https://www.sciencedirect.com/science/article/pii/S016794730100069X", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S016794730100069X", "snippet": "<b>Regularization</b> theory (see Tikhonov and Arsenin, 1977; ... This observation leads to the method of <b>Structural</b> <b>Risk</b> <b>Minimization</b> (<b>SRM</b>). The idea of <b>SRM</b> is to define a nested sequence of hypothesis spaces H 1 \u2282H 2 \u2282\u22ef\u2282H M, where each hypothesis space H m has finite capacity h m and larger than that of all previous sets, that is: h 1 \u2a7dh 2,\u2026,\u2a7dh M. For example H m could be the set of polynomials of degree m, or a set of splines with m nodes, or some more complicated nonlinear ...", "dateLastCrawled": "2021-12-06T21:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Dyadic Classification Trees via <b>Structural</b> <b>Risk</b> <b>Minimization</b>", "url": "https://proceedings.neurips.cc/paper/2002/file/6832a7b24bc06775d02b7406880b93fc-Paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/2002/file/6832a7b24bc06775d02b7406880b93fc-Paper.pdf", "snippet": "<b>Structural</b> <b>risk</b> <b>minimization</b> (<b>SRM</b>) is an inductive principle for selecting a classi\ufb01er from a sequence of sets of classi\ufb01ers based on complexity <b>regularization</b>. It was introduced by Vapnik and Chervonenkis (see [2]), and later analyzed by Lugosi and Zeger [3], [4, Ch. 18]. We formulate <b>structural</b> <b>risk</b> <b>minimization</b> for dyadic classi\ufb01cation trees by applying results from [4, Ch. 18]. <b>SRM</b> is formulated in terms of the VC dimension, which we brie\ufb02y review. Let collection of classi\ufb01ers ...", "dateLastCrawled": "2021-09-19T13:00:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Structural Risk Minimization</b> - LessWrong", "url": "https://www.lesswrong.com/posts/5bd75cc58225bf0670374f81/structural-risk-minimization", "isFamilyFriendly": true, "displayUrl": "https://www.lesswrong.com/posts/5bd75cc58225bf0670374f81/<b>structural-risk-minimization</b>", "snippet": "<b>Structural risk minimization</b>, a concept from computational learning theory, is proposed as a satisficing framework. ----- The goal of this post is similar to Creating a Satisficer and Minimax as an approach to reduced-impact AI: constructing agents which are safe even with a utility function which isn&#39;t quite what is actually wanted. The approach is similar to Paul Christiano&#39;s Model-Free Decisions. The philosophy behind this solution is to treat it as an example of the optimizer&#39;s curse ...", "dateLastCrawled": "2022-01-09T05:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Statistical Learning Theory: A Primer</b>", "url": "http://cbcl.mit.edu/publications/ps/evgeniou-pontil-IJCV-2000.pdf", "isFamilyFriendly": true, "displayUrl": "cbcl.mit.edu/publications/ps/evgeniou-pontil-IJCV-2000.pdf", "snippet": "spaces, <b>SRM</b> consists of choosing the minimizer of the empirical <b>risk</b> in the space Hm\u2044for which the bound on the <b>structural</b> <b>risk</b>, as measured by the right hand side of inequality (2), is minimized. Further information about the statistical properties of <b>SRM</b> <b>can</b> be found in Devroye et al. (1996), Vapnik (1998).", "dateLastCrawled": "2022-01-03T12:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Dyadic Classification Trees via <b>Structural</b> <b>Risk</b> <b>Minimization</b>", "url": "https://proceedings.neurips.cc/paper/2002/file/6832a7b24bc06775d02b7406880b93fc-Paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/2002/file/6832a7b24bc06775d02b7406880b93fc-Paper.pdf", "snippet": "<b>Structural</b> <b>risk</b> <b>minimization</b> (<b>SRM</b>) is an inductive principle for selecting a classi\ufb01er from a sequence of sets of classi\ufb01ers based on complexity <b>regularization</b>. It was introduced by Vapnik and Chervonenkis (see [2]), and later analyzed by Lugosi and Zeger [3], [4, Ch. 18]. We formulate <b>structural</b> <b>risk</b> <b>minimization</b> for dyadic classi\ufb01cation trees by applying results from [4, Ch. 18]. <b>SRM</b> is formulated in terms of the VC dimension, which we brie\ufb02y review. Let collection of classi\ufb01ers ...", "dateLastCrawled": "2021-09-19T13:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Lecture+5+-+<b>Regularization</b>+and+Logistic+Regression.pdf - General ...", "url": "https://www.coursehero.com/file/128225620/Lecture5-RegularizationandLogisticRegressionpdf/", "isFamilyFriendly": true, "displayUrl": "https://www.coursehero.com/file/128225620/Lecture5-<b>Regularization</b>andLogisticRegressionpdf", "snippet": "<b>Risk</b> <b>Minimization</b> \u2022 Loss \u2013 How \u201cbad\u201d a model is \u2022 Empirical <b>risk</b> <b>minimization</b> \u2013 Minimizes loss \u2022 <b>Structural</b> <b>risk</b> <b>minimization</b> (<b>SRM</b>) \u2013 Minimizes loss and complexity m \u2013 \u03bb = <b>regularization</b> rate (more to follow) \u2013 An algorithm that balances two goals: \u2022 The desire to build the most predictive model (for example, lowest loss). \u2022 The desire to keep the model as simple as possible (for example, strong <b>regularization</b>).", "dateLastCrawled": "2022-02-02T07:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A <b>Stochastic Gradient Descent Algorithm for Structural Risk Minimisation</b>", "url": "https://www.researchgate.net/publication/225132871_A_Stochastic_Gradient_Descent_Algorithm_for_Structural_Risk_Minimisation", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/225132871_A_Stochastic_Gradient_Descent...", "snippet": "The paper introduces a framework for studying <b>structural</b> <b>risk</b> minimisation. The model views <b>structural</b> <b>risk</b> minimisation in a PAC context. It then considers the more general case when the hi ...", "dateLastCrawled": "2021-09-30T08:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Structural Return Maximization for Reinforcement Learning</b> | DeepAI", "url": "https://deepai.org/publication/structural-return-maximization-for-reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>structural-return-maximization-for-reinforcement-learning</b>", "snippet": "We accomplish this by using the principle of <b>Structural</b> <b>Risk</b> <b>Minimization</b>, ... where \u03a9 <b>can</b> <b>be thought</b> of as a complexity penalty on the size of L \u2218 F = {L (\u22c5, f (\u22c5)): f \u2208 F} and the bound holds with probability 1 \u2212 \u03b4. Section 2.2.1 and 2.2.2 describes a specific forms of \u03a9 using Vapnik-Chervonenkis Dimension and Rademacher complexity, which we chose due to their popularity in the literature although many additional bounds are known, e.g., maximum discrepancy (Bartlett et al ...", "dateLastCrawled": "2022-01-26T21:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Regularization Networks and Support Vector Machines</b>", "url": "http://cbcl.mit.edu/publications/ps/evgeniou-reviewall.pdf", "isFamilyFriendly": true, "displayUrl": "cbcl.mit.edu/publications/ps/evgeniou-reviewall.pdf", "snippet": "Reproducing Kernel Hilbert Space, <b>Structural</b> <b>Risk</b> <b>Minimization</b>. 1. Introduction The purposeof this paper is to present a theoretical framework for the prob- lem of learning from examples. Learning from examples <b>can</b> be regarded as the regression problem of approximating a multivariate function from sparse data \u2013 and we will take this point of view here1. The problem of approximating a func-tion from sparse data is ill-posed and a classical way to solve it is <b>regularization</b> theory [92,10,11 ...", "dateLastCrawled": "2022-02-01T14:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Recommended system algorithm related notes | Develop Paper", "url": "https://developpaper.com/recommended-system-algorithm-related-notes/", "isFamilyFriendly": true, "displayUrl": "https://developpaper.com/recommended-system-algorithm-related-notes", "snippet": "<b>Structural</b> <b>risk</b> <b>minimization</b> (<b>SRM</b>) isEmpirical <b>risk</b> (ERM) ... that is, the more complex the model is, the greater the <b>regularization</b> value is. A typical implementation of <b>structural</b> <b>risk</b> <b>minimization</b> is <b>regularization</b> image.png. Cross validation In order to improve the test error, cross validation is introduced. 1. If the number of samples is sufficient, a simple method is to randomly cut the data set into three parts: training set, verification set and test set. The training set is used for ...", "dateLastCrawled": "2022-01-30T17:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Bousquet, Boucheron, Lugosi, 2004, Introduction to statistical learning ...", "url": "https://www.coursehero.com/file/12663954/Bousquet-Boucheron-Lugosi-2004-Introduction-to-statistical-learning-theory/", "isFamilyFriendly": true, "displayUrl": "https://www.coursehero.com/file/12663954/Bousquet-Boucheron-Lugosi-2004-Introduction...", "snippet": "Compared to <b>SRM</b>, there is here a free ... Most existing (and successful) methods <b>can</b> <b>be thought</b> <b>of as regularization</b> methods. Normalized <b>Regularization</b>. There are other possible approaches when the regularizer <b>can</b>, in some sense, be \u2018normalized\u2019, i.e. when it corresponds to some probability distribution over G. Given a probability distribution \u03c0 defined on G (usually called a prior), one <b>can</b> use as a regularizer \u2212 log \u03c0 (g) 2. Reciprocally, from a regularizer of the form g 2, if ...", "dateLastCrawled": "2022-01-08T05:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Volume <b>Regularization</b> for Binary Classi\ufb01cation", "url": "https://www.mit.edu/~talw/publications/volumereg_nips2012.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.mit.edu/~talw/publications/volumereg_nips2012.pdf", "snippet": "Volume <b>Regularization</b> for Binary Classi\ufb01cation Koby Crammer Department of Electrical Enginering The Technion - Israel Institute of Technology Haifa, 32000 Israel koby@ee.technion.ac.il Tal Wagner Faculty of Mathematics and Computer Science Weizmann Institute of Science Rehovot, 76100, Israel tal.wagner@gmail.com Abstract We introduce a large-volume box classi\ufb01cation for binary prediction, which main-tains a subset of weight vectors, and speci\ufb01cally axis-aligned boxes. Our learning ...", "dateLastCrawled": "2021-09-07T07:57:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Lecture 8: Model selection and <b>structural</b> <b>risk minimization</b>", "url": "https://people.cs.umass.edu/~akshay/courses/cs690m/files/lec8.pdf", "isFamilyFriendly": true, "displayUrl": "https://people.cs.umass.edu/~akshay/courses/cs690m/files/lec8.pdf", "snippet": "Example 2 (Regularized Regression). Another common example of this is with <b>regularization</b> or with bounds on the function class we are optimizing over. Remember that the Rademacher complexity of the class of linear functions grows with the bound on the norm of the weight vectors. Hence you might want to do linear regression over norm balls of di erent sizes and then do cross validation to choose one. This is strikingly similar to the regularized ERM w^ = argminR^ n(w) + kwk2 2: Then we <b>can</b> ...", "dateLastCrawled": "2021-12-31T02:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "[PDF] <b>Structural</b> <b>Risk</b> <b>Minimization</b> | Semantic Scholar", "url": "https://www.semanticscholar.org/paper/Structural-Risk-Minimization-Sewell/f78b2d7f1f878723d67bf9b77f1f56c353cf560f", "isFamilyFriendly": true, "displayUrl": "https://www.semanticscholar.org/paper/<b>Structural</b>-<b>Risk</b>-<b>Minimization</b>-Sewell/f78b2d7f1f...", "snippet": "<b>Structural</b> <b>risk</b> <b>minimization</b> (<b>SRM</b>) (Vapnik and Chervonenkis 1974) is an inductive principle for model selection used for learning from finite training data sets. It describes a general model of capacity control and provides a tradeoff between hypothesis space complexity (the VC dimension of approximating functions) and the quality of fitting the training data (empirical error). The procedure is outlined below. 1. Using a priori knowledge of the domain, choose a class of functions, such as ...", "dateLastCrawled": "2022-01-23T13:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Structural Risk Minimization for Character Recognition</b>", "url": "https://proceedings.neurips.cc/paper/1991/file/10a7cdd970fe135cf4f7bb55c0e3b59f-Paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/1991/file/10a7cdd970fe135cf4f7bb55c0e3b59f-Paper.pdf", "snippet": "<b>Structural Risk Minimization for Character Recognition</b> I. Guyon, V. Vapnik, B. Boser, L. Bottou, and S. A. Solla AT&amp;T Bell Laboratories Holmdel, NJ 07733, USA Abstract The method of <b>Structural</b> <b>Risk</b> <b>Minimization</b> refers to tuning the capacity of the classifier to the available amount of training data. This capac\u00ad", "dateLastCrawled": "2022-02-01T13:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Structural risk minimization of rough</b> set-based classifier | SpringerLink", "url": "https://link.springer.com/article/10.1007/s00500-019-04038-8", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00500-019-04038-8", "snippet": "The <b>structural</b> <b>risk</b> <b>minimization</b> (<b>SRM</b>) inductive principle is one of the most effective theories to control the generalization ability, which suggests a trade-off between errors in seen objects and complexity. Therefore, this paper introduces the <b>SRM</b> principle into rough set-based classifier and proposes <b>SRM</b> algorithm of rough set-based classifier called <b>SRM</b>-R algorithm. <b>SRM</b>-R algorithm uses the number of rules to characterize the actual complexity of rough set-based classifier and obtains ...", "dateLastCrawled": "2021-11-28T12:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Vicinal Risk Minimization</b> - NeurIPS", "url": "https://proceedings.neurips.cc/paper/2000/file/ba9a56ce0a9bfa26e8ed9e10b2cc8f46-Paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/2000/file/ba9a56ce0a9bfa26e8ed9e10b2cc8f46-Paper.pdf", "snippet": "<b>Structural</b> <b>Risk</b> Minimisation (<b>SRM</b>) in a learning system <b>can</b> be achieved using constraints on the parameter vectors, using <b>regularization</b> terms in the cost function, or using Support Vector Machines (SVM). All these principles have lead to well established learning algo\u00ad rithms.", "dateLastCrawled": "2022-01-28T06:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A <b>Stochastic Gradient Descent Algorithm for Structural Risk Minimisation</b>", "url": "https://www.researchgate.net/publication/225132871_A_Stochastic_Gradient_Descent_Algorithm_for_Structural_Risk_Minimisation", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/225132871_A_Stochastic_Gradient_Descent...", "snippet": "The paper introduces a framework for studying <b>structural</b> <b>risk</b> minimisation. The model views <b>structural</b> <b>risk</b> minimisation in a PAC context. It then considers the more general case when the hi ...", "dateLastCrawled": "2021-09-30T08:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Machine Learning algorithms based on &quot;<b>structural</b> <b>risk</b> <b>minimization</b>&quot;?", "url": "https://cs.stackexchange.com/questions/2006/machine-learning-algorithms-based-on-structural-risk-minimization", "isFamilyFriendly": true, "displayUrl": "https://cs.stackexchange.com/questions/2006/machine-learning-algorithms-based-on...", "snippet": "$\\begingroup$ As far as I <b>can</b> tell <b>SRM</b> is nothing but good old <b>regularization</b>, which is used absolutely everywhere. $\\endgroup$ \u2013 Emre. May 23 &#39;12 at 19:52. Add a comment | 1 Answer Active Oldest Votes. 9 $\\begingroup$ The <b>structural</b> <b>risk</b> <b>minimization</b> principle is a principle that is at least partly &#39;used&#39; in all machine learning methods, since overfitting is often to be taken into account: reducing the complexity of the model is (supposedly and in practice) a good way to limit overfitting ...", "dateLastCrawled": "2022-01-19T13:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Kernel Parameter Optimization for Kriging Based</b> on <b>Structural</b> <b>Risk</b> ...", "url": "https://www.hindawi.com/journals/mpe/2017/3021950/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.hindawi.com</b>/journals/mpe/2017/3021950", "snippet": "An improved kernel parameter optimization method based on <b>Structural</b> <b>Risk</b> <b>Minimization</b> (<b>SRM</b>) principle is proposed to enhance the generalization ability of traditional Kriging surrogate model. This article first analyses the importance of the generalization ability as an assessment criteria of surrogate model from the perspective of statistics and proves the applicability to Kriging. Kernel parameter optimization method is used to improve the fitting precision of Kriging model. With the ...", "dateLastCrawled": "2022-01-28T11:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Probably Approximately Correct (<b>PAC) Learning</b>", "url": "https://nowak.ece.wisc.edu/ece901/lecture6.pdf", "isFamilyFriendly": true, "displayUrl": "https://nowak.ece.wisc.edu/ece901/lecture6.pdf", "snippet": "3.2 Complexity <b>Regularization</b> In this case, to every f \u2208 F assign a complexity C n(f) (e.g., code length) and select f\u02c6 n = argmin f\u2208F {R\u02c6 n(f)+C n(f)} Complexity <b>Regularization</b> and <b>Structural</b> <b>Risk</b> <b>Minimization</b> (<b>SRM</b>) are very similar (di\ufb00ering only in how one measures complexity) and <b>can</b> be equivalent in certain instances. The key point ...", "dateLastCrawled": "2022-01-30T11:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Prediction of Deflection of Reinforced Concrete Beams using Machine ...", "url": "https://www.ijert.org/research/prediction-of-deflection-of-reinforced-concrete-beams-using-machine-learning-tools-IJERTV4IS050936.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.ijert.org/research/prediction-of-deflection-of-reinforced-concrete-beams...", "snippet": "<b>compared</b> with those given by the developed equation. The main parameters considered in the equation are: length of beam, breadth of beam, depth of beam, compressive strength of concrete, magnitude of load, area of tension steel, characteristic strength of steel, shear span and midspan deflection. It is seen that there is reasonable agreement between the experimental results and those obtained by using SVM (discrepancy ranging from 1 to 21%). SVM isThe experimental results are also <b>compared</b> ...", "dateLastCrawled": "2022-01-17T23:37:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Frontiers | A <b>Machine Learning Approach to Prioritizing Functionally</b> ...", "url": "https://www.frontiersin.org/articles/10.3389/fpls.2021.639253/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/fpls.2021.639253", "snippet": "Support Vector <b>Machine</b> (SVM) is another approach for variable clustering based on <b>structural</b> <b>risk</b> <b>minimization</b> (<b>SRM</b>) theory (Vanitha et al., 2015). Both RF and SVM have been widely applied for decision making upon input of a large dataset. Therefore, we also utilized these two <b>machine</b> <b>learning</b> approaches to predict functionally active and inactive", "dateLastCrawled": "2022-01-31T01:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "PAC, Generalization and <b>SRM</b>", "url": "https://elearning.unipd.it/math/mod/resource/view.php?id=38143", "isFamilyFriendly": true, "displayUrl": "https://e<b>learning</b>.unipd.it/math/mod/resource/view.php?id=38143", "snippet": "Connection to <b>learning</b> Measuring the complexity of the hypotheses space (VC-Dimension) VC-Dimension of hyperplanes <b>Structural</b> <b>Risk</b> <b>Minimization</b> Exercises VC-Dimension of other hypothesis spaces, e.g. intervals in R : h(x) = +1 if a &lt;= x &lt;= b;h(x) = 1 otherwise: Fabio Aiolli PAC, Generalization and <b>SRM</b> October 6th, 202122/22", "dateLastCrawled": "2021-11-19T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>Support Vector Machines: Theory and Applications</b>", "url": "https://www.researchgate.net/publication/221621494_Support_Vector_Machines_Theory_and_Applications", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221621494_Support_Vector_<b>Machine</b>s", "snippet": "hypothesis spaces is known as <b>Structural</b> <b>Risk</b> <b>Minimization</b> (<b>SRM</b>) (Vapnik, 1998). An important question that arises in SLT is that of meas uring the &quot;complexity&quot; of a hypothesis space - which, as ...", "dateLastCrawled": "2022-02-02T02:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>GitHub</b> - gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "Loss function 3.1 Categories 3.2 Empirical <b>risk</b> <b>minimization</b> (ERM) and <b>Structural</b> <b>risk</b> <b>minimization</b> (<b>SRM</b>) 3.2.1 MLE 3.2.2 MAP 4. Evaluation metrics 4.1 Accuracy, precision, recall and F1 4.2 ROC, AUC 4.3 BLEU 5. Sample projects 6. Classical questions 6.1 Why is logistic regression a generalized linear model ?", "dateLastCrawled": "2021-09-12T01:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and-sample-codes ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/README.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "Loss function 3.1 Categories 3.2 Empirical <b>risk</b> <b>minimization</b> (ERM) and <b>Structural</b> <b>risk</b> <b>minimization</b> (<b>SRM</b>) 3.2.1 MLE 3.2.2 MAP 4. Evaluation metrics 4.1 Accuracy, precision, recall and F1 4.2 ROC, AUC 4.3 BLEU 5. Sample projects 6. Classical questions 6.1 Why is logistic regression a generalized linear model ?", "dateLastCrawled": "2021-09-22T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A Tutorial On Support Vector Machines For Pattern Recognition Pdf ...", "url": "https://elizabethsid.org/for-pdf/11704-a-tutorial-on-support-vector-machines-for-pattern-recognition-pdf-565-290.php", "isFamilyFriendly": true, "displayUrl": "https://elizabethsid.org/for-pdf/11704-a-tutorial-on-support-vector-<b>machines</b>-for...", "snippet": "The paper starts with an overview of <b>structural</b> <b>risk</b> <b>minimization</b> <b>SRM</b> principle, and describes the mechanism of how to construct SVM. For a two-class pattern recognition problem, we discuss in detail the classification mechanism of SVM in three cases of linearly separable, linearly nonseparable and nonlinear. Finally, for nonlinear case, we give a new function mapping technique: By choosing an appropriate kernel function, the SVM can map the low-dimensional input space into the high ...", "dateLastCrawled": "2022-01-19T18:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Comparison of Text Classification Algorithms</b> \u2013 IJERT", "url": "https://www.ijert.org/comparison-of-text-classification-algorithms", "isFamilyFriendly": true, "displayUrl": "https://www.ijert.org/<b>comparison-of-text-classification-algorithms</b>", "snippet": "SVMs are efficient binary classifiers that is based on <b>structural</b> <b>risk</b> <b>minimization</b>, meaning that it describes a general model of capacity control [11] and provides a trade- off between hypothesis space complexity (the VC dimension of approximating functions) and the quality of fitting the training data (empirical error). They are <b>learning</b> machines which are based on statistical <b>learning</b> theory. Any SVM would try to maximize the boundar between the positive and negative examples in a dataset ...", "dateLastCrawled": "2022-01-29T17:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "04_PAC.pdf - PAC Generalization and <b>SRM</b> <b>Machine</b> <b>Learning</b> A.Y 2021\\/22 ...", "url": "https://www.coursehero.com/file/123933166/04-PACpdf/", "isFamilyFriendly": true, "displayUrl": "https://www.coursehero.com/file/123933166/04-PACpdf", "snippet": "View 04_PAC.pdf from MATH 224 at University of Padua. PAC, Generalization and <b>SRM</b> <b>Machine</b> <b>Learning</b>, A.Y. 2021/22, Padova Fabio Aiolli October 6th, 2021 Fabio Aiolli PAC, Generalization and", "dateLastCrawled": "2022-01-05T18:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "VC-<b>dimension and structural risk minimization for</b> the analysis of ...", "url": "https://www.sciencedirect.com/science/article/pii/S0096300305007824", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0096300305007824", "snippet": "Meir has outlined an approach that extends Vapnik\u2019s method of <b>Structural</b> <b>Risk</b> <b>Minimization</b> to time series generated by an underlying mixing stochastic process. However, this approach requires the knowledge of the mixing rate of the process, which is not at all easy to estimate. Therefore, we will straightforwardly use the standard <b>SRM</b> for nonlinear regressors and our heuristic approach will be justified only a posteriori by the results for the problem of model choice. If density ...", "dateLastCrawled": "2021-10-24T14:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Automatically Detecting Excavator Anomalies Based</b> on <b>Machine</b> <b>Learning</b>", "url": "https://www.readkong.com/page/automatically-detecting-excavator-anomalies-based-on-9697342", "isFamilyFriendly": true, "displayUrl": "https://www.readkong.com/page/<b>automatically-detecting-excavator-anomalies-based</b>-on-9697342", "snippet": "Support vector <b>machine</b> is based on the principle of <b>Structural</b> <b>Risk</b> <b>Minimization</b> (<b>SRM</b>) in statistical <b>learning</b> theory, and has good generalization performance [26]. Minimizing <b>structural</b> <b>risk</b> means maximizing profits between different categories. Thus, SVM is not only a useful statistical theory, but also a way to deal with engineering problems [27]. The idea of SVM is to divide training samples into two classes using a linearly separated hyperplane. Symmetry 2019, 11, 957 11 of 18 In this ...", "dateLastCrawled": "2022-01-16T15:53:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(structural risk minimization (srm))  is like +(regularization)", "+(structural risk minimization (srm)) is similar to +(regularization)", "+(structural risk minimization (srm)) can be thought of as +(regularization)", "+(structural risk minimization (srm)) can be compared to +(regularization)", "machine learning +(structural risk minimization (srm) AND analogy)", "machine learning +(\"structural risk minimization (srm) is like\")", "machine learning +(\"structural risk minimization (srm) is similar\")", "machine learning +(\"just as structural risk minimization (srm)\")", "machine learning +(\"structural risk minimization (srm) can be thought of as\")", "machine learning +(\"structural risk minimization (srm) can be compared to\")"]}