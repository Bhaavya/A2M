{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Least Squares Regression</b> - <b>mathsisfun.com</b>", "url": "https://www.mathsisfun.com/data/least-squares-regression.html", "isFamilyFriendly": true, "displayUrl": "https://<b>www.mathsisfun.com</b>/data/<b>least-squares-regression</b>.html", "snippet": "<b>Least Squares Regression</b> <b>Line</b> of Best Fit. Imagine you have some points, and want to have a <b>line</b> that best fits them <b>like</b> this: We can place the <b>line</b> &quot;by eye&quot;: try to have the <b>line</b> as close as possible to all points, and a similar number of points above and below the <b>line</b>. But for better accuracy let&#39;s see how to calculate the <b>line</b> using <b>Least Squares Regression</b>. The <b>Line</b>. Our aim is to calculate the values m (slope) and b (y-intercept) in <b>the equation</b> of a <b>line</b>: y = mx + b. Where: y = how ...", "dateLastCrawled": "2022-02-03T04:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>The Least Squares Regression Method</b> \u2013 How to Find the <b>Line</b> of Best Fit", "url": "https://www.freecodecamp.org/news/the-least-squares-regression-method-explained/", "isFamilyFriendly": true, "displayUrl": "https://www.freecodecamp.org/news/<b>the-least-squares-regression-method</b>-explained", "snippet": "Which is a graph that looks something <b>like</b> this: We now have a <b>line</b> that represents how many topics we expect to be solved for each hour of study. If we want to predict how many topics we expect a student to solve with 8 hours of study, we replace it in our formula: Y = -1.85 + 2.8*8; Y = 20.55 ; An in a graph we can see: The further it is in the future the <b>least</b> accuracy we should expect Limitations. Always bear in mind the limitations of a method. This will hopefully help you avoid ...", "dateLastCrawled": "2022-02-03T15:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Calculating a <b>Least</b> <b>Squares</b> <b>Regression</b> <b>Line</b>: <b>Equation</b>, Example ...", "url": "https://technologynetworks.com/informatics/articles/calculating-a-least-squares-regression-line-equation-example-explanation-310265", "isFamilyFriendly": true, "displayUrl": "https://technologynetworks.com/informatics/articles/calculating-a-<b>least</b>-<b>squares</b>...", "snippet": "Now we have all the information needed for our <b>equation</b> and are free to slot in values as we see fit. If we wanted to know the predicted grade of someone who spends 2.35 hours on their essay, all we need to do is swap that in for X. y=30.18 + 6.49 * X. y = 30.18 + (6.49 * 2.35) y = 45.43. Drawing a <b>least</b> <b>squares</b> <b>regression</b> <b>line</b> by hand", "dateLastCrawled": "2022-01-28T04:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Least Squares Regression</b> - How to Create <b>Line</b> of Best Fit?", "url": "https://www.wallstreetmojo.com/least-squares-regression/", "isFamilyFriendly": true, "displayUrl": "https://www.wallstreetmojo.com/<b>least-squares-regression</b>", "snippet": "<b>Line</b> of Best Fit in the <b>Least</b> Square <b>Regression</b>. The <b>line</b> of best fit is a <b>straight</b> <b>line</b> drawn through a scatter of data points that best represents the relationship between them. Let us consider the following graph wherein a set of data is plotted along the x and y-axis. These data points are represented using the blue dots. Three lines are drawn through these points \u2013 a green, a red, and a blue <b>line</b>. The green <b>line</b> passes through a single point, and the red <b>line</b> passes through three data ...", "dateLastCrawled": "2022-02-03T06:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Least Square Regression Line - GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/least-square-regression-line/", "isFamilyFriendly": true, "displayUrl": "https://www.geeksforgeeks.org/<b>least</b>-square-<b>regression</b>-<b>line</b>", "snippet": "<b>Like</b> Article. <b>Least</b> Square <b>Regression</b> <b>Line</b>. Difficulty Level : Medium; Last Updated : 07 Jul, 2021. Given a set of coordinates in the form of (X, Y), the task is to find the <b>least</b> <b>regression</b> <b>line</b> that can be formed. In statistics, Linear <b>Regression</b> is a linear approach to model the relationship between a scalar response (or dependent variable), say Y, and one or more explanatory variables (or independent variables), say X. <b>Regression</b> <b>Line</b>: If our data shows a linear relationship between X ...", "dateLastCrawled": "2022-01-30T18:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Linear Regression</b>-<b>Equation</b>, Formula and Properties", "url": "https://byjus.com/maths/linear-regression/", "isFamilyFriendly": true, "displayUrl": "https://byjus.com/maths/<b>linear-regression</b>", "snippet": "<b>Linear regression</b> determines the <b>straight</b> <b>line</b>, called the <b>least</b>-<b>squares</b> <b>regression</b> <b>line</b> or LSRL, that best expresses observations in a bivariate analysis of data set. Suppose Y is a dependent variable, and X is an independent variable, then the population <b>regression</b> <b>line</b> is given by; Y = B 0 +B 1 X. Where. B 0 is a constant. B 1 is the <b>regression</b> coefficient. If a random sample of observations is given, then the <b>regression</b> <b>line</b> is expressed by; \u0177 = b 0 + b 1 x. where b 0 is a constant, b 1 ...", "dateLastCrawled": "2022-02-02T23:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>What Is the Least Squares</b> <b>Regression</b> <b>Line</b>?", "url": "https://www.thoughtco.com/what-is-a-least-squares-line-3126250", "isFamilyFriendly": true, "displayUrl": "https://www.thoughtco.com/what-is-a-<b>least-squares-line</b>-3126250", "snippet": "Since the <b>least squares line</b> minimizes the squared distances between the <b>line</b> and our points, we can think of this <b>line</b> as the one that best fits our data. This is why the <b>least squares line</b> is also known as the <b>line</b> of best fit. Of all of the possible lines that could be drawn, the <b>least squares line</b> is closest to the set of data as a whole. This may mean that our <b>line</b> will miss hitting any of the points in our set of data.", "dateLastCrawled": "2022-02-03T05:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Calculating the equation of the least-squares</b> <b>line</b> (practice) | Khan ...", "url": "https://www.khanacademy.org/math/ap-statistics/bivariate-data-ap/least-squares-regression/e/calculating-equation-least-squares", "isFamilyFriendly": true, "displayUrl": "https://<b>www.khanacademy.org</b>/.../e/calculating-<b>equation</b>-<b>least</b>-<b>squares</b>", "snippet": "Calculating <b>the equation</b> of a <b>regression</b> <b>line</b>. Practice: <b>Calculating the equation of the least-squares</b> <b>line</b>. This is the currently selected item. Interpreting slope of <b>regression</b> <b>line</b>. Interpreting y-intercept in <b>regression</b> model. Practice: Interpreting slope and y-intercept for linear models. Using <b>least</b> <b>squares</b> <b>regression</b> output.", "dateLastCrawled": "2022-02-02T13:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Least Squares Calculator</b>", "url": "https://www.mathsisfun.com/data/least-squares-calculator.html", "isFamilyFriendly": true, "displayUrl": "https://<b>www.mathsisfun.com</b>/data/<b>least-squares-calculator</b>.html", "snippet": "<b>Least Squares Calculator</b>. <b>Least</b> <b>Squares</b> <b>Regression</b> is a way of <b>finding</b> a <b>straight</b> <b>line</b> that best fits the data, called the &quot;<b>Line</b> of Best Fit&quot;.. Enter your data as (x, y) pairs, and find <b>the equation</b> of a <b>line</b> that best fits the data.", "dateLastCrawled": "2022-02-02T08:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Business Stats Midterm #3</b> Flashcards | Quizlet", "url": "https://quizlet.com/391010086/business-stats-midterm-3-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/391010086/<b>business-stats-midterm</b>-3-flash-cards", "snippet": "A procedure used for <b>finding</b> <b>the equation</b> <b>of a straight</b> <b>line</b> that provides the best approximation for the relationship between the independent and dependent variables is _____. the <b>least</b> <b>squares</b> method . Application of the <b>least</b> <b>squares</b> method results in values of the y-intercept and the slope that minimizes the sum of the squared deviations between the _____. observed values of the dependent variable and the predicted values of the dependent variable. A <b>least</b> <b>squares</b> <b>regression</b> <b>line</b> ...", "dateLastCrawled": "2021-04-15T10:12:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Least Square Method</b> - Definition, Graph and Formula", "url": "https://byjus.com/maths/least-square-method/", "isFamilyFriendly": true, "displayUrl": "https://byjus.com/maths/<b>least-square-method</b>", "snippet": "The <b>least</b>-<b>squares</b> method is a crucial statistical method that is practised to find a <b>regression</b> <b>line</b> or a best-fit <b>line</b> for the given pattern. This method is described by an <b>equation</b> with specific parameters. The method of <b>least</b> <b>squares</b> is generously used in evaluation and <b>regression</b>. In <b>regression</b> analysis, this method is said to be a standard approach for the approximation of sets of equations having more equations than the number of unknowns.", "dateLastCrawled": "2022-02-02T23:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Least Squares Regression</b> - <b>mathsisfun.com</b>", "url": "https://www.mathsisfun.com/data/least-squares-regression.html", "isFamilyFriendly": true, "displayUrl": "https://<b>www.mathsisfun.com</b>/data/<b>least-squares-regression</b>.html", "snippet": "<b>Least Squares Regression</b> <b>Line</b> of Best Fit. Imagine you have some points, and want to have a <b>line</b> that best fits them like this: We can place the <b>line</b> &quot;by eye&quot;: try to have the <b>line</b> as close as possible to all points, and a <b>similar</b> number of points above and below the <b>line</b>. But for better accuracy let&#39;s see how to calculate the <b>line</b> using <b>Least Squares Regression</b>. The <b>Line</b>. Our aim is to calculate the values m (slope) and b (y-intercept) in <b>the equation</b> of a <b>line</b>: y = mx + b. Where: y = how ...", "dateLastCrawled": "2022-02-03T04:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>The Least Squares Regression Method</b> \u2013 How to Find the <b>Line</b> of Best Fit", "url": "https://www.freecodecamp.org/news/the-least-squares-regression-method-explained/", "isFamilyFriendly": true, "displayUrl": "https://www.freecodecamp.org/news/<b>the-least-squares-regression-method</b>-explained", "snippet": "What is <b>the Least Squares Regression method</b> and why use it? <b>Least</b> <b>squares</b> is a method to apply linear <b>regression</b>. It helps us predict results based on an existing set of data as well as clear anomalies in our data. Anomalies are values that are too good, or bad, to be true or that represent rare cases. For example, say we have a list of how many topics future engineers here at freeCodeCamp can solve if they invest 1, 2, or 3 hours continuously. Then we can predict how many topics will be ...", "dateLastCrawled": "2022-02-03T15:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "2.3 <b>Least</b>-<b>Squares</b> <b>Regression</b> - digfir-published.macmillanusa.com", "url": "http://digfir-published.macmillanusa.com/psbe4e/psbe4e_ch2_8.html", "isFamilyFriendly": true, "displayUrl": "digfir-published.macmillanusa.com/psbe4e/psbe4e_ch2_8.html", "snippet": "A <b>regression</b> <b>line</b> is a <b>straight</b> <b>line</b> that describes how a response variable changes as an explanatory variable changes. ... The <b>least</b>-<b>squares</b> <b>regression</b> <b>line</b> of on is the <b>line</b> that makes the sum of the <b>squares</b> of the vertical distances of the data points from the <b>line</b> as small as possible. One reason for the popularity of the <b>least</b>-<b>squares</b> <b>regression</b> <b>line</b> is that the problem of <b>finding</b> the <b>line</b> has a simple solution. We can give the recipe for the <b>least</b>-<b>squares</b> <b>line</b> in terms of the means and ...", "dateLastCrawled": "2022-02-02T12:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Linear Regression</b>-<b>Equation</b>, Formula and Properties", "url": "https://byjus.com/maths/linear-regression/", "isFamilyFriendly": true, "displayUrl": "https://byjus.com/maths/<b>linear-regression</b>", "snippet": "<b>The equation</b> of <b>linear regression</b> <b>is similar</b> to the slope formula what we have learned before in earlier classes such as linear equations in two variables. It is given by; Y= a + bX. Now, here we need to find the value of the slope of the <b>line</b>, b, plotted in scatter plot and the intercept, a. Simple <b>Linear Regression</b>. The very most straightforward case of a single scalar predictor variable x and a single scalar response variable y is known as simple <b>linear regression</b>. <b>The equation</b> for this ...", "dateLastCrawled": "2022-02-02T23:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Linear Regression</b> - Examples, <b>Equation</b>, Formula and Properties", "url": "https://www.vedantu.com/maths/linear-regression", "isFamilyFriendly": true, "displayUrl": "https://www.vedantu.com/maths/<b>linear-regression</b>", "snippet": "The most popular method to fit a <b>regression</b> <b>line</b> in the XY plot is found by using <b>least</b>-<b>squares</b>. This process is used to determine the best-fitting <b>line</b> for the given data by reducing the sum of the <b>squares</b> of the vertical deviations from each data point to the <b>line</b>. If a point rests on the fitted <b>line</b> accurately, then the value of its perpendicular deviation is 0. It is 0 because the variations are first squared, then added, so their positive and negative values will not be cancelled.", "dateLastCrawled": "2022-02-02T05:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "QMB chapter 12 Flashcards | <b>Quizlet</b>", "url": "https://quizlet.com/175019740/qmb-chapter-12-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/175019740/qmb-chapter-12-flash-cards", "snippet": "A procedure used for <b>finding</b> <b>the equation</b> <b>of a straight</b> <b>line</b> that provides the best approximation for the relationship between the independent and dependent variables is the a. correlation analysis b. <b>least</b> <b>squares</b> method c. most <b>squares</b> method d. mean <b>squares</b> method . b. <b>Regression</b> analysis is a statistical procedure for developing a mathematical <b>equation</b> that describes how Answers: a. one independent and one or more dependent variables are related b. several independent and several ...", "dateLastCrawled": "2021-12-01T22:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "9 Types of <b>Regression</b> Analysis (in ML &amp; Data Science) | FavTutor", "url": "https://favtutor.com/blogs/types-of-regression", "isFamilyFriendly": true, "displayUrl": "https://favtutor.com/blogs/types-of-<b>regression</b>", "snippet": "The type of <b>regression</b> <b>line</b>: a best fit <b>straight</b> <b>line</b>. 2) Multiple Linear <b>Regression</b>. Simple linear <b>regression</b> allows a data scientist or data analyst to make predictions about only one variable by training the model and predicting another variable. In a <b>similar</b> way, a multiple <b>regression</b> model extends to several more than one variable. Simple linear <b>regression</b> uses the following linear function to predict the value of a target variable y, with independent variable x?. y = b 0 + b 1 x 1. To ...", "dateLastCrawled": "2022-02-03T16:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Topic 13. Analysis of Covariance (ANCOVA, 13. 1. Introduction", "url": "https://psfaculty.plantsciences.ucdavis.edu/agr205/Lectures/2011_Lectures/L13_ANCOVA.pdf", "isFamilyFriendly": true, "displayUrl": "https://psfaculty.plantsciences.ucdavis.edu/agr205/Lectures/2011_Lectures/L13_ANCOVA.pdf", "snippet": "This <b>straight</b> <b>line</b> intercepts the Y axis at the value a so a is called the intercept. The coefficient b is the slope of the <b>straight</b> <b>line</b> and represents the change in Y for each unit change in X (i.e. rise/run). Any point (X,Y) on this <b>line</b> has an X coordinate, or abscissa, and a Y coordinate, or ordinate, whose values satisfy <b>the equation</b> Y= a + bX. 13.2.1 The principle of <b>least</b> <b>squares</b> To find <b>the equation</b> of the <b>straight</b> <b>line</b> that best fits a dataset consisting of (X,Y) pairs, we use a ...", "dateLastCrawled": "2022-02-02T02:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Chapter 4: Regression</b> Flashcards | Quizlet", "url": "https://quizlet.com/121866352/chapter-4-regression-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/121866352/<b>chapter-4-regression</b>-flash-cards", "snippet": "The <b>least</b> <b>squares</b> method is one way of <b>finding</b> <b>the equation</b> of a <b>regression</b> <b>line</b>. It minimises the sum of the <b>squares</b> of the residuals. It works best when there are no outliers. <b>The equation</b> of the <b>least</b> <b>squares</b> <b>regression</b> <b>line</b> is given by y = a+ bx, where a represents the y-intercept of the <b>line</b> and b the slope. Slope <b>equation</b> for LSRL. The intercept of the <b>regression</b> <b>line</b> is just the predicted value for y, when x is 0. Any <b>line</b> has an <b>equation</b>, in terms of its slope and intercept: y ...", "dateLastCrawled": "2021-04-15T10:56:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Least-squares regression line</b> - WebStat", "url": "https://webstat.une.edu.au/unit_materials/c4_descriptive_statistics/least_squares_regress.html", "isFamilyFriendly": true, "displayUrl": "https://webstat.une.edu.au/unit_materials/c4_descriptive_statistics/<b>least</b>_<b>squares</b>...", "snippet": "The value of b in the <b>regression</b> <b>equation</b> (also called the <b>regression</b> weight) is the &quot;slope&quot; of the <b>regression</b> <b>line</b>, when moving from the lower left to the upper right. The slope of the <b>line</b> is defined as the amount by which Y is predicted to increase with each one unit increase in X. So the <b>regression</b> weight for the logical reasoning score in this <b>regression</b> formula, b, <b>can</b> <b>be thought</b> of as the predicted difference in the creativity score associated with a one unit increase in X (logical ...", "dateLastCrawled": "2022-01-28T07:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>What Is the Least Squares</b> <b>Regression</b> <b>Line</b>?", "url": "https://www.thoughtco.com/what-is-a-least-squares-line-3126250", "isFamilyFriendly": true, "displayUrl": "https://www.<b>thought</b>co.com/what-is-a-<b>least-squares-line</b>-3126250", "snippet": "Since the <b>least squares line</b> minimizes the squared distances between the <b>line</b> and our points, we <b>can</b> think of this <b>line</b> as the one that best fits our data. This is why the <b>least squares line</b> is also known as the <b>line</b> of best fit. Of all of the possible lines that could be drawn, the <b>least squares line</b> is closest to the set of data as a whole. This may mean that our <b>line</b> will miss hitting any of the points in our set of data.", "dateLastCrawled": "2022-02-03T05:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "The <b>Least Squares Regression Line</b> - GitHub Pages", "url": "https://saylordotorg.github.io/text_introductory-statistics/s14-04-the-least-squares-regression-l.html", "isFamilyFriendly": true, "displayUrl": "https://saylordotorg.github.io/.../s14-04-the-<b>least-squares-regression</b>-l.html", "snippet": "The <b>Least Squares Regression Line</b>. Given any collection of pairs of numbers (except when all the x-values are the same) and the corresponding scatter diagram, there always exists exactly one <b>straight</b> <b>line</b> that fits the data better than any other, in the sense of minimizing the sum of the squared errors.It is called the <b>least squares regression line</b>.Moreover there are formulas for its slope and y-intercept.", "dateLastCrawled": "2022-02-02T02:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Statistics review 7: Correlation and <b>regression</b>", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC374386/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC374386", "snippet": "<b>The equation</b> <b>of a straight</b> <b>line</b> is given by y = a + bx, where the coefficients a and b are the intercept of the <b>line</b> on the y axis and the gradient, respectively. <b>The equation</b> of the <b>regression</b> <b>line</b> for the A&amp;E data (Fig. (Fig.7) 7) is as follows: ln urea = 0.72 + (0.017 \u00d7 age) (calculated using the method of <b>least</b> <b>squares</b>, which is described below). The gradient of this <b>line</b> is 0.017, which indicates that for an increase of 1 year in age the expected increase in ln urea is 0.017 units (and ...", "dateLastCrawled": "2022-02-02T06:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Least Squares</b> Linear <b>Regression</b> In Python | by Cory Maklin | Towards ...", "url": "https://towardsdatascience.com/least-squares-linear-regression-in-python-54b87fc49e77", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>least-squares</b>-<b>line</b>ar-<b>regression</b>-in-python-54b87fc49e77", "snippet": "<b>Least Squares</b> Linear <b>Regression</b> In Python. Cory Maklin. Aug 16, 2019 \u00b7 6 min read. As the name implies, the method of <b>Least Squares</b> minimizes the sum of the <b>squares</b> of the residuals between the observed targets in the dataset, and the targets predicted by the linear approximation. In this proceeding article, we\u2019ll see how we <b>can</b> go about <b>finding</b> the best fitting <b>line</b> using linear algebra as opposed to something like gradient descent. Algorithm. Contrary to what I had initially <b>thought</b> ...", "dateLastCrawled": "2022-02-02T08:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Least</b> <b>Squares</b> <b>Regression</b> <b>Line</b> and How to Calculate it from your Data ...", "url": "https://blog.udemy.com/least-squares-regression-line/", "isFamilyFriendly": true, "displayUrl": "https://blog.udemy.com/<b>least</b>-<b>squares</b>-<b>regression</b>-<b>line</b>", "snippet": "<b>Least</b> <b>squares</b> <b>regression</b> <b>line</b> is used to calculate the best fit <b>line</b> in such a way to minimize the difference in the <b>squares</b> of any data on a given <b>line</b>. This means the further away from the <b>line</b> the data point is, the more pull it has on the <b>line</b>. Also, this means that if a data point is exactly on the best fit <b>line</b>, it has an effective deviation of 0. The values are squared, so no negative values <b>can</b> cancel out the positive values, making the <b>least</b> <b>squares</b> <b>regression</b> <b>line</b> more accurate.", "dateLastCrawled": "2022-02-03T03:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "How To <b>Run Linear Regressions In Python Scikit-learn</b> - ActiveState", "url": "https://www.activestate.com/resources/quick-reads/how-to-run-linear-regressions-in-python-scikit-learn/", "isFamilyFriendly": true, "displayUrl": "https://www.activestate.com/resources/quick-reads/how-to-run-<b>line</b>ar-<b>regressions</b>-in...", "snippet": "Linear <b>regression</b> <b>can</b> <b>be thought</b> <b>of as finding</b> the <b>straight</b> <b>line</b> that best fits a set of scattered data points: ... Intercept \u2013 the location where the Slope intercepts the Y-axis denoted b in the slope <b>equation</b> y=ax+b. <b>Least</b> <b>Squares</b> \u2013 a method of estimating a Best Fit to data, by minimizing the sum of the <b>squares</b> of the differences between observed and estimated values. Mean \u2013 an av erage of a set of numbers, but in linear <b>regression</b>, Mean is modeled by a linear function. Ordinary ...", "dateLastCrawled": "2022-01-27T09:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Biostatistics Series Module 6: Correlation and Linear <b>Regression</b>", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5122272/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC5122272", "snippet": "This <b>straight</b> <b>line</b>, or <b>regression</b> <b>line</b>, is actually the \u201c<b>line</b> of best fit\u201d for the data points on the scatter plot showing the relationship between the variables in question. The <b>regression</b> <b>line</b> has the general formula: y = a + bx. Where \u201ca\u201d and \u201cb\u201d are two constants denoting the intercept of the <b>line</b> on the Y-axis (y-intercept) and the gradient (slope) of the <b>line</b>, respectively. The other name for b is the \u201c<b>regression</b> coefficient.\u201d Physically, \u201cb\u201d represents the change ...", "dateLastCrawled": "2022-02-02T04:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "statistics - How <b>can</b> I determine the accuracy of a hand-drawn <b>line</b> of ...", "url": "https://datascience.stackexchange.com/questions/107379/how-can-i-determine-the-accuracy-of-a-hand-drawn-line-of-best-fit", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/107379/how-<b>can</b>-i-determine-the...", "snippet": "I have used <b>least</b>-<b>squares</b> <b>regression</b> to determine the optimal <b>line</b> of best fit for the same data. How <b>can</b> I assess the quality of the user-drawn LOBF? My first <b>thought</b> was just to work out the uncertainty between the two gradients and the two y-intercepts, but that produces dramatic errors when the true value of either the gradient or the y-intercept is close to zero.", "dateLastCrawled": "2022-01-24T05:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Chapter 3 Correlation and <b>regression</b> | Montana State Introductory ...", "url": "https://mtstateintrostats.github.io/IntroStatTextbook/cor-reg.html", "isFamilyFriendly": true, "displayUrl": "https://mtstateintrostats.github.io/IntroStatTextbook/cor-reg.html", "snippet": "3.1.1 Fitting a <b>line</b> to data. Figure 3.1 shows two variables whose relationship <b>can</b> be modeled perfectly with a <b>straight</b> <b>line</b>. <b>The equation</b> for the <b>line</b> is \\(y = 5 + 64.96 x\\).Consider what a perfect linear relationship means: we know the exact value of \\(y\\) just by knowing the value of \\(x\\).This is unrealistic in almost any natural process.", "dateLastCrawled": "2022-01-31T04:04:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Least Square Method</b> - Definition, Graph and Formula", "url": "https://byjus.com/maths/least-square-method/", "isFamilyFriendly": true, "displayUrl": "https://byjus.com/maths/<b>least-square-method</b>", "snippet": "The <b>least square method</b> is the process of <b>finding</b> the best-fitting curve or <b>line</b> of best fit for a set of data points by reducing the sum of the <b>squares</b> of the offsets (residual part) of the points from the curve. During the process of <b>finding</b> the relation between two variables, the trend of outcomes are estimated quantitatively. This process is termed as <b>regression</b> analysis.The method of curve fitting is an approach to <b>regression</b> analysis.", "dateLastCrawled": "2022-02-02T23:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Statistics review 7: Correlation and <b>regression</b>", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC374386/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC374386", "snippet": "The value of r <b>can</b> <b>be compared</b> with those given in Table ... <b>The equation</b> <b>of a straight</b> <b>line</b> is given by y = a + bx, where the coefficients a and b are the intercept of the <b>line</b> on the y axis and the gradient, respectively. <b>The equation</b> of the <b>regression</b> <b>line</b> for the A&amp;E data (Fig. (Fig.7) 7) is as follows: ln urea = 0.72 + (0.017 \u00d7 age) (calculated using the method of <b>least</b> <b>squares</b>, which is described below). The gradient of this <b>line</b> is 0.017, which indicates that for an increase of 1 ...", "dateLastCrawled": "2022-02-02T06:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Least Squares Regression</b> - How to Create <b>Line</b> of Best Fit?", "url": "https://www.wallstreetmojo.com/least-squares-regression/", "isFamilyFriendly": true, "displayUrl": "https://www.wallstreetmojo.com/<b>least-squares-regression</b>", "snippet": "<b>Line</b> of Best Fit in the <b>Least</b> Square <b>Regression</b>. The <b>line</b> of best fit is a <b>straight</b> <b>line</b> drawn through a scatter of data points that best represents the relationship between them. Let us consider the following graph wherein a set of data is plotted along the x and y-axis. These data points are represented using the blue dots. Three lines are drawn through these points \u2013 a green, a red, and a blue <b>line</b>. The green <b>line</b> passes through a single point, and the red <b>line</b> passes through three data ...", "dateLastCrawled": "2022-02-03T06:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "AP Statistics: Scatterplots, Correlation, and <b>Least</b>-<b>Squares</b> <b>Regression</b>", "url": "https://procrastinote.org/wp-content/uploads/2020/12/scatterplots__correlation__and_least_squares_regression.pdf", "isFamilyFriendly": true, "displayUrl": "https://procrastinote.org/.../scatterplots__correlation__and_<b>least</b>_<b>squares</b>_<b>regression</b>.pdf", "snippet": "AP Statistics Scatterplots, Correlations, and <b>Least</b>-<b>Squares</b> <b>Regression</b> 2.2.1 <b>Equation</b> 4: ^y = a+ bx This <b>equation</b> is pretty much just slope intercept form. The best \ufb01t <b>line</b> is always that\u2014a <b>line</b>. Therefore, you use slope intercept form to \ufb01nd <b>the equation</b> of the <b>line</b>. 2.2.2 <b>Equation</b> 5: b = rS y S x This <b>equation</b> solves for b which is the ...", "dateLastCrawled": "2021-11-11T00:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Chapter 3 Multiple Linear Regression Model</b> The linear model", "url": "http://home.iitk.ac.in/~shalab/regression/Chapter3-Regression-MultipleLinearRegressionModel.pdf", "isFamilyFriendly": true, "displayUrl": "home.iitk.ac.in/~shalab/<b>regression</b>/Chapter3-<b>Regression</b>-<b>MultipleLinearRegressionModel</b>.pdf", "snippet": "So a simple linear <b>regression</b> model <b>can</b> be expressed as income education ... Principle of ordinary <b>least</b> <b>squares</b> (OLS) Let B be the set of all possible vectors . If there is no further information, the B is k-dimensional real Euclidean space. The object is to find a vector bbb b&#39; ( , ,..., ) 12 k from B that minimizes the sum of squared deviations of &#39; , i s i.e., 2 1 &#39; ( )&#39;( ) n i i S y X y X for given y and X. A minimum will always exist as S() is a real-valued, convex and differentiable ...", "dateLastCrawled": "2022-02-02T07:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Linear Regression</b> - Examples, <b>Equation</b>, Formula and Properties", "url": "https://www.vedantu.com/maths/linear-regression", "isFamilyFriendly": true, "displayUrl": "https://www.vedantu.com/maths/<b>linear-regression</b>", "snippet": "The most popular method to fit a <b>regression</b> <b>line</b> in the XY plot is found by using <b>least</b>-<b>squares</b>. This process is used to determine the best-fitting <b>line</b> for the given data by reducing the sum of the <b>squares</b> of the vertical deviations from each data point to the <b>line</b>. If a point rests on the fitted <b>line</b> accurately, then the value of its perpendicular deviation is 0. It is 0 because the variations are first squared, then added, so their positive and negative values will not be cancelled.", "dateLastCrawled": "2022-02-02T05:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Scatterplots and regression lines</b> \u2014 Krista King Math | Online math tutor", "url": "https://www.kristakingmath.com/blog/scatterplots-regression-lines", "isFamilyFriendly": true, "displayUrl": "https://www.kristakingmath.com/blog/scatterplots-<b>regression</b>-<b>lines</b>", "snippet": "A <b>regression</b> <b>line</b> is also called the best-fit <b>line</b>, <b>line</b> of best fit, or <b>least</b>-<b>squares</b> <b>line</b>. The <b>regression</b> <b>line</b> is a trend <b>line</b> we use to model a linear trend that we see in a scatterplot, but realize that some data will show a relationship that isn\u2019t necessarily linear. For example, the relationship might follow the curve of a parabola, in which case the <b>regression</b> curve would be parabolic in nature. For the rest of this lesson we\u2019ll focus mostly on linear <b>regression</b>. <b>Equation</b> of the ...", "dateLastCrawled": "2022-02-03T06:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Linear <b>Regression</b> - MATLAB &amp; Simulink", "url": "https://www.mathworks.com/help/matlab/data_analysis/linear-regression.html", "isFamilyFriendly": true, "displayUrl": "https://www.mathworks.com/help/matlab/data_analysis/<b>line</b>ar-<b>regression</b>.html", "snippet": "Linear <b>Regression</b> Introduction. A data model explicitly describes a relationship between predictor and response variables. Linear <b>regression</b> fits a data model that is linear in the model coefficients. The most common type of linear <b>regression</b> is a <b>least</b>-<b>squares</b> fit, which <b>can</b> fit both lines and polynomials, among other linear models.. Before you model the relationship between pairs of quantities, it is a good idea to perform correlation analysis to establish if a linear relationship exists ...", "dateLastCrawled": "2022-02-02T06:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Least</b> <b>squares</b> is a method of fitting a <b>regression</b> <b>line</b> which is robust ...", "url": "https://www.quora.com/Least-squares-is-a-method-of-fitting-a-regression-line-which-is-robust-i-e-safe-from-outliers-True-or-False", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>Least</b>-<b>squares</b>-is-a-method-of-fitting-a-<b>regression</b>-<b>line</b>-which-is...", "snippet": "Answer (1 of 2): This is false. So it is \u201c<b>least</b> <b>squares</b>\u201d - the square of the residual is what you are looking to minimise. Consider your point with the highest residuals and move it some small amount. Consider how your <b>line</b> of best fit will move. Consider for a given small change how much your g...", "dateLastCrawled": "2022-01-08T16:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Linear Regression Formula</b> \u2013 Definition, Formula Plotting, Properties ...", "url": "https://www.vedantu.com/formula/linear-regression-formula", "isFamilyFriendly": true, "displayUrl": "https://www.vedantu.com/formula/<b>linear-regression-formula</b>", "snippet": "The concept of linear <b>regression</b> consists of <b>finding</b> the best-fitting <b>straight</b> <b>line</b> through the given points. The best-fitting <b>line</b> is known as a <b>regression</b> <b>line</b>. The black diagonal <b>line</b> in the figure given below (Figure 2) is the <b>regression</b> <b>line</b> and consists of the predicted score on Y for each possible value of the variable X. The lines in the figure given above, the vertical lines from the points to the <b>regression</b> <b>line</b>, represent the errors of prediction. As you <b>can</b> see, the red point is ...", "dateLastCrawled": "2022-02-02T17:27:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "CS 189/289A: Introduction to <b>Machine</b> <b>Learning</b>", "url": "https://people.eecs.berkeley.edu/~jrs/189s21/", "isFamilyFriendly": true, "displayUrl": "https://people.eecs.berkeley.edu/~jrs/189s21", "snippet": "LDA vs. logistic <b>regression</b>: advantages and disadvantages. ROC curves. Weighted <b>least</b>-<b>squares</b> <b>regression</b>. <b>Least</b>-<b>squares</b> polynomial <b>regression</b>. Read ISL, Sections 4.4.3, 7.1, 9.3.3; ESL, Section 4.4.1. Optional: here is a fine short discussion of ROC curves\u2014but skip the incoherent question at the top and jump straight to the answer.", "dateLastCrawled": "2022-01-31T03:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Neurath&#39;s Speedboat</b>: <b>Least squares as springs</b>", "url": "https://joshualoftus.com/posts/2020-11-23-least-squares-as-springs/", "isFamilyFriendly": true, "displayUrl": "https://joshualoftus.com/posts/2020-11-23-<b>least-squares-as-springs</b>", "snippet": "(This is also called total <b>least</b> <b>squares</b> or a special case of Deming <b>regression</b>.) Model complexity/elasticity: <b>machine</b> <b>learning</b> or AI. We can keep building on this <b>analogy</b> by using it to understand more complex modeling methods with another very simple idea: elasticity of the model object itself. Instead of a rigid body like a line (or ...", "dateLastCrawled": "2022-02-03T04:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "A difficult <b>regression</b> parameter estimation problem is posed when the data sample is hypothesized to have been generated by more than a single <b>regression</b> model. To find the best-fitting number and ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Machine Learning by Analogy</b> - SlideShare", "url": "https://www.slideshare.net/ColleenFarrelly/machine-learning-by-analogy-59094152", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ColleenFarrelly/<b>machine-learning-by-analogy</b>-59094152", "snippet": "<b>Machine Learning by Analogy</b> 1. Colleen M. Farrelly 2. Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics behind these ...", "dateLastCrawled": "2022-01-31T07:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A Big Problem with Linear <b>Regression</b> and How to Solve It | Towards Data ...", "url": "https://towardsdatascience.com/robust-regression-23b633e5d6a5", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/robust-<b>regression</b>-23b633e5d6a5", "snippet": "Introduction to Robust <b>Regression</b> in <b>Machine</b> <b>Learning</b>. Hussein Abdulrahman . Just now \u00b7 7 min read. The idea behind classic linear <b>regression</b> is simple: draw a \u201cbest-fit\u201d line across the data points that minimizes the mean squared errors: Classic linear <b>regression</b> with ordinary <b>least</b> <b>squares</b>. (Image by author) Looks good. But we don\u2019t always get such clean, well behaved data in real life. Instead, we may get something like this: Same algorithm as above, but now performing poorly due ...", "dateLastCrawled": "2022-02-01T17:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Introduction to Machine Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/24/introduction-to-machine-learning-2/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/24/<b>introduction-to-machine-learning</b>-2", "snippet": "<b>Machine</b> <b>learning</b> terminology for model building and validation There seems to be an <b>analogy</b> between statistical modeling and <b>machine</b> <b>learning</b> that we will cover in subsequent chapters in depth. However, a quick view has been provided as follows: in statistical modeling, linear <b>regression</b> with two independent variables is trying to fit the best plane with\u2026", "dateLastCrawled": "2022-01-17T05:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Introduction to <b>Machine</b> <b>Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/30/introduction-to-machine-learning-3/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/30/introduction-to-<b>machine</b>-<b>learning</b>-3", "snippet": "<b>Machine</b> <b>learning</b> <b>Machine</b> <b>learning</b> is the branch of computer science that utilizes past experience to learn from and use its knowledge to make future decisions. <b>Machine</b> <b>learning</b> is at the intersection of computer science, engineering, and statistics. The goal of <b>machine</b> <b>learning</b> is to generalize a detectable pattern or to create an unknown rule from\u2026", "dateLastCrawled": "2022-01-28T18:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Trends <b>in artificial intelligence, machine learning, and chemometrics</b> ...", "url": "https://chemistry-europe.onlinelibrary.wiley.com/doi/10.1002/ansa.202000162", "isFamilyFriendly": true, "displayUrl": "https://chemistry-europe.onlinelibrary.wiley.com/doi/10.1002/ansa.202000162", "snippet": "The derived spectra were analyzed for classification and quantification purposes using soft independent modeling of class <b>analogy</b> (SIMCA), artificial neural network (ANN), and partial <b>least</b> <b>squares</b> <b>regression</b> (PLSR). A good classification of tomatoes based on their carotenoid profile of 93% and 100% is shown using SIMCA and ANN, respectively. Besides this result, PLSR and ANN were able to achieve a good quantification of all-", "dateLastCrawled": "2022-02-01T19:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Preparing for interview on Machine Learning</b>? Here, is a complete guide ...", "url": "https://medium.com/analytics-vidhya/preparing-for-interview-on-machine-learning-3145caeea06b", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/<b>preparing-for-interview-on-machine-learning</b>-3145...", "snippet": "In technical terms, linear <b>regression</b> is a <b>machine</b> <b>learning</b> algorithm that finds the best linear-fit relationship on any given data, between independent and dependent variables. It is mostly done ...", "dateLastCrawled": "2022-02-02T20:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "A <b>Machine Learning Application for Classification of Chemical</b> Spectra ...", "url": "https://link.springer.com/chapter/10.1007/978-1-84882-215-3_6", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/chapter/10.1007/978-1-84882-215-3_6", "snippet": "Yang, H., Griffiths, P.R. &amp; Tate, J.D. (2003). Comparison of partial <b>least</b> <b>squares</b> <b>regression</b> and multi-layer neural networks for quantification of non-linear systems and application to gas phase fourier transfrom infrared spectra. Analytica Chimica Acta, 489, 125\u2013136. CrossRef Google Scholar", "dateLastCrawled": "2021-12-24T17:36:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Bayesian <b>Learning</b> - Rebellion Research", "url": "https://www.rebellionresearch.com/bayesian-learning", "isFamilyFriendly": true, "displayUrl": "https://www.rebellionresearch.com/bayesian-<b>learning</b>", "snippet": "Linear Regression example of <b>machine learning Least Squares Regression can be thought of as</b> a very limited <b>learning</b> algorithm, where the training set consists of a number of x and y data pairs. The task would be trying to predict the y value, and the performance measure would be the sum of the squared differences between the predicted and actual y\u2019s.", "dateLastCrawled": "2022-01-19T02:15:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(least squares regression)  is like +(finding the equation of a straight line)", "+(least squares regression) is similar to +(finding the equation of a straight line)", "+(least squares regression) can be thought of as +(finding the equation of a straight line)", "+(least squares regression) can be compared to +(finding the equation of a straight line)", "machine learning +(least squares regression AND analogy)", "machine learning +(\"least squares regression is like\")", "machine learning +(\"least squares regression is similar\")", "machine learning +(\"just as least squares regression\")", "machine learning +(\"least squares regression can be thought of as\")", "machine learning +(\"least squares regression can be compared to\")"]}