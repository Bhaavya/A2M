{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "python - Slightly adapt <b>L1</b> <b>loss</b> to a weighted <b>L1</b> <b>loss</b> in Pytorch, does ...", "url": "https://stackoverflow.com/questions/58200833/slightly-adapt-l1-loss-to-a-weighted-l1-loss-in-pytorch-does-gradient-computati", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/58200833/slightly-adapt-<b>l1</b>-<b>loss</b>-to-a-<b>weight</b>ed-<b>l1</b>...", "snippet": "I implemented a neural network in Pytorch and I would <b>like</b> to use a weighted <b>L1</b> <b>loss</b> function to train the network. The implementation with the regular <b>L1</b> <b>loss</b> contains this code for each epoch:", "dateLastCrawled": "2022-01-25T13:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "How <b>L1</b> Regularization brings Sparsity` - <b>GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/how-l1-regularization-brings-sparsity/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/how-<b>l1</b>-regularization-brings-sparsity", "snippet": "And for this purpose, we mainly use two types of methods namely: <b>L1</b> regularization and L2 regularization. (where w1,w2 \u2026 wn are \u2018 d \u2018 dimensional <b>weight</b> vectors) Now while optimization, that is done based on the concept of Gradient Descent algorithm, it is seen that if we use <b>L1</b> regularization, it brings sparsity to our <b>weight</b> vector by ...", "dateLastCrawled": "2022-01-26T21:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Guide to <b>L1</b> and L2 <b>regularization</b> in Deep Learning | by Uniqtech | Data ...", "url": "https://medium.com/data-science-bootcamp/guide-to-regularization-in-deep-learning-c40ac144b61e", "isFamilyFriendly": true, "displayUrl": "https://medium.com/data-science-bootcamp/guide-to-<b>regularization</b>-in-deep-learning-c40...", "snippet": "Mathematicians and researchers found that <b>L1</b> regularizations when the <b>weight</b> is below the threshold, <b>L1</b> <b>regularization</b> pushes all the <b>weight</b> values close to zero and thus result in sparse <b>weight</b> ...", "dateLastCrawled": "2022-02-03T00:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Issue with pure gan <b>loss</b> i.e.( <b>L1</b> <b>weight</b> = 0 Gan <b>weight</b> = 1) \u00b7 Issue ...", "url": "https://github.com/affinelayer/pix2pix-tensorflow/issues/85", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/affinelayer/pix2pix-tensorflow/issues/85", "snippet": "gen_<b>loss</b>_GAN 1.53159 gen_<b>loss</b>_<b>L1</b> 0.536851 progress epoch 44 step 308 image/sec 0.2 remaining 4464m discrim_<b>loss</b> 0.925797 gen_<b>loss</b>_GAN 1.42804 gen_<b>loss</b>_<b>L1</b> 0.507922. We can see that discrim_<b>loss</b> still won. gen_<b>loss</b>_<b>L1</b>, although no gradient apply on it, seems to have increase ! That mean the image is worst than before. Let it be a few more.", "dateLastCrawled": "2021-09-20T16:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>L1</b> <b>loss</b> problem with GAN if some data has ground-truth and some not ...", "url": "https://git.chanpinqingbaoju.com/tensorflow/gan/issues/39", "isFamilyFriendly": true, "displayUrl": "https://git.chanpinqingbaoju.com/tensorflow/gan/issues/39", "snippet": "tensorflow &gt; gan <b>L1</b> <b>loss</b> problem with GAN if some data has ground-truth and some not about gan HOT 2 OPEN HymEric commented on February 19, 2021 . It&#39;s a great work. If I want to use <b>L1</b> <b>loss</b> between the generated image by generator and the ground-truth image, but some images have ground-truth and some others don&#39;t.", "dateLastCrawled": "2022-02-01T12:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>L1</b> <b>loss</b> problem with GAN if some data has ground-truth and some not ...", "url": "https://github.com/tensorflow/gan/issues/39", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/tensorflow/gan/issues/39", "snippet": "If I want to use <b>L1</b> <b>loss</b> between the generated image by generator and the ground-truth image, but some images have ground-truth and some others don&#39;t. That is to say, in a batch, some have ground-truth and some not. I only will use <b>l1</b> <b>loss</b> with the images which have ground-truth. In this problem, is there a way to address it? Thank you!", "dateLastCrawled": "2021-08-15T18:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Weight</b> Decay in Neural Networks - Programmathically", "url": "https://programmathically.com/weight-decay-in-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://<b>program</b>mathically.com/<b>weight</b>-decay-in-neural-networks", "snippet": "The closer the <b>weight</b> gets to zero, the more the penalty term will shrink relative to the <b>weight</b>. The <b>L1</b> penalty is not squared. Instead, it subtracts a bigger constant value from the <b>weight</b> in every iteration that can reach 0. As a consequence, <b>L1</b> regularized <b>weight</b> matrices end up becoming sparse, which means many of its entries equal zero.", "dateLastCrawled": "2022-01-30T08:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "python - How to add <b>L1</b> Regularization to PyTorch NN Model? - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/58172188/how-to-add-l1-regularization-to-pytorch-nn-model", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/58172188", "snippet": "<b>l1</b>_regularization = 0. for param in model.parameters(): <b>l1</b>_regularization += param.abs().sum() <b>loss</b> = criterion(out, target) + <b>l1</b>_regularization This is really what is at heart of both approaches. You use the Module.parameters method to iterate over all model parameters and you sum up their <b>L1</b> norms, which then becomes a term in your <b>loss</b> function.", "dateLastCrawled": "2022-01-13T02:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Scikit Learn - Support Vector Machines</b> - <b>Tutorialspoint</b>", "url": "https://www.tutorialspoint.com/scikit_learn/scikit_learn_support_vector_machines.htm", "isFamilyFriendly": true, "displayUrl": "https://<b>www.tutorialspoint.com</b>/scikit_learn/<b>scikit_learn_support_vector_machines</b>.htm", "snippet": "However, it supports \u2018<b>loss</b>\u2019 parameters as follows \u2212. <b>loss</b> \u2212 string, optional, default = \u2018epsilon_insensitive\u2019 It represents the <b>loss</b> function where epsilon_insensitive <b>loss</b> is the <b>L1</b> <b>loss</b> and the squared epsilon-insensitive <b>loss</b> is the L2 <b>loss</b>. Implementation Example. Following Python script uses sklearn.svm.LinearSVR class \u2212", "dateLastCrawled": "2022-02-02T13:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Loss</b> Functions in Neural Networks", "url": "https://www.theaidream.com/post/loss-functions-in-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://www.theaidream.com/post/<b>loss</b>-functions-in-neural-networks", "snippet": "<b>Loss</b> functions are mainly classified into two different categories Classification <b>loss</b> and Regression <b>Loss</b>. Classification <b>loss</b> is the case where the aim is to predict the output from the different categorical values for example, if we have a dataset of handwritten images and the digit is to be predicted that lies between (0\u20139), in these kinds of scenarios classification <b>loss</b> is used.", "dateLastCrawled": "2022-02-02T10:20:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Weight</b> <b>loss</b> effect on inflammation and LDL oxidation in metabolically ...", "url": "https://pubmed.ncbi.nlm.nih.gov/16552406/", "isFamilyFriendly": true, "displayUrl": "https://pubmed.ncbi.nlm.nih.gov/16552406", "snippet": "The treatment induced <b>weight</b> <b>loss</b> averaging 3.11% of initial body <b>weight</b>, and the degree of <b>weight</b> <b>loss</b> between the two groups was <b>similar</b>. Visceral fat at <b>L1</b> and L4 was reduced from its initial values by 3.2 and 5.4%, respectively, after <b>weight</b> <b>loss</b>. The levels of CRP (P&lt;0.05) and oxidized LDL (P&lt;0.01) were significantly reduced in the MAO group after the 12-week <b>weight</b> <b>loss</b>, whereas these effects were not seen in the MHO group.", "dateLastCrawled": "2021-03-17T23:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Weight</b> <b>loss</b> effect on inflammation and LDL oxidation in metabolically ...", "url": "https://www.ncbi.nlm.nih.gov/pubmed/16552406", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pubmed/16552406", "snippet": "The treatment induced <b>weight</b> <b>loss</b> averaging 3.11% of initial body <b>weight</b>, and the degree of <b>weight</b> <b>loss</b> between the two groups was <b>similar</b>. Visceral fat at <b>L1</b> and L4 was reduced from its initial values by 3.2 and 5.4%, respectively, after <b>weight</b> <b>loss</b>. The levels of CRP (P&lt;0.05) and oxidized LDL (P&lt;0.01) were significantly reduced in the MAO group after the 12-week <b>weight</b> <b>loss</b>, whereas these effects were not seen in the MHO group. CONCLUSIONS: Our results showed that MHO individuals exhibited ...", "dateLastCrawled": "2019-10-11T02:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Losses explanation \u00b7 Issue #99 \u00b7 affinelayer/<b>pix2pix</b>-tensorflow \u00b7 <b>GitHub</b>", "url": "https://github.com/affinelayer/pix2pix-tensorflow/issues/99", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/affinelayer/<b>pix2pix</b>-tensorflow/issues/99", "snippet": "100*gen_<b>loss</b>_<b>L1</b> + gan. but <b>l1</b>_<b>weight</b> and gan_<b>weight</b> are used to make that <b>weight</b> like : <b>l1</b>_<b>weight</b>*gen_<b>loss</b>_<b>L1</b> + gan_<b>weight</b>*gan. so that you can adjust it to your data. gan is better for details. <b>L1</b> is a very easy way to compare images. There must be better ways for your own dataset. gan is something like learning a network to compare these ...", "dateLastCrawled": "2021-11-18T13:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "python - Pytorch: how to add <b>L1</b> regularizer to activations? - Stack ...", "url": "https://stackoverflow.com/questions/44641976/pytorch-how-to-add-l1-regularizer-to-activations", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/44641976", "snippet": "This <b>similar</b> post refers to adding L2 <b>regularization</b>, but it appears to add the <b>regularization</b> penalty to all layers of the network. nn.modules.<b>loss</b>.L1Loss() seems relevant, but I do not yet understand how to use this. The legacy module L1Penalty seems relevant also, but why has it been deprecated? python pytorch. Share. Improve this question. Follow edited Mar 13 &#39;21 at 14:31. iacob. 12k 4 4 gold badges 45 45 silver badges 81 81 bronze badges. asked Jun 20 &#39;17 at 0:39. Bull Bull. 423 1 1 ...", "dateLastCrawled": "2022-01-28T20:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "What&#39;s the inspiration behind the supervised <b>loss</b> function? \u00b7 Issue #15 ...", "url": "https://github.com/senguptaumd/Background-Matting/issues/15", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/senguptaumd/Background-Matting/issues/15", "snippet": "The supervised <b>L1</b> <b>loss</b> <b>is similar</b> to Deep Image Matting paper (they have losses over alpha and composition). We did not attempt to make it differentiable and use simple <b>L1</b> <b>loss</b>, also add a <b>loss</b> over the foreground F. Note that many existing matting works do not predict the foreground layer F and simply use the image I while compositing. This causes artifacts (see our paper appendix).", "dateLastCrawled": "2021-08-28T20:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Least Squares <b>Optimization with L1-Norm Regularization</b>", "url": "https://www.cs.ubc.ca/~schmidtm/Documents/2005_Notes_Lasso.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.ubc.ca/~schmidtm/Documents/2005_Notes_Lasso.pdf", "snippet": "estimation, other <b>loss</b> functions, active application areas, and properties of <b>L1</b> regularization. Illustrative implemen- tations of each of these 8 methods are included with this document as a web resource. 1 Introduction This report focuses on optimizing on the Least Squares objective function with an <b>L1</b> penalty on the parameters. There is currently signi\ufb01cant interest in this and related problems in a wide variety of \ufb01elds, due to the appealing idea of creating accurate predictive ...", "dateLastCrawled": "2022-02-02T20:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "HNSE-<b>L1</b>-1. Validation of Sweat Rate, Fluid <b>Loss</b>, and Sodium <b>Loss</b> in ...", "url": "https://oursymposium.sites.unlv.edu/program/validation-of-sweat-rate-fluid-loss-and-sodium-loss-in-wearable-technology/", "isFamilyFriendly": true, "displayUrl": "https://oursymposium.sites.unlv.edu/<b>program</b>/validation-of-sweat-rate-fluid-<b>loss</b>-and...", "snippet": "Hydration status and sodium <b>loss</b> will be determined pre-and post-exercise via body <b>weight</b>, bioelectric impedance analysis (BIA), and urine and blood samples. While running, the participant will wear the Gx SP, an additional absorbent gauze pad, and a heart rate (HR) monitor. Running intensity will be monitored by HR and corresponding HRR values. After completing the exercise, sweat rate, fluid <b>loss</b>, and sodium <b>loss</b> will be generated via the Gx SP. The absorbent gauze will be analyzed for ...", "dateLastCrawled": "2022-01-20T21:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Regularization in Machine Learning", "url": "https://programmathically.com/regularization-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://<b>program</b>mathically.com/regularization-in-machine-learning", "snippet": "Regularization is a technique to reduce overfitting in machine learning. We can regularize machine learning methods through the cost function using <b>L1</b> regularization or L2 regularization. <b>L1</b> regularization adds an absolute penalty term to the cost function, while L2 regularization adds a squared penalty term to the cost function.", "dateLastCrawled": "2022-02-02T09:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Best single-slice measurement site for estimating visceral adipose ...", "url": "https://nutritionandmetabolism.biomedcentral.com/articles/10.1186/1743-7075-9-56", "isFamilyFriendly": true, "displayUrl": "https://nutritionandmetabolism.biomedcentral.com/articles/10.1186/1743-7075-9-56", "snippet": "As for the \u0394VAT volume in response <b>to weight</b> <b>loss</b>, the influence of the measurement site on \u0394VAT estimation became greater after <b>weight</b> <b>loss</b> (Figure 2(a)). The image located at 6 cm above L4\u2013L5 explained 80% (r = 0.90) of the variance in the \u0394VAT volume, but the image located at L4\u2013L5 explained only 35% (r = 0.59) of the variance. This result suggests that estimation of the \u0394VAT volume using a single-slice images required the use if an image taken at 5\u20136 cm above the L4\u2013L5 (the ...", "dateLastCrawled": "2022-01-31T19:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "python - Define custom <b>loss</b> (perceptual <b>loss</b>) in CNN autoencoder with ...", "url": "https://stackoverflow.com/questions/65484420/define-custom-loss-perceptual-loss-in-cnn-autoencoder-with-pre-train-vgg19-ten", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/65484420", "snippet": "i want to define perceptual_<b>loss</b> in autoencoder that build in keras. my autoencoder is look like this : Encoder: input_encoder = Input((32,32,3),name = &#39;encoder_input&#39;) encoder = Conv2D(16,(3,3),", "dateLastCrawled": "2022-01-22T02:21:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "python - Slightly adapt <b>L1</b> <b>loss</b> to a weighted <b>L1</b> <b>loss</b> in Pytorch, does ...", "url": "https://stackoverflow.com/questions/58200833/slightly-adapt-l1-loss-to-a-weighted-l1-loss-in-pytorch-does-gradient-computati", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/58200833/slightly-adapt-<b>l1</b>-<b>loss</b>-to-a-<b>weight</b>ed-<b>l1</b>...", "snippet": "Now I want to use a weighted <b>L1</b> <b>loss</b> instead. So I <b>thought</b> to use the same standard Pytorch <b>L1</b> function again and rescale the forecasts and targets with weights. Will the gradient computation still be done correctly?", "dateLastCrawled": "2022-01-25T13:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Dietary capsaicin and its anti-obesity potency: from mechanism to ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5426284/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC5426284", "snippet": "Limitations in anti-obesity approaches. It has clearly established that <b>weight</b> <b>loss</b> will significantly diminish the complications of obesity [].Emerging human epidemiology studies indicated that reducing body <b>weight</b>, with <b>weight</b> <b>loss</b> of at least 5%, has long-term benefits on metabolic health and reduces the risks of developing insulin resistance, T2DM, and cardiovascular diseases [].However, <b>weight</b> <b>loss</b> is difficult and the obese individuals are struggling to achieve it, and the efficacy of ...", "dateLastCrawled": "2022-01-25T09:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "How to Develop <b>LASSO Regression</b> Models in Python", "url": "https://machinelearningmastery.com/lasso-regression-with-python/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>lasso-regression</b>-with-python", "snippet": "A hyperparameter is used called \u201c lambda \u201d that controls the weighting of the penalty to the <b>loss</b> function. A default value of 1.0 will give full weightings to the penalty; a value of 0 excludes the penalty. Very small values of lambda, such as 1e-3 or smaller, are common. lasso_<b>loss</b> = <b>loss</b> + (lambda * <b>l1</b>_penalty)", "dateLastCrawled": "2022-02-02T15:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Update documentation for LinearSVC and supported</b> <b>loss</b> with <b>l1</b> penalty ...", "url": "https://fantashit.com/update-documentation-for-linearsvc-and-supported-loss-with-l1-penalty/", "isFamilyFriendly": true, "displayUrl": "https://fantashit.com/<b>update-documentation-for-linearsvc-and-supported</b>-<b>loss</b>-with-<b>l1</b>...", "snippet": "Describe the bug. The method LinearSVC crashes when the combination of parameters penalty = \u2018<b>l1</b>\u2019 and <b>loss</b> = \u2018hinge\u2019 are set. Steps/Code to Reproduce. Example: from sklearn. svm import LinearSVC C = 0.0001 X = [ [ 0, 0 ], [ 1, 1 ]] y = [ 0, 1 ] clf = LinearSVC ( penalty = &#39;<b>l1</b>&#39;, <b>loss</b> = &#39;hinge&#39;, C = C, dual = False ) clf. fit ( X = X, y ...", "dateLastCrawled": "2022-01-02T08:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "EFT <b>L1</b> and 2 <b>program</b> \u2014 Katie Walker EFT", "url": "https://www.katiewalkereft.com/eft-l1-and-2-program", "isFamilyFriendly": true, "displayUrl": "https://www.katiewalkereft.com/eft-<b>l1</b>-and-2-<b>program</b>", "snippet": "EFT tapping <b>can</b> be used for anxiety, stress, phobias, <b>weight</b> <b>loss</b> issues, pain, cravings and many other issues. HOW DOES IT WORK? What EFT is doing is tuning in to the negative patterns that we form around our uncomfortable thoughts, feelings, or troubling memories and we \u2018tap\u2019 on the different acupressure points while bringing those negative thoughts or emotions into our mind.", "dateLastCrawled": "2022-02-02T20:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Diet Versus Exercise in <b>Weight</b> <b>Loss</b> and Maintenance: Focus on Tryptophan", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4864009/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC4864009", "snippet": "<b>Weight</b> <b>loss</b> leads to compensatory changes in the homeostatic processes, including alterations in energy expenditure, substrate metabolism, and hormone pathways involved in appetite regulation that result in increased hunger and energy storage, favoring <b>weight</b> regain. 51 In a recent review, MacLean et al. 52 summarized the adaptations to energy-restricted <b>weight</b> <b>loss</b> that are <b>thought</b> to promote <b>weight</b> regain. During <b>weight</b> <b>loss</b>, metabolic requirements decline as a function of (i) lost body ...", "dateLastCrawled": "2022-01-15T07:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "30-Day Meal Plan and <b>Weight</b> <b>Loss</b> Guide", "url": "https://images.template.net/wp-content/uploads/2016/01/11040917/30-Day-Meal-Plan-for-Weigh-Loss-PDF-Template-Free-Download.pdf", "isFamilyFriendly": true, "displayUrl": "https://images.template.net/wp-content/uploads/2016/01/11040917/30-Day-Meal-Plan-for...", "snippet": "<b>program</b> that <b>can</b> instantly fit into your lifestyle. Remember, the success lies in simplicity, clarity, and practicality. Then, you need to adopt an eating plan that works synergistically with your exercise plan to build on the foundation of the inner work you have done in order to embrace your personal power. Remember that, with the right guide, you <b>can</b> do more than go on yet another <b>weight</b> <b>loss</b> diet; you <b>can</b> truly win in all areas of your life and achieve an outer beauty than matches your ...", "dateLastCrawled": "2022-02-03T05:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Weighted Custom Loss Function Different Training Loss</b> \u00b7 Issue #2834 ...", "url": "https://github.com/microsoft/LightGBM/issues/2834", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/microsoft/LightGBM/issues/2834", "snippet": "However, if I <b>weight</b> the samples (doesn&#39;t matter what weights - any weights), the training <b>loss</b> in the custom function becomes slightly different to that of the default <b>loss</b> function. The difference is very minor but I still do not understand why there is any difference at all. This looks like it might be a bug to me. It&#39;s not clear which is right and which is wrong. Intriguingly, the final predictions however are still", "dateLastCrawled": "2021-11-12T03:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Cutting Your Losses: <b>Loss</b> Functions &amp; the Sum of <b>Squared</b> Errors <b>Loss</b> ...", "url": "https://dustinstansbury.github.io/theclevermachine/cutting-your-losses", "isFamilyFriendly": true, "displayUrl": "https://dustinstansbury.github.io/theclevermachine/cutting-your-<b>loss</b>es", "snippet": "A helpful interpretation of the SSE <b>loss function</b> is demonstrated in Figure 2.The area of each red square is a literal geometric interpretation of each observation\u2019s contribution to the overall <b>loss</b>. We see that no matter if the errors are positive or negative (i.e. actual \\(y_i\\) are located above or below the black line), the contribution to the <b>loss</b> is always an area, and therefore positive.", "dateLastCrawled": "2022-01-31T18:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Beyond Metabolism How Your Brain Biology And The ... - <b>l1</b>.flashphoner.com", "url": "https://l1.flashphoner.com/beyond_metabolism_how_your_brain_biology_and_the_environment_create_and_perpetuate_weight_issues_and_what_you_can_do_about_it_pdf", "isFamilyFriendly": true, "displayUrl": "https://<b>l1</b>.flashphoner.com/beyond_metabolism_how_your_brain_biology_and_the...", "snippet": "The latest medical research shows that balanced hormones are the key to <b>weight</b> <b>loss</b>. In fact, those hard-to-maintain diet fads wreak havoc on your hormones, which is why the <b>weight</b> comes back the moment you stop. Dr. Michael Aziz is board-certified in internal medicine and knows that the ultimate key to good health is a diet that <b>can</b> be maintained in the real world. In The Perfect 10 Diet, he shares his revolutionary discovery: how to create the perfect balance between the 10 key hormones ...", "dateLastCrawled": "2022-01-26T16:13:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>L1</b> and <b>L2: loss function and regularization</b> | Develop Paper", "url": "https://developpaper.com/l1-and-l2-loss-function-and-regularization/", "isFamilyFriendly": true, "displayUrl": "https://developpaper.com/<b>l1</b>-and-<b>l2-loss-function-and-regularization</b>", "snippet": "Of the least square <b>loss</b> functionL1 ... <b>Compared</b> with <b>L1</b>, the edges and corners on the image are much smoother. Generally, the optimal value does not appear on the axis. When the regular term is minimized, it <b>can</b> beThe parameter tends to zeroIn the end, we live with very small parameters. In machine learning, normalization is an important technique to prevent over fitting. Mathematically speaking, it will add a regular term to prevent the coefficient from fitting too well and over fitting ...", "dateLastCrawled": "2022-02-03T00:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Guide to <b>L1</b> and L2 <b>regularization</b> in Deep Learning | by Uniqtech | Data ...", "url": "https://medium.com/data-science-bootcamp/guide-to-regularization-in-deep-learning-c40ac144b61e", "isFamilyFriendly": true, "displayUrl": "https://medium.com/data-science-bootcamp/guide-to-<b>regularization</b>-in-deep-learning-c40...", "snippet": "You <b>can</b> see the formula for penalty, <b>regularization</b> term aka Omega below is represented by the <b>L1</b> norm, double bar with subscript of 1, or the sum of absolute values of <b>weight</b>.", "dateLastCrawled": "2022-02-03T00:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Least Squares <b>Optimization with L1-Norm Regularization</b>", "url": "https://www.cs.ubc.ca/~schmidtm/Documents/2005_Notes_Lasso.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.ubc.ca/~schmidtm/Documents/2005_Notes_Lasso.pdf", "snippet": "estimation, other <b>loss</b> functions, active application areas, and properties of <b>L1</b> regularization. Illustrative implemen- tations of each of these 8 methods are included with this document as a web resource. 1 Introduction This report focuses on optimizing on the Least Squares objective function with an <b>L1</b> penalty on the parameters. There is currently signi\ufb01cant interest in this and related problems in a wide variety of \ufb01elds, due to the appealing idea of creating accurate predictive ...", "dateLastCrawled": "2022-02-02T20:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Dietary capsaicin and its anti-obesity potency: from mechanism to ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5426284/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC5426284", "snippet": "But it needs to be injected subcutaneously daily. Moreover, the <b>weight</b> <b>loss</b> is limited and it <b>can</b> increase the risk of pancreatitis . <b>Compared</b> with aforementioned anti-obesity drugs, bariatric surgery such as Roux-en-Y gastric bypass or sleeve gastrectomy is more effective. However, it is physically invasive, relatively expensive, and its long-term effect is unclear . Therefore, alternative anti-obesity treatments are urgently warranted, which should be effective, safe, and widely available ...", "dateLastCrawled": "2022-01-25T09:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Loss</b> Functions in Neural Networks", "url": "https://www.theaidream.com/post/loss-functions-in-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://www.theaidream.com/post/<b>loss</b>-functions-in-neural-networks", "snippet": "<b>Loss</b> functions are mainly classified into two different categories Classification <b>loss</b> and Regression <b>Loss</b>. Classification <b>loss</b> is the case where the aim is to predict the output from the different categorical values for example, if we have a dataset of handwritten images and the digit is to be predicted that lies between (0\u20139), in these kinds of scenarios classification <b>loss</b> is used.", "dateLastCrawled": "2022-02-02T10:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "python - How to add <b>L1</b>-regularization to one hidden layer? - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/47160937/how-to-add-l1-regularization-to-one-hidden-layer", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/47160937", "snippet": "The code above will regularize the weights in the layer. If you want to regularize both weights and the layer output, you <b>can</b> use the same tf.contrib.layers.<b>l1</b>_regularizer or create a different one with different parameters. Something like this should work for you: <b>l1</b> = tf.contrib.layers.<b>l1</b>_regularizer (scale=0.005, scope=None) hidden = tf ...", "dateLastCrawled": "2022-01-09T12:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "An Empirical Study on GAN-Based Traffic Congestion Attack Analysis: A ...", "url": "https://www.hindawi.com/journals/wcmc/2020/8823300/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.hindawi.com</b>/journals/wcmc/2020/8823300", "snippet": "Weighted <b>L1</b> regularization <b>loss</b> <b>can</b> be calculated as follows: ... <b>Weight</b> for <b>loss</b>: 100.0: Initial learning rate: 0.0002: Optimizer: Minibatch SGD, Adam: Batch size: 1: Dropout rate: 0.5: Net : Basic: Net : U-Net: Table 3 . Experimental parameter settings. 5. Evaluation. Actually, our method <b>can</b> be directly <b>compared</b> to NDSS2018\u2019s work for the same I-SIG system. In addition, there are also some similar work to discuss. Reporting road traffic congestion <b>can</b> be challenging as there is no ...", "dateLastCrawled": "2022-01-29T20:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "MR image <b>reconstruction</b> using deep learning: evaluation of network ...", "url": "https://qims.amegroups.com/article/view/29735/25733", "isFamilyFriendly": true, "displayUrl": "https://qims.amegroups.com/article/view/29735/25733", "snippet": "The images reconstructed using the network based on perceptual <b>loss</b> function <b>can</b> generate the best image quality <b>compared</b> to the other <b>loss</b> function (<b>L1</b>, L2, and SSIM), despite not generating the best SNR or SSIM score. Resnet used in this work generate reconstructed image with the similar quality <b>compared</b> to that by Unet while required only 8% of the trainable parameters needed for Unet.", "dateLastCrawled": "2022-01-20T13:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>SegAN: Adversarial Network with Multi-scale</b> <b>L1</b> <b>Loss</b> for Medical Image ...", "url": "https://link.springer.com/article/10.1007/s12021-018-9377-x", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s12021-018-9377-x", "snippet": "Instead, we use a fully convolutional neural network as the segmentor to generate segmentation label maps, and propose a novel adversarial critic network with a multi-scale <b>L1</b> <b>loss</b> function to force the critic and segmentor to learn both global and local features that capture long- and short-range spatial relationships between pixels. In our SegAN framework, the segmentor and critic networks are trained in an alternating fashion in a min-max game: The critic is trained by maximizing a multi ...", "dateLastCrawled": "2022-01-25T14:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Minor Losses In Pipes - <b>l1</b>.flashphoner.com", "url": "https://l1.flashphoner.com/minor_losses_in_pipes_pdf", "isFamilyFriendly": true, "displayUrl": "https://<b>l1</b>.flashphoner.com/minor_<b>loss</b>es_in_pipes_pdf", "snippet": "Minor <b>Loss</b> Equation: g = acceleration due to gravity = 32.174 ft/s 2 = 9.806 m/s 2. h m = head <b>loss</b> due to a fitting and has units of ft or m of fluid. It is the energy <b>loss</b> due to a fitting per unit <b>weight</b> of fluid. K = minor <b>loss</b> coefficient for valves, bends, tees, and other fittings - table of minor <b>loss</b> coefficients.", "dateLastCrawled": "2022-02-03T04:30:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Regularization \u2014 Understanding <b>L1</b> and L2 regularization for Deep <b>Learning</b>", "url": "https://medium.com/analytics-vidhya/regularization-understanding-l1-and-l2-regularization-for-deep-learning-a7b9e4a409bf", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/regularization-understanding-<b>l1</b>-and-l2...", "snippet": "Understanding what regularization is and why it is required for <b>machine</b> <b>learning</b> and diving deep to clarify the importance of <b>L1</b> and L2 regularization in Deep <b>learning</b>.", "dateLastCrawled": "2022-02-01T00:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Choosing and Customizing <b>Loss</b> Functions for Image Processing | by ...", "url": "https://towardsdatascience.com/choosing-and-customizing-loss-functions-for-image-processing-a0e4bf665b0a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/choosing-and-customizing-<b>loss</b>-functions-for-image...", "snippet": "This is what a <b>machine</b> <b>learning</b> (ML) algorithm does during training. More specifically, the ... Mean Absolute Error, <b>L1</b> <b>Loss</b> (used by PerceptiLabs\u2019 Regression component): sums the absolute differences between the predictions and ground truth, and finds the average. <b>Loss</b> functions are used in a variety of use cases. The following table shows common image processing use cases where you might apply these, and other <b>loss</b> functions: Image Source: PerceptiLabs <b>Loss</b> in PL. Configuring a <b>loss</b> ...", "dateLastCrawled": "2022-01-31T10:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "TensorFlow is an interface for expressing <b>machine</b> <b>learning</b> algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Understanding the 3 most common <b>loss</b> functions for <b>Machine</b> <b>Learning</b> ...", "url": "https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-the-3-most-common-<b>loss</b>-functions-for...", "snippet": "A <b>loss function</b> in <b>Machine</b> <b>Learning</b> is a measure of how accurately your ML model is able to predict the expected outcome i.e the ground truth. The <b>loss function</b> will take two items as input: the output value of our model and the ground truth expected value. The output of the <b>loss function</b> is called the <b>loss</b> which is a measure of how well our model did at predicting the outcome. A high value for the <b>loss</b> means our model performed very poorly. A low value for the <b>loss</b> means our model performed ...", "dateLastCrawled": "2022-02-02T13:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Solutions to the exercises for <b>Machine</b> <b>Learning</b>", "url": "http://www.perfmath.com/ml/ml_liu_text_solutions.pdf", "isFamilyFriendly": true, "displayUrl": "www.perfmath.com/ml/ml_liu_text_solutions.pdf", "snippet": "is known as <b>L1</b>-norm, while the latter is known as the L2-norm. Keep in mind that L2-norm is more sensitive than <b>L1</b>-norm to large-valued outliers. Ridge and LASSO regularizations are based on L2-norm and <b>L1</b>-norm, respectively, while Elastic Net regularization is based on the mix of two. 2.6 What does a <b>machine</b> <b>learning</b> <b>learning</b>-curve measure ...", "dateLastCrawled": "2022-01-18T07:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "regression - Why <b>L1</b> norm for sparse models - Cross Validated", "url": "https://stats.stackexchange.com/questions/45643/why-l1-norm-for-sparse-models", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/45643", "snippet": "Show activity on this post. With a sparse model, we think of a model where many of the weights are 0. Let us therefore reason about how <b>L1</b>-regularization is more likely to create 0-weights. Consider a model consisting of the weights . With <b>L1</b> regularization, you penalize the model by a <b>loss</b> function = .", "dateLastCrawled": "2022-01-26T23:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine Learning: The Optimization Perspective</b>", "url": "https://kt.era.ee/lectures/ifiss2014/2-optimization.pdf", "isFamilyFriendly": true, "displayUrl": "https://kt.era.ee/lectures/ifiss2014/2-optimization.pdf", "snippet": "Reasoning by <b>analogy</b>. The Land of <b>Machine</b> <b>Learning</b> AACIMP Summer School. August, 2012 Optimization Probability theory Reasoning by <b>analogy</b> Dragons. What do you need to know about optimization? IFI Summer School. June 2014. What do you need to know about optimization? 1. Optimization is important 2. Optimization is possible IFI Summer School. June 2014. What do you need to know about optimization? 1. Optimization is important 2. Optimization is possible* * Basic techniques Constrained ...", "dateLastCrawled": "2022-01-18T11:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "CPSC 340: Data Mining <b>Machine</b> <b>Learning</b>", "url": "https://www.cs.ubc.ca/~schmidtm/Courses/340-F16/L35.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.ubc.ca/~schmidtm/Courses/340-F16/L35.pdf", "snippet": "\u2022Exam <b>analogy</b> for types of supervised/semi-supervised <b>learning</b>: \u2013Regular supervised <b>learning</b>: ... (for supervised <b>learning</b>) \u2013<b>Loss</b> functions, change of basis, regularization, feature selection. \u2013Gradient descent and stochastic gradient. 4. Latent-factor models (for unsupervised <b>learning</b>) \u2013Typically using linear models and gradient descent. 5. Neural networks (for supervised and multi-layer latent-factor models). 6. Markov chains \u2013Random walk models for sequences and data living on ...", "dateLastCrawled": "2021-11-22T01:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>machine</b> <b>learning</b> - <b>L1</b>-norm vs l2-norm as cost function when ...", "url": "https://stackoverflow.com/questions/43301036/l1-norm-vs-l2-norm-as-cost-function-when-standardizing", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/43301036", "snippet": "I feel that the l2-norm will penalize less the model than the <b>l1</b>-norm since squaring a number that is between 0 and 1 will always result in a lower number. So my question is, is it ok to use the l2-norm when both the input and the output are standardized? <b>machine</b>-<b>learning</b> statistics gradient-descent. Share. Follow edited Apr 10 &#39;17 at 12:08. jeremie. asked Apr 8 &#39;17 at 22:34. jeremie jeremie. 807 8 8 silver badges 15 15 bronze badges. Add a comment | 1 Answer Active Oldest Votes. 1 It does ...", "dateLastCrawled": "2022-01-24T10:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Classifying and completing word analogies by <b>machine</b> <b>learning</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0888613X21000141", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0888613X21000141", "snippet": "The <b>machine</b> <b>learning</b> approaches outperform state of the art approaches on Google, BATS and DiffVec datasets. As far as we know, neither <b>analogy</b> classification nor <b>analogy</b> completion have been investigated in the same way as we have proposed in this paper, namely <b>learning</b> a model, instead of starting from the parallelogram model. The paper is structured as follows. Section 2 recalls the postulates characterizing analogical proportions and identifies a rigorous method for enlarging a set of ...", "dateLastCrawled": "2021-11-13T01:15:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Deep learning</b> - lectures.alex.balgavy.eu", "url": "https://lectures.alex.balgavy.eu/ml-notes/deep-learning/", "isFamilyFriendly": true, "displayUrl": "https://lectures.alex.balgavy.eu/ml-notes/<b>deep-learning</b>", "snippet": "<b>Deep learning</b> <b>Deep learning</b> systems (autodiff engines) Tensors. To scale up backpropagation, want to move from operations on scalars to tensors. Tensor: generalisation of vectors/matrices to higher dimensions. e.g. a 2-tensor has two dimensions, a 4-tensor has 4 dimensions. You can represent data as a tensor. e.g. an RGB image is a 3-tensor of the red, green, and blue values for each pixel. Functions on tensors. Functions have inputs and outputs, all of which are tensors. They implement ...", "dateLastCrawled": "2021-12-15T05:20:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Denoising Seismic Records with Image Translation Networks</b> | CSEG RECORDER", "url": "https://csegrecorder.com/articles/view/denoising-seismic-records-with-image-translation-networks", "isFamilyFriendly": true, "displayUrl": "https://csegrecorder.com/articles/view/<b>denoising-seismic-records-with-image</b>...", "snippet": "The pix2pix network is a generative <b>machine</b> <b>learning</b> algorithm. Based on Alec Radford, et. al\u2019s DCGAN [6] architecture, ... the <b>L1 loss is similar</b> to the L2 loss: except the second-degree norm is replaced with the first-degree norm: The alternative denoising strategies tested against the image translation network included total-variation filtering, bilateral filtering, and wavelet transform filtering. Figure 3. Results of various image denoising techniques on synthetic data. Upper left ...", "dateLastCrawled": "2022-01-12T06:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A deep <b>learning</b> framework for constitutive modeling based on temporal ...", "url": "https://www.sciencedirect.com/science/article/pii/S0021999121006793", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0021999121006793", "snippet": "These two features meet the requirement for sequence modeling in <b>machine</b> <b>learning</b>. Therefore, the nonlinear constitutive models may be classified as sequence modeling from the viewpoint of <b>machine</b> <b>learning</b>. Concrete material and steel material both exhibit significant ultra-long-term memory effects and many model-driven constitutive relationships were developed to simulate stress-strain curves of materials , , , , with ultra-long-term memory effect. For steel material, the traditional ...", "dateLastCrawled": "2022-01-20T12:37:00.0000000Z", "language": "en", "isNavigational": false}], [], [], []], "all_bing_queries": ["+(l1 loss)  is like +(weight loss program)", "+(l1 loss) is similar to +(weight loss program)", "+(l1 loss) can be thought of as +(weight loss program)", "+(l1 loss) can be compared to +(weight loss program)", "machine learning +(l1 loss AND analogy)", "machine learning +(\"l1 loss is like\")", "machine learning +(\"l1 loss is similar\")", "machine learning +(\"just as l1 loss\")", "machine learning +(\"l1 loss can be thought of as\")", "machine learning +(\"l1 loss can be compared to\")"]}