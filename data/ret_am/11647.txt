{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Embeddings | Machine Learning Crash Course | <b>Google Developers</b>", "url": "https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/machine-learning/crash-course/<b>embedding</b>", "snippet": "An <b>embedding</b> is a relatively low-dimensional <b>space</b> into which you can translate high-dimensional vectors. Embeddings make it easier to do machine learning on large inputs <b>like</b> sparse vectors representing words. Ideally, an <b>embedding</b> captures some of the semantics of the input by placing semantically similar inputs close together in the <b>embedding</b> <b>space</b>.", "dateLastCrawled": "2022-01-30T18:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "algebraic geometry - <b>Segre map is an embedding</b> - <b>Mathematics Stack Exchange</b>", "url": "https://math.stackexchange.com/questions/3683364/segre-map-is-an-embedding", "isFamilyFriendly": true, "displayUrl": "https://math.stackexchange.com/questions/3683364/<b>segre-map-is-an-embedding</b>", "snippet": "Being bijective is not enough to be an <b>embedding</b>, but what you&#39;ve done is: You have constructed an inverse morphism from \u03a3 m, n to P m \u00d7 P n, showing that the Segre <b>map</b> is an isomorphism onto its image, which is what it means to be an <b>embedding</b>. No, it is definitely not the smallest number. It is a general fact that you can embed any smooth n ...", "dateLastCrawled": "2022-01-25T21:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Embedding</b>: Reconstructing Solutions from a Delay <b>Map</b> | ThatsMaths", "url": "https://thatsmaths.com/2021/09/23/embedding-reconstructing-solutions-from-a-delay-map/", "isFamilyFriendly": true, "displayUrl": "https://thatsmaths.com/2021/09/23/<b>embedding</b>-reconstructing-solutions-from-a-delay-<b>map</b>", "snippet": "An <b>embedding</b> of a manifold in Euclidean <b>space</b> is a diffeomorphism, a one-to-one differentiable mapping from to with differentiable inverse. A fundamental result in differential topology, due to Hassler Whitney, states that if is a compact manifold of dimension , the set of diffeomorphisms is dense in the <b>space</b> of continuous mappings provided that .", "dateLastCrawled": "2022-01-21T16:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 3, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Embedding</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Embedding", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Embedding</b>", "snippet": "The fact that <b>a map</b> f : X \u2192 Y is an <b>embedding</b> is often indicated by the use of a &quot;hooked arrow&quot; (U+21AA \u21aa RIGHTWARDS ARROW WITH HOOK); thus: :. (On the other hand, this notation is sometimes reserved for inclusion maps.) Given X and Y, several different embeddings of X in Y may be possible. In many cases of interest there is a standard (or &quot;canonical&quot;) <b>embedding</b>, <b>like</b> those of the natural numbers in the integers, the integers in the rational numbers, the rational numbers in the real ...", "dateLastCrawled": "2022-02-02T15:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Embedding</b> to non-Euclidean spaces \u2014 <b>umap</b> 0.5 documentation", "url": "https://umap-learn.readthedocs.io/en/latest/embedding_space.html", "isFamilyFriendly": true, "displayUrl": "https://<b>umap</b>-learn.readthedocs.io/en/latest/<b>embedding</b>_<b>space</b>.html", "snippet": "<b>Embedding</b> on a Custom Metric <b>Space</b>\u00b6. What if you have some other custom notion of a metric <b>space</b> that you would <b>like</b> to embed data into? In the same way that <b>UMAP</b> can support custom written distance metrics for the input data (as long as they can be compiled with numba), the output_metric parameter can accept custom distance functions. One catch is that, to support gradient descent optimization, the distance function needs to return both the distance, and a vector for the gradient of the ...", "dateLastCrawled": "2022-01-30T00:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Neural Network Embeddings Explained | by Will Koehrsen | Towards Data ...", "url": "https://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/neural-network-<b>embeddings</b>-explained-4d028e6f0526", "snippet": "Embeddings. An <b>embedding</b> is a mapping of a discrete \u2014 categorical \u2014 variable to a vector of continuous numbers. In the context of neural networks, embeddings are low-dimensional, learned continuous vector representations of discrete variables. Neural network embeddings are useful because they can reduce the dimensionality of categorical variables and meaningfully represent categories in the transformed <b>space</b>.. Neural network embeddings have 3 primary purposes:", "dateLastCrawled": "2022-02-03T06:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "All you need to know about Graph Embeddings", "url": "https://analyticsindiamag.com/all-you-need-to-know-about-graph-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/all-you-need-to-know-about-graph-<b>embeddings</b>", "snippet": "We just need to <b>map</b> these structures in <b>space</b> and calculate the information. We can also understand the graph <b>embedding</b> using the following points: Graph embeddings are a type of data structure that is mainly used to compare the data structures (similar or not). We use it for compressing the complex and large graph data using the information in the vertices and edges and vertices around the main vertex. We use machine learning methods for calculating the graph embeddings. We can think of ...", "dateLastCrawled": "2022-02-02T02:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Embeddings: Translating to a Lower-Dimensional <b>Space</b> | Machine Learning ...", "url": "https://developers.google.com/machine-learning/crash-course/embeddings/translating-to-a-lower-dimensional-space", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/.../<b>embeddings</b>/translating-to-a-lower-dimensional-<b>space</b>", "snippet": "Position (distance and direction) in the <b>vector</b> <b>space</b> can encode semantics in a good <b>embedding</b>. For example, the following visualizations of real embeddings show geometrical relationships that capture semantic relations <b>like</b> the relation between a country and its capital: Figure 4. Embeddings can produce remarkable analogies.", "dateLastCrawled": "2022-02-02T04:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Embedding</b> <b>a map</b> into a struct in the go language - Stack Overflow", "url": "https://stackoverflow.com/questions/36926953/embedding-a-map-into-a-struct-in-the-go-language", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/36926953", "snippet": "<b>Embedding</b> maps in structs is a common practice. The only issues with the implementation would be outside of that, <b>like</b> using <b>a map</b> in favor of a slice when a slice is better. However, that isn&#39;t relevant here. Choosing which collection type to use (slice vs array vs <b>map</b> say) is a different discussion and is more based on your data access ...", "dateLastCrawled": "2022-01-12T19:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "differential geometry - Image of <b>Embedding</b> on a Manifold is a ...", "url": "https://math.stackexchange.com/questions/3332499/image-of-embedding-on-a-manifold-is-a-submanifold", "isFamilyFriendly": true, "displayUrl": "https://math.stackexchange.com/questions/3332499/image-of-<b>embedding</b>-on-a-manifold-is-a...", "snippet": "$\\begingroup$ actually wait I&#39;m confused by your question, did you phrase it correctly? because an <b>embedding</b> is by definition a homeomorphism and hence trivially an open <b>map</b> onto its mage. So, could you clarify whether what you actually wrote is what you intended? I feel <b>like</b> my answer doesnt answer your question as stated, but perhaps might answer what you actually intended to ask $\\endgroup$ \u2013 peek-a-boo", "dateLastCrawled": "2022-01-18T04:38:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Neural Network Embeddings Explained | by Will Koehrsen | Towards Data ...", "url": "https://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/neural-network-<b>embeddings</b>-explained-4d028e6f0526", "snippet": "Embeddings. An <b>embedding</b> is a mapping of a discrete \u2014 categorical \u2014 variable to a vector of continuous numbers. In the context of neural networks, embeddings are low-dimensional, learned continuous vector representations of discrete variables. Neural network embeddings are useful because they can reduce the dimensionality of categorical variables and meaningfully represent categories in the transformed <b>space</b>.. Neural network embeddings have 3 primary purposes:", "dateLastCrawled": "2022-02-03T06:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "All you need to know about Graph Embeddings", "url": "https://analyticsindiamag.com/all-you-need-to-know-about-graph-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/all-you-need-to-know-about-graph-<b>embeddings</b>", "snippet": "We just need to <b>map</b> these structures in <b>space</b> and calculate the information. We can also understand the graph <b>embedding</b> using the following points: Graph embeddings are a type of data structure that is mainly used to compare the data structures (<b>similar</b> or not). We use it for compressing the complex and large graph data using the information in the vertices and edges and vertices around the main vertex. We use machine learning methods for calculating the graph embeddings. We can think of ...", "dateLastCrawled": "2022-02-02T02:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Embeddings | Machine Learning Crash Course | <b>Google Developers</b>", "url": "https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/machine-learning/crash-course/<b>embedding</b>", "snippet": "An <b>embedding</b> is a relatively low-dimensional <b>space</b> into which you can translate high-dimensional vectors. Embeddings make it easier to do machine learning on large inputs like sparse vectors representing words. Ideally, an <b>embedding</b> captures some of the semantics of the input by placing semantically <b>similar</b> inputs close together in the <b>embedding</b> <b>space</b>.", "dateLastCrawled": "2022-01-30T18:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Embedding</b> in Machine Learning | Cathy\u2019s Notes", "url": "https://cathyqian.github.io/2020/08/27/Embedding_In_Machine_Learning.html", "isFamilyFriendly": true, "displayUrl": "https://cathyqian.github.io/2020/08/27/<b>Embedding</b>_In_Machine_Learning.html", "snippet": "<b>Embedding</b> in the context of deep learning is to <b>map</b> high-dimensional vectors or categorical variables to relatively low-dimensional <b>space</b> so that <b>similar</b> items are close to each other. It can be applied to any high dimensional, sparse or categorical features, i.e., IP addresses from ad impressions, raw pixels in a video, audio data from speech ...", "dateLastCrawled": "2022-01-22T06:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Embeddings: Obtaining Embeddings | Machine Learning Crash Course ...", "url": "https://developers.google.com/machine-learning/crash-course/embeddings/obtaining-embeddings", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/machine-learning/crash-course/<b>embeddings</b>/obtaining...", "snippet": "Word2vec relies on the distributional hypothesis to <b>map</b> semantically <b>similar</b> words to geometrically close <b>embedding</b> vectors. The distributional hypothesis states that words which often have the same neighboring words tend to be semantically <b>similar</b>. Both &quot;dog&quot; and &quot;cat&quot; frequently appear close to the word &quot;vet&quot;, and this fact reflects their semantic similarity. As the linguist John Firth put it in 1957, &quot;You shall know a word by the company it keeps&quot;. Word2Vec exploits contextual information ...", "dateLastCrawled": "2022-02-02T08:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Embedding</b> to non-Euclidean spaces \u2014 <b>umap</b> 0.5 documentation", "url": "https://umap-learn.readthedocs.io/en/latest/embedding_space.html", "isFamilyFriendly": true, "displayUrl": "https://<b>umap</b>-learn.readthedocs.io/en/latest/<b>embedding</b>_<b>space</b>.html", "snippet": "We see that we have gotten a result <b>similar</b> to a standard <b>embedding</b> into euclidean <b>space</b>, but with less clear clustering, and more points between clusters. To get a clearer idea of what is going on it will be necessary to devise a means to display some of the extra information contained in the extra 3 dimensions providing covariance data. To do this it will be helpful to be able to draw ellipses corresponding to super-level sets of the PDF of the 2d Gaussian. We can start on this by writing ...", "dateLastCrawled": "2022-01-30T00:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "RStudio AI Blog: <b>Word Embeddings with Keras</b>", "url": "https://blogs.rstudio.com/ai/posts/2017-12-22-word-embeddings-with-keras/", "isFamilyFriendly": true, "displayUrl": "https://blogs.rstudio.com/ai/posts/2017-12-22-<b>word-embeddings-with-keras</b>", "snippet": "Word <b>embedding</b> is a method used to <b>map</b> words of a vocabulary to dense vectors of real numbers where semantically <b>similar</b> words are mapped to nearby points. Representing words in this vector <b>space</b> help algorithms achieve better performance in natural language processing tasks like syntactic parsing and sentiment analysis by grouping <b>similar</b> words. For example, we expect that in the <b>embedding</b> <b>space</b> \u201ccats\u201d and \u201cdogs\u201d are mapped to nearby points since they are both animals, mammals, pets ...", "dateLastCrawled": "2022-02-02T20:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Graph <b>Embedding</b>: Understanding Graph <b>Embedding</b> Algorithms", "url": "https://www.tigergraph.com/blog/understanding-graph-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://www.tigergraph.com/blog/<b>understanding-graph-embeddings</b>", "snippet": "Items that are near each other in this <b>embedding</b> <b>space</b> are considered <b>similar</b> to each other in the real world. Embeddings focus on performance, not explainability. Embeddings are ideal for \u201cfuzzy\u201d match problems. If you have hundreds or thousands of lines of complex if-then statements to build cohorts, <b>graph embeddings</b> provide a way to make this code much smaller and easier to maintain. <b>Graph embeddings</b> work with other graph algorithms. If you are doing clustering or classification ...", "dateLastCrawled": "2022-02-03T02:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Getting started with NLP: Word Embeddings, GloVe and Text ...", "url": "https://edumunozsala.github.io/BlogEms/jupyter/nlp/classification/embeddings/python/2020/08/15/Intro_NLP_WordEmbeddings_Classification.html", "isFamilyFriendly": true, "displayUrl": "https://edumunozsala.github.io/BlogEms/jupyter/nlp/classification/<b>embeddings</b>/python/...", "snippet": "What is word <b>embedding</b>? Word embeddings are a type of word representation that allows words with <b>similar</b> meaning to have a <b>similar</b> representation. They are a distributed representation for text that is perhaps one of the key breakthroughs for the impressive performance of deep learning methods on challenging natural language processing problems. Word embeddings are in fact a class of techniques where individual words are represented as real-valued vectors in a predefined vector <b>space</b>. Each ...", "dateLastCrawled": "2022-02-02T08:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Word Embedding Tutorial | Word2vec</b> Model Gensim Example", "url": "https://www.guru99.com/word-embedding-word2vec.html", "isFamilyFriendly": true, "displayUrl": "https://www.guru99.com/word-<b>embedding</b>-word2vec.html", "snippet": "Word <b>Embedding</b> is a word representation type that allows machine learning algorithms to understand words with <b>similar</b> meanings. It is a language modeling and feature learning technique to <b>map</b> words into vectors of real numbers using neural networks, probabilistic models, or dimension reduction on the word co-occurrence matrix. Some word <b>embedding</b> models are Word2vec (Google), Glove (Stanford), and fastest (Facebook).", "dateLastCrawled": "2022-02-02T04:59:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Embedding Space</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/computer-science/embedding-space", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/computer-science/<b>embedding-space</b>", "snippet": "If the CNN has learned correctly to <b>map</b> images to the text <b>embedding space</b>, the distances between the embeddings of the images and the texts of a pair should be similar, and points in the plot should fall around the identity line y = x. Also, if the learned <b>space</b> has a semantic structure, both the distance between images embeddings and the distance between texts embeddings should be smaller for those pairs sharing more tags: The plot points&#39; color reflects the number of common tags of the ...", "dateLastCrawled": "2022-01-20T05:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Where are we in <b>embedding</b> spaces? A Comprehensive Analysis on Network ...", "url": "https://deepai.org/publication/where-are-we-in-embedding-spaces-a-comprehensive-analysis-on-network-embedding-approaches-for-recommender-systems", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/where-are-we-in-<b>embedding</b>-<b>spaces</b>-a-comprehensive...", "snippet": "Meanwhile, hyperbolic <b>space</b> <b>can</b> <b>be thought</b> as a continuous version of trees. Therefore, such networks are consistent with hyperbolic <b>space</b> due to their analogous structure, and <b>can</b> be naturally modeled by hyperbolic <b>space</b> with a much lower distortion compared to Euclidean <b>space</b>. Although hyperbolic embeddings are gaining great attention for recommender systems nowadays (liu2019hyperbolic; wang2019hyperbolic; chami2019hyperbolic; chamberlain2019scalable), it is not clear under what ...", "dateLastCrawled": "2022-01-17T00:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Graph <b>Embedding</b>: Understanding Graph <b>Embedding</b> Algorithms", "url": "https://www.tigergraph.com/blog/understanding-graph-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://www.tigergraph.com/blog/<b>understanding-graph-embeddings</b>", "snippet": "There may not be \u201csemantics\u201d or meaning associated with each number in an <b>embedding</b>. Embeddings <b>can</b> <b>be thought</b> of as a low-dimensional representation of an item in a vector <b>space</b>. Items that are near each other in this <b>embedding</b> <b>space</b> are considered similar to each other in the real world. Embeddings focus on performance, not explainability. Embeddings are ideal for \u201cfuzzy\u201d match problems. If you have hundreds or thousands of lines of complex if-then statements to build cohorts ...", "dateLastCrawled": "2022-02-03T02:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Document <b>Embedding</b> Techniques. A review of notable literature on the ...", "url": "https://towardsdatascience.com/document-embedding-techniques-fed3e7a6a25d", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/document-<b>embedding</b>-techniques-fed3e7a6a25d", "snippet": "Document <b>embedding</b> approaches. A possible way to <b>map</b> the field is into the following four prominent approaches: ... This, of course, <b>can</b> be taken as an <b>embedding</b> <b>space</b> for these documents, and \u2014 depending on the choice of K \u2014 it <b>can</b> be of a significantly smaller dimension than vocabulary-based ones. Indeed, while a main use case for LDA is unsupervised topic/community discovery, other cases include the use of the resulting latent topic <b>space</b> as an <b>embedding</b> <b>space</b> for the document corpus ...", "dateLastCrawled": "2022-02-03T01:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Spectral <b>Embedding</b> and Locally Linear <b>Embedding</b>", "url": "http://www.mustafahajij.com/wp-content/uploads/2018/07/Locally-Linear-Embedding-and-Spectral-Embedding.pdf", "isFamilyFriendly": true, "displayUrl": "www.mustafahajij.com/wp-content/uploads/2018/07/Locally-Linear-<b>Embedding</b>-and-Spectral...", "snippet": "Consider the digit dataset. This dataset <b>can</b> <b>be thought</b> of as a high-dimensional data with =64. So every image <b>can</b> <b>be thought</b> of as a vector =[ 1,\u2026, 64] Spectral <b>embedding</b> assigns to the point x new coordinates =[ 1,\u2026, \ud835\udc58]where \ud835\udc58\u226464. Usually we choose &lt;&lt;\ud835\udc58. In the example above we choose \ud835\udc58=2.", "dateLastCrawled": "2021-09-17T11:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Embedding</b> Story Maps in websites and blogs", "url": "https://www.esri.com/arcgis-blog/products/arcgis-online/mapping/embedding-story-maps-in-websites-and-blogs/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.esri.com</b>/<b>arcgis-blog</b>/products/arcgis-online/<b>map</b>ping/<b>embedding</b>-story-<b>map</b>s...", "snippet": "Many story maps are intended for standalone use in a browser, but they <b>can</b> also be embedded in a website or blog. <b>Embedding</b> any story <b>map</b> is easy, follow these steps to learn how. * Embed your Story <b>Map</b>. Story Maps <b>can</b> be embedded using an iFrame tag, but the needed HTML is automatically generated for you, so you don\u2019t even need to know what an iFrame is. Step 1: Sign in to Story Maps. Go to the story maps website at storymaps.arcgis.com and sign in to your account. Look for Sign In at the ...", "dateLastCrawled": "2022-01-31T16:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "algebraic geometry - how do I show that Segre <b>embedding</b> is a ...", "url": "https://math.stackexchange.com/questions/2335995/how-do-i-show-that-segre-embedding-is-a-homeomorphism", "isFamilyFriendly": true, "displayUrl": "https://math.stackexchange.com/questions/2335995/how-do-i-show-that-segre-<b>embedding</b>-is...", "snippet": "$\\begingroup$ The Segre <b>embedding</b> is a <b>map</b> $\\Bbb P^n\\times \\Bbb P^m \\to \\Bbb P^{(n+1)(m+1)-1}$. This is, at a first glance, rather far away from checking that the diagonal is closed, which you <b>can</b> prove much more easily in other ways (take an affine patch, compute, etc). If your question is something more along the lines of &quot;I want to prove that the diagonal <b>embedding</b> of $\\Bbb P^n\\hookrightarrow \\Bbb P^n\\times\\Bbb P^n$ via the Segre <b>embedding</b>&quot;, you should show your <b>thought</b> process for how ...", "dateLastCrawled": "2022-01-23T17:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "differential geometry - Image of <b>Embedding</b> on a Manifold is a ...", "url": "https://math.stackexchange.com/questions/3332499/image-of-embedding-on-a-manifold-is-a-submanifold", "isFamilyFriendly": true, "displayUrl": "https://math.stackexchange.com/questions/3332499/image-of-<b>embedding</b>-on-a-manifold-is-a...", "snippet": "$\\begingroup$ actually wait I&#39;m confused by your question, did you phrase it correctly? because an <b>embedding</b> is by definition a homeomorphism and hence trivially an open <b>map</b> onto its mage. So, could you clarify whether what you actually wrote is what you intended? I feel like my answer doesnt answer your question as stated, but perhaps might answer what you actually intended to ask $\\endgroup$ \u2013 peek-a-boo", "dateLastCrawled": "2022-01-18T04:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Skip-Thought Vectors</b> - NIPS", "url": "https://papers.nips.cc/paper/2015/file/f442d33fa06832082290ad8544a8da27-Paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://papers.nips.cc/paper/2015/file/f442d33fa06832082290ad8544a8da27-Paper.pdf", "snippet": "that appears in word2vec <b>can</b> then get a vector in the encoder word <b>embedding</b> <b>space</b>. 2 Approach 2.1 Inducing <b>skip-thought vectors</b> We treat skip-thoughts in the framework of encoder-decoder models 1 . That is, an encoder maps words to a sentence vector and a decoder is used to generate the surrounding sentences. Encoder-decoder models have gained a lot of traction for neural machine translation. In this setting, an encoder is used to <b>map</b> e.g. an English sentence into a vector. The decoder then ...", "dateLastCrawled": "2022-01-24T18:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Visualizing feature <b>vectors</b>/embeddings using t-SNE and PCA | by ...", "url": "https://towardsdatascience.com/visualizing-feature-vectors-embeddings-using-pca-and-t-sne-ef157cea3a42", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/visualizing-feature-<b>vectors</b>-<b>embeddings</b>-using-pca-and-t...", "snippet": "Visualization is a very powerful tool and <b>can</b> provide invaluable information. In this post, I\u2019ll be discussing two very powerful techniques that <b>can</b> help you visualise higher dimensional data in a lower-dimensional <b>space</b> to find trends and patterns, namely PCA and t-SNE. We will be taking a CNN based example and inject noise in the test dataset to do our visualization investigation. Introduction. I\u2019ll briefly talk about the two techniques before diving into how to use them. Principal ...", "dateLastCrawled": "2022-02-03T13:36:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Embedding Space</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/computer-science/embedding-space", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/computer-science/<b>embedding-space</b>", "snippet": "If the CNN has learned correctly to <b>map</b> images to the text <b>embedding space</b>, the distances between the embeddings of the images and the texts of a pair should be similar, and points in the plot should fall around the identity line y = x. Also, if the learned <b>space</b> has a semantic structure, both the distance between images embeddings and the distance between texts embeddings should be smaller for those pairs sharing more tags: The plot points&#39; color reflects the number of common tags of the ...", "dateLastCrawled": "2022-01-20T05:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Where are we in <b>embedding</b> spaces? A Comprehensive Analysis on Network ...", "url": "https://deepai.org/publication/where-are-we-in-embedding-spaces-a-comprehensive-analysis-on-network-embedding-approaches-for-recommender-systems", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/where-are-we-in-<b>embedding</b>-<b>spaces</b>-a-comprehensive...", "snippet": "Meanwhile, hyperbolic <b>space</b> <b>can</b> be thought as a continuous version of trees. Therefore, such networks are consistent with hyperbolic <b>space</b> due to their analogous structure, and <b>can</b> be naturally modeled by hyperbolic <b>space</b> with a much lower distortion <b>compared</b> to Euclidean <b>space</b>. Although hyperbolic embeddings are gaining great attention for recommender systems nowadays (liu2019hyperbolic; wang2019hyperbolic; chami2019hyperbolic; chamberlain2019scalable), it is not clear under what ...", "dateLastCrawled": "2022-01-17T00:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Embedding</b> in Machine Learning | Cathy\u2019s Notes", "url": "https://cathyqian.github.io/2020/08/27/Embedding_In_Machine_Learning.html", "isFamilyFriendly": true, "displayUrl": "https://cathyqian.github.io/2020/08/27/<b>Embedding</b>_In_Machine_Learning.html", "snippet": "What is <b>Embedding</b>? <b>Embedding</b> in the context of deep learning is to <b>map</b> high-dimensional vectors or categorical variables to relatively low-dimensional <b>space</b> so that similar items are close to each other. It <b>can</b> be applied to any high dimensional, sparse or categorical features, i.e., IP addresses from ad impressions, raw pixels in a video, audio data from speech, texts from a job description.", "dateLastCrawled": "2022-01-22T06:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Lexical Comparison Using Word <b>Embedding</b> Mapping from an Academic Word ...", "url": "https://uu.diva-portal.org/smash/get/diva2:1500889/FULLTEXT01.pdf", "isFamilyFriendly": true, "displayUrl": "https://uu.diva-portal.org/smash/get/diva2:1500889/FULLTEXT01.pdf", "snippet": "2. <b>Can</b> the vector <b>space</b> transformation method borrowed from cross-lingual <b>map</b>-ping methods e\u02dbectively re\u02daect word usage di\u02dberences? There are two challenges in this thesis projects: 1. One basis of cross-lingual <b>embedding</b> is the observation that the two equivalent words in two di\u02dberent languages should have a similar distribution (Mikolov,", "dateLastCrawled": "2021-02-01T02:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Joint embedding: A scalable alignment to compare individuals in</b> a ...", "url": "https://www.sciencedirect.com/science/article/pii/S1053811920307187", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1053811920307187", "snippet": "Placing the diffusion <b>map</b> <b>embedding</b> on W joint results in a set of comparable <b>embedding</b> components E. For each component the first n 1 nodes represent the latent position of C 1 and the following n 2 nodes for C 2 in the common connectivity <b>space</b>. Following prior studies (Margulies et al., 2016; Paquola et al., 2019), the functional connectivity matrices are calculated with Pearson&#39;s correlation and thresholded at the top 10% strongest connectivity at each vertex. In the current study, the ...", "dateLastCrawled": "2022-01-29T00:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "pca - What is <b>embedding</b>? (in <b>the context of dimensionality reduction</b> ...", "url": "https://stats.stackexchange.com/questions/487545/what-is-embedding-in-the-context-of-dimensionality-reduction", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/487545/what-is-<b>embedding</b>-in-the-context-of...", "snippet": "An <b>embedding</b> is a relatively low-dimensional <b>space</b> [subspace] into which you <b>can</b> translate high-dimensional vectors. Embeddings make it easier to do machine learning on large inputs like sparse vectors representing words. Ideally, an <b>embedding</b> captures some of the semantics of the input by placing semantically similar inputs close together in the <b>embedding</b> <b>space</b>", "dateLastCrawled": "2022-01-23T04:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Document <b>Embedding</b> Techniques. A review of notable literature on the ...", "url": "https://towardsdatascience.com/document-embedding-techniques-fed3e7a6a25d", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/document-<b>embedding</b>-techniques-fed3e7a6a25d", "snippet": "Document <b>embedding</b> approaches. A possible way to <b>map</b> the field is into the following four prominent approaches: Summarizing word vectors This is the classic approach. Bag-of-words does exactly this for one-hot word vectors, and the various weighing schemes you <b>can</b> apply to it are variations on this way to summarizing word vectors. However, this approach is also valid when used with the most state-of-the-art word representations (usually by averaging instead of summing), especially when word ...", "dateLastCrawled": "2022-02-03T01:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "[2111.00622] Deep Recursive <b>Embedding</b> for High-Dimensional Data", "url": "https://arxiv.org/abs/2111.00622", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/abs/2111.00622", "snippet": "<b>Embedding</b> high-dimensional data onto a low-dimensional manifold is of both theoretical and practical value. In this paper, we propose to combine deep neural networks (DNN) with mathematics-guided <b>embedding</b> rules for high-dimensional data <b>embedding</b>. We introduce a generic deep <b>embedding</b> network (DEN) framework, which is able to learn a parametric mapping from high-dimensional <b>space</b> to low-dimensional <b>space</b>, guided by well-established objectives such as Kullback-Leibler (KL) divergence ...", "dateLastCrawled": "2021-12-18T14:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Loc2Vec: Learning location embeddings with triplet</b>-loss ... - Sentiance", "url": "https://sentiance.com/2018/05/03/loc2vec-learning-location-embeddings-w-triplet-loss-networks/", "isFamilyFriendly": true, "displayUrl": "https://www.sentiance.com/2018/05/03/loc2vec-learning-location-embedd", "snippet": "In the next few paragraphs we will discuss how we designed a solution that learned to <b>map</b> location coordinates into a metric <b>space</b> that allows us to do similar things, as illustrated by figure 3. Figure 3: The proposed solution directly optimizes a metric <b>space</b>, such that basic arithmetic operations <b>can</b> be used to explore the <b>embedding</b> <b>space</b>. Tile generation Rasterizing GIS data. Given a location coordinate and a radius, we <b>can</b> query our GIS database to obtain a large amount of geographical ...", "dateLastCrawled": "2022-01-19T01:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Exploring <b>Embedding Methods in Binary Hyperdimensional Computing</b>: A ...", "url": "https://deepai.org/publication/exploring-embedding-methods-in-binary-hyperdimensional-computing-a-case-study-for-motor-imagery-based-brain-computer-interfaces", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/exploring-<b>embedding-methods-in-binary-hyperdimensional</b>...", "snippet": "The main challenge is however to formulate <b>embedding</b> methods that <b>map</b> biosignal measures to a binary HD <b>space</b>. In this paper, we explore variety of such <b>embedding</b> methods and examine them with a challenging application of motor imagery brain-computer interface (MI-BCI) from electroencephalography (EEG) recordings. We explore <b>embedding</b> methods including random projections, quantization based thermometer and Gray coding, and learning HD representations using end-to-end training. All these ...", "dateLastCrawled": "2021-12-17T09:53:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Finding the Word <b>Analogy</b> from given words using Word2Vec embeddings ...", "url": "https://www.geeksforgeeks.org/finding-the-word-analogy-from-given-words-using-word2vec-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/finding-the-word-<b>analogy</b>-from-given-words-using-word2vec...", "snippet": "In the word <b>analogy</b> task, ... What if we can use a <b>Machine</b> <b>Learning</b> algorithm to automate this task of finding the word <b>analogy</b>. In this tutorial, we will be using Word2Vec model and a pre-trained model named \u2018GoogleNews-vectors-negative300.bin\u2018 which is trained on over 50 Billion words by Google. Each word inside the pre-trained dataset is embedded in a 300-dimensional <b>space</b> and the words which are similar in context/meaning are placed closer to each other in the <b>space</b>. Methodology to ...", "dateLastCrawled": "2022-01-26T14:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Analogical <b>Embedding</b> for <b>Analogy</b>-Based <b>Learning</b> to Rank", "url": "https://link.springer.com/chapter/10.1007/978-3-030-74251-5_7", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/chapter/10.1007/978-3-030-74251-5_7", "snippet": "A suitable representation is therefore essential for the success of <b>analogy</b>-based <b>learning</b> to rank. Therefore, we propose a method for analogical <b>embedding</b>, i.e., for <b>embedding</b> the data in a target <b>space</b> such that, in this <b>space</b>, the aforementioned <b>analogy</b> assumption is as valid and strongly pronounced as possible. This is accomplished by means of a neural network with a quadruple Siamese structure, which is trained on a suitably designed set of examples in the form of quadruples of objects ...", "dateLastCrawled": "2022-01-17T09:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Analogical <b>Embedding</b> for <b>Analogy</b>-Based <b>Learning</b> to Rank", "url": "https://homepages.uni-paderborn.de/ahmadim/IDA%202021.pdf", "isFamilyFriendly": true, "displayUrl": "https://homepages.uni-paderborn.de/ahmadim/IDA 2021.pdf", "snippet": "7 Intelligent Systems and <b>Machine</b> <b>Learning</b> <b>Embedding</b> By ignoring irrelevant or noisy features, the performance can often be improved Common feature selection techniques tailored for the case of <b>analogy</b>-based <b>learning</b> to rank. <b>Analogy</b>-based <b>learning</b> to rank (able2rank) 8 Intelligent Systems and <b>Machine</b> <b>Learning</b> Extension to feature vectors Degree of <b>analogy</b>. Analogical <b>Embedding</b> 9 Intelligent Systems and <b>Machine</b> <b>Learning</b> Positive example: preferences on both sides are coherent Negative ...", "dateLastCrawled": "2022-01-06T10:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Towards Analogy-Based Explanations in Machine Learning</b> | DeepAI", "url": "https://deepai.org/publication/towards-analogy-based-explanations-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>towards-analogy-based-explanations-in-machine-learning</b>", "snippet": "More specifically, we take the view that an <b>analogy</b>-based approach is a viable alternative to existing approaches in the realm of explainable AI and interpretable <b>machine</b> <b>learning</b>, and that <b>analogy</b>-based explanations of the predictions produced by a <b>machine</b> <b>learning</b> algorithm can complement similarity-based explanations in a meaningful way. To corroborate these claims, we outline the basic idea of an <b>analogy</b>-based explanation and illustrate its potential usefulness by means of some examples.", "dateLastCrawled": "2022-01-10T12:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Classifying and completing word analogies by <b>machine</b> <b>learning</b> ...", "url": "https://www.sciencedirect.com/science/article/abs/pii/S0888613X21000141", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/abs/pii/S0888613X21000141", "snippet": "With the emergence of word <b>embedding</b> models, a lot of progress has been made in NLP, essentially assuming that a word <b>analogy</b> like m a n: k i n g:: w o m a n: q u e e n is an instance of a parallelogram within the underlying vector <b>space</b>. In this paper, we depart from this assumption to adopt a <b>machine</b> <b>learning</b> approach, i.e., <b>learning</b> a substitute of the parallelogram model. To achieve our goal, we first review the formal modeling of analogical proportions, highlighting the properties which ...", "dateLastCrawled": "2021-11-13T01:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "How to Solve <b>Analogies</b> with Word2Vec | by Khuyen Tran | Towards Data ...", "url": "https://towardsdatascience.com/how-to-solve-analogies-with-word2vec-6ebaf2354009", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/how-to-solve-<b>analogies</b>-with-word2vec-6ebaf2354009", "snippet": "To make words understood by <b>machine</b> <b>learning</b> algorithms, word <b>embedding</b> is used to map words into vectors of real numbers. There are various word <b>embedding</b> models and word2vec is one of them. In simple words, word2vec is a group of related models that are used to produce word embeddings. These models are trained to construct the linguistic contexts of words. Word2vec takes a large corpus of text and produces a vector <b>space</b>, with each unique word in the corpus being assigned to a ...", "dateLastCrawled": "2022-01-30T16:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Graph <b>Embedding</b> for Deep <b>Learning</b> | by Flawnson Tong | Towards Data Science", "url": "https://towardsdatascience.com/overview-of-deep-learning-on-graph-embeddings-4305c10ad4a4", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/overview-of-deep-<b>learning</b>-on-graph-<b>embeddings</b>-4305c10ad4a4", "snippet": "Using an <b>analogy</b> with word2vec, if a document is made of sentences (which is then made of words), then a graph is made of sub-graphs ... Graph <b>embedding</b> techniques take graphs and embed them in a lower dimensional continuous latent <b>space</b> before passing that representation through a <b>machine</b> <b>learning</b> model. Walk <b>embedding</b> methods perform graph traversals with the goal of preserving structure and features and aggregates these traversals which can then be passed through a recurrent neural ...", "dateLastCrawled": "2022-02-01T16:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "In the word <b>embedding</b> <b>space</b>, the <b>analogy</b> pairs exhibit interesting ...", "url": "https://www.researchgate.net/figure/In-the-word-embedding-space-the-analogy-pairs-exhibit-interesting-algebraic_fig1_319370400", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/figure/In-the-word-<b>embedding</b>-<b>space</b>-the-<b>analogy</b>-pairs...", "snippet": "Download scientific diagram | In the word <b>embedding</b> <b>space</b>, the <b>analogy</b> pairs exhibit interesting algebraic relationships. from publication: Visual Exploration of Semantic Relationships in Neural ...", "dateLastCrawled": "2021-12-21T11:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>What Are Word Embeddings</b> for Text? - <b>Machine</b> <b>Learning</b> Mastery", "url": "https://machinelearningmastery.com/what-are-word-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>what-are-word-embeddings</b>", "snippet": "This approach of <b>learning</b> an <b>embedding</b> layer requires a lot of training data and can be slow, but will learn an <b>embedding</b> both targeted to the specific text data and the NLP task. 2. Word2Vec. Word2Vec is a statistical method for efficiently <b>learning</b> a standalone word <b>embedding</b> from a text corpus. It was developed by Tomas Mikolov, et al. at Google in 2013 as a response to make the neural-network-based training of the <b>embedding</b> more efficient and since then has become the de facto standard ...", "dateLastCrawled": "2022-01-30T18:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Word Embeddings in NLP - GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/word-embeddings-in-nlp/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/word-<b>embeddings</b>-in-nlp", "snippet": "Word <b>Embedding</b> or Word Vector is a numeric vector input that represents a word in a lower-dimensional <b>space</b>. It allows words with similar meaning to have a similar representation. They can also approximate meaning. A word vector with 50 values can represent 50 unique features. Features: Anything that relates words to one another. Eg: Age, Sports, Fitness, Employed etc. Each word vector has values corresponding to these features. Goal of Word Embeddings. To reduce dimensionality; To use a ...", "dateLastCrawled": "2022-01-30T08:38:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Zero-shot <b>learning</b> via discriminative representation extraction ...", "url": "https://www.sciencedirect.com/science/article/pii/S0167865517303501", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0167865517303501", "snippet": "The pioneer work in ZSL can be traced to Larochelle et al. , where it verified that when test images belong to some classes that are not available at training stage, a <b>machine</b> <b>learning</b> system can still figure out what a test image is. Due to the importance of zero-shot <b>learning</b>, the number of proposed approaches has increased steadily recently.The number of new zero-shot <b>learning</b> approaches proposed every year was increasing.", "dateLastCrawled": "2021-10-30T07:08:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A self-supervised domain-general <b>learning</b> framework for human ventral ...", "url": "https://www.nature.com/articles/s41467-022-28091-4", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41467-022-28091-4", "snippet": "On this view, the <b>embedding space can be thought of as</b> a high-fidelity perceptual interface, with useful visual primitives over which separate conceptual representational systems can operate.", "dateLastCrawled": "2022-01-25T10:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Spectral Af\ufb01ne-Kernel Embeddings</b> - NSF", "url": "https://par.nsf.gov/servlets/purl/10039348", "isFamilyFriendly": true, "displayUrl": "https://par.nsf.gov/servlets/purl/10039348", "snippet": "Since <b>machine</b> <b>learn-ing</b> algorithms struggle with high dimensions (an issue known as the curse of dimensionality in this context), one typically needs to map these data points from their high-dimensional space into a lower dimensional space without signi\ufb01cant distortion. Mapping data (living in RD with D\u02db1 but sampling a manifold of low in-trinsic dimensionality d \u02ddD) into a low-dimensional <b>embedding space can be thought of as</b> a preliminary feature extraction step in <b>machine</b> <b>learning</b> ...", "dateLastCrawled": "2022-01-29T21:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Predicting affinity ties in a surname network", "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0256603", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0256603", "snippet": "<b>Machine</b> <b>learning</b>-based approaches for knowledge graph completion To cover the broadest possible range of methods and architectures in the evaluation, we identified representative methods of different model families, taking care that these methods achieve state-of-the-art performances in knowledge graph completion and have open-source implementations that favor the reproducibility of the reported results.", "dateLastCrawled": "2021-09-03T03:18:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(embedding space)  is like +(a map)", "+(embedding space) is similar to +(a map)", "+(embedding space) can be thought of as +(a map)", "+(embedding space) can be compared to +(a map)", "machine learning +(embedding space AND analogy)", "machine learning +(\"embedding space is like\")", "machine learning +(\"embedding space is similar\")", "machine learning +(\"just as embedding space\")", "machine learning +(\"embedding space can be thought of as\")", "machine learning +(\"embedding space can be compared to\")"]}