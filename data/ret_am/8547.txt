{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Softmax Classifiers Explained</b> - PyImageSearch", "url": "https://www.pyimagesearch.com/2016/09/12/softmax-classifiers-explained/", "isFamilyFriendly": true, "displayUrl": "https://www.pyimagesearch.com/2016/09/12/<b>softmax-classifiers-explained</b>", "snippet": "Just <b>like</b> in <b>hinge</b> <b>loss</b> or <b>squared</b> <b>hinge</b> <b>loss</b>, ... When constructing Deep Learning and Convolutional <b>Neural</b> <b>Network</b> models, you\u2019ll undoubtedly run in to the Softmax classifier and the cross-entropy <b>loss</b> function. While both <b>hinge</b> <b>loss</b> and <b>squared</b> <b>hinge</b> <b>loss</b> are popular choices, I can almost guarantee with absolute certainly that you\u2019ll see cross-entropy <b>loss</b> with more frequency \u2014 this is mainly due to the fact that the Softmax classifier outputs probabilities rather than margins ...", "dateLastCrawled": "2022-01-31T11:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Algorithms in the Machine Learning Toolkit - Splunk Documentation", "url": "https://docs.splunk.com/Documentation/MLApp/5.3.1/User/Algorithms", "isFamilyFriendly": true, "displayUrl": "https://docs.splunk.com/Documentation/MLApp/5.3.1/User/Algorithms", "snippet": "The <b>loss</b>=&lt;<b>hinge</b>|log|modified_huber|<b>squared</b>_<b>hinge</b>|perceptron&gt; parameter is the <b>loss</b> function to be used. Defaults to <b>hinge</b>, which gives a linear SVM. The log <b>loss</b> gives logistic regression, a probabilistic classifier. modified_huber is another smooth <b>loss</b> that brings tolerance to outliers as <b>well</b> as probability estimates. <b>squared</b>_<b>hinge</b> <b>is like</b> <b>hinge</b> but is quadratically penalized. perceptron is the linear <b>loss</b> used by the perceptron algorithm. The fit_intercept=&lt;true|false&gt; parameter ...", "dateLastCrawled": "2022-01-31T16:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "How to Choose <b>Loss</b> Functions When Training Deep Learning <b>Neural</b> Networks", "url": "https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/how-to-choose-<b>loss</b>-functions-when-training-deep...", "snippet": "<b>Neural</b> <b>network</b> models learn a mapping from inputs to outputs from examples and the choice of <b>loss</b> function must match the framing of the specific predictive modeling problem, such as classification or regression. Further, the configuration of the output layer must also be appropriate for the chosen <b>loss</b> function.", "dateLastCrawled": "2022-02-02T15:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "GitHub - jeevu94/ml-notes: Personal notes on ML", "url": "https://github.com/jeevu94/ml-notes", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/jeevu94/ml-notes", "snippet": "The <b>hinge</b> <b>loss</b> is used for &quot;maximum-margin&quot; classification, most notably for support vector machines (SVMs). To summarize, when working with an SVM, if a computed value gives a correct classification and is larger than the margin, there is no <b>hinge</b> <b>loss</b>. If a computed value gives a correct classification but is too close to zero (where too close is defined by a margin) there is a small <b>hinge</b> <b>loss</b>. If a computed value gives an incorrect classification there will always be a <b>hinge</b> <b>loss</b> ...", "dateLastCrawled": "2022-01-20T20:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>neural</b> networks - <b>Constructing a problem-specific loss function</b> - Cross ...", "url": "https://stats.stackexchange.com/questions/183953/constructing-a-problem-specific-loss-function", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/183953/<b>constructing-a-problem-specific-loss</b>...", "snippet": "You can use <b>hinge</b> <b>loss</b> which is an upper bound on the classification <b>loss</b>; that is, it penalizes the model if the label of the highest <b>scoring</b> category is different from the label of the ground-truth class.. For more details on the relation between classification <b>loss</b> and <b>hinge</b> <b>loss</b> you can read Section 2 of this awesome paper from C.N. J. Yu and T. Joachims.. In summary, there is a task <b>loss</b>, usually denoted by $\\Delta \\left( y_i, \\hat{y}(x_i) \\right)$, which measures the penalty for ...", "dateLastCrawled": "2022-01-21T15:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Loss</b> and <b>Loss</b> <b>Functions for Training Deep Learning Neural Networks</b>", "url": "https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>loss</b>-and-<b>loss</b>-<b>functions-for-training-deep-learning</b>...", "snippet": "<b>Neural</b> networks are trained using stochastic gradient descent and require that you choose a <b>loss</b> function when designing and configuring <b>your</b> model. There are many <b>loss</b> functions to choose from and it can be challenging to know what to choose, or even what a <b>loss</b> function is and the role it plays when training a <b>neural</b> <b>network</b>. In this post, you will", "dateLastCrawled": "2022-02-02T20:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Support Vector Machine</b> \u2014 Introduction to Machine Learning Algorithms ...", "url": "https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>support-vector-machine</b>-introduction-to-machine-learning...", "snippet": "The <b>loss</b> function that helps maximize the margin is <b>hinge</b> <b>loss</b>. <b>Hinge</b> <b>loss</b> function (function on left can be represented as a function on the right) The cost is 0 if the predicted value and the actual value are of the same sign. If they are not, we then calculate the <b>loss</b> value. We also add a regularization parameter the cost function. The objective of the regularization parameter is to balance the margin maximization and <b>loss</b>. After adding the regularization parameter, the cost functions ...", "dateLastCrawled": "2022-02-03T00:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "3.3. Metrics and <b>scoring</b>: quantifying the quality of predictions ...", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "isFamilyFriendly": true, "displayUrl": "https://scikit-learn.org/stable/modules/model_evaluation.html", "snippet": "Log <b>loss</b>, also called logistic regression <b>loss</b> or cross-entropy <b>loss</b>, is defined on probability estimates. It is commonly used in (multinomial) logistic regression and <b>neural</b> networks, as <b>well</b> as in some variants of expectation-maximization, and can be used to evaluate the probability outputs ( predict_proba ) of a classifier instead of its discrete predictions.", "dateLastCrawled": "2022-02-03T06:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "An SVM-<b>like Approach for Expectile Regression</b> | Request PDF", "url": "https://www.researchgate.net/publication/280104219_An_SVM-like_Approach_for_Expectile_Regression", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/280104219_An_SVM-<b>like</b>_Approach_for_Expectile...", "snippet": "While the literature on expectile regression is less extended, <b>neural</b> <b>network</b> [Jiang et al., 2017] or SVM-<b>like</b> approaches [Farooq and Steinwart, 2017] have been developed as <b>well</b>. All the ...", "dateLastCrawled": "2022-01-20T18:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "170 Machine Learning Interview Question and Answers in 2022", "url": "https://www.mygreatlearning.com/blog/machine-learning-interview-questions/", "isFamilyFriendly": true, "displayUrl": "https://www.mygreatlearning.com/blog/<b>machine-learning-interview-questions</b>", "snippet": "For the Bayesian <b>network</b> as a classifier, the features are selected based on some <b>scoring</b> functions <b>like</b> Bayesian <b>scoring</b> function and minimal description length(the two are equivalent in theory to each other given that there is enough training data). The <b>scoring</b> functions mainly restrict the structure (connections and directions) and the ...", "dateLastCrawled": "2022-02-03T05:58:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Softmax Classifiers Explained</b> - PyImageSearch", "url": "https://www.pyimagesearch.com/2016/09/12/softmax-classifiers-explained/", "isFamilyFriendly": true, "displayUrl": "https://www.pyimagesearch.com/2016/09/12/<b>softmax-classifiers-explained</b>", "snippet": "When constructing Deep Learning and Convolutional <b>Neural</b> <b>Network</b> models, ... While both <b>hinge</b> <b>loss</b> and <b>squared</b> <b>hinge</b> <b>loss</b> are popular choices, I can almost guarantee with absolute certainly that you\u2019ll see cross-entropy <b>loss</b> with more frequency \u2014 this is mainly due to the fact that the Softmax classifier outputs probabilities rather than margins. Probabilities are much easier for us as humans to interpret, so that is a particularly nice quality of Softmax classifiers. Now that we ...", "dateLastCrawled": "2022-01-31T11:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Algorithms in the Machine Learning Toolkit - Splunk Documentation", "url": "https://docs.splunk.com/Documentation/MLApp/5.3.1/User/Algorithms", "isFamilyFriendly": true, "displayUrl": "https://docs.splunk.com/Documentation/MLApp/5.3.1/User/Algorithms", "snippet": "The <b>loss</b>=&lt;<b>hinge</b>|log|modified_huber|<b>squared</b>_<b>hinge</b>|perceptron&gt; parameter is the <b>loss</b> function to be used. Defaults to <b>hinge</b>, which gives a linear SVM. The log <b>loss</b> gives logistic regression, a probabilistic classifier. modified_huber is another smooth <b>loss</b> that brings tolerance to outliers as <b>well</b> as probability estimates. <b>squared</b>_<b>hinge</b> is like <b>hinge</b> but is quadratically penalized. perceptron is the linear <b>loss</b> used by the perceptron algorithm. The fit_intercept=&lt;true|false&gt; parameter ...", "dateLastCrawled": "2022-01-31T16:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "How to Choose <b>Loss</b> Functions When Training Deep Learning <b>Neural</b> Networks", "url": "https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/how-to-choose-<b>loss</b>-functions-when-training-deep...", "snippet": "<b>Neural</b> <b>network</b> models learn a mapping from inputs to outputs from examples and the choice of <b>loss</b> function must match the framing of the specific predictive modeling problem, such as classification or regression. Further, the configuration of the output layer must also be appropriate for the chosen <b>loss</b> function.", "dateLastCrawled": "2022-02-02T15:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "1.5. Stochastic <b>Gradient Descent</b> \u2014 scikit-learn 1.0.2 documentation", "url": "https://scikit-learn.org/stable/modules/sgd.html", "isFamilyFriendly": true, "displayUrl": "https://scikit-learn.org/stable/modules/sgd.html", "snippet": "1.5.1. Classification\u00b6. The class SGDClassifier implements a plain stochastic <b>gradient descent</b> learning routine which supports different <b>loss</b> functions and penalties for classification. Below is the decision boundary of a SGDClassifier trained with the <b>hinge</b> <b>loss</b>, equivalent to a linear SVM. As other classifiers, SGD has to be fitted with two arrays: an array X of shape (n_samples, n_features) holding the training samples, and an array y of shape (n_samples,) holding the target values ...", "dateLastCrawled": "2022-02-03T05:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "3.3. Metrics and <b>scoring</b>: quantifying the quality of predictions ...", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "isFamilyFriendly": true, "displayUrl": "https://scikit-learn.org/stable/modules/model_evaluation.html", "snippet": "Log <b>loss</b>, also called logistic regression <b>loss</b> or cross-entropy <b>loss</b>, is defined on probability estimates. It is commonly used in (multinomial) logistic regression and <b>neural</b> networks, as <b>well</b> as in some variants of expectation-maximization, and can be used to evaluate the probability outputs ( predict_proba ) of a classifier instead of its discrete predictions.", "dateLastCrawled": "2022-02-03T06:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) <b>Automatic Text Scoring Using Neural Networks</b>", "url": "https://www.researchgate.net/publication/306093850_Automatic_Text_Scoring_Using_Neural_Networks", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/.../306093850_<b>Automatic_Text_Scoring_Using_Neural_Networks</b>", "snippet": "the <b>hinge</b> <b>loss</b> which ensures that the activations . of the original and \u2018noisy\u2019 n grams will differ by. at least 1: <b>loss</b> context (tar get, cor rupt) = [1 \u2212 f (s t) + f (s ck)] +, \u2200 k \u2208 Z ...", "dateLastCrawled": "2022-01-16T15:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>neural</b> networks - <b>Constructing a problem-specific loss function</b> - Cross ...", "url": "https://stats.stackexchange.com/questions/183953/constructing-a-problem-specific-loss-function", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/183953/<b>constructing-a-problem-specific-loss</b>...", "snippet": "You can use <b>hinge</b> <b>loss</b> which is an upper bound on the classification <b>loss</b>; that is, it penalizes the model if the label of the highest <b>scoring</b> category is different from the label of the ground-truth class.. For more details on the relation between classification <b>loss</b> and <b>hinge</b> <b>loss</b> you can read Section 2 of this awesome paper from C.N. J. Yu and T. Joachims.. In summary, there is a task <b>loss</b>, usually denoted by $\\Delta \\left( y_i, \\hat{y}(x_i) \\right)$, which measures the penalty for ...", "dateLastCrawled": "2022-01-21T15:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "GitHub - jeevu94/ml-notes: Personal notes on ML", "url": "https://github.com/jeevu94/ml-notes", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/jeevu94/ml-notes", "snippet": "The <b>hinge</b> <b>loss</b> is used for &quot;maximum-margin&quot; classification, most notably for support vector machines (SVMs). To summarize, when working with an SVM, if a computed value gives a correct classification and is larger than the margin, there is no <b>hinge</b> <b>loss</b>. If a computed value gives a correct classification but is too close to zero (where too close is defined by a margin) there is a small <b>hinge</b> <b>loss</b>. If a computed value gives an incorrect classification there will always be a <b>hinge</b> <b>loss</b> ...", "dateLastCrawled": "2022-01-20T20:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Loss</b> and <b>Loss</b> <b>Functions for Training Deep Learning Neural Networks</b>", "url": "https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>loss</b>-and-<b>loss</b>-<b>functions-for-training-deep-learning</b>...", "snippet": "<b>Neural</b> networks are trained using stochastic gradient descent and require that you choose a <b>loss</b> function when designing and configuring <b>your</b> model. There are many <b>loss</b> functions to choose from and it can be challenging to know what to choose, or even what a <b>loss</b> function is and the role it plays when training a <b>neural</b> <b>network</b>. In this post, you will", "dateLastCrawled": "2022-02-02T20:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "170 Machine Learning Interview Question and Answers in 2022", "url": "https://www.mygreatlearning.com/blog/machine-learning-interview-questions/", "isFamilyFriendly": true, "displayUrl": "https://www.mygreatlearning.com/blog/<b>machine-learning-interview-questions</b>", "snippet": "Deep Learning, on the other hand, is able to learn through processing data on its own and is quite <b>similar</b> to the human brain where it identifies something, analyse it, and makes a decision. The key differences are as follow: The manner in which data is presented to the <b>system</b>. Machine learning algorithms always require structured data and deep learning networks rely on layers of artificial <b>neural</b> networks. 4. What is the main key difference between supervised and unsupervised machine ...", "dateLastCrawled": "2022-02-03T05:58:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Loss functions for binary classification and class probability estimation</b>", "url": "https://www.researchgate.net/publication/45450728_Loss_functions_for_binary_classification_and_class_probability_estimation", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/45450728_<b>Loss</b>_functions_for_binary...", "snippet": "There are many types of <b>loss</b> functions, such as square <b>loss</b>, <b>hinge</b> <b>loss</b>, mean <b>squared</b> error, and cross entropy (Rosasco et al., 2004; Shen, 2014; Rosasco, 2014;Masnadi-Shirazi and Vasconcelos ...", "dateLastCrawled": "2022-01-18T03:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Support Vector Machine</b> \u2014 Introduction to Machine Learning Algorithms ...", "url": "https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>support-vector-machine</b>-introduction-to-machine-learning...", "snippet": "The <b>loss</b> function that helps maximize the margin is <b>hinge</b> <b>loss</b>. <b>Hinge</b> <b>loss</b> function (function on left <b>can</b> be represented as a function on the right) The cost is 0 if the predicted value and the actual value are of the same sign. If they are not, we then calculate the <b>loss</b> value. We also add a regularization parameter the cost function. The objective of the regularization parameter is to balance the margin maximization and <b>loss</b>. After adding the regularization parameter, the cost functions ...", "dateLastCrawled": "2022-02-03T00:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>neural</b> networks - <b>Constructing a problem-specific loss function</b> - Cross ...", "url": "https://stats.stackexchange.com/questions/183953/constructing-a-problem-specific-loss-function", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/183953/<b>constructing-a-problem-specific-loss</b>...", "snippet": "You <b>can</b> use <b>hinge</b> <b>loss</b> which is an upper bound on the classification <b>loss</b>; that is, it penalizes the model if the label of the highest <b>scoring</b> category is different from the label of the ground-truth class.. For more details on the relation between classification <b>loss</b> and <b>hinge</b> <b>loss</b> you <b>can</b> read Section 2 of this awesome paper from C.N. J. Yu and T. Joachims.. In summary, there is a task <b>loss</b>, usually denoted by $\\Delta \\left( y_i, \\hat{y}(x_i) \\right)$, which measures the penalty for ...", "dateLastCrawled": "2022-01-21T15:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "HingeBoost: ROC-Based <b>Boost for Classification and Variable Selection</b>", "url": "https://www.researchgate.net/publication/227378577_HingeBoost_ROC-Based_Boost_for_Classification_and_Variable_Selection", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/227378577_<b>Hinge</b>Boost_ROC-Based_Boost_for...", "snippet": "Wang [76] introduced a gradient boosting approach to optimize the <b>hinge</b>-<b>loss</b> (HingeBoost). The <b>hinge</b> <b>loss</b> is the standard <b>loss</b> function for support vector machines (SVM) and optimizing it is ...", "dateLastCrawled": "2021-12-20T06:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "GitHub - jeevu94/ml-notes: Personal notes on ML", "url": "https://github.com/jeevu94/ml-notes", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/jeevu94/ml-notes", "snippet": "The <b>hinge</b> <b>loss</b> is used for &quot;maximum-margin&quot; classification, most notably for support vector machines (SVMs). To summarize, when working with an SVM, if a computed value gives a correct classification and is larger than the margin, there is no <b>hinge</b> <b>loss</b>. If a computed value gives a correct classification but is too close to zero (where too close is defined by a margin) there is a small <b>hinge</b> <b>loss</b>. If a computed value gives an incorrect classification there will always be a <b>hinge</b> <b>loss</b> ...", "dateLastCrawled": "2022-01-20T20:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Artificial intelligence with deep learning in nuclear medicine and ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8665861/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8665861", "snippet": "Therefore, dropout <b>can</b> <b>be thought</b> of as a way to create and train an ensemble of many subnetworks and thereby improve the generalization performance. Another view on why dropout has a regularizing effect is that it prevents coadaptation of different neurons. By removing different neurons at every iteration, neurons that are included should perform <b>well</b> regardless of which other neurons are included in the <b>network</b>. Hence, it forces the neurons to be relevant in many contexts. Our example in ...", "dateLastCrawled": "2022-01-29T17:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "pdf.pdf", "url": "https://www.slideshare.net/zanaveen1/pdfpdf-76753543", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/zanaveen1/pdfpdf-76753543", "snippet": "viii 0 Contents 7.1.1 A Regularized Risk Minimization Viewpoint 170 7.1.2 An Exponential Family Interpretation 170 7.1.3 Specialized Algorithms for Training SVMs 172 7.2 Extensions 177 7.2.1 The \u03bd trick 177 7.2.2 <b>Squared</b> <b>Hinge</b> <b>Loss</b> 179 7.2.3 Ramp <b>Loss</b> 180 7.3 Support Vector Regression 181 7.3.1 Incorporating General <b>Loss</b> Functions 184 7.3.2 Incorporating the \u03bd Trick 186 7.4 Novelty Detection 186 7.5 Margins and Probability 189 7.6 Beyond Binary Classi\ufb01cation 189 7.6.1 Multiclass ...", "dateLastCrawled": "2021-12-22T14:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Multi-Class Classification Tutorial</b> with the Keras Deep Learning Library", "url": "https://machinelearningmastery.com/multi-class-classification-tutorial-keras-deep-learning-library/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/multi", "snippet": "The <b>network</b> topology of this simple one-layer <b>neural</b> <b>network</b> <b>can</b> be summarized as: 1. 4 inputs -&gt; [8 hidden nodes] -&gt; 3 outputs ... In the code above, as <b>well</b> as in <b>your</b> book (Which I am following) we are using code that I think is written for keras1. The code carries over to keras2, apart from some warnings, but predicts poor. The reason for this is the nb_epoch parameter in the KerasClassifier class. When you leave that as is, the model predicts the same class for every training example ...", "dateLastCrawled": "2022-02-02T22:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>NIOSH</b>/Criteria for a Recommended Standard--Occupational Noise Exposure ...", "url": "https://nonoise.org/hearing/criteria/criteria.htm", "isFamilyFriendly": true, "displayUrl": "https://nonoise.org/hearing/criteria/criteria.htm", "snippet": "<b>Well</b>-defined criteria exist for <b>scoring</b> the subelements, but the program evaluator is also given some flexibility in assigning ratings. Such a <b>system</b> is helpful in that it defines strict criteria for every aspect of the program; these must be met to have a fully successful program. However, some of the currently used criteria are not perfect, because the Center has found several highly rated HLPPs to have unacceptably high incidences of significant threshold shifts [Byrne and Monk 1993].", "dateLastCrawled": "2022-01-28T04:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Artificial Medha \u2013 Embellish that <b>can</b> conquer the world", "url": "https://aimedha.wordpress.com/", "isFamilyFriendly": true, "displayUrl": "https://aimedha.wordpress.com", "snippet": "Non-linearity <b>can</b> be effectively presented in a <b>neural</b> <b>network</b> model with different activation functions; 2. There are numerous parameters related with a Deep learning model, their tuning is frequently alluded to as hyper-parameter tuning which <b>can</b> sum up <b>well</b>. 3. The Deep learning models particularly Recurrent <b>Neural</b> Networks (RNNs), Long ...", "dateLastCrawled": "2021-12-13T04:48:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "How to Choose <b>Loss</b> Functions When Training Deep Learning <b>Neural</b> Networks", "url": "https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/how-to-choose-<b>loss</b>-functions-when-training-deep...", "snippet": "<b>Neural</b> <b>network</b> models learn a mapping from inputs to outputs from examples and the choice of <b>loss</b> function must match the framing of the specific predictive modeling problem, such as classification or regression. Further, the configuration of the output layer must also be appropriate for the chosen <b>loss</b> function.", "dateLastCrawled": "2022-02-02T15:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Semantic Pooling for Complex Event Analysis in Untrimmed Videos ...", "url": "https://europepmc.org/article/MED/28113653", "isFamilyFriendly": true, "displayUrl": "https://europepmc.org/article/MED/28113653", "snippet": "the <b>squared</b> <b>hinge</b> <b>loss</b>: \u2113 (y, t) = 1 2 (1 \u2212 y t) 2 + \u2113 (y, t) = 1 2 (1-y t) + 2; the logistic <b>loss</b>: (y, t) = log(1 + exp(\u2212yt)). Note that the <b>hinge</b> <b>loss</b> is not differentiable, however, both the <b>squared</b> <b>hinge</b> <b>loss</b> and the logistic <b>loss</b> <b>can</b> be used instead as its smooth approximation. In practice, which <b>loss</b> works best is largely problem ...", "dateLastCrawled": "2022-01-27T08:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "1.5. Stochastic <b>Gradient Descent</b> \u2014 scikit-learn 1.0.2 documentation", "url": "https://scikit-learn.org/stable/modules/sgd.html", "isFamilyFriendly": true, "displayUrl": "https://scikit-learn.org/stable/modules/sgd.html", "snippet": "1.5.1. Classification\u00b6. The class SGDClassifier implements a plain stochastic <b>gradient descent</b> learning routine which supports different <b>loss</b> functions and penalties for classification. Below is the decision boundary of a SGDClassifier trained with the <b>hinge</b> <b>loss</b>, equivalent to a linear SVM. As other classifiers, SGD has to be fitted with two arrays: an array X of shape (n_samples, n_features) holding the training samples, and an array y of shape (n_samples,) holding the target values ...", "dateLastCrawled": "2022-02-03T05:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A review of object <b>detection</b> based on deep learning | SpringerLink", "url": "https://link.springer.com/article/10.1007/s11042-020-08976-6", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11042-020-08976-6", "snippet": "<b>Hinge</b> <b>loss</b> [6, 61] is a proxy function of the 0-1 <b>loss</b> function. It <b>can</b> be used as a <b>loss</b> for the max-margin problem in machine learning or deep learning, and <b>can</b> be extended to multi-class support vector machine (SVM) <b>loss</b>. The standard form of <b>Hinge</b> <b>loss</b> is listed as follows, which is suitable for binary classification. $$ \\begin{array}{c}{L(y)=\\max (0,1-t \\cdot y)} \\\\ {y=w \\cdot x+b} \\end{array} $$ (1) where (w, b) are hyperplane parameters, x is the data vector that needs to be ...", "dateLastCrawled": "2022-01-31T02:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Loss</b> and <b>Loss</b> <b>Functions for Training Deep Learning Neural Networks</b>", "url": "https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>loss</b>-and-<b>loss</b>-<b>functions-for-training-deep-learning</b>...", "snippet": "<b>Neural</b> networks are trained using stochastic gradient descent and require that you choose a <b>loss</b> function when designing and configuring <b>your</b> model. There are many <b>loss</b> functions to choose from and it <b>can</b> be challenging to know what to choose, or even what a <b>loss</b> function is and the role it plays when training a <b>neural</b> <b>network</b>. In this post, you will", "dateLastCrawled": "2022-02-02T20:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "3.3. Metrics and <b>scoring</b>: quantifying the quality of predictions ...", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "isFamilyFriendly": true, "displayUrl": "https://scikit-learn.org/stable/modules/model_evaluation.html", "snippet": "Implementing <b>your</b> own <b>scoring</b> object\u00b6 You <b>can</b> generate even more flexible model scorers by constructing <b>your</b> own <b>scoring</b> object from scratch, without using the make_scorer factory. For a callable to be a scorer, it needs to meet the protocol specified by the following two rules: It <b>can</b> be called with parameters (estimator, X, y), where estimator is the model that should be evaluated, X is validation data, and y is the ground truth target for X (in the supervised case) or None (in the ...", "dateLastCrawled": "2022-02-03T06:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "CS231n Convolutional <b>Neural</b> Networks for Visual Recognition", "url": "https://cs231n.github.io/linear-classify/", "isFamilyFriendly": true, "displayUrl": "https://cs231n.github.io/linear-classify", "snippet": "Compute the multiclass svm <b>loss</b> for a single example (x,y) - x is a column vector representing an image (e.g. 3073 x 1 in CIFAR-10) with an appended bias dimension in the 3073-rd position (i.e. bias trick) - y is an integer giving index of correct class (e.g. between 0 and 9 in CIFAR-10) - W is the weight matrix (e.g. 10 x 3073 in CIFAR-10) &quot;&quot;&quot; delta = 1.0 # see notes about delta later in this section scores = W. dot (x) # scores becomes of size 10 x 1, the scores for each class correct ...", "dateLastCrawled": "2022-02-02T19:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "GitHub - jeevu94/ml-notes: Personal notes on ML", "url": "https://github.com/jeevu94/ml-notes", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/jeevu94/ml-notes", "snippet": "The <b>hinge</b> <b>loss</b> is used for &quot;maximum-margin&quot; classification, most notably for support vector machines (SVMs). To summarize, when working with an SVM, if a computed value gives a correct classification and is larger than the margin, there is no <b>hinge</b> <b>loss</b>. If a computed value gives a correct classification but is too close to zero (where too close is defined by a margin) there is a small <b>hinge</b> <b>loss</b>. If a computed value gives an incorrect classification there will always be a <b>hinge</b> <b>loss</b> ...", "dateLastCrawled": "2022-01-20T20:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "An SVM-<b>like Approach for Expectile Regression</b> | Request PDF", "url": "https://www.researchgate.net/publication/280104219_An_SVM-like_Approach_for_Expectile_Regression", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/280104219_An_SVM-like_Approach_for_Expectile...", "snippet": "While the literature on expectile regression is less extended, <b>neural</b> <b>network</b> [Jiang et al., 2017] or SVM-like approaches [Farooq and Steinwart, 2017] have been developed as <b>well</b>. All the ...", "dateLastCrawled": "2022-01-20T18:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "The Secrets of Machine Learning: Ten Things You Wish You Had Known ...", "url": "https://deepai.org/publication/the-secrets-of-machine-learning-ten-things-you-wish-you-had-known-earlier-to-be-more-effective-at-data-analysis", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/the-secrets-of-machine-learning-ten-things-you-wish-you...", "snippet": "The two figures in the middle row on the left are both multilayer perceptrons, a type of <b>neural</b> <b>network</b>, that produce completely different results by changing one setting. Also, Figure . 1 shows that many different types of algorithms <b>can</b> perform approximately equally <b>well</b>, depending on the choice of parameters. We will get back to these points ...", "dateLastCrawled": "2022-01-18T17:21:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Introduction to Machine Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/24/introduction-to-machine-learning-2/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/24/<b>introduction-to-machine-learning</b>-2", "snippet": "<b>Squared</b> <b>loss</b> (for regression) <b>Hinge</b> <b>loss</b> (SVM) Logistic/log <b>loss</b> (logistic regression) Some <b>loss</b> functions are as follows: When to stop tuning <b>machine</b> <b>learning</b> models. When to stop tuning the hyperparameters in a <b>machine</b> <b>learning</b> model is a million-dollar question. This problem can be mostly solved by keeping tabs on training and testing errors. While increasing the complexity of a model, the following stages occur: Stage 1: Underfitting stage \u2013 high train and high test errors (or low ...", "dateLastCrawled": "2022-01-17T05:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Introduction to <b>Machine</b> <b>Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/30/introduction-to-machine-learning-3/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/30/introduction-to-<b>machine</b>-<b>learning</b>-3", "snippet": "However, in <b>machine</b> <b>learning</b> methodology, <b>squared</b> <b>loss</b> will be minimized with respect to ... <b>Squared</b> <b>loss</b> (for regression) <b>Hinge</b> <b>loss</b> (SVM) Logistic/log <b>loss</b> (logistic regression) Some <b>loss</b> functions are as follows: When to stop tuning <b>machine</b> <b>learning</b> models. When to stop tuning the hyperparameters in a <b>machine</b> <b>learning</b> model is a million-dollar question. This problem can be mostly solved by keeping tabs on training and testing errors. While increasing the complexity of a model, the ...", "dateLastCrawled": "2022-01-28T18:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Data Science: <b>Support Vector Machines (SVM</b>)", "url": "https://www.datasciencesmachinelearning.com/2019/01/support-vector-machines-svm.html", "isFamilyFriendly": true, "displayUrl": "https://www.datasciences<b>machinelearning</b>.com/2019/01/<b>support-vector-machines-svm</b>.html", "snippet": "In this case, <b>squared</b> <b>hinge</b> <b>loss</b> function (as against <b>hinge</b> <b>loss</b> function) and l2 penalty are the major changes compared to the earlier three methods. This method is useful for when sample size is larger.", "dateLastCrawled": "2022-01-28T08:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A study on L2-<b>loss (Squared Hinge-Loss) multiclass SVM</b> | Request PDF", "url": "https://www.researchgate.net/publication/235884495_A_study_on_L2-loss_Squared_Hinge-Loss_multiclass_SVM", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/235884495_A_study_on_L2-<b>loss</b>_<b>Squared</b>_<b>Hinge</b>...", "snippet": "Taking the <b>analogy</b> to classification task, it has been previously studied [13] that using the <b>squared</b> <b>hinge</b> <b>loss</b> in SVM would yield better accuracy when \u03bb is large. In this case, underfitting ...", "dateLastCrawled": "2021-12-14T01:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Metrics to Evaluate Classification and Regression Algorithms | by ...", "url": "https://medium.com/@poojitha.penmethsa/metrics-to-evaluate-classification-and-regression-algorithms-1554f1e00a75", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@poojitha.penmethsa/metrics-to-evaluate-classification-and...", "snippet": "<b>Hinge</b> <b>loss</b>. cross-entropy <b>loss</b> / log <b>loss</b>. likelihood <b>loss</b>. MSE / Quadratic <b>loss</b> / L2 <b>loss</b>: Mean <b>Squared</b> Error, or MSE <b>loss</b> is the default <b>loss</b> to use for regression problems. Mathematically, it ...", "dateLastCrawled": "2022-01-17T01:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Fast generalization rates for distance metric</b> <b>learning</b> - <b>Machine</b> <b>Learning</b>", "url": "https://link.springer.com/article/10.1007/s10994-018-5734-0", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10994-018-5734-0", "snippet": "For example, the <b>squared</b> <b>hinge</b> <b>loss</b> \\(\\ell _s^1(x) = \\max (1-x, 0) ... <b>Analogy</b>-preserving semantic embedding for visual object categorization. In Proceedings of the 30th international conference on <b>machine</b> <b>learning</b>, Atlanta, GA (pp. 639\u2013647). Jin, R., Wang, S., &amp; Zhou, Y. (2010). Regularized distance metric <b>learning</b>: Theory and algorithm. Advances in neural information processing systems (Vol. 23, pp. 862\u2013870). Cambridge, MA: MIT Press. Google Scholar Kulis, B. (2012). Metric <b>learning</b>: A ...", "dateLastCrawled": "2021-12-28T14:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Understanding the 3 most common <b>loss</b> functions for <b>Machine</b> <b>Learning</b> ...", "url": "https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-the-3-most-common-<b>loss</b>-functions-for...", "snippet": "A <b>loss function</b> in <b>Machine</b> <b>Learning</b> is a measure of how accurately your ML model is able to predict the expected outcome i.e the ground truth. The <b>loss function</b> will take two items as input: the output value of our model and the ground truth expected value. The output of the <b>loss function</b> is called the <b>loss</b> which is a measure of how well our model did at predicting the outcome. A high value for the <b>loss</b> means our model performed very poorly. A low value for the <b>loss</b> means our model performed ...", "dateLastCrawled": "2022-02-02T13:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and-sample-codes ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/README.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "Models 1.1 Support vector <b>machine</b> 1.1.1 Principle 1.1.2 Kernel 1.1.3 Soft margin SVM 1.1.4 <b>Hinge</b> <b>loss</b> view 1.1.5 Multi-class SVM 1.1.6 Extensions 1.2 Tree-based models 1.2.1 Decision tree 1.2.2 Random forest 1.2.3 Gradient boosted decision trees 1.2.4 Tools 1.3 EM Principle 1.4 MaxEnt 1.4.1 Entropy 1.5 Model selection 1.5.1 Under-fitting / Over-fitting 1.5.2 Model ensemble, sklearn 2.", "dateLastCrawled": "2021-09-22T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>MLP for regression with TensorFlow 2 and</b> Keras \u2013 MachineCurve", "url": "https://www.machinecurve.com/index.php/2019/07/30/creating-an-mlp-for-regression-with-keras/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/index.php/2019/07/30/creating-an-mlp-for-regression-with...", "snippet": "Last Updated on 30 March 2021. <b>Machine</b> <b>learning</b> is a wide field and <b>machine</b> <b>learning</b> problems come in many flavors. If, say, you wish to group data based on similarities, you would choose an unsupervised approach called clustering.If you have a fixed number of classes which you wish to assign new data to, you\u2019ll choose a supervised approach named classification.If, however, you don\u2019t have a fixed number, but wish to estimate a real value \u2013 your approach will still be supervised, but ...", "dateLastCrawled": "2022-02-03T02:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "CPSC 340: Data Mining <b>Machine</b> <b>Learning</b>", "url": "https://www.cs.ubc.ca/~schmidtm/Courses/340-F16/L35.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.ubc.ca/~schmidtm/Courses/340-F16/L35.pdf", "snippet": "<b>Machine</b> <b>Learning</b> and Data Mining Course Review/Preview Fall 2016 Some images from this lecture are taken from Google Image Search. Admin \u2022Assignment 6: \u20131 late day to hand in next Monday, 2 for Wednesday, 3 for Friday. \u2022Final: \u2013December 12 (8:30am \u2013HEBB 100) \u2013Covers Assignments 1-6. \u2013List of topics posted. \u2013Final from last year will be posted after class. \u2013Closed-book, cheat sheet: 4-pages each double-sided. Last Time: Semi-Supervised <b>Learning</b> \u2022In semi-supervised <b>learning</b> ...", "dateLastCrawled": "2021-11-22T01:45:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>About loss and loss functions</b> \u2013 MachineCurve", "url": "https://www.machinecurve.com/index.php/2019/10/04/about-loss-and-loss-functions/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/index.php/2019/10/04/<b>about-loss-and-loss-functions</b>", "snippet": "We\u2019re then using <b>machine</b> <b>learning</b> for ... The <b>squared hinge loss is like</b> the hinge formula displayed above, but then the \\(max()\\) function output is squared. This helps achieving two things: Firstly, it makes the loss value more sensitive to outliers, just as we saw with MSE vs MAE. Large errors will add to the loss more significantly than smaller errors. Note that simiarly, this may also mean that you\u2019ll need to inspect your dataset for the presence of such outliers first. Secondly ...", "dateLastCrawled": "2022-01-25T13:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "\u635f\u5931\u51fd\u6570 - \u7b97\u6cd5\u6742\u8d27\u94fa - bjmsong.github.io", "url": "https://bjmsong.github.io/2020/02/21/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/", "isFamilyFriendly": true, "displayUrl": "https://bjmsong.github.io/2020/02/21/\u635f\u5931\u51fd\u6570", "snippet": "the training data is fed into the <b>machine</b> <b>learning</b> model; Loss : compare between some actual targets and predicted targets; the lower the loss, the more the set of targets and the set of predictions resemble each other; the more they resemble each other, the better the <b>machine</b> <b>learning</b> model performs. Backward pass", "dateLastCrawled": "2021-12-27T11:43:00.0000000Z", "language": "zh_chs", "isNavigational": false}], [], [], [], []], "all_bing_queries": ["+(squared hinge loss)  is like +(scoring system for how well your neural network is doing)", "+(squared hinge loss) is similar to +(scoring system for how well your neural network is doing)", "+(squared hinge loss) can be thought of as +(scoring system for how well your neural network is doing)", "+(squared hinge loss) can be compared to +(scoring system for how well your neural network is doing)", "machine learning +(squared hinge loss AND analogy)", "machine learning +(\"squared hinge loss is like\")", "machine learning +(\"squared hinge loss is similar\")", "machine learning +(\"just as squared hinge loss\")", "machine learning +(\"squared hinge loss can be thought of as\")", "machine learning +(\"squared hinge loss can be compared to\")"]}