{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Introduction to <b>Machine Learning</b> Algorithms: <b>Linear Regression</b> | by ...", "url": "https://towardsdatascience.com/introduction-to-machine-learning-algorithms-linear-regression-14c4e325882a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/introduction-to-<b>machine-learning</b>-<b>algorithms</b>-<b>linear</b>...", "snippet": "<b>Convex</b> vs Non-<b>convex</b> <b>function</b>. Sometimes the cost <b>function</b> can be a non-<b>convex</b> <b>function</b> where you could settle at a local minima but for <b>linear regression</b>, it is always a <b>convex</b> <b>function</b>. You may be wondering how to use gradient descent to update a_0 and a_1. To update a_0 and a_1, we take gradients from the cost <b>function</b>. To find these ...", "dateLastCrawled": "2022-02-02T18:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Why is <b>linear regression a convex optimisation problem</b>? - Quora", "url": "https://www.quora.com/Why-is-linear-regression-a-convex-optimisation-problem", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Why-is-<b>linear-regression-a-convex-optimisation-problem</b>", "snippet": "Answer (1 of 3): <b>Linear</b> <b>regression</b> fits a straight line to the datapoints, such as the error between the data points and the straight line is minimized. (Image ...", "dateLastCrawled": "2022-01-26T02:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A <b>Difference of Convex Functions Algorithm for Switched Linear Regression</b>", "url": "https://www.researchgate.net/publication/264124680_A_Difference_of_Convex_Functions_Algorithm_for_Switched_Linear_Regression", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/264124680_A_Difference_of_<b>Convex</b>_<b>Functions</b>...", "snippet": "<b>F unctions Algorithm for Switched Linear Regression</b>. IEEE T ransactions on Automatic Control, Institute of Electrical and Electronics Engineers (IEEE), 2014, &lt; 10.1109/T AC.2014.2301575 &gt; .", "dateLastCrawled": "2022-01-24T10:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "ML-02 <b>Linear</b> <b>Regression</b> - IITKGP", "url": "https://cse.iitkgp.ac.in/~saptarshi/courses/ml2018spring/ML-02-linear-regression.pdf", "isFamilyFriendly": true, "displayUrl": "https://cse.iitkgp.ac.in/~saptarshi/courses/ml2018spring/ML-02-<b>linear</b>-<b>regression</b>.pdf", "snippet": "Gradient descent <b>algorithm</b> <b>Linear</b> <b>Regression</b> Model Gradient descent for univariate <b>linear</b> <b>regression</b> . Gradient descent for univariate <b>linear</b> <b>regression</b> update and simultaneously \u201cBatch\u201d: Each step of gradient descent uses all the training examples. There are other variations <b>like</b> \u201cstochastic gradient descent\u201d (used in learning over huge datasets) \u201cBatch\u201d Gradient Descen . What about multiple local minima? \u2022 The cost <b>function</b> in <b>linear</b> <b>regression</b> is always a <b>convex</b> <b>function</b> ...", "dateLastCrawled": "2022-01-30T17:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Piecewise Linear Regression via a Difference of Convex Functions</b>", "url": "http://proceedings.mlr.press/v119/siahkamari20a/siahkamari20a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v119/siahkamari20a/siahkamari20a.pdf", "snippet": "<b>algorithm</b> does not need to specify partitions for piece-wise <b>linear</b> parts and avoids ad-hoc generalizations of splines or piece-wise <b>linear</b> methods to multi-dimensions. <b>Piecewise Linear Regression via a Difference of Convex Functions</b> In addition, the method is shown to be statistically viable, in that it is shown to attain vanishing risk as the sample size grows at a non-trivial rate, under the condition that the ground truth has bounded DC seminorm. The risk further adapts to structure such ...", "dateLastCrawled": "2022-01-31T14:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Linear</b> <b>Regression \u2014 A Supervised Learning Algorithm</b> | by Nandhini N ...", "url": "https://nandhini-aitec.medium.com/linear-regression-a-supervised-learning-algorithm-9c2713d70279", "isFamilyFriendly": true, "displayUrl": "https://nandhini-aitec.medium.com/<b>linear</b>-<b>regression-a-supervised-learning-algorithm</b>-9c...", "snippet": "Fig3 \u2014 shows MSE loss <b>function</b> for <b>regression</b>. L \u2014 loss <b>function</b> , y = actual value, yhat = predicted value, N = Total number of training samples. Why mean squared error? All the square functions take the <b>convex</b> form. If a <b>function</b> is <b>convex</b>, then finding the global minima would be effortless. The global minimum is a point in the curve ...", "dateLastCrawled": "2022-01-16T18:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A Simple Guide to Gradient Descent - A <b>Linear</b> <b>Regression</b> Example ...", "url": "https://towardsdatascience.com/gradient-descent-f7458de38365", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/gradient-descent-f7458de38365", "snippet": "This brief introduction to gradient descent aimed at providing an easy to understand and implement <b>algorithm</b> that allows you to find the minimum of a <b>convex</b> <b>function</b>. GD allowed us to overcome the computational effort of expensive processes <b>like</b> matrix inversion (as in the <b>linear</b> <b>regression</b> example), by using this iterative <b>algorithm</b> to continuously update weights/coefficients.", "dateLastCrawled": "2022-02-02T14:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "5.2 <b>Least Squares Linear Regression</b>", "url": "https://jermwatt.github.io/machine_learning_refined/notes/5_Linear_regression/5_2_Least.html", "isFamilyFriendly": true, "displayUrl": "https://jermwatt.github.io/machine_learning_refined/notes/5_<b>Linear</b>_<b>regression</b>/5_2...", "snippet": "However the Least Squares cost <b>function</b> for <b>linear</b> <b>regression</b> can mathematically shown to be - in general - a <b>convex</b> <b>function</b> for any dataset (this is because one can show that it is always a <b>convex</b> quadratic - which is shown formally below). Because of this we can easily apply either gradient descent or Newton&#39;s method in order to minimize it.", "dateLastCrawled": "2022-02-02T08:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Why is the <b>cost function in linear regression, a convex function</b>? How ...", "url": "https://www.quora.com/Why-is-the-cost-function-in-linear-regression-a-convex-function-How-do-we-prove-it-mathematically", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Why-is-the-<b>cost-function-in-linear-regression-a-convex-function</b>...", "snippet": "Answer (1 of 2): When you refer to the cost <b>function</b>, I take it that you&#39;re referring to the mean squared error (MSE) Note that <b>linear</b> <b>regression</b> need not have the ...", "dateLastCrawled": "2022-01-24T23:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Gradient Descent in Linear Regression - GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/gradient-descent-in-linear-regression/", "isFamilyFriendly": true, "displayUrl": "https://www.geeksforgeeks.org/<b>gradient-descent-in-linear-regression</b>", "snippet": "Gradient Descent step-downs the cost <b>function</b> in the direction of the steepest descent. The size of each step is determined by parameter \u03b1 known as Learning Rate . In the Gradient Descent <b>algorithm</b>, one can infer two points : If slope is +ve : \u03b8 j = \u03b8 j \u2013 (+ve value). Hence value of \u03b8 j decreases. If slope is -ve : \u03b8 j = \u03b8 j \u2013 (-ve ...", "dateLastCrawled": "2022-01-30T00:19:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Piecewise Linear Regression via a Difference of Convex Functions</b>", "url": "http://proceedings.mlr.press/v119/siahkamari20a/siahkamari20a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v119/siahkamari20a/siahkamari20a.pdf", "snippet": "estimating piecewise-liner <b>convex</b> functions, in a manner <b>similar</b> to max-af\ufb01ne <b>regression</b>, whose difference approximates the data. The choice of the <b>function</b> is regularised by a new semi-norm over the class of DC functions that con-trols the \u2018 1Lipschitz constant of the estimate. The resulting methodology can be ef\ufb01ciently im-plemented via Quadratic programming even in high dimensions, and is shown to have close to minimax statistical risk. We empirically validate the method, showing it ...", "dateLastCrawled": "2022-01-31T14:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Why is <b>linear regression a convex optimisation problem</b>? - Quora", "url": "https://www.quora.com/Why-is-linear-regression-a-convex-optimisation-problem", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Why-is-<b>linear-regression-a-convex-optimisation-problem</b>", "snippet": "Answer (1 of 3): <b>Linear</b> <b>regression</b> fits a straight line to the datapoints, such as the error between the data points and the straight line is minimized. (Image ...", "dateLastCrawled": "2022-01-26T02:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Machine Learning Path (III). <b>Linear</b> <b>Regression</b> \u2014 Cost <b>Function</b> | by ...", "url": "https://medium.com/@maximilianhuang/machine-learning-cost-function-e0abba6180ee", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@maximilianhuang/machine-learning-cost-<b>function</b>-e0abba6180ee", "snippet": "From the diagram, if you tried to plot the graph, it will result something like a parabolic line. In the field of machine learning, we often called it a <b>convex</b> <b>function</b>.<b>Convex</b> <b>function</b> often comes ...", "dateLastCrawled": "2021-12-29T22:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Price Optimisation with <b>convex</b> and non-<b>convex</b> loss functions | by ...", "url": "https://towardsdatascience.com/convex-and-non-convex-optimisation-899174802b60", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>convex</b>-and-non-<b>convex</b>-optimisation-899174802b60", "snippet": "In a <b>similar</b> manner a <b>linear</b> <b>regression</b> with cost as the dependent variable \u2018y\u2019 and volume as the independent variable \u2018X\u2019 is performed below. We can see that the bias and co-efficient for the cost model approximates the <b>function</b> used to generate the cost data. Hence we can now use these trained models to determine cost and volume for the <b>convex</b> optimisation.", "dateLastCrawled": "2022-01-30T23:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "ML-02 <b>Linear</b> <b>Regression</b> - IITKGP", "url": "https://cse.iitkgp.ac.in/~saptarshi/courses/ml2018spring/ML-02-linear-regression.pdf", "isFamilyFriendly": true, "displayUrl": "https://cse.iitkgp.ac.in/~saptarshi/courses/ml2018spring/ML-02-<b>linear</b>-<b>regression</b>.pdf", "snippet": "Gradient descent <b>algorithm</b> <b>Linear</b> <b>Regression</b> Model Gradient descent for univariate <b>linear</b> <b>regression</b> . Gradient descent for univariate <b>linear</b> <b>regression</b> update and simultaneously \u201cBatch\u201d: Each step of gradient descent uses all the training examples. There are other variations like \u201cstochastic gradient descent\u201d (used in learning over huge datasets) \u201cBatch\u201d Gradient Descen . What about multiple local minima? \u2022 The cost <b>function</b> in <b>linear</b> <b>regression</b> is always a <b>convex</b> <b>function</b> ...", "dateLastCrawled": "2022-01-30T17:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "UVA CS 6316: Machine Learning Lecture 3: <b>Linear</b> <b>Regression</b> Basics", "url": "https://qiyanjun.github.io/2019f-UVA-CS6316-MachineLearning/Lectures/L03-lr.pdf", "isFamilyFriendly": true, "displayUrl": "https://qiyanjun.github.io/2019f-UVA-CS6316-MachineLearning/Lectures/L03-lr.pdf", "snippet": "representation of the <b>linear</b> <b>regression</b> <b>function</b>: 9/18/19 Dr. Yanjun Qi / UVA CS 20 ... \u2022Intuitively, a <b>convex</b> <b>function</b> (1D case) has a single point at which the derivative goes to zero, and this point is a minimum. \u2022Intuitively, a <b>function</b> f (1D case) is <b>convex</b> on the range [a,b] if a <b>function</b>\u2019s second derivative is positive every-where in that range. \u2022Intuitively, if a multivariate <b>function</b>&#39;s Hessians is pd (positive definite!), this (multivariate) <b>function</b> is <b>Convex</b> \u2022Intuitively ...", "dateLastCrawled": "2021-09-16T02:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A <b>Convex Framework for Fair Regression</b>", "url": "https://www.cis.upenn.edu/~mkearns/papers/ConvexFATML.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cis.upenn.edu/~mkearns/papers/<b>Convex</b>FATML.pdf", "snippet": "A <b>Convex Framework for Fair Regression</b> Richard Berk, Hoda Heidari, Shahin Jabbari, Matthew Joseph Michael Kearns, Jamie Morgenstern, Seth Neel, Aaron Roth University of Pennsylvania Abstract We introduce a exible family of fairness regularizers for (<b>linear</b> and logistic) <b>regression</b> problems. These regular-izers all enjoy convexity, permitting fast optimization, and span the range from group fairness to strong indi-vidual fairness. We study the accuracy-fairness trade-o on any given dataset ...", "dateLastCrawled": "2022-01-28T01:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "7 of the Most Used <b>Regression</b> Algorithms and How to Choose the Right ...", "url": "https://towardsdatascience.com/7-of-the-most-commonly-used-regression-algorithms-and-how-to-choose-the-right-one-fc3c8890f9e3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/7-of-the-most-commonly-used-<b>regression</b>-<b>algorithms</b>-and...", "snippet": "<b>Linear</b> <b>Regression</b>: interception term and <b>regression</b> coefficients \u2014 Image by the author . Polynomial <b>Regression</b>. By transforming the input variables, e.g. by the logarithm <b>function</b>, the root <b>function</b> etc., non-<b>linear</b> and polynomial relationships can be represented. Nevertheless, these are <b>linear</b> models, as this designation is based on the linearity of the input parameters. [Has09, p.44] The modelling of such correlations is done using so-called trend models. If the rough course is already ...", "dateLastCrawled": "2022-01-29T13:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A <b>convex</b> optimization formulation for multivariate <b>regression</b>", "url": "https://proceedings.neurips.cc/paper/2020/file/ccd2d123f4ec4d777fc6ef757d0fb642-Paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/2020/file/ccd2d123f4ec4d777fc6ef757d0fb642-Paper.pdf", "snippet": "Computationally, we propose to use a proximal Newton <b>algorithm</b> [see, e.g., Lee et al., 2014] to solve the associated optimization problem for regularized negative log-likelihood loss formulation under the proposed parameterization. The proximal Newton <b>algorithm</b>, however, may not be scalable to large-scale problems. This is mainly caused by the logdet() in the Gaussian log-likelihood <b>function</b>. For large-scale problems, we propose to use the aforementioned alternative loss functions, which can ...", "dateLastCrawled": "2022-02-03T06:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Linear</b> <b>Regression</b> | gbhat.com", "url": "https://gbhat.com/machine_learning/linear_regression.html", "isFamilyFriendly": true, "displayUrl": "https://gbhat.com/machine_learning/<b>linear</b>_<b>regression</b>.html", "snippet": "Since this is a <b>regression</b> <b>algorithm</b>, it makes real valued (continuous valued) predictions. In constrast a classification <b>algorithm</b> makes descrete valued predictions. <b>Linear</b> <b>Regression</b> builds a <b>linear</b> relation (with constant slope) between feature values and target variables. Types of <b>Linear</b> Regressions. In case of only one feature in <b>Linear</b> <b>Regression</b>, it is called Simple <b>Linear</b> <b>Regression</b>. Equation for this can be written as: $$ \\hat{y} = \\theta_1 * x_1 + \\theta_2 $$ Where $\\hat{y}$ is ...", "dateLastCrawled": "2022-01-13T06:27:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Why <b>Linear</b> <b>Regression</b> works ?. The math foundation behind <b>Linear</b>\u2026 | by ...", "url": "https://medium.com/analytics-vidhya/why-linear-regression-works-ad52a9e3de6d", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/why-<b>linear</b>-<b>regression</b>-works-ad52a9e3de6d", "snippet": "Well, minimizing the Cost <b>Function</b> is nothing but the main goal of <b>Linear</b> <b>Regression</b> <b>Algorithm</b>. But have you ever <b>thought</b> on what ensures us that the gradient descent will ever converge? This is ...", "dateLastCrawled": "2021-08-23T12:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Convex</b> Problems", "url": "https://inst.eecs.berkeley.edu/~ee227a/fa10/login/l_cvx_pbs.html", "isFamilyFriendly": true, "displayUrl": "https://inst.eecs.berkeley.edu/~ee227a/fa10/login/l_cvx_pbs.html", "snippet": "This model <b>can</b> <b>be thought</b> of as a generalization of both the least-squares and <b>linear</b> programming problems. QP\u2019s are popular in many areas, such as finance, where the <b>linear</b> term in the objective refers to the expected negative return on an investment, and the squared term corresponds to the risk (or variance of the return). This model was introduced by Markowitz (who was a student of Dantzig) in the 50\u2019s, to model investment problemsfootnote{Markowitz won the Nobel prize in Economics in ...", "dateLastCrawled": "2022-02-01T18:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A test for <b>linear</b> versus <b>convex</b> <b>regression</b> <b>function</b> using shape ...", "url": "https://www.researchgate.net/publication/5207206_A_test_for_linear_versus_convex_regression_function_using_shape-restricted_regression", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/5207206_A_test_for_<b>linear</b>_versus_<b>convex</b>...", "snippet": "The shapereg <b>function</b> uses coneB to provide a least a least-squares estimator for a <b>regression</b> <b>function</b> with several choices of constraints including isotonic and <b>convex</b> <b>regression</b> <b>function</b>, as ...", "dateLastCrawled": "2021-12-16T10:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Can</b> We Use Stochastic Gradient Descent (SGD) on a <b>Linear</b> <b>Regression</b> ...", "url": "https://towardsdatascience.com/can-we-use-stochastic-gradient-descent-sgd-on-a-linear-regression-model-e50327b07d33", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>can</b>-we-use-stochastic-gradient-descent-sgd-on-a-<b>linear</b>...", "snippet": "Since L(w) is a <b>convex</b> <b>function</b>, the default way of minimizing it is to compute its derivative with respect to w, ... Although the above proof is done on a <b>linear</b> <b>regression</b> model, you <b>can</b> recognize that the structure of the loss <b>function</b> of many neural network models are the same as the <b>linear</b> <b>regression</b> model. This means the above proof also applies to those neural network models, and this is the reason why stochastic gradient descent, or variations of it, such as Adam, <b>can</b> be the ...", "dateLastCrawled": "2022-01-31T07:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Linear</b> <b>Regression</b> Using <b>Gradient</b> Descent for Beginners\u2014 Intuition, Math ...", "url": "https://medium.com/analytics-vidhya/linear-regression-gradient-descent-intuition-and-math-c9a8f5aeeb22", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/<b>linear</b>-<b>regression</b>-<b>gradient</b>-descent-intuition-and...", "snippet": "There is a general <b>algorithm</b> to minimize any <b>function</b> (**<b>function</b> must be <b>convex</b> else it will only find local minima) known as <b>gradient</b> descent. We <b>can</b> summarize <b>gradient</b> descent <b>algorithm</b> as ...", "dateLastCrawled": "2022-02-03T17:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Convex Optimization</b> - NUS Computing - Home", "url": "https://www.comp.nus.edu.sg/~rahul/allfiles/cs6234-16-convex-opt.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.comp.nus.edu.sg/~rahul/allfiles/cs6234-16-<b>convex</b>-opt.pdf", "snippet": "<b>Convex</b> Domain <b>Linear</b> <b>Regression</b> method is applicable only if nonlinear <b>function</b> is <b>linear</b> in terms of <b>function</b> parameters: f(x;a) = Xm k=1 a kh k(x) Many nonlinear functions are not like that, for example: f 1(x) = x2 a 1 +(x-a 2) f 2(x,y,z) = x2 a 1 +x2 + y2 a 2 +y2 + z2 a 3 +z2 Advanced Algorithms <b>Convex Optimization</b> Jan 20th, 2016 17 / 42", "dateLastCrawled": "2022-01-27T18:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine Learning: Linear Regression Algorithm With One Variable</b> - DEV ...", "url": "https://dev.to/kpose/machine-learning-linear-regression-algorithm-with-one-variable-25fe", "isFamilyFriendly": true, "displayUrl": "https://dev.to/kpose/<b>machine-learning-linear-regression-algorithm-with-one-variable</b>-25fe", "snippet": "It is called supervised learning because the process of an <b>algorithm</b> learning from the training dataset <b>can</b> <b>be thought</b> of as a teacher supervising the learning process. We know the correct answers, the <b>algorithm</b> iteratively makes predictions on the training data and is corrected by the teacher. Learning stops when the <b>algorithm</b> achieves an acceptable level of performance. Supervised learning problems are further categorized into . <b>Regression</b>: A <b>regression</b> problem is when the output variable ...", "dateLastCrawled": "2021-12-22T20:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Gradient Descent For Linear Regression</b> \u2013 Wingshore", "url": "https://wingshore.wordpress.com/2014/11/20/gradient-descent-for-linear-regression/", "isFamilyFriendly": true, "displayUrl": "https://wingshore.wordpress.com/2014/11/20/<b>gradient-descent-for-linear-regression</b>", "snippet": "Hi guys, we believe you are thoroughly excited as we are. Finally, the time has arrived where we will discuss the first ever Learning <b>Algorithm</b> called \u2013 \u201c<b>Gradient Descent for Linear Regression</b>\u201c. So, far we have discussed the cost <b>function</b>, it\u2019s model, the basic foundation of machine learning, our goal to minimize cost <b>function</b> in order to have accuracy in predicting the ideal values and further, we have scooped our way through in understanding the Gradient Descent <b>Algorithm</b> in general.", "dateLastCrawled": "2022-01-28T04:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Machine Learning \u2014 <b>Regression</b>. <b>Regression</b> is an important approach for ...", "url": "https://chisoftware.medium.com/machine-learning-regression-118a5bfb4fd", "isFamilyFriendly": true, "displayUrl": "https://chisoftware.medium.com/machine-learning-<b>regression</b>-118a5bfb4fd", "snippet": "The <b>linear</b> <b>regression</b> cost <b>function</b> is <b>convex</b> and has a simple form. Its minimum cost <b>can</b> even be found analytically, without gradient descent. However, in more complicated cases with a non-<b>convex</b> cost <b>function</b>, gradient descent <b>can</b> fall into the trap of local minima, and a choice of the learning rate helps to avoid it. We have seen the concept of simple <b>linear</b> <b>regression</b> where a single feature x was used to predict the target y. In many applications, more than one factor influences the ...", "dateLastCrawled": "2022-01-31T04:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Why is Convex Optimization such a big</b> deal in Machine Learning? - Quora", "url": "https://www.quora.com/Why-is-Convex-Optimization-such-a-big-deal-in-Machine-Learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>Why-is-Convex-Optimization-such-a-big</b>-deal-in-Machine-Learning", "snippet": "Answer (1 of 10): <b>Convex</b> optimization is the core of most machine learning methods Some key concepts here: - <b>Convex</b> functions are those for which it&#39;s possible to draw a line segment from any two points on the graph and this line will always be inside the graph (except at the endpoints). This m...", "dateLastCrawled": "2022-01-17T17:05:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A Difference of <b>Convex</b> Functions <b>Algorithm</b> for Switched <b>Linear</b> <b>Regression</b>", "url": "https://hal.archives-ouvertes.fr/hal-00931206/document", "isFamilyFriendly": true, "displayUrl": "https://hal.archives-ouvertes.fr/hal-00931206/document", "snippet": "<b>Algorithm</b> for Switched <b>Linear</b> <b>Regression</b>. IEEE Transactions on Automatic Control, Institute of Electrical and Electronics Engineers, 2014, \uffff10.1109/TAC.2014.2301575\uffff. \uffffhal-00931206\uffff IEEE TRANSACTIONS ON AUTOMATIC CONTROL 1 A Difference of <b>Convex</b> Functions <b>Algorithm</b> for Switched <b>Linear</b> <b>Regression</b> Tao PHAM DINH, Hoai Minh LE, Hoai An LE THI, and Fabien LAUER Abstract\u2014This paper deals with switched <b>linear</b> system iden-ti\ufb01cation and more particularly aims at solving switched <b>linear</b> ...", "dateLastCrawled": "2022-01-18T05:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A <b>Difference of Convex Functions Algorithm for Switched Linear Regression</b>", "url": "https://www.researchgate.net/publication/264124680_A_Difference_of_Convex_Functions_Algorithm_for_Switched_Linear_Regression", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/264124680_A_Difference_of_<b>Convex</b>_<b>Functions</b>...", "snippet": "<b>F unctions Algorithm for Switched Linear Regression</b>. IEEE T ransactions on Automatic Control, Institute of Electrical and Electronics Engineers (IEEE), 2014, &lt; 10.1109/T AC.2014.2301575 &gt; .", "dateLastCrawled": "2022-01-24T10:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A Test for <b>Linear</b> versus <b>Convex</b> <b>Regression</b> <b>Function</b> Using Shape ...", "url": "https://www.jstor.org/stable/30042031", "isFamilyFriendly": true, "displayUrl": "https://www.jstor.org/stable/30042031", "snippet": "A test for <b>linear</b> versus <b>convex</b> <b>regression</b> <b>function</b> using shape-restricted <b>regression</b> By MARY C. MEYER Department of Statistics, The University of Georgia, Athens, Georgia 30602-1952, U.S.A. mmeyer@stat.uga.edu SUMMARY An unbiased test for the appropriateness of the simple <b>linear</b> <b>regression</b> model is pre-sented. The null hypothesis is that the underlying <b>regression</b> <b>function</b> is indeed a line, and the alternative is that it is <b>convex</b>. The exact distribution for a likelihood ratio test statistic ...", "dateLastCrawled": "2021-11-09T21:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A DIFFERENCE OF <b>CONVEX</b> OPTIMIZATION <b>ALGORITHM</b> FOR PIECEWISE <b>LINEAR</b> ...", "url": "https://www.aimsciences.org/article/exportPdf?id=843d75f3-c2d1-4c24-a97c-8057f6256f97", "isFamilyFriendly": true, "displayUrl": "https://www.aimsciences.org/article/exportPdf?id=843d75f3-c2d1-4c24-a97c-8057f6256f97", "snippet": "A DIFFERENCE OF <b>CONVEX</b> OPTIMIZATION <b>ALGORITHM</b> FOR PIECEWISE <b>LINEAR</b> <b>REGRESSION</b> Adil Bagirov and Sona Taheri Faculty of Science and Technology, Federation University Australia Victoria, Australia Soodabeh Asadi Department of Mathematics, Shahrekord University, Iran (Communicated by Gerhard-Wilhelm Weber) Abstract. The problem of nding a continuous piecewise <b>linear</b> <b>function</b> ap-proximating a <b>regression</b> <b>function</b> is considered. This problem is formulated as a nonconvex nonsmooth optimization ...", "dateLastCrawled": "2022-01-01T20:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Machine Learning-<b>Regression</b> Algorithms(<b>Linear</b> <b>regression</b>) | by Mohamed ...", "url": "https://medium.com/nerd-for-tech/machine-learning-regression-algorithms-linear-regression-ea97c92f2d9b", "isFamilyFriendly": true, "displayUrl": "https://medium.com/nerd-for-tech/machine-learning-<b>regression</b>-<b>algorithms</b>-<b>linear</b>...", "snippet": "<b>Linear</b> <b>regression</b> is to find the best fit line ( orange line ), so a new point <b>can</b> be predicted, e.g., for the given orange line, the x value of 21 will correspond to a y value of 495. x and y are ...", "dateLastCrawled": "2022-01-26T18:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) <b>Convex Factorization Machine for Regression</b>", "url": "https://www.researchgate.net/publication/279864688_Convex_Factorization_Machine_for_Regression", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/.../279864688_<b>Convex_Factorization_Machine_for_Regression</b>", "snippet": "A key advantage of CFM over existing FMs is that it <b>can</b> find a globally optimal solution, while FMs may get a poor locally optimal solution since the objective <b>function</b> of FMs is non-<b>convex</b>. In ...", "dateLastCrawled": "2022-01-14T14:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "From <b>Linear</b> <b>Regression</b> to Ridge <b>Regression</b>, the Lasso, and the Elastic ...", "url": "https://towardsdatascience.com/from-linear-regression-to-ridge-regression-the-lasso-and-the-elastic-net-4eaecaf5f7e6", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/from-<b>linear</b>-<b>regression</b>-to-ridge-<b>regression</b>-the-lasso...", "snippet": "<b>Linear</b>, Ridge and the Lasso <b>can</b> all be seen as special cases of the Elastic net. In 2014, it was proven that the Elastic Net <b>can</b> be reduced to a <b>linear</b> support vector machine. The loss <b>function</b> is strongly <b>convex</b>, and hence a unique minimum exists. The Elastic Net is an extension of the Lasso, it combines both L1 and L2 regularization. So we ...", "dateLastCrawled": "2022-02-02T00:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Optimization Algorithm vs Regression Models</b> - Stack Overflow", "url": "https://stackoverflow.com/questions/37393040/optimization-algorithm-vs-regression-models", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/37393040", "snippet": "I have a reference that used <b>linear</b> <b>function</b> to represent the input and output data. y = po + p1.x1 + p2.x2. Both of x1 and x2 are known input; y is output; p0, p1, and p2 are the coefficient. Then, he used all the training data and Least Square Estimation (LSE) method to find the optimal coefficient (p0, p1, p2) to build the model.", "dateLastCrawled": "2022-01-28T21:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "7 of the Most Used <b>Regression</b> Algorithms and How to Choose the Right ...", "url": "https://towardsdatascience.com/7-of-the-most-commonly-used-regression-algorithms-and-how-to-choose-the-right-one-fc3c8890f9e3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/7-of-the-most-commonly-used-<b>regression</b>-<b>algorithms</b>-and...", "snippet": "<b>Linear</b> <b>Regression</b>: interception term and <b>regression</b> coefficients \u2014 Image by the author. Polynomial <b>Regression</b> . By transforming the input variables, e.g. by the logarithm <b>function</b>, the root <b>function</b> etc., non-<b>linear</b> and polynomial relationships <b>can</b> be represented. Nevertheless, these are <b>linear</b> models, as this designation is based on the linearity of the input parameters. [Has09, p.44] The modelling of such correlations is done using so-called trend models. If the rough course is already ...", "dateLastCrawled": "2022-01-29T13:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Can</b> theta calculated by normal <b>equation and gradient descent algorithm</b> ...", "url": "https://www.quora.com/Can-theta-calculated-by-normal-equation-and-gradient-descent-algorithm-for-linear-regression-multivariables-be-different", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>Can</b>-theta-calculated-by-normal-equation-and-gradient-descent...", "snippet": "Answer (1 of 4): Because the objective of (appropriately regularized) <b>linear</b> <b>regression</b> is a *strongly* <b>convex</b> <b>function</b> - it has a single and unique solution - and the normal equations are a property satisfied by this solution point. But this is a mathematical / analytical statement - it doesn&#39;t ...", "dateLastCrawled": "2022-01-21T02:54:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Introduction to Machine Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/24/introduction-to-machine-learning-2/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/24/<b>introduction-to-machine-learning</b>-2", "snippet": "<b>Machine</b> <b>learning</b> terminology for model building and validation There seems to be an <b>analogy</b> between statistical modeling and <b>machine</b> <b>learning</b> that we will cover in subsequent chapters in depth. However, a quick view has been provided as follows: in statistical modeling, linear regression with two independent variables is trying to fit the best plane with\u2026", "dateLastCrawled": "2022-01-17T05:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "11.2. Convexity \u2014 Dive into Deep <b>Learning</b> 0.17.2 documentation", "url": "http://d2l.ai/chapter_optimization/convexity.html", "isFamilyFriendly": true, "displayUrl": "d2l.ai/chapter_optimization/<b>convex</b>ity.html", "snippet": "A twice-differentiable <b>function</b> is <b>convex</b> if and only if its Hessian (a matrix of second derivatives) is positive semidefinite. <b>Convex</b> constraints can be added via the Lagrangian. In practice we may simply add them with a penalty to the objective <b>function</b>. Projections map to points in the <b>convex</b> set closest to the original points.", "dateLastCrawled": "2022-02-02T17:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Deep Learning Basics III</b> \u2013 Theoretical <b>Machine</b> <b>Learning</b>", "url": "https://mltf16.wordpress.com/2017/07/19/lecture-3-scribe-notes/", "isFamilyFriendly": true, "displayUrl": "https://mltf16.wordpress.com/2017/07/19/lecture-3-scribe-notes", "snippet": "In this post, we discuss two theorems that provide guarantees on the convergence of Gradient Descent algorithm for minimizing a <b>convex</b> <b>function</b>. We show how making assumptions about the functions that we want to minimize can result in faster convergence. Then we discuss other <b>learning</b> algorithms to train a deep network, along with techniques to address over-fitting. Finally, we talk briefly about two of the popular architectures for deep network- Convolutional Neural Network and Recurrent ...", "dateLastCrawled": "2022-01-27T14:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Introduction to <b>Machine</b> <b>Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/30/introduction-to-machine-learning-3/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/30/introduction-to-<b>machine</b>-<b>learning</b>-3", "snippet": "The loss <b>function</b> or cost <b>function</b> in <b>machine</b> <b>learning</b> is a <b>function</b> that maps the values of variables onto a real number intuitively representing some cost associated with the variable values. Optimization methods are applied to minimize the loss <b>function</b> by changing the parameter values, which is the central theme of <b>machine</b> <b>learning</b>.", "dateLastCrawled": "2022-01-28T18:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Gradient</b> Descent: A Quick, Simple Introduction | Built In", "url": "https://builtin.com/data-science/gradient-descent", "isFamilyFriendly": true, "displayUrl": "https://builtin.com/data-science/<b>gradient</b>-descent", "snippet": "<b>Gradient</b> descent is an optimization algorithm that&#39;s used when training a <b>machine</b> <b>learning</b> model. It&#39;s based on a <b>convex</b> <b>function</b> and tweaks its parameters iteratively to minimize a given <b>function</b> to its local minimum.", "dateLastCrawled": "2022-02-02T07:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Weak <b>learning</b> <b>convex</b> sets under normal distributions", "url": "http://proceedings.mlr.press/v134/de21a/de21a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v134/de21a/de21a.pdf", "snippet": "Keywords: weak <b>learning</b>, <b>convex</b> geometry, Gaussian space 1. Introduction Background and motivation. Several results in Boolean <b>function</b> analysis and computational <b>learning</b> theory suggest an <b>analogy</b> between <b>convex</b> sets in Gaussian space and monotone Boolean functions1 with respect to the uniform distribution over the hypercube. As an example ...", "dateLastCrawled": "2022-01-21T10:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Basic Concepts in Machine Learning</b>", "url": "https://machinelearningmastery.com/basic-concepts-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>basic-concepts-in-machine-learning</b>", "snippet": "Probability Estimation: when the output of the <b>function</b> is a probability. <b>Machine Learning</b> in Practice. <b>Machine learning</b> algorithms are only a very small part of using <b>machine learning</b> in practice as a data analyst or data scientist. In practice, the process often looks like: Start Loop Understand the domain, prior knowledge and goals. Talk to ...", "dateLastCrawled": "2022-02-02T15:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>machine</b> <b>learning</b> - How does Gradient Descent work? - Data Science Stack ...", "url": "https://datascience.stackexchange.com/questions/102509/how-does-gradient-descent-work", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/102509/how-does-gradient-descent-work", "snippet": "If the <b>function</b> we minimize was <b>convex</b>, it would not matter what we choose for initial values, as gradient descent would get us to the minimum no matter what. But as the dimensions of the model increase, it is extremely unlikely that we have a <b>convex</b> loss <b>function</b>. And in this case, initialization of the weight depends on the activation functions used in the model. As discussed in", "dateLastCrawled": "2022-01-16T12:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Regularization : What? Why? and How? (Part -1) | by Siddhant Rai ...", "url": "https://medium.com/mlearning-ai/regularization-what-why-and-how-part-1-ef6bdb6bafea", "isFamilyFriendly": true, "displayUrl": "https://medium.com/m<b>learning</b>-ai/regularization-what-why-and-how-part-1-ef6bdb6bafea", "snippet": "In general <b>machine</b> <b>learning</b> sense, it is solving an objective <b>function</b> to perform maximum or minimum evaluation. In reality, optimization is lot more profound in usage. Then we have two terms ...", "dateLastCrawled": "2022-01-28T00:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>machine</b> <b>learning</b> - Cost <b>function</b> of neural network is non-<b>convex</b> ...", "url": "https://stats.stackexchange.com/questions/106334/cost-function-of-neural-network-is-non-convex", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/106334", "snippet": "$\\begingroup$ I mean, this is how it should be interpreted, not just an <b>analogy</b>. $\\endgroup$ \u2013 avocado. May 23 &#39;16 at 12:27 . 5 $\\begingroup$ @loganecolss You are correct that this is not the only reason why cost functions are non-<b>convex</b>, but one of the most obvious reasons. Depdending on the network and the training set, there might be other reasons why there are multiple minima. But the bottom line is: The permuation alone creates non-convexity, regardless of other effects. $\\endgroup ...", "dateLastCrawled": "2022-02-03T01:18:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(convex function)  is like +(linear regression algorithm)", "+(convex function) is similar to +(linear regression algorithm)", "+(convex function) can be thought of as +(linear regression algorithm)", "+(convex function) can be compared to +(linear regression algorithm)", "machine learning +(convex function AND analogy)", "machine learning +(\"convex function is like\")", "machine learning +(\"convex function is similar\")", "machine learning +(\"just as convex function\")", "machine learning +(\"convex function can be thought of as\")", "machine learning +(\"convex function can be compared to\")"]}