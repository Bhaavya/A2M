{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "More <b>Reasons to Listen: Learning Lessons from Pupil</b> Voice for ...", "url": "https://ijsv.psu.edu/article/more-reasons-to-listen-learning-lessons-from-pupil-voice-for-psychology-and-education/", "isFamilyFriendly": true, "displayUrl": "https://ijsv.psu.edu/article/more-<b>reasons-to-listen-learning-lessons-from-pupil</b>-voice...", "snippet": "Much of what makes a secure attachment relationship, whether it be <b>child</b>-parent or <b>child</b>-teacher in nature, is the importance of listening to the <b>child</b>, taking them seriously and respecting their views. Pupil voice research has escalated and shown the importance of listening to pupils for teaching and <b>learning</b>. As we can learn from attachment theory for teaching, we can likewise learn from pupil voice for parenting. Learners, <b>like</b> anyone else, not least children with their parents, want to ...", "dateLastCrawled": "2022-02-02T08:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Perplexity</b> | Illustrations portrayed from her slivered perspective", "url": "https://perplexityshe.wordpress.com/", "isFamilyFriendly": true, "displayUrl": "https://<b>perplexity</b>she.wordpress.com", "snippet": "So, imagine <b>a child</b>, a baby girl being born into a hostile and dangerous environment. Again, there are a number of different things I could be speaking about. So, let\u2019s narrow it down. Her mother was adviced not to have <b>a child</b> (reasons remain unspoken of), her father was an alcoholic but also a pedophile. The events of this little girls upbrining and how it would mold her are terrifying. This is a girl who is taught no limits, no self respect, and to love someone who sufferes her ...", "dateLastCrawled": "2022-01-18T07:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>VIRTUE, PRACTICE, AND PERPLEXITY IN PLATO</b>&#39;S MENO | William Wians ...", "url": "https://www.academia.edu/37024689/VIRTUE_PRACTICE_AND_PERPLEXITY_IN_PLATOS_MENO", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/37024689/<b>VIRTUE_PRACTICE_AND_PERPLEXITY_IN</b>_PLATOS_MENO", "snippet": "Among those cited elsewhere in this paper, those who <b>speak</b> this way include Edward Halper, \u201cA Lesson from the Meno\u201d [\u201cLesson\u201d], in Erler and Brisson (eds.), Gorgias-Menon, 234-242; Sharples, Meno, 4; Gordon, Turning, 93-94; Garreth Matthews, Socratic <b>Perplexity</b> and the Nature of Philosophy [<b>Perplexity</b>] (Oxford: Oxford University Press, 1999), 43; J. M. Day (ed.), Plato\u2019s Meno in Focus [Meno in Focus] (London: Routledge, 1994), 1, 3, 19; K. V. Wilkes, \u201cConclusions in the Meno ...", "dateLastCrawled": "2021-12-20T23:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Design Scandal \u2013 Perplexity</b> \u2013 Kairosis", "url": "https://kairosis.wordpress.com/article/design-scandal-perplexity-13h9pp7wb34y8-11/", "isFamilyFriendly": true, "displayUrl": "https://kairosis.wordpress.com/article/<b>design-scandal-perplexity</b>-13h9pp7wb34y8-11", "snippet": "Project-driven <b>learning</b> may be the outcome of <b>perplexity</b>, however, it is not tied to <b>perplexity</b> and the engagement of the learner. Project-driven <b>learning</b> has no need of <b>perplexity</b> \u2013 here problems can be external to the learner. For example, the professional need to have a particular technical skill is directly owned by the profession and may only be indirectly acknowledged by the learner. Often the acquisition of a particular technical skill is disputed by the learner as genuinely ...", "dateLastCrawled": "2022-01-24T16:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "\u2018I think she\u2019s learnt how to sort of let the class <b>speak</b>\u2019: Children\u2019s ...", "url": "https://www.sciencedirect.com/science/article/pii/S1871187115300158", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1871187115300158", "snippet": "Some suggested that it was hard to know what to say when given opportunity <b>to speak</b>: <b>Child</b> Two: it\u2019s <b>like</b> one of the lessons where I need quite a bit of help to know what to say and stuff. Another challenge involved worrying about speaking in front of others. Interestingly none of the children used the first person when mentioning this: <b>Child</b> Three: they were nervous <b>to speak</b>. <b>Child</b> Six: Well, sometimes when they\u2019ve said the wrong thing they think, \u201cOh I regret saying that.\u201d The use ...", "dateLastCrawled": "2021-10-19T09:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>What perplexities do state-of-the-art language models achieve</b>? - Quora", "url": "https://www.quora.com/What-perplexities-do-state-of-the-art-language-models-achieve", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>What-perplexities-do-state-of-the-art-language-models-achieve</b>", "snippet": "Answer: There are a few benchmarks that people compare against for word-level language modeling. The difference in size, style, and pre-processing results in different challenges and thus different state-of-the-art perplexities. I will list a few: Penn Treebank. This contains articles from the ...", "dateLastCrawled": "2022-01-17T20:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "\u201cPrison Treated Me Way Better Than You\u201d: Reentry, <b>Perplexity</b>, and the ...", "url": "https://abolitionjournal.org/prison-treated-me-way-better/", "isFamilyFriendly": true, "displayUrl": "https://abolitionjournal.org/prison-treated-me-way-better", "snippet": "<b>Perplexity</b> is a conceptual platform to think about the experiential contradictions of globalization as a series of processes that often overwhelm subjects. As an analytic with multiple subtexts, <b>perplexity</b> is a way of marking the tension between overlapping, opposing, and asymmetric forces or fields of power. <b>Perplexity</b> indexes the puzzlement ...", "dateLastCrawled": "2022-01-30T00:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "TAKING THE <b>PERPLEXITY</b> OUT OF PROVERBS May-July 2021", "url": "https://hopelooksup.org/wp-content/uploads/2021/08/TAKING-THE-PERPLEXITY-OUT-OF-PROVERBS-May-July-2021.pdf", "isFamilyFriendly": true, "displayUrl": "https://hopelooksup.org/wp-content/uploads/2021/08/TAKING-THE-<b>PERPLEXITY</b>-OUT-OF...", "snippet": "\u2018Taking the <b>Perplexity</b> out of Proverbs\u2019 will be a 10\u2010week study about some of the opposite characteristics Solomon told his sons to develop and to avoid. The word \u201cproverb\u201d in Hebrew is", "dateLastCrawled": "2021-11-20T16:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "How We Think: John Dewey on the Art of Reflection and Fruitful ...", "url": "https://www.themarginalian.org/2014/08/18/how-we-think-john-dewey/", "isFamilyFriendly": true, "displayUrl": "https://www.themarginalian.org/2014/08/18/how-we-think-john-dewey", "snippet": "Much <b>like</b> getting lost helps us find ourselves, being uncertain drives us to reflect, to seek knowledge. The spark of thinking, Dewey argues, is a kind of psychological restlessness rooted in ambiguity \u2014 what John Keats memorably termed \u201cnegative capability\u201d \u2014 which precipitates our effort to resolve the unease by coming to, by way of reflection and deliberation, a conclusion:", "dateLastCrawled": "2022-02-02T21:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How to make my students <b>speak</b> more - Quora", "url": "https://www.quora.com/How-can-I-make-my-students-speak-more", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/How-can-I-make-my-students-<b>speak</b>-more", "snippet": "Answer (1 of 7): By engaging responses from them by asking them, at random, questions about what you are teaching. Spontaneously pick a student and ask them a question, this will keep your students on their toes because they won\u2019t know who you will pick and what the question will be. Mix it up by...", "dateLastCrawled": "2022-01-12T15:42:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Natural <b>Language Processing: How is perplexity related</b> to shannon ...", "url": "https://www.quora.com/Natural-Language-Processing-How-is-perplexity-related-to-shannon-information", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Natural-<b>Language-Processing-How-is-perplexity-related</b>-to-shannon...", "snippet": "Answer (1 of 2): If by Shannon information you are referring to the standard entropy formula H(p) = -\\int p(\\textbf{x}) \\log p(\\textbf{x}) d\\textbf{x} where p is a probability distribution, then the <b>perplexity</b> is simply 2^{H(p)}", "dateLastCrawled": "2022-01-18T03:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "What is neural language model?", "url": "https://treehozz.com/what-is-neural-language-model", "isFamilyFriendly": true, "displayUrl": "https://treehozz.com/what-is-neural-language-model", "snippet": "What is <b>perplexity</b> in of language model? <b>Perplexity</b> per word In natural language processing, <b>perplexity</b> is a way of evaluating language models. A language model is a probability distribution over entire sentences or texts. It is often possible to achieve lower <b>perplexity</b> on more specialized corpora, as they are more predictable. 39 Related Question Answers Found What is bigram language model? The Bigram Model. As the name suggests, the bigram model approximates the probability of a word ...", "dateLastCrawled": "2022-02-02T06:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "\u201cPrison Treated Me Way Better Than You\u201d: Reentry, <b>Perplexity</b>, and the ...", "url": "https://abolitionjournal.org/prison-treated-me-way-better/", "isFamilyFriendly": true, "displayUrl": "https://abolitionjournal.org/prison-treated-me-way-better", "snippet": "<b>Perplexity</b> is a conceptual platform to think about the experiential contradictions of globalization as a series of processes that often overwhelm subjects. As an analytic with multiple subtexts, <b>perplexity</b> is a way of marking the tension between overlapping, opposing, and asymmetric forces or fields of power. <b>Perplexity</b> indexes the puzzlement ...", "dateLastCrawled": "2022-01-30T00:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "The Autism <b>Perplexity</b>", "url": "https://beautyties.blogspot.com/2022/02/the-autism-perplexity.html", "isFamilyFriendly": true, "displayUrl": "https://beautyties.blogspot.com/2022/02/the-autism-<b>perplexity</b>.html", "snippet": "The Autism <b>Perplexity</b> February 01, 2022 This blog post is two years in the making. I&#39;ve been writing my thoughts as a parent of an autistic teenager and my notes on my Iphone read like a cryptic crossword in The Times. Trying to decipher my thoughts and my difficulty in figuring out the way forward. If it wasn&#39;t for Anna Kennedy, we would be in a worse situation having no information on what autism issues Natalie was struggling with. Autism is a puzzle I\u2019ve needed to learn about. I did a ...", "dateLastCrawled": "2022-02-01T17:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) &quot;What Was It You Showed Me?&quot; - <b>Perplexity</b> and Forgiveness: The ...", "url": "https://www.academia.edu/9599598/_What_Was_It_You_Showed_Me_Perplexity_and_Forgiveness_The_Tree_of_Life_as_Augustinian_Confession", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/9599598/_What_Was_It_You_Showed_Me_<b>Perplexity</b>_and_Forgiveness...", "snippet": "The <b>child</b> awakens in the mother\u2019s embrace, <b>learning</b> through his fundamental encounter with an orig- inal \u2018Thou\u2019 that he is \u201ccontained, a\ufb03rmed and loved in a relationship which is 10 incomprehensively encompassing, already actual, sheltering and nourishing.\u201d It is the original gift of welcoming love that makes possible Jack\u2019s early rapport with being, and his spontaneous faith in the goodness of nature. The experience of this gift makes possible the gratuity of play: \u201c[the ...", "dateLastCrawled": "2022-02-02T08:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>What perplexities do state-of-the-art language models achieve</b>? - Quora", "url": "https://www.quora.com/What-perplexities-do-state-of-the-art-language-models-achieve", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>What-perplexities-do-state-of-the-art-language-models-achieve</b>", "snippet": "Answer: There are a few benchmarks that people compare against for word-level language modeling. The difference in size, style, and pre-processing results in different challenges and thus different state-of-the-art perplexities. I will list a few: Penn Treebank. This contains articles from the ...", "dateLastCrawled": "2022-01-17T20:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "The Beggar Summary, Explanation Class 9 English Chapter 10", "url": "https://www.successcds.net/learn-english/class-9/the-beggar-class-9-cbse-english.html", "isFamilyFriendly": true, "displayUrl": "https://www.successcds.net/learn-english/class-9/the-beggar-class-9-cbse-english.html", "snippet": "<b>Perplexity</b>: state of being puzzled; bewilderment Irresolutely: hesitantly; undecidedly Gait: walk Inclination: interest toil: hard work . The beggar resembled a scarecrow. His reluctance to go to the shed was signified by his act of shrugging his shoulders. He followed the cook hesitantly. He was hesitant because he was hungry and although he wanted to work, he did not feel energetic enough to do it. As he had promised Sergei, he had to go to work. He did not feel strong enough because he ...", "dateLastCrawled": "2022-02-02T16:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "What <b>is another word for stuttering</b>? | Stuttering Synonyms - WordHippo ...", "url": "https://www.wordhippo.com/what-is/another-word-for/stuttering.html", "isFamilyFriendly": true, "displayUrl": "https://www.wordhippo.com/what-<b>is/another-word-for/stuttering</b>.html", "snippet": "Noun. A speech disorder in which the flow of speech is disrupted by involuntary repetitions and prolongations of sounds, syllables, words or phrases, and by involuntary silent pauses or blocks in which the stutterer is unable to produce sounds. The characteristic of being tentative. Lack of fluency. Adjective.", "dateLastCrawled": "2022-01-25T14:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Pride Prejudice and <b>Perplexity</b> Chapter 18: A Private Conversation, a ...", "url": "https://www.fanfiction.net/s/7500441/18/Pride-Prejudice-and-Perplexity", "isFamilyFriendly": true, "displayUrl": "https://<b>www.fanfiction.net</b>/s/7500441/18/Pride-Prejudice-and-<b>Perplexity</b>", "snippet": "&quot;I think you do not allow sufficiently for your sister&#39;s intelligence, Mr Darcy; she is no longer a <b>child</b>. Though she may feel some confusion upon <b>learning</b> the truth, it is, I believe, her right to know. I am certain that she will always love you as her brother \u2013 as you continue to love her as your sister, even though you know she is not. And ...", "dateLastCrawled": "2020-02-04T15:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Competitive Intelligence in a world of <b>perplexity</b>", "url": "https://kristinmbnewman.blogspot.com/", "isFamilyFriendly": true, "displayUrl": "https://kristinmbnewman.blogspot.com", "snippet": "Are you from a <b>similar</b> region, traveled somewhere <b>similar</b>, like the same sport, college, etc. TRUST me - people like to talk about themself or their passions. It is much more enjoyable for me too talking to someone from my home state or someone who likes my favorite NBA team. It helps set yourself apart too and people are more willing to talk openly about business when they share a bond with you. I&#39;m like this anyway with no motive really because I like people in general, but if you&#39;re just ...", "dateLastCrawled": "2021-12-26T06:09:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) Philosophy for Children and Children for Philosophy ...", "url": "https://www.academia.edu/36827477/Philosophy_for_Children_and_Children_for_Philosophy_Possibilities_and_Problems", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/36827477/Philosophy_for_<b>Child</b>ren_and_<b>Child</b>ren_for_Philosophy...", "snippet": "The <b>child</b>\u2019s questions <b>can</b> disclose the culture we have been initiated into and raise further questions of our consent to it. In this sense, the questions <b>can</b> remind adults of their own childhood. <b>Learning</b> <b>to speak</b> and act in a culture means accepting \u201cwhat my \u2018elders\u2019 do as consequential\u201d (Cavell 1979: 28). I give tacit consent to the social and cultural world I inherit. Still, it also means that my elders \u201chave to accept, even applaud, what I say and do as what they say and do ...", "dateLastCrawled": "2022-01-27T00:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Design Scandal \u2013 Perplexity</b> \u2013 Kairosis", "url": "https://kairosis.wordpress.com/article/design-scandal-perplexity-13h9pp7wb34y8-11/", "isFamilyFriendly": true, "displayUrl": "https://kairosis.wordpress.com/article/<b>design-scandal-perplexity</b>-13h9pp7wb34y8-11", "snippet": "Project-driven <b>learning</b> may be the outcome of <b>perplexity</b>, however, it is not tied to <b>perplexity</b> and the engagement of the learner. Project-driven <b>learning</b> has no need of <b>perplexity</b> \u2013 here problems <b>can</b> be external to the learner. For example, the professional need to have a particular technical skill is directly owned by the profession and may only be indirectly acknowledged by the learner. Often the acquisition of a particular technical skill is disputed by the learner as genuinely ...", "dateLastCrawled": "2022-01-24T16:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "The Autism <b>Perplexity</b>", "url": "https://beautyties.blogspot.com/2022/02/the-autism-perplexity.html", "isFamilyFriendly": true, "displayUrl": "https://beautyties.blogspot.com/2022/02/the-autism-<b>perplexity</b>.html", "snippet": "The Autism <b>Perplexity</b> February 01, 2022 This blog post is two years in the making. I&#39;ve been writing my thoughts as a parent of an autistic teenager and my notes on my Iphone read like a cryptic crossword in The Times. Trying to decipher my thoughts and my difficulty in figuring out the way forward. If it wasn&#39;t for Anna Kennedy, we would be in a worse situation having no information on what autism issues Natalie was struggling with. Autism is a puzzle I\u2019ve needed to learn about. I did a ...", "dateLastCrawled": "2022-02-01T17:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "TAKING THE <b>PERPLEXITY</b> OUT OF PROVERBS May-July 2021", "url": "https://hopelooksup.org/wp-content/uploads/2021/08/TAKING-THE-PERPLEXITY-OUT-OF-PROVERBS-May-July-2021.pdf", "isFamilyFriendly": true, "displayUrl": "https://hopelooksup.org/wp-content/uploads/2021/08/TAKING-THE-<b>PERPLEXITY</b>-OUT-OF...", "snippet": "negative and positive characteristics; my hope is that we will have the complete <b>thought</b> that God and the Book of Proverbs has for us. Each of these 10 studies of opposite characteristics will help us to see how we <b>can</b> gain the most from proverbial truths. 1. \u201cBecoming Wise and Refusing to Become Foolish\u201d", "dateLastCrawled": "2021-11-20T16:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "How We Think: John Dewey on the Art of Reflection and Fruitful ...", "url": "https://www.themarginalian.org/2014/08/18/how-we-think-john-dewey/", "isFamilyFriendly": true, "displayUrl": "https://www.themarginalian.org/2014/08/18/how-we-think-john-dewey", "snippet": "Demand for the solution of a <b>perplexity</b> is the steadying and guiding factor in the entire process of reflection\u2026 This need of straightening out a <b>perplexity</b> also controls the kind of inquiry undertaken. A traveler whose end is the most beautiful path will look for other considerations and will test suggestions occurring to him on another principle than if he wishes to discover the way to a given city. The problem fixes the end of <b>thought</b> and the end controls the process of thinking. This ...", "dateLastCrawled": "2022-02-02T21:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "The <b>Socratic Method</b>: What it is and How to Use it in the Classroom ...", "url": "https://tomprof.stanford.edu/posting/810", "isFamilyFriendly": true, "displayUrl": "https://tomprof.stanford.edu/posting/810", "snippet": "UP NEXT: My <b>Child</b> Doesn&#39;t Test Well. Tomorrow&#39;s Teaching and <b>Learning</b> ----- 1,864 words -----The <b>Socratic Method</b>: What it is and How to Use it in the Classroom Political Science professor Rob Reich, recipient of the 2001 Walter J. Gores Award for Teaching Excellence, delivered a talk on May 22, 2003 as part of the Center for Teaching and <b>Learning</b>&#39;s Award Winning Teachers on Teaching lecture series. In his talk, Professor Reich discussed the <b>Socratic method</b> of teaching-a method which has ...", "dateLastCrawled": "2022-02-03T07:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Going for perplexity</b> | The Jewish Standard", "url": "https://jewishstandard.timesofisrael.com/going-for-perplexity/", "isFamilyFriendly": true, "displayUrl": "https://jewishstandard.timesofisrael.com/<b>going-for-perplexity</b>", "snippet": "<b>Going for perplexity</b>. The video burst on YouTube in September 2011 with an abundance of energy and good cheer. \u201cDip your apple in the honey,\u201d sang the young Israelis, in a parody filled with catchy music, dancing, shofar blowing, humor, adorable children, and even a Jedi battle. The video, so far viewed more than 3 million times (a lot for ...", "dateLastCrawled": "2022-01-24T20:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Plato&#39;s Meno Plot, Analysis, and Commentary on virtue", "url": "https://www.thoughtco.com/platos-meno-2670343", "isFamilyFriendly": true, "displayUrl": "https://www.<b>thought</b>co.com/platos-meno-2670343", "snippet": "The enslaved boy demonstration: Meno asks Socrates if he <b>can</b> prove that &quot;all <b>learning</b> is recollection.&quot; ... something; they now realize their belief was mistaken; but this new awareness of their own ignorance, this feeling of <b>perplexity</b>, is, in fact, an improvement. Socrates then proceeds to guide the boy to the right answer: you double the area of a square by using its diagonal as the basis for the larger square. He claims at the end to have demonstrated that the boy in some sense already ...", "dateLastCrawled": "2022-02-02T12:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>What perplexities do state-of-the-art language models achieve</b>? - Quora", "url": "https://www.quora.com/What-perplexities-do-state-of-the-art-language-models-achieve", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>What-perplexities-do-state-of-the-art-language-models-achieve</b>", "snippet": "Answer: There are a few benchmarks that people compare against for word-level language modeling. The difference in size, style, and pre-processing results in different challenges and thus different state-of-the-art perplexities. I will list a few: Penn Treebank. This contains articles from the ...", "dateLastCrawled": "2022-01-17T20:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Pride Prejudice and <b>Perplexity</b> Chapter 18: A Private Conversation, a ...", "url": "https://www.fanfiction.net/s/7500441/18/Pride-Prejudice-and-Perplexity", "isFamilyFriendly": true, "displayUrl": "https://<b>www.fanfiction.net</b>/s/7500441/18/Pride-Prejudice-and-<b>Perplexity</b>", "snippet": "And she will have the joy of <b>learning</b> that she has another brother \u2013 as you have had the joy of discovering a sister, in Julia.&quot; &quot;You are quite correct, Miss Bennet; how fortunate I am to have your incisive mind to help me unravel this conundrum. I <b>can</b> see, now, that I am too close to it all, to view it as clearly as you. Georgiana must be ...", "dateLastCrawled": "2020-02-04T15:39:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Philosophy and language learning</b> | SpringerLink", "url": "https://link.springer.com/article/10.1007/s11217-006-9013-3", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11217-006-9013-3", "snippet": "Wonder freezes to <b>perplexity</b>; the lines of thought opened by wonder are sedimented into seemingly impossible structures; it is as though children learn <b>to speak</b> on the basis of far too little. The input seems incredibly poor <b>compared</b> to the output: take almost any concept, our explanations are quite patchy, little more than gestures, and the applications children are exposed to amount to a tiny fraction of those possible. All they see is a fragment of the concept. Yet they learn.", "dateLastCrawled": "2022-01-10T22:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "\u201cPrison Treated Me Way Better Than You\u201d: Reentry, <b>Perplexity</b>, and the ...", "url": "https://abolitionjournal.org/prison-treated-me-way-better/", "isFamilyFriendly": true, "displayUrl": "https://abolitionjournal.org/prison-treated-me-way-better", "snippet": "<b>Perplexity</b> is a conceptual platform to think about the experiential contradictions of globalization as a series of processes that often overwhelm subjects. As an analytic with multiple subtexts, <b>perplexity</b> is a way of marking the tension between overlapping, opposing, and asymmetric forces or fields of power. <b>Perplexity</b> indexes the puzzlement ...", "dateLastCrawled": "2022-01-30T00:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Children <b>Learning</b> Reading Review - Amazing Reading Program Parents Love ...", "url": "http://www.cbmreviews.com/children-learning-reading-review/", "isFamilyFriendly": true, "displayUrl": "www.cbmreviews.com/<b>child</b>ren-<b>learning</b>-reading-review", "snippet": "Children <b>Learning</b> Reading is a step-by-step program that helps parents to easily teach their children to read. As long as your <b>child</b> is able <b>to speak</b>, this guide will help you teach your <b>child</b> to read fluently. However this program is designed for parents who have children between the ages of 2 to 6 years old.", "dateLastCrawled": "2022-01-11T13:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Critical Analysis Of &#39;Single Parent Struggle</b> On The Family&#39; | <b>ipl.org</b>", "url": "https://www.ipl.org/essay/Critical-Analysis-Of-Single-Parent-Struggle-On-PKHZDQKRJE86", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ipl.org</b>/essay/<b>Critical-Analysis-Of-Single-Parent-Struggle</b>-On-PKHZDQKRJE86", "snippet": "Lead In: Cultural conflict in a family <b>can</b> lead to many events that <b>can</b> affect a <b>child</b>\u2019s life. The <b>child</b> may become confused on what life to live or how to live it, especially when their goal is to ultimately make their parents proud. The <b>child</b> will also have a hard time growing up as he or she tries to figure out what path to choose regarding culture. Cultural conflict though, <b>can</b> make a person become stronger and give them a sense of being their own person II. Introduction Paragraph 2 ...", "dateLastCrawled": "2022-01-13T12:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>How Smart Is My Labrador</b>? \u2013 Loving Your Lab", "url": "https://www.lovingyourlab.com/how-smart-is-my-labrador/", "isFamilyFriendly": true, "displayUrl": "https://www.lovingyourlab.com/<b>how-smart-is-my-labrador</b>", "snippet": "It\u2019s remarkable that an animal without the ability <b>to speak</b> <b>can</b> recognize more spoken words than the average 12-month-old <b>child</b>. According to Dr. Coren, Labradors are among the breeds that <b>can</b> learn as many as 250 words, as well as visual and auditory signals. That\u2019s about 85 more words than the average dog. This is an example of a dog\u2019s working intelligence. In his book, The Intelligence of Dogs, Dr. Coren identified three distinct types of dog intelligence: instinctive (the ...", "dateLastCrawled": "2022-01-29T08:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Plato <b>and Socrates: From an Educator of Childhood</b> to a Childlike ...", "url": "https://link.springer.com/article/10.1007/s11217-012-9348-x", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11217-012-9348-x", "snippet": "Maybe some implications for the theme of <b>child</b> as educator <b>can</b> be anticipated at this point. All these conditions for philosophising seem more childlike than adultlike. Let\u2019s focus on time: chronos and aion are two Greek words denoting time. The former designates the \u201cnumber of movements according to before-after relationships\u201d (Aristotle, Physics, IV 11, 219b); the latter \u201ca <b>child</b> playing a game of oppositions; the realm of a <b>child</b>\u201d (Heraclitus, fragment 52). Adult experience of ...", "dateLastCrawled": "2022-01-11T20:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Soul-filled teaching and learning</b> - CYC-Net: The International <b>Child</b> ...", "url": "https://cyc-net.org/cyc-online/cyconline-may2010-vanbockern.html", "isFamilyFriendly": true, "displayUrl": "https://cyc-net.org/cyc-online/cyconline-may2010-vanbockern.html", "snippet": "While there is a long history in education of attending to emotional <b>learning</b>, Goleman&#39;s book, Emotional Intelligence: Why It <b>Can</b> Matter More than IQ (1997) created renewed interest in something as subjective as human emotions. Feelings left the realm of \u201csoft science\u201d and became worthy of important research. The education of the heart took on new meaning. With the advent of brain imaging techniques, researchers were able to go beyond the traditional experimentation with laboratory ...", "dateLastCrawled": "2021-10-27T12:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Torch | Language modeling a billion words", "url": "http://torch.ch/blog/2016/07/25/nce.html?imm_mid=0e6973&cmp=em-data-na-na-newsltr_20160810", "isFamilyFriendly": true, "displayUrl": "torch.ch/blog/2016/07/25/nce.html?imm_mid=0e6973&amp;cmp=em-data-na-na-newsltr_20160810", "snippet": "If you are only interested in generated samples, <b>perplexity</b> and <b>learning</b> curves, please jump to the results section. Word versus character language models . In recent months you may have noticed increased interest in generative character-level RNNLMs like char-rnn and the more recent torch-rnn. These models are very interesting as they <b>can</b> be used to generate sequences of characters like the following: &lt; post &gt; Diablo &lt; comment score = 1 &gt; I liked this game so much!! Hope telling that ...", "dateLastCrawled": "2021-12-11T16:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Is <b>deep learning harder than regular machine learning</b>? - Quora", "url": "https://www.quora.com/Is-deep-learning-harder-than-regular-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Is-<b>deep-learning-harder-than-regular-machine-learning</b>", "snippet": "Answer (1 of 2): Yes and No. Deep <b>learning</b> has some advantages over regular machine <b>learning</b>. For one, you don\u2019t need to come up with features since deep neural networks are capable of extracting features from raw data on their own. This makes it easier to get started with deep <b>learning</b> since y...", "dateLastCrawled": "2022-01-27T20:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How to learn deep <b>learning</b> practically - Quora", "url": "https://www.quora.com/How-can-I-learn-deep-learning-practically", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/How-<b>can</b>-I-learn-deep-<b>learning</b>-practically", "snippet": "Answer (1 of 3): In order to learn Deep <b>Learning</b> practically, you should have a good understanding of what is Deep <b>Learning</b> and how is it implemented in real life scenarios. These days, Deep <b>Learning</b> has become the most debated technology of the 21st century. Techies are interested in <b>learning</b> D...", "dateLastCrawled": "2022-01-24T23:01:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Better Word Representation Vectors Using Syllabic Alphabet: A Case ...", "url": "https://res.mdpi.com/d_attachment/applsci/applsci-09-03648/article_deploy/applsci-09-03648.pdf", "isFamilyFriendly": true, "displayUrl": "https://res.mdpi.com/d_attachment/applsci/applsci-09-03648/article_deploy/applsci-09...", "snippet": "model; <b>perplexity</b>; word <b>analogy</b> 1. Introduction Natural language processing (NLP) relies on word embeddings as input for <b>machine</b> <b>learning</b> or deep <b>learning</b> algorithms. For decades, NLP solutions were restricted to <b>machine</b> <b>learning</b> approaches that trained on handcrafted, high dimensional and sparse features [1]. Nowadays, the trend is neural networks [2], which use dense vector representations. Hence, the superior results on NLP tasks is attributed to word embeddings [3,4] and deep <b>learning</b> ...", "dateLastCrawled": "2021-12-31T08:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "N-gram language models. Part 1: The <b>unigram</b> model | by Khanh Nguyen ...", "url": "https://medium.com/mti-technology/n-gram-language-model-b7c2fc322799", "isFamilyFriendly": true, "displayUrl": "https://medium.com/mti-technology/n-gram-language-model-b7c2fc322799", "snippet": "For example, \u201cstatistics\u201d is a <b>unigram</b> (n = 1), \u201c<b>machine</b> <b>learning</b>\u201d is a bigram (n = 2), \u201cnatural language processing\u201d is a trigram (n = 3), and so on. For longer n-grams, people just ...", "dateLastCrawled": "2022-02-03T03:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "NLP with LDA: Analyzing Topics in the <b>Enron</b> Email dataset | by Sho Fola ...", "url": "https://medium.datadriveninvestor.com/nlp-with-lda-analyzing-topics-in-the-enron-email-dataset-20326b7ae36f", "isFamilyFriendly": true, "displayUrl": "https://medium.datadriveninvestor.com/nlp-with-lda-analyzing-topics-in-the-<b>enron</b>-email...", "snippet": "A low <b>perplexity</b> indicates the probability distribution is good at predicting the sample. Said differently: <b>Perplexity</b> tries to measure how this model is surprised when it is given a new dataset \u2014 Sooraj Subrahmannian. So, when comparing models a lower <b>perplexity</b> score is a good sign. The less the surprise the better. Here\u2019s how we compute ...", "dateLastCrawled": "2022-01-29T12:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Human\u2013machine dialogue modelling with the fusion</b> of word- and sentence ...", "url": "https://www.sciencedirect.com/science/article/pii/S0950705119305970", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0950705119305970", "snippet": "However, <b>machine</b> <b>learning</b> ... <b>Perplexity</b>, and Accuracy, and then look into the quality of generation and the ability to express emotions of the model. 5.1. Experiment settings. As we discussed in the previous sections, after mapping into the VAD space, both the dimensions of emotional word embeddings and that of emotional features of the sentence are 3. To control the computational scale, we set the size of vocabulary size to 20,000, the dimensions of the word embedding to 128, the batch ...", "dateLastCrawled": "2021-11-25T13:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Word2Vec in Gensim Explained for Creating Word Embedding Models ...", "url": "https://machinelearningknowledge.ai/word2vec-in-gensim-explained-for-creating-word-embedding-models-pretrained-and-custom/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>knowledge.ai/word2vec-in-gensim-explained-for-creating-word...", "snippet": "<b>Machine</b> <b>learning</b> and ... This is another way putting that word2vec can draw the <b>analogy</b> that if Man is to Woman then Kind is to Queen! The publicly released model of word2vec by Google consists of 300 features and the model is trained in the Google news dataset. The vocabulary size of the model is around 1.6 billion words. However, this might have taken a huge time for the model to be trained on but they have applied a method of simple subsampling approach to optimize the time. Word2Vec ...", "dateLastCrawled": "2022-02-02T15:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Beginner\u2019s Guide to LDA <b>Topic</b> Modelling with R | by Farren tang ...", "url": "https://towardsdatascience.com/beginners-guide-to-lda-topic-modelling-with-r-e57a5a8e7a25", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/beginners-guide-to-lda-<b>topic</b>-modelling-with-r-e57a5a8e7a25", "snippet": "In <b>machine</b> <b>learning</b> and natural language processing, a <b>topic</b> model is a type of statistical model for discovering the abstract \u201ctopics\u201d that occur in a collection of documents. - wikipedia. After a formal introduction to <b>topic</b> modelling, the remaining part of the article will describe a step by step process on how to go about <b>topic</b> modeling ...", "dateLastCrawled": "2022-01-31T23:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Understanding UMAP - PAIR", "url": "https://pair-code.github.io/understanding-umap/", "isFamilyFriendly": true, "displayUrl": "https://pair-code.github.io/understanding-umap", "snippet": "Dimensionality reduction is a powerful tool for <b>machine</b> <b>learning</b> practitioners to visualize and understand large, high dimensional datasets. One of the most widely used techniques for visualization is t-SNE, but its performance suffers with large datasets and using it correctly can be challenging.. UMAP is a new technique by McInnes et al. that offers a number of advantages over t-SNE, most notably increased speed and better preservation of the data&#39;s global structure. In this article, we&#39;ll ...", "dateLastCrawled": "2022-02-03T02:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Software crowdsourcing task pricing based on topic model analysis ...", "url": "https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/iet-sen.2019.0168", "isFamilyFriendly": true, "displayUrl": "https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/iet-sen.2019.0168", "snippet": "PTMA integrates six <b>machine</b> <b>learning</b> algorithms and three <b>analogy</b>-based models for topic-based pricing analysis. The proposed PTMA approach is evaluated using 2016 software crowdsourcing tasks extracted from TopCoder, the largest software crowdsourcing platform. The results show that (i) textual task requirement information can be used to predict software crowdsourcing task prices, based on topic model analysis; (ii) the best predictor in PTMA, based on logistic regression, achieves an ...", "dateLastCrawled": "2022-01-29T04:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "The Illustrated GPT-2 (Visualizing Transformer Language Models) \u2013 Jay ...", "url": "http://jalammar.github.io/illustrated-gpt2/", "isFamilyFriendly": true, "displayUrl": "jalammar.github.io/illustrated-gpt2", "snippet": "Discussions: Hacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments) Translations: Korean, Russian This year, we saw a dazzling application of <b>machine</b> <b>learning</b>. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn\u2019t a particularly novel architecture \u2013 it\u2019s architecture is very similar to the decoder-only transformer.", "dateLastCrawled": "2022-02-01T04:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Solve Artificial Intelligence | HackerRank", "url": "https://www.hackerrank.com/domains/ai?filters%5Bsubdomains%5D%5B%5D=nlp", "isFamilyFriendly": true, "displayUrl": "https://www.hackerrank.com/domains/ai?filters[subdomains][]=nlp", "snippet": "Develop intelligent agents. Challenges related to bot-building, path planning, search techniques and Game Theory. Exercise your creativity in heuristic design.", "dateLastCrawled": "2021-05-25T20:07:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>machine</b> <b>learning</b> - How may I <b>convert Perplexity to F Measure</b> - Cross ...", "url": "https://stats.stackexchange.com/questions/204402/how-may-i-convert-perplexity-to-f-measure", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/204402", "snippet": "In the practice of <b>Machine</b> <b>Learning</b> accuracy of some models are determined by perplexity, (like LDA), while many of them (Naive Bayes, HMM,etc..) by F Measure. I like to evaluate all the models with some common standards. I am looking to convert perplexity values to precision, recall, f measure etc. Is there a way to do it? Or may I calculate F Measure for LDA? I am using Python&#39;s NLTK library for Naive Bayes, HMM, etc and Gensim for LDA. I am using Python2.7+ on MS-Windows. If any one may ...", "dateLastCrawled": "2022-01-09T20:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "US20040148164A1 - Dual search acceleration technique for speech ...", "url": "https://patents.google.com/patent/US20040148164A1/en", "isFamilyFriendly": true, "displayUrl": "https://patents.google.com/patent/US20040148164A1/en", "snippet": "A speech recognition method, system and program product, the method in one embodiment comprising: obtaining input speech data; initiating a first speech recognition search process with at least one hypothesis; initiating a second speech recognition search process with a plurality of hypotheses; obtaining partial results from the second speech recognition search process, where the partial results include an evaluation of at least one hypothesis that the first speech recognition search process ...", "dateLastCrawled": "2022-01-29T22:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "US20040158468A1 - Speech recognition with soft pruning - Google Patents", "url": "https://patents.google.com/patent/US20040158468A1/en", "isFamilyFriendly": true, "displayUrl": "https://patents.google.com/patent/US20040158468A1/en", "snippet": "A method, program product, and system for speech recognition, the method comprising in one embodiment pruning a hypothesis based on a first criteria; storing information about the pruned hypothesis; and reactivating the pruned hypothesis if a second criterion is met. In an embodiment, the first criteria may be that another hypothesis has a better score at that time by some predetermined amount. In an embodiment, the stored information may comprise at least one of a score for the pruned ...", "dateLastCrawled": "2022-01-21T21:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "The <b>Project Gutenberg</b> eBook of <b>First</b> Principles, by Herbert Spencer", "url": "https://www.gutenberg.org/files/55046/55046-h/55046-h.htm", "isFamilyFriendly": true, "displayUrl": "https://<b>www.gutenberg.org</b>/files/55046/55046-h/55046-h.htm", "snippet": "<b>Learning</b> by long experience that they can, if needful, be verified, we are led habitually to accept them without verification. And thus we open the door to some which profess to stand for known things, but which really stand for things that cannot be known in any way. To sum up, we must say of conceptions in general, that they are complete only when the attributes of the object conceived are of such number and kind that they can be represented in consciousness so nearly at the same time as ...", "dateLastCrawled": "2021-12-03T22:00:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>New Game: Dreamy Perplexity</b> | c0deb0t&#39;s Blog", "url": "https://c0deb0t.wordpress.com/2017/04/10/new-game-dreamy-perplexity/", "isFamilyFriendly": true, "displayUrl": "https://c0deb0t.wordpress.com/2017/04/10/<b>new-game-dreamy-perplexity</b>", "snippet": "Algorithms, <b>machine</b> <b>learning</b>, and game dev. Primary Menu Menu. Home; Finished Projects; Tutorials; Experiences, Tips, &amp; Tricks; About; <b>New Game: Dreamy Perplexity</b> . April 10, 2017 April 10, 2017 c0deb0t. It has been a while since I\u2019ve updated this website. I have been busy with coding this new game in Unreal Engine 4 for the last 3-4 weeks. This game, called Dreamy <b>Perplexity, is similar</b> to my last game, Two Bot\u2019s Journey. However, I am going to support mobile platforms, like Android and ...", "dateLastCrawled": "2022-01-14T11:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Reservoir Transformers: train faster with fewer</b> parameters, and get ...", "url": "https://medium.com/@LightOnIO/reservoir-transformers-train-faster-with-fewer-parameters-and-get-better-results-e24b2584949", "isFamilyFriendly": true, "displayUrl": "https://<b>medium</b>.com/@LightOnIO/<b>reservoir-transformers-train-faster-with-fewer</b>...", "snippet": "The pretraining <b>perplexity is similar</b>, the training time is reduced up to ... LightOn is a hardware company that develops new optical processors that considerably speed up <b>Machine</b> <b>Learning</b> ...", "dateLastCrawled": "2021-08-20T09:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Mapping the technology evolution path: a novel</b> model for dynamic topic ...", "url": "https://link.springer.com/article/10.1007/s11192-020-03700-5", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11192-020-03700-5", "snippet": "It can be seen that their algorithm performance on the <b>perplexity is similar</b>. However, the perplexity of LDA decreases very slowly (the number of iterations needs to be 2000), and the final convergence value of the perplexity is higher than others. It can be seen that the algorithm performance of CIHDP and HDP on the perplexity is better than LDA (Fig. 4). Fig. 4. Perplexity curve of LDA trained by Citeseer. Full size image. In the process of topic modeling for Cora and Aminer, we also found ...", "dateLastCrawled": "2022-02-01T10:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Learning</b> K-way D-<b>dimensional Discrete Code For Compact</b> Embedding ...", "url": "https://deepai.org/publication/learning-k-way-d-dimensional-discrete-code-for-compact-embedding-representations", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>learning</b>-k-way-d-<b>dimensional-discrete-code-for-compact</b>...", "snippet": "For the discrete code <b>learning</b>, we have three cases: random assignment, code learned by a linear transformation, and code learned by a LSTM transformation function; the latter two can also be utilized in the symbol embedding re-<b>learning</b> model. Firstly, we observe that the discrete code <b>learning</b> is critical for KD encoding, as random discrete codes produce much worse performance. Secondly, we observe that with appropriate code <b>learning</b>, the test <b>perplexity is similar</b> or better compared to the ...", "dateLastCrawled": "2021-12-03T11:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "LightOn Meetup #11 with Douwe Kiela (FAIR) | Reservoir Transformers", "url": "https://lighton.ai/blog/summary-of-lighton-ai-meetup-12-reservoir-transformers/", "isFamilyFriendly": true, "displayUrl": "https://lighton.ai/blog/summary-of-lighton-ai-meetup-12-reservoir-transformers", "snippet": "Software is eating the world, <b>machine</b> <b>learning</b> is eating software, and, well, transformers \ud83e\udd16 are eating <b>machine</b> <b>learning</b>. ... The pretraining <b>perplexity is similar</b>, the training time is reduced up to 25%, and, strikingly, the downstream performance is better overall! Reservoir layers seem to improve efficiency and generalization, acting as \u201ccheap\u201d additional parameters. The better efficiency stems from \ud83e\udd98 skipping the weight update portion for some of the weights (this is so simple ...", "dateLastCrawled": "2022-01-12T05:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Unsupervised language model adaptation</b> for handwritten Chinese text ...", "url": "https://www.sciencedirect.com/science/article/pii/S0031320313003877", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0031320313003877", "snippet": "The <b>perplexity is similar</b> to the negative log-likelihood of the language model on the text C. They show that lower perplexity indicates a better model. Each n-gram model above (e.g, cbi, cti.) can be seen as a discrete probability distribution on all n-grams, which can be represented as a vector with the dimensionality as the number of all n-grams. This concept of vector representation will be adopted in the following sections. 5. Language model adaptation. This section presents three ...", "dateLastCrawled": "2022-01-22T07:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Bayesian Nonparametric Topic Modeling Hierarchical Dirichlet Processes</b>", "url": "https://www.slideshare.net/NoSyu/bayesian-nonparametric-topic-modeling-hierarchical-dirichlet-processes", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/NoSyu/<b>bayesian-nonparametric-topic-modeling-hierarchical</b>...", "snippet": "Christopher M Bishop and Nasser M Nasrabadi, Pattern recognition and <b>machine</b> <b>learning</b>, vol. 1, springer New York, 2006. David M Blei, Andrew Y Ng, and Michael I Jordan, Latent dirichlet allocation, the Journal of <b>machine</b> <b>Learning</b> research 3 (2003), 993\u20131022. Emily B Fox, Erik B Sudderth, Michael I Jordan, and Alan S Willsky, An hdp-hmm for ...", "dateLastCrawled": "2022-01-21T17:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) <b>Describing Verbs in Disjoining Writing Systems</b>", "url": "https://www.researchgate.net/publication/221005900_Describing_Verbs_in_Disjoining_Writing_Systems", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221005900_Describing_Verbs_in_Disjoining...", "snippet": "<b>machine</b>-readable dictionary resources and from printed re- sources using optical character recognition, the addition of derivational morpho logy and the develop- ment of morphological guessers.", "dateLastCrawled": "2021-10-01T18:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) <b>Philosophy of the Internet: A Discourse</b> on the Nature of the ...", "url": "https://www.academia.edu/14386742/Philosophy_of_the_Internet_A_Discourse_on_the_Nature_of_the_Internet", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/14386742/<b>Philosophy_of_the_Internet_A_Discourse</b>_on_the_Nature...", "snippet": "Academia.edu is a platform for academics to share research papers.", "dateLastCrawled": "2022-01-06T22:32:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Plato and Dionysis | Plato | Socrates - Scribd", "url": "https://www.scribd.com/document/7237753/Plato-and-Dionysis", "isFamilyFriendly": true, "displayUrl": "https://www.scribd.com/document/7237753/Plato-and-Dionysis", "snippet": "The sophists placed great emphasis on rote <b>learning</b> and listening to lectures. Socrates, ... avoid them. [WC:XV] <b>Just as perplexity</b> and the process of cure are deeply unpleasant so enlightenment brings jouissance and delight. The repetitious, open-ended, interrogative method\u2014prompting people to self-knowledge\u2014can generate a peculiar kind of intellectual excitement. The whole soul of man seems to be brought into activity. We do not merely register an answer or acquiesce to a piece of ...", "dateLastCrawled": "2022-01-05T12:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Wittgenstein, Plato, and The Historical Socrates - M. W. Rowe | Plato ...", "url": "https://www.scribd.com/document/230792154/Wittgenstein-Plato-And-the-Historical-Socrates-M-W-Rowe", "isFamilyFriendly": true, "displayUrl": "https://www.scribd.com/document/230792154/Wittgenstein-Plato-And-the-Historical...", "snippet": "Plato, Socrates, Wittgenstein", "dateLastCrawled": "2022-01-05T14:32:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Assessing Single-Cell Transcriptomic Variability through Density ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8195812/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8195812", "snippet": "<b>Perplexity can be thought of as</b> a \u201csmooth\u201d analog of the number of nearest neighbors and is formally defined as Perp i = 2 H i, where H i denotes the entropy of the conditional distribution P \u00b7|i: H i = \u2212 \u2211 j P j \u2223 i log 2 P j \u2223 i. (7) Since perplexity monotonically increases in \u03c3 i (more points are significantly represented in P \u00b7|i as \u03c3 i increases), t-SNE performs a binary search on each \u03c3 i to obtain a constant perplexity for all i. UMAP\u2019s length-scale selection is ...", "dateLastCrawled": "2021-10-20T13:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>How t-SNE</b> works \u2014 openTSNE 0.3.13 documentation", "url": "https://opentsne.readthedocs.io/en/latest/tsne_algorithm.html", "isFamilyFriendly": true, "displayUrl": "https://opentsne.readthedocs.io/en/latest/tsne_algorithm.html", "snippet": "<b>Perplexity can be thought of as</b> a continuous analogue to the \\(k\\) nearest neighbours, to which t-SNE will attempt to preserve ... Journal of <b>machine</b> <b>learning</b> research 9.Nov (2008): 2579-2605. [2] (1, 2) Van Der Maaten, Laurens. \u201cAccelerating t-SNE using tree-based algorithms.\u201d The Journal of <b>Machine</b> <b>Learning</b> Research 15.1 (2014): 3221-3245. [3] (1, 2) Linderman, George C., et al. \u201cEfficient Algorithms for t-distributed Stochastic Neighborhood Embedding.\u201d arXiv preprint arXiv:1712 ...", "dateLastCrawled": "2022-01-30T23:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "GitHub - krishnarevi/NLP_Evaluation_Metrics", "url": "https://github.com/krishnarevi/NLP_Evaluation_Metrics", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/krishnarevi/NLP_Evaluation_Metrics", "snippet": "<b>Machine</b> <b>learning</b> model to detect sentiment of movie reviews from IMDb dataset using PyTorch and TorchText. ... Intuitively, <b>Perplexity can be thought of as</b> an evaluation of the model\u2019s ability to predict uniformly among the set of specified tokens in a corpus. Smaller the perplexity better the model . Here we can observe perplexity for train set keep on decreasing ,which is good. But for validation set it increases after dip in some initial epochs . This might be due to overfitting of our ...", "dateLastCrawled": "2022-02-03T06:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Lerna: Transformer Architectures for Configuring Error Correction Tools ...", "url": "https://deepai.org/publication/lerna-transformer-architectures-for-configuring-error-correction-tools-for-short-and-long-read-genome-sequencing", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/lerna-transformer-architectures-for-configuring-error...", "snippet": "<b>Perplexity can be thought of as</b> the probability of selecting a given word uniformly from the effective vocabulary, given some context. Thus, a lower perplexity indicates that the LM is better at making predictions. If an LM learns a uniform probability distribution over a given piece of text, the perplexity generated would be equal to the actual vocabulary size. The smaller the perplexity score is, compared to the actual vocabulary size, the better is the LM at <b>learning</b> patterns that occur ...", "dateLastCrawled": "2022-01-22T16:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Transformers, Roll Out!", "url": "https://christina.kim/2020/11/06/transformers-roll-out/", "isFamilyFriendly": true, "displayUrl": "https://christina.kim/2020/11/06/transformers-roll-out", "snippet": "<b>Perplexity can be thought of as</b> the measure of uncertainty your model has for predictions. So the lower the perplexity, the higher confidence your model has about it\u2019s predictions. Bits per word, or character, can be thought of as the entropy of the language. BPW measures the average number of bits required to encode the word. Given a language\u2019s probability of P and our model\u2019s learned probability Q, cross-entropy measures the total average amount of bits needed to represent events ...", "dateLastCrawled": "2022-02-02T08:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Assessing single-cell transcriptomic variability through density</b> ...", "url": "https://www.nature.com/articles/s41587-020-00801-7", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41587-020-00801-7", "snippet": "<b>Perplexity can be thought of as</b> a \u2018smooth\u2019 analog of the number of nearest neighbors and is formally defined ... T. L. Detecting racial bias in algorithms and <b>machine</b> <b>learning</b>. J. Inf. Commun ...", "dateLastCrawled": "2022-02-02T09:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) Why I like it: <b>multi-task learning for recommendation and explanation</b>", "url": "https://www.researchgate.net/publication/327947836_Why_I_like_it_multi-task_learning_for_recommendation_and_explanation", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/327947836", "snippet": "natively, <b>perplexity can be thought of as</b> a \u201cbranching\u201d factor, i.e., if we pick the word from the probability distribution given by the . language model, how many times in average do we need ...", "dateLastCrawled": "2021-12-07T13:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Questions and answers for dimensionality reductions</b>", "url": "http://www.datasciencelovers.com/blog/important-questions-and-answers-for-dimensionality-reductions/", "isFamilyFriendly": true, "displayUrl": "www.datasciencelovers.com/blog/important-<b>questions-and-answers-for-dimensionality</b>...", "snippet": "PCA is a very common way to speed up your <b>Machine</b> <b>Learning</b> algorithm by getting rid of correlated variables which don\u2019t contribute in any decision making. Improve Visualization \u2013 It is very hard to visualize and understand the data in high dimensions. PCA transforms a high dimensional data to low dimensional data (2 dimension) so that it can be visualized easily. Following are the limitation of PCA. Independent variable become less interpretable \u2013 After implementing PCA on the dataset ...", "dateLastCrawled": "2022-02-01T15:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>ML interview questions and answers</b>", "url": "http://www.datasciencelovers.com/tag/ml-interview-questions-and-answers/", "isFamilyFriendly": true, "displayUrl": "www.datasciencelovers.com/tag/<b>ml-interview-questions-and-answers</b>", "snippet": "PCA is a very common way to speed up your <b>Machine</b> <b>Learning</b> algorithm by getting rid of correlated variables which don\u2019t contribute in any decision making. Improve Visualization \u2013 It is very hard to visualize and understand the data in high dimensions. PCA transforms a high dimensional data to low dimensional data (2 dimension) so that it can be visualized easily. Following are the limitation of PCA. Independent variable become less interpretable \u2013 After implementing PCA on the dataset ...", "dateLastCrawled": "2021-12-23T13:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Use <b>Machine</b> <b>Learning</b> Algorithms to Explore the Potential of Your High ...", "url": "https://media.beckman.com/-/media/pdf-assets/application-notes/flow-cytometry-software-cytobank-cytoflex-20c-analysis-workflow-technical-note.pdf?country=TW", "isFamilyFriendly": true, "displayUrl": "https://media.beckman.com/-/media/pdf-assets/application-notes/flow-cytometry-software...", "snippet": "Many <b>machine</b> <b>learning</b> algorithmic tools are developed for dimensionality reduction and clustering to handle this increase in data complexity (Figure 1). Cytobank is a cloud\u2013based analysis platform with integrated analysis algorithms, as well as a structured . and secure content management system for flow cytometry and other single cell data. Cytobank\u2019s clustering, dimensionality reduction, and visualization tools (SPADE, viSNE, CITRUS, FlowSOM) leverage the scalable compute and ...", "dateLastCrawled": "2022-02-02T20:17:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(perplexity)  is like +(a child learning to speak)", "+(perplexity) is similar to +(a child learning to speak)", "+(perplexity) can be thought of as +(a child learning to speak)", "+(perplexity) can be compared to +(a child learning to speak)", "machine learning +(perplexity AND analogy)", "machine learning +(\"perplexity is like\")", "machine learning +(\"perplexity is similar\")", "machine learning +(\"just as perplexity\")", "machine learning +(\"perplexity can be thought of as\")", "machine learning +(\"perplexity can be compared to\")"]}