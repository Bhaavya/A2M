{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 0, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Empirical risk minimization</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Empirical_risk_minimization", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Empirical_risk_minimization</b>", "snippet": "<b>Empirical risk minimization</b> (<b>ERM</b>) is a principle in statistical <b>learning</b> theory which defines a family of <b>learning</b> algorithms and is used to give theoretical bounds on their performance. The core idea is that we cannot know exactly how well an <b>algorithm</b> will work in practice (the true &quot;<b>risk</b>&quot;) because we don&#39;t know the true distribution of data that the <b>algorithm</b> will work on, but we can instead measure its performance on a known set of training data (the &quot;<b>empirical</b>&quot; <b>risk</b>).", "dateLastCrawled": "2022-02-03T06:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>)", "url": "https://mdav.ece.gatech.edu/ece-6254-spring2022/notes/03-bayes-nearest-neighbors-marked.pdf", "isFamilyFriendly": true, "displayUrl": "https://mdav.ece.gatech.edu/ece-6254-spring2022/notes/03-bayes-nearest-neighbors...", "snippet": "<b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) Recall the definitions of <b>risk</b>/<b>empirical</b> <b>risk</b> Ideally, we would <b>like</b> to choose Since we do not actually know , instead we choose", "dateLastCrawled": "2022-01-21T05:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Learning</b> Theory: <b>Empirical Risk</b> <b>Minimization</b> | by Marin Vlastelica ...", "url": "https://towardsdatascience.com/learning-theory-empirical-risk-minimization-d3573f90ff77", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>learning</b>-theory-<b>empirical-risk</b>-<b>minimization</b>-d3573f90ff77", "snippet": "<b>Empirical Risk</b> <b>Minimization</b> is a fundamental concept in machine <b>learning</b>, yet surprisingly many prac t itioners are not familiar with it. Understanding <b>ERM</b> is essential to understanding the limits of machine <b>learning</b> algorithms and to form a good basis for practical problem-solving skills. The theory behind <b>ERM</b> is the theory that explains the VC-dimension, Probably Approximately Correct (PAC) <b>Learning</b> and other fundamental concepts. In my opinion, anybody that is serious about machine ...", "dateLastCrawled": "2022-02-03T00:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "11.1 <b>Empirical</b> <b>Risk</b> <b>Minimization</b> - Carnegie Mellon University", "url": "https://www.cs.cmu.edu/~aarti/Class/10704_Fall16/lec11.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.cmu.edu/~aarti/Class/10704_Fall16/lec11.pdf", "snippet": "Hence, we choose the f^ that minimizes the <b>empirical</b> <b>risk</b> over some class F, such as parametric models, histogram classi ers, decision trees or linear/polynomial functions, etc. f^<b>ERM</b> = argmin f2F R^(f) (11.3) To justify this <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) method, we need to know how similar the R(f) and R^(f) are. For bounded loss function ...", "dateLastCrawled": "2022-01-29T15:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A first model of <b>learning</b> <b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>)", "url": "http://anderson.ece.gatech.edu/ece6254/assets/03-bayes-nearest-neighbors-4up.pdf", "isFamilyFriendly": true, "displayUrl": "anderson.ece.gatech.edu/ece6254/assets/03-bayes-nearest-neighbors-4up.pdf", "snippet": "<b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) Recall the definitions of <b>risk</b>/<b>empirical</b> <b>risk</b> Ideally, we would <b>like</b> to choose Since is supposed to be a good estimate of , an incredibly natural (and common) strategy is to pick The excess <b>risk</b> Note that by definition, we must have We would <b>like</b> to guarantee that is small Last time we took an extended detour through the exciting world of concentration inequalities to show (using Hoeffding) that for any fixed or equivalently, that with probability at least ...", "dateLastCrawled": "2021-10-13T21:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Tilted <b>Empirical</b> <b>Risk</b> <b>Minimization</b> \u2013 Machine <b>Learning</b> Blog | ML@CMU ...", "url": "https://blog.ml.cmu.edu/2021/04/02/term/", "isFamilyFriendly": true, "displayUrl": "https://blog.ml.cmu.edu/2021/04/02/t<b>erm</b>", "snippet": "Our work explores tilted <b>empirical</b> <b>risk</b> <b>minimization</b> (TERM), a simple and general alternative to <b>ERM</b>, which is ubiquitous throughout machine <b>learning</b>. Our hope is that the TERM framework will allow machine <b>learning</b> practitioners to easily modify the <b>ERM</b> objective to handle practical concerns such as enforcing fairness amongst subgroups, mitigating the effect of outliers, and ensuring robust performance on new, unseen data. Critical to the success of such a framework is understanding the ...", "dateLastCrawled": "2022-01-30T13:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "In-depth analysis of the regularized least-squares <b>algorithm</b> over the ...", "url": "https://towardsdatascience.com/in-depth-analysis-of-the-regularized-least-squares-algorithm-over-the-empirical-risk-minimization-729a1433447f", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/in-depth-analysis-of-the-regularized-least-squares...", "snippet": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) <b>ERM</b> is a widely known concept in machine <b>learning</b>, and I recommend going over this explanation about <b>ERM</b> before proceeding to the actual implementation. <b>ERM</b> is used to classify the performance of <b>learning</b> algorithms, and we can solve the <b>ERM</b> optimization problem by finding a vector w that minimizes the formula below [1].", "dateLastCrawled": "2022-01-26T02:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Privacy in <b>Statistics and Machine Learning Spring 2021 Lecture</b> 11 ...", "url": "https://dpcourse.github.io/lecnotes-web/lec-11-ERM.pdf", "isFamilyFriendly": true, "displayUrl": "https://dpcourse.github.io/lecnotes-web/lec-11-<b>ERM</b>.pdf", "snippet": "<b>Lecture 11: Empirical Risk Minimization</b> Adam Smith and Jonathan Ullman 1 Optimization for Fitting Models For many natural problems in machine <b>learning</b> and statistics, the output we desire can phrased as minimizing some loss function de\u02d9ned by the data set. For example, the mean of a set x of numbers G 1\u0141\ufb02\ufb02\ufb02\u0141G =2R is the number \u2018that minimizes the sum of the squares1 of the di\u02dberences between \u2018and the G 8\u2019s: \u2018\u201ex\u201d= argmin F2R!\u201eF;x\u201d where !\u201eF;x\u201d= 1 = \u00d5= 8=1 \u201eF G ...", "dateLastCrawled": "2021-11-18T11:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Learning</b> theory: stability is suf\ufb01cient for generalization and ...", "url": "http://cbcl.mit.edu/publications/ps/mukherjee-ACM-06.pdf", "isFamilyFriendly": true, "displayUrl": "cbcl.mit.edu/publications/ps/mukherjee-ACM-06.pdf", "snippet": "Solutions of <b>learning</b> problems by <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) \u2013 and almost-<b>ERM</b> when the minimizer does not exist \u2013 need to be consistent, so that they may be predictive. They also need to be well-posed in the sense of being stable, so that they might be used robustly. We propose a statistical form of stability, de\ufb01ned as leave-one-out (LOO) stability. We prove that for bounded loss classes LOO stability is (a) suf\ufb01cient for generalization, that is conver-gence in probability of ...", "dateLastCrawled": "2022-01-28T11:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Statistical <b>Learning</b> : stability is suf\ufb01cient for generalization and ...", "url": "http://cbcl.mit.edu/publications/ps/mukherjee-AImemoOctNov.pdf", "isFamilyFriendly": true, "displayUrl": "cbcl.mit.edu/publications/ps/mukherjee-AImemoOctNov.pdf", "snippet": "Solutions of <b>learning</b> problems by <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) \u2013 and almost-<b>ERM</b> when the minimizer does not exist \u2013 need to be consis-tent, so that they may be predictive. They also need to be well-posed in the sense of being stable, so that they might be used robustly. We propose a sta-tistical form of leave-one-out stability, called CVEEEloo stability. Our main new results are two. We prove that for bounded loss classes CVEEEloo stability is (a) suf\ufb01cient for generalization ...", "dateLastCrawled": "2021-12-20T18:22:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "11.1 <b>Empirical</b> <b>Risk</b> <b>Minimization</b> - Carnegie Mellon University", "url": "https://www.cs.cmu.edu/~aarti/Class/10704_Fall16/lec11.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.cmu.edu/~aarti/Class/10704_Fall16/lec11.pdf", "snippet": "Hence, we choose the f^ that minimizes the <b>empirical</b> <b>risk</b> over some class F, such as parametric models, histogram classi ers, decision trees or linear/polynomial functions, etc. f^<b>ERM</b> = argmin f2F R^(f) (11.3) To justify this <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) method, we need to know how <b>similar</b> the R(f) and R^(f) are. For bounded loss function ...", "dateLastCrawled": "2022-01-29T15:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>)", "url": "https://mdav.ece.gatech.edu/ece-6254-spring2022/notes/03-bayes-nearest-neighbors-marked.pdf", "isFamilyFriendly": true, "displayUrl": "https://mdav.ece.gatech.edu/ece-6254-spring2022/notes/03-bayes-nearest-neighbors...", "snippet": "Using a <b>similar</b> argument as before, one can show that Thus, by letting and as , we can (asymptotically) expect to perform arbitrarily close to the Bayes <b>risk</b> This is known as universal consistency: given enough data, the <b>algorithm</b> will eventually converge to a classifier that matches the Bayes <b>risk</b>", "dateLastCrawled": "2022-01-21T05:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Tilted <b>Empirical</b> <b>Risk</b> <b>Minimization</b> \u2013 Machine <b>Learning</b> Blog | ML@CMU ...", "url": "https://blog.ml.cmu.edu/2021/04/02/term/", "isFamilyFriendly": true, "displayUrl": "https://blog.ml.cmu.edu/2021/04/02/t<b>erm</b>", "snippet": "Our work explores tilted <b>empirical</b> <b>risk</b> <b>minimization</b> (TERM), a simple and general alternative to <b>ERM</b>, which is ubiquitous throughout machine <b>learning</b>. Our hope is that the TERM framework will allow machine <b>learning</b> practitioners to easily modify the <b>ERM</b> objective to handle practical concerns such as enforcing fairness amongst subgroups, mitigating the effect of outliers, and ensuring robust performance on new, unseen data. Critical to the success of such a framework is understanding the ...", "dateLastCrawled": "2022-01-30T13:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "In-depth analysis of the regularized least-squares <b>algorithm</b> over the ...", "url": "https://towardsdatascience.com/in-depth-analysis-of-the-regularized-least-squares-algorithm-over-the-empirical-risk-minimization-729a1433447f", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/in-depth-analysis-of-the-regularized-least-squares...", "snippet": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) <b>ERM</b> is a widely known concept in machine <b>learning</b>, and I recommend going over this explanation about <b>ERM</b> before proceeding to the actual implementation. <b>ERM</b> is used to classify the performance of <b>learning</b> algorithms, and we can solve the <b>ERM</b> optimization problem by finding a vector w that minimizes the formula below [1].", "dateLastCrawled": "2022-01-26T02:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Private <b>Empirical Risk</b> <b>Minimization</b>: Efficient Algorithms and Tight ...", "url": "https://par.nsf.gov/servlets/purl/10092778", "isFamilyFriendly": true, "displayUrl": "https://par.nsf.gov/servlets/purl/10092778", "snippet": "in machine <b>learning</b> and statistics. We provide new algorithms and matching lower bounds for differentially private convex <b>empirical risk</b> <b>minimization</b> assuming only that each data point\u2019s contribution to the loss function is Lipschitz and that the domain of optimization is bounded. We provide a separate set of algorithms and matching lower bounds for the setting in which the loss functions are known to also be strongly convex. Our algorithms run in polynomial time, and in some cases even ...", "dateLastCrawled": "2022-01-28T12:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Federated Optimization of Smooth Loss Functions | DeepAI", "url": "https://deepai.org/publication/federated-optimization-of-smooth-loss-functions", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/federated-optimization-of-smooth-loss-functions", "snippet": "In this work, we study <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) within a federated <b>learning</b> framework, where a central server minimizes an <b>ERM</b> objective function using training data that is stored across m clients. In this setting, the Federated Averaging (FedAve) <b>algorithm</b> is the staple for determining \u03f5-approximate solutions to the <b>ERM</b> problem. <b>Similar</b> to standard optimization algorithms, the convergence analysis of FedAve only relies on smoothness of the", "dateLastCrawled": "2022-02-02T00:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Stability Properties of Empirical Risk Minimization over Donsker</b> Classes", "url": "https://www.mit.edu/~rakhlin/papers/erm.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.mit.edu/~rakhlin/papers/<b>erm</b>.pdf", "snippet": "The <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) <b>algorithm</b> has been studied in <b>learning</b> theory to a great extent. Vapnik and Chervonenkis (1971, 1991) showed necessary and suf\ufb01cient conditions for its consistency. In recent developments, Bartlett and Mendelson (2006); Bartlett et al. (2004); Koltchin-skii (2006) proved sharp bounds on the performance of <b>ERM</b>. Tools from <b>empirical</b> process theory have been successfully applied, and, in particular, it has been shown that the localized Rademacher averages ...", "dateLastCrawled": "2022-01-10T01:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Tilted <b>Empirical Risk</b> <b>Minimization</b> | DeepAI", "url": "https://deepai.org/publication/tilted-empirical-risk-minimization", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/tilted-<b>empirical-risk</b>-<b>minimization</b>", "snippet": "<b>Empirical risk</b> <b>minimization</b> (<b>ERM</b>) is typically designed to perform well on the average loss, which can result in estimators that are sensitive to outliers, generalize poorly, or treat subgroups unfairly.While many methods aim to address these problems individually, in this work, we explore them through a unified framework\u2014tilted <b>empirical risk</b> <b>minimization</b> (TERM).", "dateLastCrawled": "2022-01-11T23:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Stability Properties of <b>Empirical</b> <b>Risk</b> <b>Minimization</b> over Donsker Classes", "url": "https://sites.stat.washington.edu/jaw/COURSES/EPWG/erm.pdf", "isFamilyFriendly": true, "displayUrl": "https://sites.stat.washington.edu/jaw/COURSES/EPWG/<b>erm</b>.pdf", "snippet": "<b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) <b>algorithm</b> has been studied in <b>learning</b> theory to a great extent. Vapnik and Chervonenkis (1971, 1991) showed necessary and su\ufb03cient conditions for its consistency. In recent developments, Bartlett and Mendelson (2004); Bartlett et al. (2004); Koltchinskii (2003) proved sharp bounds on the performance of <b>ERM</b>. Tools from <b>empirical</b> processtheory have been successfully applied, and, in particular, it hasbeen shown that the localized Rademacher averages play an ...", "dateLastCrawled": "2021-07-19T19:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "[1411.5417] <b>Private Empirical Risk Minimization</b> Beyond the Worst Case ...", "url": "https://arxiv.org/abs/1411.5417", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/abs/1411.5417", "snippet": "Download PDF Abstract: <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) is a standard technique in machine <b>learning</b>, where a model is selected by minimizing a loss function over constraint set. When the training dataset consists of private information, it is natural to use a differentially private <b>ERM</b> <b>algorithm</b>, and this problem has been the subject of a long line of work started with Chaudhuri and Monteleoni 2008.", "dateLastCrawled": "2021-09-20T11:36:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Part VI <b>Learning</b> Theory", "url": "https://elektrotehnika.github.io/ml/notes/ML-notes4.pdf", "isFamilyFriendly": true, "displayUrl": "https://elektrotehnika.github.io/ml/notes/ML-notes4.pdf", "snippet": "Wecallthisprocessempirical <b>risk</b> <b>minimization</b>(<b>ERM</b>),andtheresulting hypothesis output by the <b>learning</b> <b>algorithm</b> is \u02c6h = h ... <b>Empirical</b> <b>risk</b> <b>minimization</b> <b>can</b> now <b>be thought</b> of as a <b>minimization</b> over the class of functions H, in which the <b>learning</b> <b>algorithm</b> picks the hypothesis: \u02c6h = argmin h\u2208H \u03b5\u02c6(h) 2PAC stands for \u201cprobably approximately correct,\u201d which is a framework and set of assumptions under which numerous results on <b>learning</b> theory were proved. Of these, the assumption of ...", "dateLastCrawled": "2022-01-18T04:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Empirical Risk Minimization for Probabilistic Grammars: Sample</b> ...", "url": "https://direct.mit.edu/coli/article/38/3/479/2169/Empirical-Risk-Minimization-for-Probabilistic", "isFamilyFriendly": true, "displayUrl": "https://direct.mit.edu/coli/article/38/3/479/2169/<b>Empirical</b>-<b>Risk</b>-<b>Minimization</b>-for...", "snippet": "The more data we have, the more complex our <b>can</b> be for <b>empirical</b> <b>risk</b> <b>minimization</b>. Structural <b>risk</b> <b>minimization</b> (Vapnik 1998) and the method of sieves (Grenander 1981) are examples of methods that adopt such an approach. Structural <b>risk</b> <b>minimization</b>, for example, <b>can</b> be represented in many cases as a penalization of the <b>empirical</b> <b>risk</b> method ...", "dateLastCrawled": "2021-12-08T04:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "svm - difference between <b>empirical</b> <b>risk</b> <b>minimization</b> and structural ...", "url": "https://datascience.stackexchange.com/questions/66729/difference-between-empirical-risk-minimization-and-structural-risk-minimization", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/66729", "snippet": "<b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) is a principle in statistical <b>learning</b> theory that defines a family of <b>learning</b> algorithms and is used to give theoretical bounds on their performance. The core idea is that we cannot know exactly how well an <b>algorithm</b> will work in practice (the true &quot;<b>risk</b>&quot;) because we don&#39;t know the true distribution of data that the <b>algorithm</b> will work on, but we <b>can</b> instead measure its performance on a known set of training data (the &quot;<b>empirical</b>&quot; <b>risk</b>).", "dateLastCrawled": "2022-01-24T04:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>2.2 Relationship to Prior Work</b>", "url": "https://www.cs.cmu.edu/~jcl/papers/thesis/mathml/thesisse7.xml", "isFamilyFriendly": true, "displayUrl": "https://www.cs.cmu.edu/~jcl/papers/thesis/mathml/thesisse7.xml", "snippet": "Results in this model apply only for the <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) <b>algorithm</b>. The <b>ERM</b> ... The work in this thesis <b>can</b> <b>be thought</b> of as directly addressing the altered question which alleviates problem 1. Since our goal is addressing this altered question rather than deriving the answer from other results, we will be able to state and prove tighter results. Furthermore, because we address the question people encounter in practice, the bounds presented here will be more directly ...", "dateLastCrawled": "2020-12-11T13:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) <b>Empirical</b> <b>Risk</b> <b>Minimization</b> for Time Series: Nonparametric ...", "url": "https://www.researchgate.net/publication/353838623_Empirical_Risk_Minimization_for_Time_Series_Nonparametric_Performance_Bounds_for_Prediction", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/353838623_<b>Empirical</b>_<b>Risk</b>_<b>Minimization</b>_for...", "snippet": "<b>Empirical</b> <b>risk</b> <b>minimization</b> is a standard principle for choosing algorithms in <b>learning</b> theory. In this paper we study the properties of <b>empirical</b> <b>risk</b> <b>minimization</b> for time series.", "dateLastCrawled": "2021-12-23T08:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Establishing connection between <b>ERM</b> (<b>Empirical</b> <b>Risk</b> <b>Minimization</b>) and MLE", "url": "https://stats.stackexchange.com/questions/464622/establishing-connection-between-erm-empirical-risk-minimization-and-mle", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/464622/establishing-connection-between-<b>erm</b>...", "snippet": "$\\begingroup$ He shows MLE, that is a way to infer an unknown parameter with an estimator which is a function of the observed data, that is $\\hat{\\theta} \\in \\Theta^{\\mathcal{X}}$, function from $\\mathcal{X} \\rightarrow \\Theta$. In section 2.3.4 he states &quot;Most model classes will have some parameters $\\theta \\in \\Theta$ that the <b>learning</b> <b>algorithm</b> will adjust to fit the data.", "dateLastCrawled": "2022-01-09T10:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Part VI <b>Learning</b> Theory - CS229: Machine <b>Learning</b>", "url": "http://cs229.stanford.edu/notes_archive/cs229-notes4.pdf", "isFamilyFriendly": true, "displayUrl": "cs229.stanford.edu/notes_archive/cs229-notes4.pdf", "snippet": "Wecall this process <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>), andthe resulting hypothesis output by the <b>learning</b> <b>algorithm</b> is \u02c6h = h \u02c6\u03b8. We think of <b>ERM</b> as the most \u201cbasic\u201d <b>learning</b> <b>algorithm</b>, and it will be this <b>algorithm</b> that we focus on in these notes. (Algorithms such as logistic regression <b>can</b> also be viewed as approximations to <b>empirical</b> <b>risk</b> <b>minimization</b>.) In our study of <b>learning</b> theory, it will be useful to abstract away from the speci\ufb01c parameterization of hypotheses and from ...", "dateLastCrawled": "2022-01-26T13:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) <b>Empirical risk minimization for probabilistic grammars: Sample</b> ...", "url": "https://www.academia.edu/2825039/Empirical_risk_minimization_for_probabilistic_grammars_Sample_complexity_and_hardness_of_learning", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/2825039/<b>Empirical_risk_minimization_for_probabilistic</b>...", "snippet": "<b>Empirical risk minimization for probabilistic grammars: Sample complexity</b> and hardness of <b>learning</b>. Download. Related Papers. <b>Empirical</b> <b>risk</b> <b>minimization</b> with approximations of probabilistic grammars. By Noah A. Smith. Regularization in statistics. By Carlos Rivero. Advanced Lectures on Machine <b>Learning</b>: ML Summer Schools 2003, Canberra, Australia, February 2-14, 2003, T bingen, Germany, August 4-16, 2003, Revised Lectures. By Gunnar Ratsch. Covariance in unsupervised <b>learning</b> of ...", "dateLastCrawled": "2022-01-17T04:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Adaptive Newton Method for <b>Empirical</b> <b>Risk</b> <b>Minimization</b> to Statistical ...", "url": "https://slideblast.com/adaptive-newton-method-for-empirical-risk-minimization-to-statistical-_59b73a4a1723dd731a2c7a55.html", "isFamilyFriendly": true, "displayUrl": "https://slideblast.com/adaptive-newton-method-for-<b>empirical</b>-<b>risk</b>-<b>minimization</b>-to...", "snippet": "A hallmark of <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) on large datasets is that evaluating descent directions requires a complete pass over the dataset. Since this is undesirable due to the large number of training samples, stochastic optimization algorithms with descent directions estimated from a subset of samples are the method of choice. First order stochastic optimization has a long history [20, 19] but the last decade has seen fundamental progress in developing alternatives with faster ...", "dateLastCrawled": "2021-10-28T06:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Chapter 10 Supervised Learning</b> | Introduction to Data Science", "url": "https://bookdown.org/ronsarafian/IntrotoDS/supervised.html", "isFamilyFriendly": true, "displayUrl": "https://bookdown.org/ronsarafian/IntrotoDS/supervised.html", "snippet": "We now present the <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) approach to supervised <b>learning</b>, a.k.a. M-estimation in the statistical literature. Remark. We do not discuss purely algorithmic approaches such as K-nearest neighbour and kernel smoothing due to space constraints. For a broader review of supervised <b>learning</b>, see the Bibliographic Notes. Example 10.1 (Spam classification) Consider the problem of predicting if a mail is spam or not based on its attributes: length, number of exclamation ...", "dateLastCrawled": "2022-02-03T00:57:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "In-depth analysis of the regularized least-squares <b>algorithm</b> over the ...", "url": "https://towardsdatascience.com/in-depth-analysis-of-the-regularized-least-squares-algorithm-over-the-empirical-risk-minimization-729a1433447f", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/in-depth-analysis-of-the-regularized-least-squares...", "snippet": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) <b>ERM</b> is a widely known concept in machine <b>learning</b>, and I recommend going over this explanation about <b>ERM</b> before proceeding to the actual implementation. <b>ERM</b> is used to classify the performance of <b>learning</b> algorithms, and we <b>can</b> solve the <b>ERM</b> optimization problem by finding a vector w that minimizes the formula below [1].", "dateLastCrawled": "2022-01-26T02:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Empirical Risk Minimization</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/mathematics/empirical-risk-minimization", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/mathematics/<b>empirical-risk-minimization</b>", "snippet": "Now we <b>can</b> define the <b>ERM</b> <b>learning</b> <b>algorithm</b> for the regression problem associated with a general regressing loss function ... It turns out the conditions required to render <b>empirical risk minimization</b> consistent involve restricting the set of admissible functions. The main insight of VC (Vapnik-Chervonenkis) theory is that the consistency of <b>empirical risk minimization</b> is determined by the worst case behavior over all functions f \u2208 F that the <b>learning</b> machine could choose. We will see ...", "dateLastCrawled": "2022-01-16T03:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "machine <b>learning</b> - <b>Empirical</b> <b>Risk</b> <b>Minimization</b>: <b>empirical</b> vs expected ...", "url": "https://stats.stackexchange.com/questions/265551/empirical-risk-minimization-empirical-vs-expected-and-true-vs-surrogate", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/265551/<b>empirical</b>-<b>risk</b>-<b>minimization</b>-<b>empirical</b>...", "snippet": "In Tie-Yan Liu&#39;s book, he says that in a statistical <b>learning</b> theory for <b>empirical</b> <b>risk</b> <b>minimization</b> has to observe four <b>risk</b> functions: We also need to de\ufb01ne the true loss of the <b>learning</b> problem, which serves as a reference to study the properties of different surrogate loss functions used by various <b>learning</b> algorithms.", "dateLastCrawled": "2022-02-03T02:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "An <b>Empirical</b> Study of <b>Invariant Risk Minimization</b> | DeepAI", "url": "https://deepai.org/publication/an-empirical-study-of-invariant-risk-minimization", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/an-<b>empirical</b>-study-of-<b>invariant-risk-minimization</b>", "snippet": "<b>Invariant risk minimization</b> (IRM) is a recently proposed machine <b>learning</b> framework where the goal is to learn invariances across multiple training environments . <b>Compared</b> to the widely used framework of <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>), IRM does not assume that training samples are identically distributed. Rather, IRM assumes that training ...", "dateLastCrawled": "2022-01-24T09:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Tilted <b>Empirical Risk</b> <b>Minimization</b> | DeepAI", "url": "https://deepai.org/publication/tilted-empirical-risk-minimization", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/tilted-<b>empirical-risk</b>-<b>minimization</b>", "snippet": "<b>Empirical risk</b> <b>minimization</b> (<b>ERM</b>) is typically designed to perform well on the average loss, which <b>can</b> result in estimators that are sensitive to outliers, generalize poorly, or treat subgroups unfairly.While many methods aim to address these problems individually, in this work, we explore them through a unified framework\u2014tilted <b>empirical risk</b> <b>minimization</b> (TERM).", "dateLastCrawled": "2022-01-11T23:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "The importance of k-<b>fold cross-validation</b> for model prediction in ...", "url": "https://towardsdatascience.com/the-importance-of-k-fold-cross-validation-for-model-prediction-in-machine-learning-4709d3fed2ef", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/the-importance-of-k-<b>fold-cross-validation</b>-for-model...", "snippet": "This article will discuss and analyze the importance of k-<b>fold cross-validation</b> for model prediction in machine <b>learning</b> using the least-squares <b>algorithm</b> for <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>). We\u2019ll use a polynomial curve-fitting problem to predict the best polynomial for the sample dataset. Also, we\u2019ll go over the implementation step-by ...", "dateLastCrawled": "2022-02-02T10:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Lecture 1 - University of Texas at Austin", "url": "https://users.ece.utexas.edu/~dimakis/DataScience/Lecture1.pdf", "isFamilyFriendly": true, "displayUrl": "https://users.ece.utexas.edu/~dimakis/DataScience/Lecture1.pdf", "snippet": "This will lead to the main <b>algorithm</b> used in <b>learning</b>: <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>). We need a mathematical model of how data is generated and labeled. We assume we are given a distribution D over the feature space. Each sample x i (weight and height of a nanochip) is assumed to be randomly and independently sampled from this distribution. We use bold for x because it is a vector of p numbers (the features), i.e. x 2Rp. We assume a true labeling function h T. The universe samples a ...", "dateLastCrawled": "2021-11-21T16:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "3. <b>Robust algorithms for Regression, Classification and</b> Clustering ...", "url": "https://scikit-learn-extra.readthedocs.io/en/stable/modules/robust.html", "isFamilyFriendly": true, "displayUrl": "https://scikit-learn-extra.readthedocs.io/en/stable/modules/robust.html", "snippet": "3.2. Robust estimation with robust weighting\u00b6. A lot of <b>learning</b> algorithms are based on a paradigm known as <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) which consists in finding the estimator \\(\\widehat{f}\\) that minimizes an estimation of the <b>risk</b>.", "dateLastCrawled": "2022-01-26T20:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Extracurricular <b>Learning</b>: Knowledge Transfer Beyond <b>Empirical</b> Distribution", "url": "https://openaccess.thecvf.com/content/CVPR2021W/ECV/papers/Pouransari_Extracurricular_Learning_Knowledge_Transfer_Beyond_Empirical_Distribution_CVPRW_2021_paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://openaccess.thecvf.com/content/CVPR2021W/ECV/papers/Pouransari_Extracurricular...", "snippet": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>): min \u03b8 1 n X i l(f\u03b8(xi),yi) (2) In KD [28], a student model f\u03b8 is encouraged to match the output of a teacher \u03c4 on the training set: min \u03b8 1 n X i l(f\u03b8(xi),\u03c4(xi)) (3) \u03c4 in (3) <b>can</b> be a single more powerful model or an ensemble of several models. In the original KD [28] an average of losses in (2) and (3 ...", "dateLastCrawled": "2022-01-17T16:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Insuring against the perils in distributed <b>learning</b>: privacy-preserving ...", "url": "https://www.aimspress.com/aimspress-data/mbe/2021/4/PDF/mbe-18-04-151.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.aimspress.com/aimspress-data/mbe/2021/4/PDF/mbe-18-04-151.pdf", "snippet": "models is <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>). It\u2019s Di erentially Private version is Di erential Private <b>Empirical</b> <b>Risk</b> <b>Minimization</b> [3,4] (DP-<b>ERM</b>) which <b>can</b> be de\ufb01ned as follows: De\ufb01nition 1. (DP-<b>ERM</b>). Given a dataset D = fz 1;z 2;z ngfrom a data universe Xand a closed convex set C Rp, DP-<b>ERM</b> is to \ufb01nd xpriv 2Cso as to minimize the <b>empirical</b> <b>risk</b>, i.e., x 2argmin x2C Fr(x;D) = F(x;D) + r(x) = 1 n Xn i=1 f (x;z i) + r(x) with the guarantee of being di erentially private, where f is the ...", "dateLastCrawled": "2021-12-29T12:09:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> and Stochastic Gradient Descent for ...", "url": "http://proceedings.mlr.press/v89/veitch19a/veitch19a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v89/veitch19a/veitch19a.pdf", "snippet": "where F(Sn) is the <b>empirical</b> distribution.2 The <b>ERM</b> dogma is to select the predictor \u03c0\u02c6\u03b8 n given by \u02c6\u03b8 n = argmin\u03b8 R\u02c6(\u03b8,Sn). That is, the objective function that de\ufb01nes <b>learning</b> is the <b>empirical</b> <b>risk</b>. <b>ERM</b> has two useful properties. (1) It provides a prin-cipled framework for de\ufb01ning new <b>machine</b> <b>learning</b> methods. In particular, when ...", "dateLastCrawled": "2021-09-18T06:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Computational and Statistical <b>Learning</b> Theory", "url": "https://home.ttic.edu/~nati/Teaching/TTIC31120/2015/Lecture17.pdf", "isFamilyFriendly": true, "displayUrl": "https://home.ttic.edu/~nati/Teaching/TTIC31120/2015/Lecture17.pdf", "snippet": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) / Sample Average Approximation (SAA): Collect sample z1UYU zm ... SGD for <b>Machine</b> <b>Learning</b> Initialize S 4 L r At iteration t: Draw T \u00e7\u00e1U \u00e71\u00de If U \u00e7 S \u00e7 \u00e1\u00f6 T \u00e7 O s\u00e1 S \u00e7 &gt; 5 Z S \u00e7 E\u00df \u00e7U \u00e7\u00f6 T \u00e7 else: S \u00e7 &gt; 5 Z S \u00e7 Return S % \u00cd L 5 \u00cd \u00c3 \u00cd S \u00e7 \u00e7 @ 5 Draw T 5\u00e1U 5 \u00e1\u00e5\u00e1 T \u00e0 \u00e1U \u00e0 1\u00de Initialize S 4 L r At iteration t: Pick E \u00d0 s\u00e5I at random If U \u00dc S \u00e7 \u00e1\u00f6 T \u00dc O s\u00e1 S \u00e7 &gt; 5 Z S \u00e7 E\u00df \u00e7U \u00dc\u00f6 T \u00dc else: S \u00e7 &gt; 5 Z S \u00e7 S \u00e7 &gt; 5 Z ...", "dateLastCrawled": "2022-01-26T03:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Basics of <b>Machine</b> <b>Learning</b>", "url": "https://courses.cs.duke.edu/spring21/compsci527/slides/s_04_deep_learning.pdf", "isFamilyFriendly": true, "displayUrl": "https://courses.cs.duke.edu/spring21/compsci527/slides/s_04_deep_<b>learning</b>.pdf", "snippet": "This is called <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) COMPSCI 527 \u2014 Computer Vision Basics of <b>Machine</b> <b>Learning</b> 15/26. Loss and <b>Risk</b> <b>Machine</b> <b>Learning</b> and the Statistical <b>Risk</b> <b>ERM</b>: w^ 2argmin w2R m L T(w) In <b>machine</b> <b>learning</b>, we go much farther: We also want h to do well on previously unseen inputs To relate past and future data, assume that all data comes from the same joint probability distribution p(x;y) p is called the generative data model or just model The goal of <b>machine</b> <b>learning</b> is to ...", "dateLastCrawled": "2021-11-06T09:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Statistical <b>Learning</b> Theory and the C-Loss cost function", "url": "http://www.cnel.ufl.edu/courses/EEL6814/closs.pdf", "isFamilyFriendly": true, "displayUrl": "www.cnel.ufl.edu/courses/EEL6814/closs.pdf", "snippet": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) principle Let us consider a <b>learning</b> <b>machine</b> x,d are real r.v. with joint distribution P(x,y). F(x) is a function of some parameters w, i.e. f(x,w). d d. <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) principle How can we find the possible best <b>learning</b> <b>machine</b> that generalizes for unseen data from the same distribution? Define the <b>Risk</b> functional as L(.) is called the Loss function, and minimize it w.r.t. w achieving the best possible loss. But we can not do this ...", "dateLastCrawled": "2022-01-28T20:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Stratified <b>Sampling Meets Machine Learning</b>", "url": "http://proceedings.mlr.press/v48/liberty16.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v48/liberty16.pdf", "snippet": "3. <b>Empirical</b> <b>Risk</b> <b>Minimization</b> <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) is a standard ap-proach in <b>machine</b> <b>learning</b> in which the chosen model is the minimizer of the <b>empirical</b> <b>risk</b>. The <b>empirical</b> <b>risk</b> R emp(p) is de\ufb01ned as an average loss of the model over the training set Q. Here Qis a query log containing a ran-", "dateLastCrawled": "2021-10-13T10:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Topics in <b>Machine</b> <b>Learning</b> (TIML-09)", "url": "https://www.cse.iitb.ac.in/~saketh/teaching/cs689.html", "isFamilyFriendly": true, "displayUrl": "https://www.cse.iitb.ac.in/~saketh/teaching/cs689.html", "snippet": "Introduction to Statistical <b>Learning</b> Theory (SLT): Definitions of loss function, <b>risk</b>, <b>empirical</b> <b>risk</b>, motivation for <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) Further Reading, Supplementary: Jan 12: Consistency of <b>ERM</b>, Sufficient condition for <b>ERM</b> as one-sided uniform convergence, Analysis for finite sets of functions and extensions to general case using Symmetrization trick, Shattering Coeff. Further Reading, Supplementary: Jan 15: Shattering coeff., growth function, VC dimension, Annealed Entropy ...", "dateLastCrawled": "2022-01-11T09:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Lecture 1: Reinforcement <b>Learning</b>: What and Why?", "url": "https://imada.sdu.dk/~kandemir/rl-lecture1.pdf", "isFamilyFriendly": true, "displayUrl": "https://imada.sdu.dk/~kandemir/rl-lecture1.pdf", "snippet": "<b>machine</b> <b>learning</b> and is referred to as <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>). 3 Challenges of reinforcement <b>learning</b> Consider the cart pole balancing problem, where a cart carrying an unactuated pole \ufb02oats on a straight horizontal track. The cart is actuated by a torque applied either to the right or the left direction. Seeherefor a real cart ...", "dateLastCrawled": "2021-09-30T12:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>GitHub</b> - gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "Loss function 3.1 Categories 3.2 <b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) and Structural <b>risk</b> <b>minimization</b> (SRM) 3.2.1 MLE 3.2.2 MAP 4. Evaluation metrics 4.1 Accuracy, precision, recall and F1 4.2 ROC, AUC 4.3 BLEU 5. Sample projects 6. Classical questions 6.1 Why is logistic regression a generalized linear model ?", "dateLastCrawled": "2021-09-12T01:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and-sample-codes ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/README.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "Loss function 3.1 Categories 3.2 <b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) and Structural <b>risk</b> <b>minimization</b> (SRM) 3.2.1 MLE 3.2.2 MAP 4. Evaluation metrics 4.1 Accuracy, precision, recall and F1 4.2 ROC, AUC 4.3 BLEU 5. Sample projects 6. Classical questions 6.1 Why is logistic regression a generalized linear model ?", "dateLastCrawled": "2021-09-22T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "[2006.09461] Robust <b>Compressed Sensing using Generative Models</b> - arXiv", "url": "https://arxiv.org/abs/2006.09461", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/abs/2006.09461", "snippet": "Classical recovery approaches such as <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) are guaranteed to succeed when the measurement matrix is sub-Gaussian. However, when the measurement matrix and measurements are heavy-tailed or have outliers, recovery may fail dramatically. In this paper we propose an algorithm inspired by the Median-of-Means (MOM). Our algorithm guarantees recovery for heavy-tailed data, even in the presence of outliers. Theoretically, our results show our novel MOM-based algorithm ...", "dateLastCrawled": "2021-06-27T11:19:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "ToyotaTechnologicalInstituteatChicago UniversityofTexasatAustin surbhi ...", "url": "https://arxiv.org/pdf/2005.07652", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/pdf/2005.07652", "snippet": "<b>Just as empirical risk minimization (ERM</b>) is central for non-robust PAC <b>learning</b>, a core component of adversarially robust <b>learning</b> is minimizing the robust empirical risk on a dataset S, \u02c6h \u2208 RERM U(S) ,argmin h\u2208H 1 m Xm i=1 sup z\u2208U(x) 1 [h(z) 6= y]. In this paper, we provide necessary and su\ufb03cient conditions on perturbation sets U, under which the robust empirical risk minimization (RERM) problem is e\ufb03ciently solvable in the realizable setting. We show that an e\ufb03cient ...", "dateLastCrawled": "2021-10-06T13:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Efficiently Learning Adversarially Robust Halfspaces with</b> Noise | DeepAI", "url": "https://deepai.org/publication/efficiently-learning-adversarially-robust-halfspaces-with-noise", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>efficiently-learning-adversarially-robust-halfspaces</b>...", "snippet": "<b>Just as empirical risk minimization (ERM</b>) is central for non-robust PAC <b>learning</b>, a core component of adversarially robust <b>learning</b> is minimizing the robust empirical risk on a dataset S, ^ h \u2208 R E R M U ( S ) \u225c argmin h \u2208 H 1 m m \u2211 i = 1 sup z \u2208 U ( x ) 1 [ h ( z ) \u2260 y ] .", "dateLastCrawled": "2021-12-05T23:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Efficiently <b>Learning</b> Adversarially Robust Halfspaces with Noise", "url": "http://proceedings.mlr.press/v119/montasser20a/montasser20a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v119/montasser20a/montasser20a.pdf", "snippet": "remains a major challenge in <b>machine</b> <b>learning</b>. A line of work has shown that predictors learned by deep neural networks are not robust to adversarial examples (Szegedy et al.,2014;Biggio et al.,2013;Goodfellow et al.,2015). This has led to a long line of research studying different aspects of robustness to adversarial examples. In this paper, we consider the problem of distribution-independent <b>learning</b> of halfspaces that are robust to ad-versarial examples at test time, also referred to as ...", "dateLastCrawled": "2021-11-21T12:03:00.0000000Z", "language": "en", "isNavigational": false}], [], []], "all_bing_queries": ["+(empirical risk minimization (erm))  is like +(learning algorithm)", "+(empirical risk minimization (erm)) is similar to +(learning algorithm)", "+(empirical risk minimization (erm)) can be thought of as +(learning algorithm)", "+(empirical risk minimization (erm)) can be compared to +(learning algorithm)", "machine learning +(empirical risk minimization (erm) AND analogy)", "machine learning +(\"empirical risk minimization (erm) is like\")", "machine learning +(\"empirical risk minimization (erm) is similar\")", "machine learning +(\"just as empirical risk minimization (erm)\")", "machine learning +(\"empirical risk minimization (erm) can be thought of as\")", "machine learning +(\"empirical risk minimization (erm) can be compared to\")"]}