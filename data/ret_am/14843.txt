{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine</b> <b>Learning</b> algorithms based on &quot;<b>structural</b> <b>risk</b> <b>minimization</b>&quot;?", "url": "https://cs.stackexchange.com/questions/2006/machine-learning-algorithms-based-on-structural-risk-minimization", "isFamilyFriendly": true, "displayUrl": "https://cs.stackexchange.com/questions/2006/<b>machine</b>-<b>learning</b>-<b>algorithms</b>-based-on...", "snippet": "The <b>structural</b> <b>risk</b> <b>minimization</b> principle is a principle that is at least partly &#39;used&#39; in all <b>machine</b> <b>learning</b> methods, since overfitting is often to be taken into account: reducing the complexity of the model is (supposedly and in practice) a good way to limit overfitting.", "dateLastCrawled": "2022-01-19T13:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Structural</b> <b>risk</b> <b>minimization</b> and minimum description length", "url": "https://andrewcharlesjones.github.io/journal/mdl.html", "isFamilyFriendly": true, "displayUrl": "https://andrewcharlesjones.github.io/journal/mdl.html", "snippet": "We have seen that <b>structural</b> <b>risk</b> <b>minimization</b> is a general framework for searching among a large set of hypotheses by incorporating knowledge about which types of hypotheses we prefer. Minimum description length is one possible strategy for formally assigning preferences, which as we saw is suprisingly agnostic to the langauge in which we represent the hypotheses. These ideas are deeply connected to several other well-known concepts in statistics and <b>machine</b> <b>learning</b>, some of which are ...", "dateLastCrawled": "2022-01-02T01:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Lecture 8: Model selection and <b>structural</b> <b>risk minimization</b>", "url": "https://people.cs.umass.edu/~akshay/courses/cs690m/files/lec8.pdf", "isFamilyFriendly": true, "displayUrl": "https://people.cs.umass.edu/~akshay/courses/cs690m/files/lec8.pdf", "snippet": "Lecture 8: Model selection and <b>structural</b> <b>risk minimization</b> Akshay Krishnamurthy akshay@cs.umass.edu September 28, 2017 1 Recap Last time we saw our rst real <b>learning</b> <b>algorithm</b> (that wasn\u2019t obviously ERM), namely kernel regression. Recall the estimator took the form ^(x) = P n i=1 Y iK(kX i xk=h) P n i=1 K(kX i xk=h) where the data (X i;Y", "dateLastCrawled": "2021-12-31T02:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>machine</b> <b>learning</b> - <b>Structural</b> <b>risk</b> <b>minimization</b> in SVMs - Cross Validated", "url": "https://stats.stackexchange.com/questions/278306/structural-risk-minimization-in-svms", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/278306/<b>structural</b>-<b>risk</b>-<b>minimization</b>-in-svms", "snippet": "In <b>SRM</b> (<b>structural</b> <b>risk</b> <b>minimization</b>) as larger is the VC dimension as larger is the <b>risk</b>. I know that SVM <b>algorithm</b> selects the hyperplane with minimum VC dimension (namely with minimum margin) however if the VC-dimension is infinite (<b>like</b> in gaussian kernels) this minimum is infinite and the <b>risk</b> will be likely high. How is it possible?", "dateLastCrawled": "2022-01-23T15:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "11.1 Empirical <b>Risk</b> <b>Minimization</b> - Carnegie Mellon University", "url": "https://www.cs.cmu.edu/~aarti/Class/10704_Fall16/lec11.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.cmu.edu/~aarti/Class/10704_Fall16/lec11.pdf", "snippet": "11.2 Complexity Regularized Empirical <b>Risk</b> <b>Minimization</b> aka <b>Structural</b> <b>Risk</b> <b>Minimization</b> To achieve better estimation of the true <b>risk</b>, we should minimize both the empirical <b>risk</b> and complexity, instead of only minimizing the empirical <b>risk</b>. f^<b>SRM</b> = argmin f2F fR^(f) + (f)g (11.14) where (f) = q c(f)+log 2 2n. With probability 1 , we have the ...", "dateLastCrawled": "2022-01-29T15:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Model Selection in <b>Machine</b> <b>Learning</b> | by ANUSHKA BAJPAI | Medium", "url": "https://medium.com/@anushkhabajpai/model-selection-in-machine-learning-c568e5a42dcc", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@anushkhabajpai/model-selection-in-<b>machine</b>-<b>learning</b>-c568e5a42dcc", "snippet": "<b>Structural</b> <b>Risk</b> <b>Minimization</b> (<b>SRM</b>) <b>Machine</b> <b>learning</b> models face the inevitable problem of defining a generalized theory from a set of finite data. This leads to cases of overfitting where the ...", "dateLastCrawled": "2022-01-29T03:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Structural</b> <b>Risk</b> <b>Minimization</b> for Character Recognition", "url": "https://papers.nips.cc/paper/1991/file/10a7cdd970fe135cf4f7bb55c0e3b59f-Paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://papers.nips.cc/paper/1991/file/10a7cdd970fe135cf4f7bb55c0e3b59f-Paper.pdf", "snippet": "The method of <b>Structural</b> <b>Risk</b> <b>Minimization</b> refers to tuning the capacity of the classifier to the available amount of training data. This capac\u00ad ity is influenced by several factors, including: (1) properties of the input space, (2) nature and structure of the classifier, and (3) <b>learning</b> <b>algorithm</b>.", "dateLastCrawled": "2021-11-21T16:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Understanding Machine Learning</b>: From Theory to Algorithms", "url": "https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.huji.ac.il/.../<b>understanding-machine-learning</b>-theory-<b>algorithms</b>.pdf", "snippet": "the Empirical <b>Risk</b> <b>Minimization</b> (ERM), <b>Structural</b> <b>Risk</b> <b>Minimization</b> (<b>SRM</b>), and Minimum Description Length (MDL) <b>learning</b> rules, which shows \\how can a <b>machine</b> learn&quot;. We quantify the amount of data needed for <b>learning</b> using the ERM, <b>SRM</b>, and MDL rules and show how <b>learning</b> might fail by deriving", "dateLastCrawled": "2022-01-29T21:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A <b>Gentle Introduction to Model Selection for Machine Learning</b>", "url": "https://machinelearningmastery.com/a-gentle-introduction-to-model-selection-for-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/a-<b>gentle-introduction-to-model-selection-for</b>...", "snippet": "<b>Structural</b> <b>Risk</b> <b>Minimization</b> (<b>SRM</b>). Probabilistic measures are appropriate when using simpler linear models <b>like</b> linear regression or logistic regression where the calculating of model complexity penalty (e.g. in sample bias) is known and tractable. Resampling Methods", "dateLastCrawled": "2022-02-02T06:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>machine</b> <b>learning</b> - Is there an equivalent to VC-dimension for density ...", "url": "https://cstheory.stackexchange.com/questions/47744/is-there-an-equivalent-to-vc-dimension-for-density-estimation-as-opposed-to-clas", "isFamilyFriendly": true, "displayUrl": "https://cstheory.stackexchange.com/questions/47744/is-there-an-equivalent-to-vc...", "snippet": "I did find this article which seems <b>like</b> it might come pretty close to answering my question, since they apply <b>structural</b> <b>risk</b> <b>minimization</b> (<b>SRM</b>) to gaussian mixture models, and it looks <b>like</b> they estimate the capacity using &quot;annealed entropy&quot; applied to a class of threshold-based indicator functions associated with the log-likelihood function. However, they don&#39;t provide references to theorems/bounds to show whether this is a principled way to assess the capacity of a probability density ...", "dateLastCrawled": "2022-01-12T03:49:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Lecture 8: Model selection and <b>structural</b> <b>risk minimization</b>", "url": "https://people.cs.umass.edu/~akshay/courses/cs690m/files/lec8.pdf", "isFamilyFriendly": true, "displayUrl": "https://people.cs.umass.edu/~akshay/courses/cs690m/files/lec8.pdf", "snippet": "<b>Structural</b> <b>risk minimization</b> is a way to basically do this for free, and get the best of both worlds. The rst observation is that if we weight the classes appropriately by taking a union bound, we can get a convergence result that is fairly <b>similar</b> to the uniform convergence results we have used in the past. This is a form of non-uniform ...", "dateLastCrawled": "2021-12-31T02:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Statistical <b>Learning</b> Theory: The <b>Structural</b> <b>Risk</b> <b>Minimization</b> Principle", "url": "http://www.cnel.ufl.edu/courses/EEL6814/srm.pdf", "isFamilyFriendly": true, "displayUrl": "www.cnel.ufl.edu/courses/EEL6814/<b>srm</b>.pdf", "snippet": "<b>Structural</b> <b>Risk</b> <b>Minimization</b> (<b>SRM</b>) Principle Vapnik posed four questions that need to be addressed in the design of <b>learning</b> machines (LMs): 1. What are the necessary and sufficient conditions for consistency of a <b>learning</b> process. 2. How fast is the rate of convergence to the solution. 3. How can we control the generalization ability of the LM. 4. How can we construct an <b>algorithm</b> that implement these pre requisites. <b>Structural</b> <b>Risk</b> <b>Minimization</b> (<b>SRM</b>) principle Vapnik argues that the ...", "dateLastCrawled": "2022-01-25T07:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Structural</b> <b>Risk</b> <b>Minimization</b> (<b>SRM</b>) Task - GM-RKB", "url": "https://www.gabormelli.com/RKB/Structural_Risk_Minimization_(SRM)_Task", "isFamilyFriendly": true, "displayUrl": "https://www.gabormelli.com/RKB/<b>Structural</b>_<b>Risk</b>_<b>Minimization</b>_(<b>SRM</b>)_Task", "snippet": "<b>Structural</b> <b>risk</b> <b>minimization</b> (<b>SRM</b>) is an inductive principle of use in <b>machine</b> <b>learning</b>. Commonly in <b>machine</b> <b>learning</b>, a generalized model must be selected from a finite data set, with the consequent problem of overfitting \u2013 the model becoming too strongly tailored to the particularities of the training set and generalizing poorly to new data. The <b>SRM</b> principle addresses this problem by balancing the model&#39;s complexity against its success at fitting the training data. In practical terms ...", "dateLastCrawled": "2021-12-09T18:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Structural</b> <b>risk</b> <b>minimization</b> and minimum description length", "url": "https://andrewcharlesjones.github.io/journal/mdl.html", "isFamilyFriendly": true, "displayUrl": "https://andrewcharlesjones.github.io/journal/mdl.html", "snippet": "We have seen that <b>structural</b> <b>risk</b> <b>minimization</b> is a general framework for searching among a large set of hypotheses by incorporating knowledge about which types of hypotheses we prefer. Minimum description length is one possible strategy for formally assigning preferences, which as we saw is suprisingly agnostic to the langauge in which we represent the hypotheses. These ideas are deeply connected to several other well-known concepts in statistics and <b>machine</b> <b>learning</b>, some of which are ...", "dateLastCrawled": "2022-01-02T01:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Structural risk minimization and SVMs</b> - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/27036690/structural-risk-minimization-and-svms", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/27036690/<b>structural-risk-minimization-and-svms</b>", "snippet": "<b>Structural</b> <b>risk</b> <b>minimization</b> is the Vapnik&#39;s concept of <b>learning</b> which is conceptually <b>similar</b> to the other &quot;minimum assumption approaches&quot;. In short words, most of the <b>learning</b> algorithms (in classification) try to find some mapping from input space to classes in such a way that this mapping behaves &quot;well&quot; on the training set (returns correct answers). It is however a known phenomena, that this is not enough to build a good model, as one could for example memorize all training examples and ...", "dateLastCrawled": "2022-01-03T15:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A. <b>Structural</b> <b>Risk</b> <b>Minimization</b>", "url": "https://cs.nyu.edu/~mohri/aml21/sol2.pdf", "isFamilyFriendly": true, "displayUrl": "https://cs.nyu.edu/~mohri/aml21/sol2.pdf", "snippet": "Advanced <b>Machine</b> <b>Learning</b> 2021 Courant Institute of Mathematical Sciences Homework assignment 2 April 20, 2021 Due: May 04, 2021 A. <b>Structural</b> <b>Risk</b> <b>Minimization</b> As discussed in class, the <b>Structural</b> <b>Risk</b> <b>Minimization</b> (<b>SRM</b>) technique is based on a hypothesis set H de ned as a countable union of hypothesis sets H n with nite VC-dimension or favorable Rademacher complexity. In this problem, we study several questions related to such countable union hypothesis sets. 1.Let H = S +1 n=1 fh ngbe a ...", "dateLastCrawled": "2021-12-28T06:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "11.1 Empirical <b>Risk</b> <b>Minimization</b> - Carnegie Mellon University", "url": "https://www.cs.cmu.edu/~aarti/Class/10704_Fall16/lec11.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.cmu.edu/~aarti/Class/10704_Fall16/lec11.pdf", "snippet": "11.2 Complexity Regularized Empirical <b>Risk</b> <b>Minimization</b> aka <b>Structural</b> <b>Risk</b> <b>Minimization</b> To achieve better estimation of the true <b>risk</b>, we should minimize both the empirical <b>risk</b> and complexity, instead of only minimizing the empirical <b>risk</b>. f^<b>SRM</b> = argmin f2F fR^(f) + (f)g (11.14) where (f) = q c(f)+log 2 2n. With probability 1 , we have the ...", "dateLastCrawled": "2022-01-29T15:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Model Selection in <b>Machine</b> <b>Learning</b> | by ANUSHKA BAJPAI | Medium", "url": "https://medium.com/@anushkhabajpai/model-selection-in-machine-learning-c568e5a42dcc", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@anushkhabajpai/model-selection-in-<b>machine</b>-<b>learning</b>-c568e5a42dcc", "snippet": "<b>Structural</b> <b>Risk</b> <b>Minimization</b> (<b>SRM</b>) <b>Machine</b> <b>learning</b> models face the inevitable problem of defining a generalized theory from a set of finite data. This leads to cases of overfitting where the ...", "dateLastCrawled": "2022-01-29T03:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "An Efficient <b>Machine</b> <b>Learning</b> Regression Model for Rainfall Prediction", "url": "https://research.ijcaonline.org/volume115/number23/pxc3902681.pdf", "isFamilyFriendly": true, "displayUrl": "https://research.ijcaonline.org/volume115/number23/pxc3902681.pdf", "snippet": "SVM is fundamentally a linear <b>machine</b>, which can be seen as a statistical tool that procedures the problem identical to Artificial Neural Networks (ANN). <b>Structural</b> <b>Risk</b> <b>Minimization</b> (<b>SRM</b>) is used to relay wellbeing on unseen data.SVM is a <b>similar</b> implementation principle of <b>Structural</b> <b>Risk</b> <b>minimization</b>. While on one hand it has all the strengths", "dateLastCrawled": "2021-12-08T18:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(PDF) <b>Support Vector Machines: Theory and Applications</b>", "url": "https://www.researchgate.net/publication/221621494_Support_Vector_Machines_Theory_and_Applications", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221621494_Support_Vector_<b>Machine</b>s", "snippet": "hypothesis spaces is known as <b>Structural</b> <b>Risk</b> <b>Minimization</b> (<b>SRM</b>) (Vapnik, 1998). An important question that arises in SLT is that of meas uring the &quot;complexity&quot; of a hypothesis space - which, as ...", "dateLastCrawled": "2022-02-02T02:47:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "svm - difference between <b>empirical</b> <b>risk</b> <b>minimization</b> and <b>structural</b> ...", "url": "https://datascience.stackexchange.com/questions/66729/difference-between-empirical-risk-minimization-and-structural-risk-minimization", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/66729", "snippet": "<b>Structural</b> <b>risk</b> <b>minimization</b> (<b>SRM</b>) is an inductive principle of use in <b>machine</b> <b>learning</b>. Commonly in <b>machine</b> <b>learning</b>, a generalized model must be selected from a finite data set, with the consequent problem of overfitting \u2013 the model becoming too strongly tailored to the particularities of the training set and generalizing poorly to new data. The <b>SRM</b> principle addresses this problem by balancing the model&#39;s complexity against its success at fitting the training data.", "dateLastCrawled": "2022-01-24T04:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Understanding <b>Machine</b> <b>Learning</b>: From Theory To Algorithms 1107057132 ...", "url": "https://ebin.pub/understanding-machine-learning-from-theory-to-algorithms-1107057132-9781107057135.html", "isFamilyFriendly": true, "displayUrl": "https://ebin.pub/understanding-<b>machine</b>-<b>learning</b>-from-theory-to-<b>algorithms</b>-1107057132...", "snippet": "We describe the Empirical <b>Risk</b> <b>Minimization</b> (ERM), <b>Structural</b> <b>Risk</b> <b>Minimization</b> (<b>SRM</b>), and Minimum Description Length (MDL) <b>learning</b> rules, which show \u201chow a <b>machine</b> <b>can</b> learn.\u201d We quantify the amount of data needed for <b>learning</b> using the ERM, <b>SRM</b>, and MDL rules and show how <b>learning</b> might fail by deriving a \u201cno-free-lunch\u201d theorem. We also discuss how much computation time is required for <b>learning</b>. In the second part of the book we describe various <b>learning</b> algorithms. For some of ...", "dateLastCrawled": "2021-12-31T17:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "VC-<b>dimension and structural risk minimization for</b> the analysis of ...", "url": "https://www.sciencedirect.com/science/article/pii/S0096300305007824", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0096300305007824", "snippet": "Meir has outlined an approach that extends Vapnik\u2019s method of <b>Structural</b> <b>Risk</b> <b>Minimization</b> to time series generated by an underlying mixing stochastic process. However, this approach requires the knowledge of the mixing rate of the process, which is not at all easy to estimate. Therefore, we will straightforwardly use the standard <b>SRM</b> for nonlinear regressors and our heuristic approach will be justified only a posteriori by the results for the problem of model choice. If density ...", "dateLastCrawled": "2021-10-24T14:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Understanding Machine Learning: From Theory</b> to Algorithms [pdf] [PDF ...", "url": "https://authorzilla.com/JjpMG/understanding-machine-learning-from-theory-to-algorithms-pdf.html", "isFamilyFriendly": true, "displayUrl": "https://authorzilla.com/JjpMG/<b>understanding-machine-learning-from-theory</b>-to-<b>algorithms</b>...", "snippet": "We describe the Empirical <b>Risk</b> <b>Minimization</b> (ERM), <b>Structural</b> <b>Risk</b> <b>Minimization</b> (<b>SRM</b>), and Minimum Description Length (MDL) <b>learning</b> rules, which shows how <b>can</b> a <b>machine</b> learn. We quantify the amount of data needed for <b>learning</b> using the ERM, <b>SRM</b>, and MDL rules and show how <b>learning</b> might fail by deriving . 7 viii a no-free-lunch theorem. We also discuss how much computation time is re- quired for <b>learning</b>. In the second part of the book we describe various <b>learning</b> algorithms. For some of ...", "dateLastCrawled": "2022-02-03T04:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>understanding-machine-learning</b>-theory-algorithms[1] Pages 1 - 50 - Flip ...", "url": "https://fliphtml5.com/flqg/grxi/basic", "isFamilyFriendly": true, "displayUrl": "https://fliphtml5.com/flqg/grxi/basic", "snippet": "We describethe Empirical <b>Risk</b> <b>Minimization</b> (ERM), <b>Structural</b> <b>Risk</b> <b>Minimization</b> (<b>SRM</b>),and Minimum Description Length (MDL) <b>learning</b> rules, which shows \u201chow cana <b>machine</b> learn\u201d. We quantify the amount of data needed for <b>learning</b> usingthe ERM, <b>SRM</b>, and MDL rules and show how <b>learning</b> might fail by deriving viii a \u201cno-free-lunch\u201d theorem. We also discuss how much computation time is re- quired for <b>learning</b>. In the second part of the book we describe various <b>learning</b> algorithms. For some ...", "dateLastCrawled": "2021-12-18T06:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>machine</b> <b>learning</b> - Is there an equivalent to VC-dimension for density ...", "url": "https://cstheory.stackexchange.com/questions/47744/is-there-an-equivalent-to-vc-dimension-for-density-estimation-as-opposed-to-clas", "isFamilyFriendly": true, "displayUrl": "https://cstheory.stackexchange.com/questions/47744/is-there-an-equivalent-to-vc...", "snippet": "I did find this article which seems like it might come pretty close to answering my question, since they apply <b>structural</b> <b>risk</b> <b>minimization</b> (<b>SRM</b>) to gaussian mixture models, and it looks like they estimate the capacity using &quot;annealed entropy&quot; applied to a class of threshold-based indicator functions associated with the log-likelihood function. However, they don&#39;t provide references to theorems/bounds to show whether this is a principled way to assess the capacity of a probability density ...", "dateLastCrawled": "2022-01-12T03:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Automatically Detecting Excavator Anomalies Based</b> on <b>Machine</b> <b>Learning</b>", "url": "https://www.readkong.com/page/automatically-detecting-excavator-anomalies-based-on-9697342", "isFamilyFriendly": true, "displayUrl": "https://www.readkong.com/page/<b>automatically-detecting-excavator-anomalies-based</b>-on-9697342", "snippet": "Support vector <b>machine</b> is based on the principle of <b>Structural</b> <b>Risk</b> <b>Minimization</b> (<b>SRM</b>) in statistical <b>learning</b> theory, and has good generalization performance [26]. Minimizing <b>structural</b> <b>risk</b> means maximizing profits between different categories. Thus, SVM is not only a useful statistical theory, but also a way to deal with engineering problems [27]. The idea of SVM is to divide training samples into two classes using a linearly separated hyperplane. Symmetry 2019, 11, 957 11 of 18 In this ...", "dateLastCrawled": "2022-01-16T15:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) <b>Understanding Machine Learning: From Theory</b> to Algorithms | Keep ...", "url": "https://www.academia.edu/40679311/Understanding_Machine_Learning_From_Theory_to_Algorithms", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/40679311/<b>Understanding_Machine_Learning_From_Theory</b>_to_<b>Algorithms</b>", "snippet": "<b>Understanding Machine Learning: From Theory</b> to Algorithms. Keep Connected. Dariusz Pietrzak. Maestre Sanmiguel Colombia. Weilyu Wang. Shai Shalev-shwartz. Download Download PDF. Full PDF Package Download Full PDF Package. This Paper. A short summary of this paper. 37 Full PDFs related to this paper. Read Paper. Download Download PDF. Download Full PDF Package ...", "dateLastCrawled": "2022-01-23T01:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A <b>Bilevel Optimization Approach to Machine Learning</b>", "url": "https://www.slideshare.net/butest/a-bilevel-optimization-approach-to-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/butest/a-<b>bilevel-optimization-approach-to-machine-learning</b>", "snippet": "1.1.3 <b>Structural</b> <b>Risk</b> <b>Minimization</b> Theorem 1.4 provides a distribution-independent bound on the true <b>risk</b>; it is a combination of the empirical <b>risk</b> and a con\ufb01dence interval term that is able to control the capacity of a class of functions measured through the VC dimension, h. Thus, the best model <b>can</b> be obtained by minimizing the left- hand side of the inequality (1.5), and <b>SRM</b> aims to do precisely this. <b>SRM</b> is an inductive principle like ERM which is used to learn models from \ufb01nite ...", "dateLastCrawled": "2022-02-02T12:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(PDF) <b>Understanding Machine Learning: From Theory</b> to Algorithms ...", "url": "https://www.academia.edu/27872471/Understanding_Machine_Learning_From_Theory_to_Algorithms", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/27872471", "snippet": "<b>Understanding Machine Learning: From Theory</b> to Algorithms. M. Colombia. Download Download PDF. Full PDF Package Download Full PDF Package. This Paper. A short summary of this paper. 37 Full PDFs related to this paper. Read Paper. Download Download PDF. Download Full PDF Package ...", "dateLastCrawled": "2022-01-28T13:42:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) A Comparative Analysis of <b>One-class Structural Risk Minimization</b> ...", "url": "https://www.academia.edu/11049393/A_Comparative_Analysis_of_One_class_Structural_Risk_Minimization_by_Support_Vector_Machines_and_Nearest_Neighbor_Rule", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/11049393/A_Comparative_Analysis_of_One_class_<b>Structural</b>_<b>Risk</b>...", "snippet": "One-class classification is an important problem with applications in several different areas such as outlier detection and <b>machine</b> monitoring. In this paper we propose a novel method for one-class classification, referred to as kernel k-NNDDSRM.", "dateLastCrawled": "2021-02-17T20:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Machine</b> <b>Learning</b> algorithms based on &quot;<b>structural</b> <b>risk</b> <b>minimization</b>&quot;?", "url": "https://cs.stackexchange.com/questions/2006/machine-learning-algorithms-based-on-structural-risk-minimization", "isFamilyFriendly": true, "displayUrl": "https://cs.stackexchange.com/questions/2006/<b>machine</b>-<b>learning</b>-<b>algorithms</b>-based-on...", "snippet": "The <b>structural</b> <b>risk</b> <b>minimization</b> principle is a principle that is at least partly &#39;used&#39; in all <b>machine</b> <b>learning</b> methods, since overfitting is often to be taken into account: reducing the complexity of the model is (supposedly and in practice) a good way to limit overfitting.", "dateLastCrawled": "2022-01-19T13:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Model selection in omnivariate decision trees using <b>Structural</b> <b>Risk</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0020025511003604", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0020025511003604", "snippet": "We propose to use <b>Structural Risk Minimization</b> (<b>SRM</b>) to choose between node types in omnivariate decision tree construction to match the complexity of a node to the complexity of the data reaching that node. In order to apply <b>SRM</b> for model selection, one needs the VC-dimension of the candidate models. In this paper, we first derive the VC-dimension of the univariate model, and estimate the VC-dimension of all three models (univariate, linear multivariate or quadratic multivariate ...", "dateLastCrawled": "2021-12-25T12:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Lecture 8: Model selection and <b>structural</b> <b>risk minimization</b>", "url": "https://people.cs.umass.edu/~akshay/courses/cs690m/files/lec8.pdf", "isFamilyFriendly": true, "displayUrl": "https://people.cs.umass.edu/~akshay/courses/cs690m/files/lec8.pdf", "snippet": "Lecture 8: Model selection and <b>structural</b> <b>risk minimization</b> Akshay Krishnamurthy akshay@cs.umass.edu September 28, 2017 1 Recap Last time we saw our rst real <b>learning</b> <b>algorithm</b> (that wasn\u2019t obviously ERM), namely kernel regression. Recall the estimator took the form ^(x) = P n i=1 Y iK(kX i xk=h) P n i=1 K(kX i xk=h) where the data (X i;Y", "dateLastCrawled": "2021-12-31T02:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Structural</b> <b>Risk</b> <b>Minimization</b> for Character Recognition", "url": "https://papers.nips.cc/paper/1991/file/10a7cdd970fe135cf4f7bb55c0e3b59f-Paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://papers.nips.cc/paper/1991/file/10a7cdd970fe135cf4f7bb55c0e3b59f-Paper.pdf", "snippet": "<b>Structural</b> <b>Risk</b> <b>Minimization</b> for Character Recognition I. Guyon, V. Vapnik, B. Boser, L. Bottou, and S. A. Solla AT&amp;T Bell Laboratories Holmdel, NJ 07733, USA Abstract The method of <b>Structural</b> <b>Risk</b> <b>Minimization</b> refers to tuning the capacity of the classifier to the available amount of training data. This capac\u00ad", "dateLastCrawled": "2021-11-21T16:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Optimization for Cutting Conditions of Surface Roughness in Machining ...", "url": "http://ijens.org/Vol_19_I_05/190701-1905-3939-IJMME-IJENS.pdf", "isFamilyFriendly": true, "displayUrl": "ijens.org/Vol_19_I_05/190701-1905-3939-IJMME-IJENS.pdf", "snippet": "it <b>structural</b> <b>risk</b> <b>minimization</b> (<b>SRM</b>), to obtain a nonlinearity relationship between the cutting conditions and its surface roughness. The alternative framework gave more accurate prediction model <b>compared</b> to RSM and soft-computing techniques based regression model in a machining dataset, and has less complex structure of regression <b>compared</b> to KPCR and GA. Index Terms\u2014Surface roughness, support vector regression, nonlinear regression, genetic algorithms. I. INTRODUCTION REGRESSION models ...", "dateLastCrawled": "2021-09-17T22:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine</b> <b>learning</b> theory - Nonuniform learnability", "url": "http://ce.sharif.edu/courses/98-99/2/ce718-1/resources/root/Slides/Lect-10.pdf", "isFamilyFriendly": true, "displayUrl": "ce.sharif.edu/courses/98-99/2/ce718-1/resources/root/Slides/Lect-10.pdf", "snippet": "<b>Structural</b> <b>risk</b> <b>minimization</b> The bound that the <b>SRM</b> rule wishes to minimize is given in the following theorem. Theorem Let w : N 7![0;1] be a function such that P1 n=1 w(n) 1. Let H be a hypothesis class that <b>can</b> be written as H = S n2N H n, where for each n, H n satis es the uniform convergence property with a sample complexity function mUC H ...", "dateLastCrawled": "2021-07-11T14:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Learning</b> Gaussian Mixture Models by <b>Structural</b> <b>Risk</b> <b>Minimization</b>", "url": "http://www.cis.pku.edu.cn/faculty/vision/wangliwei/pdf/Learning_Gaussian_Mixture.pdf", "isFamilyFriendly": true, "displayUrl": "www.cis.pku.edu.cn/faculty/vision/wangliwei/pdf/<b>Learning</b>_Gaussian_Mixture.pdf", "snippet": "<b>Learning</b> Gaussian Mixture Models by <b>Structural</b> <b>Risk</b> <b>Minimization</b> Liwei Wang, and Jufu Feng Center for Information Sciences, School of Electronics Engineering and Computer Science, Peking University, Beijing 100871, China, fwanglw, fjf g@cis.pku.edu.cn Abstract Gaussian mixture models are often used for probability density estimation in pattern recognition and <b>machine</b> <b>learning</b> systems. Selecting an optimal number of components in mixture model is important to ensure an accurate and ef\ufb01cient ...", "dateLastCrawled": "2022-01-13T04:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "CS-E4710 <b>Machine</b> <b>Learning</b>: Supervised Methods", "url": "https://mycourses.aalto.fi/pluginfile.php/1628003/mod_resource/content/2/MLSM_Lecture4_051021.pdf", "isFamilyFriendly": true, "displayUrl": "https://mycourses.aalto.fi/pluginfile.php/1628003/mod_resource/content/2/MLSM_Lecture4...", "snippet": "<b>SRM</b> model selection: pros and cons <b>Structural</b> <b>risk</b> <b>minimization</b> bene ts from strong <b>learning</b> guarantees However, the assumption of a countable decomposition of the hypothesis class is a restrictive one The computational price to pay is large, especially when a large number of hypothesis classes H k has to be processed 17", "dateLastCrawled": "2022-01-17T18:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "An Anomaly Detection Method Based on Normalized Mutual Information ...", "url": "https://link.springer.com/article/10.1007%2Fs11277-017-4320-2", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11277-017-4320-2", "snippet": "Also, we developed a quantum wavelet neural network (QWNN) model and a <b>structural</b> <b>risk</b> <b>minimization</b> extreme <b>learning</b> <b>machine</b> (<b>SRM</b>-ELM) <b>learning</b> <b>algorithm</b>. Finally, we developed an anomaly detection model based on normalized mutual information and a quantum wavelet neural network. Experimental results on anomaly data from real network traffic showed that when <b>compared</b> with existing anomaly detection methods, the proposed method has a higher detection accuracy, lower false positive rate, and ...", "dateLastCrawled": "2022-01-29T13:55:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Frontiers | A <b>Machine Learning Approach to Prioritizing Functionally</b> ...", "url": "https://www.frontiersin.org/articles/10.3389/fpls.2021.639253/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/fpls.2021.639253", "snippet": "Support Vector <b>Machine</b> (SVM) is another approach for variable clustering based on <b>structural</b> <b>risk</b> <b>minimization</b> (<b>SRM</b>) theory (Vanitha et al., 2015). Both RF and SVM have been widely applied for decision making upon input of a large dataset. Therefore, we also utilized these two <b>machine</b> <b>learning</b> approaches to predict functionally active and inactive", "dateLastCrawled": "2022-01-31T01:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>GitHub</b> - gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "Loss function 3.1 Categories 3.2 Empirical <b>risk</b> <b>minimization</b> (ERM) and <b>Structural</b> <b>risk</b> <b>minimization</b> (<b>SRM</b>) 3.2.1 MLE 3.2.2 MAP 4. Evaluation metrics 4.1 Accuracy, precision, recall and F1 4.2 ROC, AUC 4.3 BLEU 5. Sample projects 6. Classical questions 6.1 Why is logistic regression a generalized linear model ?", "dateLastCrawled": "2021-09-12T01:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "PAC, Generalization and <b>SRM</b>", "url": "https://elearning.unipd.it/math/mod/resource/view.php?id=38143", "isFamilyFriendly": true, "displayUrl": "https://e<b>learning</b>.unipd.it/math/mod/resource/view.php?id=38143", "snippet": "Connection to <b>learning</b> Measuring the complexity of the hypotheses space (VC-Dimension) VC-Dimension of hyperplanes <b>Structural</b> <b>Risk</b> <b>Minimization</b> Exercises VC-Dimension of other hypothesis spaces, e.g. intervals in R : h(x) = +1 if a &lt;= x &lt;= b;h(x) = 1 otherwise: Fabio Aiolli PAC, Generalization and <b>SRM</b> October 6th, 202122/22", "dateLastCrawled": "2021-11-19T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Structural Risk Minimization for Character Recognition</b>", "url": "https://proceedings.neurips.cc/paper/1991/file/10a7cdd970fe135cf4f7bb55c0e3b59f-Paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/1991/file/10a7cdd970fe135cf4f7bb55c0e3b59f-Paper.pdf", "snippet": "<b>Structural Risk Minimization for Character Recognition</b> I. Guyon, V. Vapnik, B. Boser, L. Bottou, and S. A. Solla AT&amp;T Bell Laboratories Holmdel, NJ 07733, USA Abstract The method of <b>Structural</b> <b>Risk</b> <b>Minimization</b> refers to tuning the capacity of the classifier to the available amount of training data. This capac\u00ad", "dateLastCrawled": "2022-02-01T13:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and-sample-codes ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/README.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "Loss function 3.1 Categories 3.2 Empirical <b>risk</b> <b>minimization</b> (ERM) and <b>Structural</b> <b>risk</b> <b>minimization</b> (<b>SRM</b>) 3.2.1 MLE 3.2.2 MAP 4. Evaluation metrics 4.1 Accuracy, precision, recall and F1 4.2 ROC, AUC 4.3 BLEU 5. Sample projects 6. Classical questions 6.1 Why is logistic regression a generalized linear model ?", "dateLastCrawled": "2021-09-22T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A Tutorial On Support Vector Machines For Pattern Recognition Pdf ...", "url": "https://elizabethsid.org/for-pdf/11704-a-tutorial-on-support-vector-machines-for-pattern-recognition-pdf-565-290.php", "isFamilyFriendly": true, "displayUrl": "https://elizabethsid.org/for-pdf/11704-a-tutorial-on-support-vector-<b>machines</b>-for...", "snippet": "The paper starts with an overview of <b>structural</b> <b>risk</b> <b>minimization</b> <b>SRM</b> principle, and describes the mechanism of how to construct SVM. For a two-class pattern recognition problem, we discuss in detail the classification mechanism of SVM in three cases of linearly separable, linearly nonseparable and nonlinear. Finally, for nonlinear case, we give a new function mapping technique: By choosing an appropriate kernel function, the SVM can map the low-dimensional input space into the high ...", "dateLastCrawled": "2022-01-19T18:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) <b>Support Vector Machines: Theory and Applications</b>", "url": "https://www.researchgate.net/publication/221621494_Support_Vector_Machines_Theory_and_Applications", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221621494_Support_Vector_<b>Machine</b>s", "snippet": "hypothesis spaces is known as <b>Structural</b> <b>Risk</b> <b>Minimization</b> (<b>SRM</b>) (Vapnik, 1998). An important question that arises in SLT is that of meas uring the &quot;complexity&quot; of a hypothesis space - which, as ...", "dateLastCrawled": "2022-02-02T02:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Data gravitation based classification", "url": "http://www.isda03.softcomputing.net/dgc.pdf", "isFamilyFriendly": true, "displayUrl": "www.isda03.softcomputing.net/dgc.pdf", "snippet": "SVM is a relatively new <b>machine</b> <b>learning</b> method based on the statistical <b>learning</b> theory and <b>structural</b> <b>risk</b> <b>minimization</b> (<b>SRM</b>) principle. SVM is gaining popularity due to many attractive features, and promising empirical performance. SVM is based on the hypothesis that the training samples obey a certain distribution, which restricts its application scope. Rough set [17] theory has also been applied to classi\ufb01cation in recent years especially for feature selection [10] or as a ...", "dateLastCrawled": "2021-12-23T20:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Topics in <b>Machine</b> <b>Learning</b> (TIML-09)", "url": "https://www.cse.iitb.ac.in/~saketh/teaching/cs689.html", "isFamilyFriendly": true, "displayUrl": "https://www.cse.iitb.ac.in/~saketh/teaching/cs689.html", "snippet": "Introduction to Statistical <b>Learning</b> Theory (SLT): Definitions of loss function, <b>risk</b>, empirical <b>risk</b>, motivation for Empirical <b>Risk</b> <b>Minimization</b> (ERM) Further Reading, Supplementary: Jan 12: Consistency of ERM, Sufficient condition for ERM as one-sided uniform convergence, Analysis for finite sets of functions and extensions to general case using Symmetrization trick, Shattering Coeff. Further Reading, Supplementary: Jan 15: Shattering coeff., growth function, VC dimension, Annealed Entropy ...", "dateLastCrawled": "2022-01-11T09:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(PDF) A comparative <b>analysis of structural risk minimization by support</b> ...", "url": "https://www.academia.edu/10904454/A_comparative_analysis_of_structural_risk_minimization_by_support_vector_machines_and_nearest_neighbor_rule", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/10904454/A_comparative_analysis_of_<b>structural</b>_<b>risk</b>...", "snippet": "A Comparative <b>Analysis of Structural Risk Minimization by Support Vector Machines</b> and Nearest Neighbor Rule Bilge Kara\u00b8cal\u0131 , Rajeev Ramanath, Wesley E. Snyder a,b,c a Dept. of Radiology, University of Pennsylvania, Philadephia, PA 19104 b Dept. of Electrical and Computer Engineering, North Carolina State University, Raleigh, NC 27695-7914 c Dept. of Electrical and Computer Engineering, North Carolina State University, Raleigh, NC 27695-7911 Abstract Support Vector Machines (SVMs) are by ...", "dateLastCrawled": "2021-07-19T02:43:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(structural risk minimization (srm))  is like +(machine learning algorithm)", "+(structural risk minimization (srm)) is similar to +(machine learning algorithm)", "+(structural risk minimization (srm)) can be thought of as +(machine learning algorithm)", "+(structural risk minimization (srm)) can be compared to +(machine learning algorithm)", "machine learning +(structural risk minimization (srm) AND analogy)", "machine learning +(\"structural risk minimization (srm) is like\")", "machine learning +(\"structural risk minimization (srm) is similar\")", "machine learning +(\"just as structural risk minimization (srm)\")", "machine learning +(\"structural risk minimization (srm) can be thought of as\")", "machine learning +(\"structural risk minimization (srm) can be compared to\")"]}