{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Reinforcement Learning Made Simple - Intro to Basic Concepts and ...", "url": "https://ketanhdoshi.github.io/Reinforcement-Learning-Intro/", "isFamilyFriendly": true, "displayUrl": "https://ketanhdoshi.github.io/Reinforcement-Learning-Intro", "snippet": "To apply RL, the first step is to structure the problem as something called a <b>Markov Decision Process</b> (<b>MDP</b>). If you haven\u2019t worked with RL before, chances are that the only thing you know about an <b>MDP</b> is that it sounds scary \ud83d\ude04. So let\u2019s try to understand what an <b>MDP</b> is. An <b>MDP</b> has five components that work together in a well-defined way.", "dateLastCrawled": "2022-01-28T17:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Policy Explanation in Factored Markov Decision Processes</b>", "url": "https://www.researchgate.net/publication/236156179_Policy_Explanation_in_Factored_Markov_Decision_Processes", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/236156179_Policy_Explanation_in_Factored...", "snippet": "A <b>Markov decision process</b> (<b>MDP</b>) (Puterman, 1994) models a sequential <b>decision</b> problem, in. which a system evolv es in time and is controlled. by an agent. The system dynamics is governed. by a ...", "dateLastCrawled": "2022-01-12T03:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Reinforcement Learning</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/neuroscience/reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/neuroscience/<b>reinforcement-learning</b>", "snippet": "The basic notion that we will explore is that of a homomorphism of a <b>Markov Decision Process</b> (<b>MDP</b>). We mention various extensions of the basic <b>MDP</b> homomorphism framework in order to accommodate different commonly understood notions of abstraction, namely, aspects of selective attention. Parts of the work described in this chapter have been reported earlier in several papers Narayanmurthy and Ravindran, 2007, 2008; Ravindran and Barto, 2002, 2003a,b; Ravindran et al., 2007). View chapter ...", "dateLastCrawled": "2022-02-02T12:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Reinforcement Learning Made Simple (Part</b> 1 ... - Towards <b>Data</b> Science", "url": "https://towardsdatascience.com/reinforcement-learning-made-simple-part-1-intro-to-basic-concepts-and-terminology-1d2a87aa060", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/<b>reinforcement-learning-made-simple-part</b>-1-intro-to...", "snippet": "An <b>MDP</b> has an Agent, Environment, States, Actions and Rewards (Image by Author) State: this represents the current \u2018state of the world\u2019 at any point. eg. it could capture the position of the robot relative to its terrain, the position of objects around it, and perhaps the direction and speed of the wind. There could be a finite or infinite set of states. Action: these are the actions that the agent <b>takes</b> to interact with the environment. eg. The robot can turn right, left, move forward ...", "dateLastCrawled": "2022-02-02T15:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Adversarial attack and defense in reinforcement <b>learning</b>-from AI ...", "url": "https://cybersecurity.springeropen.com/articles/10.1186/s42400-019-0027-x", "isFamilyFriendly": true, "displayUrl": "https://cybersecurity.springeropen.com/articles/10.1186/s42400-019-0027-x", "snippet": "It is actually a variant of <b>Markov Decision Process</b> (<b>MDP</b>)(<b>Markov</b> 1907). The idea of Q-<b>Learning</b> is based on the value iteration, which can be concluded as, the agent perceives surrounding information from the environment and selects appropriate methods to change the sate of environment according to its own method, and obtains corresponding incentives and penalties to correct the strategy. Q-<b>Learning</b> proposes a method to update the Q-value, which can be concluded as", "dateLastCrawled": "2022-02-02T10:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) <b>PEGASUS: A Policy Search Method for Large MDPs</b> and POMDPs", "url": "https://www.researchgate.net/publication/234140124_PEGASUS_A_Policy_Search_Method_for_Large_MDPs_and_POMDPs", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/234140124_<b>PEGASUS_A_Policy_Search_Method</b>_for...", "snippet": "Abstract. We propose a new approach to the problem of searching a space of policies for a <b>Markov decision process</b> (<b>MDP</b>) or a partially observable <b>Markov decision process</b> (POMDP), given a model ...", "dateLastCrawled": "2021-09-26T06:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Enabling UAV Navigation with Sensor and Environmental Uncertainty in ...", "url": "https://europepmc.org/article/MED/27171096", "isFamilyFriendly": true, "displayUrl": "https://europepmc.org/article/MED/27171096", "snippet": "2.1. <b>Markov</b> <b>Decision</b> Processes and Partially-Observable <b>Markov</b> <b>Decision</b> Processes. A <b>Markov Decision Process</b> (<b>MDP</b>) is an effective mathematical framework to model sequential <b>decision</b> problems affected by uncertainties [].When <b>MDP</b> is used for UAV or robotic missions, the objective is to generate a policy that allows the UAV or robot to decide what sequence of actions it should take, taking into account the uncertainties in motion, in order to maximise a return or cost function.", "dateLastCrawled": "2021-07-16T04:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Score Following as <b>a Multi-Modal Reinforcement Learning Problem</b>", "url": "https://transactions.ismir.net/articles/10.5334/tismir.31/", "isFamilyFriendly": true, "displayUrl": "https://transactions.ismir.net/articles/10.5334/tismir.31", "snippet": "In Section 2, we start by defining the task of score following as a <b>Markov Decision Process</b> (<b>MDP</b>) and explaining its basic building blocks. Section 3 introduces the concept of Policy Gradient Methods and provides details on three learning algorithms we will use. Section 4 proceeds with a description of our experiments and presents results for the case of synthesized piano <b>data</b>. Section 5 then briefly looks at model interpretability, providing some glimpses into the learned representations ...", "dateLastCrawled": "2022-01-28T21:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A Survey on Deep Reinforcement Learning for <b>Data</b> Processing and Analytics", "url": "http://www.vertexdoc.com/doc/a-survey-on-deep-reinforcement-learning-for-data-processing-and-analytics", "isFamilyFriendly": true, "displayUrl": "www.vertexdoc.com/doc/a-survey-on-deep-reinforcement-learning-for-<b>data</b>-<b>process</b>ing-and...", "snippet": "Ling et al. [ ] propose modeling the integration of external evidence to capture diagnostic concept as <b>Markov Decision Process</b> (<b>MDP</b>). The objective is to find the value function mapping from the state to the action. The inputs are case narratives and the <b>outputs</b> are improved concepts and inferred diagnoses. The states are a set of measures over the similarity of current concepts and externally extracted concepts. The actions are whether to accept (part of) the extracted concepts from ...", "dateLastCrawled": "2022-01-19T00:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>GitHub</b> - <b>chauvinSimon/IV19</b>: My 10 takeaways from the 2019 Intelligent ...", "url": "https://github.com/chauvinSimon/IV19", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/chauvinSimon/IV19", "snippet": "The first day I could take part to the workshop \u201c<b>Prediction</b> and <b>Decision</b> Making for Socially Interactive Autonomous Driving, shortened SIPD. ... The AD <b>decision</b> or AD control can be formulated as a <b>Markov Decision Process</b> (<b>MDP</b>), or its extension POMDP with partial observation. There are two types of methods to solve it, i.e. to find the optimal policy, depending if the transition and reward models are known or not: If the transition and reward models are known, the solving task is referred ...", "dateLastCrawled": "2022-01-31T21:08:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Reinforcement Learning Made Simple - Intro to Basic Concepts and ...", "url": "https://ketanhdoshi.github.io/Reinforcement-Learning-Intro/", "isFamilyFriendly": true, "displayUrl": "https://ketanhdoshi.github.io/Reinforcement-Learning-Intro", "snippet": "To apply RL, the first step is to structure the problem as something called a <b>Markov Decision Process</b> (<b>MDP</b>). If you haven\u2019t worked with RL before, chances are that the only thing you know about an <b>MDP</b> is that it sounds scary \ud83d\ude04. So let\u2019s try to understand what an <b>MDP</b> is. An <b>MDP</b> has five components that work together in a well-defined way.", "dateLastCrawled": "2022-01-28T17:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Reinforcement Learning and Markov Decision Processes</b>", "url": "https://www.researchgate.net/publication/235004620_Reinforcement_Learning_and_Markov_Decision_Processes", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/235004620_Reinforcement_Learning_and_<b>Markov</b>...", "snippet": "The <b>Markov Decision Process</b>. Putting all elements together results in the de\ufb01nition of a <b>Markov decision process</b> , which will be the base model for the large majority of methods described in this", "dateLastCrawled": "2022-01-24T17:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Adversarial attack and defense in reinforcement <b>learning</b>-from AI ...", "url": "https://cybersecurity.springeropen.com/articles/10.1186/s42400-019-0027-x", "isFamilyFriendly": true, "displayUrl": "https://cybersecurity.springeropen.com/articles/10.1186/s42400-019-0027-x", "snippet": "It is actually a variant of <b>Markov Decision Process</b> (<b>MDP</b>)(<b>Markov</b> 1907). The idea of Q-<b>Learning</b> is based on the value iteration, which can be concluded as, the agent perceives surrounding information from the environment and selects appropriate methods to change the sate of environment according to its own method, and obtains corresponding incentives and penalties to correct the strategy. Q-<b>Learning</b> proposes a method to update the Q-value, which can be concluded as", "dateLastCrawled": "2022-02-02T10:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Enabling UAV Navigation with Sensor and Environmental Uncertainty in ...", "url": "https://europepmc.org/article/MED/27171096", "isFamilyFriendly": true, "displayUrl": "https://europepmc.org/article/MED/27171096", "snippet": "2.1. <b>Markov</b> <b>Decision</b> Processes and Partially-Observable <b>Markov</b> <b>Decision</b> Processes. A <b>Markov Decision Process</b> (<b>MDP</b>) is an effective mathematical framework to model sequential <b>decision</b> problems affected by uncertainties [].When <b>MDP</b> is used for UAV or robotic missions, the objective is to generate a policy that allows the UAV or robot to decide what sequence of actions it should take, taking into account the uncertainties in motion, in order to maximise a return or cost function.", "dateLastCrawled": "2021-07-16T04:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) <b>PEGASUS: A Policy Search Method for Large MDPs</b> and POMDPs", "url": "https://www.researchgate.net/publication/234140124_PEGASUS_A_Policy_Search_Method_for_Large_MDPs_and_POMDPs", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/234140124_<b>PEGASUS_A_Policy_Search_Method</b>_for...", "snippet": "Abstract. We propose a new approach to the problem of searching a space of policies for a <b>Markov decision process</b> (<b>MDP</b>) or a partially observable <b>Markov decision process</b> (POMDP), given a model ...", "dateLastCrawled": "2021-09-26T06:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A <b>review On reinforcement learning: Introduction and applications</b> in ...", "url": "https://www.sciencedirect.com/science/article/pii/S0098135420300557", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0098135420300557", "snippet": "This review starts with an introduction to RL, the <b>Markov decision process</b> (<b>MDP</b>) and different families of RL methods. Concepts explored will also be intuitively correlated to <b>process</b> control ideas to enhance understanding. Section 3 compares RL qualitatively to traditional control methods and also introduces some of successful RL applications. A quantitative example where RL was applied onto an industrial pumping system will also be shown here to provide additional intuition. The section is ...", "dateLastCrawled": "2022-01-14T17:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Reinforcement Learning Made Simple (Part</b> 1 ... - Towards <b>Data</b> Science", "url": "https://towardsdatascience.com/reinforcement-learning-made-simple-part-1-intro-to-basic-concepts-and-terminology-1d2a87aa060", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/<b>reinforcement-learning-made-simple-part</b>-1-intro-to...", "snippet": "An <b>MDP</b> has an Agent, Environment, States, Actions and Rewards (Image by Author) State: this represents the current \u2018state of the world\u2019 at any point. eg. it could capture the position of the robot relative to its terrain, the position of objects around it, and perhaps the direction and speed of the wind. There could be a finite or infinite set of states. Action: these are the actions that the agent <b>takes</b> to interact with the environment. eg. The robot can turn right, left, move forward ...", "dateLastCrawled": "2022-02-02T15:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Score Following as <b>a Multi-Modal Reinforcement Learning Problem</b>", "url": "https://transactions.ismir.net/articles/10.5334/tismir.31/", "isFamilyFriendly": true, "displayUrl": "https://transactions.ismir.net/articles/10.5334/tismir.31", "snippet": "In Section 2, we start by defining the task of score following as a <b>Markov Decision Process</b> (<b>MDP</b>) and explaining its basic building blocks. Section 3 introduces the concept of Policy Gradient Methods and provides details on three learning algorithms we will use. Section 4 proceeds with a description of our experiments and presents results for the case of synthesized piano <b>data</b>. Section 5 then briefly looks at model interpretability, providing some glimpses into the learned representations ...", "dateLastCrawled": "2022-01-28T21:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A Survey on Deep Reinforcement Learning for <b>Data</b> Processing and Analytics", "url": "http://www.vertexdoc.com/doc/a-survey-on-deep-reinforcement-learning-for-data-processing-and-analytics", "isFamilyFriendly": true, "displayUrl": "www.vertexdoc.com/doc/a-survey-on-deep-reinforcement-learning-for-<b>data</b>-<b>process</b>ing-and...", "snippet": "Ling et al. [ ] propose modeling the integration of external evidence to capture diagnostic concept as <b>Markov Decision Process</b> (<b>MDP</b>). The objective is to find the value function mapping from the state to the action. The inputs are case narratives and the <b>outputs</b> are improved concepts and inferred diagnoses. The states are a set of measures over the similarity of current concepts and externally extracted concepts. The actions are whether to accept (part of) the extracted concepts from ...", "dateLastCrawled": "2022-01-19T00:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Papers for 2021-11-09 \u00b7 Discussion #15 \u00b7 aryanpandey/Paper_Collection ...", "url": "https://github.com/aryanpandey/Paper_Collection/discussions/15", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/aryanpandey/Paper_Collection/discussions/15", "snippet": "19. Proximal Reinforcement Learning: Efficient Off-Policy Evaluation in Partially Observed <b>Markov</b> <b>Decision</b> Processes. Paper PDF Link. Abstract: In applications of offline reinforcement learning to observational <b>data</b>, such as in healthcare or education, a general concern is that observed actions might be affected by unobserved factors, inducing confounding and biasing estimates derived under the assumption of a perfect <b>Markov decision process</b> (<b>MDP</b>) model. Here we tackle this by considering ...", "dateLastCrawled": "2022-01-22T09:13:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Reinforcement Learning Made Simple - Intro to Basic Concepts and ...", "url": "https://ketanhdoshi.github.io/Reinforcement-Learning-Intro/", "isFamilyFriendly": true, "displayUrl": "https://ketanhdoshi.github.io/Reinforcement-Learning-Intro", "snippet": "To apply RL, the first step is to structure the problem as something called a <b>Markov Decision Process</b> (<b>MDP</b>). If you haven\u2019t worked with RL before, chances are that the only thing you know about an <b>MDP</b> is that it sounds scary \ud83d\ude04. So let\u2019s try to understand what an <b>MDP</b> is. An <b>MDP</b> has five components that work together in a well-defined way.", "dateLastCrawled": "2022-01-28T17:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Reinforcement Learning Made Simple (Part 1): Intro to Basic Concepts ...", "url": "https://satowaki.com/reinforcement-learning-made-simple-part-1-intro-to-basic-concepts-and-terminology-1d2a87aa060", "isFamilyFriendly": true, "displayUrl": "https://satowaki.com/reinforcement-learning-made-simple-part-1-intro-to-basic-concepts...", "snippet": "An <b>MDP</b> has an Agent, Environment, States, Actions and Rewards (Image by Author) State: this represents the current \u2018state of the world\u2019 at any point. eg. it could capture the position of the robot relative to its terrain, the position of objects around it, and perhaps the direction and speed of the wind. There could be a finite or infinite set of states. Action: these are the actions that the agent <b>takes</b> to interact with the environment. eg. The robot <b>can</b> turn right, left, move forward ...", "dateLastCrawled": "2022-02-07T03:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Reinforcement Learning</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/neuroscience/reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/neuroscience/<b>reinforcement-learning</b>", "snippet": "The basic notion that we will explore is that of a homomorphism of a <b>Markov Decision Process</b> (<b>MDP</b>). We mention various extensions of the basic <b>MDP</b> homomorphism framework in order to accommodate different commonly understood notions of abstraction, namely, aspects of selective attention. Parts of the work described in this chapter have been reported earlier in several papers Narayanmurthy and Ravindran, 2007, 2008; Ravindran and Barto, 2002, 2003a,b; Ravindran et al., 2007). View chapter ...", "dateLastCrawled": "2022-02-02T12:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Review of Incident <b>Prediction</b>, Resource Allocation, and Dispatch ...", "url": "https://www.sciencedirect.com/science/article/pii/S0001457521005327", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0001457521005327", "snippet": "For example, it <b>can</b> be formulated as a <b>Markov decision process</b> (<b>MDP</b>) (Kochenderfer, 2015). This formulation is particularly relevant for problems seeking to find policies for dispatch. The aim is to find an optimal policy (i.e. control choices for every possible state of the system) that maximizes the expected sum of rewards. The second approach is to directly model the planning problem as an optimization problem according to a specific measure of interest. As an example, a lot of prior work ...", "dateLastCrawled": "2022-01-30T09:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) <b>PEGASUS: A Policy Search Method for Large MDPs</b> and POMDPs", "url": "https://www.researchgate.net/publication/234140124_PEGASUS_A_Policy_Search_Method_for_Large_MDPs_and_POMDPs", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/234140124_<b>PEGASUS_A_Policy_Search_Method</b>_for...", "snippet": "Abstract. We propose a new approach to the problem of searching a space of policies for a <b>Markov decision process</b> (<b>MDP</b>) or a partially observable <b>Markov decision process</b> (POMDP), given a model ...", "dateLastCrawled": "2021-09-26T06:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Reinforcement Learning Made Simple (Part</b> 1 ... - Towards <b>Data</b> Science", "url": "https://towardsdatascience.com/reinforcement-learning-made-simple-part-1-intro-to-basic-concepts-and-terminology-1d2a87aa060", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/<b>reinforcement-learning-made-simple-part</b>-1-intro-to...", "snippet": "An <b>MDP</b> has an Agent, Environment, States, Actions and Rewards (Image by Author) State: this represents the current \u2018state of the world\u2019 at any point. eg. it could capture the position of the robot relative to its terrain, the position of objects around it, and perhaps the direction and speed of the wind. There could be a finite or infinite set of states. Action: these are the actions that the agent <b>takes</b> to interact with the environment. eg. The robot <b>can</b> turn right, left, move forward ...", "dateLastCrawled": "2022-02-02T15:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Frontiers | Generalize Robot Learning From Demonstration to Variant ...", "url": "https://www.frontiersin.org/articles/10.3389/fnbot.2020.00021/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/fnbot.2020.00021", "snippet": "Solving such problem <b>can</b> be modeled as <b>Markov Decision Process</b> (<b>MDP</b>). Policy Gradient method is one kind of reinforcement learning algorithms. For exploration in action space, stochastic policy samples from a Gaussian distribution \u03c0 \u03b8 ~ N (\u03bc (s), \u03c3 (s) 2 I) with \u03bc(s) and \u03c3(s) parameterized by \u03b8, at each time step. Stochastic policy gradient methods maximize the expected cumulative reward by estimating the performance gradient \u2207 \u03b8 J(\u03c0 \u03b8) based on the Stochastic Policy Gradient ...", "dateLastCrawled": "2022-01-19T09:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) Knowledge Induced Deep Q-Network for a Slide-to-Wall Object Grasping", "url": "https://www.researchgate.net/publication/336371943_Knowledge_Induced_Deep_Q-Network_for_a_Slide-to-Wall_Object_Grasping", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/336371943_Knowledge_Induced_Deep_Q-Network...", "snippet": "Slide-to-Wall grasping pr oblem as a <b>Markov Decision Process</b>. and propose a reinforcement lear ning approach. Though a. standard Deep Q-Network (DQN) method is capable of solving. <b>MDP</b> problems, it ...", "dateLastCrawled": "2022-01-18T03:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 8, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Artificial neural network</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Artificial_neural_network", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Artificial_neural_network</b>", "snippet": "This <b>can</b> <b>be thought</b> of as learning with a &quot;teacher&quot;, in the form of a function that provides continuous feedback on the quality of solutions obtained thus far. Unsupervised learning. In unsupervised learning, input <b>data</b> is given along with the cost function, some function of the <b>data</b> and the network&#39;s output. The cost function is dependent on the task (the model domain) and any a priori assumptions (the implicit properties of the model, its parameters and the observed variables). As a ...", "dateLastCrawled": "2022-02-03T16:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Deep Learning and Reinforcement Learning for Autonomous Unmanned Aerial ...", "url": "https://www.arxiv-vanity.com/papers/2009.03349/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2009.03349", "snippet": "<b>MDP</b> <b>Markov Decision Process</b> MPC Model Predictive Control NUI Natural User Interface ODE Open Dynamics Engine OGRE Object-Oriented Graphics Rendering Engine PPM Pulse Position Modulation PPO Proximal Policy Optimization PID Proportional-Integral-Derivative PS-DNN Partially Shared-Deep Neural Network RAM Random Access Memory RDPG Recurrent Deterministic Policy Gradient RNN Recurrent Neural Network RL Reinforcement Learning ROS Robot Operating System SARSA State-action-reward-state-action SDF ...", "dateLastCrawled": "2021-12-11T05:11:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Reinforcement Learning and Markov Decision Processes</b>", "url": "https://www.researchgate.net/publication/235004620_Reinforcement_Learning_and_Markov_Decision_Processes", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/235004620_Reinforcement_Learning_and_<b>Markov</b>...", "snippet": "The <b>Markov Decision Process</b>. Putting all elements together results in the de\ufb01nition of a <b>Markov decision process</b> , which will be the base model for the large majority of methods described in this", "dateLastCrawled": "2022-01-24T17:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Policy Explanation in Factored Markov Decision Processes</b>", "url": "https://www.researchgate.net/publication/236156179_Policy_Explanation_in_Factored_Markov_Decision_Processes", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/236156179_Policy_Explanation_in_Factored...", "snippet": "A <b>Markov decision process</b> (<b>MDP</b>) (Puterman, 1994) models a sequential <b>decision</b> problem, in. which a system evolv es in time and is controlled. by an agent. The system dynamics is governed. by a ...", "dateLastCrawled": "2022-01-12T03:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Diversity Evolutionary Policy Deep Reinforcement Learning", "url": "https://www.hindawi.com/journals/cin/2021/5300189/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.hindawi.com</b>/journals/cin/2021/5300189", "snippet": "<b>Markov Decision Process</b> (<b>MDP</b>) In reinforcement learning, the interaction <b>process</b> between reinforcement learning agents and the environment <b>can</b> be represented by <b>Markov decision process</b> (<b>MDP</b>). <b>MDP</b> <b>can</b> be represented by a tuple , where is the state space, is the action space, is the reward function, is the state transition probability, and is the discount factor.", "dateLastCrawled": "2022-01-24T19:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Reinforcement Learning Made Simple - Intro to Basic Concepts and ...", "url": "https://ketanhdoshi.github.io/Reinforcement-Learning-Intro/", "isFamilyFriendly": true, "displayUrl": "https://ketanhdoshi.github.io/Reinforcement-Learning-Intro", "snippet": "To apply RL, the first step is to structure the problem as something called a <b>Markov Decision Process</b> (<b>MDP</b>). If you haven\u2019t worked with RL before, chances are that the only thing you know about an <b>MDP</b> is that it sounds scary \ud83d\ude04. So let\u2019s try to understand what an <b>MDP</b> is. An <b>MDP</b> has five components that work together in a well-defined way.", "dateLastCrawled": "2022-01-28T17:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) <b>Reinforcement Learning and Markov Decision Processes</b> | Marco A ...", "url": "https://www.academia.edu/23684694/Reinforcement_Learning_and_Markov_Decision_Processes", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/23684694/<b>Reinforcement_Learning_and_Markov_Decision_Processes</b>", "snippet": "The <b>Markov Decision Process</b>. Putting all elements together results in the definition of a <b>Markov decision process</b>, which will be the base model for the large majority of methods described in this book. Definition 3.1 A <b>Markov decision process</b> is a tuple hS, A, T, Ri in which S is a finite set of states, A a finite set of actions, T a transition function defined as T : S \u00d7 A \u00d7 S \u2192 [0, 1] and R a reward function defined as R : S \u00d7 A \u00d7 S \u2192 R. The transition function T and the reward ...", "dateLastCrawled": "2022-01-14T01:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A <b>review On reinforcement learning: Introduction and applications</b> in ...", "url": "https://www.sciencedirect.com/science/article/pii/S0098135420300557", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0098135420300557", "snippet": "This review starts with an introduction to RL, the <b>Markov decision process</b> (<b>MDP</b>) and different families of RL methods. Concepts explored will also be intuitively correlated to <b>process</b> control ideas to enhance understanding. Section 3 compares RL qualitatively to traditional control methods and also introduces some of successful RL applications. A quantitative example where RL was applied onto an industrial pumping system will also be shown here to provide additional intuition. The section is ...", "dateLastCrawled": "2022-01-14T17:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Reinforcement Learning Made Simple (Part</b> 1 ... - Towards <b>Data</b> Science", "url": "https://towardsdatascience.com/reinforcement-learning-made-simple-part-1-intro-to-basic-concepts-and-terminology-1d2a87aa060", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/<b>reinforcement-learning-made-simple-part</b>-1-intro-to...", "snippet": "An <b>MDP</b> has an Agent, Environment, States, Actions and Rewards (Image by Author) State: this represents the current \u2018state of the world\u2019 at any point. eg. it could capture the position of the robot relative to its terrain, the position of objects around it, and perhaps the direction and speed of the wind. There could be a finite or infinite set of states. Action: these are the actions that the agent <b>takes</b> to interact with the environment. eg. The robot <b>can</b> turn right, left, move forward ...", "dateLastCrawled": "2022-02-02T15:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Score Following as <b>a Multi-Modal Reinforcement Learning Problem</b>", "url": "https://transactions.ismir.net/articles/10.5334/tismir.31/", "isFamilyFriendly": true, "displayUrl": "https://transactions.ismir.net/articles/10.5334/tismir.31", "snippet": "In Section 2, we start by defining the task of score following as a <b>Markov Decision Process</b> (<b>MDP</b>) and explaining its basic building blocks. Section 3 introduces the concept of Policy Gradient Methods and provides details on three learning algorithms we will use. Section 4 proceeds with a description of our experiments and presents results for the case of synthesized piano <b>data</b>. Section 5 then briefly looks at model interpretability, providing some glimpses into the learned representations ...", "dateLastCrawled": "2022-01-28T21:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A Surrogate-Assisted Controller for Expensive Evolutionary ...", "url": "https://deepai.org/publication/a-surrogate-assisted-controller-for-expensive-evolutionary-reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/a-surrogate-assisted-controller-for-expensive...", "snippet": "In DRL, each problem is modeled as a <b>Markov Decision Process</b> (<b>MDP</b>), which <b>can</b> be specified by a 5-tuple S , A , P , r , \u03b3 . With the state space S and the action space A , P : S \u00d7 S \u00d7 A \u2192 [ 0 , 1 ] is the transition function of the environment; r ( s , a ) : S \u00d7 A \u2192 R is the reward function, and the discount factor \u03b3 \u2208 ( 0 , 1 ] specifies the degree to which rewards are discounted over time.", "dateLastCrawled": "2022-02-02T02:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "A Survey on Deep Reinforcement Learning for <b>Data</b> Processing and Analytics", "url": "http://www.vertexdoc.com/doc/a-survey-on-deep-reinforcement-learning-for-data-processing-and-analytics", "isFamilyFriendly": true, "displayUrl": "www.vertexdoc.com/doc/a-survey-on-deep-reinforcement-learning-for-<b>data</b>-<b>process</b>ing-and...", "snippet": "Ling et al. [ ] propose modeling the integration of external evidence to capture diagnostic concept as <b>Markov Decision Process</b> (<b>MDP</b>). The objective is to find the value function mapping from the state to the action. The inputs are case narratives and the <b>outputs</b> are improved concepts and inferred diagnoses. The states are a set of measures over the similarity of current concepts and externally extracted concepts. The actions are whether to accept (part of) the extracted concepts from ...", "dateLastCrawled": "2022-01-19T00:57:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Why does <b>Markov Decision Process</b> matter in Reinforcement <b>Learning</b>? | by ...", "url": "https://towardsdatascience.com/why-does-malkov-decision-process-matter-in-reinforcement-learning-b111b46b41bd", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/why-does-malkov-<b>decision</b>-<b>process</b>-matter-in...", "snippet": "It is named by <b>analogy</b> to \u201cone-armed bandit\u201d(= a slot <b>machine</b>) although the framework has k levers instead of one. ... we introduce <b>Markov Decision Process</b>(<b>MDP</b>) to solve such a problem. An <b>MDP</b> consists of two elements; the agent and the environment. The agent is a learner or <b>decision</b>-maker. In the above example, the agent is the rabbit. The environment is everything surrounding the agent. In the example, the environment includes everything in the field where the rabbit is with food and ...", "dateLastCrawled": "2022-01-31T01:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Markov Decision Process</b>: How Does Value Iteration Work? | Baeldung on ...", "url": "https://www.baeldung.com/cs/mdp-value-iteration", "isFamilyFriendly": true, "displayUrl": "https://www.baeldung.com/cs/<b>mdp</b>-value-iteration", "snippet": "From this point, we can make an <b>analogy</b> with the <b>Markov</b> model since the solution for this problem is a sequence of actions. A <b>Markov Decision Process</b> is used to model the agent, considering that the agent itself generates a series of actions. In the real world, we can have observable, hidden, or partially observed states, depending on the ...", "dateLastCrawled": "2022-01-30T20:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Introduction to <b>Machine</b> <b>Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/30/introduction-to-machine-learning-3/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/30/introduction-to-<b>machine</b>-<b>learning</b>-3", "snippet": "<b>Markov decision process</b> (<b>MDP</b>): In reinforcement <b>learning</b>, <b>MDP</b> is a mathematical framework for modeling <b>decision</b>-making of an agent in situations or environments where outcomes are partly random and partly under control. In this model, environment is modeled as a set of states and actions that can be performed by an agent to control the system\u2019s state. The objective is to control the system in such a way that the agent\u2019s total payoff is maximized.", "dateLastCrawled": "2022-01-28T18:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Introduction to Machine Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/24/introduction-to-machine-learning-2/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/24/<b>introduction-to-machine-learning</b>-2", "snippet": "<b>Markov decision process</b> (<b>MDP</b>): In reinforcement <b>learning</b>, <b>MDP</b> is a mathematical framework for modeling <b>decision</b>-making of an agent in situations or environments where outcomes are partly random and partly under control. In this model, environment is modeled as a set of states and actions that can be performed by an agent to control the system\u2019s state. The objective is to control the system in such a way that the agent\u2019s total payoff is maximized.", "dateLastCrawled": "2022-01-17T05:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Markov decision process</b>: value iteration with code implementation | by ...", "url": "https://medium.com/@ngao7/markov-decision-process-value-iteration-2d161d50a6ff", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@ngao7/<b>markov-decision-process</b>-value-iteration-2d161d50a6ff", "snippet": "<b>Markov decision process</b>, <b>MDP</b>, value iteration, policy iteration, policy evaluation, policy improvement, sweep, iterative policy evaluation, policy, optimal policy ...", "dateLastCrawled": "2022-01-08T01:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "18.1. <b>Markov Decision Process</b> (<b>MDP</b>) \u2014 Dive into Deep <b>Learning</b> 0.17.0 ...", "url": "http://preview.d2l.ai.s3-website-us-west-2.amazonaws.com/d2l-en/master/chapter_reinforcement_learning/mdp.html", "isFamilyFriendly": true, "displayUrl": "preview.d2l.ai.s3-website-us-west-2.amazonaws.com/...reinforcement_<b>learning</b>/<b>mdp</b>.html", "snippet": "In this section, we will discuss how to formulate reinforcement <b>learning</b> problems using <b>Markov</b> <b>decision</b> processes (MDPs) and describe in detail various components of MDPs. Definition of an <b>MDP</b> \u00b6 A <b>Markov decision process</b> (<b>MDP</b>) is a model for how the state of a system evolves as different actions are applied to it.", "dateLastCrawled": "2022-01-27T05:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Markov decision process</b>: policy iteration with code implementation | by ...", "url": "https://medium.com/@ngao7/markov-decision-process-policy-iteration-42d35ee87c82?source=post_internal_links---------0-------------------------------", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@ngao7/<b>markov-decision-process</b>-policy-iteration-42d35ee87c82?source=...", "snippet": "<b>Markov decision process</b>: policy iteration with code implementation . Nan. Dec 19, 2021 \u00b7 16 min read. In today\u2019s story we focus on policy iteration of <b>MDP</b>. We are still using the grid world ...", "dateLastCrawled": "2022-01-22T13:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Meta-<b>Learning</b> as a <b>Markov Decision Process</b>", "url": "https://hal.archives-ouvertes.fr/tel-02422144v2/document", "isFamilyFriendly": true, "displayUrl": "https://hal.archives-ouvertes.fr/tel-02422144v2/document", "snippet": "Meta-<b>Learning</b> as a <b>Markov Decision Process</b>. <b>Machine</b> <b>Learning</b> [cs.LG]. Uni-versit\u00e9 Paris Saclay (COmUE), 2019. English. \uffffNNT: 2019SACLS588\uffff. \ufffftel-02422144v2\uffff I would like to dedicate this thesis to my loving parents doctorat CLS588 Meta-<b>Learning</b> as a <b>Markov Decision Process</b> Th\u00e8se de doctorat de l\u2019Universit\u00e9 Paris-Saclay pr\u00e9par\u00e9e \u00e0 l\u2019Universit\u00e9 Paris-Sud Ecole doctorale n 580 Sciences et Technologies de l\u2019Information et de la Communication (STIC) Sp\u00e9cialit\u00e9 de doctorat ...", "dateLastCrawled": "2022-01-13T07:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>machine</b> <b>learning</b> - Why is the <b>optimal policy</b> in <b>Markov Decision Process</b> ...", "url": "https://stats.stackexchange.com/questions/132890/why-is-the-optimal-policy-in-markov-decision-process-mdp-independent-of-the-i", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/132890", "snippet": "The intuition behind the argument saying that the <b>optimal policy</b> is independent of initial state is the following: The <b>optimal policy</b> is defined by a function that selects an action for every possible state and actions in different states are independent.. Formally speaking, for an unknown initial distribution, the value function to maximize would be the following (not conditioned on initial state)", "dateLastCrawled": "2022-01-25T23:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Real-life <b>examples</b> of <b>Markov</b> <b>Decision</b> Processes - Cross Validated", "url": "https://stats.stackexchange.com/questions/145122/real-life-examples-of-markov-decision-processes", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/145122", "snippet": "A Markovian <b>Decision</b> <b>Process</b> indeed has to do with going from one state to another and is mainly used for planning and <b>decision</b> making. The theory. Just repeating the theory quickly, an <b>MDP</b> is: $$\\text{<b>MDP</b>} = \\langle S,A,T,R,\\gamma \\rangle$$", "dateLastCrawled": "2022-01-24T10:32:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Overview: Representation Techniques", "url": "https://webcms3.cse.unsw.edu.au/static/uploads/course/COMP4418/17s2/9c0e19e0e02df66fffb5d0bd4c20697922f5ffbf9a602b66bec3f74ac83fb77c/DecisionMaking.pdf", "isFamilyFriendly": true, "displayUrl": "https://webcms3.cse.unsw.edu.au/static/uploads/course/COMP4418/17s2/9c0e19e0e02df66...", "snippet": "<b>Markov Decision Process MDP is like</b> a Markov process, except every round we make a decision Transition probabilities depend on actions taken P(St+1 = S&#39; | St = s, At = a) = P(S, a, S&#39;) Rewards for every state, action pair u(St = s, At = a) Discount factor \u03b4 Example. A <b>machine</b> can be in one of three states: good, deteriorating, broken Can take ...", "dateLastCrawled": "2022-01-21T05:20:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], []], "all_bing_queries": ["+(markov decision process (mdp))  is like +(black box that takes in data and outputs a prediction)", "+(markov decision process (mdp)) is similar to +(black box that takes in data and outputs a prediction)", "+(markov decision process (mdp)) can be thought of as +(black box that takes in data and outputs a prediction)", "+(markov decision process (mdp)) can be compared to +(black box that takes in data and outputs a prediction)", "machine learning +(markov decision process (mdp) AND analogy)", "machine learning +(\"markov decision process (mdp) is like\")", "machine learning +(\"markov decision process (mdp) is similar\")", "machine learning +(\"just as markov decision process (mdp)\")", "machine learning +(\"markov decision process (mdp) can be thought of as\")", "machine learning +(\"markov decision process (mdp) can be compared to\")"]}