{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Sparse</b> Distributed Representations", "url": "https://numenta.com/assets/pdf/biological-and-machine-intelligence/BaMI-SDR.pdf", "isFamilyFriendly": true, "displayUrl": "https://numenta.com/assets/pdf/biological-and-machine-intelligence/BaMI-SDR.pdf", "snippet": "What is a <b>Sparse</b> Distributed <b>Representation</b>? One of the most interesting challenges in AI is the problem of knowledge <b>representation</b>. Representing everyday facts and relationships in a form that computers can work with has proven to be difficult with traditional computer science methods. The basic problem is that our knowledge of the world is not divided into discrete facts with well-defined relationships. Almost everything we know has exceptions, and the relationships between concepts are ...", "dateLastCrawled": "2022-02-03T00:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Generating Images with <b>Sparse</b> Representations | DeepMind", "url": "https://deepmind.com/research/publications/2021/Generating-Images-with-Sparse-Representations", "isFamilyFriendly": true, "displayUrl": "https://deepmind.com/.../publications/2021/Generating-Images-with-<b>Sparse</b>-<b>Representations</b>", "snippet": "One of the key challenges in generative modelling of images is the high dimensionality of the data. Previous likelihood-based approaches <b>use</b> neural lossy compression to obtain compact latent codes that are more practical for generative modelling. In this paper we instead take inspiration from ubiquitous image compression methods <b>like</b> JPEG, and represent images using quantized discrete cosine transform (DCT) blocks, that are converted into <b>sparse</b> lists of non-zero components. We propose a ...", "dateLastCrawled": "2022-01-08T21:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Sparse</b> Distributed <b>Representation</b> (SDR)", "url": "https://www.linkedin.com/pulse/20141031184447-140750669-sparse-distributed-representation-sdr", "isFamilyFriendly": true, "displayUrl": "https://www.linkedin.com/pulse/20141031184447-140750669-<b>sparse</b>-distributed...", "snippet": "The way human brain works is by having semantic meaning for each of the bits. For example, these could be meanings of eight bit <b>sparse</b> distributed <b>representation</b> for letters of the <b>alphabet</b>:", "dateLastCrawled": "2021-07-08T20:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) Semantic <b>Place Recognition Based on Unsupervised Deep</b> Learning of ...", "url": "https://www.researchgate.net/publication/340581561_Semantic_Place_Recognition_Based_on_Unsupervised_Deep_Learning_of_Spatial_Sparse_Features", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/340581561_Semantic_Place_Recognition_Based_on...", "snippet": "The <b>sparse</b> <b>representation</b> is assumed to be linearly separable, and therefore a simple classifier, <b>like</b> softmax regression, is suitable to perform the classification process. To investigate that ...", "dateLastCrawled": "2022-01-11T02:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) Human voice\u2014a <b>sparse, meaningful and capable representation of</b> ...", "url": "https://www.academia.edu/3336789/Human_voice_a_sparse_meaningful_and_capable_representation_of_sounds", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/3336789/Human_voice_a_<b>sparse</b>_meaningful_and_capable...", "snippet": "Human Voice \u2013 a <b>Sparse, Meaningful and Capable Representation of</b> Sounds R. Mores HAW University of Applied Sciences Hamburg, Germany, Email: mores@mt.haw-hamburg.de Abstract Technical representations, such as transforms, model parameters, and decompositions do only partially cover This paper encourages <b>the use</b> of human voice features for psychoacoustic reality and do not bridge to human\u2019s per- mid-level representations of sounds.", "dateLastCrawled": "2022-01-24T05:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Canonical representation of polynomials</b> - Everything2.com", "url": "https://everything2.com/title/Canonical+representation+of+polynomials", "isFamilyFriendly": true, "displayUrl": "https://everything2.com/title/<b>Canonical+representation+of+polynomials</b>", "snippet": "This motivates the <b>sparse</b> <b>representation</b> of a polynomial, ... The system is not quite <b>like</b> a dictionary- xx outweighs x so that we can preserve the decreasing order of the powers. But if you consider x as x_ , where _ is a bonus letter added to the very end of your <b>alphabet</b>, then it behaves as expected. Given this alphabetical dominance, we can effectively write a multivariate polynomial as a polynomial purely in the first variable, whose coefficients are themselves polynomials. This gives ...", "dateLastCrawled": "2022-02-03T11:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Years Later, <b>Alphabet</b>\u2019s Everyday Robots Have Made Some Progress - IEEE ...", "url": "https://spectrum.ieee.org/alphabet-robots", "isFamilyFriendly": true, "displayUrl": "https://spectrum.ieee.org/<b>alphabet</b>-robots", "snippet": "For instance, if we begin with a 1.2-meter-aperture radar operating at the K band and put in an appropriately designed <b>sparse</b> array having just 12 transmitting and 16 receiving elements, it would ...", "dateLastCrawled": "2022-01-25T18:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Shortest unambiguous <b>representation</b> of a graph over an <b>alphabet</b>", "url": "https://cs.stackexchange.com/questions/126606/shortest-unambiguous-representation-of-a-graph-over-an-alphabet", "isFamilyFriendly": true, "displayUrl": "https://cs.stackexchange.com/questions/126606/shortest-unambiguous-<b>representation</b>-of-a...", "snippet": "I just started reading a book on theoretical computer science and here are a couple of beginner questions about graphs, which I am struggling to answer: Given the graph with the matrix represent...", "dateLastCrawled": "2022-01-27T06:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Trying to Understand Tries. In every installment of this series\u2026 | by ...", "url": "https://medium.com/basecs/trying-to-understand-tries-3ec6bede0014", "isFamilyFriendly": true, "displayUrl": "https://medium.com/basecs/trying-to-understand-<b>trie</b>s-3ec6bede0014", "snippet": "<b>Trie</b>: a definition. A <b>trie</b> is a tree-<b>like</b> data structure whose nodes store the letters <b>of an alphabet</b>. By structuring the nodes in a particular way, words and strings can be retrieved from the ...", "dateLastCrawled": "2022-01-30T13:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "The Byte Atlas: <b>Upgrading ApiScout: Introducing ApiVectors</b>", "url": "https://byte-atlas.blogspot.com/2018/04/apivectors.html", "isFamilyFriendly": true, "displayUrl": "https://byte-atlas.blogspot.com/2018/04/apivectors.html", "snippet": "Now, one thing that is obvious is that vectors can be very <b>sparse</b> and we can probably condense the <b>representation</b> further. For this we <b>use</b> runlength-encoding, with which we can remove the repetitive consecutive symbols, for which we freed up the 10 numbers from the original base64 <b>alphabet</b> before.", "dateLastCrawled": "2022-01-20T04:38:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Sparse</b> Document Image Coding for Restoration", "url": "https://cvit.iiit.ac.in/images/ConferencePapers/2013/Vijay2013Sparse.pdf", "isFamilyFriendly": true, "displayUrl": "https://cvit.iiit.ac.in/images/ConferencePapers/2013/Vijay2013<b>Sparse</b>.pdf", "snippet": "the <b>use</b> of <b>sparse</b> <b>representation</b> based methods speci\ufb01cally to restore the degraded document images. While natural images form a very small subset of all possible images admitting the possibility of <b>sparse</b> <b>representation</b>, document images are signi\ufb01cantly more restricted and are expected to be ideally suited for such a <b>representation</b>. However, the binary nature of textual document images makes dictionary learning and coding techniques unsuitable to be applied directly. We leverage the fact ...", "dateLastCrawled": "2021-11-06T21:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Sparse</b> Distributed Representations", "url": "https://numenta.com/assets/pdf/biological-and-machine-intelligence/BaMI-SDR.pdf", "isFamilyFriendly": true, "displayUrl": "https://numenta.com/assets/pdf/biological-and-machine-intelligence/BaMI-SDR.pdf", "snippet": "In this chapter we introduce <b>Sparse</b> Distributed Representations (SDRs), the fundamental form of information <b>representation</b> in the brain, and in HTM systems. We talk about several interesting and useful mathematical properties of SDRs and then discuss how SDRs are used in the brain. What is a <b>Sparse</b> Distributed <b>Representation</b>? One of the most interesting challenges in AI is the problem of knowledge <b>representation</b>. Representing everyday facts and relationships in a form that computers can work ...", "dateLastCrawled": "2022-02-03T00:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Visual Object Tracking Using Structured <b>Sparse</b> PCA-Based Appearance ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6209897/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC6209897", "snippet": "On the other hand, <b>sparse</b> <b>representation</b>-based visual object tracking systems like <b>sparse</b> collaborative appearance (SCM) , visual tracking decomposition (VTD) , the <b>sparse</b> <b>representation</b>-based l 1 tracker , the structured <b>sparse</b> tracking (SST) model and <b>sparse</b> mask models [33,34] <b>use</b> an appearance model to find the sparsest linear combination of basis functions from an over-complete dictionary. However, most dictionary learning-based systems still have problems in high-dimensional reduction ...", "dateLastCrawled": "2021-07-02T16:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "SUBMITTED TO IEEE DESIGN &amp; TEST OF COMPUTERS, SPECIAL ISSUE ON SELF ...", "url": "https://ieeexplore.ieee.org/ielaam/6221038/8107686/8012434-aam.pdf", "isFamilyFriendly": true, "displayUrl": "https://ieeexplore.ieee.org/ielaam/6221038/8107686/8012434-aam.pdf", "snippet": "letter in the <b>alphabet</b>. In <b>sparse</b> <b>representation</b>, the hypervectors are identi\ufb01ed by the place of 1\u2019s rather than their holographic distribution. Therefore, the de\ufb01nition of orthogonality is different from that in dense <b>representation</b>, as in the <b>sparse</b> <b>representation</b> the orthogonal hypervectors need to have a dot product close to zero.", "dateLastCrawled": "2022-01-05T20:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "eneral Interest Low-Power <b>Sparse</b> Hyperdimensional Encoder for Language ...", "url": "http://varys.ucsd.edu/media/papers/imani2017low.pdf", "isFamilyFriendly": true, "displayUrl": "varys.ucsd.edu/media/papers/imani2017low.pdf", "snippet": "the <b>alphabet</b>. In <b>sparse</b> <b>representation</b>, the hypervec-tors are identified by the place of 1\u2019s rather than their holographic distribution. Therefore, the definition of orthogonality is different from that in dense representa - tion, as in the <b>sparse</b> <b>representation</b> the orthogonal hypervectors need to have a dot product close to zero.", "dateLastCrawled": "2021-08-12T01:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A competitive scheme for storing <b>sparse</b> <b>representation</b> of X-Ray medical ...", "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0201455", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0201455", "snippet": "A competitive scheme for economic storage of the informational content of an X-Ray image, as it can be used for further processing, is presented. It is demonstrated that <b>sparse</b> <b>representation</b> of that type of data can be encapsulated in a small file without affecting the quality of the recovered image. The proposed <b>representation</b>, which is inscribed within the context of data reduction, provides a format for saving the image information in a way that could assist methodologies for analysis ...", "dateLastCrawled": "2021-09-15T23:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Feature assessment and ranking for classification with nonlinear <b>sparse</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0167923619300806", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0167923619300806", "snippet": "Recently, <b>sparse</b> <b>representation</b> has been playing increasingly important roles in feature selection and <b>sparse</b> reconstruction [21, 38, 41]. In general, <b>sparse</b> <b>representation</b> can be formalized as the determination of an optimal solution of a minimization programming wherein the objective function is the sum of a data-fitting term in \u2113 2 -norm and a regularization term in \u2113 0 -norm.", "dateLastCrawled": "2021-10-30T21:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Towards Arabic <b>Alphabet</b> and Numbers Sign Language Recognition", "url": "https://globaljournals.org/GJCST_Volume17/3-Towards-Arabic-Alphabet.pdf", "isFamilyFriendly": true, "displayUrl": "https://globaljournals.org/GJCST_Volume17/3-Towards-Arabic-<b>Alphabet</b>.pdf", "snippet": "Towards Arabic <b>Alphabet</b> and Numbers Sign Language Recognition ... Networks) leads to an efficient <b>sparse</b> <b>representation</b> of the initial data in the feature space. A complex problem of classification in the input space is thus transformed into an easier one in the feature space. After appropriate coding, a softmax regression in the feature space must be sufficient to recognize a hand sign according to the input image. To our knowledge, this is the first attempt that tiny images feature ...", "dateLastCrawled": "2022-01-10T10:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Shift Invariant <b>Sparse</b> Coding of Image and Music Data", "url": "http://www2.imm.dtu.dk/pubdb/edoc/imm4659.pdf", "isFamilyFriendly": true, "displayUrl": "www2.imm.dtu.dk/pubdb/edoc/imm4659.pdf", "snippet": "The features in the <b>sparse</b> over-complete <b>representation</b> are complex structures that form an \u201cacoustic icon <b>alphabet</b>\u201d. Furthermore, infants can distinguish melodies regardless of pitch [32], and since a change of pitch relates to a shift on a logarithmic frequency axis, shift invariance appears a natural constraint for audio signals modelling. Thus, we \ufb01nd ample motivation for <b>sparse</b> coding with shift invariance as a starting point for analysis of image and audio signals. We present our ...", "dateLastCrawled": "2022-01-11T22:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Word Embeddings: Intuition behind the <b>vector</b> <b>representation</b> of the ...", "url": "https://towardsdatascience.com/word-embeddings-intuition-behind-the-vector-representation-of-the-words-7e4eb2410bba", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/word-embeddings-intuition-behind-the-<b>vector</b>...", "snippet": "Photo by Sincerely Media on Unsplash. In this article I would like to talk about how the words are commonly represented in Natural Language Processing (NLP), and what are the drawbacks of the \u201cclassical\u201d word-<b>vector</b> <b>representation</b>, which word embeddings alleviate.In the practical section of the chapter, I am going to train a simple embedding on a character level (using a <b>similar</b> approach, one can <b>use</b> to train embeddings on a word level as well).", "dateLastCrawled": "2022-02-03T11:22:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Gesture recognition based on an improved local <b>sparse</b> <b>representation</b> ...", "url": "https://link.springer.com/article/10.1007/s10586-017-1237-1", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10586-017-1237-1", "snippet": "<b>Thought</b> of local <b>sparse</b> <b>representation</b> of <b>thought</b>. For a typical gesture recognition scene, a training sample data set containing a class C sample is given to identify a test image \\(y\\in \\mathfrak {R}^{m}\\).Suppose that the class \\(j\\in [1,\\ldots ,C]\\) contains \\(n_j \\) gesture samples, which is a column vector matrix \\(A_j =[a_j, \\ldots , a_{n_j } ]\\), which contains the matrix \\(A=[A_1, \\ldots , A_C]\\in \\mathfrak {R}^{m\\times n}\\) of the gesture samples of all classes, where m is the ...", "dateLastCrawled": "2022-01-04T07:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Sparse</b> Distributed <b>Representation</b> (SDR)", "url": "https://www.linkedin.com/pulse/20141031184447-140750669-sparse-distributed-representation-sdr", "isFamilyFriendly": true, "displayUrl": "https://www.linkedin.com/pulse/20141031184447-140750669-<b>sparse</b>-distributed...", "snippet": "The way human brain works is by having semantic meaning for each of the bits. For example, these could be meanings of eight bit <b>sparse</b> distributed <b>representation</b> for letters of the <b>alphabet</b>:", "dateLastCrawled": "2021-07-08T20:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Spatially-sparse convolutional neural networks</b> | DeepAI", "url": "https://deepai.org/publication/spatially-sparse-convolutional-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>spatially-sparse-convolutional-neural-networks</b>", "snippet": "The first <b>representation</b> records more accurately where the pen went, while the second is better at recording the direction the pen was taking. Using sparsity, we <b>can</b> try to get the best of both worlds. Combining the two representations gives an array of size (1 + 8) \u00d7 n \u00d7 n.Setting n = 64 gives a <b>sparse</b> <b>representation</b> of the character suitable for feeding into a CNN. This preserves sparsity as the histogram is all zero at sites the pen does not touch.", "dateLastCrawled": "2022-01-20T22:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "\u201cAn Efficient <b>Representation</b> for <b>Sparse</b> Sets\u201d | MetaFilter", "url": "https://www.metafilter.com/69932/An-Efficient-Representation-for-Sparse-Sets", "isFamilyFriendly": true, "displayUrl": "https://www.metafilter.com/69932", "snippet": "I <b>thought</b> that what the point of this scheme. What you want to store is used as an index into <b>sparse</b>. No, it&#39;s more general than that--the interface is the same as any array. The example given uses the same values as both the index and the stored value, but that&#39;s not required. <b>Sparse</b> could be indexed by employee number, for example, and each ...", "dateLastCrawled": "2022-01-26T12:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Google AI Blog", "url": "https://ai.googleblog.com/", "isFamilyFriendly": true, "displayUrl": "https://ai.googleblog.com", "snippet": "Detecting previously unseen conditions <b>can</b> <b>be thought</b> of as an out-of-distribution (OOD) detection task. By successfully identifying OOD samples, preventive measures <b>can</b> be taken, like abstaining from prediction or deferring to a human expert. Traditional computer vision OOD detection benchmarks work to detect dataset distribution shifts. For example, a model may be trained on CIFAR images but be presented with street view house numbers (SVHN) as OOD samples, two datasets with very different ...", "dateLastCrawled": "2022-02-02T16:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A Pattern Recognition Model based on Invariant Representations of Space ...", "url": "http://worldcomp-proceedings.com/proc/p2014/ICA2740.pdf", "isFamilyFriendly": true, "displayUrl": "worldcomp-proceedings.com/proc/p2014/ICA2740.pdf", "snippet": "functional equivalence in aspects such as <b>sparse</b> coding, hierarchical architecture and prediction. 2 Theoretical Background The relevant concepts to formulate the model are introduced in this section. 2.1 Local, dense and <b>sparse</b> codes A neural system like the brain, with a set of N binary neurons, could <b>use</b> one of the following coding schemes to represent information [9]. A local code represents items using separate neurons. This scheme is simple and easy to decode. However, it has a very ...", "dateLastCrawled": "2021-11-22T13:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "The Structure of Analog <b>Representation</b> - Lee - - No&amp;#251;s - Wiley ...", "url": "https://onlinelibrary.wiley.com/doi/10.1111/nous.12404?af=R", "isFamilyFriendly": true, "displayUrl": "https://onlinelibrary.wiley.com/doi/10.1111/nous.12404?af=R", "snippet": "Mirroring may thus <b>be thought</b> of as the gatekeeper to the realm of the analog: without at least some mirroring, a system has no purity or mass, because it has no mirrored subsystem. 5.1 Analog Purity. Consider the following system: _____ City Populations: Vehicles: 10 letters of the <b>alphabet</b>, each coming in three different sizes\u2014big, medium ...", "dateLastCrawled": "2022-02-01T11:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) <b>On Finite Alphabet Compressive Sensing</b>", "url": "https://www.researchgate.net/publication/261347518_On_Finite_Alphabet_Compressive_Sensing", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/261347518_<b>On_Finite_Alphabet_Compressive_Sensing</b>", "snippet": "signal, with a b-<b>sparse</b> <b>representation</b> in some basis, <b>can</b> be. captured using m = O (b log(n/b)) measurements based on. linear combinations of the signal values (b, n, m \u2208 N, b &lt; n). As such ...", "dateLastCrawled": "2022-02-02T05:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Two chapters in Finite Frames: Theory and Applications \u2013 Short, Fat ...", "url": "https://dustingmixon.wordpress.com/2013/06/27/two-chapters-in-finite-frames-theory-and-applications/", "isFamilyFriendly": true, "displayUrl": "https://dustingmixon.wordpress.com/2013/06/27/two-chapters-in-finite-frames-theory-and...", "snippet": "Section 9.3 is dedicated to such typical guarantees, which <b>use</b> the other matrix parameters above to achieve performance for most -<b>sparse</b> vectors with . First, Theorems 9.10 and 9.11 give that and minimization recover most -<b>sparse</b> vectors when , provided and are small.", "dateLastCrawled": "2022-01-07T15:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Why does word2vec <b>use</b> 2 representations for each word? - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/29381505/why-does-word2vec-use-2-representations-for-each-word", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/29381505", "snippet": "Theoreticaly you could <b>use</b> as contexts bigrams, trying to maximize for instance the probability of (word=&quot;for&quot;, context=&quot;to maximize&quot;), and you would assign a vector <b>representation</b> to &quot;to maximize&quot;. We don&#39;t do this because there would be too many representations to compute, and we would have a reeeeeally <b>sparse</b> matrix, but I think the idea is here : the fact that we <b>use</b> &quot;1-grams&quot; as context is just a particular case of all the kinds of context we could <b>use</b>.", "dateLastCrawled": "2022-01-22T01:47:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Gesture recognition based on an improved local <b>sparse</b> <b>representation</b> ...", "url": "https://link.springer.com/article/10.1007/s10586-017-1237-1", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10586-017-1237-1", "snippet": "ASL finger spelling <b>alphabet</b>. Full size image. Table 1 Part of the ASL gesture sample . Full size table. Effect of parameter K on algorithm recognition rate. The \\(l_{1}\\)-LSRC and KNN-SRC algorithms <b>use</b> the local <b>sparse</b> <b>representation</b> to select the K training samples most relevant to the test samples for classification recognition, where \\(K&lt;n\\), which excludes unrelated training samples, reduces the computational complexity while maintaining the robustness of the SRC algorithm. In this ...", "dateLastCrawled": "2022-01-04T07:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A competitive scheme for storing <b>sparse</b> <b>representation</b> of X-Ray medical ...", "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0201455", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0201455", "snippet": "A competitive scheme for economic storage of the informational content of an X-Ray image, as it <b>can</b> be used for further processing, is presented. It is demonstrated that <b>sparse</b> <b>representation</b> of that type of data <b>can</b> be encapsulated in a small file without affecting the quality of the recovered image. The proposed <b>representation</b>, which is inscribed within the context of data reduction, provides a format for saving the image information in a way that could assist methodologies for analysis ...", "dateLastCrawled": "2021-09-15T23:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Feature assessment and ranking for classification with nonlinear <b>sparse</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0167923619300806", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0167923619300806", "snippet": "The experimental results show the superiority of the proposed method <b>compared</b> with the representative information theoretic and model-based methods in classification for data-driven decision support systems. Previous article in issue; Next article in issue; Keywords. Feature selection . Dimensionality reduction. Classification. <b>Sparse</b> <b>representation</b>. Dependence analysis. 1. Introduction. Selecting salient features that preserve or promote the performance of data mining and decision-making is ...", "dateLastCrawled": "2021-10-30T21:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Is <b>Sparse</b> Attention more Interpretable? | DeepAI", "url": "https://deepai.org/publication/is-sparse-attention-more-interpretable", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/is-<b>sparse</b>-attention-more-interpretable", "snippet": "Our prior results demonstrated that\u2014even when using <b>sparse</b> attention\u2014we cannot identify a subset of influential inputs directly through intermediate representations; we explore whether a subset <b>can</b> still be identified through FI metrics. In the case where the normalized FI distribution highlights only a few key items, the distribution will, by definition, have low entropy. Thus, we explore whether <b>sparse</b> attention leads to lower entropy input FI distributions in comparison to standard ...", "dateLastCrawled": "2022-01-11T15:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Object Tracking with Sparse Representation based</b> on HOG and LBP Features", "url": "http://www.kpubs.org/article/articleMain.kpubs?articleANo=E1CTBR_2015_v11n3_47", "isFamilyFriendly": true, "displayUrl": "www.kpubs.org/article/articleMain.kpubs?articleANo=E1CTBR_2015_v11n3_47", "snippet": "Received : June 22, 2015 Accepted : August 13, 2015 Download. Article", "dateLastCrawled": "2021-12-18T17:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Sparse</b> Linear <b>Representation</b> - ResearchGate", "url": "https://www.researchgate.net/publication/224578477_Sparse_Linear_Representation", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/224578477_<b>Sparse</b>_Linear_<b>Representation</b>", "snippet": "The noisy <b>sparse</b> <b>representation</b> problem is to find <b>sparse</b> representations of a signal r satisfying a distortion criterion. In this case, we establish a lower bound on the tradeoff between the ...", "dateLastCrawled": "2021-12-18T05:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "4349-Article Text-7396-1-10-20190706.pdf - The Thirty-Third AAAI ...", "url": "https://www.coursehero.com/file/108293112/4349-Article-Text-7396-1-10-20190706pdf/", "isFamilyFriendly": true, "displayUrl": "https://www.coursehero.com/file/108293112/4349-Article-Text-7396-1-10-20190706pdf", "snippet": "In this work, we first highlight that learned <b>sparse</b> represen-tations <b>can</b> significantly improve control performance, under an incremental learning setting, <b>compared</b> to dense neural networks. We visualize the activation of the hidden nodes for the <b>sparse</b> <b>representation</b> as well as the action-values for particular states.", "dateLastCrawled": "2021-12-23T02:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) <b>Sparse</b> Gaussian Processes on Discrete Domains", "url": "https://www.researchgate.net/publication/351792665_Sparse_Gaussian_Processes_on_Discrete_Domains", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/351792665_<b>Sparse</b>_Gaussian_Processes_on...", "snippet": "The input domain is discrete, in this case the set of strings over some <b>alphabet</b> A. Inducing points for the <b>sparse</b> Gaussian Process are chosen from the data points, but also from the rest of the ...", "dateLastCrawled": "2022-01-31T11:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Graph Representation: Adjacency Matrix and Adjacency List</b>", "url": "https://iq.opengenus.org/graph-representation-adjacency-matrix-adjacency-list/", "isFamilyFriendly": true, "displayUrl": "https://iq.opengenus.org/<b>graph-representation-adjacency-matrix</b>-adjacency-list", "snippet": "It is recommended that we should <b>use</b> Adjacency Matrix for representing Dense Graphs and Adjacency List for representing <b>Sparse</b> Graphs. Note: Dense Graph are those which has large number of edges and <b>sparse</b> graphs are those which has small number of edges. Application . Adjacency Matrix: Adjacency matrix is used where information about each and every possible edge is required for the proper working of an algorithm like :- Floyd-Warshall Algorithm where shortest path from each vertex to each ...", "dateLastCrawled": "2022-02-02T19:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(PDF) <b>C questions and answer</b> | Mohan Singh - Academia.edu", "url": "https://www.academia.edu/7065053/C_questions_and_answer", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/7065053/<b>C_questions_and_answer</b>", "snippet": "Q.36 The maximum number of dimensions an array <b>can</b> have in C is (A) 3 (B) 4 (C) 5 (D) compiler dependent Ans: D C allows arrays of three or more dimensions. The exact limit is determined by the compiler. Q.37 puts (argv [0]); (A) prints the name of the source code file. (B) prints argv.", "dateLastCrawled": "2022-02-03T01:04:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine Learning by Analogy</b> - SlideShare", "url": "https://www.slideshare.net/ColleenFarrelly/machine-learning-by-analogy-59094152", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ColleenFarrelly/<b>machine-learning-by-analogy</b>-59094152", "snippet": "<b>Machine Learning by Analogy</b> 1. Colleen M. Farrelly 2. Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics behind these ...", "dateLastCrawled": "2022-01-31T07:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "TensorFlow is an interface for expressing <b>machine</b> <b>learning</b> algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PPT) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> | Colleen Farrelly - Academia.edu", "url": "https://www.academia.edu/30404581/Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/30404581/<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "<b>Machine</b> <b>Learning</b> by <b>Analogy</b> Colleen M. Farrelly Overview of Problem Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics ...", "dateLastCrawled": "2022-01-02T06:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Machine</b> <b>Learning</b> by <b>Analogy</b> II - slideshare.net", "url": "https://www.slideshare.net/ColleenFarrelly/machine-learning-by-analogy-ii", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ColleenFarrelly/<b>machine</b>-<b>learning</b>-by-<b>analogy</b>-ii", "snippet": "Updated <b>Machine</b> <b>Learning</b> by <b>Analogy</b> presentation that builds to more advanced methods (TensorFlow, geometry/topology-based methods...) and adds a section on ti\u2026", "dateLastCrawled": "2021-12-31T12:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Neural Networks: Analogies. When our brains form analogies, they\u2026 | by ...", "url": "https://towardsdatascience.com/neural-networks-analogies-7ebeb3ac5d5e", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/neural-networks-analogies-7ebeb3ac5d5e", "snippet": "I\u2019ll outline a potential route to artificial neural networks which exhibit transfer <b>learning</b>: First, <b>Sparse</b> Distributed Representations. Numenta\u2019s Hierarchical Te m poral Memory, along with other techniques, relies upon a <b>sparse</b> distributed <b>representation</b>. An example of this is a very long string of ones and zeroes, where almost all the values are zero \u2014 there is a <b>sparse</b> distribution of the ones. If each digit represented a different thing, like \u2018pointy ears\u2019, \u2018tail ...", "dateLastCrawled": "2022-01-28T03:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "On <b>Machine</b> <b>Learning</b> \u2014 Data, ML &amp; Leadership", "url": "https://bugra.github.io/posts/2014/8/23/on-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://bugra.github.io/posts/2014/8/23/on-<b>machine</b>-<b>learning</b>", "snippet": "<b>Sparse</b> Colorful Filters. Recently, I wrote how we do classification at CB Insights.The post outlines some of the things that I have been thinking about how to apply <b>machine</b> <b>learning</b> for a given problem along with the process that we adopted for the classification problem at CB Insights, but also gave me a good opportunity to reflect even further about the <b>machine</b> <b>learning</b> process; shortcomings of papers, books and even traditional education system when it comes to teach the <b>machine</b> <b>learning</b>.", "dateLastCrawled": "2021-12-10T17:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Compressed Sensing Meets <b>Machine</b> <b>Learning</b>: Classification via <b>Sparse</b> ...", "url": "https://nuit-blanche.blogspot.com/2008/05/cs-mini-course-classification-via.html", "isFamilyFriendly": true, "displayUrl": "https://nuit-blanche.blogspot.com/2008/05/cs-mini-course-classification-via.html", "snippet": "Compressed Sensing Meets <b>Machine</b> <b>Learning</b>: Classification via <b>Sparse</b> <b>Representation</b> and Distributed Pattern Recognition This Spring, Allen Yang has given a mini course at Berkeley entitled Compressed Sensing Meets <b>Machine</b> <b>Learning</b>. The three lectures are listed here (it includes accompanying code): lecture 1: Classification via <b>Sparse</b> <b>Representation</b>; lecture 2: Classification of Mixture Subspace Models via <b>Sparse</b> <b>Representation</b>, lecture 3: Distributed Pattern Recognition; The third lecture ...", "dateLastCrawled": "2022-01-25T10:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Accelerating Innovation Through <b>Analogy</b> Mining", "url": "http://hyadatalab.com/papers/analogy-kdd17.pdf", "isFamilyFriendly": true, "displayUrl": "hyadatalab.com/papers/<b>analogy</b>-kdd17.pdf", "snippet": "<b>machine</b> <b>learning</b> models that develop similarity metrics suited for <b>analogy</b> mining. We demonstrate that <b>learning</b> purpose and mechanism representations allows us to \u2022nd analogies with higher precision and recall than traditional information-retrieval methods based on TF-IDF, LSA, LDA and GlOVe, in challenging noisy set-tings. Furthermore, we ...", "dateLastCrawled": "2022-01-29T02:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Regio-<b>selectivity</b> prediction with a <b>machine</b>-learned reaction ...", "url": "https://pubs.rsc.org/en/content/articlelanding/2021/sc/d0sc04823b#!", "isFamilyFriendly": true, "displayUrl": "https://pubs.rsc.org/en/content/articlelanding/2021/sc/d0sc04823b#!", "snippet": "A thorough benchmarking shows that <b>machine</b> learned <b>representation</b> and chemically meaningful descriptors complement each other in the fusion model, enhancing performance, and allow <b>learning</b> from a tiny experimental dataset. Second, we implement a multi-task neural network that is trained on DFT calculations of 136k organic molecules to enable on-the-fly calculations for six key atomic/bond descriptors. Finally, we demonstrate the fusion model using on-the-fly descriptors on three general ...", "dateLastCrawled": "2022-02-01T01:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>What Are Word Embeddings</b> for Text? - <b>Machine</b> <b>Learning</b> Mastery", "url": "https://machinelearningmastery.com/what-are-word-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>what-are-word-embeddings</b>", "snippet": "Word embeddings are a type of word <b>representation</b> that allows words with similar meaning to have a similar <b>representation</b>. They are a distributed <b>representation</b> for text that is perhaps one of the key breakthroughs for the impressive performance of deep <b>learning</b> methods on challenging natural language processing problems. In this post, you will discover the word embedding approach for representing text data. After", "dateLastCrawled": "2022-01-30T18:50:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Sparse Adaptive Local Machine Learning</b> Algorithms for Sensing and Analytics", "url": "https://pdxscholar.library.pdx.edu/cgi/viewcontent.cgi?article=1000&context=mcecs_mentoring", "isFamilyFriendly": true, "displayUrl": "https://pdxscholar.library.pdx.edu/cgi/viewcontent.cgi?article=1000&amp;context=mcecs...", "snippet": "Fig. 2: A <b>sparse representation can be thought of as</b> the dot product of a dictionary vector and a sparse code vector. Given a . dictionary . of general components, we can use a . sparse code. to select as few of them as possible to reconstruct an image of interest (Fig. 2). This reconstruction is called a . sparse representation. Sparse Coding. Image processing is expensive. Instead of working with the original image, we can identify its most relevant components and discard the rest. This ...", "dateLastCrawled": "2021-08-31T12:20:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(sparse representation)  is like +(the use of an alphabet)", "+(sparse representation) is similar to +(the use of an alphabet)", "+(sparse representation) can be thought of as +(the use of an alphabet)", "+(sparse representation) can be compared to +(the use of an alphabet)", "machine learning +(sparse representation AND analogy)", "machine learning +(\"sparse representation is like\")", "machine learning +(\"sparse representation is similar\")", "machine learning +(\"just as sparse representation\")", "machine learning +(\"sparse representation can be thought of as\")", "machine learning +(\"sparse representation can be compared to\")"]}