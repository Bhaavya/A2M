{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Interrater</b> <b>Agreement</b> Measures: Comments on Kappa n , Cohen&#39;s Kappa ...", "url": "https://www.researchgate.net/publication/232604628_Interrater_Agreement_Measures_Comments_on_Kappa_n_Cohen's_Kappa_Scott's_p_and_Aickin's_a", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/232604628_<b>Interrater</b>_<b>Agreement</b>_Measures...", "snippet": "Huddleston (2003) and Rashed (2010) There are <b>two</b> measures commonly used in <b>inter-rater</b> reliability, namely Cohen-Kappa (K) and percentage of <b>agreement</b> (%) (Gwet, 2002; Hsu and Field, 2003). Their ...", "dateLastCrawled": "2022-01-28T03:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "David <b>Bassiuni asking them to maintain the momentum of dialogue</b> and ...", "url": "https://chicagorazom.com/david-bassiuni-asking-them-to-maintain-the-momentum-of-dialogue-and-agreement/", "isFamilyFriendly": true, "displayUrl": "https://chicagorazom.com/david-<b>bassiuni-asking-them-to-maintain</b>-the-momentum-of...", "snippet": "In statistics, <b>inter-rater</b> reliability (also called by various similar names, such as <b>inter-rater</b> <b>agreement</b>, <b>inter-rater</b> concordance, inter-observer reliability, and so on) is the degree of <b>agreement</b> among raters. It is a score of how much homogeneity or consensus exists in the ratings given by various judges. When comparing <b>two</b> methods of measurement, it is not only of interest to estimate both bias and limits of <b>agreement</b> between the <b>two</b> methods (<b>inter-rater</b> <b>agreement</b>), but also to assess ...", "dateLastCrawled": "2022-01-18T16:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "The noun <b>agreement</b> can be countable or uncountable. - Indian Thoughts", "url": "https://indianthoughts.in/the-noun-agreement-can-be-countable-or-uncountable/", "isFamilyFriendly": true, "displayUrl": "https://indianthoughts.in/the-noun-<b>agreement</b>-can-be-countable-or-uncountable", "snippet": "A letter of <b>agreement</b> is an <b>agreement</b> between <b>two</b> parties that puts the terms of the <b>agreement</b> in writing as a means of resolving later disputes that may arise.3 min read <b>AGREEMENT</b>, contract. The consent of <b>two</b> or more persons concurring, respecting the transmission of some property, right or benefit, with a view of contracting an obligation. Bac. Ab. h.t.; Com. Dig. h.t.; Vin. Ab. h.t.; Plowd. 17; 1 Com. Contr. 2; 5 East\u2019s R. 16. It will be proper to consider, 1, the requisites of an ...", "dateLastCrawled": "2022-01-28T08:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Intercoder <b>Reliability</b> in Qualitative Research: Debates and Practical ...", "url": "https://journals.sagepub.com/doi/10.1177/1609406919899220", "isFamilyFriendly": true, "displayUrl": "https://journals.sagepub.com/doi/10.1177/1609406919899220", "snippet": "ICR is a numerical measure of the <b>agreement</b> between different coders regarding how the same data should be coded. ICR is sometimes conflated with <b>interrater</b> <b>reliability</b> (IRR), and the <b>two</b> terms are often used interchangeably. However, technically IRR refers to cases where data are rated on some ordinal or interval scale (e.g., the intensity of an emotion), whereas ICR is appropriate when categorizing data at a nominal level (e.g., the presence or absence of an emotion).", "dateLastCrawled": "2022-02-02T14:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) When Human Coders (and Machines) Disagree on the Meaning of ...", "url": "https://www.academia.edu/4149622/When_Human_Coders_and_Machines_Disagree_on_the_Meaning_of_Facial_Affect_in_Spontaneous_Videos", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/4149622/When_Human_Coders_and_Machines_Disagree_on_the...", "snippet": "<b>Inter- rater</b> reliability was measured using Kohen\u2019s kappa between self vs. peer (Coder 1 vs. Coder 2), self vs. judge1 (Coder 1 vs. Coder 3), self vs. judge2 (Coder 1 vs. Coder 4), peer vs. judge1 (Coder 2 vs. Coder 3), peer vs. judge2 (Coder 2 vs. Coder 4), judge1 vs. judge2 (Coder 3 vs. Coder 4). Among all these pairs, judge1 and judge2 had the highest <b>agreement</b> (kappa =0.71). We took the subset of videos where these <b>two</b> judges perfectly agreed and used these videos with the judges ...", "dateLastCrawled": "2021-12-17T00:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Memory underpinnings of future intentions: Would you <b>like</b> to see the ...", "url": "https://europepmc.org/article/PMC/PMC5407789", "isFamilyFriendly": true, "displayUrl": "https://europepmc.org/article/PMC/PMC5407789", "snippet": "Scenes were evaluated by <b>two</b> independent judges with good <b>inter-rater</b> <b>agreement</b> (Cohen\u2019s kappa = .71, computed on a random selection\u201410%\u2014of retrieved scenes). Disagreements were reconciled through the raters\u2019 joint discussion. The analysis showed that over the 95% of the retrieved scenes were valid episodic recollections. This shows that episodic information was highly accessible and accurate after one week from the experience. The episodic-derived score was computed on the ...", "dateLastCrawled": "2022-01-04T05:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Agreement between two machine learning models</b> - <b>Cross Validated</b>", "url": "https://stats.stackexchange.com/questions/315313/agreement-between-two-machine-learning-models", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/315313", "snippet": "So my plan is to compare the outcome of <b>two</b> models: One that uses a biased sample and another one that uses an unbiased sample (both using the same sample size n). I would <b>like</b> to compare both the outcomes of the model, but what I&#39;m also very interested in is to to say something about the level of <b>agreement</b> of both models. For this last part I&#39;m having difficulty finding more information. Both models will output a probability on the same test set, so I&#39;m looking for a method to quantify ...", "dateLastCrawled": "2022-01-25T12:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 7, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>DSM-5</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/DSM-5", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>DSM-5</b>", "snippet": "The Diagnostic and Statistical Manual of Mental Disorders, Fifth Edition (<b>DSM-5</b>), is the 2013 update to the Diagnostic and Statistical Manual of Mental Disorders, the taxonomic and diagnostic tool published by the American Psychiatric Association (APA). In the United States, the DSM serves as the principal authority for psychiatric diagnoses. Treatment recommendations, as well as payment by health care providers, are often determined by DSM classifications, so the appearance of a new version ...", "dateLastCrawled": "2022-02-02T22:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>The last agreement expired on June 30</b>, 2014. - Indian Thoughts", "url": "https://indianthoughts.in/the-last-agreement-expired-on-june-30-2014/", "isFamilyFriendly": true, "displayUrl": "https://indianthoughts.in/<b>the-last-agreement-expired-on-june-30</b>-2014", "snippet": "Even when making significant purchases <b>like</b> heating and air conditioning systems, consumers are apt to forget the name of the installing company within <b>two</b> years of the purchase unless theres a service <b>agreement</b> in place. A service <b>agreement</b> means there is an ongoing relationship. Warranty. If the manufacturer\u2019s warranty is still in effect on the used car, you may have to pay a fee to get coverage, making it a service contract. However, if the dealer absorbs the cost of the manufacturer ...", "dateLastCrawled": "2021-12-25T01:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Self-fulfilling prophecy, Social Evolution, Chapter</b> 10, Chapter 9 ...", "url": "https://quizlet.com/210247383/self-fulfilling-prophecy-social-evolution-chapter-10-chapter-9-chapter-5-chapter-3-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/210247383/<b>self-fulfilling-prophecy-social-evolution-chapter</b>-10...", "snippet": "-Lonely <b>people</b> attributed more human-<b>like</b> mental states (consciousness, free will, emotions) to gadgets -Inducing loneliness made subjects&#39; pets appear more human <b>like</b> - increased belief in supernatural agents (e.g., ghosts, God)-<b>People</b> are motivated to maintain social connection with others, and those who lack social connection with other humans may try to compensate by creating a sense of human connection with nonhuman agents-Physical Attractiveness. Berscheid &amp; Walster (1978) - High level ...", "dateLastCrawled": "2018-11-09T14:45:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "An Introduction to Inter-Annotator <b>Agreement</b> and Cohen&#39;s Kappa Statistic", "url": "http://blog.echen.me/2021/12/23/an-introduction-to-inter-annotator-agreement-and-cohens-kappa-statistic/", "isFamilyFriendly": true, "displayUrl": "blog.echen.me/2021/12/23/an-introduction-to-inter-annotator-<b>agreement</b>-and-cohens-kappa...", "snippet": "The problem with simple measures of <b>inter-rater</b> reliability, like the percentage of samples both raters label identically, is that they don\u2019t account for the likelihood that <b>two</b> <b>people</b> would agree by random chance. To understand how this works, let\u2019s consider the confusion matrix for the 100 essays Alix and Bob graded in their first week:", "dateLastCrawled": "2022-01-23T02:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Interrater</b> <b>Agreement</b> Measures: Comments on Kappa n , Cohen&#39;s Kappa ...", "url": "https://www.researchgate.net/publication/232604628_Interrater_Agreement_Measures_Comments_on_Kappa_n_Cohen's_Kappa_Scott's_p_and_Aickin's_a", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/232604628_<b>Interrater</b>_<b>Agreement</b>_Measures...", "snippet": "Huddleston (2003) and Rashed (2010) There are <b>two</b> measures commonly used in <b>inter-rater</b> reliability, namely Cohen-Kappa (K) and percentage of <b>agreement</b> (%) (Gwet, 2002; Hsu and Field, 2003). Their ...", "dateLastCrawled": "2022-01-28T03:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "David <b>Bassiuni asking them to maintain the momentum of dialogue</b> and ...", "url": "https://chicagorazom.com/david-bassiuni-asking-them-to-maintain-the-momentum-of-dialogue-and-agreement/", "isFamilyFriendly": true, "displayUrl": "https://chicagorazom.com/david-<b>bassiuni-asking-them-to-maintain</b>-the-momentum-of...", "snippet": "In statistics, <b>inter-rater</b> reliability (also called by various <b>similar</b> names, such as <b>inter-rater</b> <b>agreement</b>, <b>inter-rater</b> concordance, inter-observer reliability, and so on) is the degree of <b>agreement</b> among raters. It is a score of how much homogeneity or consensus exists in the ratings given by various judges. When comparing <b>two</b> methods of measurement, it is not only of interest to estimate both bias and limits of <b>agreement</b> between the <b>two</b> methods (<b>inter-rater</b> <b>agreement</b>), but also to assess ...", "dateLastCrawled": "2022-01-18T16:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Rating of personality disorder features in popular <b>movie</b> characters ...", "url": "https://europepmc.org/abstract/MED/16336663", "isFamilyFriendly": true, "displayUrl": "https://europepmc.org/abstract/MED/16336663", "snippet": "The ten rating scales representing personality disorders have previously been used in <b>two</b> studies, one of <b>inter-rater</b> <b>agreement</b> of personality disorders , and one of convergent validity of personality disorders . The rating scales range from 0 to 100, with scores from 0\u201329 representing the absence or very mild forms of the personality disorder, scores from 30\u201369 representing a moderate degree of the disorder, and scores of 70 or above represent marked presence. Each personality disorder ...", "dateLastCrawled": "2020-05-24T05:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "SimLex-999: <b>Evaluating Semantic Models With (Genuine) Similarity</b> ...", "url": "https://direct.mit.edu/coli/article/41/4/665/1517/SimLex-999-Evaluating-Semantic-Models-With-Genuine", "isFamilyFriendly": true, "displayUrl": "https://direct.mit.edu/coli/article/41/4/665/1517/SimLex-999-Evaluating-Semantic...", "snippet": "The SimLex-999 <b>inter-rater</b> <b>agreement</b> suggests that participants were able to understand the (single) characterization of similarity presented in the instructions and to apply it to concepts of various types consistently. This conclusion was supported by inspection of the brief feedback offered by the majority of annotators in a final text field in the questionnaire: 78% expressed sentiment that the test was clear, easy to complete, or some <b>similar</b> sentiment.", "dateLastCrawled": "2022-01-19T23:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Intercoder <b>Reliability</b> in Qualitative Research: Debates and Practical ...", "url": "https://journals.sagepub.com/doi/10.1177/1609406919899220", "isFamilyFriendly": true, "displayUrl": "https://journals.sagepub.com/doi/10.1177/1609406919899220", "snippet": "ICR is a numerical measure of the <b>agreement</b> between different coders regarding how the same data should be coded. ICR is sometimes conflated with <b>interrater</b> <b>reliability</b> (IRR), and the <b>two</b> terms are often used interchangeably. However, technically IRR refers to cases where data are rated on some ordinal or interval scale (e.g., the intensity of an emotion), whereas ICR is appropriate when categorizing data at a nominal level (e.g., the presence or absence of an emotion).", "dateLastCrawled": "2022-02-02T14:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) When Human Coders (and Machines) Disagree on the Meaning of ...", "url": "https://www.academia.edu/4149622/When_Human_Coders_and_Machines_Disagree_on_the_Meaning_of_Facial_Affect_in_Spontaneous_Videos", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/4149622/When_Human_Coders_and_Machines_Disagree_on_the...", "snippet": "<b>Inter- rater</b> reliability was measured using Kohen\u2019s kappa between self vs. peer (Coder 1 vs. Coder 2), self vs. judge1 (Coder 1 vs. Coder 3), self vs. judge2 (Coder 1 vs. Coder 4), peer vs. judge1 (Coder 2 vs. Coder 3), peer vs. judge2 (Coder 2 vs. Coder 4), judge1 vs. judge2 (Coder 3 vs. Coder 4). Among all these pairs, judge1 and judge2 had the highest <b>agreement</b> (kappa =0.71). We took the subset of videos where these <b>two</b> judges perfectly agreed and used these videos with the judges ...", "dateLastCrawled": "2021-12-17T00:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "PSY105 L4b Reliability &amp; Validity in Research (Student - Big).pdf ...", "url": "https://www.coursehero.com/file/93888834/PSY105-L4b-Reliability-Validity-in-Research-Student-Bigpdf/", "isFamilyFriendly": true, "displayUrl": "https://www.coursehero.com/file/93888834/PSY105-L4b-Reliability-Validity-in-Research...", "snippet": "Form equivalence - Test-equating-<b>Two</b> different tests on the same content (but different items) 4. <b>Inter-rater</b> reliability -<b>Agreement</b> between <b>two</b> raters / coders / observers-Useful for subjective measurements (e.g., for qualitative observational research) -Measured by Pearson\u2019s r (continuous scores), Cohen\u2019s Kappa (categorical scores), etc. Types of Validity 1. Content validity (= logical validity)-Extent to which a test represents all facets of a variable-Content of test items should be ...", "dateLastCrawled": "2021-12-05T08:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "PSY105 L4b Reliability Validity in Research (Notes).pdf - Lecture 4b ...", "url": "https://www.coursehero.com/file/65369899/PSY105-L4b-Reliability-Validity-in-Research-Notespdf/", "isFamilyFriendly": true, "displayUrl": "https://www.coursehero.com/file/65369899/PSY105-L4b-Reliability-Validity-in-Research...", "snippet": "Types of Reliability Types of Validity Temporal Stability Internal Consistency Form Equivalence <b>Inter-rater</b> Reliability Content Validity Construct Validity Criterion Validity Reliability and Validity of Measurement Tools. Types of Reliability-Same test, 2 or more occasions-Behaviours of subjects could be affected by repeated tests-Consistency of items within a test-Commonly measured by Cronbach\u2019s alpha, \u03b1-E.g., \u201cI dislike watching movies\u201d vs \u201cI feel very happy after watching a <b>movie</b> ...", "dateLastCrawled": "2022-01-06T12:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Content Analysis</b> in the Study of Crime, Media, and Popular Culture ...", "url": "https://oxfordre.com/criminology/view/10.1093/acrefore/9780190264079.001.0001/acrefore-9780190264079-e-23", "isFamilyFriendly": true, "displayUrl": "https://oxfordre.com/criminology/view/10.1093/acrefore/9780190264079.001.0001/acrefore...", "snippet": "In brief, <b>inter-rater</b> reliability is the extent to which different <b>people</b> code the same text in the same way. Differences, for example, may occur when coding rules or categories are not clear, or when there are cognitive differences across coders. The pre-test process and adequate coder training may reduce these differences, but <b>inter-rater</b> reliability should also be assessed at the end of coding. Various statistical tests exist to assess <b>inter-rater</b> reliability.", "dateLastCrawled": "2022-02-01T08:06:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "An Introduction to Inter-Annotator <b>Agreement</b> and Cohen&#39;s Kappa Statistic", "url": "http://blog.echen.me/2021/12/23/an-introduction-to-inter-annotator-agreement-and-cohens-kappa-statistic/", "isFamilyFriendly": true, "displayUrl": "blog.echen.me/2021/12/23/an-introduction-to-inter-annotator-<b>agreement</b>-and-cohens-kappa...", "snippet": "<b>Can</b> be easily adapted to measure <b>agreement</b> about more than <b>two</b> labels. (For example, if Alix and Bob gave every essay an A-F grade instead of just pass/fail.) Negative scores <b>can</b> be used to identify raters with diverse viewpoints. Cons. <b>Can</b> only compare <b>two</b> raters, not three or more. (Unlike next week&#39;s spotlight metric, Fleiss\u2019 kappa!)", "dateLastCrawled": "2022-01-23T02:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Interrater</b> <b>Agreement</b> Measures: Comments on Kappa n , Cohen&#39;s Kappa ...", "url": "https://www.researchgate.net/publication/232604628_Interrater_Agreement_Measures_Comments_on_Kappa_n_Cohen's_Kappa_Scott's_p_and_Aickin's_a", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/232604628_<b>Interrater</b>_<b>Agreement</b>_Measures...", "snippet": "Huddleston (2003) and Rashed (2010) There are <b>two</b> measures commonly used in <b>inter-rater</b> reliability, namely Cohen-Kappa (K) and percentage of <b>agreement</b> (%) (Gwet, 2002; Hsu and Field, 2003). Their ...", "dateLastCrawled": "2022-01-28T03:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "PSY105 L4b Reliability &amp; Validity in Research (Student - Big).pdf ...", "url": "https://www.coursehero.com/file/93888834/PSY105-L4b-Reliability-Validity-in-Research-Student-Bigpdf/", "isFamilyFriendly": true, "displayUrl": "https://www.coursehero.com/file/93888834/PSY105-L4b-Reliability-Validity-in-Research...", "snippet": "Form equivalence - Test-equating-<b>Two</b> different tests on the same content (but different items) 4. <b>Inter-rater</b> reliability -<b>Agreement</b> between <b>two</b> raters / coders / observers-Useful for subjective measurements (e.g., for qualitative observational research)-Measured by Pearson\u2019s r (continuous scores), Cohen\u2019s Kappa (categorical scores), etc. Types of Validity 1. Content validity (= logical validity)-Extent to which a test represents all facets of a variable-Content of test items should be ...", "dateLastCrawled": "2021-12-05T08:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) The Application of <b>Interrater Reliability as a Solidification</b> ...", "url": "https://www.researchgate.net/publication/228348213_The_Application_of_Interrater_Reliability_as_a_Solidification_Instrument_in_a_Phenomenological_Study", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/228348213_The_Application_of_<b>Interrater</b>...", "snippet": "The <b>agreement</b> was determined on <b>two</b> counts: (1) On the basi s of exact listing, which was the case with 7 of these 10 themes and (2) on the basis of similar interpretability, such as \u201cgiving to", "dateLastCrawled": "2022-01-24T08:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "The noun <b>agreement</b> <b>can</b> be countable or uncountable. - Indian Thoughts", "url": "https://indianthoughts.in/the-noun-agreement-can-be-countable-or-uncountable/", "isFamilyFriendly": true, "displayUrl": "https://indian<b>thoughts</b>.in/the-noun-<b>agreement</b>-<b>can</b>-be-countable-or-uncountable", "snippet": "A verbal <b>agreement</b> <b>can</b> also be a fully binding <b>agreement</b>, but it <b>can</b> be difficult to prove a verbal <b>agreement</b> in situations where you may need this . 3.6.1 The Employer agrees to translate each clause of this <b>agreement</b> from the language in which the clause was negotiated at the bargaining table into the other official language of Canada at the time of their ratification at the negotiation table. Within three months of the ratification of the new <b>agreement</b>, the Employer shall provide a ...", "dateLastCrawled": "2022-01-28T08:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) Tool for Observing Play Outdoors (TOPO): A New Typology for ...", "url": "https://www.academia.edu/63669481/Tool_for_Observing_Play_Outdoors_TOPO_A_New_Typology_for_Capturing_Childrens_Play_Behaviors_in_Outdoor_Environments", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/63669481/Tool_for_Observing_Play_Outdoors_TOPO_A_New_Typology...", "snippet": "During reliability rounds, all observers independently code the same play behaviors to compare levels of <b>agreement</b>; Kappa analysis <b>can</b> then be conducted to compare the reliability of <b>inter-rater</b> observations across the full dataset. Int. J. Environ. Res. Public Health 2020, 17, 5611 30 of 34 See Tables A1 and A2 for sample templates for both the TOPO-9 and TOPO-32 versions of the tool. Table A1. Sample Template for TOPO-9 (Collapsed Version). Sample Template for TOPO-9 Play Type Codes: 1 ...", "dateLastCrawled": "2022-01-15T07:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Cohen Lab Mission | Cohen LabCohen Lab | Intervening Wisely Since 1999", "url": "http://cohenlab.stanford.edu/wp-content/uploads/2013/09/survey.final_.notes_.doc", "isFamilyFriendly": true, "displayUrl": "cohenlab.stanford.edu/wp-content/uploads/2013/09/survey.final_.notes_.doc", "snippet": "<b>Inter-rater</b> <b>agreement</b> (look at what has convergence across rater, as convergence is desirable) Interviewer Effects (stronger the interviewer effect, the more problematic it is) Question order effects. If the format of the questionnaire is good, it is resistant to interviewer and question order effects. Questionnaire Construction. Specify Constructs. Start with a list of all the things we need to measure. Constructs are often fictions that social scientists make up like self-esteem. We posit ...", "dateLastCrawled": "2021-11-04T12:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Self-fulfilling prophecy, Social Evolution, Chapter</b> 10, Chapter 9 ...", "url": "https://quizlet.com/210247383/self-fulfilling-prophecy-social-evolution-chapter-10-chapter-9-chapter-5-chapter-3-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/210247383/<b>self-fulfilling-prophecy-social-evolution-chapter</b>-10...", "snippet": "- High level of <b>inter-rater</b> <b>agreement</b> - Significant cross-cultural and historical <b>agreement</b> - Variability in physical adornments and ideal body weight . Boom and Bust. Men in cultures with scarce resources prefer heavier women - Symons 1979; Anderson et al., 1992. Hungry for More-Nelson and Morrison (2005)-Men going into the dining hall prefer heavier women than men coming out of the dining hall-No significant effect for women - More specifically, hungry males have been found to prefer ...", "dateLastCrawled": "2018-11-09T14:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "The duration of the connection <b>agreement</b> is determined by the Parties ...", "url": "https://indianthoughts.in/the-duration-of-the-connection-agreement-is-determined-by-the-parties/", "isFamilyFriendly": true, "displayUrl": "https://indian<b>thoughts</b>.in/the-duration-of-the-connection-<b>agreement</b>-is-determined-by...", "snippet": "Where a creditor enters into a settlement <b>agreement</b> <b>agreeing</b> to compromise a debt owing but requiring the debtor to pay a greater sum if the debtor defaults, the default clause may constitute a penalty and be unenforceable. However, the Lachlan decision confirms the line of authority that the doctrine of penalties <b>can</b> be avoided in such a situation provided the <b>agreement</b> contains an acknowledgment by the debtor that the full amount of the debt is presently owing default clause settlement ...", "dateLastCrawled": "2021-12-25T07:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What does <b>agreement</b> mean - Definition of <b>agreement</b> - Word finder", "url": "https://findwords.info/term/agreement", "isFamilyFriendly": true, "displayUrl": "https://findwords.info/term/<b>agreement</b>", "snippet": "<b>People</b> sometimes sign credit agreements and then realize they <b>can</b>\u2019t afford the payments. a disarmament treaty/<b>agreement</b> . There will be US-Russian talks on a new disarmament treaty. a lease <b>agreement</b>. The organization has signed a lease <b>agreement</b> on a 50-acre site. a loan <b>agreement</b> (=that says how much the loan will be, how much you will pay back each month etc) Read the terms of your loan <b>agreement</b> carefully. a peace treaty/<b>agreement</b>/accord. The formal signing of the peace <b>agreement</b> took ...", "dateLastCrawled": "2021-07-21T03:23:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Interrater</b> <b>Agreement</b> Measures: Comments on Kappa n , Cohen&#39;s Kappa ...", "url": "https://www.researchgate.net/publication/232604628_Interrater_Agreement_Measures_Comments_on_Kappa_n_Cohen's_Kappa_Scott's_p_and_Aickin's_a", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/232604628_<b>Interrater</b>_<b>Agreement</b>_Measures...", "snippet": "Huddleston (2003) and Rashed (2010) There are <b>two</b> measures commonly used in <b>inter-rater</b> reliability, namely Cohen-Kappa (K) and percentage of <b>agreement</b> (%) (Gwet, 2002; Hsu and Field, 2003). Their ...", "dateLastCrawled": "2022-01-28T03:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Introducing a new definition of a near fall: intra-rater and inter ...", "url": "https://europepmc.org/article/MED/23972512", "isFamilyFriendly": true, "displayUrl": "https://europepmc.org/article/MED/23972512", "snippet": "<b>Inter-rater</b> reliability was determined by calculating the ICC for the four and five raters when the traditional and new definitions were used, respectively. Since the raters classified <b>two</b> movies for each definition, <b>two</b> ICCs were calculated, one for the first <b>movie</b> and one for the second. The average of these <b>two</b> ICCs was calculated for each definition. Scores between 0\u20130.2 indicated poor <b>agreement</b>; 0.3\u20130.4 fair <b>agreement</b>; 0.5\u20130.6 moderate <b>agreement</b>; 0.7\u20130.8 strong <b>agreement</b>; and &gt;0 ...", "dateLastCrawled": "2022-01-27T00:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Intercoder <b>Reliability</b> in Qualitative Research: Debates and Practical ...", "url": "https://journals.sagepub.com/doi/10.1177/1609406919899220", "isFamilyFriendly": true, "displayUrl": "https://journals.sagepub.com/doi/10.1177/1609406919899220", "snippet": "ICR is a numerical measure of the <b>agreement</b> between different coders regarding how the same data should be coded. ICR is sometimes conflated with <b>interrater</b> <b>reliability</b> (IRR), and the <b>two</b> terms are often used interchangeably. However, technically IRR refers to cases where data are rated on some ordinal or interval scale (e.g., the intensity of an emotion), whereas ICR is appropriate when categorizing data at a nominal level (e.g., the presence or absence of an emotion).", "dateLastCrawled": "2022-02-02T14:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) When Human Coders (and Machines) Disagree on the Meaning of ...", "url": "https://www.academia.edu/4149622/When_Human_Coders_and_Machines_Disagree_on_the_Meaning_of_Facial_Affect_in_Spontaneous_Videos", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/4149622/When_Human_Coders_and_Machines_Disagree_on_the...", "snippet": "<b>Inter- rater</b> reliability was measured using Kohen\u2019s kappa between self vs. peer (Coder 1 vs. Coder 2), self vs. judge1 (Coder 1 vs. Coder 3), self vs. judge2 (Coder 1 vs. Coder 4), peer vs. judge1 (Coder 2 vs. Coder 3), peer vs. judge2 (Coder 2 vs. Coder 4), judge1 vs. judge2 (Coder 3 vs. Coder 4). Among all these pairs, judge1 and judge2 had the highest <b>agreement</b> (kappa =0.71). We took the subset of videos where these <b>two</b> judges perfectly agreed and used these videos with the judges ...", "dateLastCrawled": "2021-12-17T00:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Developmental differences in affective representation between ...", "url": "https://academic.oup.com/scan/advance-article/doi/10.1093/scan/nsab093/6332882", "isFamilyFriendly": true, "displayUrl": "https://academic.oup.com/s<b>can</b>/advance-article/doi/10.1093/s<b>can</b>/nsab093/6332882", "snippet": "Calculating <b>inter-rater</b> reliability of stimuli Media content is often subjective, and, in confirming <b>agreement</b> on the constructs represented in the content, Krippendorff\u2019s alpha ( Lombard et al. , 2002 ; Krippendorff, 2004 ) is a common statistical comparison test used by media scholars to assess content constancy ( Lombard et al. , 2002 ; Lombard, 2013 ).", "dateLastCrawled": "2022-01-01T21:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "The noun <b>agreement</b> <b>can</b> be countable or uncountable. - Indian Thoughts", "url": "https://indianthoughts.in/the-noun-agreement-can-be-countable-or-uncountable/", "isFamilyFriendly": true, "displayUrl": "https://indianthoughts.in/the-noun-<b>agreement</b>-<b>can</b>-be-countable-or-uncountable", "snippet": "A verbal <b>agreement</b> <b>can</b> also be a fully binding <b>agreement</b>, but it <b>can</b> be difficult to prove a verbal <b>agreement</b> in situations where you may need this . 3.6.1 The Employer agrees to translate each clause of this <b>agreement</b> from the language in which the clause was negotiated at the bargaining table into the other official language of Canada at the time of their ratification at the negotiation table. Within three months of the ratification of the new <b>agreement</b>, the Employer shall provide a ...", "dateLastCrawled": "2022-01-28T08:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Midlife Eriksonian Psychosocial Development: Setting the Stage for ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5398200/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC5398200", "snippet": "Erikson\u2019s (1950) model of adult psychosocial development outlines the significance of successful involvement within one\u2019s relationships, work, and community for healthy aging. He theorized that the consequences of not meeting developmental challenges included stagnation and emotional despair. Drawing on this model, the present study uses prospective longitudinal data to examine how the quality of assessed Eriksonian psychosocial development in midlife relates to late-life cognitive and ...", "dateLastCrawled": "2022-02-02T08:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>FINAL psy 301</b> Flashcards | <b>Quizlet</b>", "url": "https://quizlet.com/55062703/final-psy-301-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/55062703/<b>final-psy-301</b>-flash-cards", "snippet": "4. <b>inter-rater</b> - give <b>two</b> <b>people</b> one measure at one time. test-retest reliability . test administered to same person on <b>two</b> separate occasions and correlation between <b>two</b> tests is calculated - time one and time <b>two</b> advantages: uses same test items, simple to do disadvantages: testing effects, maturation, delay of info. alternate forms of reliability. correlation between <b>two</b> different (but similar versions of a measure) - form a and form b (ex: parallel forms of reliability; measuring the ...", "dateLastCrawled": "2021-05-25T14:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "The duration of the connection <b>agreement</b> is determined by the Parties ...", "url": "https://indianthoughts.in/the-duration-of-the-connection-agreement-is-determined-by-the-parties/", "isFamilyFriendly": true, "displayUrl": "https://indianthoughts.in/the-duration-of-the-connection-<b>agreement</b>-is-determined-by...", "snippet": "Where a creditor enters into a settlement <b>agreement</b> <b>agreeing</b> to compromise a debt owing but requiring the debtor to pay a greater sum if the debtor defaults, the default clause may constitute a penalty and be unenforceable. However, the Lachlan decision confirms the line of authority that the doctrine of penalties <b>can</b> be avoided in such a situation provided the <b>agreement</b> contains an acknowledgment by the debtor that the full amount of the debt is presently owing default clause settlement ...", "dateLastCrawled": "2021-12-25T07:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "For mining leases, they <b>are known as compensation agreements</b>. - Enjoii ...", "url": "http://enjoii.dk/for-mining-leases-they-are-known-as-compensation-agreements/", "isFamilyFriendly": true, "displayUrl": "enjoii.dk/for-mining-leases-they-<b>are-known-as-compensation-agreements</b>", "snippet": "The matrimonial <b>agreement</b> <b>can</b> include a clause dealing with how and when divorce proceedings may be subsequently issued e.g. you both agree after <b>two</b> years separation either party may apply to the Court for divorce.As a part of the divorce proceedings an application will be made on the date of the divorce hearing to make the <b>agreement</b> a Rule of Court or an Order of Court.This means the original <b>agreement</b> is lodged with the Court and is thus enforceable through the Courts so if any part of ...", "dateLastCrawled": "2022-01-30T23:35:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Understanding Interobserver <b>Agreement</b>: The Kappa Statistic", "url": "http://web2.cs.columbia.edu/~julia/courses/CS6998/Interrater_agreement.Kappa_statistic.pdf", "isFamilyFriendly": true, "displayUrl": "web2.cs.columbia.edu/~julia/courses/CS6998/<b>Interrater</b>_<b>agreement</b>.Kappa_statistic.pdf", "snippet": "call the <b>analogy</b> of a target and how close we get to the bull\u2019s-eye (Figure 1). If we actually hit the bull\u2019s-eye (representing <b>agreement</b> with the gold standard), we are accurate. If all our shots land together, we have good precision (good reliability). If all our shots land together and we hit the bull\u2019s-eye, we are accurate as well as precise. It is possible, however, to hit the bull\u2019s-eye purely by chance. Referring to Figure 1, only the center black dot in target A is accurate ...", "dateLastCrawled": "2022-01-28T23:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Leveraging Inter-rater Agreement for Audio-Visual Emotion Recognition</b>", "url": "https://www.researchgate.net/publication/283487589_Leveraging_Inter-rater_Agreement_for_Audio-Visual_Emotion_Recognition", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/283487589_Leveraging_<b>Inter-rater</b>_<b>Agreement</b>...", "snippet": "In <b>machine</b> <b>learning</b> tasks an actual \u2018ground truth\u2019 may not be available. Then, machines often have to rely on human labelling of data. This becomes challenging the more subjective the <b>learning</b> ...", "dateLastCrawled": "2021-08-28T14:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Multilingual <b>Twitter Sentiment Classification</b>: The Role of Human ... - PLOS", "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0155036", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0155036", "snippet": "The researchers in the fields of <b>inter-rater</b> <b>agreement</b> and <b>machine</b> <b>learning</b> typically employ different evaluation measures. We report all the results in terms of four selected measures which we deem appropriate for the three-valued sentiment classification task (the details are in the Evaluation measures subsection in Methods). In this section, however, the results are summarized only in terms of Krippendorff\u2019s Alpha-reliability Alpha) , to highlight the main conclusions. Alpha is a ...", "dateLastCrawled": "2021-03-30T23:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Machine</b> <b>learning</b> glossary - DataTime", "url": "https://www.dtalg.com/article-1/", "isFamilyFriendly": true, "displayUrl": "https://www.dtalg.com/article-1", "snippet": "See also Cohen\u2019s kappa, which is one of the most popular <b>inter-rater</b> <b>agreement</b> measurements. intersection over union (IoU) #image. The intersection of two sets divided by their union. In <b>machine</b>-<b>learning</b> image-detection tasks, IoU is used to measure the accuracy of the model\u2019s predicted bounding box with respect to the ground-truth bounding ...", "dateLastCrawled": "2022-01-25T17:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "The Art <b>and Science of Analyzing Software Data</b>", "url": "https://www.slideshare.net/timmenzies/the-art-and-science-of-analyzing-software-data", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/timmenzies/the-art-<b>and-science-of-analyzing-software-data</b>", "snippet": "Analyzing survey data \u2022 <b>Inter-rater</b> <b>agreement</b> \u2013 Coding is a subjective activity \u2013 Increase reliability by using multiple raters for entire data or a subset of the data \u2013 Cohen\u2019s Kappa or Fleiss\u2019 Kappa can be used to measure the <b>agreement</b> between multiple raters. \u2013 \u201cWe measured <b>inter-rater</b> <b>agreement</b> for the first author\u2019s categorization on a simple random sample of 100 cards with a closed card sort and two additional raters (third and fourth author); the Fleiss\u2019 Kappa ...", "dateLastCrawled": "2022-01-19T09:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Analyzing and Interpreting Data From Rating Scales</b> | by Kevin C Lee ...", "url": "https://towardsdatascience.com/analyzing-and-interpreting-data-from-rating-scales-d169d66211db", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>analyzing-and-interpreting-data-from-rating-scales</b>-d169...", "snippet": "<b>Inter-Rater</b> Reliability. In B), we plot the pairwise correlations between the students with a heatmap. Most of the correlations are &gt; 0.6 with a few exceptions. A small number of respondents showing low correlations with others is acceptable as long as most students are able to respond similarly. P.S. The use of Pearson Correlation is only ...", "dateLastCrawled": "2022-01-29T00:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Use of analogies, metaphors, and similes by students and reviewers at ...", "url": "https://www.cambridge.org/core/journals/ai-edam/article/use-of-analogies-metaphors-and-similes-by-students-and-reviewers-at-an-undergraduate-architectural-design-review/FB80EB57099A898FE15564497D5B06C7", "isFamilyFriendly": true, "displayUrl": "https://www.cambridge.org/core/journals/ai-edam/article/use-of-analogies-metaphors-and...", "snippet": "We used the Delphi Method to determine the <b>inter-rater</b> <b>agreement</b>. In the first step after the second round of discussion, there was 66.67% <b>agreement</b> between the authors\u2019 coding and that of the independent coder. In the second step, <b>agreement</b> on the type of similarities was determined using the Delphi Method. At the end of second round of discussions, there was 90.1% <b>agreement</b>. Table 1. Categories and sub-categories used for coding the reviews. Any statement which explicitly or implicitly ...", "dateLastCrawled": "2022-02-02T16:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Target <b>analogy</b> of accuracy and precision | Download Scientific Diagram", "url": "https://researchgate.net/figure/Target-analogy-of-accuracy-and-precision_fig1_24399044", "isFamilyFriendly": true, "displayUrl": "https://researchgate.net/figure/Target-<b>analogy</b>-of-accuracy-and-precision_fig1_24399044", "snippet": "The intraclass correlation coefficient (ICC) was calculated to assess intra-rater and <b>inter-rater</b> <b>agreement</b> of I 3M . 31 A sample of OPTs was randomly divided into training dataset (819) and test ...", "dateLastCrawled": "2021-06-28T05:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Quadratic weighted kappa</b> strength of <b>agreement</b> - Cross Validated", "url": "https://stats.stackexchange.com/questions/46296/quadratic-weighted-kappa-strength-of-agreement", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/46296", "snippet": "In the case of the kappa-value there are some attempts to qualify how good or bad the agreements are. For example Landis &amp; Koch in the article The Measurement of Observer <b>Agreement</b> for Categorical Data talks about &quot;strength of <b>agreement</b>&quot; based on kappa values:. Kappa Strength of <b>agreement</b> ===== ===== 0.0-0.20 Slight 0.21-0.40 Fair 0.41-0.60 Moderate 0.61-0.80 Substantial 0.81-0.90 Almost perfect", "dateLastCrawled": "2022-01-20T17:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "ENH/FAQ: Fleiss Kappa giving nan results, randolph&#39;s kappa \u00b7 Issue ...", "url": "https://github.com/statsmodels/statsmodels/issues/4387", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/statsmodels/statsmodels/issues/4387", "snippet": "I&#39;m trying to use Fleiss Kappa to calculate the <b>agreement</b> of several raters. First I have an array of two subjects and 7 raters per subject. There are two categories, however, the second category is never chosen. ratings = [[0,0,0,0,0,0,...", "dateLastCrawled": "2022-01-12T20:40:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Reliability and Learnability of Human Bandit Feedback for Sequence-to ...", "url": "https://aclanthology.org/P18-1165.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/P18-1165.pdf", "snippet": "intra- and <b>inter-rater agreement is similar</b> for both tasks, with highest inter-rater reliability for stan-dardized 5-point ratings. In a next step, we address the issue of <b>machine</b> learnability of human rewards. We use deep learn- ing models to train reward estimators by regres-sion against cardinal feedback, and by \ufb01tting a Bradley-Terry model (Bradley and Terry,1952) to ordinal feedback. Learnability is understood by a slight misuse of the <b>machine</b> <b>learning</b> notion of learnability (Shalev ...", "dateLastCrawled": "2021-12-22T00:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "arXiv:1805.10627v3 [cs.CL] 13 Dec 2018", "url": "https://www.researchgate.net/profile/Joshua-Uyheng/publication/325413588_Reliability_and_Learnability_of_Human_Bandit_Feedback_for_Sequence-to-Sequence_Reinforcement_Learning/links/5ea04de5a6fdccd7cee0eebe/Reliability-and-Learnability-of-Human-Bandit-Feedback-for-Sequence-to-Sequence-Reinforcement-Learning.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/profile/Joshua-Uyheng/publication/325413588_Reliability...", "snippet": "\ufb01ed by bandit <b>learning</b> for neural <b>machine</b> trans-lation (NMT). Our aim is to show that successful <b>learning</b> from simulated bandit feedback (Sokolov et al.,2016b;Kreutzer et al.,2017;Nguyen et al ...", "dateLastCrawled": "2021-08-22T12:54:00.0000000Z", "language": "en", "isNavigational": false}], [], [], []], "all_bing_queries": ["+(inter-rater agreement)  is like +(two people agreeing on a movie)", "+(inter-rater agreement) is similar to +(two people agreeing on a movie)", "+(inter-rater agreement) can be thought of as +(two people agreeing on a movie)", "+(inter-rater agreement) can be compared to +(two people agreeing on a movie)", "machine learning +(inter-rater agreement AND analogy)", "machine learning +(\"inter-rater agreement is like\")", "machine learning +(\"inter-rater agreement is similar\")", "machine learning +(\"just as inter-rater agreement\")", "machine learning +(\"inter-rater agreement can be thought of as\")", "machine learning +(\"inter-rater agreement can be compared to\")"]}