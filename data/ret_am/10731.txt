{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Gradient of <b>Hinge loss</b> - Cross Validated", "url": "https://stats.stackexchange.com/questions/4608/gradient-of-hinge-loss", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/4608", "snippet": "This answer is useful. 39. This answer is not useful. Show activity on this post. To get the gradient we differentiate the <b>loss</b> with respect to i th component of w. Rewrite <b>hinge loss</b> in terms of w as f ( g ( w)) where f ( z) = max ( 0, 1 \u2212 y z) and g ( w) = x \u22c5 w. Using chain rule we get. \u2202 \u2202 w i f ( g ( w)) = \u2202 f \u2202 z \u2202 g \u2202 w i.", "dateLastCrawled": "2022-02-03T02:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Why smoothed <b>hinge</b> <b>loss</b> is better than original <b>hinge</b> <b>loss</b> for (sub ...", "url": "https://www.quora.com/Why-smoothed-hinge-loss-is-better-than-original-hinge-loss-for-sub-gradient-descent-optimization", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Why-smoothed-<b>hinge</b>-<b>loss</b>-is-better-than-original-<b>hinge</b>-<b>loss</b>-for...", "snippet": "Answer: Think of it this way: when applying a gredient decent optimization to any <b>loss</b> function what you basically do is trying to minimize it. The way you (generally ...", "dateLastCrawled": "2022-01-16T15:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "What is the <b>loss</b> <b>function of the standard perceptron algorithm</b>? - Quora", "url": "https://www.quora.com/What-is-the-loss-function-of-the-standard-perceptron-algorithm", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-the-<b>loss</b>-<b>function-of-the-standard-perceptron-algorithm</b>", "snippet": "Answer (1 of 6): I\u2019m impressed, all answers are <b>wrong</b>. How can this be possible ? A perceptron is a linear regression with <b>Hinge</b> <b>Loss</b> as <b>loss</b> function. Usually linear regression uses MSE as <b>loss</b> function ang logistic regression uses a sigmoid, but the definition of Perceptron is linear regressio...", "dateLastCrawled": "2021-12-31T08:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Machine Learning with Python - From Linear Models to Deep Learning ...", "url": "https://aditya-jaishankar.github.io/coursenotes/machinelearning/", "isFamilyFriendly": true, "displayUrl": "https://aditya-jaishankar.github.io/coursenotes/machinelearning", "snippet": "where the <b>Loss</b> can either be <b>hinge</b> <b>loss</b> or a <b>squared</b> <b>loss</b> or something else. It is easy to show that for the <b>squared</b> <b>loss</b> (with a 1/2 factor to make the derivative easier), the updates to $\\theta$ in the gradient descent algorithm expression can be written as Some intuition for regularization: Regularization is one way to try and prevent overfitting. By forcing the norm of the vector to be 0, we are simplifying the problem and decreasing the number of parameters. Gradient descent with ...", "dateLastCrawled": "2021-12-20T22:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Support Vector Machines with the Ramp <b>Loss</b> and the Hard Margin <b>Loss</b> ...", "url": "https://www.researchgate.net/publication/220243848_Support_Vector_Machines_with_the_Ramp_Loss_and_the_Hard_Margin_Loss", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/220243848_Support_Vector_Machines_with_the...", "snippet": "An impressive body of work has designed the <b>loss</b> functions: popular candidates include convex ones <b>like</b> the <b>squared</b> <b>Hinge</b> <b>loss</b> in [39] and the pinball <b>loss</b> in [18] and some non-convex ones in [28 ...", "dateLastCrawled": "2022-01-27T13:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Ensemble Learning \u2013 Yet Another ML Blog \u2013 data science and ML theory", "url": "https://bllguo.github.io/Ensemble-learning/", "isFamilyFriendly": true, "displayUrl": "https://bllguo.github.io/Ensemble-learning", "snippet": "which is just <b>taking</b> the log likelihood and flipping the fraction of the sigmoid to turn the maximization problem into a minimization. So we are minimizing $\\log(1+\\exp(-y_iw^Tx_i))$ Boosting is effectively minimizing $\\exp(-y_i\\alpha^T(x_i))$ Below is a plot of these <b>loss</b> functions, along with the <b>hinge</b> <b>loss</b> in SVM.", "dateLastCrawled": "2022-01-27T00:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Subgradient Descent</b> Does Not Descend \u2013 Parameter-free Learning and ...", "url": "https://parameterfree.com/2018/06/20/54/", "isFamilyFriendly": true, "displayUrl": "https://parameterfree.com/2018/06/20/54", "snippet": "Note that this situation is more common than one would think. For example, the <b>hinge</b> <b>loss</b>, , and the ReLU activation ... Here, in the point , any positive <b>step</b> in the <b>direction</b> of the negative subgradient will increase the objective function. Another effect of the fact that the objective function is non-differentiable is that we cannot use a constant stepsize. This is easy to visualize, even for one-dimensional functions. Take a look for example at the function in Figure 3(left). For any ...", "dateLastCrawled": "2022-01-29T06:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "How to Choose <b>Loss</b> Functions When Training Deep Learning Neural Networks", "url": "https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/how-to-choose-<b>loss</b>-functions-when-training-deep...", "snippet": "The Mean <b>Squared</b> Error, or MSE, <b>loss</b> is the default <b>loss</b> to use for regression problems. Mathematically, it is the preferred <b>loss</b> function under the inference framework of maximum likelihood if the distribution of the target variable is Gaussian.", "dateLastCrawled": "2022-02-02T15:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>MACHINE LEARNING QUESTIONS</b> \u2014 PROGRAMMING REVIEW", "url": "https://programming-review.com/machine-learning/questions", "isFamilyFriendly": true, "displayUrl": "https://programming-review.com/<b>machine-learning/questions</b>", "snippet": "A regression model predicts continuous values. For example, regression models make predictions <b>like</b> what is the price of a car <b>taking</b> many different factors in. 21. Find the intruder: 1) regression 2) classification 3) clustering. Regression and classification analysis are supervised machine learning techniques in general. Clustering is an unsupervised learning approach together with association. Regression predicts continuous values for the output. Predicting a person\u2019s income from their ...", "dateLastCrawled": "2022-01-21T22:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Gradient Descent</b> in Python. When you venture into machine learning ...", "url": "https://towardsdatascience.com/gradient-descent-in-python-a0d07285742f", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>gradient-descent</b>-in-python-a0d07285742f", "snippet": "Nothing <b>wrong</b> in this however remember machine learning is about programming in matrices. For sake of machine learning I can express the equation for a line in terms of machine learning in a different way. I would call y as my hypothesis and represent it as J(theta) and call b as theta0 and m as theta1. I can write same equation as : Machine learning way. To solve for the Theta0 and Theta1 analytical way I would have to write the following program: theta_best = np.linalg.inv(X.T.dot(X)).dot ...", "dateLastCrawled": "2022-02-02T17:02:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Why smoothed <b>hinge</b> <b>loss</b> is better than original <b>hinge</b> <b>loss</b> for (sub ...", "url": "https://www.quora.com/Why-smoothed-hinge-loss-is-better-than-original-hinge-loss-for-sub-gradient-descent-optimization", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Why-smoothed-<b>hinge</b>-<b>loss</b>-is-better-than-original-<b>hinge</b>-<b>loss</b>-for...", "snippet": "Answer: Think of it this way: when applying a gredient decent optimization to any <b>loss</b> function what you basically do is trying to minimize it. The way you (generally ...", "dateLastCrawled": "2022-01-16T15:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "How to Choose <b>Loss</b> Functions When Training Deep Learning Neural Networks", "url": "https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/how-to-choose-<b>loss</b>-functions-when-training-deep...", "snippet": "model.compile(<b>loss</b>=&#39;...&#39;, optimizer=opt) # fit model. history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=100, verbose=0) Now that we have the basis of a problem and model, we can take a look evaluating three common <b>loss</b> functions that are appropriate for a regression predictive modeling problem.", "dateLastCrawled": "2022-02-02T15:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "midterm_Fall_2020_SOLUTIONS.pdf - MA4270 Semester 1 AY2020\\/21 Mid-Term ...", "url": "https://www.coursehero.com/file/127561662/midterm-Fall-2020-SOLUTIONSpdf/", "isFamilyFriendly": true, "displayUrl": "https://www.coursehero.com/file/127561662/midterm-Fall-2020-SOLUTIONSpdf", "snippet": "(c) <b>Loss</b> (I), because it is the <b>loss</b> that increases the fastest when z becomes large in the negative <b>direction</b>. (d) <b>Loss</b> (VI), because it penalizes positive z (correct side of the decision boundary) and the corresponding-z (incorrect side of the decision boundary) equally, so it doesn\u2019t even favor having correctly-classified points. 2", "dateLastCrawled": "2022-01-28T12:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Awesome-Machine-Learning/readme.md at main \u00b7 Billy1900/Awesome-Machine ...", "url": "https://github.com/Billy1900/Awesome-Machine-Learning/blob/main/CV/readme.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/Billy1900/Awesome-Machine-Learning/blob/main/CV/readme.md", "snippet": "1.3.5 Why we should use cross entropy/<b>hinge</b> <b>loss</b> than <b>squared</b> <b>loss</b> in classification problem? Cross entropy has some advantages: Cross entropy <b>loss</b> penalizes heavily the predictions that are confident but <b>wrong</b>. However, for <b>squared</b> <b>loss</b>, it has $\\hat{y}(1-\\hat{y})$ in its gradient term. When $\\hat{y}$ is close to 0.0 or 1.0, the speed of ...", "dateLastCrawled": "2021-08-09T05:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Subgradient Descent</b> Does Not Descend \u2013 Parameter-free Learning and ...", "url": "https://parameterfree.com/2018/06/20/54/", "isFamilyFriendly": true, "displayUrl": "https://parameterfree.com/2018/06/20/54", "snippet": "It <b>is similar</b> to the gradient descent algorithm, but it has some important conceptual differences. First, here, we do not assume differentiability of the function so we only have access to a subgradient in any point in , i.e. . Note that this situation is more common than one would think. For example, the <b>hinge</b> <b>loss</b>, , and the ReLU activation function used in neural networks, , are not differentiable. A second difference, is that SD is not a descent method, that is, it can happen that . The ...", "dateLastCrawled": "2022-01-29T06:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>CPSC 340: Machine Learning and Data Mining</b>", "url": "https://ubc-cs.github.io/cpsc340/lectures/L30.pdf?raw=1", "isFamilyFriendly": true, "displayUrl": "https://ubc-cs.github.io/cpsc340/lectures/L30.pdf?raw=1", "snippet": "\u2022 With <b>squared</b> <b>loss</b>, our objective function for one hidden layer is: \u2022 Usual training procedure: stochastic gradient. \u2013 Compute gradient of random example \u2018i\u2019, update both \u2018v\u2019 and \u2018W\u2019. \u2013 Highly non-convex and can be difficult to tune. \u2022 Computing the gradient is known as \u201cbackpropagation\u201d. \u2013 Video giving motivation on course webpage. 5 Backpropagation \u2022 Overview of how we compute neural network gradient: \u2013 Forward propagation: \u2022 (1)Compute z i from x i ...", "dateLastCrawled": "2021-11-18T10:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Optimization</b> - mlstory.org", "url": "https://mlstory.org/optimization.html", "isFamilyFriendly": true, "displayUrl": "https://mlstory.org/<b>optimization</b>.html", "snippet": "It is helpful to keep the next picture in mind that summarizes how each of these different <b>loss</b> functions approximate the zero-one <b>loss</b>. We can ensure that the <b>squared</b> <b>loss</b> is an upper bound on the zero-one <b>loss</b> by dropping the factor 1/2. Three different convex losses compared with the zero-one <b>loss</b>. Insights from quadratic functions", "dateLastCrawled": "2022-01-30T03:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Machine Learning with Python - From Linear Models to Deep Learning ...", "url": "https://aditya-jaishankar.github.io/coursenotes/machinelearning/", "isFamilyFriendly": true, "displayUrl": "https://aditya-jaishankar.github.io/coursenotes/machinelearning", "snippet": "The idea here is to move in the <b>direction</b> of decreasing gradient because at the optimum, the gradient is 0. Therefore, we can formulate the algorithm as stepping through different values of $\\theta$ such that . where $\\eta$ is the learning rate. Now, gradient descent can be very slow in the case of large data sets becuase we have to evaluate the <b>loss</b> function at each data point. One way around this is to use stochastic gradient descent (SGD), where instead of doing the whole average at each ...", "dateLastCrawled": "2021-12-20T22:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Gradient Descent</b> in Python. When you venture into machine learning ...", "url": "https://towardsdatascience.com/gradient-descent-in-python-a0d07285742f", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>gradient-descent</b>-in-python-a0d07285742f", "snippet": "Nothing <b>wrong</b> in this however remember machine learning is about programming in matrices. For sake of machine learning I can express the equation for a line in terms of machine learning in a different way. I would call y as my hypothesis and represent it as J(theta) and call b as theta0 and m as theta1. I can write same equation as : Machine learning way. To solve for the Theta0 and Theta1 analytical way I would have to write the following program: theta_best = np.linalg.inv(X.T.dot(X)).dot ...", "dateLastCrawled": "2022-02-02T17:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(PDF) <b>Problem Solution To Mechanical Engineering</b> | jayaraju chundru ...", "url": "https://www.academia.edu/5604223/Problem_Solution_To_Mechanical_Engineering", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/5604223/<b>Problem_Solution_To_Mechanical_Engineering</b>", "snippet": "Academia.edu is a platform for academics to share research papers.", "dateLastCrawled": "2022-02-02T04:15:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Supervised</b> learning - mlstory.org", "url": "https://mlstory.org/supervised.html", "isFamilyFriendly": true, "displayUrl": "https://mlstory.org/<b>supervised</b>.html", "snippet": "As a <b>thought</b> experiment, ... We <b>can</b> see that the <b>hinge</b> <b>loss</b> gives us part of the update rule in the perceptron algorithm. The other part comes from adding a weight penalty \\frac\\lambda 2\\Vert w\\Vert^2 to the <b>loss</b> function that discourages the weights from growing out of bounds. This weight penalty is called \\ell_2-regularization, weight decay, or Tikhonov regularization depending on which field you work in. The purpose of regularization is to promote generalization. We will therefore return ...", "dateLastCrawled": "2022-02-03T15:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Gradient of <b>Hinge loss</b> - Cross Validated", "url": "https://stats.stackexchange.com/questions/4608/gradient-of-hinge-loss", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/4608", "snippet": "This answer is useful. 39. This answer is not useful. Show activity on this post. To get the gradient we differentiate the <b>loss</b> with respect to i th component of w. Rewrite <b>hinge loss</b> in terms of w as f ( g ( w)) where f ( z) = max ( 0, 1 \u2212 y z) and g ( w) = x \u22c5 w. Using chain rule we get. \u2202 \u2202 w i f ( g ( w)) = \u2202 f \u2202 z \u2202 g \u2202 w i.", "dateLastCrawled": "2022-02-03T02:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "CS489 | Anthony Zhang", "url": "https://anthony-zhang.me/University-Notes/CS489/CS489.html", "isFamilyFriendly": true, "displayUrl": "https://anthony-zhang.me/University-Notes/CS489/CS489.html", "snippet": "The Lagrangian variables <b>can</b> <b>be thought</b> of as &quot;adversarial&quot; - the \\max_{\\alpha \\ge 0} is trying to ... the subscript plus is called the <b>hinge</b> <b>loss</b> function, because the graph looks like a door <b>hinge</b> on its side. Here , C \\sum (1 - y_i \\hat y_i)_+ are the soft constraints, \\sum (1 - y_i \\hat y_i)_+ is the training error, C is the cost parameter (which we tune as a hyperparameter), and \\hat y_i = \\vec w \\cdot \\vec x_i + b is the hyperplane&#39;s prediction. Note that (1 - y_i \\hat y_i)_+ has 3 ...", "dateLastCrawled": "2021-10-29T00:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>An Intro to Linear Classification with Python</b> - PyImageSearch", "url": "https://www.pyimagesearch.com/2016/08/22/an-intro-to-linear-classification-with-python/", "isFamilyFriendly": true, "displayUrl": "https://www.pyimagesearch.com/2016/08/22/<b>an-intro-to-linear-classification-with-python</b>", "snippet": "We <b>can</b> then obtain the total <b>loss</b> over the three examples by <b>taking</b> the average: &gt;&gt;&gt; (0.0 + 5.96 + 5.2) / 3.0 3.72 . Therefore, given our three training examples our overall <b>hinge</b> <b>loss</b> is 3.72 for the parameters W and b. Also take note that our <b>loss</b> was zero for only one of the three input images, implying that two of our predictions were incorrect. In a future lesson, we\u2019ll learn how to optimize W and b to make better predictions by using the <b>loss</b> function to help drive and steer us in ...", "dateLastCrawled": "2022-02-03T01:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Module 2: Machine Learning Deep Dive", "url": "https://www.slideshare.net/SaraHooker/module-2-machine-learning-deep-dive", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/SaraHooker/module-2-machine-learning-deep-dive", "snippet": "What is <b>hinge</b> <b>loss</b>?<b>Hinge</b> <b>loss</b> <b>Hinge</b> <b>loss</b> is the logical extension of the regression <b>loss</b> function, absolute <b>loss</b>. Absolute <b>loss</b>: Y- Y*, where Y and Y* are integers. <b>Hinge</b> <b>loss</b>: max(0,1-(Y*)(Y)) Where Y <b>can</b> equal -1 (no) or 1 (yes) for each class. For each observation, if Y* == Y (both are 1 or both are -1), <b>hinge</b> <b>loss</b> = 0. If Y =/= Y*, <b>hinge</b> <b>loss</b> increases. The cumulated <b>hinge</b> <b>loss</b> is therefore the upper bound of the number of mistakes made by the classifier. Sources: https://en.wikipedia ...", "dateLastCrawled": "2022-01-12T22:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) <b>Support Vector Machines with a Reject Option</b>", "url": "https://www.researchgate.net/publication/221618498_Support_Vector_Machines_with_a_Reject_Option", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221618498_<b>Support_Vector_Machines_with_a</b>...", "snippet": "Though not a proper negative log-likelihood, the <b>hinge</b> <b>loss</b> <b>can</b> be interpreted in a maximum a posteriori framework: The <b>hinge</b> <b>loss</b> <b>can</b> be derived as a relaxed minimization of negativ e log ...", "dateLastCrawled": "2022-01-03T07:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Support Vector Machines with the Ramp <b>Loss</b> and the Hard Margin <b>Loss</b> ...", "url": "https://www.researchgate.net/publication/220243848_Support_Vector_Machines_with_the_Ramp_Loss_and_the_Hard_Margin_Loss", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/220243848_Support_Vector_Machines_with_the...", "snippet": "An impressive body of work has designed the <b>loss</b> functions: popular candidates include convex ones like the <b>squared</b> <b>Hinge</b> <b>loss</b> in [39] and the pinball <b>loss</b> in [18] and some non-convex ones in [28 ...", "dateLastCrawled": "2022-01-27T13:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Gradient Boosting</b> working and applications. | by Sai Karthik - Medium", "url": "https://medium.com/gradient-boosting-working-limitations-time/gradient-boosting-working-and-applications-28e8d4ba866d", "isFamilyFriendly": true, "displayUrl": "https://medium.com/<b>gradient-boosting</b>-working-limitations-time/<b>gradient-boosting</b>...", "snippet": "The <b>loss</b> function <b>can</b> be replaced with any other <b>loss</b> function like <b>hinge</b> <b>loss</b> or log <b>loss</b> which are differentiable and find the negative <b>gradient</b> of the <b>loss</b> function to obtain the residual ...", "dateLastCrawled": "2022-02-03T15:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "How To Implement The Perceptron Algorithm From Scratch In Python", "url": "https://machinelearningmastery.com/implement-perceptron-algorithm-scratch-python/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/implement-perceptron", "snippet": "The Perceptron algorithm is the simplest type of artificial neural network. It is a model of a single neuron that <b>can</b> be used for two-class classification problems and provides the foundation for later developing much larger networks. In this tutorial, you will discover how to implement the Perceptron algorithm from scratch with Python. After completing this tutorial, you will know: How to train", "dateLastCrawled": "2022-01-30T02:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What is the significance of Lin and Tegmark\u2019s paper &#39;Why does deep and ...", "url": "https://www.quora.com/What-is-the-significance-of-Lin-and-Tegmark%E2%80%99s-paper-Why-does-deep-and-cheap-learning-work-so-well", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-the-signifi<b>can</b>ce-of-Lin-and-Tegmark\u2019s-paper-Why-does...", "snippet": "Answer (1 of 2): I wrote the paper, so I <b>can</b>\u2019t answer the question of whether or not it\u2019s important. I <b>can</b> say what I hope the takeaway from the paper will be: A) My take on machine learning is roughly something like this. You <b>can</b> think of a neural network as a key, and a generic machine learnin...", "dateLastCrawled": "2022-01-16T02:00:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Supervised</b> learning - mlstory.org", "url": "https://mlstory.org/supervised.html", "isFamilyFriendly": true, "displayUrl": "https://mlstory.org/<b>supervised</b>.html", "snippet": "The <b>hinge</b> <b>loss</b> is not the only reasonable choice. There are numerous <b>loss</b> functions that approximate the zero-one <b>loss</b> in different ways. The <b>hinge</b> <b>loss</b> is \\max\\{1-y\\hat y, 0\\} and support vector machine refers to empirical risk minimization with the <b>hinge</b> <b>loss</b> and \\ell_2-regularization. This is what the perceptron is optimizing. The <b>squared</b> ...", "dateLastCrawled": "2022-02-03T15:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Why smoothed <b>hinge</b> <b>loss</b> is better than original <b>hinge</b> <b>loss</b> for (sub ...", "url": "https://www.quora.com/Why-smoothed-hinge-loss-is-better-than-original-hinge-loss-for-sub-gradient-descent-optimization", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Why-smoothed-<b>hinge</b>-<b>loss</b>-is-better-than-original-<b>hinge</b>-<b>loss</b>-for...", "snippet": "Answer: Think of it this way: when applying a gredient decent optimization to any <b>loss</b> function what you basically do is trying to minimize it. The way you (generally ...", "dateLastCrawled": "2022-01-16T15:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Understanding Loss Functions in Machine Learning</b> | Engineering ...", "url": "https://www.section.io/engineering-education/understanding-loss-functions-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.section.io/engineering-education/<b>understanding-loss-functions-in-machine</b>...", "snippet": "On <b>taking</b> a closer look at the formulas, one <b>can</b> observe that if the difference between the predicted and the actual value is high, L2 <b>loss</b> magnifies the effect when <b>compared</b> to L1. Since L2 succumbs to outliers, L1 <b>loss</b> function is the more robust <b>loss</b> function. L1 <b>loss</b> is less stable than L2 <b>loss</b>. Since L1 <b>loss</b> deals with the difference in distances, a small horizontal change <b>can</b> lead to the regression line jumping a large amount. Such an effect <b>taking</b> place across multiple iterations ...", "dateLastCrawled": "2022-02-01T06:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "How to Choose <b>Loss</b> Functions When Training Deep Learning Neural Networks", "url": "https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/how-to-choose-<b>loss</b>-functions-when-training-deep...", "snippet": "Now that we have the basis of a problem and model, we <b>can</b> take a look evaluating three common <b>loss</b> functions that are appropriate for a binary classification predictive modeling problem. Although an MLP is used in these examples, the same <b>loss</b> functions <b>can</b> be used when training CNN and RNN models for binary classification. Binary Cross-Entropy ...", "dateLastCrawled": "2022-02-02T15:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Margin distribution based bagging pruning - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S0925231212000689", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0925231212000689", "snippet": "In this work, we just consider the <b>squared</b> <b>hinge</b> <b>loss</b> in the optimization objective. In fact there are a collection of <b>loss</b> functions to be used, such as the exponential <b>loss</b> and logistic <b>loss</b>. Moreover l 2 regularization <b>can</b> also be considered and combined with different <b>loss</b> functions. We will work along these directions in future.", "dateLastCrawled": "2021-11-24T02:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "midterm_Fall_2020_SOLUTIONS.pdf - MA4270 Semester 1 AY2020\\/21 Mid-Term ...", "url": "https://www.coursehero.com/file/127561662/midterm-Fall-2020-SOLUTIONSpdf/", "isFamilyFriendly": true, "displayUrl": "https://www.coursehero.com/file/127561662/midterm-Fall-2020-SOLUTIONSpdf", "snippet": "By letting the decision boundary\u2019s hyperplane be perpendicular to the <b>direction</b> of x 1 (e.g., \u03b8 = y 1 x 1 suffices), we get that the closest point on the decision boundary to x 1 is 0. (b) In this case, the margin <b>can</b> be arbitrarily large, e.g., by <b>taking</b> the solution from part (a) and shifting it further and further away from x 1.", "dateLastCrawled": "2022-01-28T12:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "3.3. Metrics and scoring: quantifying the quality of predictions ...", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "isFamilyFriendly": true, "displayUrl": "https://scikit-learn.org/stable/modules/model_evaluation.html", "snippet": "<b>hinge</b>_<b>loss</b> (y_true, pred_decision, *[, ...]) Average <b>hinge</b> <b>loss</b> (non-regularized). ... This is because, by analogy with the bias-variance decomposition of the mean <b>squared</b> error, the Brier <b>score</b> <b>loss</b> <b>can</b> be decomposed as the sum of calibration <b>loss</b> and refinement <b>loss</b> [Bella2012]. Calibration <b>loss</b> is defined as the mean <b>squared</b> deviation from empirical probabilities derived from the slope of ROC segments. Refinement <b>loss</b> <b>can</b> be defined as the expected optimal <b>loss</b> as measured by the area ...", "dateLastCrawled": "2022-02-03T06:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Support Vector Machines with the Ramp <b>Loss</b> and the Hard Margin <b>Loss</b> ...", "url": "https://www.researchgate.net/publication/220243848_Support_Vector_Machines_with_the_Ramp_Loss_and_the_Hard_Margin_Loss", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/220243848_Support_Vector_Machines_with_the...", "snippet": "An impressive body of work has designed the <b>loss</b> functions: popular candidates include convex ones like the <b>squared</b> <b>Hinge</b> <b>loss</b> in [39] and the pinball <b>loss</b> in [18] and some non-convex ones in [28 ...", "dateLastCrawled": "2022-01-27T13:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Machine Learning with Python - From Linear Models to Deep Learning ...", "url": "https://aditya-jaishankar.github.io/coursenotes/machinelearning/", "isFamilyFriendly": true, "displayUrl": "https://aditya-jaishankar.github.io/coursenotes/machinelearning", "snippet": "The idea here is to move in the <b>direction</b> of decreasing gradient because at the optimum, the gradient is 0. Therefore, we <b>can</b> formulate the algorithm as stepping through different values of $\\theta$ such that . where $\\eta$ is the learning rate. Now, gradient descent <b>can</b> be very slow in the case of large data sets becuase we have to evaluate the <b>loss</b> function at each data point. One way around this is to use stochastic gradient descent (SGD), where instead of doing the whole average at each ...", "dateLastCrawled": "2021-12-20T22:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Optimization</b> - mlstory.org", "url": "https://mlstory.org/optimization.html", "isFamilyFriendly": true, "displayUrl": "https://mlstory.org/<b>optimization</b>.html", "snippet": "It is helpful to keep the next picture in mind that summarizes how each of these different <b>loss</b> functions approximate the zero-one <b>loss</b>. We <b>can</b> ensure that the <b>squared</b> <b>loss</b> is an upper bound on the zero-one <b>loss</b> by dropping the factor 1/2. Three different convex losses <b>compared</b> with the zero-one <b>loss</b>. Insights from quadratic functions", "dateLastCrawled": "2022-01-30T03:27:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Introduction to Machine Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/24/introduction-to-machine-learning-2/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/24/<b>introduction-to-machine-learning</b>-2", "snippet": "<b>Squared</b> <b>loss</b> (for regression) <b>Hinge</b> <b>loss</b> (SVM) Logistic/log <b>loss</b> (logistic regression) Some <b>loss</b> functions are as follows: When to stop tuning <b>machine</b> <b>learning</b> models. When to stop tuning the hyperparameters in a <b>machine</b> <b>learning</b> model is a million-dollar question. This problem can be mostly solved by keeping tabs on training and testing errors. While increasing the complexity of a model, the following stages occur: Stage 1: Underfitting stage \u2013 high train and high test errors (or low ...", "dateLastCrawled": "2022-01-17T05:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Data Science: <b>Support Vector Machines (SVM</b>)", "url": "https://www.datasciencesmachinelearning.com/2019/01/support-vector-machines-svm.html", "isFamilyFriendly": true, "displayUrl": "https://www.datasciences<b>machinelearning</b>.com/2019/01/<b>support-vector-machines-svm</b>.html", "snippet": "In this case, <b>squared</b> <b>hinge</b> <b>loss</b> function (as against <b>hinge</b> <b>loss</b> function) and l2 penalty are the major changes compared to the earlier three methods. This method is useful for when sample size is larger.", "dateLastCrawled": "2022-01-28T08:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Introduction to <b>Machine</b> <b>Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/30/introduction-to-machine-learning-3/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/30/introduction-to-<b>machine</b>-<b>learning</b>-3", "snippet": "However, in <b>machine</b> <b>learning</b> methodology, <b>squared</b> <b>loss</b> will be minimized with respect to ... <b>Squared</b> <b>loss</b> (for regression) <b>Hinge</b> <b>loss</b> (SVM) Logistic/log <b>loss</b> (logistic regression) Some <b>loss</b> functions are as follows: When to stop tuning <b>machine</b> <b>learning</b> models. When to stop tuning the hyperparameters in a <b>machine</b> <b>learning</b> model is a million-dollar question. This problem can be mostly solved by keeping tabs on training and testing errors. While increasing the complexity of a model, the ...", "dateLastCrawled": "2022-01-28T18:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A study on L2-<b>loss (Squared Hinge-Loss) multiclass SVM</b> | Request PDF", "url": "https://www.researchgate.net/publication/235884495_A_study_on_L2-loss_Squared_Hinge-Loss_multiclass_SVM", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/235884495_A_study_on_L2-<b>loss</b>_<b>Squared</b>_<b>Hinge</b>...", "snippet": "Taking the <b>analogy</b> to classification task, it has been previously studied [13] that using the <b>squared</b> <b>hinge</b> <b>loss</b> in SVM would yield better accuracy when \u03bb is large. In this case, underfitting ...", "dateLastCrawled": "2021-12-14T01:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Metrics to Evaluate Classification and Regression Algorithms | by ...", "url": "https://medium.com/@poojitha.penmethsa/metrics-to-evaluate-classification-and-regression-algorithms-1554f1e00a75", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@poojitha.penmethsa/metrics-to-evaluate-classification-and...", "snippet": "<b>Hinge</b> <b>loss</b>. cross-entropy <b>loss</b> / log <b>loss</b>. likelihood <b>loss</b>. MSE / Quadratic <b>loss</b> / L2 <b>loss</b>: Mean <b>Squared</b> Error, or MSE <b>loss</b> is the default <b>loss</b> to use for regression problems. Mathematically, it ...", "dateLastCrawled": "2022-01-17T01:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Understanding the 3 most common <b>loss</b> functions for <b>Machine</b> <b>Learning</b> ...", "url": "https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-the-3-most-common-<b>loss</b>-functions-for...", "snippet": "A <b>loss function</b> in <b>Machine</b> <b>Learning</b> is a measure of how accurately your ML model is able to predict the expected outcome i.e the ground truth. The <b>loss function</b> will take two items as input: the output value of our model and the ground truth expected value. The output of the <b>loss function</b> is called the <b>loss</b> which is a measure of how well our model did at predicting the outcome. A high value for the <b>loss</b> means our model performed very poorly. A low value for the <b>loss</b> means our model performed ...", "dateLastCrawled": "2022-02-02T13:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and-sample-codes ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/README.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "Models 1.1 Support vector <b>machine</b> 1.1.1 Principle 1.1.2 Kernel 1.1.3 Soft margin SVM 1.1.4 <b>Hinge</b> <b>loss</b> view 1.1.5 Multi-class SVM 1.1.6 Extensions 1.2 Tree-based models 1.2.1 Decision tree 1.2.2 Random forest 1.2.3 Gradient boosted decision trees 1.2.4 Tools 1.3 EM Principle 1.4 MaxEnt 1.4.1 Entropy 1.5 Model selection 1.5.1 Under-fitting / Over-fitting 1.5.2 Model ensemble, sklearn 2.", "dateLastCrawled": "2021-09-22T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Fast generalization rates for distance metric</b> <b>learning</b> - <b>Machine</b> <b>Learning</b>", "url": "https://link.springer.com/article/10.1007/s10994-018-5734-0", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10994-018-5734-0", "snippet": "For example, the <b>squared</b> <b>hinge</b> <b>loss</b> \\(\\ell _s^1(x) = \\max (1-x, 0) ... <b>Analogy</b>-preserving semantic embedding for visual object categorization. In Proceedings of the 30th international conference on <b>machine</b> <b>learning</b>, Atlanta, GA (pp. 639\u2013647). Jin, R., Wang, S., &amp; Zhou, Y. (2010). Regularized distance metric <b>learning</b>: Theory and algorithm. Advances in neural information processing systems (Vol. 23, pp. 862\u2013870). Cambridge, MA: MIT Press. Google Scholar Kulis, B. (2012). Metric <b>learning</b>: A ...", "dateLastCrawled": "2021-12-28T14:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>MLP for regression with TensorFlow 2 and</b> Keras \u2013 MachineCurve", "url": "https://www.machinecurve.com/index.php/2019/07/30/creating-an-mlp-for-regression-with-keras/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/index.php/2019/07/30/creating-an-mlp-for-regression-with...", "snippet": "Last Updated on 30 March 2021. <b>Machine</b> <b>learning</b> is a wide field and <b>machine</b> <b>learning</b> problems come in many flavors. If, say, you wish to group data based on similarities, you would choose an unsupervised approach called clustering.If you have a fixed number of classes which you wish to assign new data to, you\u2019ll choose a supervised approach named classification.If, however, you don\u2019t have a fixed number, but wish to estimate a real value \u2013 your approach will still be supervised, but ...", "dateLastCrawled": "2022-02-03T02:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "CPSC 340: Data Mining <b>Machine</b> <b>Learning</b>", "url": "https://www.cs.ubc.ca/~schmidtm/Courses/340-F16/L35.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.ubc.ca/~schmidtm/Courses/340-F16/L35.pdf", "snippet": "<b>Machine</b> <b>Learning</b> and Data Mining Course Review/Preview Fall 2016 Some images from this lecture are taken from Google Image Search. Admin \u2022Assignment 6: \u20131 late day to hand in next Monday, 2 for Wednesday, 3 for Friday. \u2022Final: \u2013December 12 (8:30am \u2013HEBB 100) \u2013Covers Assignments 1-6. \u2013List of topics posted. \u2013Final from last year will be posted after class. \u2013Closed-book, cheat sheet: 4-pages each double-sided. Last Time: Semi-Supervised <b>Learning</b> \u2022In semi-supervised <b>learning</b> ...", "dateLastCrawled": "2021-11-22T01:45:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>About loss and loss functions</b> \u2013 MachineCurve", "url": "https://www.machinecurve.com/index.php/2019/10/04/about-loss-and-loss-functions/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/index.php/2019/10/04/<b>about-loss-and-loss-functions</b>", "snippet": "We\u2019re then using <b>machine</b> <b>learning</b> for ... The <b>squared hinge loss is like</b> the hinge formula displayed above, but then the \\(max()\\) function output is squared. This helps achieving two things: Firstly, it makes the loss value more sensitive to outliers, just as we saw with MSE vs MAE. Large errors will add to the loss more significantly than smaller errors. Note that simiarly, this may also mean that you\u2019ll need to inspect your dataset for the presence of such outliers first. Secondly ...", "dateLastCrawled": "2022-01-25T13:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "\u635f\u5931\u51fd\u6570 - \u7b97\u6cd5\u6742\u8d27\u94fa - bjmsong.github.io", "url": "https://bjmsong.github.io/2020/02/21/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/", "isFamilyFriendly": true, "displayUrl": "https://bjmsong.github.io/2020/02/21/\u635f\u5931\u51fd\u6570", "snippet": "the training data is fed into the <b>machine</b> <b>learning</b> model; Loss : compare between some actual targets and predicted targets; the lower the loss, the more the set of targets and the set of predictions resemble each other; the more they resemble each other, the better the <b>machine</b> <b>learning</b> model performs. Backward pass", "dateLastCrawled": "2021-12-27T11:43:00.0000000Z", "language": "zh_chs", "isNavigational": false}], [], [], [], []], "all_bing_queries": ["+(squared hinge loss)  is like +(taking a step in the wrong direction)", "+(squared hinge loss) is similar to +(taking a step in the wrong direction)", "+(squared hinge loss) can be thought of as +(taking a step in the wrong direction)", "+(squared hinge loss) can be compared to +(taking a step in the wrong direction)", "machine learning +(squared hinge loss AND analogy)", "machine learning +(\"squared hinge loss is like\")", "machine learning +(\"squared hinge loss is similar\")", "machine learning +(\"just as squared hinge loss\")", "machine learning +(\"squared hinge loss can be thought of as\")", "machine learning +(\"squared hinge loss can be compared to\")"]}