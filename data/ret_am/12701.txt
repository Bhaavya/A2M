{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Black</b>-<b>Box</b> Attacks against <b>RNN</b> based Malware Detection Algorithms ...", "url": "https://www.researchgate.net/publication/317088315_Black-Box_Attacks_against_RNN_based_Malware_Detection_Algorithms", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/317088315_<b>Black</b>-<b>Box</b>_Attacks_against_<b>RNN</b>_based...", "snippet": "The work in [70] proposes a <b>black</b>-<b>box</b> attack framework that targeting <b>RNN</b> model used in detecting malware. The framework consists of two models: one is a generative <b>RNN</b>, the other is a substitute ...", "dateLastCrawled": "2022-01-30T13:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Process structure-based recurrent neural network modeling for model ...", "url": "https://www.sciencedirect.com/science/article/pii/S095915241930825X", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S095915241930825X", "snippet": "Specifically, instead of treating the <b>RNN</b> system of Eq. (3) <b>like</b> a <b>black</b> <b>box</b> and training it using all the inputs and outputs available (termed the fully-connected model throughout the manuscript), we modify the <b>RNN</b> structure according to the structural process knowledge of the nonlinear system of Eq. (1). The details of the proposed new structure are discussed in the following section. Remark 2 . It is noted that in Eq. (3), we use a one-hidden-layer <b>RNN</b> model with n states in order to ...", "dateLastCrawled": "2022-01-08T13:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>RNN</b> to C code?", "url": "https://jam-world.github.io/rnn/ttic/study/2016/06/23/rnn-to-c-code.html", "isFamilyFriendly": true, "displayUrl": "https://jam-world.github.io/<b>rnn</b>/ttic/study/2016/06/23/<b>rnn</b>-to-c-code.html", "snippet": "We may get something that only compares the numerical variables and all the program is still <b>like</b> an <b>black</b> <b>box</b>. However, I believe it may be possible for us to go one step further. Is understanding <b>RNN</b> the same procedure as software reverse engineering? <b>RNN</b> to math expression? So <b>rnn</b> <b>is like</b> magic it can train over programs, which means <b>rnn</b> can learn any algorithm we can write in any Turing complete language. And, It&#39;s that possible for us to reverse this procedure to produce the math ...", "dateLastCrawled": "2021-12-13T02:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "4. Recurrent Neural Networks - <b>Neural networks and deep learning</b> [Book]", "url": "https://www.oreilly.com/library/view/neural-networks-and/9781492037354/ch04.html", "isFamilyFriendly": true, "displayUrl": "https://www.oreilly.com/library/view/neural-networks-and/9781492037354/ch04.html", "snippet": "If you consider the LSTM cell as a <b>black</b> <b>box</b>, it can be used very much <b>like</b> a basic cell, except it will perform much better; training will converge faster and it will detect long-term dependencies in the data. In TensorFlow, you can simply use a BasicLSTMCell instead of a BasicRNNCell: lstm_cell = tf.contrib.<b>rnn</b>.BasicLSTMCell(num_units=n_neurons)", "dateLastCrawled": "2022-01-28T19:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Implementing A Recurrent Neural Network (RNN) From</b> Scratch", "url": "https://analyticsindiamag.com/implementing-a-recurrent-neural-network-rnn-from-scratch/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/<b>implementing-a-recurrent-neural-network-rnn-from</b>-scratch", "snippet": "Formulating the Neural Network. Let\u2019s take the example of a \u201cmany-to-many\u201d <b>RNN</b> because that\u2019s the problem type we\u2019ll be working on. The inputs and outputs are denoted by x 0, x 1, \u2026 x n and y 0, y 1, \u2026 y n, respectively, where x i and y i are vectors with arbitrary dimensions. RNNs learn the temporal information with the help of a hidden state h, which is also a vector with arbitrary dimension.", "dateLastCrawled": "2022-02-02T03:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Multiclass Text Classification Using Deep Learning | by vijay choubey ...", "url": "https://medium.com/analytics-vidhya/multiclass-text-classification-using-deep-learning-f25b4b1010e5", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/multiclass-text-classification-using-deep-learning...", "snippet": "For a simple explanation of a bidirectional <b>RNN</b>, think of an <b>RNN</b> cell as a <b>black</b> <b>box</b> taking as input a hidden state (a vector) and a word vector and giving out an output vector and the next hidden ...", "dateLastCrawled": "2022-02-01T01:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>What\u2019s in the Black Box</b>? \u2013 Convolutional Neural Networks", "url": "https://convnets578736054.wordpress.com/2019/01/23/whats-in-the-black-box/", "isFamilyFriendly": true, "displayUrl": "https://convnets578736054.wordpress.com/2019/01/23/<b>whats-in-the-black-box</b>", "snippet": "The pooling layers are used to reduce the dimension of the input that they receive form the previous layer, so, in practice, if we are considering image, then we can consider the pooling <b>like</b> a method to reduce the resolution of our images input from the previous layer. The fully connected neural network is used to compute the probabilities assigned to each possible class for the input images, so we assign a label to our input by simply choosing the most probable class for the same input.", "dateLastCrawled": "2022-01-16T10:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "[D] How do you choose which <b>Black</b>-<b>Box</b> Explainability method to use ...", "url": "https://www.reddit.com/r/MachineLearning/comments/sejdpy/d_how_do_you_choose_which_blackbox_explainability/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/.../sejdpy/d_how_do_you_choose_which_<b>blackbox</b>_explainability", "snippet": "From just watching the video and guessing, it would make sense if noising the belief state (<b>rnn</b>(h,concat(proprio,extero)) + \\eps ~ Noise) and learning to condition proprioceptive attention on the belief uncertainty is enough. Very cool work and so exciting to see robotics groups exploiting ML more and more (gate attention + learned belief states here). 239. 39 comments. share. save. hide. report. 205. Posted by 3 days ago. News [N] Easily Build Machine Learning Products. Hey, I\u2019m Merve ...", "dateLastCrawled": "2022-01-28T22:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Recurrent Neural Networks for time series forecasting</b> | Novatec", "url": "https://www.novatec-gmbh.de/en/blog/recurrent-neural-networks-for-time-series-forecasting/", "isFamilyFriendly": true, "displayUrl": "https://www.novatec-gmbh.de/en/blog/<b>recurrent-neural-networks-for-time-series-forecasting</b>", "snippet": "The input activation function uses tanh and controls the data input flow <b>like</b> in a normal <b>RNN</b>: i t = g ( W i * x t R i * y t-1 ) However the gates use Sigmoid activation functions (denoted by \u03c3) that will squash the values in a range of 0 to 1. The idea of the gates is to let a specific amount of information through and we achieve this with Sigmoid. A value of 1 means \u201clet everything through\u201d and a value of 0 means \u201clet nothing through\u201d. Gates. The gates have different tasks of ...", "dateLastCrawled": "2022-01-28T02:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Open the <b>Black</b> <b>Box: Understand What Drives Predictions in</b> Deep NLP ...", "url": "https://towardsdatascience.com/open-the-black-box-understand-what-drives-predictions-in-deep-nlp-models-833f3dc923d0", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/open-the-<b>black</b>-<b>box-understand-what-drives-predictions</b>...", "snippet": "Open the <b>Black</b> <b>Box: Understand What Drives Predictions in</b> Deep NLP Models. An applied and reproducible demonstration. Michal Mucha. Jun 1, 2020 \u00b7 9 min read. Article written together with Josh Casswell. IMDB review of the film 1917 \u2014 prediction attributions generated with Captum and visualized with ipyvuetify. Objective - Understand and sense-check model decision making through inspecting attributions - Visualize token attributions - Provide easy to use code so that anyone can apply this ...", "dateLastCrawled": "2022-01-22T16:42:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Black</b>-<b>Box</b> Attacks against <b>RNN</b> based Malware Detection Algorithms ...", "url": "https://www.researchgate.net/publication/317088315_Black-Box_Attacks_against_RNN_based_Malware_Detection_Algorithms", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/317088315_<b>Black</b>-<b>Box</b>_Attacks_against_<b>RNN</b>_based...", "snippet": "The work in [70] proposes a <b>black</b>-<b>box</b> attack framework that targeting <b>RNN</b> model used in detecting malware. The framework consists of two models: one is a generative <b>RNN</b>, the other is a substitute ...", "dateLastCrawled": "2022-01-30T13:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Black</b>-<b>Box</b> Attacks against <b>RNN</b> based Malware Detection Algorithms - NASA/ADS", "url": "https://ui.adsabs.harvard.edu/abs/2017arXiv170508131H/abstract", "isFamilyFriendly": true, "displayUrl": "https://ui.adsabs.harvard.edu/abs/2017arXiv170508131H/abstract", "snippet": "<b>Similar</b> Papers Volume Content Graphics ... NASA/ADS. <b>Black</b>-<b>Box</b> Attacks against <b>RNN</b> based Malware Detection Algorithms Hu, Weiwei; Tan, Ying; Abstract. Recent researches have shown that machine learning based malware detection algorithms are very vulnerable under the attacks of adversarial examples. These works mainly focused on the detection algorithms which use features with fixed dimension, while some researchers have begun to use recurrent neural networks (<b>RNN</b>) to detect malware based on ...", "dateLastCrawled": "2020-12-24T22:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "[PDF] <b>Black</b>-<b>Box</b> Attacks <b>against RNN based Malware Detection Algorithms</b> ...", "url": "https://www.semanticscholar.org/paper/Black-Box-Attacks-against-RNN-based-Malware-Hu-Tan/d333881628cac38cb63ff8240950f3561308d34b", "isFamilyFriendly": true, "displayUrl": "https://www.semanticscholar.org/paper/<b>Black</b>-<b>Box</b>-Attacks-against-<b>RNN</b>-based-Malware-Hu...", "snippet": "Experimental results showed that <b>RNN based malware detection algorithms</b> fail to detect most of the generated malicious adversarial examples, which means the proposed model is able to effectively bypass the detection algorithms. Recent researches have shown that machine learning <b>based malware detection algorithms</b> are very vulnerable under the attacks of adversarial examples. These works mainly focused on the detection algorithms which use features with fixed dimension, while some researchers ...", "dateLastCrawled": "2021-11-24T18:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>RNN</b>- and LSTM-Based Soft Sensors Transferability for an Industrial ...", "url": "https://pubmed.ncbi.nlm.nih.gov/33530476/", "isFamilyFriendly": true, "displayUrl": "https://pubmed.ncbi.nlm.nih.gov/33530476", "snippet": "<b>Black</b>-<b>box</b> machine learning (ML) methods are often used as an efficient tool to implement SSs. Many efforts are, however, required to properly select input variables, model class, model order and the needed hyperparameters. The aim of this work was to investigate the possibility to transfer the knowledge acquired in the design of a SS for a given process to a <b>similar</b> one. This has been approached as a transfer learning problem from a source to a target domain. The implementation of a transfer ...", "dateLastCrawled": "2021-12-23T04:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Recurrent neural networks: building a custom</b> LSTM cell | AI Summer", "url": "https://theaisummer.com/understanding-lstm/", "isFamilyFriendly": true, "displayUrl": "https://theaisummer.com/understanding-lstm", "snippet": "More or less, another <b>black</b> <b>box</b> in the pile. However, in this tutorial, we will attempt to open the <b>RNN</b> magic <b>black</b> <b>box</b> and unravel its mysteries! Even though I have come across hundreds of tutorials on LSTM\u2019s out there, I felt there was something missing. Therefore, I honestly hope that this tutorial serves as a modern guide to RNNs. We try to deal with multiple details of practical nature. To this end, we will build upon their fundamental concepts. The vast application field of <b>RNN</b>\u2019s ...", "dateLastCrawled": "2022-01-30T16:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Multiclass Text Classification Using Deep Learning | by vijay choubey ...", "url": "https://medium.com/analytics-vidhya/multiclass-text-classification-using-deep-learning-f25b4b1010e5", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/multiclass-text-classification-using-deep-learning...", "snippet": "For a simple explanation of a bidirectional <b>RNN</b>, think of an <b>RNN</b> cell as a <b>black</b> <b>box</b> taking as input a hidden state (a vector) and a word vector and giving out an output vector and the next hidden ...", "dateLastCrawled": "2022-02-01T01:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Very simple example of RNN</b>? : learnmachinelearning", "url": "https://www.reddit.com/r/learnmachinelearning/comments/84bk5r/very_simple_example_of_rnn/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/learnmachinelearning/comments/84bk5r/<b>very_simple_example_of_rnn</b>", "snippet": "Most samples I found simply show the <b>RNN</b> as a loop with the NN as a <b>black</b> <b>box</b> in a loop taking input and do not bother showing you how the vectors/data is actually worked with. A <b>similar</b> example with an LSTM would be cool too.", "dateLastCrawled": "2021-01-08T07:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Understanding LSTMs | <b>Black</b> <b>Box</b> ML", "url": "https://kushalj001.github.io/black-box-ml/lstm/pytorch/gates/vanishing%20gradient/2019/12/28/Understanding-LSTMs.html", "isFamilyFriendly": true, "displayUrl": "https://kushalj001.github.io/<b>black</b>-<b>box</b>-ml/lstm/pytorch/gates/vanishing gradient/2019/12...", "snippet": "Before we get into the abstract details of the LSTM, it is important to understand what the <b>black</b> <b>box</b> actually contains. The LSTM cell is nothing but a pack of 3-4 mini neural networks. These networks are comprised of linear layers that are parameterized by weight matrices and biases. The values of these weights are learnt by backpropagation. The following figure shows an LSTM cell with labelled gates and all the computations that take place inside the cell. Each cell has 3 inputs: the ...", "dateLastCrawled": "2021-10-14T13:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Binary <b>Black</b>-<b>box</b> Evasion Attacks Against Deep Learning-based Static ...", "url": "https://mohammadrezaebrahimi.github.io/publications/Ebrahimi_MalRNN.pdf", "isFamilyFriendly": true, "displayUrl": "https://mohammadrezaebrahimi.github.io/publications/Ebrahimi_Mal<b>RNN</b>.pdf", "snippet": "<b>black</b>-<b>box</b> scenario in which not only no a priori knowledge is assumed about the target, but also the adversary does not have access to a real-valued feedback from the attack target. Instead, in binary <b>black</b>-<b>box</b> scenario, the adversary can only observe a binary response associated with the success or fail-ure of the crafted instance in evading the attack target. This type of attack is also known as binary <b>black</b> <b>box</b> (Anderson et al. 2018). Binary <b>black</b>-<b>box</b> AEG is the most restrictive and the ...", "dateLastCrawled": "2021-11-07T14:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "[D] Are <b>black</b>-<b>box</b> explainers (e.g. SHAP) just another <b>black</b>-<b>box</b> ...", "url": "https://www.reddit.com/r/MachineLearning/comments/sd973t/d_are_blackbox_explainers_eg_shap_just_another/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/MachineLearning/comments/sd973t/d_are_<b>blackbox</b>_explainers_eg...", "snippet": "The point is that the explaination would be so unintelligible to be basically useless. I see a <b>similar</b> risk for tools such as SHAP, since their assumptions and complexity, and the lack of knowledge on how they work by many of their users, essentially make them just another <b>black</b> <b>box</b> to trust.", "dateLastCrawled": "2022-01-26T18:15:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Real-Time <b>Black</b>-<b>Box</b> Modelling With Recurrent Neural Networks", "url": "https://www.researchgate.net/profile/Alec-Wright-3/publication/335714458_Real-Time_Black-Box_Modelling_with_Recurrent_Neural_Networks/links/5d77441592851cacdb2e0219/Real-Time-Black-Box-Modelling-with-Recurrent-Neural-Networks.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/profile/Alec-Wright-3/publication/335714458_Real-Time...", "snippet": "This paper is concerned with <b>black</b>-<b>box</b> modelling of tube ampli\ufb01ers and distortion pedals using a recurrent neural net- work (<b>RNN</b>). This is a very timely topic, as the \ufb01rst attempts to model ...", "dateLastCrawled": "2021-12-24T04:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Six Types of Neural Networks You Need</b> to Know About", "url": "https://sabrepc.com/blog/Deep-Learning-and-AI/6-types-of-neural-networks-to-know-about", "isFamilyFriendly": true, "displayUrl": "https://sabrepc.com/blog/Deep-Learning-and-AI/6-types-of-neural-networks-to-know-about", "snippet": "For a simple explanation of an <b>RNN</b>, think of an <b>RNN</b> cell as a <b>black</b> <b>box</b> taking as input a hidden state (a vector) and a word vector and giving out an output vector and the next hidden state. This <b>box</b> has some weights which need to be tuned using backpropagation of the losses. Also, the same cell is applied to all the words so that the weights are shared across the words in the sentence. This phenomenon is called weight-sharing. Hidden state, Word vector -&gt;(<b>RNN</b> Cell) -&gt; Output Vector , Next ...", "dateLastCrawled": "2022-01-20T07:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Using Deep Learning for End to End Multiclass Text <b>Classification</b> | by ...", "url": "https://towardsdatascience.com/using-deep-learning-for-end-to-end-multiclass-text-classification-39b46aecac81", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/using-deep-learning-for-end-to-end-multiclass-text...", "snippet": "For a simple explanation of a bidirectional <b>RNN</b>, think of an <b>RNN</b> cell as a <b>black</b> <b>box</b> taking as input a hidden state (a vector) and a word vector and giving out an output vector and the next hidden state. This <b>box</b> has some weights which need to be tuned using backpropagation of the losses. Also, the same cell is applied to all the words so that the weights are shared across the words in the sentence. This phenomenon is called weight-sharing.", "dateLastCrawled": "2022-02-01T18:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Assignment 5 - Introduction to Deep Learning", "url": "https://ovgu-ailab.github.io/idl2020w/ass5.html", "isFamilyFriendly": true, "displayUrl": "https://ovgu-ailab.github.io/idl2020w/ass5.html", "snippet": "A Tensorflow <b>RNN</b> \u201clayer\u201d <b>can</b> be confusing due to its <b>black</b> <b>box</b> character: All computations over a full sequence of inputs are done internally. To make sure you understand how an <b>RNN</b> \u201cworks\u201d, you are asked to implement one from the ground up, defining variables yourself and using basic operations such as tf.matmul to define the computations at each time step and over a full input sequence. There are some related tutorials available on the TF website, but all of these use Keras. For ...", "dateLastCrawled": "2021-10-21T19:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Learning Queuing Networks by Recurrent Neural Networks</b> | DeepAI", "url": "https://deepai.org/publication/learning-queuing-networks-by-recurrent-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>learning-queuing-networks-by-recurrent-neural-networks</b>", "snippet": "It is worth remarking that, in principle, one could learn a QN model by relying on a standard, <b>black</b>-<b>box</b> <b>RNN</b> architecture by treating all the QN parameters (i.e., initial population, service demand, number of servers and routing probabilities) as input features of the learning algorithm. Unfortunately, this straightforward approach would require a considerable amount of input traces since the learning algorithm could not exploit the structural information about the problem. For instance, it ...", "dateLastCrawled": "2021-12-21T16:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "tensorflow - <b>LSTM with Keras to optimize a black box function</b> - Stack ...", "url": "https://stackoverflow.com/questions/65053311/lstm-with-keras-to-optimize-a-black-box-function", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/65053311/<b>lstm-with-keras-to-optimize</b>-a-<b>black</b>-<b>box</b>...", "snippet": "Evaluate y1 = f (p1) Call the LSTM cell with input= [p1,y1], and obtain output=p2. Evaluate y2 = f (p2) Repeat for few times, for example stopping at fifth iteration: y5 = f (p5). I&#39;m trying to implement a similar model in Tensorflow/Keras but I&#39;m having some troubles. In particular, this case is different from &quot;standard&quot; ones because we don&#39;t ...", "dateLastCrawled": "2022-01-23T02:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>box</b> jenkins | Am-241", "url": "https://am241.wordpress.com/tag/box-jenkins/", "isFamilyFriendly": true, "displayUrl": "https://am241.wordpress.com/tag/<b>box</b>-jenkins", "snippet": "Indeed, a <b>RNN</b>, once unrolled, <b>can</b> <b>be thought</b> of as a standard multilayer feedforward network whose inputs are taken at different times. Because of their intrinsically temporal structure (individual inputs being applied sequentially), RNNs are suited for speech recognition, language modelling (see 1409.2329 and references therein) and time-series prediction .", "dateLastCrawled": "2022-01-30T03:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "DeepMind &amp; IDSIA Introduce Symmetries to <b>Black</b>-<b>Box</b> MetaRL to Improve ...", "url": "https://medium.com/syncedreview/deepmind-idsia-introduce-symmetries-to-black-box-metarl-to-improve-its-generalization-ability-f0de828aee03", "isFamilyFriendly": true, "displayUrl": "https://medium.com/syncedreview/deepmind-idsia-introduce-symmetries-to-<b>black</b>-<b>box</b>...", "snippet": "A research team from DeepMind and The Swiss AI Lab IDSIA explores the role of symmetries in meta generalization and shows that introducing more symmetries to <b>black</b>-<b>box</b> meta-learners <b>can</b> improve ...", "dateLastCrawled": "2021-09-28T15:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Deep Learning: No more BlackBox. Deep Learning is defamed for its\u2026 | by ...", "url": "https://sarafparam.medium.com/ai-deep-learning-no-more-blackbox-1faa5c7ffd10", "isFamilyFriendly": true, "displayUrl": "https://sarafparam.medium.com/ai-deep-learning-no-more-<b>blackbox</b>-1faa5c7ffd10", "snippet": "Deep Learning is defamed for its <b>Black</b>-<b>Box</b> nature, meaning one <b>can</b>\u2019t get an easy explanation about the model and its features, ... Deep Learning creates its own features out of the dataset. These features <b>can</b> <b>be thought</b> of as different layers of neurons that activate/deactivate based on specific criteria, hence it is difficult to decode what has the model learned. Captum : Captum (which means comprehension in latin) is a model interpretability and understanding library for PyTorch. Model ...", "dateLastCrawled": "2022-01-16T11:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What is a <b>black box in machine learning? - Quora</b>", "url": "https://www.quora.com/What-is-a-black-box-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-a-<b>black-box-in-machine-learning</b>", "snippet": "Answer (1 of 2): Any group of code where the user has no idea how the code is designed. For example, let\u2019s say you are building a model in Keras from an online tutorial. The deep learning network you are building is a <b>black</b> <b>box</b> to you because you have no idea how deep learning networks are buil...", "dateLastCrawled": "2022-01-29T15:25:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Black</b>-<b>Box</b> Attacks against <b>RNN</b> based Malware Detection Algorithms ...", "url": "https://www.researchgate.net/publication/317088315_Black-Box_Attacks_against_RNN_based_Malware_Detection_Algorithms", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/317088315_<b>Black</b>-<b>Box</b>_Attacks_against_<b>RNN</b>_based...", "snippet": "The work in [70] proposes a <b>black</b>-<b>box</b> attack framework that targeting <b>RNN</b> model used in detecting malware. The framework consists of two models: one is a generative <b>RNN</b>, the other is a substitute ...", "dateLastCrawled": "2022-01-30T13:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Black</b>-<b>Box</b> Attacks <b>against RNN based Malware Detection Algorithms</b> ...", "url": "https://www.arxiv-vanity.com/papers/1705.08131/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1705.08131", "snippet": "A substitute <b>RNN</b> is trained to fit the <b>black</b>-<b>box</b> victim <b>RNN</b>. We use Gumbel-Softmax to approximate the generated discrete APIs, which is able to propagate the gradients from the substitute <b>RNN</b> to the generative <b>RNN</b>. The proposed model has successfully made most of the generated adversarial examples able to bypass several <b>black</b>-<b>box</b> victim RNNs with different structures.", "dateLastCrawled": "2022-01-05T13:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "RNNRepair: Automatic <b>RNN</b> Repair via Model-based Analysis", "url": "https://proceedings.mlr.press/v139/xie21b.html", "isFamilyFriendly": true, "displayUrl": "https://proceedings.mlr.press/v139/xie21b.html", "snippet": "Due to their <b>black</b>-<b>box</b> nature, it is rather challenging to interpret and properly repair these incorrect behaviors. This paper focuses on interpreting and repairing the incorrect behaviors of Recurrent Neural Networks (RNNs). We propose a lightweight model-based approach (RNNRepair) to help understand and repair incorrect behaviors of an <b>RNN</b>. Specifically, we build an influence model to characterize the stateful and statistical behaviors of an <b>RNN</b> over all the training data and to perform ...", "dateLastCrawled": "2021-12-15T18:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>RNN</b>- and LSTM-Based Soft Sensors Transferability for an Industrial Process", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7865368/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7865368", "snippet": "<b>Black</b>-<b>box</b> machine learning (ML) methods are often used as an efficient tool to implement SSs. Many efforts are, however, required to properly select input variables, model class, model order and the needed hyperparameters. The aim of this work was to investigate the possibility to transfer the knowledge acquired in the design of a SS for a given process to a similar one. This has been approached as a transfer learning problem from a source to a target domain. The implementation of a transfer ...", "dateLastCrawled": "2022-01-12T03:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "RNNRepair: Automatic <b>RNN</b> Repair via Model-based Analysis", "url": "http://proceedings.mlr.press/v139/xie21b/xie21b.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v139/xie21b/xie21b.pdf", "snippet": "Due to their <b>black</b>-<b>box</b> nature, it is rather challenging to interpret and properly re-pair these incorrect behaviors. This paper focuses on interpreting and repairing the incorrect behav-iors of Recurrent Neural Networks (RNNs). We propose a lightweight model-based approach (RN-NRepair) to help understand and repair incorrect behaviors of an <b>RNN</b>. Speci\ufb01cally, we build an in\ufb02uence model to characterize the stateful and statistical behaviors of an <b>RNN</b> over all the train-ing data and to ...", "dateLastCrawled": "2022-01-27T14:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "[PDF] <b>Black</b>-<b>Box</b> Attacks <b>against RNN based Malware Detection Algorithms</b> ...", "url": "https://www.semanticscholar.org/paper/Black-Box-Attacks-against-RNN-based-Malware-Hu-Tan/d333881628cac38cb63ff8240950f3561308d34b", "isFamilyFriendly": true, "displayUrl": "https://www.semanticscholar.org/paper/<b>Black</b>-<b>Box</b>-Attacks-against-<b>RNN</b>-based-Malware-Hu...", "snippet": "Experimental results showed that <b>RNN based malware detection algorithms</b> fail to detect most of the generated malicious adversarial examples, which means the proposed model is able to effectively bypass the detection algorithms. Recent researches have shown that machine learning <b>based malware detection algorithms</b> are very vulnerable under the attacks of adversarial examples. These works mainly focused on the detection algorithms which use features with fixed dimension, while some researchers ...", "dateLastCrawled": "2021-11-24T18:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Black</b>-<b>Box</b> Modelling of a DC-DC Buck Converter Based on a Recurrent ...", "url": "https://upcommons.upc.edu/bitstream/handle/2117/188104/Black-Box%20Modelling%20of%20a%20DC-DC%20Buck%20Converter%20Based%20on%20a%20Recurrent%20Neural%20Network_Rojas.pdf;sequence=5", "isFamilyFriendly": true, "displayUrl": "https://upcommons.upc.edu/bitstream/handle/2117/188104/<b>Black</b>-<b>Box</b> Modelling of a DC-DC...", "snippet": "<b>black</b> <b>box</b> system. A Recurrent NARX NN (or NARX-<b>RNN</b>) arises as a solution for the identification of a <b>black</b>-<b>box</b> system performance. It <b>can</b> predict the output of a nonlinear system based on the expansion of past inputs and outputs. The model depends on the configuration of the system, and <b>can</b> be defined either by one of the two following equations:", "dateLastCrawled": "2022-02-02T15:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Difference between ANN, CNN and RNN - GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/difference-between-ann-cnn-and-rnn/", "isFamilyFriendly": true, "displayUrl": "https://www.geeksforgeeks.org/<b>difference-between-ann-cnn-and</b>-<b>rnn</b>", "snippet": "Differences between <b>Black</b> <b>Box</b> Testing vs White <b>Box</b> Testing; Stack vs Heap Memory Allocation; Differences between TCP and UDP; Differences between Procedural and Object Oriented Programming ; Differences between JDK, JRE and JVM; Difference between C and C++; Difference between Process and Thread; Difference between Structure and Union in C; Difference between Hardware and Software; Difference between Primary Key and Foreign Key; Web 1.0, Web 2.0 and Web 3.0 with their difference; Difference ...", "dateLastCrawled": "2022-02-02T05:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Meta-Learning for <b>Black</b>-<b>box</b> Optimization", "url": "https://www.ecmlpkdd2019.org/downloads/paper/576.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.ecmlpkdd2019.org/downloads/paper/576.pdf", "snippet": "domain constraints <b>compared</b> to an <b>RNN</b> optimizer not explicitly trained to utilize feedback. We refer to the proposed approach as <b>RNN</b>-Opt. As a result of the above con-siderations, <b>RNN</b>-Opt <b>can</b> deal with an unknown range of function values and also incorporate domain constraints. We demonstrate that <b>RNN</b>-Opt works well on optimizing unseen benchmark <b>black</b>-<b>box</b> functions and outperforms <b>RNN</b>-OI in terms of the optimal value attained under a limited budget for 2-dimensional and 6-dimensional input ...", "dateLastCrawled": "2022-01-26T07:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "[D] <b>Can</b> attention be computed implicitly by an <b>RNN</b>? : MachineLearning", "url": "https://www.reddit.com/r/MachineLearning/comments/bfnhv5/d_can_attention_be_computed_implicitly_by_an_rnn/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/.../bfnhv5/d_<b>can</b>_attention_be_computed_implicitly_by_an_<b>rnn</b>", "snippet": "[D] Proprietary <b>black</b>-<b>box</b> ML systems for decision-making in society A product, NarxCare, [1] uses ML to predict whether a patient is likely to be abusing prescription medication in the United States. Doctors in several states are legally required to consult NarxCare, and potentially base their decision on the outputted risk score, when prescribing controlled substances to patients.", "dateLastCrawled": "2021-08-15T20:09:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A Tour of <b>Recurrent Neural Network Algorithms for Deep Learning</b>", "url": "https://machinelearningmastery.com/recurrent-neural-network-algorithms-for-deep-learning/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>recurrent-neural-network-algorithms-for-deep-learning</b>", "snippet": "RNNs stand out from other <b>machine</b> <b>learning</b> methods for their ability to learn and carry out complicated transformations of data over extended periods of time. Moreover, it is known that RNNs are Turing-Complete and therefore have the capacity to simulate arbitrary procedures, if properly wired. The capabilities of standard RNNs are extended to simplify the solution of algorithmic tasks. This enrichment is primarily via a large, addressable memory, so, by <b>analogy</b> to Turing\u2019s enrichment of ...", "dateLastCrawled": "2022-02-02T22:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Deep Learning: Models for Sequence Data</b> (<b>RNN</b> and LSTM)", "url": "https://cse.iitk.ac.in/users/piyush/courses/ml_autumn16/771A_lec24_slides.pdf", "isFamilyFriendly": true, "displayUrl": "https://cse.iitk.ac.in/users/piyush/courses/ml_autumn16/771A_lec24_slides.pdf", "snippet": "Filters are like basis/dictionary (PCA <b>analogy</b>) Each lter is convolved over entire input to produce a feature map Nonlinearity and pooling and applied after each convolution layer Last layer (one that connects to outputs) is fully connected <b>Machine</b> <b>Learning</b> (CS771A) <b>Deep Learning: Models for Sequence Data</b> (<b>RNN</b> and LSTM) 3. Recap: Convolutional Neural Network Special type of feedforward neural nets (local connectivity + weight sharing) Each layer uses a set of \\ lters&quot; (basically, weights to ...", "dateLastCrawled": "2022-01-17T20:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Mathematical understanding of <b>RNN</b> and its variants - <b>GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/mathematical-understanding-of-rnn-and-its-variants/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/mathematical-understanding-of-<b>rnn</b>-and-its-variants", "snippet": "<b>RNN</b> is suitable for such work thanks to their capability of <b>learning</b> the context. Other applications include speech to text conversion, building virtual assistance, time-series stocks forecasting, sentimental analysis, language modelling and <b>machine</b> translation. On the other hand, a feed-forward neural network produces an output which only depends on the current input. Examples for such are image classification task, image segmentation or object detection task. One such type of such network ...", "dateLastCrawled": "2022-01-29T04:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Machine</b> <b>Learning</b> (ML) and Neural Networks (NN)\u2026 An Intuitive ...", "url": "https://medium.com/visionary-hub/machine-learning-ml-and-neural-networks-nn-an-intuitive-walkthrough-76bdaba8b0e3", "isFamilyFriendly": true, "displayUrl": "https://medium.com/visionary-hub/<b>machine</b>-<b>learning</b>-ml-and-neural-networks-nn-an...", "snippet": "A better <b>analogy</b> for unsupervised <b>learning</b>, and one that\u2019s more commonly used, is separating a group of blocks by colour. Suppose we have 10 blocks, each with different coloured faces. In the ...", "dateLastCrawled": "2022-01-30T17:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Chapter 8 Recurrent Neural Networks</b> | Deep <b>Learning</b> and its Applications", "url": "https://frcs.github.io/4C16-LectureNotes/recurrent-neural-networks.html", "isFamilyFriendly": true, "displayUrl": "https://frcs.github.io/4C16-LectureNotes/recurrent-neural-networks.html", "snippet": "In its simplest form, the inner structure of the hidden layer block is simply a dense layer of neurons with \\(\\mathrm{tanh}\\) activation. This is called a simple <b>RNN</b> architecture or Elman network.. We usually take a \\(\\mathrm{tanh}\\) activation as it can produce positive or negative values, allowing for increases and decreases of the state values. Also \\(\\mathrm{tanh}\\) bounds the state values between -1 and 1, and thus avoids a potential explosion of the state values.. The equations for ...", "dateLastCrawled": "2022-02-02T05:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>RNN</b>: Recurrent Neural Networks \u2013 KejiTech", "url": "https://davideliu.com/2020/02/29/rnn-recurrent-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://davideliu.com/2020/02/29/<b>rnn</b>-recurrent-neural-networks", "snippet": "A part of a neural network that preserves some state across time-steps is called a memory cell.Thus, a single layer of recurrent neurons such as h i is a very basic form of memory cell. Now, diving a little deeper into the mathematical details, it will result the new hidden state h i to be computed as:. Where i is the index of the time-step, x i is the corresponding input and W x is its associated weights matrix. Similarly, h i-1 is the previous hidden state and W h is its corresponding ...", "dateLastCrawled": "2022-01-26T14:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Visual Agnosia: A Neural Network Analogy</b> | by Aryan Singh | Towards ...", "url": "https://towardsdatascience.com/visual-agnosia-a-neural-network-analogy-8e208438b1ec", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>visual-agnosia-a-neural-network-analogy</b>-8e208438b1ec", "snippet": "In this case this <b>RNN</b> is an encoder decoder network with the encoder encoding the past actions into a context vector and inputting this to a decoder to forecast the next actions. In case of Dr. P, this particular context vector seems to be reinforced at each timestep by the music being generated by the brain. This music sort of act us an attention context that tells the brain to pay attention to what and when. But as soon as the music stops this context gets lost and the decoder stops ...", "dateLastCrawled": "2022-01-18T18:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Sentiment Analysis</b> from Tweets using Recurrent Neural Networks | by ...", "url": "https://medium.com/@gabriel.mayers/sentiment-analysis-from-tweets-using-recurrent-neural-networks-ebf6c202b9d5", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@gabriel.mayers/<b>sentiment-analysis</b>-from-tweets-using-recurrent...", "snippet": "LSTM Architeture. This is a variation from <b>RNN</b> and very powerful alternative when you need that your network is able to memorize information for a longer period of time. LSTM is based in gates ...", "dateLastCrawled": "2022-01-23T01:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Reservoir Computing Approaches to Recurrent Neural Network Training", "url": "https://www.ai.rug.nl/minds/uploads/2261_LukoseviciusJaeger09.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.ai.rug.nl/minds/uploads/2261_LukoseviciusJaeger09.pdf", "snippet": "network (<b>RNN</b>) training, where an <b>RNN</b> (the reservoir) is generated randomly and only a readout is trained. The paradigm, becoming known as reservoir computing, greatly facilitated the practical application of RNNs and outperformed classical fully trained RNNs in many tasks. It has lately become a vivid research eld with numerous extensions of the basic idea, including reservoir adaptation, thus broadening the initial paradigm to using di erent methods for training the reservoir and the ...", "dateLastCrawled": "2022-01-29T01:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "A Gentle Introduction to <b>Long Short-Term Memory</b> Networks by the Experts", "url": "https://machinelearningmastery.com/gentle-introduction-long-short-term-memory-networks-experts/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/gentle-introduction-<b>long-short-term-memory</b>-networks...", "snippet": "<b>Long Short-Term Memory</b> (LSTM) networks are a type of recurrent neural network capable of <b>learning</b> order dependence in sequence prediction problems. This is a behavior required in complex problem domains like <b>machine</b> translation, speech recognition, and more. LSTMs are a complex area of deep <b>learning</b>. It can be hard to get your hands around what LSTMs are, and how terms like bidirectional", "dateLastCrawled": "2022-01-31T03:31:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Introduction to Recurrent Neural Networks | <b>Machine</b> <b>Learning</b> lab", "url": "https://en.mlab.ai/blog/introduction-recurrent-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://en.mlab.ai/blog/introduction-recurrent-neural-networks", "snippet": "The <b>Machine</b> <b>Learning</b> Blog. 09/27/2018. Introduction to Recurrent Neural Networks In this article, I will explain what are Recurrent Neural Networks (RNN), how they work and what you can do with them. I will also show a very cool example of music generation using artificial intelligence. However, before discussing RNN, we need to explain the concept of sequence data. Sequence Data As the name indicates, sequence data is a collection of data in different states through time so it can form ...", "dateLastCrawled": "2022-02-01T17:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Notes on Recurrent Neural Networks</b> \u2013 humblesoftwaredev", "url": "https://humblesoftwaredev.wordpress.com/2016/12/04/notes-on-recurrent-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://humblesoftwaredev.wordpress.com/2016/12/04/<b>notes-on-recurrent-neural-networks</b>", "snippet": "Recurrent neural nets have states, unlike feed-forward networks. An analogy for RNN is the C strtok function, where calling it with the same parameter typically yields a different value (but of course, unlike strtok, RNN does not modify the input). An analogy for feed-forward networks is a function in the mathematical sense, where y=f(x) regardless of how many times\u2026", "dateLastCrawled": "2022-01-14T08:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Machine Learning for NLP</b> - Aurelie Herbelot", "url": "http://aurelieherbelot.net/resources/slides/teaching/RNNs.pdf", "isFamilyFriendly": true, "displayUrl": "aurelieherbelot.net/resources/slides/teaching/RNNs.pdf", "snippet": "An RNN, step by step Now we backpropagate through time. We need to compute gradients for three matrices: Why, Whh and Wxh. The gradient of matrix Why is straightforward \u2013 it is simply the sum", "dateLastCrawled": "2021-09-18T14:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "State-of-the-art in artificial <b>neural network applications</b>: A survey ...", "url": "https://www.sciencedirect.com/science/article/pii/S2405844018332067", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2405844018332067", "snippet": "Unlike a recurrent neural network, an <b>RNN is like</b> a hierarchical network where the input need processing hierarchically in the form of a tree because there is no time to the input sequence. 2.4. Deep <b>learning</b>. Artificial intelligence (AI) has existed over many decades, and the field is wide. AI can be view as a set that contains <b>machine</b> <b>learning</b> (ML), and deep <b>learning</b> (DL). The ML is a subset of AI, meanwhile, DL, in turn, a subset of ML. That is DL is an aspect of AI; the term deep ...", "dateLastCrawled": "2022-01-27T17:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>NLP - Transformers</b> | Blog Posts | Lumenci", "url": "https://www.lumenci.com/post/nlp-transformers", "isFamilyFriendly": true, "displayUrl": "https://www.lumenci.com/post/<b>nlp-transformers</b>", "snippet": "Thus, because weights are shared across time, <b>RNN is like</b> a state <b>machine</b> that takes actions temporally based on its historical sequential information. For example, RNN can be trained on a sequence of characters to generate the next character correctly. RNN - The activation at each time step is feedback to the next time step. For many years, RNN and its gated variants were the most popular architectures used for NLP. However, one of the main problems with RNN is the vanishing gradient ...", "dateLastCrawled": "2022-01-26T07:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Very simple example of RNN</b>? : learnmachinelearning", "url": "https://www.reddit.com/r/learnmachinelearning/comments/84bk5r/very_simple_example_of_rnn/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/learn<b>machinelearning</b>/comments/84bk5r/<b>very_simple_example_of_rnn</b>", "snippet": "basically, an <b>RNN is like</b> a regular layer (the dense layer where all neurons are connected to the next layer&#39;s neurons), except that it takes as an additional paramenter its own output from the previous training iteration.", "dateLastCrawled": "2021-01-08T07:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Deep <b>Learning Approaches for Phantom Movement Recognition</b>", "url": "https://www.researchgate.net/publication/336367291_Deep_Learning_Approaches_for_Phantom_Movement_Recognition", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/336367291_Deep_<b>Learning</b>_Approaches_for...", "snippet": "<b>RNN is, like</b> MLP, only. have good results for T A WD while other region successes are. far behind other algorithms. For <b>machine</b> <b>learning</b> algorithms, cross validation (k=10) is used to split the ...", "dateLastCrawled": "2022-01-04T05:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Artificial intelligence in drug design: algorithms, applications ...", "url": "https://www.future-science.com/doi/full/10.4155/fdd-2020-0028", "isFamilyFriendly": true, "displayUrl": "https://www.future-science.com/doi/full/10.4155/fdd-2020-0028", "snippet": "The discovery paradigm of drugs is rapidly growing due to advances in <b>machine</b> <b>learning</b> (ML) and artificial intelligence (AI). This review covers myriad faces of AI and ML in drug design. There is a plethora of AI algorithms, the most common of which are summarized in this review. In addition, AI is fraught with challenges that are highlighted along with plausible solutions to them. Examples are provided to illustrate the use of AI and ML in drug discovery and in predicting drug properties ...", "dateLastCrawled": "2022-01-29T02:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Applications of artificial intelligence in water treatment for ...", "url": "https://www.sciencedirect.com/science/article/pii/S1385894721015965", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1385894721015965", "snippet": "k-NN is a simple <b>machine</b> <b>learning</b> technique used for regression and classification. k-NN save all the existing data and perform classification on new data points on the basis of similarity .For example, consider a classification problem having two categories W and Z, as shown in Fig. 2. If a new data point occurred, having a placement issue with W and Z category, the new data point should be placed in a suitable category based on calculating Euclidean distance.", "dateLastCrawled": "2022-02-03T02:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>The future of AI music is Magenta</b> | DataDrivenInvestor", "url": "https://www.datadriveninvestor.com/2020/04/25/the-future-of-ai-music-is-magenta/", "isFamilyFriendly": true, "displayUrl": "https://www.datadriveninvestor.com/2020/04/25/<b>the-future-of-ai-music-is-magenta</b>", "snippet": "<b>The future of AI music is Magenta</b>. Music seems to be one of the fields that, at a surface level at least, AI just can\u2019t seem to penetrate. AI is rapidly taking over so many fields, and there\u2019s huge progress in music too! There are so many awesome developments (check out the app Transformer) and progress is moving at a breakneck pace.", "dateLastCrawled": "2022-01-28T07:09:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "End to end <b>machine</b> <b>learning</b> for fault detection and classification in ...", "url": "https://www.sciencedirect.com/science/article/pii/S0378779621004119", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0378779621004119", "snippet": "The training process for <b>RNN is similar</b> to traditional ANNs. However, since the parameters are shared among time instances in RNNs, the back-propagation algorithm for RNNs is termed as Backpropagation through time (BPTT) . As the number of time steps increase in RNN, it faces a problem termed as \u201cvanishing gradients\u201d due to which it cannot retain long term dependencies. Description can be seen in 39,40]. This phenomenon makes RNNs difficult to train and render them impractical in most of ...", "dateLastCrawled": "2021-12-14T22:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "2_tensorflow_lstm", "url": "http://ethen8181.github.io/machine-learning/deep_learning/rnn/2_tensorflow_lstm.html", "isFamilyFriendly": true, "displayUrl": "ethen8181.github.io/<b>machine</b>-<b>learning</b>/deep_<b>learning</b>/rnn/2_tensorflow_lstm.html", "snippet": "Training a <b>RNN is similar</b> to training a traditional Neural Network, we also use the backpropagation algorithm, but with a little twist. Because the parameters are shared by all time steps in the network, the gradient at each output depends not only on the calculations of the current time step, but also the previous time steps. For example, in order to calculate the gradient at t=4 we would need to backpropagate 3 steps and sum up the gradients. This is called Backpropagation Through Time ...", "dateLastCrawled": "2022-02-03T13:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Deep <b>Recurrent Neural Networks</b> with Keras | <b>Paperspace Blog</b>", "url": "https://blog.paperspace.com/advanced-recurrent-neural-networks-deep-rnns/", "isFamilyFriendly": true, "displayUrl": "https://blog.paperspace.com/advanced-<b>recurrent-neural-networks</b>-deep-rnns", "snippet": "The training of a deep <b>RNN is similar</b> to the Backpropagation Through Time (BPTT) algorithm, as in an RNN but with additional hidden units. Now that you\u2019ve got an idea of what a deep RNN is, in the next section we&#39;ll build a music generator using a deep RNN and Keras. Generating Music Using a Deep RNN. Music is the ultimate language. We have been creating and rendering beautiful melodies since time unknown. In this context, do you think a computer can generate musical notes comparable to ...", "dateLastCrawled": "2022-02-03T18:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Machine</b> <b>Learning</b> - <b>Kbeznak Parmatonic</b>", "url": "https://sites.google.com/view/kbeznak-parmatonic-guru-of-ml/home", "isFamilyFriendly": true, "displayUrl": "https://<b>sites.google.com</b>/view/<b>kbeznak-parmatonic</b>-guru-of-ml/home", "snippet": "Backpropagation in <b>RNN is similar</b> to Neural Network, but we have to take care of the weight with respect to all the time steps. So, the gradient has to be calculated for all those steps going backwards, this is called Backpropagation Through Time(BPTT). Software and Tools: <b>Kbeznak Parmatonic</b> prefers Tensorflow and Caffe2 for deeplearning, and keras would help you lot in the initial stages. Author <b>Kbeznak Parmatonic</b>: Dr. <b>Kbeznak Parmatonic</b>, was a chief scientist at NASA and was well deserved ...", "dateLastCrawled": "2021-12-23T18:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A Motor-Imagery BCI System Based on Deep <b>Learning</b> Networks and Its ...", "url": "https://www.intechopen.com/chapters/60241", "isFamilyFriendly": true, "displayUrl": "https://www.intechopen.com/chapters/60241", "snippet": "Training an <b>RNN is similar</b> to training a traditional neural network (TNN). Because RNNs trained by TNN\u2019s style have difficulties in <b>learning</b> long-term dependencies due to the vanishing and exploding gradient problem. LSTMs do not have a fundamentally different architecture from RNNs, but they use a different function to calculate the states in hidden layer. The memory in LSTMs is called cells and can be thought as black boxes that take as input the previous state and current input ...", "dateLastCrawled": "2022-02-02T11:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Review of Vibration-Based Structural Health Monitoring Using Deep <b>Learning</b>", "url": "https://www.mdpi.com/2076-3417/10/5/1680/htm", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/2076-3417/10/5/1680/htm", "snippet": "An <b>RNN is similar</b> to recurrent neural networks in that it is good at dealing with sequential data. Recurrent neural networks are also called RNNs in the literature; to distinguish between the architectures, only the recursive neural network is abbreviated as RNN in this paper. An RNN models hierarchical structures in a tree fashion, which is overly time-consuming and costly. This has led to a lack of attention being given to RNNs. Because an RNN processes all information of the input ...", "dateLastCrawled": "2022-01-12T00:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Deep Neural Network</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/computer-science/deep-neural-network", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/computer-science/<b>deep-neural-network</b>", "snippet": "This dataset is designed for <b>machine</b> <b>learning</b> classification tasks and includes 60,000 training and 10,000 test gray scale images composed of 28-by-28 pixels. Every training and test case is related to one of ten labels (0\u20139). Zalando\u2019s new dataset is mainly the same as the original handwritten digits data. But instead of having images of the digits 0\u20139, Zalando\u2019s data involves images with 10 different fashion products. Hence the dataset is named fashion-MNIST dataset and can be ...", "dateLastCrawled": "2022-01-30T06:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Deep Learning</b> - SlideShare", "url": "https://www.slideshare.net/JunWang5/deep-learning-61493694", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/JunWang5/<b>deep-learning</b>-61493694", "snippet": "\u2022 ClockWork-<b>RNN is similar</b> to a simple RNN with an input, output and hidden layer \u2022 Difference lies in \u2013 The hidden layer is partitioned into g modules each with its own clock rate \u2013 Neurons in faster module are connected to neurons in a slower module RNN applications: time series Koutnik, Jan, et al. &quot;A clockwork rnn.&quot; arXiv preprint arXiv:1402.3511 (2014). A Clockwork RNN Figure 1. CW-RNN architecture is similar to a simple RNN with an input, output and hidden layer. The hidden ...", "dateLastCrawled": "2022-01-31T18:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Deep Learning</b> for Geophysics: Current and Future Trends - Yu - 2021 ...", "url": "https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2021RG000742", "isFamilyFriendly": true, "displayUrl": "https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2021RG000742", "snippet": "Different from traditional model-driven methods, <b>machine</b> <b>learning</b> (ML) is a type of data-driven approach that trains a regression or classification model through a complex nonlinear mapping with adjustable parameters based on a training data set. The comparison of model-driven and data-driven approaches is summarized in Figure 1. For decades, ML methods have been widely adopted in various geophysical applications, such as exploration geophysics (Huang et al., 2006; Helmy et al., 2010; Jia ...", "dateLastCrawled": "2022-01-31T08:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Different Architecture of Deep <b>Learning</b> Algorithms Extensive number of ...", "url": "https://www.researchgate.net/figure/Different-Architecture-of-Deep-Learning-Algorithms-Extensive-number-of-deep-learning_fig1_324149367", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/figure/Different-Architecture-of-Deep-<b>Learning</b>-Algorithms...", "snippet": "Unlike classical <b>machine</b> <b>learning</b> (support vector <b>machine</b>, k-nearest neighbour, k-mean, etc.) that require a human engineered feature to perform optimally (LeCun, et al., 2015). Over the years ...", "dateLastCrawled": "2022-01-29T15:47:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Towards deep entity resolution via soft schema matching - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S0925231221016635", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0925231221016635", "snippet": "Technically, TLM is a new fundamental architecture for deep ER, <b>just as RNN</b>. Our work and TLM based approaches falls into different lines of deep ER research, which are orthogonal and complementary to each other. Our major contribution is proposing soft schema mapping and incorporating it into (RNN based) deep ER models, which does not require huge amounts of NLP corpora for pre-training, while TLM based approaches exploit the deeper language understanding capability from tremendously pre ...", "dateLastCrawled": "2022-01-21T02:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Positional encoding, residual connections, padding masks</b>: covering the ...", "url": "https://data-science-blog.com/blog/2021/04/22/positional-encoding-residual-connections-padding-masks-all-the-details-of-transformer-model/", "isFamilyFriendly": true, "displayUrl": "https://data-science-blog.com/blog/2021/04/22/positional-encoding-residual-connections...", "snippet": "Transformer decoder also predicts the output sequences autoregressively one token at a time step, <b>just as RNN</b> decoders. I think it easy to understand this process because RNN decoder generates tokens just as you connect RNN cells one after another, like connecting rings to a chain. In this way it is easy to make sure that generating of one token in only affected by the former tokens. On the other hand, during training Transformer decoders, you input the whole sentence at once. That means ...", "dateLastCrawled": "2022-01-30T16:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Machine</b> <b>Learning</b> Archives - Data Science Blog", "url": "https://data-science-blog.com/blog/category/main-category/machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://data-science-blog.com/blog/category/main-category/<b>machine</b>-<b>learning</b>", "snippet": "Most <b>machine</b> <b>learning</b> algorithms covered by major introductory textbooks tend to be too deterministic and dependent on the size of data. Many of those algorithms have another \u201cparallel world,\u201d where you can handle inaccuracy in better ways. I hope I can also write about them, and I might prepare another trilogy for such PCA. But I will not disappoint you, like \u201cThe Phantom Menace.\u201d Appendix: making a model of a bunch of grape with ellipsoid berries. If you can control quadratic ...", "dateLastCrawled": "2022-01-05T04:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "1561982779 | PDF | Equity Crowdfunding | Investor", "url": "https://www.scribd.com/document/550868164/1878586842-1561982779", "isFamilyFriendly": true, "displayUrl": "https://www.scribd.com/document/550868164/1878586842-1561982779", "snippet": "Scribd is the world&#39;s largest social reading and publishing site.", "dateLastCrawled": "2022-01-25T03:48:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Recurrent Neural Networks and LSTM explained", "url": "https://dhrubajitdas44.blogspot.com/2018/10/recurrent-neural-networks-and-lstm.html", "isFamilyFriendly": true, "displayUrl": "https://dhrubajitdas44.blogspot.com/2018/10/recurrent-neural-networks-and-lstm.html", "snippet": "A <b>RNN can be thought of as</b> multiple copies of the same network , each passing message to . the next. Because of their internal memory, RNN\u2019s are able to remember important things about the input they received, which enables them to be very precise in predicting what\u2019s coming next. This is the reason why they are the preferred algorithm for sequential data like time series, speech, text, financial data, audio, video, weather and much more because they can form a much deeper understanding ...", "dateLastCrawled": "2022-01-10T10:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Decoding Your Genes</b>. Can Neural Networks Unravel The Secrets\u2026 | by ...", "url": "https://towardsdatascience.com/decoding-your-genes-4a23e89aba98", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>decoding-your-genes</b>-4a23e89aba98", "snippet": "Conceptually, an <b>RNN can be thought of as</b> a connected sequence of feed-forward networks with information passed between them. The information being passed is the hidden-state which represents all the previous inputs to the network. At each step of the RNN, the hidden state generated from the previous step is passed in, as well as the next sequence input. This then returns an output as well as the new hidden state to be passed on again. This allows the RNN to retain a \u2018memory\u2019 of the ...", "dateLastCrawled": "2022-01-26T03:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "PHC6937-<b>Machine</b> <b>Learning</b>-Guest Lecture", "url": "https://slides.com/benh-hu/phc6937machinelearning", "isFamilyFriendly": true, "displayUrl": "https://slides.com/benh-hu/phc6937<b>machinelearning</b>", "snippet": "<b>Machine</b> <b>learning</b> is predicated on this idea of <b>learning</b> from example ... A <b>RNN can be thought of as</b> the addition of loops to the archetecture of a standard feedforward NN - the output of the network may feedback as an input to the network with the next input vector, and so on The recurrent connections add state or memory to the network and allow it to learn broader abstractions from the input sequences; Reading. PHC6937-<b>Machine</b> <b>Learning</b>-Guest Lecture. By Hui Hu. PHC6937-<b>Machine</b> <b>Learning</b> ...", "dateLastCrawled": "2022-01-25T13:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Using RNNs for <b>Machine Translation</b> | by Aryan Misra | Towards Data Science", "url": "https://towardsdatascience.com/using-rnns-for-machine-translation-11ddded78ddf", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/using-rnns-for-<b>machine-translation</b>-11ddded78ddf", "snippet": "3. Sequence to Sequence. The RNN takes in an input sequence and outputs a sequence. <b>Machine Translation</b>: an RNN reads a sentence in one language and then outputs it in another. This should help you get a high-level understanding of RNNs, if you want to learn more about the math behind the operations an RNN performs, I recommend you check out ...", "dateLastCrawled": "2022-02-01T23:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Time series prediction of COVID-19 transmission in America using LSTM ...", "url": "https://www.sciencedirect.com/science/article/pii/S2211379721005775", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2211379721005775", "snippet": "The <b>machine</b> <b>learning</b> algorithm XGBoost was employed to build the models to predict the criticality , mortality , and ... RNNs can use their internal state (memory) to process variable length sequences of inputs. A <b>RNN can be thought of as</b> multiple copies of the same network, each passing a message to a successor (see Fig. 4). They might be able to connect previous information to the present task. However, as that gap grows, RNNs become unable to learn to connect the information. The short ...", "dateLastCrawled": "2022-01-24T06:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "[DL] 11. RNN <b>2(Bidirectional, Deep RNN, Long term connection</b>) | by Jun ...", "url": "https://medium.com/jun-devpblog/dl-11-rnn-2-bidirectional-deep-rnn-long-term-connection-8a836a7f2260", "isFamilyFriendly": true, "displayUrl": "https://medium.com/jun-devpblog/dl-11-rnn-<b>2-bidirectional-deep-rnn-long-term</b>...", "snippet": "Basically, Bidirectional <b>RNN can be thought of as</b> two RNNs in a network, one is moving forwards in time and the other one is moving backward and both are contributing to producing output ...", "dateLastCrawled": "2021-08-12T14:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Convolutional Neural Network and RNN</b> for OCR problem.", "url": "https://www.slideshare.net/vishalmishra982/convolutional-neural-network-and-rnn-for-ocr-problem-86087045", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/vishalmishra982/<b>convolutional-neural-network-and-rnn</b>-for...", "snippet": "Sequence-to-Sequence <b>Learning</b> using Deep <b>Learning</b> for Optical Character Recognition. ... <b>RNN can be thought of as</b> multiple copies of the same network, each passing a message to a successor. An unrolled RNN is shown below. \u2022 In fast last few years, there have been incredible success applying RNNs to a variety of problems: speech recognition, language modeling, translation, image captioning\u2026. The list goes on. An Unrolled RNN 44. DRAWBACK OF AN RNN \u2022 RNN has a problem of long term ...", "dateLastCrawled": "2022-01-17T23:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A diagram of (a) the RNN and its (b) unrolled version. | Download ...", "url": "https://researchgate.net/figure/A-diagram-of-a-the-RNN-and-its-b-unrolled-version_fig1_342349801", "isFamilyFriendly": true, "displayUrl": "https://researchgate.net/figure/A-diagram-of-a-the-RNN-and-its-b-unrolled-version_fig1...", "snippet": "Download scientific diagram | A diagram of (a) the RNN and its (b) unrolled version. from publication: ML-descent: an optimization algorithm for FWI using <b>machine</b> <b>learning</b> | Full-waveform ...", "dateLastCrawled": "2021-06-06T20:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) Remaining useful life prediction of PEMFC based on long short ...", "url": "https://www.researchgate.net/publication/328587416_Remaining_useful_life_prediction_of_PEMFC_based_on_long_short-term_memory_recurrent_neural_networks", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/328587416_Remaining_useful_life_prediction_of...", "snippet": "LSTM <b>RNN can be thought of as</b> a series of BPNN with equal. Fig. 10 e Prognostic results of LSTM RNN at T. p. \u00bc 550 h. Fig. 11 e System training loss and test loss. Table 3 e Prediction results of ...", "dateLastCrawled": "2022-01-29T16:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>How I Used Deep Learning To Train A Chatbot</b> To Talk Like Me (Sorta ...", "url": "https://adeshpande3.github.io/How-I-Used-Deep-Learning-to-Train-a-Chatbot-to-Talk-Like-Me", "isFamilyFriendly": true, "displayUrl": "https://adeshpande3.github.io/<b>How-I-Used-Deep-Learning-to-Train-a-Chatbot</b>-to-Talk-Like-Me", "snippet": "This paper showed great results in <b>machine</b> translation specifically, but Seq2Seq models have grown to encompass a variety of NLP tasks. ... By this logic, the final hidden state vector of the encoder <b>RNN can be thought of as</b> a pretty accurate representation of the whole input text. The decoder is another RNN, which takes in the final hidden state vector of the encoder and uses it to predict the words of the output reply. Let&#39;s look at the first cell. The cell&#39;s job is to take in the vector ...", "dateLastCrawled": "2022-01-30T02:41:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(rnn)  is like +(black box)", "+(rnn) is similar to +(black box)", "+(rnn) can be thought of as +(black box)", "+(rnn) can be compared to +(black box)", "machine learning +(rnn AND analogy)", "machine learning +(\"rnn is like\")", "machine learning +(\"rnn is similar\")", "machine learning +(\"just as rnn\")", "machine learning +(\"rnn can be thought of as\")", "machine learning +(\"rnn can be compared to\")"]}