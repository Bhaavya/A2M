{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Details for Neural Network <b>Self Attention</b> and Related Queries", "url": "https://www.affiliatejoin.com/neural-network-self-attention", "isFamilyFriendly": true, "displayUrl": "https://www.affiliatejoin.com/neural-network-<b>self-attention</b>", "snippet": "<b>Self-Attention</b> <b>Self-attention</b>, <b>also</b> known as intra-<b>attention</b>, is an <b>attention</b> mechanism relating different positions of a single sequence in order to compute a representation of the same sequence. It has been shown to be very useful in machine reading, abstractive summarization, or image description generation.", "dateLastCrawled": "2022-02-02T09:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) Self-Awareness Safety of Deep Reinforcement Learning in <b>Road</b> ...", "url": "https://www.researchgate.net/publication/357987847_Self-Awareness_Safety_of_Deep_Reinforcement_Learning_in_Road_Traffic_Junction_Driving", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/357987847_Self-Awareness_Safety_of_Deep...", "snippet": "the proposed <b>self-attention</b> architecture with a baseline DRL model to build a self-awareness safety DRL, such as <b>attention</b>- DQN, to evaluate the safety performance of the ego v ehicle", "dateLastCrawled": "2022-01-27T14:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Blog - Page 4 of 10 - Combine | Combine", "url": "https://combine.se/blog/page/4/", "isFamilyFriendly": true, "displayUrl": "https://combine.se/blog/page/4", "snippet": "encoder/decoder in the Transformer contains a <b>self-attention</b> <b>layer</b>, which aims to determine which part of the sequence is most important when processing a particular word, e.g. in the sentence \u201cJames enjoys the beach because he likes to swim\u201d, the <b>self-attention</b> <b>layer</b> should learn to link the word \u201che\u201d to \u201cJames\u201d as the most important for its embedding. Additionally, each decoder contains an \u201cEncoder-Decoder <b>Attention</b>\u201d <b>layer</b>, which refers to the relative importance of each ...", "dateLastCrawled": "2022-01-30T08:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Explainable deep learning for efficient and robust pattern recognition ...", "url": "https://www.sciencedirect.com/science/article/pii/S0031320321002892", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0031320321002892", "snippet": "Gao et al. proposed a <b>self-attention</b> driven adversarial similarity learning network to reasonably measure the relevance between given objects. To demonstrate the part-level similarity of objects, discriminative <b>self-attention</b> weights were assigned to different parts of objects before similarity learning. To enhance the semantic interpretability of the similarity score, the topic vectors were introduced to distinguish similar or dissimilar object pairs accurately. A generator-discriminator ...", "dateLastCrawled": "2022-01-27T08:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "MST: Masked Self-Supervised Transformer for Visual Representation ...", "url": "https://www.arxiv-vanity.com/papers/2106.05656/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2106.05656", "snippet": "Inspired by the MLM in NLP, the <b>attention</b>-guided mask strategy is first introduced to mask the tokens of the student network based on the output <b>self-attention</b> map of the teacher network. The basic principle is to mask some patches with low responses and does not destroying the important foreground regions. Then, a global image decoder is used to reconstruct the original image based on the masked and unmasked tokens. Finally, the total loss function consists of the self-supervised cross ...", "dateLastCrawled": "2022-01-29T16:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Knowledge Distillation</b>: A Survey | SpringerLink", "url": "https://link.springer.com/article/10.1007/s11263-021-01453-z", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11263-021-01453-z", "snippet": "In recent years, deep neural networks have been successful in both industry and academia, especially for computer vision tasks. The great success of deep learning is mainly due to its scalability to encode large-scale data and to maneuver billions of model parameters. However, it is a challenge to deploy these cumbersome deep models on devices with limited resources, e.g., mobile phones and embedded devices, not only because of the high computational complexity but <b>also</b> the large storage ...", "dateLastCrawled": "2022-01-28T19:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "My_<b>Bibliography_for_Research_on_Autonomous</b>_<b>Driving</b>/2_bc_end2end.md at ...", "url": "https://github.com/chauvinSimon/My_Bibliography_for_Research_on_Autonomous_Driving/blob/master/sections/2_bc_end2end.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/chauvinSimon/My_<b>Bibliography_for_Research_on_Autonomous</b>_<b>Driving</b>/...", "snippet": "&quot;Graph <b>attention</b> network (GAT) is a GCN variant which aggregates node information with weights learned in a <b>self-attention</b> mechanism. Such adaptiveness of GAT makes it more effective than GCN in graph representation learning.&quot; GAT is used here to model the interaction among <b>road</b> agents during <b>driving</b>, which is composed of multiple graph layers.", "dateLastCrawled": "2022-01-29T01:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>New submissions for Fri, 31 Jul</b> 20 \u00b7 Issue #139 \u00b7 kobiso/daily-arxiv ...", "url": "https://github.com/kobiso/daily-arxiv-noti/issues/139", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/kobiso/daily-arxiv-noti/issues/139", "snippet": "We evaluate our method by training a <b>Self-Attention</b> GAN on ImageNet at 64x64 resolution, where we outperform the current state-of-the-art models on this task <b>while</b> using 1/2 of the parameters. We <b>also</b> highlight training time savings by training a BigGAN on ImageNet at 128x128 resolution, achieving a 66% increase in Inception Score and a 16% improvement in FID over the baseline model with less than 1/4 the training time. Growing Efficient Deep Networks by Structured Continuous Sparsification ...", "dateLastCrawled": "2021-08-12T04:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "ICMR &#39;21: Proceedings of the 2021 International Conference on ...", "url": "http://www.sigmm.org/opentoc/ICMR2021-TOC", "isFamilyFriendly": true, "displayUrl": "www.sigmm.org/opentoc/ICMR2021-TOC", "snippet": "To solve this problem, most approaches try to learn a joint embedding space to measure the cross-modal similarities, <b>while</b> <b>paying</b> little <b>attention</b> to the representation of each modality. Video is more complicated than the commonly used visual feature, since the audio and caption on the screen <b>also</b> contain rich information. Recently, the aggregations of multiple features in videos boost the benchmark of the video-text retrieval system. However, they usually handle each feature independently ...", "dateLastCrawled": "2022-02-02T21:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Climate Change AI Workshop Papers | Climate Change AI", "url": "https://www.climatechange.ai/papers", "isFamilyFriendly": true, "displayUrl": "https://www.climatechange.ai/papers", "snippet": "I bridge ideas from ensemble data assimilation with <b>self-attention</b>, resulting into the self-attentive ensemble transformer. Here, interactions between ensemble members are represented as additive and dynamic self-attentive part. As proof-of-concept, I regress global ECMWF ensemble forecasts to 2-metre-temperature fields from the ERA5 reanalysis. I demonstrate that the ensemble transformer can calibrate the ensemble spread and extract additional information from the ensemble. As it is a ...", "dateLastCrawled": "2022-01-26T11:09:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Knowledge Distillation</b>: A Survey | SpringerLink", "url": "https://link.springer.com/article/10.1007/s11263-021-01453-z", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11263-021-01453-z", "snippet": "<b>Similar</b> to the self-distillation in Zhang et al. , a <b>self-attention</b> distillation method was proposed for lane detection (Hou et al. 2019). The network utilizes the <b>attention</b> maps of its own layers as distillation targets for its lower layers.", "dateLastCrawled": "2022-01-28T19:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A hierarchical temporal <b>attention</b>-based LSTM encoder-decoder model for ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7252178/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7252178", "snippet": "<b>Similar</b> to one-<b>layer</b> temporal <b>attention</b>, we combine H t \u2032 ^ and y t \u2032 using , and use the combined value y t \u2032 ^ for y t + 1 \u2032 prediction through . H t \u2032 ^ = \u2211 d = 1 N d u t d \u2032 d \u00b7 H t d \u2032 u t d \u2032 d = V a T tanh (W a \u2032 [h t \u2212 1 \u2032; c t \u2212 1 \u2032] + W a H t d \u2032 + b a) (11) u t d \u2032 d = e u t d \u2032 d \u2211 d = 1 N d e u t d \u2032 d y t \u2032 ^ = W c ^ [y t \u2032; H t \u2032 ^] (12) where N d is the number of days used for global <b>attention</b> computation. In our case, N d equals ...", "dateLastCrawled": "2022-01-28T00:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) Self-Awareness Safety of Deep Reinforcement Learning in <b>Road</b> ...", "url": "https://www.researchgate.net/publication/357987847_Self-Awareness_Safety_of_Deep_Reinforcement_Learning_in_Road_Traffic_Junction_Driving", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/357987847_Self-Awareness_Safety_of_Deep...", "snippet": "the proposed <b>self-attention</b> architecture with a baseline DRL model to build a self-awareness safety DRL, such as <b>attention</b>- DQN, to evaluate the safety performance of the ego v ehicle", "dateLastCrawled": "2022-01-27T14:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Double Similarity Distillation for Semantic Image Segmentation | DeepAI", "url": "https://deepai.org/publication/double-similarity-distillation-for-semantic-image-segmentation", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/double-<b>similar</b>ity-distillation-for-semantic-image...", "snippet": "How to improve the performance of compact semantic image segmentation networks has received increased <b>attention</b>. The knowledge distillation strategy [] has been investigated as a good alternative, which is a popular solution to improve the performance of compact models (<b>also</b> <b>called</b> student networks) by transferring knowledge from the cumbersome models (<b>also</b> <b>called</b> teacher networks). The effectiveness of this solution has been verified in several fields, such as image classification [23, 24 ...", "dateLastCrawled": "2022-01-27T12:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Blog - Page 4 of 10 - Combine | Combine", "url": "https://combine.se/blog/page/4/", "isFamilyFriendly": true, "displayUrl": "https://combine.se/blog/page/4", "snippet": "encoder/decoder in the Transformer contains a <b>self-attention</b> <b>layer</b>, which aims to determine which part of the sequence is most important when processing a particular word, e.g. in the sentence \u201cJames enjoys the beach because he likes to swim\u201d, the <b>self-attention</b> <b>layer</b> should learn to link the word \u201che\u201d to \u201cJames\u201d as the most important for its embedding. Additionally, each decoder contains an \u201cEncoder-Decoder <b>Attention</b>\u201d <b>layer</b>, which refers to the relative importance of each ...", "dateLastCrawled": "2022-01-30T08:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "IJGI | Free Full-Text | Location Extraction and Prediction Method Based ...", "url": "https://www.mdpi.com/2220-9964/9/5/302/htm", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/2220-9964/9/5/302/htm", "snippet": "The <b>attention</b> coefficient of the proposed <b>self-attention</b> mechanism (SAM) can be calculated via azimuth information, which can <b>also</b> be obtained from the original trajectory sequence. After that, a feature vector of the network input <b>layer</b> can be obtained by concatenating the sub-tracks with the trajectory description information, which goes through the embedding <b>layer</b>, and the <b>attention</b> coefficients are added into the RNN network; <b>also</b>, the <b>attention</b> mechanism is adopted to make the network ...", "dateLastCrawled": "2021-12-02T01:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "MST: Masked Self-Supervised Transformer for Visual Representation ...", "url": "https://www.arxiv-vanity.com/papers/2106.05656/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2106.05656", "snippet": "Inspired by the MLM in NLP, the <b>attention</b>-guided mask strategy is first introduced to mask the tokens of the student network based on the output <b>self-attention</b> map of the teacher network. The basic principle is to mask some patches with low responses and does not destroying the important foreground regions. Then, a global image decoder is used to reconstruct the original image based on the masked and unmasked tokens. Finally, the total loss function consists of the self-supervised cross ...", "dateLastCrawled": "2022-01-29T16:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Journal of Intelligent</b> &amp; Fuzzy Systems - Volume Pre-press, issue Pre ...", "url": "https://content.iospress.com/journals/journal-of-intelligent-and-fuzzy-systems/Pre-press/Pre-press?start=0&rows=100", "isFamilyFriendly": true, "displayUrl": "https://content.iospress.com/journals/<b>journal-of-intelligent</b>-and-fuzzy-systems/Pre...", "snippet": "This paper presents a multi-headed <b>self-attention</b> mechanism implemented in the BiLSTM-CRF neural network structure to recognize Arabic \u2026 named entities on social media using two embeddings. Unlike other state-of-the-art approaches, this approach combines character and word embedding at the embedding <b>layer</b>, and the <b>attention</b> mechanism calculates the similarity over the entire sequence of characters and captures local context information. The proposed approach better recognized NEs in ...", "dateLastCrawled": "2022-02-01T23:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "My_<b>Bibliography_for_Research_on_Autonomous</b>_<b>Driving</b>/2_bc_end2end.md at ...", "url": "https://github.com/chauvinSimon/My_Bibliography_for_Research_on_Autonomous_Driving/blob/master/sections/2_bc_end2end.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/chauvinSimon/My_<b>Bibliography_for_Research_on_Autonomous</b>_<b>Driving</b>/...", "snippet": "&quot;Graph <b>attention</b> network (GAT) is a GCN variant which aggregates node information with weights learned in a <b>self-attention</b> mechanism. Such adaptiveness of GAT makes it more effective than GCN in graph representation learning.&quot; GAT is used here to model the interaction among <b>road</b> agents during <b>driving</b>, which is composed of multiple graph layers.", "dateLastCrawled": "2022-01-29T01:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "ICMR &#39;21: Proceedings of the 2021 International Conference on ...", "url": "http://www.sigmm.org/opentoc/ICMR2021-TOC", "isFamilyFriendly": true, "displayUrl": "www.sigmm.org/opentoc/ICMR2021-TOC", "snippet": "To solve this problem, most approaches try to learn a joint embedding space to measure the cross-modal similarities, <b>while</b> <b>paying</b> little <b>attention</b> to the representation of each modality. Video is more complicated than the commonly used visual feature, since the audio and caption on the screen <b>also</b> contain rich information. Recently, the aggregations of multiple features in videos boost the benchmark of the video-text retrieval system. However, they usually handle each feature independently ...", "dateLastCrawled": "2022-02-02T21:20:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Details for Neural Network <b>Self Attention</b> and Related Queries", "url": "https://www.affiliatejoin.com/neural-network-self-attention", "isFamilyFriendly": true, "displayUrl": "https://www.affiliatejoin.com/neural-network-<b>self-attention</b>", "snippet": "<b>Self-Attention</b> <b>Self-attention</b>, <b>also</b> known as intra-<b>attention</b>, is an <b>attention</b> mechanism relating different positions of a single sequence in order to compute a representation of the same sequence. It has been shown to be very useful in machine reading, abstractive summarization, or image description generation.", "dateLastCrawled": "2022-02-02T09:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) Self-Awareness Safety of Deep Reinforcement Learning in <b>Road</b> ...", "url": "https://www.researchgate.net/publication/357987847_Self-Awareness_Safety_of_Deep_Reinforcement_Learning_in_Road_Traffic_Junction_Driving", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/357987847_Self-Awareness_Safety_of_Deep...", "snippet": "the proposed <b>self-attention</b> architecture with a baseline DRL model to build a self-awareness safety DRL, such as <b>attention</b>- DQN, to evaluate the safety performance of the ego v ehicle", "dateLastCrawled": "2022-01-27T14:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Details for Dual <b>Attention</b> Network and Related Queries", "url": "https://www.affiliatejoin.com/dual-attention-network", "isFamilyFriendly": true, "displayUrl": "https://www.affiliatejoin.com/dual-<b>attention</b>-network", "snippet": "Introduction We propose a Dual <b>Attention</b> Network (DANet) to adaptively integrate local features with their global dependencies based on the <b>self-attention</b> mechanism. And we achieve new state-of-the-art segmentation performance on three challenging scene segmentation datasets, i.e., Cityscapes, PASCAL Context and COCO Stuff-10k dataset.", "dateLastCrawled": "2022-01-14T03:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "NeurIPS 2021 papers", "url": "https://tanelp.github.io/neurips2021/", "isFamilyFriendly": true, "displayUrl": "https://tanelp.github.io/neurips2021", "snippet": "The key idea is to treat the <b>self-attention</b> mechanism as a conditional expectation over embeddings at each location, and approximate the conditional distribution with a structured factorization. Each location <b>can</b> attend to all other locations, either via direct <b>attention</b>, or through indirect <b>attention</b> to abstractions, which are again conditional expectations of embeddings from corresponding local regions. We show that most sparse <b>attention</b> patterns used in existing sparse transformers are ...", "dateLastCrawled": "2022-02-03T00:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>New submissions for Tue, 23 Mar</b> 21 \u00b7 Issue #297 \u00b7 kobiso/daily-arxiv ...", "url": "https://github.com/kobiso/daily-arxiv-noti/issues/297", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/kobiso/daily-arxiv-noti/issues/297", "snippet": "This fact demonstrates that in deeper layers of ViTs, the <b>self-attention</b> mechanism fails to learn effective concepts for representation learning and hinders the model from getting expected performance gain. Based on above observation, we propose a simple yet effective method, named Re-<b>attention</b>, to re-generate the <b>attention</b> maps to increase their diversity at different layers with negligible computation and memory cost. The pro-posed method makes it feasible to train deeper ViT models with ...", "dateLastCrawled": "2021-10-16T23:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Meditation: a Conversational Model</b> \u2013 Matthew Remski", "url": "http://matthewremski.com/wordpress/meditation-a-conversational-model/", "isFamilyFriendly": true, "displayUrl": "matthewremski.com/wordpress/<b>meditation-a-conversational-model</b>", "snippet": "It <b>can</b> be helpful to view meditation as the gradual process of improving numerous layers of internal conversation between the \u201cfeeling-self\u201d and the \u201cconscious-self\u201d. The practice may not be attainable or appropriate for those recovering from traumas that have left these selves distrustful of each other, or that have forced the conscious-self to dissociate from the feeling-self to remain safe. For those privileged enough to practice, improved conversation may be felt physiologically ...", "dateLastCrawled": "2021-12-06T18:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "What is &#39;<b>attention</b>&#39; in the context of deep learning? - Quora", "url": "https://www.quora.com/What-is-attention-in-the-context-of-deep-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>attention</b>-in-the-context-of-deep-learning", "snippet": "Answer (1 of 5): In feed-forward deep networks, the entire input is presented to the network, which computes an output in one pass. In recurrent networks, new inputs <b>can</b> be presented at each time step, and the output of the previous time step <b>can</b> be used as an input to the network. This <b>can</b> be ...", "dateLastCrawled": "2022-01-15T04:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "New submissions for Tue, 23 Nov 21 \u00b7 Issue #468 \u00b7 kobiso/daily-arxiv ...", "url": "https://github.com/kobiso/daily-arxiv-noti/issues/468", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/kobiso/daily-arxiv-noti/issues/468", "snippet": "Rather than training a robust model to resist adversarial patches which may inevitably sacrifice accuracy, PatchVeto reuses a pretrained ViT model without any additional training, which <b>can</b> achieve high accuracy on clean inputs <b>while</b> detecting adversarial patched inputs by simply manipulating the <b>attention</b> map of ViT. Specifically, each input is tested by voting over multiple inferences with different <b>attention</b> masks, where at least one inference is guaranteed to exclude the adversarial ...", "dateLastCrawled": "2021-12-17T04:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) Neural network modelling of cognitive disinhibition and ...", "url": "https://www.academia.edu/2726585/Neural_network_modelling_of_cognitive_disinhibition_and_neurotransmitter_dysfunction_in_obsessive_compulsive_disorder", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/2726585/Neural_network_modelling_of_cognitive_disinhibition...", "snippet": "In recent years there has been a dramatic revolution in our conceptualization of obsessive\u00b1compulsive disorder (OCD). OCD has long been considered a prototypical psychogenic condition, one that allowed an important window onto the workings of the", "dateLastCrawled": "2022-01-23T04:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "/robowaifu/ - AI Design principles and philosophy", "url": "https://alogs.theguntretort.com/robowaifu/res/27.html", "isFamilyFriendly": true, "displayUrl": "https://alogs.theguntretort.com/robowaifu/res/27.html", "snippet": "The paper is basically a jab at the Transformer paper and shows that simple neural networks we&#39;ve been using for decades perform nearly as well without <b>self-attention</b>, <b>while</b> using other recent advances in machine learning like <b>layer</b> normalization and GELU as a non-linearity, which Transformers <b>also</b> use. What I take from it is that <b>self-attention</b> is incredibly efficient for small models but becomes wasted compute as the model scales. In a way it confirms what the Linformer paper found that ...", "dateLastCrawled": "2022-01-05T19:20:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Details for Neural Network <b>Self Attention</b> and Related Queries", "url": "https://www.affiliatejoin.com/neural-network-self-attention", "isFamilyFriendly": true, "displayUrl": "https://www.affiliatejoin.com/neural-network-<b>self-attention</b>", "snippet": "<b>Self-Attention</b> <b>Self-attention</b>, <b>also</b> known as intra-<b>attention</b>, is an <b>attention</b> mechanism relating different positions of a single sequence in order to compute a representation of the same sequence. It has been shown to be very useful in machine reading, abstractive summarization, or image description generation.", "dateLastCrawled": "2022-02-02T09:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A hierarchical temporal <b>attention</b>-based LSTM encoder-decoder model for ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7252178/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7252178", "snippet": "Meanwhile, next location prediction may work well in instantaneous applications, <b>while</b> long-term prediction is <b>also</b> needed to achieve a better long-term planning. Long Short-Term Memory (LSTM) have been used for individual mobility prediction, however it may fail to capture long-term dependencies when the length of input sequence increases . In addition, LSTM cannot uncover the mobility regularities hidden in the black-box framework, which is useful for understanding of travel behaviors ...", "dateLastCrawled": "2022-01-28T00:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Blog - Page 4 of 10 - Combine | Combine", "url": "https://combine.se/blog/page/4/", "isFamilyFriendly": true, "displayUrl": "https://combine.se/blog/page/4", "snippet": "encoder/decoder in the Transformer contains a <b>self-attention</b> <b>layer</b>, which aims to determine which part of the sequence is most important when processing a particular word, e.g. in the sentence \u201cJames enjoys the beach because he likes to swim\u201d, the <b>self-attention</b> <b>layer</b> should learn to link the word \u201che\u201d to \u201cJames\u201d as the most important for its embedding. Additionally, each decoder contains an \u201cEncoder-Decoder <b>Attention</b>\u201d <b>layer</b>, which refers to the relative importance of each ...", "dateLastCrawled": "2022-01-30T08:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "ICMR &#39;21: Proceedings of the 2021 International Conference on ...", "url": "http://www.sigmm.org/opentoc/ICMR2021-TOC", "isFamilyFriendly": true, "displayUrl": "www.sigmm.org/opentoc/ICMR2021-TOC", "snippet": "Furthermore, connecting weights of <b>self-attention</b> blocks are target position invariant, which lacks the expected adaptability. To tackle these limitations, in this paper, we propose a novel Global Relation-aware <b>Attention</b> Network (GRAN) for image-text retrieval by designing Global <b>Attention</b> Module (GAM) and Relation-aware <b>Attention</b> Module (RAM) which play an important role in modeling the global feature and the relationships of local fragments. Firstly, we propose Global <b>Attention</b> Module ...", "dateLastCrawled": "2022-02-02T21:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Sensors | Free Full-Text | Global-and-Local Context Network for ...", "url": "https://www.mdpi.com/1424-8220/20/10/2907/htm", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/1424-8220/20/10/2907/htm", "snippet": "<b>Self-attention</b> is <b>also</b> used in OCNet to learn pixel-level object context information to enhance context aggregation. Attempt to incorporate ... and <b>paying</b> <b>attention</b> to fine details. Figure 9 shows visualization results of DenseASPP and GLNet, which are the top two performers in Table 4. Although the performance improvement of GLNet over DenseASPP is incremental in terms of mean IoU, it <b>can</b> be seen from Figure 9 that GLNet generates much better visualization results <b>compared</b> to DenseASPP ...", "dateLastCrawled": "2022-01-10T02:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "NeurIPS 2021 papers", "url": "https://tanelp.github.io/neurips2021/", "isFamilyFriendly": true, "displayUrl": "https://tanelp.github.io/neurips2021", "snippet": "Our approach uses <b>self-attention</b> to reason about relationships between datapoints explicitly, which <b>can</b> be seen as realizing non-parametric models using parametric <b>attention</b> mechanisms. However, unlike conventional non-parametric models, we let the model learn end-to-end from the data how to make use of other datapoints for prediction. Empirically, our models solve cross-datapoint lookup and complex reasoning tasks unsolvable by traditional deep learning models. We show highly competitive ...", "dateLastCrawled": "2022-02-03T00:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Learning to detect anomaly events <b>in crowd scenes</b> from ... - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S0925231221000527", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0925231221000527", "snippet": "Unlike previous works <b>paying</b> more <b>attention</b> to data annotation, we focus more on data collection, due to the rare appearance and record of abnormal events in real world. 2.3. Generative models. Generative Adversarial Network (GAN) is proposed by firstly for producing fake pictures. presents a deep convolutional GAN (DCGAN) to generate more photo-realistic images than previous GAN. These GANs <b>can</b> randomly produce the images by given random signals. proposes a conditional GAN (CGAN) to ...", "dateLastCrawled": "2021-12-28T10:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "What is &#39;<b>attention</b>&#39; in the context of deep learning? - Quora", "url": "https://www.quora.com/What-is-attention-in-the-context-of-deep-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>attention</b>-in-the-context-of-deep-learning", "snippet": "Answer (1 of 5): In feed-forward deep networks, the entire input is presented to the network, which computes an output in one pass. In recurrent networks, new inputs <b>can</b> be presented at each time step, and the output of the previous time step <b>can</b> be used as an input to the network. This <b>can</b> be ...", "dateLastCrawled": "2022-01-15T04:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "My_<b>Bibliography_for_Research_on_Autonomous</b>_<b>Driving</b>/2_bc_end2end.md at ...", "url": "https://github.com/chauvinSimon/My_Bibliography_for_Research_on_Autonomous_Driving/blob/master/sections/2_bc_end2end.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/chauvinSimon/My_<b>Bibliography_for_Research_on_Autonomous</b>_<b>Driving</b>/...", "snippet": "&quot;Graph <b>attention</b> network (GAT) is a GCN variant which aggregates node information with weights learned in a <b>self-attention</b> mechanism. Such adaptiveness of GAT makes it more effective than GCN in graph representation learning.&quot; GAT is used here to model the interaction among <b>road</b> agents during <b>driving</b>, which is composed of multiple graph layers. To sum up: ingredients of DiGNet (&quot;<b>driving</b> in graphs&quot;): 1-Semantic bird\u2019s-eye view (BEV) images to model <b>road</b> topologies and traffic rules. 7 ...", "dateLastCrawled": "2022-01-29T01:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "New submissions for Tue, 23 Nov 21 \u00b7 Issue #468 \u00b7 kobiso/daily-arxiv ...", "url": "https://github.com/kobiso/daily-arxiv-noti/issues/468", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/kobiso/daily-arxiv-noti/issues/468", "snippet": "The developed algorithm&#39;s accuracy was tested on four datasets comprising the synthetic and real and <b>compared</b> against four other popular algorithms, which <b>can</b> <b>also</b> be used to the described problem. It was found that the proposed IAD algorithm has an average F1-score of 83.7% averaged across four datasets, and <b>also</b> outperforms other algorithms by an average F1-score of 11%.", "dateLastCrawled": "2021-12-17T04:36:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Attention in Psychology, Neuroscience, and <b>Machine</b> <b>Learning</b>", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7177153/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7177153", "snippet": "These weightings scale the word encodings themselves to create the next <b>layer</b> in the model, a process known as \u201c<b>self-attention</b>.\u201d This process repeats, and eventually interacts with the autoregressive decoder which <b>also</b> has attention mechanisms that allow it to flexibly focus on the encoded input (as in the standard form of attention) and on the previously generated output. The Transformer\u2014the name given to this new attention architecture\u2014outperformed many previous models and quickly ...", "dateLastCrawled": "2021-12-27T10:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Machine</b> <b>learning</b> glossary - DataTime", "url": "https://www.dtalg.com/article-1/", "isFamilyFriendly": true, "displayUrl": "https://www.dtalg.com/article-1", "snippet": "<b>self-attention</b> (<b>also</b> <b>called</b> <b>self-attention</b> <b>layer</b>) #language. A neural network <b>layer</b> that transforms a sequence of embeddings (for instance, token embeddings) into another sequence of embeddings. Each embedding in the output sequence is constructed by integrating information from the elements of the input sequence through an attention mechanism.", "dateLastCrawled": "2022-01-25T17:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "10.6. <b>Self-Attention</b> and <b>Positional Encoding</b> \u2014 Dive into Deep <b>Learning</b> ...", "url": "http://d2l.ai/chapter_attention-mechanisms/self-attention-and-positional-encoding.html", "isFamilyFriendly": true, "displayUrl": "d2l.ai/chapter_attention-mechanisms/<b>self-attention</b>-and-<b>positional-encoding</b>.html", "snippet": "In deep <b>learning</b>, we often use CNNs or RNNs to encode a sequence. Now with attention mechanisms, imagine that we feed a sequence of tokens into attention pooling so that the same set of tokens act as queries, keys, and values. Specifically, each query attends to all the key-value pairs and generates one attention output. Since the queries, keys, and values come from the same place, this performs <b>self-attention</b> [Lin et al., 2017b] [Vaswani et al., 2017], which is <b>also</b> <b>called</b> intra-attention ...", "dateLastCrawled": "2022-02-02T23:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) Enhancing LSTM Models with <b>Self-attention</b> and Stateful Training ...", "url": "https://www.academia.edu/69040947/Enhancing_LSTM_Models_with_Self_attention_and_Stateful_Training", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/69040947/Enhancing_LSTM_Models_with_<b>Self_attention</b>_and_Statef...", "snippet": "<b>Self-attention</b>, <b>also</b> known as intra-attention, is an attention mechanism relat- ing di\ufb00erent positions of a sequence in order to model dependencies between dif- ferent parts of the sequence. This di\ufb00ers from general attention in that instead of seeking to discover the \u201cimportant\u201d parts of the sequence relating to the net- work output, <b>self-attention</b> seeks to \ufb01nd the \u201cimportant\u201d portions of the sequence that relate to each other. This is done in order to leverage those intra ...", "dateLastCrawled": "2022-02-03T12:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine Learning Papers: Molecules</b> - <b>Machine Learning</b> Applied", "url": "https://machinelearningapplied.com/machine-learning-papers-molecules/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>applied.com/<b>machine-learning-papers-molecules</b>", "snippet": "A <b>self-attention</b> based message passing neural network for predicting molecular lipophilicity and aqueous solubility - Tang et al 2020 . Efficient and accurate prediction of molecular properties, such as lipophilicity and solubility, is highly desirable for rational compound design in chemical and pharmaceutical industries. To this end, we build and apply a graph-neural-network framework <b>called</b> <b>self-attention</b>-based message-passing neural network (SAMPN) to study the relationship between ...", "dateLastCrawled": "2021-12-22T02:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Lecture 7: Transformers</b> - Deep <b>Learning</b>", "url": "https://chinmayhegde.github.io/dl-notes/notes/lecture07/", "isFamilyFriendly": true, "displayUrl": "https://chinmayhegde.github.io/dl-notes/notes/lecture07", "snippet": "<b>Self-Attention</b>. This is the point where papers-blogs-tweets-slides etc start talking about keys/values and attention mechanisms and everything goes a bit haywire. Let\u2019s just ignore all that for now, and instead talk about something <b>called</b> <b>self-attention</b>. The use of the \u201cself-\u201c prefix will become clear later on. Here is how it is defined.", "dateLastCrawled": "2022-02-03T05:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Transformers in NLP: A beginner friendly explanation | Towards Data Science", "url": "https://towardsdatascience.com/transformers-89034557de14", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>transformer</b>s-89034557de14", "snippet": "<b>Self attention</b>, sometimes <b>called</b> intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. In simpler terms, <b>self attention</b> helps us create similar connections but within the same sentence. Look at the following example: \u201cI poured water from the bottle into the cup until it was full.\u201d it =&gt; cup \u201cI poured water from the bottle into the cup until it was empty.\u201d it=&gt; bottle. By changing one word ...", "dateLastCrawled": "2022-02-02T13:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Attending to Attention. A summary of a revolutionary paper\u2026 | by Akash ...", "url": "https://towardsdatascience.com/attending-to-attention-eba798f0e940", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/attending-to-attention-eba798f0e940", "snippet": "The encoder is composed of a stack of N = 6 identical layers. Each <b>layer</b> has two sub-layers. The first is a multi-head <b>self-attention</b> mechanism, and the second is a simple, position-wise fully connected feed-forward network. We employ a residual connection around each of the two sub-layers, followed by <b>layer</b> normalization. That is, the output ...", "dateLastCrawled": "2022-01-25T10:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Journal of Physics: Conference Series PAPER OPEN ACCESS You may <b>also</b> ...", "url": "https://iopscience.iop.org/article/10.1088/1742-6596/1651/1/012019/pdf", "isFamilyFriendly": true, "displayUrl": "https://iopscience.iop.org/article/10.1088/1742-6596/1651/1/012019/pdf", "snippet": "Different <b>machine</b> <b>learning</b> techniques have been used in this field for many years. But recently, deep <b>learning</b> has caused more and more attention in the field of education. Deep <b>learning</b> is a <b>machine</b> <b>learning</b> method based on neural network structure of multi-<b>layer</b> processing units, and it has been successfully applied to a series of problems in the field of image recognition and natural language processing[2]. With the diversified cultivation of traditional universities and the development ...", "dateLastCrawled": "2021-12-29T04:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What is &#39;attention&#39; in the context of deep <b>learning</b>? - Quora", "url": "https://www.quora.com/What-is-attention-in-the-context-of-deep-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-attention-in-the-context-of-deep-<b>learning</b>", "snippet": "Answer (1 of 5): In feed-forward deep networks, the entire input is presented to the network, which computes an output in one pass. In recurrent networks, new inputs can be presented at each time step, and the output of the previous time step can be used as an input to the network. This can be ...", "dateLastCrawled": "2022-01-15T04:26:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(self-attention (also called self-attention layer))  is like +(paying attention to the road while driving)", "+(self-attention (also called self-attention layer)) is similar to +(paying attention to the road while driving)", "+(self-attention (also called self-attention layer)) can be thought of as +(paying attention to the road while driving)", "+(self-attention (also called self-attention layer)) can be compared to +(paying attention to the road while driving)", "machine learning +(self-attention (also called self-attention layer) AND analogy)", "machine learning +(\"self-attention (also called self-attention layer) is like\")", "machine learning +(\"self-attention (also called self-attention layer) is similar\")", "machine learning +(\"just as self-attention (also called self-attention layer)\")", "machine learning +(\"self-attention (also called self-attention layer) can be thought of as\")", "machine learning +(\"self-attention (also called self-attention layer) can be compared to\")"]}