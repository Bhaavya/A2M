{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Bias</b> in A.I. &amp; <b>Machine</b> <b>Learning</b> Examples \u2013 Tanner Abraham", "url": "https://tannerabraham.com/bias-in-machine-learning-and-ai-examples/", "isFamilyFriendly": true, "displayUrl": "https://tannerabraham.com/<b>bias</b>-in-<b>machine</b>-<b>learning</b>-and-ai-examples", "snippet": "These biases include sample <b>bias</b>, reporting <b>bias</b>, prejudice <b>bias</b>, confirmation <b>bias</b>, <b>group</b> <b>attribution</b> <b>bias</b>, <b>algorithm</b> <b>bias</b>, measurement <b>bias</b>, recall <b>bias</b>, exclusion <b>bias</b>, and automation <b>bias</b>. <b>Machine</b> <b>learning</b> is highly susceptible to many forms of <b>bias</b> that can undermine model performance. After all, AI is assembled by humans, and humans are ...", "dateLastCrawled": "2022-01-28T07:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "AI <b>Bias</b> Example, Meaning And How To Avoid Them - Kotai Electronics Pvt ...", "url": "https://kotaielectronics.com/ai-bias-example-meaning-and-how-to-avoid-them/", "isFamilyFriendly": true, "displayUrl": "https://kotaielectronics.com/ai-<b>bias</b>-example-meaning-and-how-to-avoid-them", "snippet": "<b>Group</b> <b>attribution</b> AI <b>bias</b> meaning, The <b>Bias</b> that takes place when the <b>algorithm</b> puts more weight onto an individual it happens because the <b>algorithm</b> classifies the data and extrapolates a certain set of data from the rest of the data set. Here the AI <b>bias</b> example can be a tool that is used for admission and recruiting people, Here the system can put more importance on students who graduate from certain universities over others.", "dateLastCrawled": "2022-01-18T15:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "HUMAN <b>BIAS</b> THAT CAN RESULT INTO ML BIASES | by CoffeeBeans Consulting ...", "url": "https://coffeebeansconsulting.medium.com/human-bias-that-can-result-into-ml-biases-b1ea9f6d2767?source=post_internal_links---------6----------------------------", "isFamilyFriendly": true, "displayUrl": "https://coffeebeansconsulting.medium.com/human-<b>bias</b>-that-can-result-into-ml-<b>bias</b>es-b1...", "snippet": "<b>Group</b> <b>attribution</b> <b>Bias</b>: This <b>bias</b> assumes a particular attribute to the entire <b>group</b>. For example, if the majority of women are in the designing industry and a majority of men in the hardware, it tends to assume the respective professions for both. 6. <b>Algorithm</b> <b>Bias</b>: In <b>machine</b> <b>learning</b>, <b>bias</b> is a mathematical property of an <b>algorithm</b>. The counterpart to <b>bias</b> in this context is variance. ML algorithms with a high value of variance can easily fit into training data and welcome complexity but ...", "dateLastCrawled": "2022-01-25T05:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "AI <b>Bias</b>: Definition, Types, Examples, and Debiasing Strategies \u2014 ITRex", "url": "https://itrexgroup.com/blog/ai-bias-definition-types-examples-debiasing-strategies/", "isFamilyFriendly": true, "displayUrl": "https://itrex<b>group</b>.com/blog/ai-<b>bias</b>-definition-types-examples-de<b>bias</b>ing-strategies", "snippet": "A simple definition of AI <b>bias</b> could sound <b>like</b> that: a phenomenon that occurs when an AI <b>algorithm</b> produces results that are systemically prejudiced due to erroneous assumptions in the <b>machine</b> <b>learning</b> process. <b>Bias</b> in artificial intelligence can take many forms \u2014 from racial <b>bias</b> and gender prejudice to recruiting inequity and age discrimination. &quot;The underlying reason for AI <b>bias</b> lies in human prejudice - conscious or unconscious - lurking in AI algorithms throughout their development ...", "dateLastCrawled": "2022-01-31T20:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Welcome To The <b>Machine</b> <b>Learning</b> Biases That Still Exist In 2019", "url": "https://analyticsindiamag.com/welcome-to-the-machine-learning-biases-that-still-exist-in-2019/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/welcome-to-the-<b>machine</b>-<b>learning</b>-<b>bias</b>es-that-still-exist...", "snippet": "5.<b>Group</b> <b>attribution</b> <b>Bias</b>: This <b>bias</b> assumes a particular attribute to the entire <b>group</b>. For example, if the majority of women are in the designing industry and a majority of men in the hardware, it tends to assume the respective professions for both. 4.<b>Algorithm</b> <b>Bias</b>: In <b>machine</b> <b>learning</b>, <b>bias</b> is a mathematical property of an <b>algorithm</b>. The ...", "dateLastCrawled": "2022-01-08T19:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Biases in <b>machine</b> <b>learning</b> models and big data analytics: The ...", "url": "https://www.cambridge.org/core/journals/international-review-of-the-red-cross/article/biases-in-machine-learning-models-and-big-data-analytics-the-international-criminal-and-humanitarian-law-implications/86BEAC9ADD165C90B2931AB2B665FFDF", "isFamilyFriendly": true, "displayUrl": "https://www.cambridge.org/core/journals/international-review-of-the-red-cross/article/...", "snippet": "<b>Group</b> <b>attribution</b> <b>bias</b> is a tendency to impute what is true of a few individuals to an entire <b>group</b> to which they belong. For instance, imagine that an ML model is created to identify the most suitable candidates for a position with the OTP. In creating this model, the designers assume that the \u201cbest\u201d candidates are individuals with a doctorate degree from a Western European university and internship experience with the ICC, purely because some successful employees possess those traits ...", "dateLastCrawled": "2021-12-21T11:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine</b> <b>Learning</b> Fairness: Types of <b>Bias</b> | by Svs Nagesh | Medium", "url": "https://nageshsomayajula.medium.com/machine-learning-fairness-types-of-bias-82bcf3df2d47", "isFamilyFriendly": true, "displayUrl": "https://nageshsomayajula.medium.com/<b>machine</b>-<b>learning</b>-fairness-types-of-<b>bias</b>-82bcf3df2d47", "snippet": "AI a p plications are <b>like</b> a small kid, we must train with the right data otherwise they can be misguided, and correcting machines or AI applications will be big challenging, for kids also (pun intended). The AI systems themselves will construct models that will explain how it works and follow anti-<b>bias</b> rules. In the <b>machine</b>, <b>learning</b> <b>bias</b> is one of the most common problems and every <b>algorithm</b> falls trap on this various kind of <b>bias</b>, let\u2019s discuss in detail various types of <b>bias</b> and how to ...", "dateLastCrawled": "2022-01-30T12:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Bias</b> in AI: What it is, Types, Examples &amp; 6 Ways to Fix it in 2022", "url": "https://research.aimultiple.com/ai-bias/", "isFamilyFriendly": true, "displayUrl": "https://research.aimultiple.com/ai-<b>bias</b>", "snippet": "AI <b>bias</b> is an anomaly in the output of <b>machine</b> <b>learning</b> algorithms, due to the prejudiced assumptions made during the <b>algorithm</b> development process or prejudices in the training data. What are the types of AI <b>bias</b>? AI systems contain biases due to two reasons: Cognitive biases: These are unconscious errors in thinking that affects individuals\u2019 judgements and decisions. These biases arise from the brain\u2019s attempt to simplify processing information about the world. More than 180 human ...", "dateLastCrawled": "2022-02-02T07:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "What Is Model <b>Bias</b> In <b>Machine</b> <b>Learning</b>? \u2013 charmestrength.com", "url": "https://charmestrength.com/what-is-model-bias-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://charmestrength.com/what-is-model-<b>bias</b>-in-<b>machine</b>-<b>learning</b>", "snippet": "What is model <b>bias</b> in <b>machine</b> <b>learning</b>? <b>Machine</b> <b>learning</b> <b>bias</b>, also sometimes called <b>algorithm</b> <b>bias</b> or AI <b>bias</b>, ... What is <b>bias</b> in Ann? <b>Bias</b> <b>is like</b> the intercept added in a linear equation. It is an additional parameter in the Neural Network which is used to adjust the output along with the weighted sum of the inputs to the neuron. Thus, <b>Bias</b> is a constant which helps the model in a way that it can fit best for the given data. What are the 3 types of <b>machine</b> <b>learning</b> <b>bias</b>? Types of <b>Bias</b> in ...", "dateLastCrawled": "2022-01-15T06:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Review into bias in algorithmic decision-making</b> - <b>GOV.UK</b>", "url": "https://www.gov.uk/government/publications/cdei-publishes-review-into-bias-in-algorithmic-decision-making/main-report-cdei-review-into-bias-in-algorithmic-decision-making", "isFamilyFriendly": true, "displayUrl": "https://<b>www.gov.uk</b>/government/publications/cdei-publishes-review-into-<b>bias</b>-in...", "snippet": "Second, A <b>machine</b> <b>learning</b> <b>algorithm</b> is chosen, and uses historical data (e.g. a set of past input data (e.g. a set of past input data, the decisions reached) to build a model, optimising against ...", "dateLastCrawled": "2022-02-03T02:05:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Bias</b> in A.I. &amp; <b>Machine</b> <b>Learning</b> Examples \u2013 Tanner Abraham", "url": "https://tannerabraham.com/bias-in-machine-learning-and-ai-examples/", "isFamilyFriendly": true, "displayUrl": "https://tannerabraham.com/<b>bias</b>-in-<b>machine</b>-<b>learning</b>-and-ai-examples", "snippet": "These biases include sample <b>bias</b>, reporting <b>bias</b>, prejudice <b>bias</b>, confirmation <b>bias</b>, <b>group</b> <b>attribution</b> <b>bias</b>, <b>algorithm</b> <b>bias</b>, measurement <b>bias</b>, recall <b>bias</b>, exclusion <b>bias</b>, and automation <b>bias</b>. <b>Machine</b> <b>learning</b> is highly susceptible to many forms of <b>bias</b> that can undermine model performance. After all, AI is assembled by humans, and humans are ...", "dateLastCrawled": "2022-01-28T07:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Biases in <b>machine</b> <b>learning</b> models and big data analytics: The ...", "url": "https://www.cambridge.org/core/journals/international-review-of-the-red-cross/article/biases-in-machine-learning-models-and-big-data-analytics-the-international-criminal-and-humanitarian-law-implications/86BEAC9ADD165C90B2931AB2B665FFDF", "isFamilyFriendly": true, "displayUrl": "https://www.cambridge.org/core/journals/international-review-of-the-red-cross/article/...", "snippet": "<b>Group</b> <b>attribution</b> <b>bias</b> is a tendency to impute what is true of a few individuals to an entire <b>group</b> to which they belong. For instance, imagine that an ML model is created to identify the most suitable candidates for a position with the OTP. In creating this model, the designers assume that the \u201cbest\u201d candidates are individuals with a doctorate degree from a Western European university and internship experience with the ICC, purely because some successful employees possess those traits ...", "dateLastCrawled": "2021-12-21T11:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Bias</b>, <b>Fairness</b> and Explainability \u2014 steps towards building Responsible ...", "url": "https://medium.com/walmartglobaltech/bias-fairness-and-explainability-steps-towards-building-responsible-ai-dc735b06279", "isFamilyFriendly": true, "displayUrl": "https://medium.com/walmartglobaltech/<b>bias</b>-<b>fairness</b>-and-explainability-steps-towards...", "snippet": "<b>Group</b> <b>Attribution</b> <b>bias</b>: ... As mentioned in Wikipedia: \u201cIn <b>machine</b> <b>learning</b>, a given <b>algorithm</b> is said to be fair, or to have <b>fairness</b>, if its results are independent of given variables ...", "dateLastCrawled": "2022-01-23T22:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Comparing different supervised <b>machine</b> <b>learning</b> algorithms for disease ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6925840/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC6925840", "snippet": "To avoid the risk of selection <b>bias</b>, from the literature we extracted those articles that used more than one supervised <b>machine</b> <b>learning</b> <b>algorithm</b>. The same supervised <b>learning</b> <b>algorithm</b> can generate different results across various study settings. There is a chance that a performance comparison between two supervised <b>learning</b> algorithms can generate imprecise results if they were employed in different studies separately. On the other side, the results of this study could suffer a variable ...", "dateLastCrawled": "2022-01-28T14:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) <b>Algorithmic Factors Influencing Bias in Machine Learning</b>", "url": "https://www.researchgate.net/publication/351222202_Algorithmic_Factors_Influencing_Bias_in_Machine_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/351222202_<b>Algorithm</b>ic_Factors_Influencing...", "snippet": "PDF | It is fair to say that many of the prominent examples of <b>bias</b> in <b>Machine</b> <b>Learning</b> (ML) arise from <b>bias</b> that is there in the training data. In... | Find, read and cite all the research you ...", "dateLastCrawled": "2021-08-08T08:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Introduction <b>to Machine</b> <b>Learning</b>, Neural Networks, and Deep <b>Learning</b>", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7347027/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7347027", "snippet": "The performance of the <b>algorithm</b> is evaluated on the test dataset, data that the <b>algorithm</b> has never seen before.8, 9 The basic steps of supervised <b>machine</b> <b>learning</b> are (1) acquire a dataset and split it into separate training, validation, and test datasets; (2) use the training and validation datasets to inform a model of the relationship between features and target; and (3) evaluate the model via the test dataset to determine how well it predicts housing prices for unseen instances. In ...", "dateLastCrawled": "2022-02-02T05:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Bias</b>, awareness, and ignorance in deep-<b>learning</b>-based face recognition ...", "url": "https://link.springer.com/article/10.1007%2Fs43681-021-00108-6", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s43681-021-00108-6", "snippet": "As Nyarko et al. show, people are quick to apply this concept of removing sensitive attributes <b>to machine</b> <b>learning</b> models\u2014despite the potentially harmful consequences for the disadvantaged <b>group</b> (see, e.g., [21,22,23]). The underlying assumption of those people is that removing sensitive attributes would reduce <b>bias</b> and thus help the disadvantaged <b>group</b>.", "dateLastCrawled": "2022-02-03T10:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Evolution and impact of <b>bias</b> in human and <b>machine learning</b> <b>algorithm</b> ...", "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0235502", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0235502", "snippet": "Traditionally, <b>machine learning</b> algorithms relied on reliable labels from experts to build predictions. More recently however, algorithms have been receiving data from the general population in the form of labeling, annotations, etc. The result is that algorithms are subject to <b>bias</b> that is born from ingesting unchecked information, such as biased samples and biased labels. Furthermore, people and algorithms are increasingly engaged in interactive processes wherein neither the human nor the ...", "dateLastCrawled": "2021-11-15T00:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Frontiers | Addressing Fairness, <b>Bias</b>, and Appropriate Use of ...", "url": "https://www.frontiersin.org/articles/10.3389/frai.2020.561802/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/frai.2020.561802", "snippet": "A well-known example of <b>machine</b> <b>learning</b> <b>bias</b>, publicized by Joy Boulamwini in 2017 ... to enable the proper tuning of a <b>machine</b> <b>learning</b> <b>algorithm</b>. Individual vs. <b>Group</b> Fairness. Given the dictionary definition of fairness (impartial and just treatment), we can consider fairness at the level of an individual or a <b>group</b>. We can ask whether a computer <b>algorithm</b> disproportionately helps or harms specific individuals or specific groups of people. Ideally, an <b>algorithm</b> would be customized to an ...", "dateLastCrawled": "2022-01-29T07:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Machine</b> <b>Learning</b> <b>Algorithm</b> for Estimating Surface PM2.5 in Thailand ...", "url": "https://aaqr.org/articles/aaqr-21-05-oa-0105", "isFamilyFriendly": true, "displayUrl": "https://aaqr.org/articles/aaqr-21-05-oa-0105", "snippet": "ABSTRACT We have used NASA&#39;s Modern-Era Retrospective analysis for Research and Applications, Version 2 (MERRA2) reanalysis data of aerosols and meteorology into a <b>machine</b> <b>learning</b> <b>algorithm</b> (MLA) to estimate surface PM2.5 concentration in Thailand. One year of hourly data from 51 ground monitoring stations in Thailand was spatiotemporally collocated with MERRA2 fields. The integrated data then used to train and validate a supervised MLA&#39; random forest&#39; to estimate hourly and daily PM2.5 ...", "dateLastCrawled": "2022-02-03T00:47:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Frontiers | Addressing Fairness, <b>Bias</b>, and Appropriate Use of ...", "url": "https://www.frontiersin.org/articles/10.3389/frai.2020.561802/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/frai.2020.561802", "snippet": "A well-known example of <b>machine</b> <b>learning</b> <b>bias</b>, publicized by Joy Boulamwini in 2017 ... to enable the proper tuning of a <b>machine</b> <b>learning</b> <b>algorithm</b>. Individual vs. <b>Group</b> Fairness. Given the dictionary definition of fairness (impartial and just treatment), we <b>can</b> consider fairness at the level of an individual or a <b>group</b>. We <b>can</b> ask whether a computer <b>algorithm</b> disproportionately helps or harms specific individuals or specific groups of people. Ideally, an <b>algorithm</b> would be customized to an ...", "dateLastCrawled": "2022-01-29T07:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Chapter 11 Bias and Fairness</b> | Big Data and Social Science", "url": "https://textbook.coleridgeinitiative.org/chap-bias.html", "isFamilyFriendly": true, "displayUrl": "https://textbook.coleridgeinitiative.org/chap-<b>bias</b>.html", "snippet": "Unfortunately, just as there is no single <b>machine</b> <b>learning</b> <b>algorithm</b> that is best suited to every application, no one fairness metric will fit every situation. However, we hope this chapter will provide you with a grounding in the available ways of measuring algorithmic fairness that will help you navigate the trade-offs involved putting these into practice in your own applications. 11.2 Sources of <b>Bias</b>. <b>Bias</b> may be introduced into a <b>machine</b> <b>learning</b> project at any step along the way and it ...", "dateLastCrawled": "2022-01-30T01:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Evolution and impact of <b>bias</b> in human and <b>machine learning</b> <b>algorithm</b> ...", "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0235502", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0235502", "snippet": "Traditionally, <b>machine learning</b> algorithms relied on reliable labels from experts to build predictions. More recently however, algorithms have been receiving data from the general population in the form of labeling, annotations, etc. The result is that algorithms are subject to <b>bias</b> that is born from ingesting unchecked information, such as biased samples and biased labels. Furthermore, people and algorithms are increasingly engaged in interactive processes wherein neither the human nor the ...", "dateLastCrawled": "2021-11-15T00:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A <b>machine</b> <b>learning</b> toolkit for genetic engineering <b>attribution</b> to ...", "url": "https://www.nature.com/articles/s41467-020-19612-0", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41467-020-19612-0", "snippet": "With a framework for calibration of <b>attribution</b> models that <b>can</b> in principle be applied to any deep-<b>learning</b> classification <b>algorithm</b>, we next sought to expand the toolkit of genetic engineering ...", "dateLastCrawled": "2022-01-13T13:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Bias</b> does not equal <b>bias</b>. A socio-technical typology of <b>bias</b> in data ...", "url": "https://policyreview.info/articles/analysis/bias-does-not-equal-bias-socio-technical-typology-bias-data-based-algorithmic", "isFamilyFriendly": true, "displayUrl": "https://policyreview.info/articles/analysis/<b>bias</b>-does-not-equal-<b>bias</b>-socio-technical...", "snippet": "Zooming into the technical details of a <b>machine</b> <b>learning</b> system\u2019s life cycle, Suresh and Guttag (2020) described various issues that <b>can</b> introduce <b>bias</b> into a system: historical <b>bias</b>, representation <b>bias</b>, measurement <b>bias</b>, aggregation <b>bias</b>, <b>learning</b> <b>bias</b>, evaluation <b>bias</b> and deployment <b>bias</b>. Some of these types of data <b>bias</b> <b>can</b> only be identified through extensive knowledge and close examination of the development process of a particular system including the underlying data used to build ...", "dateLastCrawled": "2022-01-22T12:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Bias</b>, awareness, and ignorance in deep-<b>learning</b>-based face recognition ...", "url": "https://link.springer.com/article/10.1007%2Fs43681-021-00108-6", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s43681-021-00108-6", "snippet": "We investigate how well <b>machine</b> <b>learning</b> models <b>can</b> predict the sensitive features, such as ethnicity and gender, based on the face embedding. The intuition is that an FR model is \u201caware\u201d of a sensitive feature if it <b>can</b> be predicted from the embedding vectors produced by the FR model. This inference is a classification task and the performance depends on the classification model at hand. If simple models, more precisely models with a low number of parameters, <b>can</b> properly infer the ...", "dateLastCrawled": "2022-02-03T10:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Comparing different supervised <b>machine</b> <b>learning</b> algorithms for disease ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6925840/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC6925840", "snippet": "To avoid the risk of selection <b>bias</b>, from the literature we extracted those articles that used more than one supervised <b>machine</b> <b>learning</b> <b>algorithm</b>. The same supervised <b>learning</b> <b>algorithm</b> <b>can</b> generate different results across various study settings. There is a chance that a performance comparison between two supervised <b>learning</b> algorithms <b>can</b> generate imprecise results if they were employed in different studies separately. On the other side, the results of this study could suffer a variable ...", "dateLastCrawled": "2022-01-28T14:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Review into bias in algorithmic decision-making</b> - <b>GOV.UK</b>", "url": "https://www.gov.uk/government/publications/cdei-publishes-review-into-bias-in-algorithmic-decision-making/main-report-cdei-review-into-bias-in-algorithmic-decision-making", "isFamilyFriendly": true, "displayUrl": "https://<b>www.gov.uk</b>/government/publications/cdei-publishes-review-into-<b>bias</b>-in...", "snippet": "A <b>machine</b> <b>learning</b> <b>algorithm</b> takes data as ... not available to the <b>algorithm</b>) but <b>can</b> also introduce human <b>bias</b> into the system. Humans \u2018over the loop\u2019 monitoring the fairness of the whole ...", "dateLastCrawled": "2022-02-03T02:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Towards a pragmatist dealing with algorithmic <b>bias</b> in medical <b>machine</b> ...", "url": "https://link.springer.com/article/10.1007/s11019-021-10008-5", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11019-021-10008-5", "snippet": "The underlying assumption is that greater transparency will render algorithmic <b>bias</b> easier to detect and help understand a program\u2019s erroneous decisions, so that one <b>can</b> correct the <b>algorithm</b>\u2019s mistakes and avoid <b>bias</b> by curating the input variables accordingly. For many instances this solution <b>can</b> be sufficient, e.g. to identify so-called Clever Hans predictors that base a ML program\u2019s classification strategy on irrelevant correlations. A good example for such a misleading predictor ...", "dateLastCrawled": "2022-01-13T00:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "300+ TOP <b>Neural Networks Multiple Choice Questions and Answers</b>", "url": "https://engineeringinterviewquestions.com/neural-networks-multiple-choice-questions-and-answers/", "isFamilyFriendly": true, "displayUrl": "https://engineeringinterviewquestions.com/<b>neural-networks-multiple-choice-questions</b>...", "snippet": "34. Decision Trees <b>can</b> be used for Classification Tasks. a) True b) False. Answer: a. 35. How many types of <b>learning</b> are available in <b>machine</b> <b>learning</b>? a) 1 b) 2 c) 3 d) 4. Answer: c Explanation: The three types of <b>machine</b> <b>learning</b> are supervised, unsupervised and reinforcement. 36. Choose from the following that are Decision Tree nodes. a ...", "dateLastCrawled": "2022-02-02T15:24:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "What Is Model <b>Bias</b> In <b>Machine</b> <b>Learning</b>? \u2013 charmestrength.com", "url": "https://charmestrength.com/what-is-model-bias-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://charmestrength.com/what-is-model-<b>bias</b>-in-<b>machine</b>-<b>learning</b>", "snippet": "<b>Machine</b> <b>learning</b> <b>bias</b>, also sometimes called <b>algorithm</b> <b>bias</b> or AI <b>bias</b>, ... <b>Group</b> <b>attribution</b> <b>Bias</b>. What to do if model is Overfitting? Reduce the network&#39;s capacity by removing layers or reducing the number of elements in the hidden layers. Apply regularization , which comes down to adding a cost to the loss function for large weights. Use Dropout layers, which will randomly remove certain features by setting them to zero. How <b>can</b> <b>machine</b> <b>learning</b> models reduce <b>bias</b>? Choose the correct ...", "dateLastCrawled": "2022-01-15T06:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "AI <b>Bias</b> Example, Meaning And How To Avoid Them - Kotai Electronics Pvt ...", "url": "https://kotaielectronics.com/ai-bias-example-meaning-and-how-to-avoid-them/", "isFamilyFriendly": true, "displayUrl": "https://kotaielectronics.com/ai-<b>bias</b>-example-meaning-and-how-to-avoid-them", "snippet": "<b>Group</b> <b>attribution</b> AI <b>bias</b> meaning, The <b>Bias</b> that takes place when the <b>algorithm</b> puts more weight onto an individual it happens because the <b>algorithm</b> classifies the data and extrapolates a certain set of data from the rest of the data set. Here the AI <b>bias</b> example <b>can</b> be a tool that is used for admission and recruiting people, Here the system <b>can</b> put more importance on students who graduate from certain universities over others.", "dateLastCrawled": "2022-01-18T15:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Can</b> <b>bias</b> be eliminated from algorithms?", "url": "https://www.weforum.org/agenda/2021/12/bias-eliminated-algorithms-software-code-systems/", "isFamilyFriendly": true, "displayUrl": "https://www.weforum.org/agenda/2021/12/<b>bias</b>-eliminated-<b>algorithms</b>-software-code-systems", "snippet": "<b>Algorithm</b> <b>bias</b>, also called <b>machine</b> <b>learning</b> <b>bias</b>, is a phenomenon in which algorithms <b>can</b> act in a discriminatory or prejudiced manner due to misplaced assumptions during the <b>learning</b> phase of their development. Unconscious biases regarding gender, race and social class <b>can</b> make their way into the training data fed by programmers into \u201c<b>machine</b>-<b>learning</b> algorithms\u201d, systems which constantly improve their own performance by including new data into an existing model. These biases <b>can</b> be ...", "dateLastCrawled": "2022-02-03T05:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Bias</b>, <b>Fairness</b> and Explainability \u2014 steps towards building Responsible ...", "url": "https://medium.com/walmartglobaltech/bias-fairness-and-explainability-steps-towards-building-responsible-ai-dc735b06279", "isFamilyFriendly": true, "displayUrl": "https://medium.com/walmartglobaltech/<b>bias</b>-<b>fairness</b>-and-explainability-steps-towards...", "snippet": "<b>Group</b> <b>Attribution</b> <b>bias</b>: ... in Wikipedia: \u201cIn <b>machine</b> <b>learning</b>, a given <b>algorithm</b> is said to be fair, or to have <b>fairness</b>, if its results are independent of given variables, especially those ...", "dateLastCrawled": "2022-01-23T22:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Comparing different supervised <b>machine</b> <b>learning</b> algorithms for disease ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6925840/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC6925840", "snippet": "To avoid the risk of selection <b>bias</b>, from the literature we extracted those articles that used more than one supervised <b>machine</b> <b>learning</b> <b>algorithm</b>. The same supervised <b>learning</b> <b>algorithm</b> <b>can</b> generate different results across various study settings. There is a chance that a performance comparison between two supervised <b>learning</b> algorithms <b>can</b> generate imprecise results if they were employed in different studies separately. On the other side, the results of this study could suffer a variable ...", "dateLastCrawled": "2022-01-28T14:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Frontiers | Addressing Fairness, <b>Bias</b>, and Appropriate Use of ...", "url": "https://www.frontiersin.org/articles/10.3389/frai.2020.561802/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/frai.2020.561802", "snippet": "A well-known example of <b>machine</b> <b>learning</b> <b>bias</b>, publicized by Joy Boulamwini in 2017 ... to enable the proper tuning of a <b>machine</b> <b>learning</b> <b>algorithm</b>. Individual vs. <b>Group</b> Fairness. Given the dictionary definition of fairness (impartial and just treatment), we <b>can</b> consider fairness at the level of an individual or a <b>group</b>. We <b>can</b> ask whether a computer <b>algorithm</b> disproportionately helps or harms specific individuals or specific groups of people. Ideally, an <b>algorithm</b> would be customized to an ...", "dateLastCrawled": "2022-01-29T07:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "how to reduce <b>bias</b> in <b>machine</b> <b>learning</b> - Publicaffairsworld.com", "url": "https://publicaffairsworld.com/how-to-reduce-bias-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://publicaffairsworld.com/how-to-reduce-<b>bias</b>-in-<b>machine</b>-<b>learning</b>", "snippet": "If you have ever developed or worked on any type of <b>machine</b> <b>learning</b> <b>algorithm</b>, ... <b>Group</b> <b>attribution</b> <b>Bias</b>. What are the necessary steps to avoid biases in gathering and interpreting data? There are ways, however, to try to maintain objectivity and avoid <b>bias</b> with qualitative data analysis: Use multiple people to code the data. \u2026 Have participants review your results. \u2026 Verify with more data sources. \u2026 Check for alternative explanations. \u2026 Review findings with peers. What <b>can</b> a data ...", "dateLastCrawled": "2022-01-22T04:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A <b>machine</b> <b>learning</b> toolkit for genetic engineering <b>attribution</b> to ...", "url": "https://www.nature.com/articles/s41467-020-19612-0", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41467-020-19612-0", "snippet": "With a framework for calibration of <b>attribution</b> models that <b>can</b> in principle be applied to any deep-<b>learning</b> classification <b>algorithm</b>, we next sought to expand the toolkit of genetic engineering ...", "dateLastCrawled": "2022-01-13T13:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Addressing Fairness, Bias, and Appropriate</b> Use of Artificial ...", "url": "https://europepmc.org/article/PMC/PMC8107824", "isFamilyFriendly": true, "displayUrl": "https://europepmc.org/article/PMC/PMC8107824", "snippet": "<b>Machine</b> <b>learning</b> <b>can</b> also be used to help optimize processes or to predict ... If algorithmic <b>bias</b> leads to unfavorable treatment of one patient <b>group</b> vs. another, this <b>bias</b> <b>can</b> be judged to be unfair, from a legal or ethical point of view. While <b>bias</b> is related to fairness, it should be noted that algorithmic <b>bias</b> is independent of ethics, and is simply a mathematical and statistical consequence of an <b>algorithm</b> and its data. If an <b>algorithm</b> is discovered to have <b>bias</b>, this <b>bias</b> <b>can</b> then be ...", "dateLastCrawled": "2021-06-01T04:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Review into bias in algorithmic decision-making</b> - <b>GOV.UK</b>", "url": "https://www.gov.uk/government/publications/cdei-publishes-review-into-bias-in-algorithmic-decision-making/main-report-cdei-review-into-bias-in-algorithmic-decision-making", "isFamilyFriendly": true, "displayUrl": "https://<b>www.gov.uk</b>/government/publications/cdei-publishes-review-into-<b>bias</b>-in...", "snippet": "A <b>machine</b> <b>learning</b> <b>algorithm</b> takes data as ... not available to the <b>algorithm</b>) but <b>can</b> also introduce human <b>bias</b> into the system. Humans \u2018over the loop\u2019 monitoring the fairness of the whole ...", "dateLastCrawled": "2022-02-03T02:05:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Attribution</b> Part 3: Applying <b>Machine</b> <b>Learning</b> to <b>Attribution</b> - Trust ...", "url": "https://www.trustinsights.ai/blog/2019/11/attribution-part-3-applying-machine-learning-to-attribution/", "isFamilyFriendly": true, "displayUrl": "https://www.trustinsights.ai/blog/2019/11/<b>attribution</b>-part-3-applying-<b>machine</b>-<b>learning</b>...", "snippet": "The <b>machine</b> <b>learning</b> approach to <b>attribution</b> analysis starts with no <b>bias</b>. With the inexpensive computational power now available to us you can run all of the data from every transaction, and all the data from transactions that did not complete. <b>Machine</b> <b>learning</b> will examine every single transition from one page to the next and from one goal (milestones along the customer journey) to the next and determine the probability of a customer moving from one point to the next.", "dateLastCrawled": "2022-02-02T04:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) Mitigating <b>bias</b> in <b>machine</b> <b>learning</b> for medicine", "url": "https://www.researchgate.net/publication/354073359_Mitigating_bias_in_machine_learning_for_medicine", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/354073359_Mitigating_<b>bias</b>_in_<b>machine</b>_<b>learning</b>...", "snippet": "Strategies for mitigating <b>bias</b> across the different steps in <b>machine</b> <b>learning</b> systems development. Diagram outlining proposed solutions on how to mitigate <b>bias</b> across the different development ...", "dateLastCrawled": "2022-01-21T21:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "6 \u2013 Interpretability \u2013 <b>Machine</b> <b>Learning</b> Blog | ML@CMU | Carnegie Mellon ...", "url": "https://blog.ml.cmu.edu/2020/08/31/6-interpretability/", "isFamilyFriendly": true, "displayUrl": "https://blog.ml.cmu.edu/2020/08/31/6-interpretability", "snippet": "For a person inexperienced in <b>machine</b> <b>learning</b>, it would be difficult to apply the Olah method across many images, especially if you wanted to explain why the model chose a labrador retriever instead of a beagle. In such a case, abstract dog visualizations would be uninformative. Moreover, there is a potential observer <b>bias</b>: if a human expects visualizations of dogs and cats, they might miss more abstract but important visualizations, such as the snow in the husky and wolf classification ...", "dateLastCrawled": "2022-02-03T05:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "An interpretable <b>machine</b> <b>learning</b> workflow with an application to ...", "url": "https://www.ecb.europa.eu/pub/conferences/shared/pdf/20210615_11th_cft/Joseph_slidesT4.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.ecb.europa.eu/pub/conferences/shared/pdf/20210615_11th_cft/Joseph_slidesT4.pdf", "snippet": "Shapley values as <b>analogy</b> between game theory and (ML) models Cooperative game theory <b>Machine</b> <b>learning</b> n Players Predictors / variables f^=y^ Collective payo Predicted value for one observation S Coalition of players <b>Group</b> of predictors in model Source Shapley (1953) Strumbelj and Kononenko (2010) Lundberg and Lee (2017) Model Shapley decomposition: f^(x i) = \u02da 0 + P n k=1 \u02da S k (f^;x i) Why Shapley values?Because they are the only <b>attribution</b> scheme which is local, linear, exact, respects ...", "dateLastCrawled": "2022-01-19T11:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) <b>Machine Learning Applications in Hydrology</b>", "url": "https://www.researchgate.net/publication/339751225_Machine_Learning_Applications_in_Hydrology", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/339751225_<b>Machine</b>_<b>Learning</b>_Applications_in...", "snippet": "Most <b>machine</b> <b>learning</b> techniques require a calibration and a validation dataset for training. As these data are usually correlated in time and space, the problem of <b>bias</b>-variance tradeoff arises ...", "dateLastCrawled": "2022-02-03T05:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Machine Learning, Ethics, and Open Source Licensing</b> (Part II/II)", "url": "https://thegradient.pub/machine-learning-ethics-and-open-source-licensing-2/", "isFamilyFriendly": true, "displayUrl": "https://thegradient.pub/<b>machine-learning-ethics-and-open-source-licensing</b>-2", "snippet": "A good <b>analogy</b> for this might be the GNU Image Manipulation Program, or GIMP. This is a commonly used piece of free software for editing or creating images; the software itself is licensed under GPLv3. However, when an artist creates a piece of artwork using the program, they\u2019re free to license that artwork under whatever they see fit--GIMP is only a tool used by the artist in the process. 2. The data. The data used to construct <b>machine</b> <b>learning</b> models creates similarly complex ...", "dateLastCrawled": "2022-02-02T12:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Beneficial and harmful explanatory <b>machine learning</b> | SpringerLink", "url": "https://link.springer.com/article/10.1007/s10994-020-05941-0", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10994-020-05941-0", "snippet": "(<b>Machine</b>-explained human comprehension of examples, \\(C_{ex}(D, H, M(E))\\)): Given a logic program D representing the definition of a target predicate, a <b>group</b> of humans H, a theory M(E) learned using <b>machine learning</b> algorithm M and examples E, the <b>machine</b>-explained human comprehension of examples E is the mean accuracy with which a human \\(h \\in H\\) after brief study of an explanation based on M(E) can classify new material selected from the domain of D.", "dateLastCrawled": "2022-01-21T19:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "9.6 <b>SHAP</b> (SHapley Additive exPlanations) | Interpretable <b>Machine</b> <b>Learning</b>", "url": "https://christophm.github.io/interpretable-ml-book/shap.html", "isFamilyFriendly": true, "displayUrl": "https://christophm.github.io/interpretable-ml-book/<b>shap</b>.html", "snippet": "9.6 <b>SHAP</b> (SHapley Additive exPlanations). This chapter is currently only available in this web version. ebook and print will follow. <b>SHAP</b> (SHapley Additive exPlanations) by Lundberg and Lee (2017) 69 is a method to explain individual predictions. <b>SHAP</b> is based on the game theoretically optimal Shapley values.. There are two reasons why <b>SHAP</b> got its own chapter and is not a subchapter of Shapley values.First, the <b>SHAP</b> authors proposed KernelSHAP, an alternative, kernel-based estimation ...", "dateLastCrawled": "2022-02-03T01:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "300+ TOP <b>Neural Networks Multiple Choice Questions and Answers</b>", "url": "https://engineeringinterviewquestions.com/neural-networks-multiple-choice-questions-and-answers/", "isFamilyFriendly": true, "displayUrl": "https://engineeringinterviewquestions.com/<b>neural-networks-multiple-choice-questions</b>...", "snippet": "How many types of <b>learning</b> are available in <b>machine</b> <b>learning</b>? a) 1 b) 2 c) 3 d) 4. Answer: c Explanation: The three types of <b>machine</b> <b>learning</b> are supervised, unsupervised and reinforcement. 36. Choose from the following that are Decision Tree nodes. a) Decision Nodes b) Weighted Nodes c) Chance Nodes d) End Nodes. Answer: a, c, d. 37. Decision Nodes are represented by, a) Disks b) Squares c) Circles d) Triangles. Answer: b. 38. Chance Nodes are represented by, a) Disks b) Squares c) Circles ...", "dateLastCrawled": "2022-02-02T15:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "About Us \u2013 <b>Toronto Machine Learning</b>", "url": "https://www.torontomachinelearning.com/about-us/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>torontomachinelearning</b>.com/about-us", "snippet": "The <b>Toronto Machine Learning</b> Society ... at Portland State University. Her current research focuses on conceptual abstraction, <b>analogy</b>-making, and visual recognition in artificial intelligence systems. Melanie is the author or editor of six books and numerous scholarly papers in the fields of artificial intelligence, cognitive science, and complex systems. Her book Complexity: A Guided Tour (Oxford University Press) won the 2010 Phi Beta Kappa Science Book Award and was named by Amazon.com ...", "dateLastCrawled": "2022-02-03T03:50:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Bias in A.I. &amp; <b>Machine</b> <b>Learning</b> Examples \u2013 Tanner Abraham", "url": "https://tannerabraham.com/bias-in-machine-learning-and-ai-examples/", "isFamilyFriendly": true, "displayUrl": "https://tannerabraham.com/bias-in-<b>machine</b>-<b>learning</b>-and-ai-examples", "snippet": "<b>Machine</b> <b>learning</b> algorithms are often mistaken as objective analytics and decision-making solutions to human inefficiencies. Paradoxically, humans often make <b>machine</b> <b>learning</b> algorithms inefficient by way of biases. These biases include sample bias, reporting bias, prejudice bias, confirmation bias, group attribution bias, algorithm bias, measurement bias, recall bias, exclusion bias, and automation bias. <b>Machine</b> <b>learning</b> is highly susceptible to many forms of bias that can undermine model ...", "dateLastCrawled": "2022-01-28T07:55:00.0000000Z", "language": "en", "isNavigational": false}], [], [], []], "all_bing_queries": ["+(group attribution bias)  is like +(machine learning algorithm)", "+(group attribution bias) is similar to +(machine learning algorithm)", "+(group attribution bias) can be thought of as +(machine learning algorithm)", "+(group attribution bias) can be compared to +(machine learning algorithm)", "machine learning +(group attribution bias AND analogy)", "machine learning +(\"group attribution bias is like\")", "machine learning +(\"group attribution bias is similar\")", "machine learning +(\"just as group attribution bias\")", "machine learning +(\"group attribution bias can be thought of as\")", "machine learning +(\"group attribution bias can be compared to\")"]}