{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Disparate</b> Impact in <b>Machine</b> <b>Learning</b> \u00bb Dome | Blog Archive | Boston ...", "url": "https://sites.bu.edu/dome/2020/06/08/disparate-impact-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://sites.bu.edu/dome/2020/06/08/<b>disparate</b>-impact-in-<b>machine</b>-<b>learning</b>", "snippet": "Thus, instead of relying on either <b>disparate</b> impact or <b>disparate</b> <b>treatment</b> theory, perhaps legal analysis of discrimination in <b>machine</b> <b>learning</b> should be entirely outcomes-driven. If, in fact, an <b>algorithm</b> wrongly predicts the likelihood of an event occurring, and that <b>algorithm</b> is less accurate for protected class members than unprotected class members, the <b>algorithm</b> should be considered prima facie discriminatory. Such a solution is viable for examining recidivism, interest rates and loan ...", "dateLastCrawled": "2021-12-09T00:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Handling Discriminatory Biases in Data for <b>Machine Learning</b> | by ...", "url": "https://towardsdatascience.com/machine-learning-and-discrimination-2ed1a8b01038", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>machine-learning</b>-and-<b>discrimination</b>-2ed1a8b01038", "snippet": "<b>Disparate</b> <b>Treatment</b> \u2014 Involves classifying someone in an impermissible way. It involves the intent to discriminate, evidenced by explicit reference to group membership. <b>Disparate</b> Impact \u2014 Looks at the consequences of classification/decision making on certain groups. No intent is required and it is facially neutral. <b>Disparate</b> impact is often referred to as unintentional <b>discrimination</b>, whereas <b>disparate</b> <b>treatment</b> is intentional. Practices with a disproportionate impact on a particular ...", "dateLastCrawled": "2022-01-29T20:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Fairness in Machine Learning is Tricky</b> - Arthur AI", "url": "https://www.arthur.ai/blog/fairness-in-ml", "isFamilyFriendly": true, "displayUrl": "https://www.arthur.ai/blog/fairness-in-ml", "snippet": "Similarly, that <b>algorithm</b> is said to result in <b>disparate</b> <b>treatment</b> if its decisioning is performed in part based on membership in a group. Then, one goal that a <b>fairness in machine learning</b> practitioner might have is to mathematically certify that an <b>algorithm</b> does not suffer from <b>disparate</b> <b>treatment</b> or <b>disparate</b> impact, perhaps given some expected use case or input distribution.", "dateLastCrawled": "2022-02-03T15:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Discrimination</b> in the Age of Algorithms | Journal of Legal Analysis ...", "url": "https://academic.oup.com/jla/article/doi/10.1093/jla/laz001/5476086", "isFamilyFriendly": true, "displayUrl": "https://academic.oup.com/jla/article/doi/10.1093/jla/laz001/5476086", "snippet": "We focus on one kind of <b>machine</b>-<b>learning</b> <b>algorithm</b> often applied to such problems, ... With this in mind, consider what <b>disparate</b> <b>treatment</b> looks <b>like</b>. For hiring, an HR manager might do something <b>like</b> rank-order a less-productive white applicant over a more-productive minority applicant. But with a training and a screening <b>algorithm</b>, if we have accounted for the possible sources of <b>discrimination</b> in the human\u2019s choices for the training <b>algorithm</b> (outcome, candidate predictors, and ...", "dateLastCrawled": "2022-01-28T11:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Differentially Private Fair <b>Learning</b> - Proceedings of <b>Machine</b> <b>Learning</b> ...", "url": "http://proceedings.mlr.press/v97/jagielski19a/jagielski19a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v97/jagielski19a/jagielski19a.pdf", "snippet": "can be viewed as a form of \u201c<b>disparate</b> <b>treatment</b>\u201d. Our second <b>algorithm</b> is a differentially private version of the oracle-ef\ufb01cient in-processing ap-proach of (Agarwal et al.,2018) which is more complex but need not have access to protected group membership at test time. We identify new tradeoffs between fairness, accuracy, and privacy that emerge only when requiring all three proper-ties, and show that these tradeoffs can be milder if group membership may be used at test time. We ...", "dateLastCrawled": "2022-01-22T22:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A Tutorial on <b>Fairness</b> in <b>Machine</b> <b>Learning</b> | by Ziyuan Zhong | Towards ...", "url": "https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-tutorial-on-<b>fairness</b>-in-<b>machine</b>-<b>learning</b>-3ff8ba1040cb", "snippet": "<b>Machine</b> <b>Learning</b>, the most widely used AI techniques, relies heavily on data. It is a common misconception that AI is absolutely objective. AI is objective only in the sense of <b>learning</b> what human teaches. The data provided by human can be highly-biased. It has been found in 2016 that COMPAS, the <b>algorithm</b> used for recidivism prediction produces much higher false positive rate for black people than white people(see Fig2, Larson et al. ProPublica, 2016). Fig2: The bias in COMPAS. (from Larson ...", "dateLastCrawled": "2022-02-01T18:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Fairness in Machine Learning is Tricky</b> | by John Dickerson | Arthur AI ...", "url": "https://medium.com/arthur-ai/fairness-in-machine-learning-is-tricky-1ea47111b847", "isFamilyFriendly": true, "displayUrl": "https://medium.com/arthur-ai/<b>fairness-in-machine-learning-is-tricky</b>-1ea47111b847", "snippet": "Then, one goal that a <b>fairness in machine learning</b> practitioner might have is to mathematically certify that an <b>algorithm</b> does not suffer from <b>disparate</b> <b>treatment</b> or <b>disparate</b> impact, perhaps ...", "dateLastCrawled": "2021-04-29T13:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) <b>Digital Discrimination</b> - ResearchGate", "url": "https://www.researchgate.net/publication/336792693_Digital_Discrimination", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/336792693_<b>Digital_Discrimination</b>", "snippet": "o Usage: a non-discriminatory <b>machine</b> <b>learning</b> <b>algorithm</b> can also lead to discrimination when it is used in a situation for which it was not intended. For example, an <b>algorithm</b>", "dateLastCrawled": "2022-02-02T17:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Evaluating Fairness in <b>Machine</b> <b>Learning</b> - <b>github.com</b>", "url": "https://github.com/KenSciResearch/fairMLHealth/blob/integration/docs/resources/Evaluating_Fairness.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/KenSciResearch/fairMLHealth/blob/integration/docs/resources/...", "snippet": "<b>Disparate</b> impact in a <b>machine</b> <b>learning</b> model originates from bias in either the data or the algorithms. A popular example is the prejudicially biased data used for recidivism prediction. Due to <b>disparate</b> socioeconomic factors and systemic racism in the United States, blacks have historically been (and continue to be) incarcerated at higher rates than whites . Not coincidentally, blacks are also exonerated due to wrongful accusation at a considerably higher rate than whites . A recidivism ...", "dateLastCrawled": "2022-02-02T03:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "The ethics of <b>algorithms</b>: key problems and solutions", "url": "https://link.springer.com/article/10.1007/s00146-021-01154-8", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00146-021-01154-8", "snippet": "Actions (1) and (2) may be performed by (semi-)autonomous <b>algorithms</b>\u2014such as <b>machine</b> <b>learning</b> (ML) <b>algorithms</b>\u2014and this complicates, (3) the attribution of responsibility for the effects of actions that an <b>algorithm</b> may trigger. Here, ML is of particular interest, as a field which includes deep <b>learning</b> architectures. Computer systems deploying ML <b>algorithms</b> may be described as \u201cautonomous\u201d or \u201csemi-autonomous\u201d, to the extent that their outputs are induced from data and thus, non ...", "dateLastCrawled": "2022-01-30T20:46:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "AI Fairness \u2014 Explanation of <b>Disparate Impact</b> Remover | by Stacey ...", "url": "https://towardsdatascience.com/ai-fairness-explanation-of-disparate-impact-remover-ce0da59451f1", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/ai-fairness-explanation-of-<b>disparate-impact</b>-remover-ce0...", "snippet": "The <b>algorithm</b> requires the user to specify a repair_level, this indicates how much you wish for the distributions of the groups to overlap. Let\u2019s explore the impact of two different repair levels, 1.0 and 0.8. Repair value = 1.0. This diagram shows the repaired values for Feature for the unprivileged group Blue and privileged group Orange after using DisparateImpactRemover with a repair level of 1.0. You are no longer able to select a point and infer which group it belongs to. This would ...", "dateLastCrawled": "2022-01-29T05:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Machine Learning and Discrimination</b> - GitHub Pages", "url": "https://harvard-ml-courses.github.io/cs181-web-2020/files/lecture12.pdf", "isFamilyFriendly": true, "displayUrl": "https://harvard-ml-courses.github.io/cs181-web-2020/files/lecture12.pdf", "snippet": "<b>Machine Learning and Discrimination</b> Diana Acosta-Navas PhD candidate, Harvard Philosophy Department Adjunct Lecturer in Ethics and Public Policy, Harvard Kennedy School . For Today\u2026 \u2022Discrimination/ wrongful discrimination \u2022Case Study: PredPol \u2022<b>Disparate</b> <b>treatment</b> vs. <b>Disparate</b> impact \u2022How predictive policing could wrongfully discriminate \u2022What contextual considerations are important to determine whether an <b>algorithm</b> wrongfully discriminates? For Today\u2026 \u2022Content warning ...", "dateLastCrawled": "2021-09-15T23:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>AI Fairness \u2014 Explanation of Disparate Impact Remover</b> - Adolfo Eliaz\u00e0t ...", "url": "https://adolfoeliazat.com/2021/05/06/ai-fairness-explanation-of-disparate-impact-remover/", "isFamilyFriendly": true, "displayUrl": "https://adolfoeliazat.com/2021/05/06/<b>ai-fairness-explanation-of-disparate-impact-remover</b>", "snippet": "<b>Disparate</b> Impact Remover preserves rank-ordering within groups; if an individual has the highest score for group Blue, it will still have the highest score among Blues after repair. Building <b>Machine</b> <b>Learning</b> Models. Once <b>Disparate</b> Impact Remover has been implemented, a <b>machine</b> <b>learning</b> model can be built using the repaired data. The <b>Disparate</b> ...", "dateLastCrawled": "2022-01-22T07:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Machine</b> <b>Learning</b> in Drug Discovery: A Review", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8356896/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8356896", "snippet": "<b>Machine</b> <b>learning</b> techniques improve the decision-making in pharmaceutical data across various applications like QSAR analysis, hit discoveries, de novo drug architectures to retrieve accurate outcomes. Target validation, prognostic biomarkers, digital pathology are considered under problem statements in this review. ML challenges must be applicable for the main cause of inadequacy in interpretability outcomes that may restrict the applications in drug discovery. In clinical trials, absolute ...", "dateLastCrawled": "2022-01-27T11:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Evaluating Fairness in <b>Machine</b> <b>Learning</b> - <b>github.com</b>", "url": "https://github.com/KenSciResearch/fairMLHealth/blob/integration/docs/resources/Evaluating_Fairness.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/KenSciResearch/fairMLHealth/blob/integration/docs/resources/...", "snippet": "<b>Disparate</b> impact in a <b>machine</b> <b>learning</b> model originates from bias in either the data or the algorithms. A popular example is the prejudicially biased data used for recidivism prediction. Due to <b>disparate</b> socioeconomic factors and systemic racism in the United States, blacks have historically been (and continue to be) incarcerated at higher rates than whites . Not coincidentally, blacks are also exonerated due to wrongful accusation at a considerably higher rate than whites . A recidivism ...", "dateLastCrawled": "2022-02-02T03:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "The <b>Racist Algorithm</b>? - University of Michigan", "url": "https://repository.law.umich.edu/cgi/viewcontent.cgi?article=1657&context=mlr", "isFamilyFriendly": true, "displayUrl": "https://repository.law.umich.edu/cgi/viewcontent.cgi?article=1657&amp;context=mlr", "snippet": "he calls the \u201csweet mystery of <b>machine</b> <b>learning</b>.\u201d Frank Pasquale, Bittersweet Mysteries of <b>Ma-chine</b> <b>Learning</b> (A Provocation) ... redesign the <b>algorithm</b> or to distrust its results. The distinction <b>is similar</b> to the evidentiary difference between demonstrating <b>disparate</b> <b>treatment</b> and demonstrating <b>disparate</b> impact. 10. My central claim is this: if we believe that the real-world facts, on which algorithms are trained and operate, are deeply suffused with invidious discrimination, then our ...", "dateLastCrawled": "2022-01-30T00:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A Tutorial on <b>Fairness</b> in <b>Machine</b> <b>Learning</b> | by Ziyuan Zhong | Towards ...", "url": "https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-tutorial-on-<b>fairness</b>-in-<b>machine</b>-<b>learning</b>-3ff8ba1040cb", "snippet": "<b>Machine</b> <b>Learning</b>, the most widely used AI techniques, relies heavily on data. It is a common misconception that AI is absolutely objective. AI is objective only in the sense of <b>learning</b> what human teaches. The data provided by human can be highly-biased. It has been found in 2016 that COMPAS, the <b>algorithm</b> used for recidivism prediction produces much higher false positive rate for black people than white people(see Fig2,", "dateLastCrawled": "2022-02-01T18:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Equal Protection Under the <b>Algorithm</b>: A Legal-Inspired Framework for ...", "url": "https://www.fatml.org/media/documents/equal_protection_under_the_algorithm.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.fatml.org/media/documents/equal_protection_under_the_<b>algorithm</b>.pdf", "snippet": "<b>machine</b> <b>learning</b>. There are many types of applications where this work may be relevant, and it is not possible to create a single <b>algorithm</b> that is applicable to all of them. Rather, we provide a high-level description of how one can de\ufb01ne an appropriate <b>algorithm</b>. 4.1. Terminology A <b>treatment</b> is a targeting action, such as showing an ad. A", "dateLastCrawled": "2022-02-02T07:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "6.S897 <b>Machine</b> <b>Learning</b> in Healthcare, Lecture 23: Fairness", "url": "https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-s897-machine-learning-for-healthcare-spring-2019/lecture-notes/MIT6_S897S19_lec23.pdf", "isFamilyFriendly": true, "displayUrl": "https://<b>ocw.mit.edu</b>/courses/electrical-engineering-and-computer-science/6-s897-<b>machine</b>...", "snippet": "<b>algorithm</b>, starting in October 2019. A county o\ufb03cial will then take that grade and use it to recommend whether the accused should be released or remain in jail. \u2022 \u201c\u2026 the <b>machine</b> <b>learning</b> systems used to calculate these risk scores throughout the criminal justice system, have been shown to hold severe", "dateLastCrawled": "2022-01-28T18:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Fall 2020 Journal | Algorithms and Child Welfare: The <b>Disparate</b> Impact ...", "url": "https://bppj.berkeley.edu/2021/02/02/algorithms-and-child-welfare-the-disparate-impact-of-family-surveillance-in-risk-assessment-technologies/", "isFamilyFriendly": true, "displayUrl": "https://bppj.berkeley.edu/2021/02/02/<b>algorithms</b>-and-child-welfare-the-<b>disparate</b>-impact...", "snippet": "We believe that if <b>machine</b> <b>learning</b> is to continue to be used in social services, the history of the data must be considered [34]. Through our literature review, we did not find evidence of regulation over the child welfare data used in <b>machine</b> <b>learning</b> technologies. At the time of writing, Pennsylvania\u2019s statutes on Child Protective Services did not include any guidance on the use of <b>machine</b> <b>learning</b> or artificial intelligence. Searches for the words \u201cautomated\u201d and \u201c<b>algorithm</b> ...", "dateLastCrawled": "2022-02-03T01:31:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Handling Discriminatory Biases in Data for <b>Machine Learning</b> | by ...", "url": "https://towardsdatascience.com/machine-learning-and-discrimination-2ed1a8b01038", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>machine-learning</b>-and-<b>discrimination</b>-2ed1a8b01038", "snippet": "<b>Disparate</b> <b>Treatment</b> \u2014 Involves classifying someone in an impermissible way. It involves the intent to discriminate, evidenced by explicit reference to group membership. <b>Disparate</b> Impact \u2014 Looks at the consequences of classification/decision making on certain groups. No intent is required and it is facially neutral. <b>Disparate</b> impact is often referred to as unintentional <b>discrimination</b>, whereas <b>disparate</b> <b>treatment</b> is intentional. Practices with a disproportionate impact on a particular ...", "dateLastCrawled": "2022-01-29T20:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A primer on AI <b>fairness</b>. What it is and the tradeoffs to be made | by ...", "url": "https://towardsdatascience.com/artificial-intelligence-fairness-and-tradeoffs-ce11ac284b63", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/artificial-intelligence-<b>fairness</b>-and-tradeoffs-ce11ac284b63", "snippet": "Discrimination Law: <b>Disparate</b> <b>Treatment</b> (formal vs intentional) vs <b>Disparate</b> Impact (20% rule, legal rule of thumb). <b>Disparate</b> <b>treatment</b> <b>can</b> <b>be thought</b> of as procedural <b>fairness</b>. The underlying philosophy is equality of opportunity. <b>Disparate</b> impact is distributive justice. There is tension between these two goals.", "dateLastCrawled": "2022-02-02T02:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Machine</b> <b>Learning</b> (CS 181) - 2018 Spring | Embedded EthiCS", "url": "https://embeddedethics.seas.harvard.edu/classes/cs-181-2018-spring", "isFamilyFriendly": true, "displayUrl": "https://embeddedethics.seas.harvard.edu/classes/cs-181-2018-spring", "snippet": "<b>machine</b> <b>learning</b> CS Module Overview: In this module, we probe the ways that <b>machine</b> <b>learning</b> models <b>can</b> be discriminatory and examine different methods for preventing discriminatory outcomes. We begin by introducing two concepts of discrimination: <b>disparate</b> <b>treatment</b> and <b>disparate</b> impact. We then use those concepts to argue that there are at ...", "dateLastCrawled": "2022-01-06T17:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "The promise of <b>machine learning in predicting treatment outcomes</b> in ...", "url": "https://onlinelibrary.wiley.com/doi/10.1002/wps.20882", "isFamilyFriendly": true, "displayUrl": "https://onlinelibrary.wiley.com/doi/10.1002/wps.20882", "snippet": "Predicting <b>treatment</b> response is just one relatively narrow use case where <b>machine</b> <b>learning</b> <b>can</b> add value and improve mental health care. Prediction <b>can</b> help with so many more clinical decisions and clinical processes. We could predict barriers that prevent an individual from engaging in care initially, or non-adherence or dropout from care after initiation. We could streamline patients to the appropriate level of care, such as self-guided programs vs. outpatient care, or intensive ...", "dateLastCrawled": "2022-01-22T03:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A Review of Challenges and Opportunities in <b>Machine</b> <b>Learning</b> for Health", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7233077/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7233077", "snippet": "Existing reviews of <b>machine</b> <b>learning</b> in the medical space have focused narrowly on biomedical applications 5, deep <b>learning</b> tasks well suited for healthcare 6, the need for transparency 7, and use of big data in precision medicine 8. Here, we emphasize the broad opportunities present in <b>machine</b> <b>learning</b> for healthcare and the careful considerations that must be made. We focus on the electronic health record (EHR), which documents the process of healthcare delivery and operational needs such ...", "dateLastCrawled": "2022-01-25T00:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Bias and Fairness in <b>Machine</b> <b>Learning</b>, Part 2: building a baseline ...", "url": "https://freecontent.manning.com/bias-and-fairness-in-machine-learning-part-2-building-a-baseline-model-and-features/", "isFamilyFriendly": true, "displayUrl": "https://freecontent.manning.com/bias-and-fairness-in-<b>machine</b>-<b>learning</b>-part-2-building...", "snippet": "This question goes hand in hand with our model\u2019s <b>disparate</b> <b>treatment</b>. Dalex has a very handy plot that <b>can</b> be used with tree-based models and linear models to help visualize the features our model is <b>learning</b> the most from. exp_tree.model_parts().plot() Figure 4. Feature importance of our bias-unaware model as reported by dalex. This visualization is taking feature importances directly from the Random Forest\u2019s feature importance attribute and is showing that priors_count and age are our ...", "dateLastCrawled": "2022-02-03T06:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Advancing Greater Fairness and Explainability for AI and <b>Machine</b> ...", "url": "https://www.capitalone.com/tech/machine-learning/advancing-greater-fairness-and-explainability-for-ai-and-machine-learning-across-the-banking-industry/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.capitalone.com</b>/tech/<b>machine</b>-<b>learning</b>/advancing-greater-fairness-and...", "snippet": "There is <b>disparate</b> <b>treatment</b>, treating people differently based on their protected attribute, and also <b>disparate</b> impact, in which the outcome of a policy could be evidence of discrimination. Banks want to be fair in both senses, with respect to the inputs to a decision as well as the outcomes of a decision. However, recent work by <b>machine</b> <b>learning</b> researchers shows that there are challenges when satisfying both notions of fairness in", "dateLastCrawled": "2022-01-05T10:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Fall 2020 Journal | Algorithms and Child Welfare: The <b>Disparate</b> Impact ...", "url": "https://bppj.berkeley.edu/2021/02/02/algorithms-and-child-welfare-the-disparate-impact-of-family-surveillance-in-risk-assessment-technologies/", "isFamilyFriendly": true, "displayUrl": "https://bppj.berkeley.edu/2021/02/02/<b>algorithms</b>-and-child-welfare-the-<b>disparate</b>-impact...", "snippet": "We believe that if <b>machine</b> <b>learning</b> is to continue to be used in social services, the history of the data must be considered [34]. Through our literature review, we did not find evidence of regulation over the child welfare data used in <b>machine</b> <b>learning</b> technologies. At the time of writing, Pennsylvania\u2019s statutes on Child Protective Services did not include any guidance on the use of <b>machine</b> <b>learning</b> or artificial intelligence. Searches for the words \u201cautomated\u201d and \u201c<b>algorithm</b> ...", "dateLastCrawled": "2022-02-03T01:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "The ethics of <b>algorithms</b>: key problems and solutions", "url": "https://link.springer.com/article/10.1007/s00146-021-01154-8", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00146-021-01154-8", "snippet": "Actions (1) and (2) may be performed by (semi-)autonomous <b>algorithms</b>\u2014such as <b>machine</b> <b>learning</b> (ML) <b>algorithms</b>\u2014and this complicates, (3) the attribution of responsibility for the effects of actions that an <b>algorithm</b> may trigger. Here, ML is of particular interest, as a field which includes deep <b>learning</b> architectures. Computer systems deploying ML <b>algorithms</b> may be described as \u201cautonomous\u201d or \u201csemi-autonomous\u201d, to the extent that their outputs are induced from data and thus, non ...", "dateLastCrawled": "2022-01-30T20:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "TOP: A Compiler-Based Framework for Optimizing <b>Machine</b> <b>Learning</b> ...", "url": "https://sites.cs.ucsb.edu/~yufeiding/publication/SysML18.pdf", "isFamilyFriendly": true, "displayUrl": "https://sites.cs.ucsb.edu/~yufeiding/publication/SysML18.pdf", "snippet": "<b>treatment</b> to various ML algorithms, and TOP, a compiler-based optimizer for e ectively applying TI to optimize <b>ma-chine</b> <b>learning</b> algorithms. Experiments show that TOP is able to automatically produce optimized algorithms that ei-ther matches or outperforms manually designed algorithms, giving up to 237x speedups and 2.5X on average1. 1. INTRODUCTION Vector dot products and point-to-point distance calcula-tions are essential to many important algorithms across var-ious domains. Vector dot ...", "dateLastCrawled": "2021-09-18T13:25:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine Learning and Discrimination</b> - GitHub Pages", "url": "https://harvard-ml-courses.github.io/cs181-web-2020/files/lecture12.pdf", "isFamilyFriendly": true, "displayUrl": "https://harvard-ml-courses.github.io/cs181-web-2020/files/lecture12.pdf", "snippet": "<b>Machine Learning and Discrimination</b> Diana Acosta-Navas PhD candidate, Harvard Philosophy Department Adjunct Lecturer in Ethics and Public Policy, Harvard Kennedy School . For Today\u2026 \u2022Discrimination/ wrongful discrimination \u2022Case Study: PredPol \u2022<b>Disparate</b> <b>treatment</b> vs. <b>Disparate</b> impact \u2022How predictive policing could wrongfully discriminate \u2022What contextual considerations are important to determine whether an <b>algorithm</b> wrongfully discriminates? For Today\u2026 \u2022Content warning ...", "dateLastCrawled": "2021-09-15T23:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Handling Discriminatory Biases in Data for <b>Machine Learning</b> | by ...", "url": "https://towardsdatascience.com/machine-learning-and-discrimination-2ed1a8b01038", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>machine-learning</b>-and-<b>discrimination</b>-2ed1a8b01038", "snippet": "<b>Disparate</b> <b>Treatment</b> \u2014 Involves classifying someone in an impermissible way. It involves the intent to discriminate, evidenced by explicit reference to group membership. <b>Disparate</b> Impact \u2014 Looks at the consequences of classification/decision making on certain groups. No intent is required and it is facially neutral. <b>Disparate</b> impact is often referred to as unintentional <b>discrimination</b>, whereas <b>disparate</b> <b>treatment</b> is intentional. Practices with a disproportionate impact on a particular ...", "dateLastCrawled": "2022-01-29T20:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Does mitigating ML&#39;s <b>disparate</b> impact require <b>disparate</b> <b>treatment</b>? - DeepAI", "url": "https://deepai.org/publication/does-mitigating-ml-s-disparate-impact-require-disparate-treatment", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/.../does-mitigating-ml-s-<b>disparate</b>-impact-require-<b>disparate</b>-<b>treatment</b>", "snippet": "Algorithms exhibit <b>disparate</b> impact if they affect subgroups differently. <b>Disparate</b> impact <b>can</b> arise unintentionally and absent <b>disparate</b> <b>treatment</b>. The natural way to reduce <b>disparate</b> impact would be to apply <b>disparate</b> <b>treatment</b> in favor of the disadvantaged group, i.e. to apply affirmative action. However, owing to the practice&#39;s contested ...", "dateLastCrawled": "2021-12-15T05:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Understanding and Reducing Bias in <b>Machine Learning</b> | by Jaspreet ...", "url": "https://towardsdatascience.com/understanding-and-reducing-bias-in-machine-learning-6565e23900ac", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-and-reducing-bias-in-<b>machine-learning</b>...", "snippet": "Even if we remove <b>disparate</b> <b>treatment</b> by removing the sensitive feature, discrimination <b>can</b> still happen through other correlated features such as zip code. Measuring and correcting <b>disparate</b> impact makes sure that this is corrected. This requirement should be used when the training dataset is biased. While being considered a controversial measure by many, notably by critics who hold that some scenarios cannot be freed from disproportionate outcomes.", "dateLastCrawled": "2022-01-28T17:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Algorithmic bias detection and mitigation: Best practices and policies ...", "url": "https://www.brookings.edu/research/algorithmic-bias-detection-and-mitigation-best-practices-and-policies-to-reduce-consumer-harms/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.brookings.edu</b>/research/<b>algorithm</b>ic-bias-detection-and-mitigation-best-", "snippet": "For example, the demonstration of <b>disparate</b> <b>treatment</b> does not describe the ways in which an <b>algorithm</b> <b>can</b> learn to treat similarly situated groups differently, as will be discussed later in the ...", "dateLastCrawled": "2022-02-03T04:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Two-stage <b>Algorithm</b> for <b>Fairness-aware</b> <b>Machine</b> <b>Learning</b> | DeepAI", "url": "https://deepai.org/publication/two-stage-algorithm-for-fairness-aware-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/two-stage-<b>algorithm</b>-for-<b>fairness-aware</b>-<b>machine</b>-<b>learning</b>", "snippet": "In other words, a <b>machine</b> <b>learning</b> <b>algorithm</b> that utilizes sensitive attributes is subject to biases in the existing data. This could be viewed as an algorithmic version of <b>disparate</b> <b>treatment</b> , where decisions are made on the basis of these sensitive attributes. However, removing sensitive attributes from the dataset is not sufficient solution as it has a <b>disparate</b> impact. <b>Disparate</b> impact is a notion that was born in the 1970s. The U.S. Supreme Court ruled that the hiring decision at the ...", "dateLastCrawled": "2021-12-10T00:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Disparate</b> Impact Analysis", "url": "https://h2oai.github.io/tutorials/disparate-impact-analysis/", "isFamilyFriendly": true, "displayUrl": "https://h2oai.github.io/tutorials/<b>disparate</b>-impact-analysis", "snippet": "<b>Machine</b> <b>learning</b> models <b>can</b> make drastically differing predictions for only minor changes in input variable values. For example, when looking at predictions that determine financial decisions, SA <b>can</b> be used to help you understand the impact of changing the most important input variables and the impact of changing socially sensitive variables (such as Sex, Age, Race, etc.) in the model. If the model changes in reasonable and expected ways when important variable values are changed, this <b>can</b> ...", "dateLastCrawled": "2022-02-03T09:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Interventions | <b>Machine Learning Bias Mitigation</b>", "url": "https://cdeiuk.github.io/bias-mitigation/interventions/", "isFamilyFriendly": true, "displayUrl": "https://cdeiuk.github.io/bias-mitigation/interventions", "snippet": "Furthermore, as noted above, the decision threshold modification <b>algorithm</b> of Hardt et al. is optimal among post-processing algorithms for equalised odds and equal opportunity, which means we <b>can</b>&#39;t expect better performance from this intervention. That said, since the intervention of Hardt et al. introduces some stochasticity to predictions, if that is unacceptable then this might be a viable alternative.", "dateLastCrawled": "2022-02-02T02:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Evaluating Fairness in <b>Machine</b> <b>Learning</b> - <b>github.com</b>", "url": "https://github.com/KenSciResearch/fairMLHealth/blob/integration/docs/resources/Evaluating_Fairness.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/KenSciResearch/fairMLHealth/blob/integration/docs/resources/...", "snippet": "<b>Machine</b> <b>learning</b> models <b>can</b> also be a source of <b>disparate</b> impact in their implementation, through unconscious human biases that affect the fair interpretation or use of the model&#39;s results. This reference does not cover measurement of fairness at implementation. However, if you are interested in fair implementation, we recommend looking at Google&#39;s Fairness Indicators. Harms. In evaluating the potential impact of an ML model, it <b>can</b> be helpful to first clarify what specific harm(s) <b>can</b> be ...", "dateLastCrawled": "2022-02-02T03:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Bias and Fairness in Machine Learning</b> - Abhishek Tiwari", "url": "https://www.abhishek-tiwari.com/bias-and-fairness-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.abhishek-tiwari.com/<b>bias-and-fairness-in-machine-learning</b>", "snippet": "Wall Street Journal investigators showed that Staples\u2019 online pricing <b>algorithm</b> discriminated against lower-income people; Black people were more likely to be assessed as having a higher risk of recidivism when using commercial prediction tools such as COMPAS ; An insurance company that used <b>machine</b> <b>learning</b> to workout insurance premiums involuntarily discriminated against elderly patients; A credit card company used tracking information to personalize offers steering minorities into ...", "dateLastCrawled": "2022-01-29T16:42:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine</b> <b>learning</b> and applications in microbiology", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8498514/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8498514", "snippet": "<b>Machine</b> <b>learning</b> has two main <b>learning</b> modes: supervised (also known as predictive) to make future predictions from training data, and unsupervised (descriptive), which is exploratory in nature without training data, defined target or output (Mitchell 1997). Training data are the initial information used to teach supervised ML algorithms in the process of developing a model, from which the model creates and refines its rules required for prediction. Typically, training data comprises a set ...", "dateLastCrawled": "2021-12-06T07:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Adversarial Approaches to Debiasing Word Embeddings", "url": "https://web.stanford.edu/class/cs224n/reports/final_reports/report101.pdf", "isFamilyFriendly": true, "displayUrl": "https://web.stanford.edu/class/cs224n/reports/final_reports/report101.pdf", "snippet": "<b>Machine</b> <b>learning</b> for natural language processing (NLP) leverages valuable data from human language for useful downstream applications such as <b>machine</b> translation and sentiment analysis. Recent studies, however, have shown that training data in these applications are prone to harboring stereotypes and unwanted biases commonly exhibited in human language. Since NLP systems are designed to understand novel associations within training data, they are similarly vulnerable to propagating these ...", "dateLastCrawled": "2022-01-25T20:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Realistically Integrating <b>Machine</b> <b>Learning</b> Into Clinical Pra ...", "url": "https://journals.lww.com/anesthesia-analgesia/fulltext/2020/05000/realistically_integrating_machine_learning_into.4.aspx", "isFamilyFriendly": true, "displayUrl": "https://<b>journals.lww.com</b>/.../05000/realistically_integrating_<b>machine</b>_<b>learning</b>_into.4.aspx", "snippet": "<b>Machine</b>-<b>learning</b> models have been created to predict an increasing number of clinical outcomes, such as diagnoses and mortality, with applications including C. difficile infection in the inpatient hospital setting, 2 identifying molecular markers for cancer treatments, 3 and postoperative surgical outcomes. 4 Examples of <b>machine</b> <b>learning</b> include a cardiologist using an automated interpretation of an ECG and a radiologist using an automated detection of a lung nodule in a chest x-ray. In both ...", "dateLastCrawled": "2021-11-22T21:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Machine learning</b>, artificial neural networks and social research ...", "url": "https://link.springer.com/article/10.1007/s11135-020-01037-y", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11135-020-01037-y", "snippet": "<b>Machine learning</b> (ML), and particularly algorithms based on artificial neural networks (ANNs), constitute a field of research lying at the intersection of different disciplines such as mathematics, statistics, computer science and neuroscience. This approach is characterized by the use of algorithms to extract knowledge from large and heterogeneous data sets. In addition to offering a brief introduction to ANN algorithms-based ML, in this paper we will focus our attention on its possible ...", "dateLastCrawled": "2022-01-27T04:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Fairness in machine learning with tractable models</b> - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S0950705120308443", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0950705120308443", "snippet": "<b>Machine</b> <b>Learning</b> techniques have become pervasive across a range of different applications, and are now widely used in areas as <b>disparate</b> as recidivism prediction, consumer credit\u2013risk analysis and insurance pricing. The prevalence of <b>machine</b> <b>learning</b> techniques has raised concerns about the potential for learned algorithms to become biased against certain groups. Many definitions have been proposed in the literature, but the fundamental task of reasoning about probabilistic events is a ...", "dateLastCrawled": "2022-01-20T21:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Can AI learn good manners</b>? | Odgers Berndtson", "url": "https://www.odgersberndtson.com/en-gb/insights/can-ai-learn-good-manners", "isFamilyFriendly": true, "displayUrl": "https://www.odgersberndtson.com/en-gb/insights/<b>can-ai-learn-good-manners</b>", "snippet": "\u201cThe challenge of fully mitigating both <b>disparate</b> <b>treatment</b> and <b>disparate</b> impact risks requires a discussion between business leaders, data scientists and legal experts to determine the best risk management strategy for each application. It also requires a decidedly human-centred approach to instil confidence that <b>machine</b>-generated decisions are being made with the customer\u2019s interest in mind.\u201d", "dateLastCrawled": "2022-01-22T11:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Solution Manual Alpaydin Introduction To <b>Machine</b> <b>Learning</b>", "url": "https://se.rangle.io/solution+manual+alpaydin+introduction+to+machine+learning+pdf", "isFamilyFriendly": true, "displayUrl": "https://se.rangle.io/solution+manual+alpaydin+introduction+to+<b>machine</b>+<b>learning</b>+pdf", "snippet": "With in-depth Python and MATLAB/OCTAVE-based computational exercises and a complete <b>treatment</b> of cutting edge numerical optimization techniques, this is an essential resource for students and an ideal reference for researchers and practitioners working in <b>machine</b> <b>learning</b>, computer science, electrical engineering, signal processing, and numerical optimization. Handbook of Research on Innovations in Database Technologies and Applications One of the currently most active research areas within ...", "dateLastCrawled": "2022-01-11T16:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Latent bias and the implementation of <b>artificial intelligence</b> in ...", "url": "https://academic.oup.com/jamia/article/27/12/2020/5859726", "isFamilyFriendly": true, "displayUrl": "https://academic.oup.com/jamia/article/27/12/2020/5859726", "snippet": "<b>Artificial intelligence</b> (AI) in general, and <b>machine</b> <b>learning</b> in particular, by all accounts, appear poised to revolutionize medicine. 1\u20133 With a wide spectrum of potential uses across translational research (from bench to bedside to health policy), clinical medicine (including diagnosis, <b>treatment</b>, prediction, and healthcare resource allocation), and public health, every area of medicine will be affected.", "dateLastCrawled": "2022-01-28T13:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Recurrence in biological and artificial neural <b>networks</b> | by Matthew ...", "url": "https://towardsdatascience.com/recurrence-in-biological-and-artificial-neural-networks-e8a6d5639781", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/recurrence-in-biological-and-artificial-neural-<b>networks</b>...", "snippet": "Recurrence is an overloaded term in the context of neural <b>networks</b>, with <b>disparate</b> colloquial meanings in the <b>machine</b> <b>learning</b> and the neuroscience communities. The difference is narrowing, however, as the artificial neural <b>networks</b> (ANNs) used for practical applications are increasingly sophisticated and more like biological neural <b>networks</b> (BNNs) in some ways (yet still vastly different on the whole).In this post we\u2019ll discuss the historic di f ferences in the use of term recurrence ...", "dateLastCrawled": "2022-01-14T19:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "47 <b>Analogy</b> Examples To Make You As Sharp As A Tack (and then some)", "url": "https://www.greetingcardpoet.com/good-analogy-examples-and-definition/", "isFamilyFriendly": true, "displayUrl": "https://www.greetingcardpoet.com/<b>good-analogy-examples-and-definition</b>", "snippet": "The essence of this literary device is to set up a comparison that highlights similarities between two seemingly <b>disparate</b> items. It is by examining how the two items are alike in some way that leads to a clear understanding. Differences between Similes, Metaphors, and Analogies . While similes, metaphors, and analogies are similar in that they all compare two different things, similes and metaphors are figures of speech. In contrast, an <b>analogy</b> is more akin to a logical argument. A writer ...", "dateLastCrawled": "2022-02-02T03:57:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A primer on AI <b>fairness</b>. What it is and the tradeoffs to be made | by ...", "url": "https://towardsdatascience.com/artificial-intelligence-fairness-and-tradeoffs-ce11ac284b63", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/artificial-intelligence-<b>fairness</b>-and-tradeoffs-ce11ac284b63", "snippet": "A <b>machine</b> <b>learning</b> algorithms value is being able to increase the number of true positives and true negatives, which each have a value attached. Each false positive and false negative is costly. The value assigned to each depends on each context. A false negative is more costly in medical situations while a false positive is costlier in death penalty decisions. Expected value is profits that businesses can expect from using the algorithm. The more accurate the model, the higher the profits.", "dateLastCrawled": "2022-02-02T02:42:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(disparate treatment)  is like +(machine learning algorithm)", "+(disparate treatment) is similar to +(machine learning algorithm)", "+(disparate treatment) can be thought of as +(machine learning algorithm)", "+(disparate treatment) can be compared to +(machine learning algorithm)", "machine learning +(disparate treatment AND analogy)", "machine learning +(\"disparate treatment is like\")", "machine learning +(\"disparate treatment is similar\")", "machine learning +(\"just as disparate treatment\")", "machine learning +(\"disparate treatment can be thought of as\")", "machine learning +(\"disparate treatment can be compared to\")"]}