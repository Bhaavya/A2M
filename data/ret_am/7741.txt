{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Addressing Fairness in <b>Machine</b> <b>Learning</b> Predictions: Strategic Best ...", "url": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3389631", "isFamilyFriendly": true, "displayUrl": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3389631", "snippet": "One exception is those algorithms that use <b>equalized</b> <b>odds</b> as a fairness criterion which can decrease disparity in behavior. However, they cannot be used in many practical settings. We propose a new class of fair <b>machine</b> <b>learning</b> algorithms that alleviate disparity in prediction results, disparity in behavior of prediction subjects, and does not need to account for the sensitive variable explicitly. Our algorithm also complies with the notion of equal treatment and explainable AI, and can be ...", "dateLastCrawled": "2022-01-15T12:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Fairness in Machine Learning: How</b> Can a <b>Model Trained on Aerial Imagery</b> ...", "url": "https://opendri.org/wp-content/uploads/2020/05/CatherineInness_-Fairness_in_ML_for_DRM.pdf", "isFamilyFriendly": true, "displayUrl": "https://opendri.org/wp-content/uploads/2020/05/CatherineInness_-Fairness_in_ML_for_DRM.pdf", "snippet": "One common definition of fairness in <b>machine</b> <b>learning</b> is the concept of <b>equalized</b> <b>odds</b> (Hardt, Price, &amp; Srebro [1]), ... A major limitation of the <b>equalized</b> <b>odds</b> measure is that we need to know our protected groups in advance so as to code them into the model, which often requires prior knowledge of the domain. According to a 2018 World Bank report, Building Back Better [2], the poorest people are disproportionately impacted by disasters (the report urges us to consider the experience of a 1 ...", "dateLastCrawled": "2021-12-05T09:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Fairness in Machine Learning</b> \u2014 Labelia (ex Substra Foundation)", "url": "https://www.labelia.org/en/blog/fairness-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.labelia.org/en/blog/<b>fairness-in-machine-learning</b>", "snippet": "Some are exclusive of the other (<b>like</b> <b>equalized</b> <b>odds</b> and demographic parity, see the explanation below of the 3 main definitions). A company, or organization, will have to decide which definition applies to its product(s).", "dateLastCrawled": "2022-01-30T18:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Trustworthy <b>Machine</b> <b>Learning</b> - Kush R. Varshney - Chapter 10: Fairness", "url": "http://www.trustworthymachinelearning.com/trustworthymachinelearning-10.htm", "isFamilyFriendly": true, "displayUrl": "www.trustworthy<b>machinelearning</b>.com/trustworthy<b>machinelearning</b>-10.htm", "snippet": "In assistive cases <b>like</b> receiving extra care, separation (<b>equalized</b> <b>odds</b>) is the preferred fairness metric because it relates to recall (true positive rate), which is of primary concern in these settings. If receiving care management had been a non-punitive act, then sufficiency (calibration) would have been the preferred fairness metric because precision is of primary concern in non-punitive settings. (Precision is equivalent to positive predictive value, which is one of the two components ...", "dateLastCrawled": "2022-01-07T03:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "An overview of active <b>learning</b> methods for <b>insurance</b> with fairness ...", "url": "https://deepai.org/publication/an-overview-of-active-learning-methods-for-insurance-with-fairness-appreciation", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/an-overview-of-active-<b>learning</b>-methods-for-<b>insurance</b>...", "snippet": "Indeed, the <b>insurance</b> use cases integrating <b>machine</b> <b>learning</b> are numerous, while the competition with new actors (GAFAM, Insurtechs) creates pressure on margins and a risk of adverse selection. Therefore the actuary must seize these new and efficient methodologies to keep and reinforce their expertise of the risk. The precision of <b>machine</b> <b>learning</b> algorithm to provide a better segmentation of risk, to achieve large scale automation (e.g. using IoT technologies, extracting information from ...", "dateLastCrawled": "2022-01-29T00:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "How to set up Model Governance as you operationalize <b>Machine</b> <b>Learning</b> ...", "url": "https://krishnagade.medium.com/how-to-set-up-model-governance-as-you-operationalize-machine-learning-ae2ba3700926", "isFamilyFriendly": true, "displayUrl": "https://krishnagade.medium.com/how-to-set-up-model-governance-as-you-operationalize...", "snippet": "A growing number of model metrics are being proposed to quantify model bias using demographic parity, <b>equalized</b> <b>odds</b>, and other group fairness metrics [10]. Adding more metrics to an already non-scalable, manual model monitoring process does not help.", "dateLastCrawled": "2022-02-02T09:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Evaluating Fairness in <b>Machine</b> <b>Learning</b> - <b>github.com</b>", "url": "https://github.com/KenSciResearch/fairMLHealth/blob/integration/docs/resources/Evaluating_Fairness.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/KenSciResearch/fairMLHealth/blob/integration/docs/resources/...", "snippet": "Evaluating Fairness in <b>Machine</b> <b>Learning</b>. by Christine Allen. About. This reference introduces concepts, methods, and libraries for measuring fairness in ML as it relates to problems in healthcare. This is a revamped version of the tutorial presented at the KDD 2020 Tutorial on Fairness in <b>Machine</b> <b>Learning</b> for Healthcare. There are abundant other publications covering the theoretical basis for fairness metrics, with many online and academic resources covering the details of specific fairness ...", "dateLastCrawled": "2022-02-02T03:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Machine learning fairness notions: Bridging the</b> gap with real-world ...", "url": "https://www.sciencedirect.com/science/article/pii/S0306457321001321", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0306457321001321", "snippet": "Fairness in <b>machine</b> <b>learning</b> can be categorized according to two dimensions, namely, the task and the type of <b>learning</b>. For the first dimension, there are two tasks in fairness-aware <b>machine</b> <b>learning</b>: discrimination discovery (or assessment) and discrimination removal (or prevention). Discrimination discovery task focuses on assessing and measuring bias in datasets or in predictions made by the MLDM. Discrimination removal focuses on preventing discrimination by manipulating datasets (pre ...", "dateLastCrawled": "2022-02-03T07:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "arXiv:2009.06516v1 [cs.AI] 14 Sep 2020", "url": "https://debabrota-basu.github.io/pdfs/justicia2020.pdf", "isFamilyFriendly": true, "displayUrl": "https://debabrota-basu.github.io/pdfs/justicia2020.pdf", "snippet": "<b>Machine</b> <b>learning</b> (ML) is becoming the omnipresent technology of our time. ML algorithms are being used for high-stake decisions <b>like</b> college admissions, crime recidivism, <b>insurance</b>, and loan decisions etc. Thus, human lives are now pervasively in uenced by data, ML, and their inherent bias.", "dateLastCrawled": "2021-11-21T02:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Fairness in Machine Learning is Tricky</b> | by John Dickerson | Arthur AI ...", "url": "https://medium.com/arthur-ai/fairness-in-machine-learning-is-tricky-1ea47111b847", "isFamilyFriendly": true, "displayUrl": "https://medium.com/arthur-ai/<b>fairness-in-machine-learning-is-tricky</b>-1ea47111b847", "snippet": "Human decision-making processes are known to be biased. Look at the promotions process at a typical large company. In the words of Tomas Chamorro-Premuzic, the Chief Talent Scientist at one of the\u2026", "dateLastCrawled": "2021-04-29T13:16:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Fairness in Machine Learning: How</b> Can a <b>Model Trained on Aerial Imagery</b> ...", "url": "https://opendri.org/wp-content/uploads/2020/05/CatherineInness_-Fairness_in_ML_for_DRM.pdf", "isFamilyFriendly": true, "displayUrl": "https://opendri.org/wp-content/uploads/2020/05/CatherineInness_-Fairness_in_ML_for_DRM.pdf", "snippet": "One common definition of fairness in <b>machine</b> <b>learning</b> is the concept of <b>equalized</b> <b>odds</b> (Hardt, Price, &amp; Srebro [1]), ... A major limitation of the <b>equalized</b> <b>odds</b> measure is that we need to know our protected groups in advance so as to code them into the model, which often requires prior knowledge of the domain. According to a 2018 World Bank report, Building Back Better [2], the poorest people are disproportionately impacted by disasters (the report urges us to consider the experience of a 1 ...", "dateLastCrawled": "2021-12-05T09:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Achieving Trusted AI With Model Interpretability", "url": "https://blog.dataiku.com/model-interpretability", "isFamilyFriendly": true, "displayUrl": "https://blog.dataiku.com/model-interpretability", "snippet": "In model fairness terms, this is measured as <b>equalized</b> <b>odds</b>, meaning that you had just as likely a chance of your application being rejected as other people who had their application rejected. And conversely, you had just as good a chance of being accepted to university as others who were accepted. In other words, the model showed no favoritism in either direction and treated you exactly as it would have regardless.", "dateLastCrawled": "2022-01-31T14:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "ALGORITHMIC FAIRNESS: MEASURES, METHODS AND REPRESENTATIONS", "url": "https://www.cs.utah.edu/~suresh/static/files/tutorial.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.utah.edu/~suresh/static/files/tutorial.pdf", "snippet": "<b>Machine</b> <b>Learning</b> Algorithms Databases HCI Other Sociology The Law Economics Political Science Philosophy Media Studies. GOALS FOR THIS TUTORIAL. GOALS FOR THIS TUTORIAL \u2022 An overview of the state of play in (some) areas of research in fairness. GOALS FOR THIS TUTORIAL \u2022 An overview of the state of play in (some) areas of research in fairness \u2022 Some open questions coming out of these areas. GOALS FOR THIS TUTORIAL \u2022 An overview of the state of play in (some) areas of research in ...", "dateLastCrawled": "2022-02-03T18:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Evaluating Fairness in <b>Machine</b> <b>Learning</b> - <b>github.com</b>", "url": "https://github.com/KenSciResearch/fairMLHealth/blob/integration/docs/resources/Evaluating_Fairness.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/KenSciResearch/fairMLHealth/blob/integration/docs/resources/...", "snippet": "Evaluating Fairness in <b>Machine</b> <b>Learning</b>. by Christine Allen. About. This reference introduces concepts, methods, and libraries for measuring fairness in ML as it relates to problems in healthcare. This is a revamped version of the tutorial presented at the KDD 2020 Tutorial on Fairness in <b>Machine</b> <b>Learning</b> for Healthcare. There are abundant other publications covering the theoretical basis for fairness metrics, with many online and academic resources covering the details of specific fairness ...", "dateLastCrawled": "2022-02-02T03:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine learning fairness notions: Bridging the</b> gap with real-world ...", "url": "https://www.sciencedirect.com/science/article/pii/S0306457321001321", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0306457321001321", "snippet": "Fairness in <b>machine</b> <b>learning</b> can be categorized according to two dimensions, namely, the task and the type of <b>learning</b>. For the first dimension, there are two tasks in fairness-aware <b>machine</b> <b>learning</b>: discrimination discovery (or assessment) and discrimination removal (or prevention). Discrimination discovery task focuses on assessing and measuring bias in datasets or in predictions made by the MLDM. Discrimination removal focuses on preventing discrimination by manipulating datasets (pre ...", "dateLastCrawled": "2022-02-03T07:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "EXPLAINABILITY OF <b>MACHINE</b> <b>LEARNING</b> IN THE FINANCIAL SECTOR", "url": "https://scikit-learn.fondation-inria.fr/wp-content/uploads/sites/3/2021/02/Explainabilityforscikit-learnworkshop.pdf", "isFamilyFriendly": true, "displayUrl": "https://scikit-learn.fondation-inria.fr/wp-content/uploads/sites/3/2021/02/Explain...", "snippet": "EXPLAINABILITY OF <b>MACHINE</b> <b>LEARNING</b> IN THE FINANCIAL SECTOR. 2 AUDIENCE OF AN EXPLANATION Intelligibility Fidelity Satisfaction Acceptance Compliance Internal controls Audit Dev / Data / DevOps / MLOps Impacted Individuals Societal Issues Ethics Charter Transversal and sectoral regulation End Users. EXPLAINABILITY : EXPLANATION LEVELS 3 I n c r e a s i n g d e m a n d s o n e x p l a n a tio n. EXPLAINABILITY : EXPLANATION LEVELS 4 I n c r e a s i n g d e m a n d s o n e x p l a n a tio n ...", "dateLastCrawled": "2022-01-14T09:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Fairness in Machine Learning is Tricky</b> - Arthur AI", "url": "https://www.arthur.ai/blog/fairness-in-ml", "isFamilyFriendly": true, "displayUrl": "https://www.arthur.ai/blog/fairness-in-ml", "snippet": "<b>Fairness in Machine Learning is Tricky</b>. Non-experts and experts alike have trouble even understanding popular definitions of <b>fairness in machine learning</b> \u2014 let alone agreeing on which definitions, if any, should be used in practice. Human decision-making processes are known to be biased. Look at the promotions process at a typical large company.", "dateLastCrawled": "2022-02-03T15:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "We Want Fair AI Algorithms \u2013 But How To Define Fairness? (Fairness ...", "url": "https://mostly.ai/blog/we-want-fair-ai-algorithms-but-how-to-define-fairness/", "isFamilyFriendly": true, "displayUrl": "https://<b>mostly.ai</b>/blog/we-want-fair-ai-algorithms-but-how-to-define-fairness", "snippet": "In order to build fair <b>machine</b> <b>learning</b> systems, we need to precisely define and quantify what we mean by a fair outcome. There are several mathematical definitions that do just that and on a high level, these notions fall into two categories: group and individual fairness. Group fairness and parity constraints aim to achieve the same outcomes across different demographics, or more generally, a set of protected population classes. In other words, the population that receives a given ...", "dateLastCrawled": "2022-01-29T01:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Trustworthy <b>Machine</b> <b>Learning</b> - Kush R. Varshney - Chapter 10: Fairness", "url": "http://www.trustworthymachinelearning.com/trustworthymachinelearning-10.htm", "isFamilyFriendly": true, "displayUrl": "www.trustworthy<b>machinelearning</b>.com/trustworthy<b>machinelearning</b>-10.htm", "snippet": "Your task now is to develop a detailed problem specification for a fair <b>machine</b> <b>learning</b> system for allocating care management programs to Sospital members and proceed along the different phases of the <b>machine</b> <b>learning</b> lifecycle without taking shortcuts. In this chapter, you will: \u00a7 compare and contrast definitions of fairness in a <b>machine</b> <b>learning</b> context, \u00a7 select an appropriate notion of fairness for your task, and \u00a7 mitigate unwanted biases at various points in the modeling pipeline ...", "dateLastCrawled": "2022-01-07T03:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Fairness in Machine Learning is Tricky</b> | by John Dickerson | Arthur AI ...", "url": "https://medium.com/arthur-ai/fairness-in-machine-learning-is-tricky-1ea47111b847", "isFamilyFriendly": true, "displayUrl": "https://medium.com/arthur-ai/<b>fairness-in-machine-learning-is-tricky</b>-1ea47111b847", "snippet": "Human decision-making processes are known to be biased. Look at the promotions process at a typical large company. In the words of Tomas Chamorro-Premuzic, the Chief Talent Scientist at one of the\u2026", "dateLastCrawled": "2021-04-29T13:16:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Fairness in Machine Learning: How</b> <b>Can</b> a <b>Model Trained on Aerial Imagery</b> ...", "url": "https://opendri.org/wp-content/uploads/2020/05/CatherineInness_-Fairness_in_ML_for_DRM.pdf", "isFamilyFriendly": true, "displayUrl": "https://opendri.org/wp-content/uploads/2020/05/CatherineInness_-Fairness_in_ML_for_DRM.pdf", "snippet": "One common definition of fairness in <b>machine</b> <b>learning</b> is the concept of <b>equalized</b> <b>odds</b> (Hardt, Price, &amp; Srebro [1]), which is a form of classification parity that quantifies the extent to which false-positive and false-negative errors occur at the same rate for a protected group (a subgroup of the", "dateLastCrawled": "2021-12-05T09:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Accuracy and Fairness Trade-o s in <b>Machine</b> <b>Learning</b>: A Stochastic Multi ...", "url": "https://engineering.lehigh.edu/sites/engineering.lehigh.edu/files/_DEPARTMENTS/ise/pdf/tech-papers/20/20T_016.pdf", "isFamilyFriendly": true, "displayUrl": "https://engineering.lehigh.edu/sites/engineering.lehigh.edu/files/_DEPARTMENTS/ise/pdf/...", "snippet": "disadvantageous groups. In the literature of fair <b>machine</b> <b>learning</b>, several prevailing criteria for fairness include dis-parate impact (Barocas &amp; Selbst,2016) (also called demo-graphic parity (Calders et al.,2009)), <b>equalized</b> <b>odds</b> (Hardt etal.,2016), anditsspecialcaseof equalopportunity (Hardt et al.,2016), corresponding to different aspects of ...", "dateLastCrawled": "2022-01-28T20:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "ACCURACY AND FAIRNESS TRADE-OFFS IN <b>MACHINE</b> APPROACH", "url": "https://www.mat.uc.pt/preprints/ps/p2037.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.mat.uc.pt/preprints/ps/p2037.pdf", "snippet": "ACCURACY AND FAIRNESS TRADE-OFFS IN <b>MACHINE</b> <b>LEARNING</b>: A STOCHASTIC MULTI-OBJECTIVE APPROACH S. LIU AND L. N. VICENTE Abstract: In the application of <b>machine</b> <b>learning</b> to real-life decision-making sys- tems, e.g., credit scoring and criminal justice, the prediction outcomes might dis-criminate against people with sensitive attributes, leading to unfairness. The com-monly used strategy in fair <b>machine</b> <b>learning</b> is to include fairness as a constraint or a penalization term in the minimization of ...", "dateLastCrawled": "2022-01-09T06:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Accuracy and Fairness Trade-offs in <b>Machine</b> <b>Learning</b>: A Stochastic ...", "url": "http://www.optimization-online.org/DB_FILE/2020/08/7950.pdf", "isFamilyFriendly": true, "displayUrl": "www.optimization-online.org/DB_FILE/2020/08/7950.pdf", "snippet": "Accuracy and Fairness Trade-offs in <b>Machine</b> <b>Learning</b>: A Stochastic Multi-Objective Approach Suyun Liu* 1 Luis Nunes Vicente* 1 2 Abstract In the application of <b>machine</b> <b>learning</b> to real- life decision-making systems, e.g., credit scor-ing and criminal justice, the prediction outcomes might discriminate against people with sensi-tive attributes, leading to unfairness. The com-monly used strategy in fair <b>machine</b> <b>learning</b> is to include fairness as a constraint or a penal-ization term in the ...", "dateLastCrawled": "2021-11-22T20:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "AI RMF RFI Comments - Monitaur", "url": "https://www.nist.gov/document/ai-rmf-rfi-comments-monitaur-inc", "isFamilyFriendly": true, "displayUrl": "https://<b>www.nist.gov</b>/document/ai-rmf-rfi-comments-monitaur-inc", "snippet": "AI actors <b>can</b> use the methodology of <b>Machine</b> <b>Learning</b> Assurance, which takes advantage of the established, effective CRISP-DM framework already familiar to many organizations in order to accelerate adoption and education. Achieving transparency, fairness, and accountability with AI systems will require organizations to pursue context, veri\ufb01ability, and objectivity as the primar y goals of their governance and assurance effor ts. We look for ward to par ticipating in NIST\u2019s AI RMF in the ...", "dateLastCrawled": "2021-12-12T16:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Fairness in Machine Learning</b>", "url": "http://www.homepages.ucl.ac.uk/~ucgtrbd/talks/counterfactual_fairness_talk.pdf", "isFamilyFriendly": true, "displayUrl": "www.homepages.ucl.ac.uk/~ucgtrbd/talks/counterfactual_fairness_talk.pdf", "snippet": "\u2022<b>Machine</b> <b>learning</b> is good old statistical science with a fancy hat. \u2022Granted, having a different motivation (Artificial Intelligence) does have a practical implication on how we do data analysis. \u2022In particular, <b>machine</b> <b>learning</b> does come with one major cultural baggage: an emphasis on (semi)autonomous decision making \u2022What is a more down-to-earth name for AI? Autonomous systems. Implications \u2022The pipeline from data to decisions is less supervised by humans. \u2022There is a risk of ...", "dateLastCrawled": "2021-11-09T17:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Equalizing Recourse across Groups</b> | DeepAI", "url": "https://deepai.org/publication/equalizing-recourse-across-groups", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>equalizing-recourse-across-groups</b>", "snippet": "The rise in <b>machine</b> <b>learning</b>-assisted decision-making has led to concerns about the fairness of the decisions and techniques to mitigate problems of discrimination.If a negative decision is made about an individual (denying a loan, rejecting an application for housing, and so on) justice dictates that we be able to ask how we might change circumstances to get a favorable decision the next time.", "dateLastCrawled": "2021-12-22T15:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Trustworthy <b>Machine</b> <b>Learning</b> - Kush R. Varshney - Chapter 10: Fairness", "url": "http://www.trustworthymachinelearning.com/trustworthymachinelearning-10.htm", "isFamilyFriendly": true, "displayUrl": "www.trustworthy<b>machinelearning</b>.com/trustworthy<b>machinelearning</b>-10.htm", "snippet": "Your task now is to develop a detailed problem specification for a fair <b>machine</b> <b>learning</b> system for allocating care management programs to Sospital members and proceed along the different phases of the <b>machine</b> <b>learning</b> lifecycle without taking shortcuts. In this chapter, you will: \u00a7 compare and contrast definitions of fairness in a <b>machine</b> <b>learning</b> context, \u00a7 select an appropriate notion of fairness for your task, and \u00a7 mitigate unwanted biases at various points in the modeling pipeline ...", "dateLastCrawled": "2022-01-07T03:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Racial treatment disparities after <b>machine</b> <b>learning</b> surgical risk ...", "url": "https://link.springer.com/article/10.1007/s10742-020-00231-7", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10742-020-00231-7", "snippet": "<b>Machine</b> <b>learning</b>-based variable selection optimizes the model fit to better match the relationship between clinical variables and a given treatment. Analysis includes models with and without hospital and physician fixed effects to investigate the potential roles of access, physician selection, and other time-invariant endogeneity. I compare the proposed adjustment method with the standard Elixhauser-based adjustment, which uses the presence of predetermined comorbid conditions to adjust for ...", "dateLastCrawled": "2021-10-18T23:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "doi : https://doi.org/10.32628/CSEIT217251 PCIV method for Indirect ...", "url": "https://ijsrcseit.com/paper/CSEIT217251.pdf", "isFamilyFriendly": true, "displayUrl": "https://ijsrcseit.com/paper/CSEIT217251.pdf", "snippet": "etc. <b>can</b> form the basis of unethical bias in data or the algorithm. As the world is becoming more and more dependent on AI algorithms for making a wide range of decisions such as to determine access to services such as credit, <b>insurance</b>, and employment, the fairness &amp; ethical aspects of the models are becoming increasingly important. There are many bias detection &amp; mitigation algorithms which have evolved and many of the algorithms handle indirect attributes as well without requiring to ...", "dateLastCrawled": "2021-11-18T17:04:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Fairness in Machine Learning: How</b> <b>Can</b> a <b>Model Trained on Aerial Imagery</b> ...", "url": "https://opendri.org/wp-content/uploads/2020/05/CatherineInness_-Fairness_in_ML_for_DRM.pdf", "isFamilyFriendly": true, "displayUrl": "https://opendri.org/wp-content/uploads/2020/05/CatherineInness_-Fairness_in_ML_for_DRM.pdf", "snippet": "dollar loss for a rich and a poor person). They are less likely to have savings or <b>insurance</b> and they are more likely to be living in a subsistence manner. This suggests that the pursuit of <b>equalized</b> <b>odds</b> with regard to income level may be a valuable aim for a <b>machine</b> <b>learning</b> model in this area.", "dateLastCrawled": "2021-12-05T09:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "An overview of active <b>learning</b> methods for <b>insurance</b> with fairness ...", "url": "https://deepai.org/publication/an-overview-of-active-learning-methods-for-insurance-with-fairness-appreciation", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/an-overview-of-active-<b>learning</b>-methods-for-<b>insurance</b>...", "snippet": "This paper addresses and solves some challenges in the adoption of <b>machine</b> <b>learning</b> in <b>insurance</b> with the democratization of model deployment. The first challenge is reducing the labelling effort (hence focusing on the data quality) with the help of active <b>learning</b>, a feedback loop between the model inference and an oracle: as in <b>insurance</b> the unlabeled data is usually abundant, active <b>learning</b> <b>can</b> become a significant asset in reducing the labelling cost.For that purpose, this paper ...", "dateLastCrawled": "2022-01-29T00:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Accuracy and Fairness Trade-o s in <b>Machine</b> <b>Learning</b>: A Stochastic Multi ...", "url": "https://engineering.lehigh.edu/sites/engineering.lehigh.edu/files/_DEPARTMENTS/ise/pdf/tech-papers/20/20T_016.pdf", "isFamilyFriendly": true, "displayUrl": "https://engineering.lehigh.edu/sites/engineering.lehigh.edu/files/_DEPARTMENTS/ise/pdf/...", "snippet": "disadvantageous groups. In the literature of fair <b>machine</b> <b>learning</b>, several prevailing criteria for fairness include dis-parate impact (Barocas &amp; Selbst,2016) (also called demo-graphic parity (Calders et al.,2009)), <b>equalized</b> <b>odds</b> (Hardt etal.,2016), anditsspecialcaseof equalopportunity (Hardt et al.,2016), corresponding to different aspects of ...", "dateLastCrawled": "2022-01-28T20:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Counterfactual</b> Reasoning for Fair Clinical Risk Prediction", "url": "http://proceedings.mlr.press/v106/pfohl19a/pfohl19a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v106/pfohl19a/pfohl19a.pdf", "snippet": "We propose an extension of <b>counterfactual</b> fairness (Kusner et al.,2017) and <b>equalized</b> <b>odds</b> (Hardt et al.,2016) that we call individual <b>equalized</b> <b>counterfactual</b> <b>odds</b>. This metric is motivated by clinical risk prediction, but may be of interest to the general <b>machine</b> <b>learning</b> community for use in other applications. The algorithm we propose for ...", "dateLastCrawled": "2021-11-14T01:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine learning fairness notions: Bridging the</b> gap with real-world ...", "url": "https://www.sciencedirect.com/science/article/pii/S0306457321001321", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0306457321001321", "snippet": "Fairness in <b>machine</b> <b>learning</b> <b>can</b> be categorized according to two dimensions, namely, the task and the type of <b>learning</b>. For the first dimension, there are two tasks in fairness-aware <b>machine</b> <b>learning</b>: discrimination discovery (or assessment) and discrimination removal (or prevention). Discrimination discovery task focuses on assessing and measuring bias in datasets or in predictions made by the MLDM. Discrimination removal focuses on preventing discrimination by manipulating datasets (pre ...", "dateLastCrawled": "2022-02-03T07:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Fairness in Machine Learning</b> \u2014 Labelia (ex Substra Foundation)", "url": "https://www.labelia.org/en/blog/fairness-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.labelia.org/en/blog/<b>fairness-in-machine-learning</b>", "snippet": "<b>Insurance</b> companies have been using algorithms for a long time to determine the fees they apply to individuals, based on age, health condition and other criteria. How <b>can</b> we know we are treated fairly by those algorithms <b>compared</b> to all the other persons around us? Do the developers know whether they inject biases in their algorithms ? That field of study emerged recently, and we\u2019ve seen articles on trustworthy AI popping-up everywhere, covering several topics, such as: Non-Discrimination ...", "dateLastCrawled": "2022-01-30T18:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Evaluating Fairness in <b>Machine</b> <b>Learning</b> - <b>github.com</b>", "url": "https://github.com/KenSciResearch/fairMLHealth/blob/integration/docs/resources/Evaluating_Fairness.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/KenSciResearch/fairMLHealth/blob/integration/docs/resources/...", "snippet": "<b>Machine</b> <b>learning</b> models <b>can</b> also be a source of disparate impact in their implementation, through unconscious human biases that affect the fair interpretation or use of the model&#39;s results. This reference does not cover measurement of fairness at implementation. However, if you are interested in fair implementation, we recommend looking at Google&#39;s Fairness Indicators. Harms. In evaluating the potential impact of an ML model, it <b>can</b> be helpful to first clarify what specific harm(s) <b>can</b> be ...", "dateLastCrawled": "2022-02-02T03:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Accuracy and Fairness Trade-offs in <b>Machine</b> <b>Learning</b>: A Stochastic ...", "url": "http://www.optimization-online.org/DB_FILE/2020/08/7950.pdf", "isFamilyFriendly": true, "displayUrl": "www.optimization-online.org/DB_FILE/2020/08/7950.pdf", "snippet": "Accuracy and Fairness Trade-offs in <b>Machine</b> <b>Learning</b>: A Stochastic Multi-Objective Approach Suyun Liu* 1 Luis Nunes Vicente* 1 2 Abstract In the application of <b>machine</b> <b>learning</b> to real- life decision-making systems, e.g., credit scor-ing and criminal justice, the prediction outcomes might discriminate against people with sensi-tive attributes, leading to unfairness. The com-monly used strategy in fair <b>machine</b> <b>learning</b> is to include fairness as a constraint or a penal-ization term in the ...", "dateLastCrawled": "2021-11-22T20:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "ACCURACY AND FAIRNESS TRADE-OFFS IN <b>MACHINE</b> APPROACH", "url": "https://www.mat.uc.pt/preprints/ps/p2037.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.mat.uc.pt/preprints/ps/p2037.pdf", "snippet": "ACCURACY AND FAIRNESS TRADE-OFFS IN <b>MACHINE</b> <b>LEARNING</b>: A STOCHASTIC MULTI-OBJECTIVE APPROACH S. LIU AND L. N. VICENTE Abstract: In the application of <b>machine</b> <b>learning</b> to real-life decision-making sys- tems, e.g., credit scoring and criminal justice, the prediction outcomes might dis-criminate against people with sensitive attributes, leading to unfairness. The com-monly used strategy in fair <b>machine</b> <b>learning</b> is to include fairness as a constraint or a penalization term in the minimization of ...", "dateLastCrawled": "2022-01-09T06:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Trustworthy <b>Machine</b> <b>Learning</b> - Kush R. Varshney - Chapter 10: Fairness", "url": "http://www.trustworthymachinelearning.com/trustworthymachinelearning-10.htm", "isFamilyFriendly": true, "displayUrl": "www.trustworthy<b>machinelearning</b>.com/trustworthy<b>machinelearning</b>-10.htm", "snippet": "Your task now is to develop a detailed problem specification for a fair <b>machine</b> <b>learning</b> system for allocating care management programs to Sospital members and proceed along the different phases of the <b>machine</b> <b>learning</b> lifecycle without taking shortcuts. In this chapter, you will: \u00a7 compare and contrast definitions of fairness in a <b>machine</b> <b>learning</b> context, \u00a7 select an appropriate notion of fairness for your task, and \u00a7 mitigate unwanted biases at various points in the modeling pipeline ...", "dateLastCrawled": "2022-01-07T03:04:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine Learning</b> Glossary | <b>Google Developers</b>", "url": "https://developers.google.com/machine-learning/glossary/", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine-learning</b>/glossary", "snippet": "A <b>machine learning</b> technique that iteratively combines a set of simple and not very accurate classifiers ... Contrast with <b>equalized</b> <b>odds</b> and equality of opportunity, which permit classification results in aggregate to depend on sensitive attributes, but do not permit classification results for certain specified ground-truth labels to depend on sensitive attributes. See &quot;Attacking discrimination with smarter <b>machine learning</b>&quot; for a visualization exploring the tradeoffs when optimizing for ...", "dateLastCrawled": "2022-02-03T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Residual Unfairness in Fair <b>Machine</b> <b>Learning</b> from Prejudiced Data", "url": "http://proceedings.mlr.press/v80/kallus18a/kallus18a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v80/kallus18a/kallus18a.pdf", "snippet": "chine <b>learning</b> has considered some subcomponents of the overall problem we study of <b>learning</b> fair policies from bi-ased datasets. Hardt et al. (2016) formalize the criteria of equal opportunity and <b>equalized</b> <b>odds</b>. Lum &amp; Isaac (2016) show that a predictive policing algorithm for drug enforce-ment in Oakland, trained on police records, will ...", "dateLastCrawled": "2022-01-31T07:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A Survey on Bias and Fairness in <b>Machine</b> <b>Learning</b> \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/1908.09635/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1908.09635", "snippet": "With the widespread use of AI systems and applications in our everyday lives, it is important to take fairness issues into consideration while designing and engineering these types of systems. Such systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that the decisions do not reflect discriminatory behavior toward certain groups or populations. We have recently seen work in <b>machine</b> <b>learning</b>, natural language ...", "dateLastCrawled": "2021-11-15T17:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Human-centric Approach to Fairness in AI", "url": "https://www.timlrx.com/blog/a-human-centric-approach-to-fairness-in-ai", "isFamilyFriendly": true, "displayUrl": "https://www.timlrx.com/blog/a-human-centric-approach-to-fairness-in-ai", "snippet": "We embed the evaluation of AI fairness within the best practices of <b>machine</b> <b>learning</b> development and operations such as version control, ... This includes measures such as Demographic Parity / Statistical Parity (Dwork et al., 2012), <b>Equalized</b> <b>Odds</b> Metric (Hardt et al., 2016) and Calibration within Groups (Chouldechova, 2017). They are all statistical measures derived from the predictions of a classification model and differ in terms of which element(s) of the confusion matrix they are ...", "dateLastCrawled": "2022-02-03T02:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Mitigating Unwanted Biases with Adversarial Learning</b> | DeepAI", "url": "https://deepai.org/publication/mitigating-unwanted-biases-with-adversarial-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>mitigating-unwanted-biases-with-adversarial-learning</b>", "snippet": "<b>Machine</b> <b>learning</b> leverages data to build models capable of assessing the labels and properties of novel data. Unfortunately, the available training data frequently contains biases with respect to things that we would rather not use for decision making. <b>Machine</b> <b>learning</b> builds models faithful to training data and can lead to perpetuating these undesirable biases. For example, systems designed to predict creditworthiness and systems designed to perform <b>analogy</b> completion have been demonstrated ...", "dateLastCrawled": "2021-12-10T20:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Mitigating Unwanted Biases with Adversarial Learning</b>", "url": "https://www.aies-conference.com/2018/contents/papers/main/AIES_2018_paper_162.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.aies-conference.com/2018/contents/papers/main/AIES_2018_paper_162.pdf", "snippet": "<b>Machine</b> <b>learning</b> is a tool for building models that accurately represent input training data. When undesired biases concern- ing demographic groups are in the training data, well-trained models will re\ufb02ect those biases. We present a framework for mitigating such biases by including a variable for the group of interest and simultaneously <b>learning</b> a predictor and an ad-versary. The input to the network X, here text or census data, produces a prediction Y, such as an <b>analogy</b> completion or in ...", "dateLastCrawled": "2021-12-17T22:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "RStudio AI Blog: Starting to think about AI Fairness", "url": "https://blogs.rstudio.com/ai/posts/2021-07-15-ai-fairness/", "isFamilyFriendly": true, "displayUrl": "https://blogs.rstudio.com/ai/posts/2021-07-15-ai-fairness", "snippet": "Papers on fairness in <b>machine</b> <b>learning</b>, as is common in fields like computer science, abound with formulae. Even the papers referenced here, though selected not for their theorems and proofs but for the ideas they harbor, are no exception. But to start thinking about fairness as it might apply to an ML process at hand, common language \u2013 and common sense \u2013 will do just fine. If, after analyzing your use case, you judge that the more technical results are relevant to the process in ...", "dateLastCrawled": "2022-01-31T09:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Classification - Fairness and <b>machine</b> <b>learning</b>", "url": "https://fairmlbook.org/classification.html", "isFamilyFriendly": true, "displayUrl": "https://fairmlbook.org/classification.html", "snippet": "Simply put, the goal of classification is to determine a plausible value for an unknown variable Y given an observed variable X.For example, we might try to predict whether a loan applicant will pay back her loan by looking at various characteristics such as credit history, income, and net worth. Classification also applies in situations where the variable Y does not refer to an event that lies in the future. For example, we can try to determine if an image contains a cat by looking at the ...", "dateLastCrawled": "2022-02-03T07:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "On Predicting Recidivism: Epistemic Risk, Tradeoffs, and Values in ...", "url": "https://www.cambridge.org/core/journals/canadian-journal-of-philosophy/article/on-predicting-recidivism-epistemic-risk-tradeoffs-and-values-in-machine-learning/7E541FA03E78C3141A65EA99A0CA6E9A", "isFamilyFriendly": true, "displayUrl": "https://www.cambridge.org/core/journals/canadian-journal-of-philosophy/article/on...", "snippet": "This paper examines the role of value judgments in the design of <b>machine</b>-<b>learning</b> (ML) systems generally and in recidivism-prediction algorithms specifically. Drawing on work on inductive and epistemic risk, the paper argues that ML systems are value laden in ways similar to human decision making, because the development and design of ML systems requires human decisions that involve tradeoffs that reflect values. In many cases, these decisions have significant\u2014and, in some cases, disparate ...", "dateLastCrawled": "2022-01-26T20:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Measuring discrimination in algorithmic <b>decision making</b> | SpringerLink", "url": "https://link.springer.com/article/10.1007/s10618-017-0506-1", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10618-017-0506-1", "snippet": "A <b>machine</b> <b>learning</b> algorithm is a procedure used for producing a predictive model from historical data. A model is a collection of decision rules used for <b>decision making</b> for new incoming data. The model would take personal characteristics as inputs (for example, income, credit history, employment status), and produce a prediction (for example, credit risk level). Fig. 1. A typical <b>machine</b> <b>learning</b> setting. Full size image. <b>Learning</b> algorithms as such cannot discriminate, because they are ...", "dateLastCrawled": "2022-01-29T20:54:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(equalized odds)  is like +(machine learning insurance)", "+(equalized odds) is similar to +(machine learning insurance)", "+(equalized odds) can be thought of as +(machine learning insurance)", "+(equalized odds) can be compared to +(machine learning insurance)", "machine learning +(equalized odds AND analogy)", "machine learning +(\"equalized odds is like\")", "machine learning +(\"equalized odds is similar\")", "machine learning +(\"just as equalized odds\")", "machine learning +(\"equalized odds can be thought of as\")", "machine learning +(\"equalized odds can be compared to\")"]}