{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "machine learning - What is <b>cross-entropy</b>? - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/41990250/what-is-cross-entropy", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/41990250", "snippet": "Correct, <b>cross-entropy</b> describes the loss <b>between</b> <b>two</b> <b>probability</b> <b>distributions</b>. It is one of many possible loss functions. Then we can use, for example, gradient descent algorithm to find the minimum. Yes, the <b>cross-entropy</b> loss function can be used as part of gradient descent. Further reading: one of my other answers related to TensorFlow.", "dateLastCrawled": "2022-01-28T16:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Intuitively, why is <b>cross entropy</b> a measure of <b>distance</b> of <b>two</b> ...", "url": "https://stats.stackexchange.com/questions/209107/intuitively-why-is-cross-entropy-a-measure-of-distance-of-two-probability-distr", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/209107", "snippet": "I think <b>cross entropy</b> cannot be used as a <b>distance</b> since H(p,q) is not H(q,p). It is a uni-directional measure, so it can be used to measure the loss (or difference) from the original <b>probability</b> <b>like</b> KD divergence.", "dateLastCrawled": "2022-01-27T19:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A Gentle Introduction to <b>Cross-Entropy for Machine Learning</b>", "url": "https://machinelearningmastery.com/cross-entropy-for-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>cross-entropy-for-machine-learning</b>", "snippet": "Last Updated on December 22, 2020. <b>Cross-entropy</b> is commonly used in machine learning as a loss function. <b>Cross-entropy</b> is a measure from the field of information theory, building upon entropy and generally calculating the difference <b>between</b> <b>two</b> <b>probability</b> <b>distributions</b>. It is closely related to but is different from KL divergence that calculates the relative entropy <b>between</b> <b>two</b> <b>probability</b> <b>distributions</b>, whereas <b>cross-entropy</b> can be thought to calculate the total entropy <b>between</b> the ...", "dateLastCrawled": "2022-02-03T03:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Friendly Introduction to <b>Cross-Entropy</b> for Machine Learning | by ...", "url": "https://medium.com/analytics-vidhya/a-friendly-introduction-to-cross-entropy-for-machine-learning-b4e9f2b1f6", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/a-friendly-introduction-to-<b>cross-entropy</b>-for...", "snippet": "NOTE:-<b>Cross Entropy</b> is definitely a good loss function for Classification Problems, because it minimizes the <b>distance</b> <b>between</b> <b>two</b> <b>probability</b> <b>distributions</b> \u2014 predicted and actual. Introduction ...", "dateLastCrawled": "2021-10-09T05:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Cross entropy</b> calculator | Taskvio", "url": "https://taskvio.com/maths/probability-distributions/cross-entropy/", "isFamilyFriendly": true, "displayUrl": "https://taskvio.com/maths/<b>probability</b>-<b>distributions</b>/<b>cross-entropy</b>", "snippet": "<b>Cross-entropy</b>, it&#39;s a measure of the degree of dissimilarities <b>between</b> <b>two</b> <b>probability</b> <b>distributions</b>, within the reference to supervised machine learning. <b>Cross-Entropy</b> is expressed by the equation; The <b>cross-entropy</b> equation. Where x represents the anticipated results by ML algorithm, p (x) is that the <b>probability</b> distribution of.", "dateLastCrawled": "2022-02-02T15:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "machine learning - <b>Cross-entropy loss</b> explanation - Data Science Stack ...", "url": "https://datascience.stackexchange.com/questions/20296/cross-entropy-loss-explanation", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/20296", "snippet": "Bottom line: In layman terms, one could think of <b>cross-entropy</b> as the <b>distance</b> <b>between</b> <b>two</b> <b>probability</b> <b>distributions</b> in terms of the amount of information (bits) needed to explain that <b>distance</b>. It is a neat way of defining a loss which goes down as the <b>probability</b> vectors get closer to one another. Share. Improve this answer. Follow edited Jul 25 &#39;19 at 8:55. answered Jul 22 &#39;19 at 6:06. hmi789 hmi789. 106 1 1 silver badge 3 3 bronze badges $\\endgroup$ Add a comment | 2 $\\begingroup$ Let&#39;s ...", "dateLastCrawled": "2022-01-27T03:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "What is the motivation for using <b>cross-entropy</b> to compare <b>two</b> ...", "url": "https://math.stackexchange.com/questions/3389976/what-is-the-motivation-for-using-cross-entropy-to-compare-two-probability-vector", "isFamilyFriendly": true, "displayUrl": "https://math.stackexchange.com/questions/3389976/what-is-the-motivation-for-using...", "snippet": "The goal of your question is to measure the <b>distance</b> <b>between</b> <b>two</b> <b>probability</b> <b>distributions</b>, not <b>two</b> individual <b>probability</b> value points. For a <b>probability</b> distribution, we are talking about multiple <b>probability</b> value points. To most people, it should makes sense to first compute the difference at each <b>probability</b> value point, and then to take their average (weighted by their <b>probability</b> values, i.e.", "dateLastCrawled": "2022-01-20T07:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "kullback leibler - What is the difference <b>Cross-entropy</b> and KL ...", "url": "https://stats.stackexchange.com/questions/357963/what-is-the-difference-cross-entropy-and-kl-divergence", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/357963", "snippet": "Now look at the definition of <b>KL divergence</b> <b>between</b> <b>distributions</b> A and B where the first term of the right hand side is the entropy of distribution A, the second term can be interpreted as the expectation of distribution B in terms of A. And the describes how different B is from A from the perspective of A. It&#39;s worth of noting usually stands ...", "dateLastCrawled": "2022-02-03T02:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Cross Entropy Method</b> \u2013 Applied <b>Probability</b> Notes", "url": "https://appliedprobability.blog/2020/06/22/cross-entropy-method-2/", "isFamilyFriendly": true, "displayUrl": "https://applied<b>probability</b>.blog/2020/06/22/<b>cross-entropy-method</b>-2", "snippet": "The <b>Cross Entropy Method</b> (CEM) is a generic optimization technique. It is a zero-th order method, i.e. you don\u2019t gradients. 1 So, for instance, it works well on combinatorial optimization problems, as well as reinforcement learning. The Basic Idea. You want to maximize a function over .We assume you can sample RVs from according to some parameterized distribution .We assume that you can fit the parameter given data .We let be the fit to this data. Finally is an importance parameter used by ...", "dateLastCrawled": "2022-01-24T20:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "In binary classification, is it okay to use <b>cross entropy</b> loss instead ...", "url": "https://machinelearning1.quora.com/In-binary-classification-is-it-okay-to-use-cross-entropy-loss-instead-of-binary-cross-entropy-loss-I-am-using-pre-defi", "isFamilyFriendly": true, "displayUrl": "https://machinelearning1.quora.com/In-binary-classification-is-it-okay-to-use-cross...", "snippet": "<b>Cross Entropy</b> is definitely a good loss function for Classification Problems, because it minimizes the <b>distance</b> <b>between</b> <b>two</b> <b>probability</b> <b>distributions</b> - predicted and actual. Conceptually, you can understand it <b>like</b> this - Consider a classifier which predicts whether the given animal is dog, cat or horse with a <b>probability</b> associated with each.", "dateLastCrawled": "2022-01-19T03:11:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "machine learning - What is <b>cross-entropy</b>? - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/41990250/what-is-cross-entropy", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/41990250", "snippet": "<b>Cross-entropy</b> is closely related to relative entropy or KL-divergence that computes <b>distance</b> <b>between</b> <b>two</b> <b>probability</b> <b>distributions</b>. For example, in <b>between</b> <b>two</b> discrete pmfs, the relation <b>between</b> them is shown in the following figure:", "dateLastCrawled": "2022-01-28T16:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A Gentle Introduction to <b>Cross-Entropy for Machine Learning</b>", "url": "https://machinelearningmastery.com/cross-entropy-for-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>cross-entropy-for-machine-learning</b>", "snippet": "The <b>cross-entropy</b> <b>between</b> <b>two</b> <b>probability</b> <b>distributions</b>, such as Q from P, can be stated formally as: H(P, Q) ... This calculation is for discrete <b>probability</b> <b>distributions</b>, although a <b>similar</b> calculation can be used for continuous <b>probability</b> <b>distributions</b> using the integral across the events instead of the sum. The result will be a positive number measured in bits and will be equal to the entropy of the distribution if the <b>two</b> <b>probability</b> <b>distributions</b> are identical. Note: this notation ...", "dateLastCrawled": "2022-02-03T03:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Intuitively, why is <b>cross entropy</b> a measure of <b>distance</b> of <b>two</b> ...", "url": "https://stats.stackexchange.com/questions/209107/intuitively-why-is-cross-entropy-a-measure-of-distance-of-two-probability-distr", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/209107", "snippet": "Agree with the top answer that <b>cross-entropy</b>, as a distribution dissimilarity measure, may be more narrowly applicable to situations when we are comparing an estimated <b>probability</b> distribution (q) against the true <b>probability</b> distribution (p).", "dateLastCrawled": "2022-01-27T19:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "An Intuitive Understanding of Entropy, <b>Cross Entropy</b> and Kullback ...", "url": "https://blog.zhaoyongsheng.com/2019/12/04/An-Intuitive-Understanding-of-Kullback-Leibler-Divergence-and-Cross-Entropy/", "isFamilyFriendly": true, "displayUrl": "https://blog.zhaoyongsheng.com/2019/12/04/An-Intuitive-Understanding-of-Kullback...", "snippet": "Based on the definition of entropy, people further proposed <b>Cross Entropy</b> and Kullback Leibler (KL) Divergence to address the issues of information analysis <b>between</b> <b>two</b> different <b>probability</b> <b>distributions</b>. They are good metric that could measure the similarity or <b>distance</b> <b>between</b> <b>two</b> <b>probability</b> <b>distributions</b>, therefore they are widely used in machine learning and deep learning. In this article, I want to provide you an intuitive understanding of entropy, <b>cross entropy</b> and kullback-leibler ...", "dateLastCrawled": "2022-01-28T03:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "TF-IDF is simply a <b>cross-entropy</b> | searchivarius.org", "url": "http://searchivarius.org/blog/tf-idf-simply-cross-entropy", "isFamilyFriendly": true, "displayUrl": "searchivarius.org/blog/tf-idf-simply-<b>cross-entropy</b>", "snippet": "Many people complain that there is no simple statistical interpretation for the TF-IDF ranking formula (the formula that is commonly used in information retrieval). However, it can be easily shown that the TF-IDF ranking is based on the <b>distance</b> <b>between</b> <b>two</b> <b>probability</b> <b>distributions</b>, which is expressed as the <b>cross-entropy</b> One is the global distribution of query words in the collection and another is a distribution of query words in documents. The TF-IDF ranking is a measure of perplexity ...", "dateLastCrawled": "2021-12-14T17:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Cross entropy</b> <b>loss function \uff08CrossEntropy Loss\uff09\uff08 Principle explanation</b> ...", "url": "https://www.iodraw.com/en/blog/210591340", "isFamilyFriendly": true, "displayUrl": "https://www.iodraw.com/en/blog/210591340", "snippet": "The <b>cross entropy</b> loss function is the most commonly used loss function in classification , <b>Cross entropy</b> is used to measure the difference <b>between</b> <b>two</b> <b>probability</b> <b>distributions</b> , It is used to measure the difference <b>between</b> the learned distribution and the real distribution . &lt;&gt; Dichotomy In the case of dichotomy , Only <b>two</b> values can be predicted for each category , Suppose the prediction is good melon \uff081\uff09 The <b>probability</b> is P P P, Bad melon \uff080\uff09 The <b>probability</b> is 1 \u2212 P 1-P 1\u2212P ...", "dateLastCrawled": "2022-01-29T22:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "python - <b>Understanding Cross Entropy Loss</b> - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/49480344/understanding-cross-entropy-loss", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/49480344", "snippet": "<b>Cross entropy</b> measures <b>distance</b> <b>between</b> any <b>two</b> <b>probability</b> <b>distributions</b>. In what you describe (the VAE), MNIST image pixels are interpreted as probabilities for pixels being &quot;on/off&quot;. In that case your target <b>probability</b> distribution is simply not a dirac distribution (0 or 1) but can have different values. See the <b>cross entropy</b> definition on Wikipedia. With the above as a reference, let&#39;s say your model outputs a reconstruction for a certain pixel of 0.7. This essentially says that your ...", "dateLastCrawled": "2022-01-29T00:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "2.3 Measures of Distributional Similarity", "url": "https://www.cs.cornell.edu/courses/cs6742/2017fa/handouts/lee-ch2.3.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.cornell.edu/courses/cs6742/2017fa/handouts/lee-ch2.3.pdf", "snippet": "the \u201csimilarity\u201d <b>between</b> <b>distributions</b>. We refer to these functions as <b>distance</b> functions, rather than similarity functions, since most of them achieve their minimum when the <b>two</b> <b>distributions</b> being compared aremaximally <b>similar</b> (i.e., identical). The work described in chapters 4 and 5 uses", "dateLastCrawled": "2021-12-03T11:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "In binary classification, is it okay to use <b>cross entropy</b> loss instead ...", "url": "https://machinelearning1.quora.com/In-binary-classification-is-it-okay-to-use-cross-entropy-loss-instead-of-binary-cross-entropy-loss-I-am-using-pre-defi", "isFamilyFriendly": true, "displayUrl": "https://machinelearning1.quora.com/In-binary-classification-is-it-okay-to-use-cross...", "snippet": "<b>Cross Entropy</b> is definitely a good loss function for Classification Problems, because it minimizes the <b>distance</b> <b>between</b> <b>two</b> <b>probability</b> <b>distributions</b> - predicted and actual. Conceptually, you can understand it like this - Consider a classifier which predicts whether the given animal is dog, cat or horse with a <b>probability</b> associated with each.", "dateLastCrawled": "2022-01-19T03:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "The Kullback\u2013Leibler <b>divergence</b> <b>between</b> discrete <b>probability</b> <b>distributions</b>", "url": "https://blogs.sas.com/content/iml/2020/05/26/kullback-leibler-divergence-discrete.html", "isFamilyFriendly": true, "displayUrl": "https://blogs.sas.com/content/iml/2020/05/26/kullback-leibler-<b>divergence</b>-discrete.html", "snippet": "The Kullback\u2013Leibler <b>divergence</b> is a measure of dissimilarity <b>between</b> <b>two</b> <b>probability</b> <b>distributions</b>. It measures how much one distribution differs from a reference distribution. This article explains the Kullback\u2013Leibler <b>divergence</b> and shows how to compute it for discrete <b>probability</b> <b>distributions</b>. Recall that there are many statistical methods that indicate how much <b>two</b> <b>distributions</b> differ. For example, a maximum likelihood estimate involves finding parameters for a reference ...", "dateLastCrawled": "2022-02-03T06:35:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A Gentle Introduction to <b>Cross-Entropy for Machine Learning</b>", "url": "https://machinelearningmastery.com/cross-entropy-for-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>cross-entropy-for-machine-learning</b>", "snippet": "Last Updated on December 22, 2020. <b>Cross-entropy</b> is commonly used in machine learning as a loss function. <b>Cross-entropy</b> is a measure from the field of information theory, building upon entropy and generally calculating the difference <b>between</b> <b>two</b> <b>probability</b> <b>distributions</b>. It is closely related to but is different from KL divergence that calculates the relative entropy <b>between</b> <b>two</b> <b>probability</b> <b>distributions</b>, whereas <b>cross-entropy</b> <b>can</b> <b>be thought</b> to calculate the total entropy <b>between</b> the ...", "dateLastCrawled": "2022-02-03T03:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Cross entropy</b> calculator | Taskvio", "url": "https://taskvio.com/maths/probability-distributions/cross-entropy/", "isFamilyFriendly": true, "displayUrl": "https://taskvio.com/maths/<b>probability</b>-<b>distributions</b>/<b>cross-entropy</b>", "snippet": "<b>Cross-entropy</b>, it&#39;s a measure of the degree of dissimilarities <b>between</b> <b>two</b> <b>probability</b> <b>distributions</b>, within the reference to supervised machine learning. <b>Cross-Entropy</b> is expressed by the equation; The <b>cross-entropy</b> equation. Where x represents the anticipated results by ML algorithm, p (x) is that the <b>probability</b> distribution of.", "dateLastCrawled": "2022-02-02T15:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Cross-Entropy</b> Method Variants for Optimization", "url": "https://web.stanford.edu/~mossr/pdf/cem_variants.pdf", "isFamilyFriendly": true, "displayUrl": "https://web.stanford.edu/~mossr/pdf/cem_variants.pdf", "snippet": "is a metric used to measure the <b>distance</b> <b>between</b> <b>two</b> <b>proba-bility</b> <b>distributions</b>, where the <b>distance</b> may not be symmetric [3]. The <b>distance</b> used to de\ufb01ne <b>cross-entropy</b> is called the Kullback-Leibler (KL) <b>distance</b> or KL divergence. The KL <b>distance</b> is also called the relative entropy, and we <b>can</b> use this to derive the <b>cross-entropy</b>. Formally, for a random variable X = (X 1;:::;X n) with a support of X, the KL <b>distance</b> <b>between</b> <b>two</b> continuous <b>probability</b> density functions fand g is de\ufb01ned to ...", "dateLastCrawled": "2022-01-12T10:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Cross Entropy</b> for Tensorflow | Mustafa Murat ARAT", "url": "https://mmuratarat.github.io/2018-12-21/cross-entropy", "isFamilyFriendly": true, "displayUrl": "https://mmuratarat.github.io/2018-12-21/<b>cross-entropy</b>", "snippet": "<b>Cross Entropy</b> for Tensorflow. <b>Cross entropy</b> <b>can</b> be used to define a loss function (cost function) in machine learning and optimization. It is defined on <b>probability</b> <b>distributions</b>, not single values. It works for classification because classifier output is (often) a <b>probability</b> distribution over class labels. For discrete <b>distributions</b> p and q ...", "dateLastCrawled": "2022-01-27T22:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Understanding <b>Entropy</b>: the Golden Measurement of Machine Learning | by ...", "url": "https://towardsdatascience.com/understanding-entropy-the-golden-measurement-of-machine-learning-4ea97c663dc3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-<b>entropy</b>-the-golden-measurement-of-machine...", "snippet": "Information gain and relative <b>entropy</b>, used in the training of Decision Trees, are defined as the \u2018<b>distance</b>\u2019 <b>between</b> <b>two</b> <b>probability</b> mass <b>distributions</b> p(x) and q(x). It\u2019s also known as Kullback\u2013Leibler (KL) divergence or Earth Mover\u2019s <b>Distance</b>, which is used in the training of Generative Adversarial Networks to evaluate the performance of generated images compared to images in the original dataset.", "dateLastCrawled": "2022-02-02T21:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Connections: <b>Log Likelihood</b>, <b>Cross Entropy</b>, KL Divergence, Logistic ...", "url": "https://glassboxmedicine.com/2019/12/07/connections-log-likelihood-cross-entropy-kl-divergence-logistic-regression-and-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://glassboxmedicine.com/2019/12/07/connections-<b>log-likelihood</b>-<b>cross-entropy</b>-kl...", "snippet": "The difference <b>between</b> MLE and <b>cross-entropy</b> is that MLE represents a structured and principled approach to modeling and training, and binary/softmax <b>cross-entropy</b> simply represent special cases of that applied to problems that people typically care about. Entropy. After that aside on maximum <b>likelihood</b> estimation, let\u2019s delve more into the relationship <b>between</b> negative <b>log likelihood</b> and <b>cross entropy</b>. First, we\u2019ll define entropy: <b>Cross Entropy</b>. Section references: Wikipedia Cross ...", "dateLastCrawled": "2022-01-31T14:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 6, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Kullback\u2013Leibler divergence</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Kullback\u2013Leibler_divergence</b>", "snippet": "Introduction and context. Consider <b>two</b> <b>probability</b> <b>distributions</b> and .Usually, represents the data, the observations, or a measured <b>probability</b> distribution. Distribution represents instead a theory, a model, a description or an approximation of .The <b>Kullback\u2013Leibler divergence</b> is then interpreted as the average difference of the number of bits required for encoding samples of using a code optimized for rather than one optimized for .Note that the roles of and <b>can</b> be reversed in some ...", "dateLastCrawled": "2022-02-02T08:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "In <b>between</b> Real <b>or Fake: Generative Adversarial Networks (GANs</b>) | by ...", "url": "https://medium.datadriveninvestor.com/in-between-real-or-fake-generative-adversarial-networks-gans-f46f64577fb5", "isFamilyFriendly": true, "displayUrl": "https://medium.datadriveninvestor.com/in-<b>between</b>-real-or-fake-generative-adversarial...", "snippet": "In the loss schemes, we\u2019ll look at here, the generator and discriminator losses derive from a single measure of the <b>distance</b> <b>between</b> <b>probability</b> <b>distributions</b>. In both of these schemes, however, the generator <b>can</b> only affect one term in the <b>distance</b> measure: the term that reflects the distribution of the fake data. So during generator training, we drop the other term, which reflects the distribution of the real data.", "dateLastCrawled": "2022-01-12T22:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Kullback-Leibler Divergence</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/mathematics/kullback-leibler-divergence", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/mathematics/<b>kullback-leibler-divergence</b>", "snippet": "Note that the relative entropy <b>between</b> <b>two</b> <b>probability</b> <b>distributions</b> P 1 and P 2 is defined as D P 1 | P 2 = E P 1 log P 1 (T) P 2 (T). The relative entropy (assuming P 1 \u226a P 2) is always nonnegative, and equal to 0 if and only if P 1 \u2261 P 2. Thus, we <b>can</b> use it as a <b>distance</b> measure, and choose to find an approximation W to P F that ...", "dateLastCrawled": "2022-02-02T02:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Understanding <b>Loss</b> Functions in Computer Vision! | by Sowmya ...", "url": "https://medium.com/ml-cheat-sheet/winning-at-loss-functions-2-important-loss-functions-in-computer-vision-b2b9d293e15a", "isFamilyFriendly": true, "displayUrl": "https://medium.com/ml-cheat-sheet/winning-at-<b>loss</b>-functions-2-important-<b>loss</b>-functions...", "snippet": "Instead, they propose modeling the problem on the Earth-Mover\u2019s <b>distance</b>, which calculates the <b>distance</b> <b>between</b> <b>two</b> <b>probability</b> <b>distributions</b> in terms of the cost of turning one distribution ...", "dateLastCrawled": "2022-02-03T13:02:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A Gentle Introduction to <b>Cross-Entropy for Machine Learning</b>", "url": "https://machinelearningmastery.com/cross-entropy-for-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>cross-entropy-for-machine-learning</b>", "snippet": "Last Updated on December 22, 2020. <b>Cross-entropy</b> is commonly used in machine learning as a loss function. <b>Cross-entropy</b> is a measure from the field of information theory, building upon entropy and generally calculating the difference <b>between</b> <b>two</b> <b>probability</b> <b>distributions</b>. It is closely related to but is different from KL divergence that calculates the relative entropy <b>between</b> <b>two</b> <b>probability</b> <b>distributions</b>, whereas <b>cross-entropy</b> <b>can</b> be thought to calculate the total entropy <b>between</b> the ...", "dateLastCrawled": "2022-02-03T03:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Cross-Entropy</b> Method Variants for Optimization", "url": "https://web.stanford.edu/~mossr/pdf/cem_variants.pdf", "isFamilyFriendly": true, "displayUrl": "https://web.stanford.edu/~mossr/pdf/cem_variants.pdf", "snippet": "is a metric used to measure the <b>distance</b> <b>between</b> <b>two</b> <b>proba-bility</b> <b>distributions</b>, where the <b>distance</b> may not be symmetric [3]. The <b>distance</b> used to de\ufb01ne <b>cross-entropy</b> is called the Kullback-Leibler (KL) <b>distance</b> or KL divergence. The KL <b>distance</b> is also called the relative entropy, and we <b>can</b> use this to derive the <b>cross-entropy</b>. Formally, for a random variable X = (X 1;:::;X n) with a support of X, the KL <b>distance</b> <b>between</b> <b>two</b> continuous <b>probability</b> density functions fand g is de\ufb01ned to ...", "dateLastCrawled": "2022-01-12T10:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A Friendly Introduction to <b>Cross-Entropy</b> for Machine Learning | by ...", "url": "https://medium.com/analytics-vidhya/a-friendly-introduction-to-cross-entropy-for-machine-learning-b4e9f2b1f6", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/a-friendly-introduction-to-<b>cross-entropy</b>-for...", "snippet": "<b>Cross-entropy</b> is a measure of the difference <b>between</b> <b>two</b> <b>probability</b> <b>distributions</b> for a given random variable or set of events in the connection with Supervised machine learning , one of the ...", "dateLastCrawled": "2021-10-09T05:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Cross-Entropy Loss in ML</b>. What is Entropy in ML? | by Inara Koppert ...", "url": "https://medium.com/unpackai/cross-entropy-loss-in-ml-d9f22fc11fe0", "isFamilyFriendly": true, "displayUrl": "https://medium.com/unpackai/<b>cross-entropy-loss-in-ml</b>-d9f22fc11fe0", "snippet": "The <b>cross-entropy</b> <b>between</b> <b>two</b> <b>probability</b> <b>distributions</b>, such as Q from P, <b>can</b> be stated formally as: H(P, Q) Where H() is the <b>cross-entropy</b> function, P may be the target distribution and Q is the ...", "dateLastCrawled": "2022-01-31T20:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Diversity-Promoting GAN: A <b>Cross-Entropy</b> Based Generative Adversarial ...", "url": "https://aclanthology.org/D18-1428.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/D18-1428.pdf", "snippet": "<b>distance</b> <b>between</b> the <b>two</b> <b>distributions</b>, and cannot give reasonable reward to the model for generating real and diverse text (Arjovsky et al.,2017). Instead of using a classi\ufb01er, we propose a novel language-model based discriminator and use the output of the language model, <b>cross-entropy</b>, as the reward. The main advantage of our model lies in", "dateLastCrawled": "2022-02-02T19:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "2 From entropy to maximum likelihood | Statistical Methods: Likelihood ...", "url": "https://strimmerlab.github.io/publications/lecture-notes/MATH20802/from-entropy-to-maximum-likelihood.html", "isFamilyFriendly": true, "displayUrl": "https://strimmerlab.github.io/publications/lecture-notes/MATH20802/from-entropy-to...", "snippet": "Example 2.6 <b>Cross-entropy</b> <b>between</b> <b>two</b> normals: ... here \u201cdivergence\u201d measures the dissimilarity <b>between</b> <b>probability</b> <b>distributions</b>. This type of divergence is not related and should not be confused with divergence (div) as used in vector analysis.) The term divergence (rather than <b>distance</b>) implies also that the <b>distributions</b> \\(F\\) and \\(G\\) are not interchangeable in \\(D_{\\text{KL}}(F, G)\\). In applications in statistics the typical roles of \\(F\\) and \\(G\\) are: \\(F\\) as the (unknown ...", "dateLastCrawled": "2022-01-29T08:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Understanding <b>Entropy</b>: the Golden Measurement of Machine Learning | by ...", "url": "https://towardsdatascience.com/understanding-entropy-the-golden-measurement-of-machine-learning-4ea97c663dc3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-<b>entropy</b>-the-golden-measurement-of-machine...", "snippet": "Information gain and relative <b>entropy</b>, used in the training of Decision Trees, are defined as the \u2018<b>distance</b>\u2019 <b>between</b> <b>two</b> <b>probability</b> mass <b>distributions</b> p(x) and q(x). It\u2019s also known as Kullback\u2013Leibler (KL) divergence or Earth Mover\u2019s <b>Distance</b>, which is used in the training of Generative Adversarial Networks to evaluate the performance of generated images <b>compared</b> to images in the original dataset.", "dateLastCrawled": "2022-02-02T21:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "2.3 Measures of Distributional Similarity", "url": "https://www.cs.cornell.edu/courses/cs6742/2017fa/handouts/lee-ch2.3.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.cornell.edu/courses/cs6742/2017fa/handouts/lee-ch2.3.pdf", "snippet": "the \u201csimilarity\u201d <b>between</b> <b>distributions</b>. We refer to these functions as <b>distance</b> functions, rather than similarity functions, since most of them achieve their minimum when the <b>two</b> <b>distributions</b> being <b>compared</b> aremaximally similar (i.e., identical). The work described in chapters 4 and 5 uses", "dateLastCrawled": "2021-12-03T11:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "What is <b>the advantage/disadvantage of Hinge-loss compared</b> to <b>cross-entropy</b>?", "url": "https://www.quora.com/What-is-the-advantage-disadvantage-of-Hinge-loss-compared-to-cross-entropy", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>the-advantage-disadvantage-of-Hinge-loss-compared</b>-to...", "snippet": "Answer (1 of 2): <b>Cross Entropy</b> (or Log Loss), Hing Loss (SVM Loss), Squared Loss etc. are different forms of Loss functions. Log Loss in the classification context gives Logistic Regression, while the Hinge Loss is Support Vector Machines. Logistic Regression and SVMs both are linear classifiers,...", "dateLastCrawled": "2022-01-29T16:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "optimization - Relationship <b>between</b> <b>KL divergence</b> and entropy - Cross ...", "url": "https://stats.stackexchange.com/questions/465464/relationship-between-kl-divergence-and-entropy", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/465464/relationship-<b>between</b>-<b>kl-divergence</b>...", "snippet": "Intuitively, the entropy of a random variable X with a <b>probability</b> distribution p ( x) is related to how much p ( x) diverges from the uniform distribution on the support of X. The more p ( x) diverges the lesser its entropy and vice versa. H ( X) = \u2211 x \u2208 X p ( x) log. \u2061. 1 p ( x) = l o g | X | \u2212 \u2211 x \u2208 X p ( x) log. \u2061.", "dateLastCrawled": "2022-01-23T12:08:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Why and How to use <b>Cross Entropy</b>. The fundamental reasons for ...", "url": "https://towardsdatascience.com/why-and-how-to-use-cross-entropy-4e983cbdd873", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/why-and-how-to-use-<b>cross-entropy</b>-4e983cbdd873", "snippet": "The fundamental reasons for minimizing binary <b>cross entropy</b> (log loss) with probabilistic classification models . Will Arliss. Sep 26, 2020 \u00b7 7 min read. Introduction. This post discusses why logistic regression necessarily uses a different loss function than linear regression. First, the simple yet inefficient way to solve logistic regression will be presented, then the slightly less simple but much more efficient way will be explained and compared. The simple way. Linear regression is the ...", "dateLastCrawled": "2022-01-31T14:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Cross-Entropy</b> Demystified. What is it? Is there any relation to\u2026 | by ...", "url": "https://naokishibuya.medium.com/demystifying-cross-entropy-e80e3ad54a8", "isFamilyFriendly": true, "displayUrl": "https://naokishibuya.medium.com/demystifying-<b>cross-entropy</b>-e80e3ad54a8", "snippet": "However, the <b>machine</b> <b>learning</b> application uses the base e logarithm for implementation convenience. Binary <b>Cross-Entropy</b>. We can use the binary <b>cross-entropy</b> for binary classification where we have yes/no answer. For example, there are only dogs or cats in images. For the binary classifications, the <b>cross-entropy</b> formula contains only two ...", "dateLastCrawled": "2022-01-26T13:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A Gentle Introduction to Information Entropy - <b>Machine</b> <b>Learning</b> Mastery", "url": "https://machinelearningmastery.com/what-is-information-entropy/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/what-is-information-entropy", "snippet": "Calculating information and entropy is a useful tool in <b>machine</b> <b>learning</b> and is used as the basis for techniques such as feature selection, building decision trees, and, more generally, fitting classification models. As such, a <b>machine</b> <b>learning</b> practitioner requires a strong understanding and intuition for information and entropy. In this post, you will discover a gentle introduction to information entropy. After reading this post, you will know: Information theory is concerned with data ...", "dateLastCrawled": "2022-02-02T13:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>machine</b> <b>learning</b> - <b>Cross-entropy loss</b> explanation - Data Science Stack ...", "url": "https://datascience.stackexchange.com/questions/20296/cross-entropy-loss-explanation", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/20296", "snippet": "The answer from Neil is correct. However I think its important to point out that while the loss does not depend on the distribution between the incorrect classes (only the distribution between the correct class and the rest), the gradient of this loss function does effect the incorrect classes differently depending on how wrong they are. So when you use cross-ent in <b>machine</b> <b>learning</b> you will change weights differently for [0.1 0.5 0.1 0.1 0.2] and [0.1 0.6 0.1 0.1 0.1].", "dateLastCrawled": "2022-01-27T03:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine</b> <b>Learning</b> Concepts for Revision | by Raunak Sarada | Medium", "url": "https://raunaksarada-cse21.medium.com/machine-learning-concepts-for-revision-491384952d27", "isFamilyFriendly": true, "displayUrl": "https://raunaksarada-cse21.medium.com/<b>machine</b>-<b>learning</b>-concepts-for-revision-491384952d27", "snippet": "ML Concepts. A.I \u2014 Intelligence showed by machines which is common for humans <b>Machine</b> <b>Learning</b>- Recognize the pattern in data and automatically learn and improve through experience without explicitly being programmed Deep <b>Learning</b>- branch of <b>machine</b> <b>learning</b>.We have to deal with lots of data so in that case problems can\u2019t be solved with simple ML algorithms.", "dateLastCrawled": "2022-01-25T20:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Shannon <b>entropy</b> in the context of <b>machine</b> <b>learning</b> and AI | by Frank ...", "url": "https://medium.com/swlh/shannon-entropy-in-the-context-of-machine-learning-and-ai-24aee2709e32", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/shannon-<b>entropy</b>-in-the-context-of-<b>machine</b>-<b>learning</b>-and-ai-24...", "snippet": "Closely related to <b>cross entropy</b>, the KL divergence from q to p, written DKL(p||q), is another similarity measure often used in <b>machine</b> <b>learning</b>. In the language of Bayesian Inference, DKL(p||q ...", "dateLastCrawled": "2022-01-30T12:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Main concepts behind Machine Learning</b> | by Bruno Eidi Nishimoto ...", "url": "https://medium.com/neuronio/main-concepts-behind-machine-learning-22cd81d68a11", "isFamilyFriendly": true, "displayUrl": "https://medium.com/neuronio/<b>main-concepts-behind-machine-learning</b>-22cd81d68a11", "snippet": "<b>Machine</b> <b>Learning</b> is a concept that is currently trending. It is a subarea from Artificial Intelligence and it consists on the fact that the <b>machine</b> can learn by itself without being explicitly ...", "dateLastCrawled": "2022-01-19T01:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Lecture 4 Fundamentals of deep <b>learning</b> and neural networks", "url": "https://web.stanford.edu/class/biods388/downloads/BIODS388_Lecture_4.pdf", "isFamilyFriendly": true, "displayUrl": "https://web.stanford.edu/class/biods388/downloads/BIODS388_Lecture_4.pdf", "snippet": "Deep <b>learning</b>: <b>Machine</b> <b>learning</b> models based on \u201cdeep\u201d neural networks comprising millions (sometimes billions) of parameters organized into hierarchical layers. Features are multiplied and added together repeatedly, with the outputs from one layer of parameters being fed into the next layer -- before a prediction is made. Contrast with linear regression: Agenda for today - More on the structure of neural network models - <b>Machine</b> <b>learning</b> training loop and concept of loss, in the context ...", "dateLastCrawled": "2022-02-02T09:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Deep Learning and Information Theory</b> \u2013 Deep &amp; Shallow", "url": "https://deep-and-shallow.com/2020/01/09/deep-learning-and-information-theory/", "isFamilyFriendly": true, "displayUrl": "https://deep-and-shallow.com/2020/01/09/<b>deep-learning-and-information-theory</b>", "snippet": "If you have tried to understand the maths behind <b>machine</b> <b>learning</b>, including deep <b>learning</b>, you would have come across topics from Information Theory \u2013 Entropy, <b>Cross Entropy</b>, KL Divergence, etc. The concepts from information theory is ever prevalent in the realm of <b>machine</b> <b>learning</b>, right from the splitting criteria of a Decision Tree to loss functions in Generative Adversarial Networks.", "dateLastCrawled": "2022-02-01T00:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "[D] A Short Introduction to Entropy, <b>Cross-Entropy</b> and KL-Divergence ...", "url": "https://www.reddit.com/r/MachineLearning/comments/7vhmp7/d_a_short_introduction_to_entropy_crossentropy/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/<b>MachineLearning</b>/comments/7vhmp7/d_a_short_introduction_to...", "snippet": "I am having trouble reconciling the concept with the <b>analogy</b>. At 2:35 even if a rainy day was 25% likely, there&#39;s still only two states, rainy and sunny, and therefor only 1 bit of information is needed to convey that, so only one bit of data needs to be sent, even though the 1 bit of data reduces the uncertainty of a rainy day by a factor of 4. I quite don&#39;t get what he means by this being 2 bits of information. I guess where I am stuck is how the uncertainty reduction factor translates to ...", "dateLastCrawled": "2021-08-20T08:03:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Beat the Bookmakers With Tree-Based <b>Machine</b> <b>Learning</b> Algorithms | by ...", "url": "https://medium.com/analytics-vidhya/beat-the-bookmakers-with-tree-based-machine-learning-algorithms-1d349335b54", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/beat-the-bookmakers-with-tree-based-<b>machine</b>...", "snippet": "<b>Cross-entropy is similar</b> to Gini Impurity, but it involves using the concept of entropy from information theory. This article won\u2019t go in depth about it, but essentially, as the cross-entropy ...", "dateLastCrawled": "2022-01-26T04:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>A Traveler\u2019s Diary on the Road to Machine</b> <b>Learning</b> - Chapter 1 | by ...", "url": "https://medium.com/swlh/a-travelers-diary-on-the-road-to-machine-learning-chapter-1-8850ec5b4243", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/<b>a-travelers-diary-on-the-road-to-machine</b>-<b>learning</b>-chapter-1...", "snippet": "Types of <b>Machine</b> <b>Learning</b> algorithms: ... Sparse categorical <b>cross entropy is similar</b> to categorical cross entropy, only difference is it uses only one value as target. It saves memory as well as ...", "dateLastCrawled": "2021-05-21T04:27:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Deep Learning for Computer Architects</b> | Chen Jeff - Academia.edu", "url": "https://www.academia.edu/40860009/Deep_Learning_for_Computer_Architects", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/40860009/<b>Deep_Learning_for_Computer_Architects</b>", "snippet": "This text serves as a primer for computer architects in a new and rapidly evolving \ufb01eld. We review how <b>machine</b> <b>learning</b> has evolved since its inception in the 1960s and track the key developments leading up to the emergence of the powerful deep <b>learning</b> techniques that emerged in the last decade.", "dateLastCrawled": "2022-01-28T02:18:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(cross-entropy)  is like +(distance between two probability distributions)", "+(cross-entropy) is similar to +(distance between two probability distributions)", "+(cross-entropy) can be thought of as +(distance between two probability distributions)", "+(cross-entropy) can be compared to +(distance between two probability distributions)", "machine learning +(cross-entropy AND analogy)", "machine learning +(\"cross-entropy is like\")", "machine learning +(\"cross-entropy is similar\")", "machine learning +(\"just as cross-entropy\")", "machine learning +(\"cross-entropy can be thought of as\")", "machine learning +(\"cross-entropy can be compared to\")"]}