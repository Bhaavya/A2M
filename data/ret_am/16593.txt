{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "AI Fairness | Data Science Portfolio", "url": "https://sourestdeeds.github.io/blog/ai-fairness/", "isFamilyFriendly": true, "displayUrl": "https://sourestdeeds.github.io/blog/ai-fairness", "snippet": "The <b>true</b> <b>positive</b> <b>rate</b> (<b>TPR</b>) for Group A is 77.23%, and the <b>TPR</b> for Group B is 98.03%. These percentages can be calculated directly from the confusion matrix; for instance, for Group A, the <b>TPR</b> is 7528/(7528+2219). 1) Varieties of fairness. Consider three different types of fairness covered in the tutorial:", "dateLastCrawled": "2022-02-02T22:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Classification Table</b> | Real Statistics Using Excel", "url": "https://www.real-statistics.com/logistic-regression/classification-table/", "isFamilyFriendly": true, "displayUrl": "https://www.real-statistics.com/logistic-regression/<b>classification-table</b>", "snippet": "<b>True</b> <b>Positive</b> <b>Rate</b> (<b>TPR</b>), aka Sensitivity = TP/OP = 483/522 = .925287 (cell AE10) <b>True</b> Negative <b>Rate</b> (TNR), aka Specificity = TN/ON = 137/338 = .405325 (cell AF10) Accuracy (ACC) = (TP + TN)/Tot = (483 + 137) / 860 = .720930 (cell AG10) False <b>Positive</b> <b>Rate</b> (FPR) = 1 \u2013 TNR = FP/ON = 201/338 = .594675. <b>Positive</b> Predictive Value (PPV) = TP/PP = 483/684 = .70614. Negative Predictive Value (NPV) = TN/PN = 137/176 = .77841. The overall accuracy of the logistic regression model is a measure of ...", "dateLastCrawled": "2022-02-02T16:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Confusion Matrix</b> for Your Multi-Class Machine Learning Model | by ...", "url": "https://towardsdatascience.com/confusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>confusion-matrix</b>-for-your-multi-class-machine-learning...", "snippet": "It is also known as <b>True</b> <b>Positive</b> <b>Rate</b> (<b>TPR</b>), Sensitivity, Probability of Detection. To calculate Recall, use the following formula: TP/(TP+FN). Specificity: It tells you what fraction of all negative samples are correctly predicted as negative by the classifier. It is also known as <b>True</b> Negative <b>Rate</b> (TNR). To calculate specificity, use the following formula: TN/(TN+FP). F1-score: It combines precision and recall into a single measure. Mathematically it\u2019s the harmonic mean of precision ...", "dateLastCrawled": "2022-01-30T03:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Classification Accuracy</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/engineering/classification-accuracy", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/engineering/<b>classification-accuracy</b>", "snippet": "<b>True</b> <b>positive</b> (TP) False <b>positive</b> (FP) Class 0: False negative (FN) <b>True</b> negative (TN) Here, the accuracy could be calculated by taking the ratio of the <b>true</b> predictions versus all entries as (TP+TN)/(TP+TN+FP+FN). The <b>true</b> <b>positive</b> <b>rate</b> (<b>TPR</b>), or sensitivity, is defined as TP/(TP+FN), and corresponds to the proportion of <b>positive</b> data points that are correctly classified as <b>positive</b> with respect to all the <b>positive</b> data points. On the other hand, the <b>true</b> negative <b>rate</b>, or specificity, is ...", "dateLastCrawled": "2022-01-28T05:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Evaluation of Classification Model Accuracy</b>: Essentials - Articles - STHDA", "url": "http://www.sthda.com/english/articles/36-classification-methods-essentials/143-evaluation-of-classification-model-accuracy-essentials/", "isFamilyFriendly": true, "displayUrl": "www.sthda.com/english/articles/36-classification-methods-essentials/143-evaluation-of...", "snippet": "Since we don\u2019t usually know the probability cutoff in advance, the ROC curve is typically used to plot the <b>true</b> <b>positive</b> <b>rate</b> (or sensitivity on y-axis) against the false <b>positive</b> <b>rate</b> (or \u201c1-specificity\u201d on x-axis) at all possible probability cutoffs. This shows the trade off between the <b>rate</b> at which you can correctly predict something with the <b>rate</b> of incorrectly predicting something. Another visual representation of the ROC plot is to simply display the sensitive against the ...", "dateLastCrawled": "2022-02-03T05:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "An illustrative example of the ROC curve. The values of <b>TPR</b> and FPR of ...", "url": "https://www.researchgate.net/figure/An-illustrative-example-of-the-ROC-curve-The-values-of-TPR-and-FPR-of-each_fig4_327148996", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/figure/An-illustrative-example-of-the-ROC-curve-The...", "snippet": "In micro averaging of <b>true</b> <b>positive</b> <b>rate</b> (<b>TPR</b>), the number of valid <b>classifications</b> for each class is calculated and used as the numerator, while the total number of samples is used as the ...", "dateLastCrawled": "2021-11-17T19:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "How to evaluate my <b>Classification</b> Model results | by Songhao Wu ...", "url": "https://towardsdatascience.com/top-5-metrics-for-evaluating-classification-model-83ede24c7584", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/top-5-metrics-for-evaluating-<b>classification</b>-model-83ede...", "snippet": "Each point on the curve represents a different cut-off point between <b>TPR</b> and FPR. For example, if I cannot tolerate any false <b>positive</b> <b>rate</b> (&lt;0.01), then the <b>true</b> <b>positive</b> <b>rate</b> I can reach is around 0.6(see the green dot on the graph). If I loosen my criteria a bit and I only need to control FPR below 0.1, then my <b>TPR</b> can reach 0.9(see the red ...", "dateLastCrawled": "2022-02-02T13:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Classification: <b>True</b> vs. False and <b>Positive</b> vs. Negative | Machine ...", "url": "https://developers.google.com/machine-learning/crash-course/classification/true-false-positive-negative", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/.../crash-course/classification/<b>true</b>-false-<b>positive</b>-negative", "snippet": "A <b>true</b> <b>positive</b> is an outcome where the model correctly predicts the <b>positive</b> class. Similarly, a <b>true</b> negative is an outcome where the model correctly predicts the negative class.. A false <b>positive</b> is an outcome where the model incorrectly predicts the <b>positive</b> class. And a false negative is an outcome where the model incorrectly predicts the negative class.. In the following sections, we&#39;ll look at how to evaluate classification models using metrics derived from these four outcomes.", "dateLastCrawled": "2022-02-02T23:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Classification Accuracy is Not Enough: More Performance Measures You ...", "url": "https://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/classification-accuracy-is-not-enough-", "snippet": "It is also called Sensitivity or the <b>True</b> <b>Positive</b> <b>Rate</b>. Recall can be thought of as a measure of a classifiers completeness. A low recall indicates many False Negatives. The recall of the All No Recurrence model is 0/(0+85) or 0. The recall of the All Recurrence model is 85/(85+0) or 1. The recall of CART is 10/(10+75) or 0.12. As you would expect, the All Recurrence model has a perfect recall because it predicts \u201crecurrence\u201d for all instances. The recall for CART is lower than that of ...", "dateLastCrawled": "2022-02-03T05:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Introduction to the Classification Model Evaluation</b> | Baeldung on ...", "url": "https://www.baeldung.com/cs/classification-model-evaluation", "isFamilyFriendly": true, "displayUrl": "https://www.baeldung.com/cs/classification-model-evaluation", "snippet": "It measures \u201chow complete the results are\u201d \u2014 that is, which <b>percentage</b> of <b>true</b> positives are predicted as <b>positive</b>. The representation is: That is to say, a recall of 90% would mean that the classifier correctly labels 90% of all the spam e-mails, hence 10% are marked as not spam.", "dateLastCrawled": "2022-01-31T13:48:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "AI Fairness | Data Science Portfolio", "url": "https://sourestdeeds.github.io/blog/ai-fairness/", "isFamilyFriendly": true, "displayUrl": "https://sourestdeeds.github.io/blog/ai-fairness", "snippet": "The <b>true</b> <b>positive</b> <b>rate</b> (<b>TPR</b>) for Group A is 77.23%, and the <b>TPR</b> for Group B is 98.03%. These percentages can be calculated directly from the confusion matrix; for instance, for Group A, the <b>TPR</b> is 7528/(7528+2219). 1) Varieties of fairness. Consider three different types of fairness covered in the tutorial:", "dateLastCrawled": "2022-02-02T22:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Confusion Matrix</b> for Your Multi-Class Machine Learning Model | by ...", "url": "https://towardsdatascience.com/confusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>confusion-matrix</b>-for-your-multi-class-machine-learning...", "snippet": "It is also known as <b>True</b> <b>Positive</b> <b>Rate</b> (<b>TPR</b>), Sensitivity, Probability of Detection. To calculate Recall, use the following formula: TP/(TP+FN). Specificity: It tells you what fraction of all negative samples are correctly predicted as negative by the classifier. It is also known as <b>True</b> Negative <b>Rate</b> (TNR). To calculate specificity, use the following formula: TN/(TN+FP). F1-score: It combines precision and recall into a single measure. Mathematically it\u2019s the harmonic mean of precision ...", "dateLastCrawled": "2022-01-30T03:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Classification: <b>True</b> vs. False and <b>Positive</b> vs. Negative | Machine ...", "url": "https://developers.google.com/machine-learning/crash-course/classification/true-false-positive-negative", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/.../crash-course/classification/<b>true</b>-false-<b>positive</b>-negative", "snippet": "A <b>true</b> <b>positive</b> is an outcome where the model correctly predicts the <b>positive</b> class. Similarly, a <b>true</b> negative is an outcome where the model correctly predicts the negative class.. A false <b>positive</b> is an outcome where the model incorrectly predicts the <b>positive</b> class. And a false negative is an outcome where the model incorrectly predicts the negative class.. In the following sections, we&#39;ll look at how to evaluate classification models using metrics derived from these four outcomes.", "dateLastCrawled": "2022-02-02T23:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "An illustrative example of the ROC curve. The values of <b>TPR</b> and FPR of ...", "url": "https://www.researchgate.net/figure/An-illustrative-example-of-the-ROC-curve-The-values-of-TPR-and-FPR-of-each_fig4_327148996", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/figure/An-illustrative-example-of-the-ROC-curve-The...", "snippet": "In micro averaging of <b>true</b> <b>positive</b> <b>rate</b> (<b>TPR</b>), the number of valid <b>classifications</b> for each class is calculated and used as the numerator, while the total number of samples is used as the ...", "dateLastCrawled": "2021-11-17T19:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Correct</b> Classification <b>Rate</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/engineering/correct-classification-rate", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/engineering/<b>correct</b>-classification-<b>rate</b>", "snippet": "The <b>correct</b> classification <b>rate</b> (CCR) can be defined as a key gauge employed for analyzing one particular or even classifier. Nevertheless, CCR only can be inadequate regarding gauging a functionality of the classifier for a static security index data set. And so, the <b>true</b> negative <b>rate</b> (TNR) and <b>true</b> <b>positive</b> <b>rate</b> (<b>TPR</b>) were used to evaluate the classifier performance. Moreover, geometric mean (GM) was additionally utilized in this research to assess the actual overall performance regarding ...", "dateLastCrawled": "2022-02-02T03:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "All Metrices you need to know for becoming a Data Scientist - CloudyML", "url": "https://www.cloudyml.com/blog/all-evaluation-metrics-you-need-to-know/", "isFamilyFriendly": true, "displayUrl": "https://www.cloudyml.com/blog/all-evaluation-metrics-you-need-to-know", "snippet": "AUC score is calculated from the plot for False <b>Positive</b> <b>Rate</b>(FPS) vs <b>True</b> <b>Positive</b> <b>Rate</b>(<b>TPR</b>). The ROC curve is plotted with <b>TPR</b> (Sensitivity) against the FPR (1-Specificity) where <b>TPR</b> is on the y-axis and FPR is on the x-axis. Greater the value of ROC-AUC, the greater will be the performance of the model. Its range is [0, 1]. It can be implemented using sklearn\u2019s \u2018 roc_auc_score\u2019 method. 7. Mean Absolute Error(MAE) Where, y j = Predicted Value. \u0177 j = Actual Value. It measures the ...", "dateLastCrawled": "2021-12-22T09:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "How to evaluate my <b>Classification</b> Model results | by Songhao Wu ...", "url": "https://towardsdatascience.com/top-5-metrics-for-evaluating-classification-model-83ede24c7584", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/top-5-metrics-for-evaluating-<b>classification</b>-model-83ede...", "snippet": "Each point on the curve represents a different cut-off point between <b>TPR</b> and FPR. For example, if I cannot tolerate any false <b>positive</b> <b>rate</b> (&lt;0.01), then the <b>true</b> <b>positive</b> <b>rate</b> I can reach is around 0.6(see the green dot on the graph). If I loosen my criteria a bit and I only need to control FPR below 0.1, then my <b>TPR</b> can reach 0.9(see the red ...", "dateLastCrawled": "2022-02-02T13:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Confusion Matrix</b>, Accuracy, Precision, Recall, F1 Score | by ...", "url": "https://medium.com/analytics-vidhya/confusion-matrix-accuracy-precision-recall-f1-score-ade299cf63cd", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/<b>confusion-matrix</b>-accuracy-precision-recall-f1...", "snippet": "Accuracy represents the number of correctly classified data instances over the total number of data instances. In this example, Accuracy = (55 + 30)/(55 + 5 + 30 + 10 ) = 0.85 and in <b>percentage</b> ...", "dateLastCrawled": "2022-02-02T15:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Top <b>10 Evaluation Metrics for Classification</b> Models", "url": "https://www.explorium.ai/blog/top-10-evaluation-metrics-for-classification-models/", "isFamilyFriendly": true, "displayUrl": "https://www.explorium.ai/blog/top-<b>10-evaluation-metrics-for-classification</b>-models", "snippet": "This metric basically shows the number <b>of correct</b> <b>positive</b> class predictions made as a proportion of all of the predictions made. Detection <b>Rate</b> = TP / TP + FP + FN + TN. 4. Logarithmic loss . Also known as log loss, logarithmic loss basically functions by penalizing all false/incorrect <b>classifications</b>. The classifier must assign a specific probability to each class for all samples while working with this metric. The formula for calculating log loss is as follows: Yij - Indicates if sample i ...", "dateLastCrawled": "2022-01-25T01:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How to Calculate Precision, Recall, and F-Measure for Imbalanced ...", "url": "https://machinelearningmastery.com/precision-recall-and-f-measure-for-imbalanced-classification/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/precision-recall-and-f-measure-for-", "snippet": "You can see that precision is simply the ratio <b>of correct</b> <b>positive</b> predictions out of all <b>positive</b> predictions made, or the accuracy of minority class predictions. Consider the same dataset, where a model predicts 50 examples belonging to the minority class, 45 of which are <b>true</b> positives and five of which are false positives. We can calculate the precision for this model as follows: Precision = TruePositives / (TruePositives + FalsePositives) Precision = 45 / (45 + 5) Precision = 45 / 50 ...", "dateLastCrawled": "2022-02-03T02:15:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Binary Classification Task</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/engineering/binary-classification-task", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/engineering/<b>binary-classification-task</b>", "snippet": "The <b>true</b> <b>positive</b> <b>rate</b> (<b>TPR</b>), or sensitivity, is defined as TP/(TP+FN), and corresponds to the proportion of <b>positive</b> data points that are correctly classified as <b>positive</b> with respect to all the <b>positive</b> data points. On the other hand, the <b>true</b> negative <b>rate</b>, or specificity, is defined as TN/(TN+FP), which is the ratio of the correctly classified negative data points to all the negative data points. Another metric called the false <b>positive</b> <b>rate</b> (FPR) is defined as FP/(FP+TN). FPR ...", "dateLastCrawled": "2022-01-17T06:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Classification Accuracy is Not Enough: More Performance Measures You ...", "url": "https://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/classification-accuracy-is-not-enough-", "snippet": "Put another way it is the number of <b>positive</b> predictions divided by the number of <b>positive</b> class values in the test data. It is also called Sensitivity or the <b>True</b> <b>Positive</b> <b>Rate</b>. Recall <b>can</b> <b>be thought</b> of as a measure of a classifiers completeness. A low recall indicates many False Negatives. The recall of the All No Recurrence model is 0/(0+85 ...", "dateLastCrawled": "2022-02-03T05:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Top 70+ <b>Data Science Interview Questions and Answers</b> for 2022 - Intellipaat", "url": "https://intellipaat.com/blog/interview-question/data-science-interview-questions/", "isFamilyFriendly": true, "displayUrl": "https://intellipaat.com/blog/interview-question/data-science-interview-", "snippet": "<b>True</b> <b>positive</b> <b>rate</b>: In Machine Learning, <b>true</b>-<b>positive</b> rates, which are also referred to as sensitivity or recall, are used to measure the <b>percentage</b> of actual positives which are correctly identified. Formula: <b>True</b> <b>Positive</b> <b>Rate</b> = <b>True</b> Positives/Positives False <b>positive</b> <b>rate</b>: False <b>positive</b> <b>rate</b> is basically the probability of falsely rejecting the null hypothesis for a particular test. The false-<b>positive</b> <b>rate</b> is calculated as the ratio between the number of negative events wrongly ...", "dateLastCrawled": "2022-02-03T02:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Bootstrap estimated true and false positive</b> rates and ROC curve ...", "url": "https://www.researchgate.net/publication/23629765_Bootstrap_estimated_true_and_false_positive_rates_and_ROC_curve", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/23629765_<b>Bootstrap_estimated_true_and_false</b>...", "snippet": "Recall or sensitivity (<b>true</b> <b>positive</b> <b>rate</b>, <b>TPR</b>) and missing <b>rate</b> (false negative <b>rate</b>, FNR) are the rates at which shadow areas are correctly and incorrectly segmented, respectively, while the ...", "dateLastCrawled": "2021-10-23T04:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "IT Support <b>Ticket</b> Classification and Deployment using Machine Learning ...", "url": "https://towardsdatascience.com/it-support-ticket-classification-and-deployment-using-machine-learning-and-aws-lambda-8ef8b82643b6", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/it-support-<b>ticket</b>-classification-and-deployment-using...", "snippet": "\u00b7 TP represents the number of <b>true</b> <b>positive</b> <b>classifications</b>. That is, the records with the actual label A that have been correctly classified, or \u201epredicted\u201d, as label A. \u00b7 TN is the number of <b>true</b> negative <b>classifications</b>. That is, the records with an actual label not equal to A that have been correctly classified as not belonging to label A. \u00b7 FP is the number of false <b>positive</b> <b>classifications</b>, i.e., records with an actual label other than A that have been incorrectly classified as ...", "dateLastCrawled": "2022-02-03T02:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "20 Popular Machine Learning <b>Metrics</b>. Part 1: Classification ...", "url": "https://towardsdatascience.com/20-popular-machine-learning-metrics-part-1-classification-regression-evaluation-metrics-1ca3e282a2ce", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/20-popular-machine-learning-<b>metrics</b>-part-1...", "snippet": "Recall= <b>True</b>_<b>Positive</b>/ (<b>True</b>_<b>Positive</b>+ False_Negative) Therefore, for our example above, the recall <b>rate</b> of cat and non-cat classes <b>can</b> be found as: Recall_cat= 90/100= 90%. Recall_NonCat= 940/1000= 94%. 5- F1 Score. Depending on application, you may want to give higher priority to recall or precision. But there are many applications in which ...", "dateLastCrawled": "2022-01-29T14:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Receiver operating characteristic</b> - WikiMili, The Best Wikipedia Reader", "url": "https://wikimili.com/en/Receiver_operating_characteristic", "isFamilyFriendly": true, "displayUrl": "https://wikimili.com/en/<b>Receiver_operating_characteristic</b>", "snippet": "The contingency table <b>can</b> derive several evaluation &quot;metrics&quot; (see infobox). To draw a ROC curve, only the <b>true</b> <b>positive</b> <b>rate</b> (<b>TPR</b>) and false <b>positive</b> <b>rate</b> (FPR) are needed (as functions of some classifier parameter). The <b>TPR</b> defines how many <b>correct</b> <b>positive</b> results occur among all <b>positive</b> samples available during the test. FPR, on the other ...", "dateLastCrawled": "2021-05-29T18:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Entry 23: Scoring Classification Models - Theory</b> - Data Science Diaries", "url": "https://julielinx.github.io/blog/23_class_score_theory/", "isFamilyFriendly": true, "displayUrl": "https://julielinx.github.io/blog/23_class_score_theory", "snippet": "LinkedIn. <b>Entry 23: Scoring Classification Models - Theory</b>. 14 minute read. Classification models present a different challenge than regression models. Because a numeric value isn\u2019t returned, another way of measuring goodness of fit has to be used. The Problem Permalink. Regression models return a numeric prediction.", "dateLastCrawled": "2022-01-29T03:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Not so soft softmax</b> - OSM", "url": "https://www.optionstocksmachines.com/post/2021-03-24-neural-nets-3/", "isFamilyFriendly": true, "displayUrl": "https://www.optionstocksmachines.com/post/2021-03-24-neural-nets-3", "snippet": "The NN model has a much better <b>true</b> <b>positive</b> <b>rate</b> (<b>TPR</b>) than the logistic regression for Good outcomes, but the false <b>positive</b> <b>rate</b> (FPR) is high too. While the NN model is worse than the regression model on Stinky outcomes, it\u2019s FPR is less than half that of the regression. Interestingly, both models are poor at predicting one category: Poor outcomes for the regression model vs. Mediocre outcomes for the NN. We\u2019re not sure why that would be the case.", "dateLastCrawled": "2022-01-09T02:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How To Deal With <b>Class Imbalance</b> And Machine Learning On Big Data - VMware", "url": "https://tanzu.vmware.com/content/blog/how-to-deal-with-class-imbalance-and-machine-learning-on-big-data", "isFamilyFriendly": true, "displayUrl": "https://tanzu.vmware.com/content/blog/how-to-deal-with-<b>class-imbalance</b>-and-machine...", "snippet": "A confusion matrix is a 2\u00d72 table which contains the number <b>of correct</b> <b>classifications</b> and misclassifications for both <b>positive</b> and negative responses. A <b>positive</b> response is an observation that belongs to the class we are trying to predict (namely the rare class), and a negative response is an observation that does not belong to the rare class. Table 1: Definition of the confusion matrix. Confusion matrices consist of four values\u2014the number of <b>true</b> positives, false positives, false ...", "dateLastCrawled": "2022-01-21T02:25:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A30: Logistic Regression (Part-2)&gt;&gt; Behind the Scene! | by Junaid Qazi ...", "url": "https://medium.com/mlearning-ai/a30-logistic-regression-part-2-behind-the-scene-38a98b70192a", "isFamilyFriendly": true, "displayUrl": "https://medium.com/mlearning-ai/a30-logistic-regression-part-2-behind-the-scene-38a98b...", "snippet": "&gt;&gt; 7.2.3: Recall / Sensitivity / <b>True</b> <b>Positive</b> <b>Rate</b> (<b>TPR</b>) &lt;&lt; The recall measures out of all the times the <b>true</b> label was <b>positive</b>, the predicted label was also <b>positive</b>. recall = tp / (tp + fn)", "dateLastCrawled": "2022-02-03T12:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Classification: <b>ROC</b> Curve and AUC | Machine Learning Crash Course ...", "url": "https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/machine-learning/crash-course/classification/<b>roc</b>-and-auc", "snippet": "An <b>ROC</b> curve plots <b>TPR</b> vs. FPR at different classification thresholds. Lowering the classification threshold classifies more items as <b>positive</b>, thus increasing both False Positives and <b>True</b> Positives. The following figure shows a typical <b>ROC</b> curve. Figure 4. TP vs. FP <b>rate</b> at different classification thresholds.", "dateLastCrawled": "2022-02-02T09:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Classification: <b>True</b> vs. False and <b>Positive</b> vs. Negative | Machine ...", "url": "https://developers.google.com/machine-learning/crash-course/classification/true-false-positive-negative", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/.../crash-course/classification/<b>true</b>-false-<b>positive</b>-negative", "snippet": "A <b>true</b> <b>positive</b> is an outcome where the model correctly predicts the <b>positive</b> class. Similarly, a <b>true</b> negative is an outcome where the model correctly predicts the negative class.. A false <b>positive</b> is an outcome where the model incorrectly predicts the <b>positive</b> class. And a false negative is an outcome where the model incorrectly predicts the negative class.. In the following sections, we&#39;ll look at how to evaluate classification models using metrics derived from these four outcomes.", "dateLastCrawled": "2022-02-02T23:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Bone metabolic biomarker-based diagnosis of type 2 diabetes ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7944260/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7944260", "snippet": "The <b>percentage</b> of <b>true</b> <b>positive</b> samples in the real <b>positive</b> data set is referred to as Sen. The ROC-AUC value is a score obtained from the receiver operating characteristic (ROC) analysis. ROC plots the function of <b>true</b> <b>positive</b> <b>rate</b> (<b>TPR</b>) and false <b>positive</b> <b>rate</b> (FPR) when verifying threshold. The increase of <b>TPR</b> comes at the cost of increasing FPR. The ROC-AUC was used as another evaluation of model accuracy. Values of ROC-AUC always lie between 0 and 1 among which the values above 0.9 ...", "dateLastCrawled": "2022-01-10T18:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "BUILDING CLASSIFICATION MODELS FROM IMBALANCED FRAUD DETECTION DATA", "url": "http://mjoc.uitm.edu.my/source/journal/2014/2/2014_2_2.pdf", "isFamilyFriendly": true, "displayUrl": "mjoc.uitm.edu.my/source/journal/2014/2/2014_2_2.pdf", "snippet": "<b>True</b> <b>Positive</b> <b>Rate</b>, <b>TPR</b> = <b>True</b> <b>Positive</b> / (False Negative + <b>True</b> <b>Positive</b>) <b>TPR</b> is the <b>percentage</b> of <b>positive</b> instances correctly classified within the <b>positive</b> class. In another paper, Nguyen, Bouzerdoum &amp; Phung (2010) introduced three evaluation metrics namely Precision, Recall and F-measure. These metrics are developed from the fields of information retrieval. They are used in situations when performance for the <b>positive</b> class (the minority class) is preferred, since both precision and ...", "dateLastCrawled": "2021-11-21T00:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Predicting Customer Churn in Mobile Telephony Industry Using ...", "url": "http://didawiki.di.unipi.it/lib/exe/fetch.php/dm/churn-mobilephone.pdf", "isFamilyFriendly": true, "displayUrl": "didawiki.di.unipi.it/lib/exe/fetch.php/dm/churn-mobilephone.pdf", "snippet": "number <b>of correct</b> <b>classifications</b> from the 10 iterations, ... among others, <b>can</b> be obtained. Accuracy: The <b>percentage</b> of correctly classified instances over the total number of instances. <b>True</b> <b>positive</b> <b>rate</b> (<b>TPR</b>) or sensitivity: fraction of <b>positive</b> instances predicted correctly. False <b>positive</b> <b>rate</b> (FPR): fraction of negative instances wrongly predicted as <b>positive</b>. Precision: fraction of records that actually turn out to be <b>positive</b> in the group the classifier has declared as <b>positive</b>. The ...", "dateLastCrawled": "2022-01-27T23:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Performance</b> Metrics for Machine Learning Models | by Sachin D N ...", "url": "https://medium.com/analytics-vidhya/performance-metrics-for-machine-learning-models-80d7666b432e", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/<b>performance</b>-metrics-for-machine-learning-models-80...", "snippet": "Specificity (or) <b>True</b> Negative <b>Rate</b>: Specificity, in contrast, to recall, may be defined as the number of negatives returned by our ML model. We <b>can</b> easily calculate it by confusion matrix with ...", "dateLastCrawled": "2022-01-30T13:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Classification <b>metrics</b> based on <b>True</b>/False positives &amp; negatives", "url": "https://keras.io/api/metrics/classification_metrics/", "isFamilyFriendly": true, "displayUrl": "https://<b>keras</b>.io/api/<b>metrics</b>/classification_<b>metrics</b>", "snippet": "Computes the recall of the predictions with respect to the labels. This metric creates two local variables, <b>true</b>_positives and false_negatives, that are used to compute the recall.This value is ultimately returned as recall, an idempotent operation that simply divides <b>true</b>_positives by the sum of <b>true</b>_positives and false_negatives.. If sample_weight is None, weights default to 1.Use sample_weight of 0 to mask values.. If top_k is set, recall will be computed as how often on average a class ...", "dateLastCrawled": "2022-02-02T21:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "How to Calculate Precision, Recall, and F-Measure for Imbalanced ...", "url": "https://machinelearningmastery.com/precision-recall-and-f-measure-for-imbalanced-classification/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/precision-recall-and-f-measure-for-", "snippet": "You <b>can</b> see that precision is simply the ratio <b>of correct</b> <b>positive</b> predictions out of all <b>positive</b> predictions made, or the accuracy of minority class predictions. Consider the same dataset, where a model predicts 50 examples belonging to the minority class, 45 of which are <b>true</b> positives and five of which are false positives. We <b>can</b> calculate the precision for this model as follows: Precision = TruePositives / (TruePositives + FalsePositives) Precision = 45 / (45 + 5) Precision = 45 / 50 ...", "dateLastCrawled": "2022-02-03T02:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Classification Accuracy is Not Enough: More Performance Measures You ...", "url": "https://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/classification-accuracy-is-not-enough-", "snippet": "Put another way it is the number of <b>positive</b> predictions divided by the number of <b>positive</b> class values in the test data. It is also called Sensitivity or the <b>True</b> <b>Positive</b> <b>Rate</b>. Recall <b>can</b> be thought of as a measure of a classifiers completeness. A low recall indicates many False Negatives. The recall of the All No Recurrence model is 0/(0+85 ...", "dateLastCrawled": "2022-02-03T05:36:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine Learning</b> <b>Evaluation Metrics</b> - GitHub Pages", "url": "https://kevalnagda.github.io/evaluation-metrics", "isFamilyFriendly": true, "displayUrl": "https://kevalnagda.github.io/<b>evaluation-metrics</b>", "snippet": "It essentially shows the <b>True</b> <b>Positive</b> <b>Rate</b> (<b>TPR</b>) against the False <b>Positive</b> <b>Rate</b> (FPR) for various threshold values. AUC The Area Under the Curve (AUC), is an aggregated measure of performance of a binary classifier on all possible threshold values (and therefore it is threshold invariant). AUC calculates the area under the ROC curve, and therefore it is between 0 and 1. One way of interpreting AUC is the probability that the model ranks a random <b>positive</b> example more highly than a random ...", "dateLastCrawled": "2021-10-13T19:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) A Comparison of Various <b>Machine</b> <b>Learning</b> Algorithms in a ...", "url": "https://www.academia.edu/68902781/A_Comparison_of_Various_Machine_Learning_Algorithms_in_a_Distributed_Denial_of_Service_Intrusion", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/68902781/A_Comparison_of_Various_<b>Machine</b>_<b>Learning</b>_Algorithms...", "snippet": "2) <b>True</b> <b>Positive</b> <b>Rate</b> (<b>TPR</b>) 4) Decision Tree (DT) This metric calculates how often the model is able to predict a This algorithm uses a tree structure <b>analogy</b> to represent a <b>positive</b> result correctly. Similar to Accuracy, but difference is series of rules that lead to a class or value [16]. It starts with a it only takes <b>positive</b> observation. root node, which is the best predictor. Then, it progresses <b>TPR</b>:: \ud835\udc47\ud835\udc43 through branch nodes to other predictors. Ultimately it reaches \ud835\udc47\ud835\udc43 ...", "dateLastCrawled": "2022-02-05T20:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Evaluation Metric Special ROC-AUC Candra\u2019s blog", "url": "https://saltfarmer.github.io/blog/machine%20learning/Evaluation-Metrics-Special-ROCAUC/", "isFamilyFriendly": true, "displayUrl": "https://saltfarmer.github.io/blog/<b>machine</b> <b>learning</b>/Evaluation-Metrics-Special-ROCAUC", "snippet": "The ROC curve is created by plotting the <b>true</b> <b>positive</b> <b>rate</b> (<b>TPR</b>) against the false <b>positive</b> <b>rate</b> (FPR) at various threshold settings. ROC is a probability curve and AUC represents degree or measure of separability. It tells how much model is capable of distinguishing between classes. Higher the AUC, better the model is at predicting 0s as 0s ...", "dateLastCrawled": "2022-02-03T01:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Classification - Fairness and <b>machine</b> <b>learning</b>", "url": "https://fairmlbook.org/classification.html", "isFamilyFriendly": true, "displayUrl": "https://fairmlbook.org/classification.html", "snippet": "To be clear, the <b>true</b> <b>positive</b> <b>rate</b> corresponds to the frequency with which the classifier correctly assigns a <b>positive</b> label to a <b>positive</b> instance. We call this a <b>true</b> <b>positive</b>. The other terms false <b>positive</b>, false negative, and <b>true</b> negative derive analogously from the respective definitions. It is not important to memorize all these terms ...", "dateLastCrawled": "2022-02-03T07:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "More Performance Evaluation Metrics for Classification Problems You ...", "url": "https://www.kdnuggets.com/2020/04/performance-evaluation-metrics-classification.html", "isFamilyFriendly": true, "displayUrl": "https://www.<b>kdnuggets</b>.com/2020/04/performance-evaluation-metrics-classification.html", "snippet": "A ROC curve plots the <b>true</b> <b>positive</b> <b>rate</b> (<b>tpr</b>) versus the false <b>positive</b> <b>rate</b> (fpr) as a function of the model\u2019s threshold for classifying a <b>positive</b>. Given that c is a constant known as decision threshold, the below ROC curve suggests that by default c=0.5, when c=0.2, both <b>tpr</b> and fpr increase.", "dateLastCrawled": "2022-01-26T05:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Evaluation metric for Supervised <b>Learning</b>: | by Anuganti Suresh | Medium", "url": "https://anugantisuresh.medium.com/evaluation-metric-for-supervised-learning-ba063f1bb1af", "isFamilyFriendly": true, "displayUrl": "https://anugantisuresh.medium.com/evaluation-metric-for-supervised-<b>learning</b>-ba063f1bb1af", "snippet": "A higher <b>TPR</b> and a lower FNR is desirable since we want to correctly classify the <b>positive</b> class. The area under the curve represents the area under the curve when the false <b>positive</b> <b>rate</b> is plotted against the <b>True</b> <b>positive</b> <b>rate</b> as below. AUC ranges between 0 and 1. A value of 0 means 100% prediction of the model is incorrect. A value of 1 ...", "dateLastCrawled": "2022-01-06T20:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Understanding <b>AUC</b> - ROC Curve | by Sarang Narkhede | Towards Data Science", "url": "https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-<b>auc</b>-roc-curve-68b2303cc9c5", "snippet": "In <b>Machine</b> <b>Learning</b>, performance measurement is an essential task. So when it comes to a classification problem, we can count on an <b>AUC</b> - ROC Curve. When we need to check or visualize the performance\u2026 Get started. Open in app. Sign in. Get started. Follow. 617K Followers \u00b7 Editors&#39; Picks Features Deep Dives Grow Contribute. About. Get started. Open in app. Understanding <b>AUC</b> - ROC Curve. Sarang Narkhede. Jun 26, 2018 \u00b7 5 min read. Understanding <b>AUC</b> - ROC Curve [Image 1] (Image courtesy ...", "dateLastCrawled": "2022-01-30T12:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Understanding <b>Classification</b> Thresholds Using Isocurves | by Druce ...", "url": "https://towardsdatascience.com/understanding-classification-thresholds-using-isocurves-9e5e7e00e5a2", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-<b>classification</b>-<b>threshold</b>s-using-isocurves...", "snippet": "The <b>true</b>-<b>positive</b> <b>rate</b> (<b>TPR</b>) is the number of <b>true</b> positives / ground truth positives (also called recall or sensitivity). Ground truth positives = <b>true</b> positives + false negatives: <b>TPR</b> = tp / (tp+fn) A false <b>positive</b> is a false observation incorrectly predicted to be <b>true</b>. The false-<b>positive</b> <b>rate</b> (FPR) is the number of false positives / ground truth negatives (1 \u2014 FPR is the specificity). Ground truth negatives = <b>true</b> negatives + false positives: FPR = fp / (tn + fp) The best point to be ...", "dateLastCrawled": "2022-02-02T19:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and-sample-codes ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/README.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "ROC (Receiver Operating Characteristic curve): A curve of <b>true</b> <b>positive</b> <b>rate</b> vs. false <b>positive</b> <b>rate</b> at different classification thresholds. The x-axis is False <b>Positive</b> <b>rate</b>, and the y-axis is <b>True</b> <b>Positive</b> <b>rate</b>. [3] . Close to the up left point (<b>TPR</b>=1.0, FPR=0.0) indicates the model is better.", "dateLastCrawled": "2021-09-22T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What is the AUC \u2014 <b>ROC</b> Curve?. AUC-<b>ROC</b> CURVE | CONFUSION MATRIX |\u2026 | by ...", "url": "https://medium.com/computer-architecture-club/what-is-the-auc-roc-curve-47fbdcbf7a4a", "isFamilyFriendly": true, "displayUrl": "https://medium.com/computer-architecture-club/what-is-the-auc-<b>roc</b>-curve-47fbdcbf7a4a", "snippet": "By <b>analogy</b>, Higher the AUC, ... Sensitivity / <b>TPR</b> (<b>True</b> <b>Positive</b> <b>Rate</b>) / Recall. Sensitivity tells us what proportion of the <b>positive</b> class got correctly classified. A simple example would be to ...", "dateLastCrawled": "2022-01-26T00:32:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "How <b>to calculate the image accuracy through ROC method</b>?", "url": "https://www.researchgate.net/post/How_to_calculate_the_image_accuracy_through_ROC_method", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/post/How_<b>to_calculate_the_image_accuracy_through_ROC_method</b>", "snippet": "<b>True Positive Rate (TPR) is like</b> a recall and is defined as mathematically . TPR = (TP/TP+FN) False Positive Rate (FPR) is defined as mathematically . FPR = (FP/FP+TN) An ROC curve plots TPR vs ...", "dateLastCrawled": "2022-01-17T03:34:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], []], "all_bing_queries": ["+(true positive rate (tpr))  is like +(percentage of correct classifications)", "+(true positive rate (tpr)) is similar to +(percentage of correct classifications)", "+(true positive rate (tpr)) can be thought of as +(percentage of correct classifications)", "+(true positive rate (tpr)) can be compared to +(percentage of correct classifications)", "machine learning +(true positive rate (tpr) AND analogy)", "machine learning +(\"true positive rate (tpr) is like\")", "machine learning +(\"true positive rate (tpr) is similar\")", "machine learning +(\"just as true positive rate (tpr)\")", "machine learning +(\"true positive rate (tpr) can be thought of as\")", "machine learning +(\"true positive rate (tpr) can be compared to\")"]}