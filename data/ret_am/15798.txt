{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Machine <b>Learning</b> Theory - Part 2: Generalization Bounds", "url": "https://mostafa-samir.github.io/ml-theory-pt2/", "isFamilyFriendly": true, "displayUrl": "https://mostafa-samir.github.io/ml-theory-pt2", "snippet": "<b>Independently</b>, <b>and Identically</b> <b>Distributed</b>. The <b>world</b> can be a very messy place! This is a problem that faces any theoretical analysis of a real <b>world</b> phenomenon; because usually we can\u2019t really capture all the messiness in mathematical terms, and even if we\u2019re able to; we usually don\u2019t have the tools to get any results from such a messy mathematical model. So in order for theoretical analysis to move forward, some assumptions must be made to simplify the situation at hand, we can then ...", "dateLastCrawled": "2022-02-01T09:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Neighborhood linear discriminant analysis - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S0031320321005987", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0031320321005987", "snippet": "Linear Discriminant Analysis (LDA) assumes that all samples from the same class are <b>independently</b> <b>and identically</b> <b>distributed</b> (<b>i.i.d</b>.). LDA may fail in the cases where the assumption does not hold. Particularly when a class contains several clusters (or subclasses), LDA cannot correctly depict the internal structure as the scatter matrices that LDA relies on are defined at the class level. In order to mitigate the problem, this paper proposes a neighborhood linear discriminant analysis (nLDA ...", "dateLastCrawled": "2022-01-26T22:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>Simulated iterative classification a new learning procedure</b> for ...", "url": "https://www.academia.edu/14716432/Simulated_iterative_classification_a_new_learning_procedure_for_graph_labeling", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/14716432", "snippet": "1 Introduction A fundamental assumption that underlies most existing work in machine learn- ing is that data is <b>independently</b> <b>and identically</b> <b>distributed</b> (<b>i.i.d</b>.). Web pages classification, WebSpam detection, community identification in social networks and peer-to-peer files analysis are typical applications where data is naturally organized according to a graph structure. In these applications, the elements to classify (Web pages or users of files for example) are interdependent: the label ...", "dateLastCrawled": "2022-01-24T21:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Machine Learning</b> - Term Paper", "url": "https://www.termpaperwarehouse.com/essay-on/Machine-Learning/309411", "isFamilyFriendly": true, "displayUrl": "https://www.termpaperwarehouse.com/essay-on/<b>Machine-Learning</b>/309411", "snippet": "We assume that examples are <b>independently</b> <b>and identically</b> <b>distributed</b> (<b>i.i.d</b>.) according to some \ufb01xed but unknown distribution D. The <b>learning</b> problem is then formulated as follows. The learner considers a \ufb01xed set of possible concepts H, called a hypothesis set, which may not coincide with C. He receives a sample S = (x1 , . . . , xm ) drawn <b>i.i.d</b>. according to D as well as the labels (c(x1 ), . . . , c(xm )), which are based on a speci\ufb01c target concept c \u2208 C to learn. His task is ...", "dateLastCrawled": "2022-02-02T19:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Google Machine Learning Glossary</b> Flashcards | <b>Quizlet</b>", "url": "https://quizlet.com/256349161/google-machine-learning-glossary-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/256349161/<b>google-machine-learning-glossary</b>-flash-cards", "snippet": "<b>independently</b> <b>and identically</b> <b>distributed</b> (<b>IID</b>) Data drawn from a distribution that doesn&#39;t change, and where each value drawn doesn&#39;t depend on values that have been drawn previously. An <b>i.i.d</b>. is the ideal gas of machine <b>learning</b>\u2014a useful mathematical construct but almost never exactly found in the real <b>world</b>. For example, the distribution of visitors to a web page may be <b>i.i.d</b>. over a brief window of time; that is, the distribution doesn&#39;t change during that brief window and one person ...", "dateLastCrawled": "2018-10-18T12:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Enhancement to <b>Selective Incremental Approach for Transductive</b> ...", "url": "https://www.grin.com/document/205436", "isFamilyFriendly": true, "displayUrl": "https://www.grin.com/document/205436", "snippet": "Typically it is assumed that the points are drawn <b>i.i.d</b>. (<b>independently</b> <b>and identically</b> <b>distributed</b>) from a common distribution on X. It is often convenient to define the (n \u00d7 d)-matrix [Abbildung in dieser Leseprobe nicht enthalten] that contains the data points as its rows. The goal of unsupervised <b>learning</b> [Abbildung in dieser Leseprobe nicht enthalten] is to find interesting structure in the data X. It has been argued that the problem of unsupervised <b>learning</b> is fundamentally that of ...", "dateLastCrawled": "2020-05-16T11:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Engineering a Less Artificial Intelligence: <b>Neuron</b>", "url": "https://www.cell.com/neuron/fulltext/S0896-6273(19)30740-8", "isFamilyFriendly": true, "displayUrl": "https://www.cell.com/<b>neuron</b>/fulltext/S0896-6273(19)30740-8", "snippet": "The ability to generalize beyond the standard assumption of independent <b>and identically</b> <b>distributed</b> (<b>i.i.d</b>.) samples at test time would be highly desirable for machine <b>learning</b> algorithms, as many real-<b>world</b> applications involve such shifts in the input distribution. For instance, recognition systems of autonomous driving cars should be robust against a large spectrum of weather phenomena that they might not have experienced at training time, such as ash falling from a nearby volcano. Thus ...", "dateLastCrawled": "2022-02-01T11:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "5.1 MLBasics-<b>Learning</b>.ppt - Pennsylvania State University", "url": "http://clgiles.ist.psu.edu/IST597/materials/slides/lect2/ch5.pptx", "isFamilyFriendly": true, "displayUrl": "clgiles.ist.psu.edu/IST597/materials/slides/lect2/ch5.pptx", "snippet": ")} are <b>independently</b> <b>and identically</b> <b>distributed</b> according to. p (x (i))= N (x (i);\u00b5,\u03c32) \u2013 Sample mean is an estimator of the meanparameter \u2013 To determine bias of the samplemean: \u2013 Thus the samplemeanis an unbiased estimator ofthe Gaussian mean \u02c6m. x (i) m . i 1. m 1", "dateLastCrawled": "2022-01-31T17:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "The Non-<b>IID</b> Data Quagmire of Decentralized Machine <b>Learning</b> | DeepAI", "url": "https://deepai.org/publication/the-non-iid-data-quagmire-of-decentralized-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/the-non-<b>iid</b>-data-quagmire-of-decentralized-machine-<b>learning</b>", "snippet": "Many large-scale machine <b>learning</b> (ML) applications need to train ML models over decentralized datasets that are generated at <b>different</b> devices and locations. These decentralized datasets pose a fundamental challenge to ML because they are typically generated in very <b>different</b> contexts, which leads to significant differences in data distribution across devices/locations (i.e., they are not independent <b>and identically</b> <b>distributed</b> (<b>IID</b>)).", "dateLastCrawled": "2022-01-31T23:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "DeepCOMBI: explainable artificial intelligence for the analysis and ...", "url": "https://academic.oup.com/nargab/article/3/3/lqab065/6324603", "isFamilyFriendly": true, "displayUrl": "https://academic.oup.com/nargab/article/3/3/lqab065/6324603", "snippet": "<b>learning</b> rate |$\\eta = 0.01$| with <b>learning</b> rate reduction on a plateau with factor 0.7125 after 50 epochs of no improvement and. number of epochs e = 500 [100, 500, 1000]. A few <b>different</b> parameter values of the |$\\alpha \\beta$| - backpropagation rule were manually investigated on", "dateLastCrawled": "2022-01-27T20:15:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Machine <b>Learning</b> Theory - Part 2: Generalization Bounds", "url": "https://mostafa-samir.github.io/ml-theory-pt2/", "isFamilyFriendly": true, "displayUrl": "https://mostafa-samir.github.io/ml-theory-pt2", "snippet": "<b>Independently</b>, <b>and Identically</b> <b>Distributed</b>. The <b>world</b> can be a very messy place! This is a problem that faces any theoretical analysis of a real <b>world</b> phenomenon; because usually we can\u2019t really capture all the messiness in mathematical terms, and even if we\u2019re able to; we usually don\u2019t have the tools to get any results from such a messy mathematical model. So in order for theoretical analysis to move forward, some assumptions must be made to simplify the situation at hand, we can then ...", "dateLastCrawled": "2022-02-01T09:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Machine <b>Learning</b> Strategies When Transitioning between Biological Assays", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8317157/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8317157", "snippet": "A core assumption of all ML methods is that the data used for training the model is <b>i.i.d</b>., i.e., independent <b>and identically</b> <b>distributed</b>. If data from, e.g., an old assay and a new assay stem from <b>different</b> distributions, then a model trained on pooled data from both assays might not be valid and predictions cannot necessarily be trusted. There are methods devised to detect violations against <b>i.i.d</b>., commonly called data set shifts, 15,16 but these are restricted to specific versions of ...", "dateLastCrawled": "2022-01-06T03:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Why Global Artificial Intelligence is the Next Big Thing", "url": "https://www.bbntimes.com/science/why-global-artificial-intelligence-is-the-next-big-thing", "isFamilyFriendly": true, "displayUrl": "https://www.bbntimes.com/science/why-global-artificial-intelligence-is-the-next-big-thing", "snippet": "One of the leading examples of such independent <b>and identically</b> <b>distributed</b> (<b>i.i.d</b>.) data -driven, numerical and statistical Narrow and Weak ML/AI is GPT-3 marked by 175 billion parameters/synapses, while the human brain has 86 billion neurons with 1000 trillion synapses at least.", "dateLastCrawled": "2022-01-26T08:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Machine Learning</b> - Term Paper", "url": "https://www.termpaperwarehouse.com/essay-on/Machine-Learning/309411", "isFamilyFriendly": true, "displayUrl": "https://www.termpaperwarehouse.com/essay-on/<b>Machine-Learning</b>/309411", "snippet": "We assume that examples are <b>independently</b> <b>and identically</b> <b>distributed</b> (<b>i.i.d</b>.) according to some \ufb01xed but unknown distribution D. The <b>learning</b> problem is then formulated as follows. The learner considers a \ufb01xed set of possible concepts H, called a hypothesis set, which may not coincide with C. He receives a sample S = (x1 , . . . , xm ) drawn <b>i.i.d</b>. according to D as well as the labels (c(x1 ), . . . , c(xm )), which are based on a speci\ufb01c target concept c \u2208 C to learn. His task is ...", "dateLastCrawled": "2022-02-02T19:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Midterm 2.docx - Module 11 Variable Selection 1 Intro to Variable ...", "url": "https://www.coursehero.com/file/127342731/Midterm-2docx/", "isFamilyFriendly": true, "displayUrl": "https://www.coursehero.com/file/127342731/Midterm-2docx", "snippet": "Each Bernoulli trial is independent <b>and identically</b> <b>distributed</b> (<b>i.i.d</b>) ii. Can compare data to Geometric to test whether <b>i.i.d</b> is true 3. \u019f, Exponential, Weibull 1) Poisson: random arrival: Probability distribution to model a certain number of events occurring during a fixed time interval when the events occur <b>independently</b> with a constant rate. \u028e: Average number of arrivals/time period (Arrivals independent, <b>identically</b> <b>distributed</b>) f x (x) = \u03bb x \u2147 \u2212 \u03bb x ! 2) Exponential: a ...", "dateLastCrawled": "2022-01-27T14:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Enhancement to <b>Selective Incremental Approach for Transductive</b> ...", "url": "https://www.grin.com/document/205436", "isFamilyFriendly": true, "displayUrl": "https://www.grin.com/document/205436", "snippet": "Typically it is assumed that the points are drawn <b>i.i.d</b>. (<b>independently</b> <b>and identically</b> <b>distributed</b>) from a common distribution on X. It is often convenient to define the (n \u00d7 d)-matrix [Abbildung in dieser Leseprobe nicht enthalten] that contains the data points as its rows. The goal of unsupervised <b>learning</b> [Abbildung in dieser Leseprobe nicht enthalten] is to find interesting structure in the data X. It has been argued that the problem of unsupervised <b>learning</b> is fundamentally that of ...", "dateLastCrawled": "2020-05-16T11:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "The Non-<b>IID</b> Data Quagmire of Decentralized Machine <b>Learning</b> | DeepAI", "url": "https://deepai.org/publication/the-non-iid-data-quagmire-of-decentralized-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/the-non-<b>iid</b>-data-quagmire-of-decentralized-machine-<b>learning</b>", "snippet": "Many large-scale machine <b>learning</b> (ML) applications need to train ML models over decentralized datasets that are generated at <b>different</b> devices and locations. These decentralized datasets pose a fundamental challenge to ML because they are typically generated in very <b>different</b> contexts, which leads to significant differences in data distribution across devices/locations (i.e., they are not independent <b>and identically</b> <b>distributed</b> (<b>IID</b>)).", "dateLastCrawled": "2022-01-31T23:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Proceedings of the 2019 Conference on Empirical Methods in Natural ...", "url": "https://aclanthology.org/volumes/D19-1/", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/volumes/D19-1", "snippet": "The hope is that active sampling leads to better performance than would be achieved under independent <b>and identically</b> <b>distributed</b> (<b>i.i.d</b>.) random samples. While AL has shown promise in retrospective evaluations, these studies often ignore practical obstacles to its use. In this paper, we show that while AL may provide benefits when used with specific models and for particular domains, the benefits of current approaches do not generalize reliably across models and tasks. This is problematic ...", "dateLastCrawled": "2022-01-31T01:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "DeepCOMBI: explainable artificial intelligence for the analysis and ...", "url": "https://academic.oup.com/nargab/article/3/3/lqab065/6324603", "isFamilyFriendly": true, "displayUrl": "https://academic.oup.com/nargab/article/3/3/lqab065/6324603", "snippet": "<b>learning</b> rate |$\\eta = 0.01$| with <b>learning</b> rate reduction on a plateau with factor 0.7125 after 50 epochs of no improvement and. number of epochs e = 500 [100, 500, 1000]. A few <b>different</b> parameter values of the |$\\alpha \\beta$| - backpropagation rule were manually investigated on", "dateLastCrawled": "2022-01-27T20:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Learning from positive</b> and unlabeled data: a survey | SpringerLink", "url": "https://link.springer.com/article/10.1007/s10994-020-05877-5", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10994-020-05877-5", "snippet": "<b>Learning from positive</b> and unlabeled data or PU <b>learning</b> is the setting where a learner only has access to positive examples and unlabeled data. The assumption is that the unlabeled data can contain both positive and negative examples. This setting has attracted increasing interest within the machine <b>learning</b> literature as this type of data naturally arises in applications such as medical diagnosis and knowledge base completion. This article provides a survey of the current state of the art ...", "dateLastCrawled": "2022-02-02T03:04:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Crystal Clear <b>Reinforcement</b> <b>Learning</b> | by Baijayanta Roy | Towards Data ...", "url": "https://towardsdatascience.com/crystal-clear-reinforcement-learning-7e6c1541365e", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/crystal-clear-<b>reinforcement</b>-<b>learning</b>-7e6c1541365e", "snippet": "As we randomly sample from the replay buffer, the data is more independent of each other and closer to <b>i.i.d</b> (independent <b>and identically</b> <b>distributed</b>). This makes raining stable. Target network: We create two deep networks \u03b8- (Target Network) and \u03b8 (Q-network). We use the first one to retrieve Q values while the second one includes all updates in training. After say 50,000 updates, we synchronize \u03b8- with \u03b8. The purpose is to fix the Q-value targets temporarily, so we don\u2019t have a ...", "dateLastCrawled": "2022-01-29T12:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Junbin GAO</b> | Professor | PhD | The University of Sydney, Sydney | The ...", "url": "https://www.researchgate.net/profile/Junbin-Gao", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/profile/<b>Junbin-Gao</b>", "snippet": "Linear Discriminant Analysis (LDA) assumes that all samples from the same class are <b>independently</b> <b>and identically</b> <b>distributed</b> (<b>i.i.d</b>.). LDA may fail in the cases where the assumption does not hold ...", "dateLastCrawled": "2022-01-24T19:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Enhancement to <b>Selective Incremental Approach for Transductive</b> ...", "url": "https://www.grin.com/document/205436", "isFamilyFriendly": true, "displayUrl": "https://www.grin.com/document/205436", "snippet": "Typically it is assumed that the points are drawn <b>i.i.d</b>. (<b>independently</b> <b>and identically</b> <b>distributed</b>) from a common distribution on X. It is often convenient to define the (n \u00d7 d)-matrix [Abbildung in dieser Leseprobe nicht enthalten] that contains the data points as its rows. The goal of unsupervised <b>learning</b> [Abbildung in dieser Leseprobe nicht enthalten] is to find interesting structure in the data X. It has been argued that the problem of unsupervised <b>learning</b> is fundamentally that of ...", "dateLastCrawled": "2020-05-16T11:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Dimensionality Estimation for Optimal Detection of Functional Networks ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3052418/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC3052418", "snippet": "This problem may be reduced by running the analytical methods on a small sub-sample of the data that is constructed so the observations are independent <b>and identically</b> <b>distributed</b> (<b>i.i.d</b>.; Li et al., 2007). However, reducing the total sample size is likely to reduce the accuracy of the resulting estimates making this technique applicable only when there is a strong, well-defined signal with many more than the minimal number of observations required to detect that signal. For the technique ...", "dateLastCrawled": "2022-01-03T02:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "What if I apply deep <b>learning</b> to non <b>i.i.d</b> data, like medical images ...", "url": "https://www.quora.com/What-if-I-apply-deep-learning-to-non-i-i-d-data-like-medical-images-collected-from-different-hospitals", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-if-I-apply-deep-<b>learning</b>-to-non-<b>i-i-d</b>-data-like-medical...", "snippet": "Answer (1 of 2): <b>IID</b> stands for <b>Independently</b> <b>and Identically</b> <b>Distributed</b> So, when you say non-<b>IID</b>, it <b>can</b> mean 1. Non-independent, but <b>identically</b> <b>distributed</b> 2. Independent, but non-<b>identically</b> <b>distributed</b> 3. Non-independent, non-<b>identically</b> <b>distributed</b> I believe that in case of medical data ...", "dateLastCrawled": "2022-01-18T20:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Week8-L1.pdf - Graph Anomaly Detection COMP90073 Security Analytics ...", "url": "https://www.coursehero.com/file/74294367/Week8-L1pdf/", "isFamilyFriendly": true, "displayUrl": "https://www.coursehero.com/file/74294367/Week8-L1pdf", "snippet": "Most relational data <b>can</b> <b>be thought</b> of as inter-dependent, ... While in traditional anomaly detection, the objects or data points are treated as independent <b>and identically</b> <b>distributed</b> (<b>i.i.d</b>.) from each other, the objects in graph data have long-range correlations. \u2022 Variety of Definitions: The definitions of anomalies in graphs are much more diverse than in traditional anomaly detection, given the rich representation of graphs. \u2022 Size of Search Space: The enumeration of possible ...", "dateLastCrawled": "2021-12-24T23:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A survey and critique of multiagent deep <b>reinforcement learning</b> ...", "url": "https://link.springer.com/article/10.1007/s10458-019-09421-1", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10458-019-09421-1", "snippet": "Second, deep <b>learning</b> <b>can</b> be used to reduce (or eliminate) the need for manually designing features to represent state information [184, 281]. However, extending deep <b>learning</b> to RL problems comes with additional challenges including non-<b>i.i.d</b>. (not <b>independently</b> <b>and identically</b> <b>distributed</b>) data.", "dateLastCrawled": "2022-01-29T11:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>TYBSC CS SEM 5 AI NOTES</b> - SlideShare", "url": "https://www.slideshare.net/SiddheshZele/tybsc-cs-sem-5-ai-notes", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/SiddheshZele/<b>tybsc-cs-sem-5-ai-notes</b>", "snippet": "Online <b>Learning</b> \u2022 what to do when the data are not <b>i.i.d</b>. (independent <b>and identically</b> <b>distributed</b>); when they <b>can</b> change over time. In this case, it matters when we make a prediction, so we will adopt the perspective called online <b>learning</b>: \u2022 an agent receives an input xj from nature, predicts the corresponding yj , and then is told the correct answer. Then the process repeats with xj+1, and so on. \u2022 One might think this task is hopeless\u2014if nature is adversarial, all the predictions ...", "dateLastCrawled": "2022-01-26T09:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Statistical Mechanics of Deep <b>Learning</b> | Annual Review of Condensed ...", "url": "https://www.annualreviews.org/doi/10.1146/annurev-conmatphys-031119-050745", "isFamilyFriendly": true, "displayUrl": "https://www.annualreviews.org/doi/10.1146/annurev-conmatphys-031119-050745", "snippet": "A key idea in training deep networks involves moving the weights and, consequently, the activations in each layer l so as to move the output in the final layer in a desired direction. A fundamental linear operator determining how we should change to move is the susceptibility matrix or Jacobian .This Jacobian is an important component of the back propagation of errors at the outputs to weights at layer l.For randomly initialized networks, this Jacobian is a random matrix, and its spectral ...", "dateLastCrawled": "2022-02-02T06:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Statistics And Probability Archive | November 10, 2015 | <b>Chegg</b>.com", "url": "https://www.chegg.com/homework-help/questions-and-answers/statistics-and-probability-archive-2015-november-10", "isFamilyFriendly": true, "displayUrl": "https://www.<b>chegg</b>.com/homework-help/questions-and-answers/statistics-and-probability...", "snippet": "The concentration of active ingredient in a liquid laundrydetergent is <b>thought</b> to be affected by the type of catalyst used inthe process. The standard deviation of active concentrationis known to be 3 . 3 answers A coffee shop makes and dispenses hot beverages and orders arrive according to a Poisson process at a rate of 10 per hour. After placing an order, the customer immediately gets his or her drink, and t. 1 answer A random group of thirty customers at a local theater was interviewed ...", "dateLastCrawled": "2022-02-02T14:09:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Neighborhood linear discriminant analysis - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S0031320321005987", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0031320321005987", "snippet": "It assumes that the samples from the same class are independent <b>and identically</b> <b>distributed</b> (<b>i.i.d</b>.) in linear discriminant analysis (LDA). When a class in a dataset contains several clusters or subclasses, LDA will perform poorly on this dataset. This weakness stems from the fact that the scatter matrices in LDA are defined at the whole class level. In this paper, we define the scatter matrices on neighborhood instead. The neighborhood consists of reverse nearest neighbors, which <b>can</b> ...", "dateLastCrawled": "2022-01-26T22:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Review of recent advances in dealing with data size challenges in Deep ...", "url": "https://suneeta-mall.github.io/2021/12/31/data-in-deep-learning.html", "isFamilyFriendly": true, "displayUrl": "https://suneeta-mall.github.io/2021/12/31/data-in-deep-<b>learning</b>.html", "snippet": "Transfer <b>learning</b> relaxes the assumption that the training data must be independent <b>and identically</b> <b>distributed</b> (<b>i.i.d</b>.) ... If we look at humans are <b>learning</b> <b>machines</b>, they have infinite data at their disposal to learn from. Our system had evolved into efficient strategies to parse through infinite data streams to select the samples we are interested in. How our vision system performs foveal fixation utilizing saccadic eye movements to conduct efficient subsampling of interesting and useful ...", "dateLastCrawled": "2022-02-01T05:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Machine <b>Learning</b> Strategies When Transitioning between Biological Assays", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8317157/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8317157", "snippet": "A core assumption of all ML methods is that the data used for training the model is <b>i.i.d</b>., i.e., independent <b>and identically</b> <b>distributed</b>. If data from, e.g., an old assay and a new assay stem from <b>different</b> distributions, then a model trained on pooled data from both assays might not be valid and predictions cannot necessarily be trusted. There are methods devised to detect violations against <b>i.i.d</b>., commonly called data set shifts, 15,16 but these are restricted to specific versions of ...", "dateLastCrawled": "2022-01-06T03:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Recent advances in dealing with data size challenges in Deep <b>Learning</b> ...", "url": "https://towardsdatascience.com/review-of-recent-advances-in-dealing-with-data-size-challenges-in-deep-learning-ac5c1844af73", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/review-of-recent-advances-in-dealing-with-data-size...", "snippet": "Transfer <b>learning</b> relaxes the assumption that the training data must be independent <b>and identically</b> <b>distributed</b> (<b>i.i.d</b>.) ... If we look at humans are <b>learning</b> <b>machines</b>, they have infinite data at their disposal to learn from. Our system had evolved into efficient strategies to parse through infinite data streams to select the samples we are interested in. How our vision system performs foveal fixation utilizing saccadic eye movements to conduct efficient subsampling of interesting and useful ...", "dateLastCrawled": "2022-02-03T08:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Engineering a Less Artificial Intelligence: <b>Neuron</b>", "url": "https://www.cell.com/neuron/fulltext/S0896-6273(19)30740-8", "isFamilyFriendly": true, "displayUrl": "https://www.cell.com/<b>neuron</b>/fulltext/S0896-6273(19)30740-8", "snippet": "The ability to generalize beyond the standard assumption of independent <b>and identically</b> <b>distributed</b> (<b>i.i.d</b>.) samples at test time would be highly desirable for machine <b>learning</b> algorithms, as many real-<b>world</b> applications involve such shifts in the input distribution. For instance, recognition systems of autonomous driving cars should be robust against a large spectrum of weather phenomena that they might not have experienced at training time, such as ash falling from a nearby volcano. Thus ...", "dateLastCrawled": "2022-02-01T11:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Machine Learning</b> - Term Paper", "url": "https://www.termpaperwarehouse.com/essay-on/Machine-Learning/309411", "isFamilyFriendly": true, "displayUrl": "https://www.termpaperwarehouse.com/essay-on/<b>Machine-Learning</b>/309411", "snippet": "We assume that examples are <b>independently</b> <b>and identically</b> <b>distributed</b> (<b>i.i.d</b>.) according to some \ufb01xed but unknown distribution D. The <b>learning</b> problem is then formulated as follows. The learner considers a \ufb01xed set of possible concepts H, called a hypothesis set, which may not coincide with C. He receives a sample S = (x1 , . . . , xm ) drawn <b>i.i.d</b>. according to D as well as the labels (c(x1 ), . . . , c(xm )), which are based on a speci\ufb01c target concept c \u2208 C to learn. His task is ...", "dateLastCrawled": "2022-02-02T19:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Enhancement to <b>Selective Incremental Approach for Transductive</b> ...", "url": "https://www.grin.com/document/205436", "isFamilyFriendly": true, "displayUrl": "https://www.grin.com/document/205436", "snippet": "Typically it is assumed that the points are drawn <b>i.i.d</b>. (<b>independently</b> <b>and identically</b> <b>distributed</b>) from a common distribution on X. It is often convenient to define the (n \u00d7 d)-matrix [Abbildung in dieser Leseprobe nicht enthalten] that contains the data points as its rows. The goal of unsupervised <b>learning</b> [Abbildung in dieser Leseprobe nicht enthalten] is to find interesting structure in the data X. It has been argued that the problem of unsupervised <b>learning</b> is fundamentally that of ...", "dateLastCrawled": "2020-05-16T11:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Midterm 2.docx - Module 11 Variable Selection 1 Intro to Variable ...", "url": "https://www.coursehero.com/file/127342731/Midterm-2docx/", "isFamilyFriendly": true, "displayUrl": "https://www.coursehero.com/file/127342731/Midterm-2docx", "snippet": "Each Bernoulli trial is independent <b>and identically</b> <b>distributed</b> (<b>i.i.d</b>) ii. <b>Can</b> compare data to Geometric to test whether <b>i.i.d</b> is true 3. \u019f, Exponential, Weibull 1) Poisson: random arrival: Probability distribution to model a certain number of events occurring during a fixed time interval when the events occur <b>independently</b> with a constant rate. \u028e: Average number of arrivals/time period (Arrivals independent, <b>identically</b> <b>distributed</b>) f x (x) = \u03bb x \u2147 \u2212 \u03bb x ! 2) Exponential: a ...", "dateLastCrawled": "2022-01-27T14:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>The Generalization Performance of Regularized Regression Algorithms</b> ...", "url": "https://www.researchgate.net/publication/258252903_The_Generalization_Performance_of_Regularized_Regression_Algorithms_Based_on_Markov_Sampling", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/258252903_The_Generalization_Performance_of...", "snippet": "Previous known results for the <b>learning</b> with \u2113\u00b9-regularizer are based on the assumption that samples are independent <b>and identically</b> <b>distributed</b> (<b>i.i.d</b>.), and the best obtained <b>learning</b> rate ...", "dateLastCrawled": "2021-12-24T20:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "DeepCOMBI: explainable artificial intelligence for the analysis and ...", "url": "https://academic.oup.com/nargab/article/3/3/lqab065/6324603", "isFamilyFriendly": true, "displayUrl": "https://academic.oup.com/nargab/article/3/3/lqab065/6324603", "snippet": "It refers to techniques, which open the so-called \u2018black box\u2019 of machine <b>learning</b> methods and reveal the processes underlying their decisions so that the results <b>can</b> be better understood. The explanation method used by Romagnoni et al. ( 28 )\u2014permutation feature importance (PFI)\u2014is a generalized, model-agnostic approach and more sophisticated methods specifically tailored to DNN are available.", "dateLastCrawled": "2022-01-27T20:15:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "What is <b>Learning</b>? <b>Machine</b> <b>Learning</b>: Introduction and Unsupervised <b>Learning</b>", "url": "https://pages.cs.wisc.edu/~dyer/cs540/notes/08_learning-intro.pdf", "isFamilyFriendly": true, "displayUrl": "https://pages.cs.wisc.edu/~dyer/cs540/notes/08_<b>learning</b>-intro.pdf", "snippet": "the inputto the <b>learning</b> process \u2022x i=(x i1, . . . , x iD) \u2022Assume these instances are all sampled independentlyfrom the same, unknown (population) distribution, P(x) \u2022We denote this by x i\u223cP(x), where <b>i.i.d</b>. stands for independent <b>and identically</b> <b>distributed</b> \u2022Example: Repeated throws of dice <b>i.i.d</b>. 13", "dateLastCrawled": "2022-02-03T16:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "What is <b>Learning</b>? <b>Machine Learning: Introduction and Unsupervised Learning</b>", "url": "http://pages.cs.wisc.edu/~bgibson/cs540/handouts/learning_intro.pdf", "isFamilyFriendly": true, "displayUrl": "pages.cs.wisc.edu/~bgibson/cs540/handouts/<b>learning</b>_intro.pdf", "snippet": "<b>learning</b> process \u2022x i = (x i1, . . . , x iD) \u2022Assume these instances are sampled <b>independently</b> from an unknown (population) distribution, P(x) \u2022We denote this by x i \u223c P(x), where <b>i.i.d</b>. stands for independent <b>and identically</b> <b>distributed</b> <b>i.i.d</b>. Training Sample \u2022A training sample is the \u201cexperience\u201d given to a <b>learning</b> algorithm", "dateLastCrawled": "2021-08-25T13:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "2 - The Process of <b>Learning</b>.pdf - CMPSC 448 <b>Machine</b> <b>Learning</b> Lecture 2 ...", "url": "https://www.coursehero.com/file/113918059/2-The-Process-of-Learningpdf/", "isFamilyFriendly": true, "displayUrl": "https://www.coursehero.com/file/113918059/2-The-Process-of-<b>Learning</b>pdf", "snippet": "<b>I.I.D</b> assumption Training/test data is independent <b>and identically</b> <b>distributed</b> (<b>i.i.d</b>) if: All objects come from the same distribution (<b>identically</b> <b>distributed</b>). The object are sampled <b>independently</b> (order doesn\u2019t matter). We do NOT need to know the underlying distribution as long as the samples are sampled <b>i.i.d</b>. Examples in terms of cards: Pick a card, put it back in the deck, re-shuffle, repeat. Pick a card, put it back in the deck, repeat.", "dateLastCrawled": "2021-12-30T01:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Learning</b> from Examples as an <b>Inverse Problem</b> - Journal of <b>Machine</b> ...", "url": "https://jmlr.csail.mit.edu/papers/volume6/devito05a/devito05a.pdf", "isFamilyFriendly": true, "displayUrl": "https://jmlr.csail.mit.edu/papers/volume6/devito05a/devito05a.pdf", "snippet": "<b>and identically</b> <b>distributed</b> (<b>i.i.d</b>.) according to \u03c1. Given the sample z, the aim of <b>learning</b> theory is to \ufb01nd a function fz: X \u2192R such that fz(x) is a good estimate of the output y when a new input x is given. The function fz is called estimator and the map providing fz, for any training set z, is called <b>learning</b> algorithm.", "dateLastCrawled": "2021-09-19T04:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "11._Intro_to_<b>Machine</b>_<b>Learning</b>.pdf - CMPSC 442 Artificial Intelligence ...", "url": "https://www.coursehero.com/file/121916721/11-Intro-to-Machine-Learningpdf/", "isFamilyFriendly": true, "displayUrl": "https://www.coursehero.com/file/121916721/11-Intro-to-<b>Machine</b>-<b>Learning</b>pdf", "snippet": "<b>I.I.D</b> assumption Training/test data is independent <b>and identically</b> <b>distributed</b> (<b>i.i.d</b>) if: All objects come from the same distribution (<b>identically</b> <b>distributed</b>). The object are sampled <b>independently</b> (order doesn\u2019t matter). We do NOT need to know the underlying distribution as long as the samples are sampled <b>i.i.d</b>. Examples in terms of cards: Pick a card, put it back in the deck, re-shuffle, repeat. Pick a card, put it back in the deck, repeat. Pick a card, don\u2019t put it back, re-shuffle ...", "dateLastCrawled": "2022-01-15T13:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Background on <b>machine</b> <b>learning</b> and <b>learning</b> theory", "url": "https://matthewhirn.files.wordpress.com/2020/02/cmse890_spring2020_chapter1.pdf", "isFamilyFriendly": true, "displayUrl": "https://matthewhirn.files.wordpress.com/2020/02/cmse890_spring2020_chapter1.pdf", "snippet": "Often we will assume that the #i are <b>independently</b> <b>and identically</b> <b>distributed</b> (<b>i.i.d</b>.) according to the normal distribution with mean zero and variance s2, i.e. #i \u21e0N(0,s2). In this case, if X is the random variable that takes values in Rd according to the probability distribution PX, and Y is the random variable that takes values in R ...", "dateLastCrawled": "2021-08-12T13:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "8. Recurrent Neural Networks \u2014 Dive into <b>Deep Learning</b> 0.17.2 documentation", "url": "https://d2l.ai/chapter_recurrent-neural-networks/index.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_recurrent-neural-networks/index.html", "snippet": "Most importantly, so far we tacitly assumed that our data are all drawn from some distribution, and all the examples are <b>independently</b> <b>and identically</b> <b>distributed</b> (<b>i.i.d</b>.). Unfortunately, this is not true for most data. For instance, the words in this paragraph are written in sequence, and it would be quite difficult to decipher its meaning if they were permuted randomly. Likewise, image frames in a video, the audio signal in a conversation, and the browsing behavior on a website, all follow ...", "dateLastCrawled": "2022-02-03T01:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>The Essence of RNNs</b>. The intuition behind the building\u2026 | by Taha ...", "url": "https://towardsdatascience.com/the-essence-of-rnns-44dfb4107a47", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>the-essence-of-rnns</b>-44dfb4107a47", "snippet": "When considering CNNs and MLPs, we always assumed that the data was sampled from and <b>independently</b> <b>and identically</b> <b>distributed</b> data(<b>i.i.d</b>), but with sequential data, that is not the case. Contrary to (<b>i.i.d</b>) data, the previous input points affect the outcome of the next output. Since RNNs are most widely used in natural language processing(NLP), an <b>analogy</b> from that field would suffice to make the point clear. Imagine textual data, all the words in a sequence affect the outcome of the ...", "dateLastCrawled": "2022-01-23T05:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Assignment 1</b> - Department of Computer Science and Electrical Engineering", "url": "https://www.csee.umbc.edu/courses/undergraduate/473/f19/content/materials/a1.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.csee.umbc.edu/courses/undergraduate/473/f19/content/materials/a1.pdf", "snippet": "i is an <b>i.i.d</b>. sample, where <b>i.i.d</b>. means \u201c<b>independently</b> <b>and identically</b> <b>distributed</b>. ... Using a programming <b>analogy</b>, we can say that word types are like classes while word tokens are like instances of that class. For example, in the following sentence there are six types and eight tokens: the gray cat chased the tabby cat . Notice that this computation includes punctuation. (b)In the training \ufb01le, how many different word types and tokens are there? Do not perform any processing that ...", "dateLastCrawled": "2022-02-02T03:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Learning from positive</b> and unlabeled data: a survey - <b>Machine</b> <b>Learning</b>", "url": "https://link.springer.com/article/10.1007/s10994-020-05877-5", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10994-020-05877-5", "snippet": "<b>Learning from positive</b> and unlabeled data or PU <b>learning</b> is the setting where a learner only has access to positive examples and unlabeled data. The assumption is that the unlabeled data can contain both positive and negative examples. This setting has attracted increasing interest within the <b>machine</b> <b>learning</b> literature as this type of data naturally arises in applications such as medical diagnosis and knowledge base completion. This article provides a survey of the current state of the art ...", "dateLastCrawled": "2022-02-02T03:04:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(independently and identically distributed (i.i.d))  is like +(machines learning in different parts of the world)", "+(independently and identically distributed (i.i.d)) is similar to +(machines learning in different parts of the world)", "+(independently and identically distributed (i.i.d)) can be thought of as +(machines learning in different parts of the world)", "+(independently and identically distributed (i.i.d)) can be compared to +(machines learning in different parts of the world)", "machine learning +(independently and identically distributed (i.i.d) AND analogy)", "machine learning +(\"independently and identically distributed (i.i.d) is like\")", "machine learning +(\"independently and identically distributed (i.i.d) is similar\")", "machine learning +(\"just as independently and identically distributed (i.i.d)\")", "machine learning +(\"independently and identically distributed (i.i.d) can be thought of as\")", "machine learning +(\"independently and identically distributed (i.i.d) can be compared to\")"]}