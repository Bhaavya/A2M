{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Loss Functions in Machine Learning</b> | Working | Different Types", "url": "https://www.educba.com/loss-functions-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.educba.com/<b>loss-functions-in-machine-learning</b>", "snippet": "MSE(<b>L2</b> error) measures the average squared <b>difference</b> <b>between</b> the <b>actual</b> and <b>predicted</b> values by the model. The output is a single number associated with a set of values. Our aim is to reduce MSE to improve the accuracy of the model.", "dateLastCrawled": "2022-02-03T02:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Loss</b>/<b>Cost Functions Every Data Scientist should</b> know about :: InBlog", "url": "https://inblog.in/Loss-Cost-Functions-Every-Data-Scientist-should-know-about-KE4roUkypE", "isFamilyFriendly": true, "displayUrl": "https://inblog.in/<b>Loss</b>-<b>Cost-Functions-Every-Data-Scientist-should</b>-know-about-KE4roUkypE", "snippet": "In <b>L2</b> <b>loss</b>, instead of taking absolute of <b>difference</b> <b>between</b> true output and <b>predicted</b> output, it is squared. Hence we get a positive <b>value</b> and also, the <b>loss</b> is magnified. It <b>is like</b> looking at a <b>loss</b> through lenses. So learning will be faster in case of <b>L2</b> <b>loss</b>.", "dateLastCrawled": "2022-01-25T23:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Loss functions</b> - GitHub Pages", "url": "https://kharshit.github.io/blog/2018/08/24/loss-functions", "isFamilyFriendly": true, "displayUrl": "https://kharshit.github.io/blog/2018/08/24/<b>loss-functions</b>", "snippet": "It minimizes the squared <b>difference</b> <b>between</b> the <b>predicted</b> <b>value</b> and the <b>actual</b> <b>value</b>. The residual sum of squares is defined as: The residual sum of squares is defined as: \\[RSS = \\sum_{i=1}^n(y_{i} - \\hat{y_i})^2\\]", "dateLastCrawled": "2022-01-25T18:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Loss</b> Functions| Cost Functions in Machine Learning - Vidyasheela", "url": "https://vidyasheela.com/post/loss-functions-cost-functions-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://vidyasheela.com/post/<b>loss</b>-functions-cost-functions-in-machine-learning", "snippet": "Mean Square Error/Quadratic <b>Loss</b>/<b>L2</b> <b>Loss</b> (MSE): It is given as the average squared <b>difference</b> <b>between</b> the <b>actual</b> <b>value</b> and <b>predicted</b> <b>value</b> by the learning model in regression. Mathematically it is given as, Where T is true <b>value</b> i.e. <b>actual</b> <b>value</b> and Y <b>predicted</b> <b>value</b>. The optimization of MSE is done by using a gradient descent algorithm. It is more sensitive to outliers (as it includes squared <b>difference</b>) than MAE. Mean Absolute Error/L1 <b>Loss</b> (MAE): It is given as the average of the ...", "dateLastCrawled": "2022-01-31T14:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "The Professionals Point: <b>Loss Functions in Machine Learning (MAE</b>, MSE ...", "url": "https://theprofessionalspoint.blogspot.com/2019/02/loss-functions-in-machine-learning-mae.html", "isFamilyFriendly": true, "displayUrl": "https://theprofessionalspoint.blogspot.com/2019/02/<b>loss-functions-in-machine-learning</b>...", "snippet": "<b>Loss</b> Function indicates the <b>difference</b> <b>between</b> the <b>actual</b> <b>value</b> and the <b>predicted</b> <b>value</b>. If the magnitude of the <b>loss</b> function is high, it means our algorithm is showing a lot of variance in the result and needs to be corrected. Lets look into the types of <b>loss functions in Machine Learning</b> in detail. There are broadly two types of losses based on the type of algorithm we are using: Types of Losses: 1. Regression Losses. 2. Classification Losses. Lets first discuss regression losses: 1. Mean ...", "dateLastCrawled": "2022-01-29T00:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Visualization of Loss Functions for Deep Learning with Tensorflow</b> | by ...", "url": "https://medium.com/@risingdeveloper/visualization-of-some-loss-functions-for-deep-learning-with-tensorflow-9f60be9d09f9", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@risingdeveloper/visualization-of-some-<b>loss</b>-functions-for-deep...", "snippet": "<b>L2</b> Norm <b>loss</b>/ Euclidean <b>loss</b> function; This is j u st the square of the <b>difference</b>/distance <b>between</b> the <b>predicted</b> <b>value</b> and the true <b>value</b>. The <b>L2</b> norm <b>loss</b> is good because it is curved or seem to ...", "dateLastCrawled": "2022-01-26T23:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Loss</b> Function - Rojina Panta", "url": "https://rojina99.github.io/loss_function/", "isFamilyFriendly": true, "displayUrl": "https://rojina99.github.io/<b>loss</b>_function", "snippet": "If the <b>difference</b> <b>between</b> <b>actual</b> <b>value</b> and <b>predicted</b> <b>value</b> is small the cross entropy <b>loss</b> will be small else the <b>loss</b> will increase logarithimically. Binary classification problen has output in the format of true or false, 0 or 1. So, sigmoid activation function is implemented at final layer. First formula can be implemented for multi class classification problem with softmax as classifier in final layer. In this kind of problem <b>actual</b> output will be in the format [0 0 1 0 0] and <b>predicted</b> ...", "dateLastCrawled": "2021-11-06T22:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "How <b>To Build Custom Loss Functions In Keras</b> For Any Use Case | cnvrg.io", "url": "https://cnvrg.io/keras-custom-loss-functions/", "isFamilyFriendly": true, "displayUrl": "https://cnvrg.io/keras-custom-<b>loss</b>-functions", "snippet": "Mean Absolute error, also known as L1 Error, is defined as the average of the absolute differences <b>between</b> the <b>actual</b> <b>value</b> and the <b>predicted</b> <b>value</b>. This is the average of the absolute <b>difference</b> <b>between</b> the <b>predicted</b> and the <b>actual</b> <b>value</b>. Mathematically, it can be shown as:", "dateLastCrawled": "2022-01-31T10:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "regression - What does it mean L1 <b>loss</b> is not differentiable? - Cross ...", "url": "https://stats.stackexchange.com/questions/429720/what-does-it-mean-l1-loss-is-not-differentiable", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/429720/what-does-it-mean-l1-<b>loss</b>-is-not...", "snippet": "L 1 <b>loss</b> uses the absolute <b>value</b> of the <b>difference</b> <b>between</b> the <b>predicted</b> and the <b>actual</b> <b>value</b> to measure the <b>loss</b> (or the error) made by the model. The absolute <b>value</b> (or the modulus function), i.e. f ( x) = | x | is not differentiable is the way of saying that its derivative is not defined for its whole domain.", "dateLastCrawled": "2022-01-25T07:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Mean <b>Square Error &amp; R2 Score Clearly Explained</b> \u2013 BMC Software | Blogs", "url": "https://www.bmc.com/blogs/mean-squared-error-r2-and-variance-in-regression-analysis/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.bmc.com</b>/blogs/mean-squared-error-r2-and-variance-in-regression-analysis", "snippet": "The goal is to have a <b>value</b> that is low. What low means is quantified by the r2 score (explained below). In the code below, this is np.var(err), where err is an array of the differences <b>between</b> observed and <b>predicted</b> values and np.var() is the numpy array variance function. What is r2 score? The r2 score varies <b>between</b> 0 and 100%", "dateLastCrawled": "2022-02-03T07:29:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Loss Function</b> - Pipline", "url": "https://nuoxu2016.gitbook.io/pipline/preprocessing-1/loss-function", "isFamilyFriendly": true, "displayUrl": "https://nuoxu2016.gitbook.io/pipline/preprocessing-1/<b>loss-function</b>", "snippet": "The distinction is the <b>difference</b> <b>between</b> <b>predicted</b> <b>and actual</b> probability. This adds data about information <b>loss</b> in the model training. The farther away the <b>predicted</b> probability distribution is from the true probability distribution, greater is the <b>loss</b>. It does not penalize the model based on the confidence of prediction, as in cross entropy <b>loss</b>, but how different is the prediction from ground truth. It usually outperforms mean square error, especially when data is not normally ...", "dateLastCrawled": "2022-01-24T22:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Loss Functions and Optimization Algorithms</b> - XpertUp", "url": "https://www.xpertup.com/blog/machine-learning/loss-functions-and-optimization-algorithms/", "isFamilyFriendly": true, "displayUrl": "https://www.xpertup.com/blog/machine-learning/<b>loss-functions-and-optimization-algorithms</b>", "snippet": "It is the square of the <b>difference</b> <b>between</b> the <b>actual</b> and the <b>predicted</b> values calculated for each training sample and also known as <b>L2</b> <b>loss</b>. This cost function less robust to outliers This cost function less robust to outliers", "dateLastCrawled": "2022-01-09T08:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A Brief Overview of <b>Loss</b> Functions in <b>Pytorch</b> | by Pratyaksha Jha ...", "url": "https://medium.com/udacity-pytorch-challengers/a-brief-overview-of-loss-functions-in-pytorch-c0ddb78068f7", "isFamilyFriendly": true, "displayUrl": "https://medium.com/udacity-<b>pytorch</b>-challengers/a-brief-overview-of-<b>loss</b>-functions-in-p...", "snippet": "The distinction is the <b>difference</b> <b>between</b> <b>predicted</b> <b>and actual</b> probability. This adds data about information <b>loss</b> in the model training. The farther away the <b>predicted</b> probability distribution is ...", "dateLastCrawled": "2022-02-02T14:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Quickly Master L1 vs <b>L2</b> Regularization - ML Interview Q&amp;A", "url": "https://analyticsarora.com/quickly-master-l1-vs-l2-regularization-ml-interview-qa/", "isFamilyFriendly": true, "displayUrl": "https://analyticsarora.com/quickly-master-l1-vs-<b>l2</b>-regularization-ml-interview-qa", "snippet": "The <b>loss</b> function is used to measure the <b>difference</b> <b>between</b> the <b>actual</b> <b>value</b> and the <b>predicted</b> output. The coefficients are chosen in such a way that they minimize the <b>loss</b> function subject to the training data. Suppose there is noise in the training data. In that case, the coefficients will not be able to generalize well to the future data, meaning that the model cannot correctly predict the output or target column for test data. By noise, we mean irrelevant information or randomness in a ...", "dateLastCrawled": "2022-01-23T14:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Loss</b> Functions. <b>Loss</b> functions explanations and\u2026 | by Tomer Kordonsky ...", "url": "https://medium.com/artificialis/loss-functions-361b2ad439a0", "isFamilyFriendly": true, "displayUrl": "https://medium.com/artificialis/<b>loss</b>-functions-361b2ad439a0", "snippet": "Graph for -log(x) This is pretty simple, the more your input increases, the more output goes lower. If you have a small input(x=0.5) so the output is going to be high(y=0.305).", "dateLastCrawled": "2022-02-03T02:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "LossFunctions in Deep Learning-DeepVidhya", "url": "https://deepvidhya.com/blog/lossfunctions-in-deep-learning-1205", "isFamilyFriendly": true, "displayUrl": "https://deepvidhya.com/blog/<b>loss</b>functions-in-deep-learning-1205", "snippet": "The above is mainly meant to say that the cross-entropy <b>loss</b> is mainly applicable to binary classification problems. The <b>predicted</b> <b>value</b> is a probability <b>value</b> and the <b>loss</b> is defined according to the cross-entropy. Note the <b>value</b> range of the above <b>value</b>: the <b>predicted</b> <b>value</b> of y should be a probability and the <b>value</b> range are [0,1].", "dateLastCrawled": "2022-01-28T23:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Loss</b> and <b>Loss</b> <b>Functions for Training Deep Learning Neural Networks</b>", "url": "https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>loss</b>-and-<b>loss</b>-<b>functions-for-training-deep-learning</b>...", "snippet": "I used dL/dAL= 2*(AL-Y) as the derivative of the <b>loss</b> function w.r.t the <b>predicted</b> <b>value</b> but am getting same prediction for all data points. Here, AL is the activation output vector of the output layer and Y is the vector containing original values. I used tanh function as the activation function for each layer and the layer config is as follows= (4,10,10,10,1)", "dateLastCrawled": "2022-02-02T20:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "4. Regression and Prediction - <b>Practical Statistics for Data Scientists</b> ...", "url": "https://www.oreilly.com/library/view/practical-statistics-for/9781491952955/ch04.html", "isFamilyFriendly": true, "displayUrl": "https://www.oreilly.com/library/view/practical-statistics-for/9781491952955/ch04.html", "snippet": "The intercept of the regression line\u2014that is, the <b>predicted</b> <b>value</b> when X = 0. Synonyms . b 0, \u03b2 0. Regression coefficient. The slope of the regression line. Synonyms. slope, b 1, \u03b2 1, parameter estimates, weights. Fitted values. The estimates Y ^ i obtained from the regression line. Synonyms. <b>predicted</b> values. Residuals. The <b>difference</b> <b>between</b> the observed values and the fitted values. Synonyms. errors. Least squares. The method of fitting a regression by minimizing the sum of squared ...", "dateLastCrawled": "2022-02-03T02:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "How To <b>Measure The Accuracy Of Predictive Models</b> - Acheron Analytics", "url": "http://www.acheronanalytics.com/acheron-blog/how-to-measure-the-accuracy-of-predictive-models", "isFamilyFriendly": true, "displayUrl": "www.acheronanalytics.com/acheron-blog/how-to-<b>measure-the-accuracy-of-predictive-models</b>", "snippet": "They both take the <b>difference</b> <b>between</b> the <b>actual</b> and the forecast. However, the RMSE also then squares the <b>difference</b>, finds the average of all the squares and then finds the square root. Now it might seem like the action of squaring and then taking the square root may cancel each other out. This isn\u2019t the case. The RMSE essentially punishes larger errors. Another way to phrase that is that it puts a heavier", "dateLastCrawled": "2022-02-03T04:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What is <b>Mean Squared Error, Mean Absolute Error</b>, Root Mean Squared ...", "url": "https://www.studytonight.com/post/what-is-mean-squared-error-mean-absolute-error-root-mean-squared-error-and-r-squared", "isFamilyFriendly": true, "displayUrl": "https://www.studytonight.com/post/what-is-<b>mean-squared-error-mean-absolute-error</b>-root...", "snippet": "4. R Squared. It is also known as the coefficient of determination.This metric gives an indication of how good a model fits a given dataset. It indicates how close the regression line (i.e the <b>predicted</b> values plotted) is to the <b>actual</b> data values. The R squared <b>value</b> lies <b>between</b> 0 and 1 where 0 indicates that this model doesn&#39;t fit the given data and 1 indicates that the model fits perfectly to the dataset provided.. import numpy as np X = np.random.randn(100) y = np.random.randn(60) # y ...", "dateLastCrawled": "2022-02-03T03:01:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Cost Function and <b>Loss</b> Function in Machine Learning - Shishir Kant Singh", "url": "http://shishirkant.com/cost-function-and-loss-function-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "shishirkant.com/cost-function-and-<b>loss</b>-function-in-machine-learning", "snippet": "Huber <b>Loss</b>. A comparison <b>between</b> L1 and <b>L2</b> <b>loss</b> yields the following results: L1 <b>loss</b> is more robust than its counterpart. On taking a closer look at the formulas, one <b>can</b> observe that if the <b>difference</b> <b>between</b> the <b>predicted</b> and the <b>actual</b> <b>value</b> is high, <b>L2</b> <b>loss</b> magnifies the effect when compared to L1. Since <b>L2</b> succumbs to outliers, L1 <b>loss</b> ...", "dateLastCrawled": "2022-01-29T23:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Loss</b> Functions. <b>Loss</b> functions explanations and\u2026 | by Tomer Kordonsky ...", "url": "https://medium.com/artificialis/loss-functions-361b2ad439a0", "isFamilyFriendly": true, "displayUrl": "https://medium.com/artificialis/<b>loss</b>-functions-361b2ad439a0", "snippet": "We <b>can</b> achieve this using the Huber <b>Loss</b> (Smooth L1 <b>Loss</b>), a combination of L1 (MAE) and <b>L2</b> (MSE) losses. <b>Can</b> be called Huber <b>Loss</b> or Smooth MAE Less sensitive to outliers in data than the squared ...", "dateLastCrawled": "2022-02-03T02:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Loss</b> <b>Function</b> and Cost <b>Function</b> in Neural Networks | by Simran ...", "url": "https://medium.com/analytics-vidhya/loss-function-and-cost-function-in-neural-networks-4ade0c9ccb18", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/<b>loss</b>-<b>function</b>-and-cost-<b>function</b>-in-neural-networks...", "snippet": "Now, it quite obvious that the <b>predicted</b> <b>value</b> would\u2019t be same as <b>actual</b> <b>value</b> only in one epoch, there would be some deviation. This is called <b>loss</b>, now this <b>loss</b> will be optimized by optimizer ...", "dateLastCrawled": "2022-01-30T23:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Loss</b> Functions in Machine Learning: An Easy Overview(2021)", "url": "https://www.jigsawacademy.com/blogs/ai-ml/loss-function", "isFamilyFriendly": true, "displayUrl": "https://www.<b>jigsawacademy</b>.com/blogs/ai-ml/<b>loss</b>-function", "snippet": "Quadratic <b>Loss</b>/ <b>L2</b> <b>Loss</b> /Mean Square Error; Mathematical formulation:- MSE <b>loss</b> performs as outlined because of the average of absolute variations <b>between</b> the particular and also the foretold <b>value</b>. It\u2019s the second most ordinarily used Regression <b>loss</b> function. The function <b>value</b> is the Mean of these Absolute Errors (MAE). The MAE <b>Loss</b> function is additional strong to outliers compared to the MSE <b>Loss</b> function. Therefore, it ought to be used if the information is liable to several outliers ...", "dateLastCrawled": "2022-01-26T08:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Choosing and Customizing <b>Loss</b> Functions for Image Processing | by ...", "url": "https://towardsdatascience.com/choosing-and-customizing-loss-functions-for-image-processing-a0e4bf665b0a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/choosing-and-customizing-<b>loss</b>-functions-for-image...", "snippet": "Mean Square Error/Quadratic <b>Loss</b>/<b>L2</b> <b>Loss</b>: averages the squared <b>difference</b> <b>between</b> predictions and ground truth, with a focus on the average magnitudes of errors regardless of direction. Mean Absolute Error, L1 <b>Loss</b> (used by PerceptiLabs\u2019 Regression component): sums the absolute differences <b>between</b> the predictions and ground truth, and finds the average.", "dateLastCrawled": "2022-01-31T10:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Famous <b>Loss</b> Functions and Cost Functions in Machine Learning", "url": "https://www.enjoyalgorithms.com/blog/loss-and-cost-functions-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.enjoyalgorithms.com/blog/<b>loss</b>-and-cost-functions-in-machine-learning", "snippet": "L1 <b>loss</b> is more robust to outliers than <b>L2</b>, or we <b>can</b> say that when the <b>difference</b> is higher, L1 is more stable than <b>L2</b>. <b>L2</b> <b>loss</b> is more stable than the L1 <b>loss</b>, especially when the <b>difference</b> <b>between</b> prediction <b>and actual</b> is smaller. Huber <b>loss</b> takes the good from both L1 and <b>L2</b> and avoids their shortcomings. It is quadratic for smaller errors ...", "dateLastCrawled": "2022-01-26T18:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Choosing and Customizing Loss Functions for Image Processing</b>", "url": "https://blog.perceptilabs.com/choosing-and-customizing-loss-functions-for-image-processing/", "isFamilyFriendly": true, "displayUrl": "https://blog.perceptilabs.com/<b>choosing-and-customizing-loss-functions-for-image-processing</b>", "snippet": "Mean Square Error/Quadratic <b>Loss</b>/<b>L2</b> <b>Loss</b>: averages the squared <b>difference</b> <b>between</b> predictions and ground truth, with a focus on the average magnitudes of errors regardless of direction. Mean Absolute Error, L1 <b>Loss</b> (used by PerceptiLabs&#39; Regression component): sums the absolute differences <b>between</b> the predictions and ground truth, and finds the average.", "dateLastCrawled": "2022-01-28T02:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Mean <b>Square Error &amp; R2 Score Clearly Explained</b> \u2013 BMC Software | Blogs", "url": "https://www.bmc.com/blogs/mean-squared-error-r2-and-variance-in-regression-analysis/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.bmc.com</b>/blogs/mean-squared-error-r2-and-variance-in-regression-analysis", "snippet": "The goal is to have a <b>value</b> that is low. What low means is quantified by the r2 score (explained below). In the code below, this is np.var(err), where err is an array of the differences <b>between</b> observed and <b>predicted</b> values and np.var() is the numpy array variance function. What is r2 score? The r2 score varies <b>between</b> 0 and 100%", "dateLastCrawled": "2022-02-03T07:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Understanding binary <b>cross-entropy</b> / log <b>loss</b>: a visual explanation ...", "url": "https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-binary-<b>cross-entropy</b>-log-<b>loss</b>-a-visual...", "snippet": "Binary <b>Cross-Entropy</b> / Log <b>Loss</b>. where y is the label (1 for green points and 0 for red points) and p(y) is the <b>predicted</b> probability of the point being green for all N points.. Reading this formula, it tells you that, for each green point (y=1), it adds log(p(y)) to the <b>loss</b>, that is, the log probability of it being green.Conversely, it adds log(1-p(y)), that is, the log probability of it being red, for each red point (y=0).Not necessarily difficult, sure, but no so intuitive too\u2026", "dateLastCrawled": "2022-02-03T02:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "python - Predictions in Tensorflow with a 1-hiddden layer Neural ...", "url": "https://stackoverflow.com/questions/43564627/predictions-in-tensorflow-with-a-1-hiddden-layer-neural-network-does-not-change", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/43564627", "snippet": "I don&#39;t think any of these are the reason for the poor performance, but I noticed 3 things: First, you use hidden1 instead of hidden_dropout for defining output, so you are essentially just doing linear regression right now because there is no activation function <b>between</b> the layers.Second, you probably want to add regularization for output_w to <b>loss</b>_<b>l2</b>.Finally, 32 bits is usually more than enough, so explicitly using 64 bit floats probably doesn&#39;t make a <b>difference</b>.", "dateLastCrawled": "2022-01-07T19:04:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Understanding Loss Functions in Machine Learning</b> | Engineering ...", "url": "https://www.section.io/engineering-education/understanding-loss-functions-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.section.io/engineering-education/<b>understanding-loss-functions-in-machine</b>...", "snippet": "On taking a closer look at the formulas, one <b>can</b> observe that if the <b>difference</b> <b>between</b> the <b>predicted</b> and the <b>actual</b> <b>value</b> is high, <b>L2</b> <b>loss</b> magnifies the effect when <b>compared</b> to L1. Since <b>L2</b> succumbs to outliers, L1 <b>loss</b> function is the more robust <b>loss</b> function. L1 <b>loss</b> is less stable than <b>L2</b> <b>loss</b>. Since L1 <b>loss</b> deals with the <b>difference</b> in distances, a small horizontal change <b>can</b> lead to the regression line jumping a large amount. Such an effect taking place across multiple iterations ...", "dateLastCrawled": "2022-02-01T06:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Cost Function and <b>Loss</b> Function in Machine Learning - Shishir Kant Singh", "url": "http://shishirkant.com/cost-function-and-loss-function-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "shishirkant.com/cost-function-and-<b>loss</b>-function-in-machine-learning", "snippet": "Huber <b>Loss</b>. A comparison <b>between</b> L1 and <b>L2</b> <b>loss</b> yields the following results: L1 <b>loss</b> is more robust than its counterpart. On taking a closer look at the formulas, one <b>can</b> observe that if the <b>difference</b> <b>between</b> the <b>predicted</b> and the <b>actual</b> <b>value</b> is high, <b>L2</b> <b>loss</b> magnifies the effect when <b>compared</b> to L1. Since <b>L2</b> succumbs to outliers, L1 <b>loss</b> ...", "dateLastCrawled": "2022-01-29T23:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Loss Functions in Machine Learning</b> | Working | Different Types", "url": "https://www.educba.com/loss-functions-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.educba.com/<b>loss-functions-in-machine-learning</b>", "snippet": "Cross-entropy <b>loss</b> increases as the <b>predicted</b> probability <b>value</b> deviate from the <b>actual</b> label. Hinge <b>loss</b>. Hinge <b>loss</b> <b>can</b> be used as an alternative to cross-entropy, which was initially developed to use with a support vector machine algorithm. Hinge <b>loss</b> works best with the classification problem because target values are in the set of {-1,1 ...", "dateLastCrawled": "2022-02-03T02:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Loss</b> Functions in Neural Networks", "url": "https://www.theaidream.com/post/loss-functions-in-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://www.theaidream.com/post/<b>loss</b>-functions-in-neural-networks", "snippet": "Cross-Entropy <b>loss</b> is also called logarithmic <b>loss</b>, log <b>loss</b>, or logistic <b>loss</b>. Each <b>predicted</b> class probability is <b>compared</b> to the <b>actual</b> class desired output 0 or 1 and a score/<b>loss</b> is calculated that penalizes the probability based on how far it is from the <b>actual</b> expected <b>value</b>. The penalty is logarithmic in nature yielding a large score for large differences close to 1 and small score for small differences tending to 0.", "dateLastCrawled": "2022-02-02T10:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Why L1 norm creates Sparsity <b>compared</b> with <b>L2</b> norm | by Satishkumar ...", "url": "https://satishkumarmoparthi.medium.com/why-l1-norm-creates-sparsity-compared-with-l2-norm-3c6fa9c607f4", "isFamilyFriendly": true, "displayUrl": "https://satishkumarmoparthi.medium.com/why-l1-norm-creates-sparsity-<b>compared</b>-with-<b>l2</b>...", "snippet": "The main intuitive <b>difference</b> <b>between</b> the L1 and <b>L2</b> regularization is that L1 regularization tries to estimate the median of the data while the <b>L2</b> regularization tries to estimate the mean of the data to avoid overfitting. Left: <b>Loss</b> function with L1 regularisation; center: <b>Loss</b> function with <b>L2</b> regularisation; right: Gradient Descent. In optimization formulation for comparison <b>Loss</b> and \u03bb <b>can</b> be ignored as they are the same for both L1 and <b>L2</b> regularization. <b>L2</b> has min(W) for (W1\u00b2+W2\u00b2 ...", "dateLastCrawled": "2022-01-30T05:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A Brief Overview of <b>Loss</b> Functions in <b>Pytorch</b> | by Pratyaksha Jha ...", "url": "https://medium.com/udacity-pytorch-challengers/a-brief-overview-of-loss-functions-in-pytorch-c0ddb78068f7", "isFamilyFriendly": true, "displayUrl": "https://medium.com/udacity-<b>pytorch</b>-challengers/a-brief-overview-of-<b>loss</b>-functions-in-p...", "snippet": "The distinction is the <b>difference</b> <b>between</b> <b>predicted</b> <b>and actual</b> probability. This adds data about information <b>loss</b> in the model training. The farther away the <b>predicted</b> probability distribution is ...", "dateLastCrawled": "2022-02-02T14:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Loss Function</b> - Pipline", "url": "https://nuoxu2016.gitbook.io/pipline/preprocessing-1/loss-function", "isFamilyFriendly": true, "displayUrl": "https://nuoxu2016.gitbook.io/pipline/preprocessing-1/<b>loss-function</b>", "snippet": "The distinction is the <b>difference</b> <b>between</b> <b>predicted</b> <b>and actual</b> probability. This adds data about information <b>loss</b> in the model training. The farther away the <b>predicted</b> probability distribution is from the true probability distribution, greater is the <b>loss</b>. It does not penalize the model based on the confidence of prediction, as in cross entropy <b>loss</b>, but how different is the prediction from ground truth. It usually outperforms mean square error, especially when data is not normally ...", "dateLastCrawled": "2022-01-24T22:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Loss</b> and <b>Loss</b> <b>Functions for Training Deep Learning Neural Networks</b>", "url": "https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>loss</b>-and-<b>loss</b>-<b>functions-for-training-deep-learning</b>...", "snippet": "Each <b>predicted</b> probability is <b>compared</b> to the <b>actual</b> class output <b>value</b> (0 or 1) and a score is calculated that penalizes the probability based on the distance from the expected <b>value</b>. The penalty is logarithmic, offering a small score for small differences (0.1 or 0.2) and enormous score for a large <b>difference</b> (0.9 or 1.0). Cross-entropy <b>loss</b> is minimized, where smaller values represent a better model than larger values. A model that predicts perfect probabilities has a cross entropy or log ...", "dateLastCrawled": "2022-02-02T20:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Ways to Evaluate Regression Models | by ... - Towards Data Science", "url": "https://towardsdatascience.com/ways-to-evaluate-regression-models-77a3ff45ba70", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/ways-to-evaluate-regression-models-77a3ff45ba70", "snippet": "It <b>can</b> <b>be compared</b> <b>between</b> models whose errors are measured in the different units. Mathematically, ... MSE of 1 point is a <b>difference</b> of 1 point of <b>actual</b> <b>between</b> <b>predicted</b> <b>and actual</b>). In RAE and Relative RSE, you divide those differences by the variation of <b>actual</b>, so they have a scale from 0 to 1, and if you multiply this <b>value</b> by 100, you get similarity in 0\u2013100 scale (i.e. percentage). The values of \u2211(MeanofActual \u2014 <b>actual</b>)\u00b2 or \u2211|MeanofActual \u2014 <b>actual</b>| tell you how much ...", "dateLastCrawled": "2022-02-02T07:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Linear, Lasso, Ridge, and Elastic Net Regression</b> | Regression Models in ...", "url": "https://www.analyticssteps.com/blogs/linear-lasso-ridge-and-elastic-net-regression-overview", "isFamilyFriendly": true, "displayUrl": "https://www.analyticssteps.com/blogs/<b>linear-lasso-ridge-and-elastic-net-regression</b>...", "snippet": "The technology has been employed to precisely and accurately examine medical information and aid in predicting if a patient is vulnerable to a heart attack or stroke. You <b>can</b> gain further knowledge of AI\u2019s role in the medical sector through our blog on Artificial Intelligence in Healthcare. 2. Nest learning thermostat.", "dateLastCrawled": "2022-02-03T02:21:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Regularization \u2014 Understanding L1 and <b>L2</b> regularization for Deep <b>Learning</b>", "url": "https://medium.com/analytics-vidhya/regularization-understanding-l1-and-l2-regularization-for-deep-learning-a7b9e4a409bf", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/regularization-understanding-l1-and-<b>l2</b>...", "snippet": "Understanding what regularization is and why it is required for <b>machine</b> <b>learning</b> and diving deep to clarify the importance of L1 and <b>L2</b> regularization in Deep <b>learning</b>.", "dateLastCrawled": "2022-02-01T00:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Understanding the 3 most common <b>loss</b> functions for <b>Machine</b> <b>Learning</b> ...", "url": "https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-the-3-most-common-<b>loss</b>-functions-for...", "snippet": "A <b>loss function</b> in <b>Machine</b> <b>Learning</b> is a measure of how accurately your ML model is able to predict the expected outcome i.e the ground truth. The <b>loss function</b> will take two items as input: the output value of our model and the ground truth expected value. The output of the <b>loss function</b> is called the <b>loss</b> which is a measure of how well our model did at predicting the outcome. A high value for the <b>loss</b> means our model performed very poorly. A low value for the <b>loss</b> means our model performed ...", "dateLastCrawled": "2022-02-02T13:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Why <b>Machine Learning Is A Metaphor For</b> Life \u2013 Adit Deshpande ...", "url": "https://adeshpande3.github.io/Why-Machine-Learning-is-a-Metaphor-For-Life", "isFamilyFriendly": true, "displayUrl": "https://adeshpande3.github.io/Why-<b>Machine-Learning-is-a-Metaphor-For</b>-Life", "snippet": "Well, in <b>machine</b> <b>learning</b>, we use regularizers. The first form (and the most common) of regularization that I first learned about was <b>L2</b> regularization or weight decay. This type of regularization is basically imposing a soft constraint on the cost function. We\u2019re telling the network \u201cHey, we want you to minimize the <b>loss</b> from the training ...", "dateLastCrawled": "2022-01-31T13:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Solutions to the exercises for <b>Machine</b> <b>Learning</b>", "url": "http://www.perfmath.com/ml/ml_liu_text_solutions.pdf", "isFamilyFriendly": true, "displayUrl": "www.perfmath.com/ml/ml_liu_text_solutions.pdf", "snippet": "is known as L1-norm, while the latter is known as the <b>L2</b>-norm. Keep in mind that <b>L2</b>-norm is more sensitive than L1-norm to large-valued outliers. Ridge and LASSO regularizations are based on <b>L2</b>-norm and L1-norm, respectively, while Elastic Net regularization is based on the mix of two. 2.6 What does a <b>machine</b> <b>learning</b> <b>learning</b>-curve measure ...", "dateLastCrawled": "2022-01-18T07:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Choosing and Customizing <b>Loss</b> Functions for Image Processing | by ...", "url": "https://towardsdatascience.com/choosing-and-customizing-loss-functions-for-image-processing-a0e4bf665b0a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/choosing-and-customizing-<b>loss</b>-functions-for-image...", "snippet": "This is what a <b>machine</b> <b>learning</b> (ML) algorithm does during training. More specifically, the optimizer , which in this mountain <b>analogy</b> roughly describes stochastic gradient descent (SGD) optimization, continually tries new weights and biases until it reaches its goal of finding the optimal values for the model to make accurate predictions.", "dateLastCrawled": "2022-01-31T10:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and-sample-codes ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/README.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "Model 1: accuracy = 92%. Model 2: accuracy = 95%. Model 2 has higher accuracy than model 1, but model 2 is useless. This is called accuracy paradox, which means the model with higher accuracy may not have better generalization power. In general, when TP &lt; FP, the accuracy will always increase when we change the classifier to always output &#39;negative&#39;.Conversely, when TN &lt; FN, the same will happen when we change the classifier to always output &#39;positive&#39; [1].. Recall@k", "dateLastCrawled": "2021-09-22T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>machine</b> <b>learning</b> - L1-norm vs <b>l2</b>-norm as cost function when ...", "url": "https://stackoverflow.com/questions/43301036/l1-norm-vs-l2-norm-as-cost-function-when-standardizing", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/43301036", "snippet": "The basic idea/motivation is how to penalize deviations. L1-norm does not care much about outliers, while <b>L2</b>-norm penalize these heavily. This is the basic difference and you will find a lot of pros and cons, even on wikipedia. So in regards to your question if it makes sense when the expected deviations are small: sure, it behaves the same.", "dateLastCrawled": "2022-01-24T10:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Bias-Variance Decomposition</b> - mlxtend", "url": "http://rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp/", "isFamilyFriendly": true, "displayUrl": "rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp", "snippet": "<b>Bias variance decomposition</b> of <b>machine</b> <b>learning</b> algorithms for various <b>loss</b> functions. from mlxtend.evaluate import bias_variance_decomp. Overview. Often, researchers use the terms bias and variance or &quot;bias-variance tradeoff&quot; to describe the performance of a model -- i.e., you may stumble upon talks, books, or articles where people say that a model has a high variance or high bias. So, what does that mean? In general, we might say that &quot;high variance&quot; is proportional to overfitting, and ...", "dateLastCrawled": "2022-01-31T12:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Data Science: <b>Support Vector Machines (SVM</b>)", "url": "https://www.datasciencesmachinelearning.com/2019/01/support-vector-machines-svm.html", "isFamilyFriendly": true, "displayUrl": "https://www.datasciences<b>machinelearning</b>.com/2019/01/<b>support-vector-machines-svm</b>.html", "snippet": "In this case, squared hinge <b>loss</b> function (as against hinge <b>loss</b> function) and <b>l2</b> penalty are the major changes compared to the earlier three methods. This method is useful for when sample size is larger.", "dateLastCrawled": "2022-01-28T08:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What are some <b>examples in everyday life analogous to &#39;overfitting</b>&#39; in ...", "url": "https://www.quora.com/What-are-some-examples-in-everyday-life-analogous-to-overfitting-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-are-some-<b>examples-in-everyday-life-analogous-to-overfitting</b>...", "snippet": "Answer (1 of 3): Exam overfitting - When you study for an exam, only by practicing questions from previous years&#39; exams. You then discover to your horror that xx% of this year&#39;s questions are new, and you get a much lower score than on your practice ones. If you are a bit older, you can expand th...", "dateLastCrawled": "2022-01-06T06:07:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "[D] Looking for papers on treating regression as classification vs ...", "url": "https://www.reddit.com/r/MachineLearning/comments/7gun87/d_looking_for_papers_on_treating_regression_as/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/<b>MachineLearning</b>/comments/7gun87/d_looking_for_papers_on...", "snippet": "Doing the <b>L2 loss is like</b> doing maximum likelihood on a gaussian with a fixed variance - so the bad regression here is largely coming from the gaussian being mis-specified. I think the richer question would involve comparing approaches that consider the ordering vs. approaches that don t consider the ordering but where both have flexible enough distributions.", "dateLastCrawled": "2021-01-17T22:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Deep learning</b> - lectures.alex.balgavy.eu", "url": "https://lectures.alex.balgavy.eu/ml-notes/deep-learning/", "isFamilyFriendly": true, "displayUrl": "https://lectures.alex.balgavy.eu/ml-notes/<b>deep-learning</b>", "snippet": "<b>Deep learning</b> <b>Deep learning</b> systems (autodiff engines) Tensors. To scale up backpropagation, want to move from operations on scalars to tensors. Tensor: generalisation of vectors/matrices to higher dimensions. e.g. a 2-tensor has two dimensions, a 4-tensor has 4 dimensions. You can represent data as a tensor. e.g. an RGB image is a 3-tensor of the red, green, and blue values for each pixel. Functions on tensors. Functions have inputs and outputs, all of which are tensors. They implement ...", "dateLastCrawled": "2021-12-15T05:20:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A deep <b>learning</b> framework for constitutive modeling based on temporal ...", "url": "https://www.sciencedirect.com/science/article/pii/S0021999121006793", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0021999121006793", "snippet": "These two features meet the requirement for sequence modeling in <b>machine</b> <b>learning</b>. Therefore, the nonlinear constitutive models may be classified as sequence modeling from the viewpoint of <b>machine</b> <b>learning</b>. Concrete material and steel material both exhibit significant ultra-long-term memory effects and many model-driven constitutive relationships were developed to simulate stress-strain curves of materials , , , , with ultra-long-term memory effect. For steel material, the traditional ...", "dateLastCrawled": "2022-01-20T12:37:00.0000000Z", "language": "en", "isNavigational": false}], [], [], []], "all_bing_queries": ["+(l2 loss)  is like +(difference between predicted and actual value)", "+(l2 loss) is similar to +(difference between predicted and actual value)", "+(l2 loss) can be thought of as +(difference between predicted and actual value)", "+(l2 loss) can be compared to +(difference between predicted and actual value)", "machine learning +(l2 loss AND analogy)", "machine learning +(\"l2 loss is like\")", "machine learning +(\"l2 loss is similar\")", "machine learning +(\"just as l2 loss\")", "machine learning +(\"l2 loss can be thought of as\")", "machine learning +(\"l2 loss can be compared to\")"]}