{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Tilted <b>Empirical</b> <b>Risk</b> <b>Minimization</b> \u2013 <b>Machine</b> <b>Learning</b> Blog | ML@CMU ...", "url": "https://blog.ml.cmu.edu/2021/04/02/term/", "isFamilyFriendly": true, "displayUrl": "https://blog.ml.cmu.edu/2021/04/02/t<b>erm</b>", "snippet": "Our work explores tilted <b>empirical</b> <b>risk</b> <b>minimization</b> (TERM), a simple and general alternative to <b>ERM</b>, which is ubiquitous throughout <b>machine</b> <b>learning</b>. Our hope is that the TERM framework will allow <b>machine</b> <b>learning</b> practitioners to easily modify the <b>ERM</b> objective to handle practical concerns such as enforcing fairness amongst subgroups, mitigating the effect of outliers, and ensuring robust performance on new, unseen data. Critical to the success of such a framework is understanding the ...", "dateLastCrawled": "2022-01-30T13:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Empirical risk minimization</b> - WikiMili, The Free Encyclopedia", "url": "https://wikimili.com/en/Empirical_risk_minimization", "isFamilyFriendly": true, "displayUrl": "https://wikimili.com/en/<b>Empirical_risk_minimization</b>", "snippet": "<b>Empirical risk minimization</b> (<b>ERM</b>) is a principle in statistical <b>learning</b> theory which defines a family of <b>learning</b> algorithms and is used to give theoretical bounds on their performance. The core idea is that we cannot know exactly how well an algorithm will work in practice (the true &quot;<b>risk</b>&quot;) because we don&#39;t know the true distribution of data that the algorithm will work on, but we can instead measure its performance on a known set of training data (the &quot;<b>empirical</b>&quot; <b>risk</b>).", "dateLastCrawled": "2021-01-18T18:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Private <b>Empirical</b> <b>Risk</b> <b>Minimization</b> with Analytic Gaussian Mechanism ...", "url": "https://par.nsf.gov/servlets/purl/10217235", "isFamilyFriendly": true, "displayUrl": "https://par.nsf.gov/servlets/purl/10217235", "snippet": "privacy preserving <b>machine</b> <b>learning</b> together with differential privacy, such as [9]\u2013[11]. <b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) as a standard <b>technique</b> covers a wide set of <b>learning</b> tasks <b>like</b> classi\ufb01cation and regression, etc., where the averaged loss of the model over a dataset is minimized. We can directly obtain private <b>machine</b>", "dateLastCrawled": "2022-02-03T10:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Tilted <b>Empirical Risk</b> <b>Minimization</b> | DeepAI", "url": "https://deepai.org/publication/tilted-empirical-risk-minimization", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/tilted-<b>empirical-risk</b>-<b>minimization</b>", "snippet": "<b>Empirical risk</b> <b>minimization</b> (<b>ERM</b>) is typically designed to perform well on the average loss, which can result in estimators that are sensitive to outliers, generalize poorly, or treat subgroups unfairly.While many methods aim to address these problems individually, in this work, we explore them through a unified framework\u2014tilted <b>empirical risk</b> <b>minimization</b> (TERM).", "dateLastCrawled": "2022-01-11T23:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Gradient-Based <b>Empirical</b> <b>Risk</b> <b>Minimization</b> using Local Polynomial ...", "url": "https://www.cs.purdue.edu/homes/amakur/docs/Gradient-Based%20Empirical%20Risk%20Minimization%20using%20Local%20Polynomial%20Regression.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.purdue.edu/homes/amakur/docs/Gradient-Based <b>Empirical</b> <b>Risk</b> <b>Minimization</b>...", "snippet": "<b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) is one of the mainstays of contem-porary <b>machine</b> <b>learning</b>. Indeed, training tasks such as classi cation, regression, or represen-tation <b>learning</b> using deep neural networks, can all be formulated as speci c instances of <b>ERM</b>. In this paper, we consider gradient-based iterative optimization methods that are used to per- form <b>ERM</b>, such as batch gradient descent (GD) [39], stochastic gradient descent (SGD) [37], and their re nements, e.g., [14,27,40]. Since the ...", "dateLastCrawled": "2022-02-01T17:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> in the Non-interactive Local Model of ...", "url": "https://shao3wangdi.github.io/papers/JMLR2020_LDPERM.pdf", "isFamilyFriendly": true, "displayUrl": "https://shao3wangdi.github.io/papers/JMLR2020_LDP<b>ERM</b>.pdf", "snippet": "Non-interactive LDP-<b>ERM</b> Thus, we have the following two types of excess <b>risk</b> measured at a particular output w priv: The <b>empirical</b> <b>risk</b>, Err D(w priv) = L(w priv;D)\u2212min w\u2208C L(w;D), and the population <b>risk</b>, ErrP(w priv) = LP(w priv)\u2212min w\u2208C LP(w). The problem considered in this paper is to design non-interactive LDP protocols that \ufb01nd ...", "dateLastCrawled": "2021-09-14T05:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "An <b>Empirical</b> Study of <b>Invariant Risk Minimization</b> | DeepAI", "url": "https://deepai.org/publication/an-empirical-study-of-invariant-risk-minimization", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/an-<b>empirical</b>-study-of-<b>invariant-risk-minimization</b>", "snippet": "<b>Invariant risk minimization</b> (IRM) is a recently proposed <b>machine</b> <b>learning</b> framework where the goal is to learn invariances across multiple training environments . Compared to the widely used framework of <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>), IRM does not assume that training samples are identically distributed. Rather, IRM assumes that training ...", "dateLastCrawled": "2022-01-24T09:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Optimization</b> - mlstory.org", "url": "https://mlstory.org/optimization.html", "isFamilyFriendly": true, "displayUrl": "https://mlstory.org/<b>optimization</b>.html", "snippet": "Then we turned to <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) where we instead rely on numerical methods to discover good decision rules when we don\u2019t have such a probability model. In this chapter, we take a closer look at how to solve <b>empirical</b> <b>risk</b> <b>minimization</b> problems effectively. We focus on the core <b>optimization</b> methods commonly used to solve <b>empirical</b> <b>risk</b> <b>minimization</b> problems and on the mathematical tools used to analyze their running times. Our main subject will be gradient descent ...", "dateLastCrawled": "2022-01-30T03:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Efficient Private <b>Empirical</b> <b>Risk</b> <b>Minimization</b> for High-dimensional <b>Learning</b>", "url": "http://www.shivakasiviswanathan.com/ICML16.pdf", "isFamilyFriendly": true, "displayUrl": "www.shivakasiviswanathan.com/ICML16.pdf", "snippet": "<b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) is a fundamental <b>tech-nique</b> in statistical <b>machine</b> <b>learning</b> that forms the basis for various <b>learning</b> algorithms. Start-ing from the results of (Chaudhuri &amp; Monteleoni, 2009;Chaudhuri et al.,2011), there is a long line of work in designing differentially private algo-rithms for <b>empirical</b> <b>risk</b> <b>minimization</b> problems", "dateLastCrawled": "2021-09-16T05:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Machine</b> <b>Learning</b>: What is the general idea of why minimizing <b>Empirical</b> ...", "url": "https://www.quora.com/Machine-Learning-What-is-the-general-idea-of-why-minimizing-Empirical-Risk-Minimization-is-NP-Complete", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>Machine</b>-<b>Learning</b>-What-is-the-general-idea-of-why-minimizing...", "snippet": "Answer: The computational complexity of <b>Empirical</b> <b>Risk</b> <b>Minimization</b> depends on the hypothesis class and the loss function that defines the <b>risk</b>. For example, <b>ERM</b> for linear regression with the square loss (ordinary least squares) can be solved in polynomial time. But <b>ERM</b> for a standard 3-layer ne...", "dateLastCrawled": "2022-01-19T10:26:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Tilted <b>Empirical</b> <b>Risk</b> <b>Minimization</b> \u2013 <b>Machine</b> <b>Learning</b> Blog | ML@CMU ...", "url": "https://blog.ml.cmu.edu/2021/04/02/term/", "isFamilyFriendly": true, "displayUrl": "https://blog.ml.cmu.edu/2021/04/02/t<b>erm</b>", "snippet": "Our work explores tilted <b>empirical</b> <b>risk</b> <b>minimization</b> (TERM), a simple and general alternative to <b>ERM</b>, which is ubiquitous throughout <b>machine</b> <b>learning</b>. Our hope is that the TERM framework will allow <b>machine</b> <b>learning</b> practitioners to easily modify the <b>ERM</b> objective to handle practical concerns such as enforcing fairness amongst subgroups, mitigating the effect of outliers, and ensuring robust performance on new, unseen data. Critical to the success of such a framework is understanding the ...", "dateLastCrawled": "2022-01-30T13:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Private <b>Empirical Risk</b> <b>Minimization</b>: Efficient Algorithms and Tight ...", "url": "https://par.nsf.gov/servlets/purl/10092778", "isFamilyFriendly": true, "displayUrl": "https://par.nsf.gov/servlets/purl/10092778", "snippet": "in <b>machine</b> <b>learning</b> and statistics. We provide new algorithms and matching lower bounds for differentially private convex <b>empirical risk</b> <b>minimization</b> assuming only that each data point\u2019s contribution to the loss function is Lipschitz and that the domain of optimization is bounded. We provide a separate set of algorithms and matching lower bounds for the setting in which the loss functions are known to also be strongly convex. Our algorithms run in polynomial time, and in some cases even ...", "dateLastCrawled": "2022-01-28T12:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Private Empirical Risk Minimization Beyond the</b> Worst Case: The Effect ...", "url": "https://ui.adsabs.harvard.edu/abs/2014arXiv1411.5417T/abstract", "isFamilyFriendly": true, "displayUrl": "https://ui.adsabs.harvard.edu/abs/2014arXiv1411.5417T/abstract", "snippet": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) is a standard <b>technique</b> in <b>machine</b> <b>learning</b>, where a model is selected by minimizing a loss function over constraint set. When the training dataset consists of private information, it is natural to use a differentially private <b>ERM</b> algorithm, and this problem has been the subject of a long line of work started ...", "dateLastCrawled": "2021-04-20T19:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Logical Approach for <b>Empirical</b> <b>Risk</b> <b>Minimization</b> in <b>Machine</b> <b>Learning</b> ...", "url": "https://www.researchgate.net/publication/324242453_A_Logical_Approach_for_Empirical_Risk_Minimization_in_Machine_Learning_for_Data_Stratification", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/324242453_A_Logical_Approach_for_<b>Empirical</b>...", "snippet": "Therefore, this paper aims at developing a logical approach for using <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) <b>technique</b> to determine the <b>machine</b> <b>learning</b> classifier with the minimum <b>risk</b> function for ...", "dateLastCrawled": "2021-11-05T03:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "[1411.5417] <b>Private Empirical Risk Minimization</b> Beyond the Worst Case ...", "url": "https://arxiv.org/abs/1411.5417", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/abs/1411.5417", "snippet": "Download PDF Abstract: <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) is a standard <b>technique</b> in <b>machine</b> <b>learning</b>, where a model is selected by minimizing a loss function over constraint set. When the training dataset consists of private information, it is natural to use a differentially private <b>ERM</b> algorithm, and this problem has been the subject of a long line of work started with Chaudhuri and Monteleoni 2008.", "dateLastCrawled": "2021-09-20T11:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Private Empirical Risk Minimization Beyond the</b> Worst Case: The Effect ...", "url": "https://www.researchgate.net/publication/268525500_Private_Empirical_Risk_Minimization_Beyond_the_Worst_Case_The_Effect_of_the_Constraint_Set_Geometry", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/268525500_Private_<b>Empirical</b>_<b>Risk</b>_<b>Minimization</b>...", "snippet": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) is a standard <b>technique</b> in <b>machine</b> <b>learning</b>, where a model is selected by minimizing a loss function over constraint set. When the training dataset consists of ...", "dateLastCrawled": "2021-10-26T05:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Private <b>Empirical</b> <b>Risk</b> <b>Minimization</b> Beyond the Worst Case: The Effect ...", "url": "https://deepai.org/publication/private-empirical-risk-minimization-beyond-the-worst-case-the-effect-of-the-constraint-set-geometry", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/private-<b>empirical</b>-<b>risk</b>-<b>minimization</b>-beyond-the-worst...", "snippet": "share <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) is a standard <b>technique</b> in <b>machine</b> <b>learning</b>, where a model is selected by minimizing a loss function linear regression (LASSO), when the data x_i satisfies |x_i|_\u221e\u2264 1 and we optimize over the \u2113_1 ball. We show new lower bounds for this setting, that together with known bounds, imply that all our ...", "dateLastCrawled": "2022-01-07T12:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Machine</b> <b>Learning</b> and <b>Optimization</b> - NYU Courant", "url": "https://cims.nyu.edu/~munoz/files/ml_optimization.pdf", "isFamilyFriendly": true, "displayUrl": "https://cims.nyu.edu/~munoz/files/ml_<b>optimization</b>.pdf", "snippet": "This is known as <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) and in a sense is the raw <b>optimization</b> part of <b>machine</b> <b>learning</b>, as we will see we will require something more than that. 3 <b>Learning</b> Guarantees De nition 3. Given a set of functions G= fg: Z!Rgand a sample S= (z i)n =1 the <b>empirical</b> Rademacher complexity of Gis de ned by: &lt; S(G) = E \u02d9 1 n sup ...", "dateLastCrawled": "2022-01-29T21:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Proximal average approximated incremental gradient descent for ...", "url": "https://link.springer.com/content/pdf/10.1007%2Fs10994-016-5609-1.pdf", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/content/pdf/10.1007/s10994-016-5609-1.pdf", "snippet": "the <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) framework in <b>machine</b> <b>learning</b>. Such composite regularizers, despite their superior performance in grasping structural sparsity properties, are often nonsmooth and even nonconvex, which makes the problem dif\ufb01cult to optimize.", "dateLastCrawled": "2022-01-12T16:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Machine</b> <b>Learning</b>: What is the general idea of why minimizing <b>Empirical</b> ...", "url": "https://www.quora.com/Machine-Learning-What-is-the-general-idea-of-why-minimizing-Empirical-Risk-Minimization-is-NP-Complete", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>Machine</b>-<b>Learning</b>-What-is-the-general-idea-of-why-minimizing...", "snippet": "Answer: The computational complexity of <b>Empirical</b> <b>Risk</b> <b>Minimization</b> depends on the hypothesis class and the loss function that defines the <b>risk</b>. For example, <b>ERM</b> for linear regression with the square loss (ordinary least squares) can be solved in polynomial time. But <b>ERM</b> for a standard 3-layer ne...", "dateLastCrawled": "2022-01-19T10:26:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Empirical Risk Minimization for Probabilistic Grammars: Sample</b> ...", "url": "https://direct.mit.edu/coli/article/38/3/479/2169/Empirical-Risk-Minimization-for-Probabilistic", "isFamilyFriendly": true, "displayUrl": "https://direct.mit.edu/coli/article/38/3/479/2169/<b>Empirical</b>-<b>Risk</b>-<b>Minimization</b>-for...", "snippet": "The more data we have, the more complex our <b>can</b> be for <b>empirical</b> <b>risk</b> <b>minimization</b>. Structural <b>risk</b> <b>minimization</b> (Vapnik 1998) and the method of sieves (Grenander 1981) are examples of methods that adopt such an approach. Structural <b>risk</b> <b>minimization</b>, for example, <b>can</b> be represented in many cases as a penalization of the <b>empirical</b> <b>risk</b> method ...", "dateLastCrawled": "2021-12-08T04:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> Under Fairness Constraints | Request PDF", "url": "https://www.researchgate.net/publication/333044604_Empirical_Risk_Minimization_Under_Fairness_Constraints", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/333044604_<b>Empirical</b>_<b>Risk</b>_<b>Minimization</b>_Under...", "snippet": "Donini et al. (2018) presented a comprehensive approach based on <b>empirical</b> <b>risk</b> <b>minimization</b>, which incorporates a fairness constraint into the <b>learning</b> problem. It encourages the conditional <b>risk</b> ...", "dateLastCrawled": "2021-12-20T07:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "An <b>Empirical</b> Analysis of the Impact of Data Augmentation on Knowledge ...", "url": "https://deepai.org/publication/an-empirical-analysis-of-the-impact-of-data-augmentation-on-knowledge-distillation", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/an-<b>empirical</b>-analysis-of-the-impact-of-data...", "snippet": "Generalization Performance of Deep <b>Learning</b> models trained using the <b>Empirical</b> <b>Risk</b> <b>Minimization</b> <b>can</b> be improved significantly by using Data Augmentation strategies such as simple transformations, or using Mixed Samples. In this work, we attempt to empirically analyse the impact of such augmentation strategies on the transfer of generalization between teacher and student models in a distillation setup.", "dateLastCrawled": "2021-12-30T12:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) <b>Empirical risk minimization for probabilistic grammars: Sample</b> ...", "url": "https://www.academia.edu/2825039/Empirical_risk_minimization_for_probabilistic_grammars_Sample_complexity_and_hardness_of_learning", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/2825039/<b>Empirical_risk_minimization_for_probabilistic</b>...", "snippet": "<b>Empirical risk minimization for probabilistic grammars: Sample complexity</b> and hardness of <b>learning</b>. Download. Related Papers. <b>Empirical</b> <b>risk</b> <b>minimization</b> with approximations of probabilistic grammars . By Noah A. Smith. Regularization in statistics. By Carlos Rivero. Advanced Lectures on <b>Machine</b> <b>Learning</b>: ML Summer Schools 2003, Canberra, Australia, February 2-14, 2003, T bingen, Germany, August 4-16, 2003, Revised Lectures. By Gunnar Ratsch. Covariance in unsupervised <b>learning</b> of ...", "dateLastCrawled": "2022-01-17T04:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Differentially Private Empirical Risk Minimization</b> Revisited: Faster ...", "url": "https://deepai.org/publication/differentially-private-empirical-risk-minimization-revisited-faster-and-more-general", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>differentially-private-empirical-risk-minimization</b>...", "snippet": "In this paper we study the <b>differentially private Empirical Risk Minimization</b> (<b>ERM</b>) problem in different settings. For smooth (strongly) convex loss function with or without (non)-smooth regularization, we give algorithms that achieve either optimal or near optimal utility bounds with less gradient complexity compared with previous work. For <b>ERM</b> with smooth convex loss function in high-dimensional (p\u226b n) setting, we give an algorithm which achieves the upper bound with less gradient ...", "dateLastCrawled": "2022-01-13T15:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Strati\ufb01ed Sampling meets <b>Machine</b> <b>Learning</b>", "url": "https://cs.yale.edu/homes/el327/papers/lls15.pdf", "isFamilyFriendly": true, "displayUrl": "https://cs.yale.edu/homes/el327/papers/lls15.pdf", "snippet": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) approach. Their so-lution of the optimization problem however does not carry over. Chaudhuri et al. [10] also use the query log to iden-tify outlier records. Those are indexed separately and not sampled. While their approach mainly focuses on query ex-ecution speed, one <b>can</b> distill a sampling scheme from it.", "dateLastCrawled": "2021-08-06T00:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Dissipativity Theory for Accelerating Stochastic Variance Reduction: A ...", "url": "https://laurentlessard.com/public/icml18_disstoch.pdf", "isFamilyFriendly": true, "displayUrl": "https://laurentlessard.com/public/icml18_disstoch.pdf", "snippet": "<b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) is a key paradigm in <b>machine</b> <b>learning</b> (Bubeck,2015;Bottou et al.,2016). Many <b>learning</b> problems, including ridge regression, logistic re-gression, and support vector machines, <b>can</b> be naturally formulated as the following \ufb01nite-sum <b>ERM</b> min x2Rp g(x) := 1 n Xn i=1 f i(x); (1) where gis strongly convex. A standard approach for solv-ing (1) is the stochastic gradient (SG) method (Robbins &amp; 1University of Wisconsin\u2013Madison, United States. Correspon-dence to ...", "dateLastCrawled": "2022-01-08T07:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Zeyuan</b> Allen-Zhu&#39;s Publications - People | MIT CSAIL", "url": "https://people.csail.mit.edu/zeyuan/publications.htm", "isFamilyFriendly": true, "displayUrl": "https://people.csail.mit.edu/<b>zeyuan</b>/publications.htm", "snippet": "In this paper we focus on one of the most fundamental <b>machine</b> <b>learning</b> tasks, <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>), and provide faster algorithms with the help from the clustering structure of the data. We introduce a simple notion of raw clustering that <b>can</b> be efficiently obtained with just one pass of the data, and propose two algorithms. Our variance-reduction based algorithm ClusterSVRG introduces a new gradient estimator using the clustering information, and our accelerated algorithm ...", "dateLastCrawled": "2022-01-30T22:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Machined Learnings: <b>Using Less Data</b>", "url": "http://www.machinedlearnings.com/2013/07/using-less-data.html", "isFamilyFriendly": true, "displayUrl": "www.<b>machine</b>d<b>learning</b>s.com/2013/07/<b>using-less-data</b>.html", "snippet": "The <b>technique</b> from the paper only discusses the process of making a smaller dataset from your original dataset which has fidelity for <b>learning</b> (<b>ERM</b>). So we do a) in the paper just to give people an idea of how this method of choosing a subsample is better than alternatives. Your question suggests spending time getting the size of the subsample &quot;optimally small&quot; in some sense, but this hasn&#39;t been how I&#39;ve applied the <b>technique</b>. In practice the size of the subsample is set by some ...", "dateLastCrawled": "2021-10-20T13:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Minimizing a sum of clipped convex functions</b> | SpringerLink", "url": "https://link.springer.com/article/10.1007/s11590-020-01565-4", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11590-020-01565-4", "snippet": "We consider the problem of <b>minimizing a sum of clipped convex functions</b>. Applications of this problem include clipped <b>empirical</b> <b>risk</b> <b>minimization</b> and clipped control. While the problem of minimizing the <b>sum of clipped convex functions</b> is NP-hard, we present some heuristics for approximately solving instances of these problems. These heuristics <b>can</b> be used to find good, if not global, solutions, and appear to work well in practice. We also describe an alternative formulation, based on the ...", "dateLastCrawled": "2021-11-19T18:30:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Private <b>Empirical</b> <b>Risk</b> <b>Minimization</b> with Analytic Gaussian Mechanism ...", "url": "https://par.nsf.gov/servlets/purl/10217235", "isFamilyFriendly": true, "displayUrl": "https://par.nsf.gov/servlets/purl/10217235", "snippet": "<b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) as a standard <b>technique</b> covers a wide set of <b>learning</b> tasks like classi\ufb01cation and regression, etc., where the averaged loss of the model over a dataset is minimized. We <b>can</b> directly obtain private <b>machine</b> model by designing a private <b>ERM</b> algorithm, in other words, solving <b>ERM</b> problem in a differentially private way. A number of approaches have been proposed for designing a differentially private <b>ERM</b> algorithm that <b>can</b> be classi\ufb01ed into three categories ...", "dateLastCrawled": "2022-02-03T10:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Private <b>Empirical</b> <b>Risk</b> <b>Minimization</b> Beyond the Worst Case: The Effect ...", "url": "https://deepai.org/publication/private-empirical-risk-minimization-beyond-the-worst-case-the-effect-of-the-constraint-set-geometry", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/private-<b>empirical</b>-<b>risk</b>-<b>minimization</b>-beyond-the-worst...", "snippet": "share <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) is a standard <b>technique</b> in <b>machine</b> <b>learning</b>, where a model is selected by minimizing a loss function linear regression (LASSO), when the data x_i satisfies |x_i|_\u221e\u2264 1 and we optimize over the \u2113_1 ball. We show new lower bounds for this setting, that together with known bounds, imply that all our ...", "dateLastCrawled": "2022-01-07T12:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Tilted <b>Empirical Risk</b> <b>Minimization</b> | DeepAI", "url": "https://deepai.org/publication/tilted-empirical-risk-minimization", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/tilted-<b>empirical-risk</b>-<b>minimization</b>", "snippet": "<b>Empirical risk</b> <b>minimization</b> (<b>ERM</b>) is typically designed to perform well on the average loss, which <b>can</b> result in estimators that are sensitive to outliers, generalize poorly, or treat subgroups unfairly.While many methods aim to address these problems individually, in this work, we explore them through a unified framework\u2014tilted <b>empirical risk</b> <b>minimization</b> (TERM).", "dateLastCrawled": "2022-01-11T23:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Empirical Risk Minimization for Probabilistic Grammars: Sample</b> ...", "url": "https://direct.mit.edu/coli/article/38/3/479/2169/Empirical-Risk-Minimization-for-Probabilistic", "isFamilyFriendly": true, "displayUrl": "https://direct.mit.edu/coli/article/38/3/479/2169/<b>Empirical</b>-<b>Risk</b>-<b>Minimization</b>-for...", "snippet": "The more data we have, the more complex our <b>can</b> be for <b>empirical</b> <b>risk</b> <b>minimization</b>. Structural <b>risk</b> <b>minimization</b> (Vapnik 1998) and the method of sieves (Grenander 1981) are examples of methods that adopt such an approach. Structural <b>risk</b> <b>minimization</b>, for example, <b>can</b> be represented in many cases as a penalization of the <b>empirical</b> <b>risk</b> method ...", "dateLastCrawled": "2021-12-08T04:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Proximal average approximated incremental gradient ... - <b>Machine</b> <b>Learning</b>", "url": "https://link.springer.com/article/10.1007/s10994-016-5609-1", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10994-016-5609-1", "snippet": "<b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) is a fundamental <b>machine</b> <b>learning</b> method that learns a model by minimizing the average loss taken from the training data. To induce better prediction performance and introduce prior knowledge about the model, the <b>empirical</b> loss is often regularized by a penalty function. Based on the specific task, the penalty functions <b>can</b> vary from smooth functions like squared", "dateLastCrawled": "2021-10-10T15:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Private Convex <b>Empirical</b> <b>Risk</b> <b>Minimization</b> and High-dimensional Regression", "url": "https://cs-people.bu.edu/ads22/pubs/KST12/KST12-2012-06-07-Colt-camera.pdf", "isFamilyFriendly": true, "displayUrl": "https://cs-people.bu.edu/ads22/pubs/KST12/KST12-2012-06-07-Colt-camera.pdf", "snippet": "Journal of <b>Machine</b> <b>Learning</b> Research1{41 Private Convex <b>Empirical</b> <b>Risk</b> <b>Minimization</b> and High-dimensional Regression Daniel Kifer dkifer@cse.psu.edu Adam Smith asmith@cse.psu.edu Abhradeep Thakurta azg161@cse.psu.edu Department of Computer Science and Engineering Pennsylvania State University Abstract We consider di erentially private algorithms for convex <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>). Di erential privacy (Dwork et al.,2006b) is a recently introduced notion of privacy which guarantees ...", "dateLastCrawled": "2022-01-01T17:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Extracurricular <b>Learning</b>: Knowledge Transfer Beyond <b>Empirical</b> Distribution", "url": "https://openaccess.thecvf.com/content/CVPR2021W/ECV/papers/Pouransari_Extracurricular_Learning_Knowledge_Transfer_Beyond_Empirical_Distribution_CVPRW_2021_paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://openaccess.thecvf.com/content/CVPR2021W/ECV/papers/Pouransari_Extracurricular...", "snippet": "major accuracy improvements <b>compared</b> to the <b>empirical</b> <b>risk</b> <b>minimization</b>-based training for various recent neural network architectures: ... implemented in many <b>machine</b> <b>learning</b> tasks, for example image classi\ufb01cation [28], object detection [12, 61], video labeling [70], natural language processing [56, 41, 53, 36, 57], and speech recognition [11, 55, 37]. The idea of KD is to encourage the student to imitate teacher\u2019s behavior over a set of data points, called transfer-set. For example ...", "dateLastCrawled": "2022-01-17T16:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Optimization</b> - mlstory.org", "url": "https://mlstory.org/optimization.html", "isFamilyFriendly": true, "displayUrl": "https://mlstory.org/<b>optimization</b>.html", "snippet": "Stepping away from <b>empirical</b> <b>risk</b> <b>minimization</b> for a moment, consider the general <b>minimization</b> problem \\begin{array}{ll} \\text{minimize}_w ... We <b>can</b> then rewrite the penalized <b>ERM</b> problem as \\text{minimize}_{\\beta} \\frac{1}{n} \\sum_{i=1}^n \\mathit{loss}(e_i^T K\\beta,y_i) + \\lambda \\beta^T K \\beta\\,, where e_i is the standard Euclidean basis vector. Hence, we <b>can</b> solve the <b>machine</b> <b>learning</b> problem only using the values in the matrix K, searching only for the coefficients \\beta in the kernel ...", "dateLastCrawled": "2022-01-30T03:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "ISO 9001:2008 Certified Volume 2, Issue 7, January 2013 Pipeline Defect ...", "url": "http://www.ijeit.com/vol%202/Issue%207/IJEIT1412201301_13.pdf", "isFamilyFriendly": true, "displayUrl": "www.ijeit.com/vol 2/Issue 7/IJEIT1412201301_13.pdf", "snippet": "implementing Kalman Filter (KF). Support Vector <b>Machine</b> has been proven as an alternative option to Artificial Neural Networks (ANN) where SVM has been identified as much better classifier <b>compared</b> to the latter [2] [3].The difference between Structural <b>Risk</b> <b>Minimization</b> (SRM) which minimizes an upper bound on the expected <b>risk</b> and <b>Empirical</b>", "dateLastCrawled": "2022-01-16T18:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "[R] <b>mixup: Beyond Empirical Risk Minimization -&gt; SOTA Imagenet</b>, CIFAR ...", "url": "https://www.reddit.com/r/MachineLearning/comments/79osj8/r_mixup_beyond_empirical_risk_minimization_sota/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/<b>MachineLearning</b>/comments/79osj8/r_mixup_beyond_<b>empirical</b>_<b>risk</b>...", "snippet": "This will be like a two-way match\u2026 the algorithm will have to process every sentence and paragraph and page with the DeepRank (Deep <b>Learning</b> algorithm) to understand its context and store it not just in a simple word-mapped index but in some kind-of database that understands what each sentence is about so it <b>can</b> serve it out to a query that is processed and understood.", "dateLastCrawled": "2020-11-28T09:24:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> and Stochastic Gradient Descent for ...", "url": "http://proceedings.mlr.press/v89/veitch19a/veitch19a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v89/veitch19a/veitch19a.pdf", "snippet": "where F(Sn) is the <b>empirical</b> distribution.2 The <b>ERM</b> dogma is to select the predictor \u03c0\u02c6\u03b8 n given by \u02c6\u03b8 n = argmin\u03b8 R\u02c6(\u03b8,Sn). That is, the objective function that de\ufb01nes <b>learning</b> is the <b>empirical</b> <b>risk</b>. <b>ERM</b> has two useful properties. (1) It provides a prin-cipled framework for de\ufb01ning new <b>machine</b> <b>learning</b> methods. In particular, when ...", "dateLastCrawled": "2021-09-18T06:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Statistical <b>Learning</b> Theory and the C-Loss cost function", "url": "http://www.cnel.ufl.edu/courses/EEL6814/closs.pdf", "isFamilyFriendly": true, "displayUrl": "www.cnel.ufl.edu/courses/EEL6814/closs.pdf", "snippet": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) principle Let us consider a <b>learning</b> <b>machine</b> x,d are real r.v. with joint distribution P(x,y). F(x) is a function of some parameters w, i.e. f(x,w). d d. <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) principle How can we find the possible best <b>learning</b> <b>machine</b> that generalizes for unseen data from the same distribution? Define the <b>Risk</b> functional as L(.) is called the Loss function, and minimize it w.r.t. w achieving the best possible loss. But we can not do this ...", "dateLastCrawled": "2022-01-28T20:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Computational and Statistical <b>Learning</b> Theory", "url": "https://home.ttic.edu/~nati/Teaching/TTIC31120/2015/Lecture17.pdf", "isFamilyFriendly": true, "displayUrl": "https://home.ttic.edu/~nati/Teaching/TTIC31120/2015/Lecture17.pdf", "snippet": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) / Sample Average Approximation (SAA): Collect sample z1UYU zm ... SGD for <b>Machine</b> <b>Learning</b> Initialize S 4 L r At iteration t: Draw T \u00e7\u00e1U \u00e71\u00de If U \u00e7 S \u00e7 \u00e1\u00f6 T \u00e7 O s\u00e1 S \u00e7 &gt; 5 Z S \u00e7 E\u00df \u00e7U \u00e7\u00f6 T \u00e7 else: S \u00e7 &gt; 5 Z S \u00e7 Return S % \u00cd L 5 \u00cd \u00c3 \u00cd S \u00e7 \u00e7 @ 5 Draw T 5\u00e1U 5 \u00e1\u00e5\u00e1 T \u00e0 \u00e1U \u00e0 1\u00de Initialize S 4 L r At iteration t: Pick E \u00d0 s\u00e5I at random If U \u00dc S \u00e7 \u00e1\u00f6 T \u00dc O s\u00e1 S \u00e7 &gt; 5 Z S \u00e7 E\u00df \u00e7U \u00dc\u00f6 T \u00dc else: S \u00e7 &gt; 5 Z S \u00e7 S \u00e7 &gt; 5 Z ...", "dateLastCrawled": "2022-01-26T03:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Basics of <b>Machine</b> <b>Learning</b>", "url": "https://courses.cs.duke.edu/spring21/compsci527/slides/s_04_deep_learning.pdf", "isFamilyFriendly": true, "displayUrl": "https://courses.cs.duke.edu/spring21/compsci527/slides/s_04_deep_<b>learning</b>.pdf", "snippet": "This is called <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) COMPSCI 527 \u2014 Computer Vision Basics of <b>Machine</b> <b>Learning</b> 15/26. Loss and <b>Risk</b> <b>Machine</b> <b>Learning</b> and the Statistical <b>Risk</b> <b>ERM</b>: w^ 2argmin w2R m L T(w) In <b>machine</b> <b>learning</b>, we go much farther: We also want h to do well on previously unseen inputs To relate past and future data, assume that all data comes from the same joint probability distribution p(x;y) p is called the generative data model or just model The goal of <b>machine</b> <b>learning</b> is to ...", "dateLastCrawled": "2021-11-06T09:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Topics in <b>Machine</b> <b>Learning</b> (TIML-09)", "url": "https://www.cse.iitb.ac.in/~saketh/teaching/cs689.html", "isFamilyFriendly": true, "displayUrl": "https://www.cse.iitb.ac.in/~saketh/teaching/cs689.html", "snippet": "Introduction to Statistical <b>Learning</b> Theory (SLT): Definitions of loss function, <b>risk</b>, <b>empirical</b> <b>risk</b>, motivation for <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) Further Reading, Supplementary: Jan 12: Consistency of <b>ERM</b>, Sufficient condition for <b>ERM</b> as one-sided uniform convergence, Analysis for finite sets of functions and extensions to general case using Symmetrization trick, Shattering Coeff. Further Reading, Supplementary: Jan 15: Shattering coeff., growth function, VC dimension, Annealed Entropy ...", "dateLastCrawled": "2022-01-11T09:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Lecture 1: Reinforcement <b>Learning</b>: What and Why?", "url": "https://imada.sdu.dk/~kandemir/rl-lecture1.pdf", "isFamilyFriendly": true, "displayUrl": "https://imada.sdu.dk/~kandemir/rl-lecture1.pdf", "snippet": "<b>machine</b> <b>learning</b> and is referred to as <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>). 3 Challenges of reinforcement <b>learning</b> Consider the cart pole balancing problem, where a cart carrying an unactuated pole \ufb02oats on a straight horizontal track. The cart is actuated by a torque applied either to the right or the left direction. Seeherefor a real cart ...", "dateLastCrawled": "2021-09-30T12:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>GitHub</b> - gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "Loss function 3.1 Categories 3.2 <b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) and Structural <b>risk</b> <b>minimization</b> (SRM) 3.2.1 MLE 3.2.2 MAP 4. Evaluation metrics 4.1 Accuracy, precision, recall and F1 4.2 ROC, AUC 4.3 BLEU 5. Sample projects 6. Classical questions 6.1 Why is logistic regression a generalized linear model ?", "dateLastCrawled": "2021-09-12T01:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and-sample-codes ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/README.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "Loss function 3.1 Categories 3.2 <b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) and Structural <b>risk</b> <b>minimization</b> (SRM) 3.2.1 MLE 3.2.2 MAP 4. Evaluation metrics 4.1 Accuracy, precision, recall and F1 4.2 ROC, AUC 4.3 BLEU 5. Sample projects 6. Classical questions 6.1 Why is logistic regression a generalized linear model ?", "dateLastCrawled": "2021-09-22T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "[2006.09461] Robust <b>Compressed Sensing using Generative Models</b> - arXiv", "url": "https://arxiv.org/abs/2006.09461", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/abs/2006.09461", "snippet": "Classical recovery approaches such as <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) are guaranteed to succeed when the measurement matrix is sub-Gaussian. However, when the measurement matrix and measurements are heavy-tailed or have outliers, recovery may fail dramatically. In this paper we propose an algorithm inspired by the Median-of-Means (MOM). Our algorithm guarantees recovery for heavy-tailed data, even in the presence of outliers. Theoretically, our results show our novel MOM-based algorithm ...", "dateLastCrawled": "2021-06-27T11:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(PDF) <b>Support Vector Machines: Theory and Applications</b>", "url": "https://www.researchgate.net/publication/221621494_Support_Vector_Machines_Theory_and_Applications", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221621494_Support_Vector_<b>Machine</b>s", "snippet": "The principle used is <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) over a set of possible functions, called hypothesis space. Formally this can be written as minimizing the <b>empirical</b> . error: \u2211 = l. 1 i. x ...", "dateLastCrawled": "2022-02-02T02:47:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "ToyotaTechnologicalInstituteatChicago UniversityofTexasatAustin surbhi ...", "url": "https://arxiv.org/pdf/2005.07652", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/pdf/2005.07652", "snippet": "<b>Just as empirical risk minimization (ERM</b>) is central for non-robust PAC <b>learning</b>, a core component of adversarially robust <b>learning</b> is minimizing the robust empirical risk on a dataset S, \u02c6h \u2208 RERM U(S) ,argmin h\u2208H 1 m Xm i=1 sup z\u2208U(x) 1 [h(z) 6= y]. In this paper, we provide necessary and su\ufb03cient conditions on perturbation sets U, under which the robust empirical risk minimization (RERM) problem is e\ufb03ciently solvable in the realizable setting. We show that an e\ufb03cient ...", "dateLastCrawled": "2021-10-06T13:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Efficiently Learning Adversarially Robust Halfspaces with</b> Noise | DeepAI", "url": "https://deepai.org/publication/efficiently-learning-adversarially-robust-halfspaces-with-noise", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>efficiently-learning-adversarially-robust-halfspaces</b>...", "snippet": "<b>Just as empirical risk minimization (ERM</b>) is central for non-robust PAC <b>learning</b>, a core component of adversarially robust <b>learning</b> is minimizing the robust empirical risk on a dataset S, ^ h \u2208 R E R M U ( S ) \u225c argmin h \u2208 H 1 m m \u2211 i = 1 sup z \u2208 U ( x ) 1 [ h ( z ) \u2260 y ] .", "dateLastCrawled": "2021-12-05T23:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Efficiently <b>Learning</b> Adversarially Robust Halfspaces with Noise", "url": "http://proceedings.mlr.press/v119/montasser20a/montasser20a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v119/montasser20a/montasser20a.pdf", "snippet": "remains a major challenge in <b>machine</b> <b>learning</b>. A line of work has shown that predictors learned by deep neural networks are not robust to adversarial examples (Szegedy et al.,2014;Biggio et al.,2013;Goodfellow et al.,2015). This has led to a long line of research studying different aspects of robustness to adversarial examples. In this paper, we consider the problem of distribution-independent <b>learning</b> of halfspaces that are robust to ad-versarial examples at test time, also referred to as ...", "dateLastCrawled": "2021-11-21T12:03:00.0000000Z", "language": "en", "isNavigational": false}], [], []], "all_bing_queries": ["+(empirical risk minimization (erm))  is like +(machine learning technique)", "+(empirical risk minimization (erm)) is similar to +(machine learning technique)", "+(empirical risk minimization (erm)) can be thought of as +(machine learning technique)", "+(empirical risk minimization (erm)) can be compared to +(machine learning technique)", "machine learning +(empirical risk minimization (erm) AND analogy)", "machine learning +(\"empirical risk minimization (erm) is like\")", "machine learning +(\"empirical risk minimization (erm) is similar\")", "machine learning +(\"just as empirical risk minimization (erm)\")", "machine learning +(\"empirical risk minimization (erm) can be thought of as\")", "machine learning +(\"empirical risk minimization (erm) can be compared to\")"]}