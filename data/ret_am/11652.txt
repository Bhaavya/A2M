{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Understanding Machine Learning Theory Algorithms</b> - Discover the <b>Best</b> ...", "url": "https://www.scribd.com/document/286988649/Understanding-Machine-Learning-Theory-Algorithms", "isFamilyFriendly": true, "displayUrl": "https://www.scribd.com/document/286988649/<b>Understanding-Machine-Learning-Theory-Algorithms</b>", "snippet": "We describe the <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>), Structural <b>Risk</b> <b>Minimization</b> (SRM), and Minimum Description Length (MDL) learning rules, which shows how can a machine learn. We quantify the amount of data needed for learning using the <b>ERM</b>, SRM, and MDL rules and show how learning might fail by deriving viii. a no-free-lunch theorem. We also discuss how much computation time is required for learning. In the second part of the book we describe various learning algorithms. For some of the ...", "dateLastCrawled": "2022-02-02T23:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Science topic - ResearchGate | <b>Find</b> and share research", "url": "https://www.researchgate.net/topic/Logistic-Regression/6", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/topic/Logistic-Regression/6", "snippet": "<b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) is a principle in statistical learning theory which defines a family of learning algorithms and is used to give theoretical bounds on the performance of learning ...", "dateLastCrawled": "2022-01-07T02:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Understanding Machine Learning: From Theory</b> to Algorithms [pdf] [PDF ...", "url": "https://authorzilla.com/JjpMG/understanding-machine-learning-from-theory-to-algorithms-pdf.html", "isFamilyFriendly": true, "displayUrl": "https://authorzilla.com/JjpMG/<b>understanding-machine-learning-from-theory</b>-to-algorithms...", "snippet": "This learning paradigm coming up with a predictor h that minimizes LS (h) is called <b>Empirical</b> <b>Risk</b> <b>Minimization</b> or <b>ERM</b> for short. 2.2.1 Something May Go Wrong Overfitting Although the <b>ERM</b> rule seems very natural, without being careful, this approach may fail miserably. To demonstrate such a failure, let us go back to the problem of learning to . 32 36 A Gentle Start predict the taste of a papaya on the basis of its softness and color. Consider a sample as depicted in the following: Assume ...", "dateLastCrawled": "2022-02-03T04:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Causality for Machine Learning", "url": "https://ff13.fastforwardlabs.com/", "isFamilyFriendly": true, "displayUrl": "https://ff13.fastforwardlabs.com", "snippet": "When we treated the problem with <b>empirical</b> <b>risk</b> <b>minimization</b> (minimizing the cross-entropy between classes), we found good performance in the train environments, but very poor performance in the test environment. We report the metrics over 120 epochs of training in the table below. The <b>best</b> test accuracy is achieved at epoch 40, after which <b>ERM</b> (<b>empirical</b> <b>risk</b> <b>minimization</b>) begins to overfit. In the case of IRM (invariant <b>risk</b> <b>minimization</b>), we paid a small price in train set accuracy, but ...", "dateLastCrawled": "2022-02-01T19:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Probabilistic Machine Learning: An Introduction [Illustrated] - EBIN.PUB", "url": "https://ebin.pub/probabilistic-machine-learning-an-introduction-illustrated.html", "isFamilyFriendly": true, "displayUrl": "https://ebin.pub/probabilistic-machine-learning-an-introduction-illustrated.html", "snippet": "4.3 <b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) 4.3.1 Example: minimizing the misclassification rate 4.3.2 Surrogate loss 4.4 Other estimation methods * 4.4.1 The method of moments 4.4.2 Online (recursive) estimation 4.5 Regularization 4.5.1 Example: MAP estimation for the Bernoulli distribution 4.5.2 Example: MAP estimation for the multivariate Gaussian * 4.5.3 Example: weight decay 4.5.4 Picking the regularizer using a validation set 4.5.5 Cross-validation 4.5.6 Early stopping 4.5.7 Using more data ...", "dateLastCrawled": "2022-01-18T20:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) <b>Understanding Machine Learning: From Theory</b> to Algorithms | Keep ...", "url": "https://www.academia.edu/40679311/Understanding_Machine_Learning_From_Theory_to_Algorithms", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/40679311/<b>Understanding_Machine_Learning_From_Theory</b>_to_Algorithms", "snippet": "Academia.edu is a platform for academics to share research papers.", "dateLastCrawled": "2022-01-23T01:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) 246616207-Understanding-Machine-Learning.pdf | moch chamadani ...", "url": "https://www.academia.edu/31654804/246616207_Understanding_Machine_Learning_pdf", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/31654804/246616207_Understanding_Machine_Learning_pdf", "snippet": "246616207-Understanding-Machine-Learning.pdf", "dateLastCrawled": "2022-01-29T09:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "International Journal of Scientific &amp; Technology Research - IJSTR.ORG", "url": "https://www.ijstr.org/research-paper-publishing.php?month=sep2019", "isFamilyFriendly": true, "displayUrl": "https://www.ijstr.org/research-paper-publishing.php?month=sep2019", "snippet": "This research was aimed <b>to find</b> the susceptibility index value of stress and heritability in the character of tomato varieties in aluminum-<b>treatment</b> with nutrient culture media. This research was conducted in the greenhouse of the Faculty of Agriculture, Universitas Sumatera Utara on December 2017 to February 2018. This research was used factorial Randomized Block Design (RBD) with the first factor of tomato varieties (SL1 = Timoty; SL2 = Fortuna; SL3 = Pandawa Lima; SL4 = Citra Asia; SL5 ...", "dateLastCrawled": "2022-01-30T18:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Constraint Classification: A New Approach to Multiclass Classification</b> ...", "url": "https://www.researchgate.net/publication/221393906_Constraint_Classification_A_New_Approach_to_Multiclass_Classification", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221393906_Constraint_Classification_A_New...", "snippet": "This quality gap can have serious health consequences and major implications <b>for patient</b>\u2019s timely and correct <b>treatment</b>. These deficiencies can manifest, for example, as a lack of quality ...", "dateLastCrawled": "2022-01-03T07:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "college paper writing service - EduHawks.com", "url": "https://eduhawks.com/paper-help/paper-writing-service/college-paper-writing-service/", "isFamilyFriendly": true, "displayUrl": "https://eduhawks.com/paper-help/paper-writing-service/college-paper-writing-service", "snippet": "The most important <b>risk</b> associated to the business is that it is a large project, and if the marketing will not be done appropriately then the <b>patient</b> will not come to the business. Secondly, the cultural factor would be a <b>risk</b> as most of the people in Saudi Arab do not allow their women to go to the male doctors. Moreover, it is expected that <b>to find</b> the professional staff would be a challenge, because in Saudi Arab, it would be hard <b>to find</b> the medical and paramedical staff.", "dateLastCrawled": "2021-08-10T01:12:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Understanding Machine Learning Theory Algorithms</b> - Discover the <b>Best</b> ...", "url": "https://www.scribd.com/document/286988649/Understanding-Machine-Learning-Theory-Algorithms", "isFamilyFriendly": true, "displayUrl": "https://www.scribd.com/document/286988649/<b>Understanding-Machine-Learning-Theory-Algorithms</b>", "snippet": "We describe the <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>), Structural <b>Risk</b> <b>Minimization</b> (SRM), and Minimum Description Length (MDL) learning rules, which shows how can a machine learn. We quantify the amount of data needed for learning using the <b>ERM</b>, SRM, and MDL rules and show how learning might fail by deriving viii. a no-free-lunch theorem. We also discuss how much computation time is required for learning. In the second part of the book we describe various learning algorithms. For some of the ...", "dateLastCrawled": "2022-02-02T23:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Probabilistic Machine Learning: An Introduction [Illustrated] - EBIN.PUB", "url": "https://ebin.pub/probabilistic-machine-learning-an-introduction-illustrated.html", "isFamilyFriendly": true, "displayUrl": "https://ebin.pub/probabilistic-machine-learning-an-introduction-illustrated.html", "snippet": "4.3 <b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) 4.3.1 Example: minimizing the misclassification rate 4.3.2 Surrogate loss 4.4 Other estimation methods * 4.4.1 The method of moments 4.4.2 Online (recursive) estimation 4.5 Regularization 4.5.1 Example: MAP estimation for the Bernoulli distribution 4.5.2 Example: MAP estimation for the multivariate Gaussian * 4.5.3 Example: weight decay 4.5.4 Picking the regularizer using a validation set 4.5.5 Cross-validation 4.5.6 Early stopping 4.5.7 Using more data ...", "dateLastCrawled": "2022-01-18T20:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Understanding Machine Learning: From Theory</b> to Algorithms [pdf] [PDF ...", "url": "https://authorzilla.com/JjpMG/understanding-machine-learning-from-theory-to-algorithms-pdf.html", "isFamilyFriendly": true, "displayUrl": "https://authorzilla.com/JjpMG/<b>understanding-machine-learning-from-theory</b>-to-algorithms...", "snippet": "This learning paradigm coming up with a predictor h that minimizes LS (h) is called <b>Empirical</b> <b>Risk</b> <b>Minimization</b> or <b>ERM</b> for short. 2.2.1 Something May Go Wrong Overfitting Although the <b>ERM</b> rule seems very natural, without being careful, this approach may fail miserably. To demonstrate such a failure, let us go back to the problem of learning to . 32 36 A Gentle Start predict the taste of a papaya on the basis of its softness and color. Consider a sample as depicted in the following: Assume ...", "dateLastCrawled": "2022-02-03T04:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Causality for Machine Learning", "url": "https://ff13.fastforwardlabs.com/", "isFamilyFriendly": true, "displayUrl": "https://ff13.fastforwardlabs.com", "snippet": "When we treated the problem with <b>empirical</b> <b>risk</b> <b>minimization</b> (minimizing the cross-entropy between classes), we found good performance in the train environments, but very poor performance in the test environment. We report the metrics over 120 epochs of training in the table below. The <b>best</b> test accuracy is achieved at epoch 40, after which <b>ERM</b> (<b>empirical</b> <b>risk</b> <b>minimization</b>) begins to overfit. In the case of IRM (invariant <b>risk</b> <b>minimization</b>), we paid a small price in train set accuracy, but ...", "dateLastCrawled": "2022-02-01T19:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "PDF | Machine Learning | Statistics - Discover the <b>Best</b> eBooks ...", "url": "https://www.scribd.com/document/246616207/Understanding-Machine-Learning", "isFamilyFriendly": true, "displayUrl": "https://www.scribd.com/document/246616207", "snippet": "We describe the <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>), Structural <b>Risk</b> <b>Minimization</b> (SRM), and Minimum Description ... a few significant differences of emphasis; if a <b>doctor</b> comes up with the hypothesis that there is a correlation between smoking and heart disease, it is the statisticians role to view samples of patients and check the validity of that hypothesis (this is the common statistical task of hypothesis testing). In contrast, machine learning aims to use the data gathered from samples ...", "dateLastCrawled": "2022-01-29T10:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Science topic - ResearchGate | <b>Find</b> and share research", "url": "https://www.researchgate.net/topic/Logistic-Regression/6", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/topic/Logistic-Regression/6", "snippet": "<b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) is a principle in statistical learning theory which defines a family of learning algorithms and is used to give theoretical bounds on the performance of learning ...", "dateLastCrawled": "2022-01-07T02:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Longevity risk and capital markets</b>: The 2019-20 update - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S0167668721000640", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0167668721000640", "snippet": "The two reinsurers shared the <b>risk</b> equally and the use of the captive ICC vehicle meant that no insurer intermediary was required, making the deal more cost-effective for the pension fund. 43 Also in September, the British Airways&#39; Airways Pension Scheme used a <b>similar</b> Guernsey-based captive insurer to set up a \u00a31.7bn longevity swap. The longevity <b>risk</b> was then reinsured with Partner Re and Canada Life Re. The scheme had previously hedged \u00a32.6bn of liabilities through two longevity swap ...", "dateLastCrawled": "2022-01-26T15:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "International Journal of Scientific &amp; Technology Research - IJSTR.ORG", "url": "https://www.ijstr.org/research-paper-publishing.php?month=sep2019", "isFamilyFriendly": true, "displayUrl": "https://www.ijstr.org/research-paper-publishing.php?month=sep2019", "snippet": "This research was aimed <b>to find</b> the susceptibility index value of stress and heritability in the character of tomato varieties in aluminum-<b>treatment</b> with nutrient culture media. This research was conducted in the greenhouse of the Faculty of Agriculture, Universitas Sumatera Utara on December 2017 to February 2018. This research was used factorial Randomized Block Design (RBD) with the first factor of tomato varieties (SL1 = Timoty; SL2 = Fortuna; SL3 = Pandawa Lima; SL4 = Citra Asia; SL5 ...", "dateLastCrawled": "2022-01-30T18:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "SAP GRC for Dummies (ISBN - 0470333170) - PDF Free Download", "url": "https://docer.tips/sap-grc-for-dummies-isbn-0470333170.html", "isFamilyFriendly": true, "displayUrl": "https://docer.tips/sap-grc-for-dummies-isbn-0470333170.html", "snippet": "Discovering <b>Enterprise Risk Management</b> <b>Enterprise risk management</b> provides a framework for your business to identify your risks and decide which risks are likely to happen and what the associated impact will be for your business. By managing this type of information, you can better protect the value that you have created across your business because you are more aligned with your risks and can mitigate the potential impact of those risks.", "dateLastCrawled": "2022-01-03T01:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(PDF) 246616207-Understanding-Machine-Learning.pdf | moch chamadani ...", "url": "https://www.academia.edu/31654804/246616207_Understanding_Machine_Learning_pdf", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/31654804/246616207_Understanding_Machine_Learning_pdf", "snippet": "246616207-Understanding-Machine-Learning.pdf", "dateLastCrawled": "2022-01-29T09:06:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "PDF | Machine Learning | Statistics - Discover the <b>Best</b> eBooks ...", "url": "https://www.scribd.com/document/246616207/Understanding-Machine-Learning", "isFamilyFriendly": true, "displayUrl": "https://www.scribd.com/document/246616207", "snippet": "We describe the <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>), Structural <b>Risk</b> <b>Minimization</b> (SRM), and Minimum ... playing the role of the teacher, <b>can</b> be <b>best</b> <b>thought</b> of as passive apples drop, stars shine, and the rain falls without regard to the needs of the learner. We model such learning scenarios by postulating that the training data (or the learners experience) is generated by some random process. This is the basic building block in the branch of statistical learning. Finally, learning also ...", "dateLastCrawled": "2022-01-29T10:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Understanding Machine Learning Theory Algorithms</b> - Discover the <b>Best</b> ...", "url": "https://www.scribd.com/document/286988649/Understanding-Machine-Learning-Theory-Algorithms", "isFamilyFriendly": true, "displayUrl": "https://www.scribd.com/document/286988649/<b>Understanding-Machine-Learning-Theory-Algorithms</b>", "snippet": "We describe the <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>), Structural <b>Risk</b> <b>Minimization</b> (SRM), and Minimum Description Length (MDL ) learning rules, which shows how <b>can</b> a machine learn. We quantify the amount of data needed for learning using the <b>ERM</b>, SRM, and MDL rules and show how learning might fail by deriving viii. a no-free-lunch theorem. We also discuss how much computation time is required for learning. In the second part of the book we describe various learning algorithms. For some of the ...", "dateLastCrawled": "2022-02-02T23:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Probabilistic Machine Learning: An Introduction [Illustrated] - EBIN.PUB", "url": "https://ebin.pub/probabilistic-machine-learning-an-introduction-illustrated.html", "isFamilyFriendly": true, "displayUrl": "https://ebin.pub/probabilistic-machine-learning-an-introduction-illustrated.html", "snippet": "4.3 <b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) 4.3.1 Example: minimizing the misclassification rate 4.3.2 Surrogate loss 4.4 Other estimation methods * 4.4.1 The method of moments 4.4.2 Online (recursive) estimation 4.5 Regularization 4.5.1 Example: MAP estimation for the Bernoulli distribution 4.5.2 Example: MAP estimation for the multivariate Gaussian * 4.5.3 Example: weight decay 4.5.4 Picking the regularizer using a validation set 4.5.5 Cross-validation 4.5.6 Early stopping 4.5.7 Using more data ...", "dateLastCrawled": "2022-01-18T20:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Causality for Machine Learning", "url": "https://ff13.fastforwardlabs.com/", "isFamilyFriendly": true, "displayUrl": "https://ff13.fastforwardlabs.com", "snippet": "The <b>best</b> test accuracy is achieved at epoch 40, after which <b>ERM</b> (<b>empirical</b> <b>risk</b> <b>minimization</b>) begins to overfit. In the case of IRM (invariant <b>risk</b> <b>minimization</b>), we paid a small price in train set accuracy, but achieved much better test results - again, reporting the highest test accuracy achieved in 120 epochs (at epoch 120).", "dateLastCrawled": "2022-02-01T19:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Understanding Machine Learning: From Theory</b> to Algorithms [pdf] [PDF ...", "url": "https://authorzilla.com/JjpMG/understanding-machine-learning-from-theory-to-algorithms-pdf.html", "isFamilyFriendly": true, "displayUrl": "https://authorzilla.com/JjpMG/<b>understanding-machine-learning-from-theory</b>-to-algorithms...", "snippet": "Intuitively, overfitting occurs when our hypothesis fits the training data too well (perhaps like the everyday experience that a person who provides a perfect detailed explanation for each of his single actions may raise suspicion). 2.3 <b>Empirical</b> <b>Risk</b> <b>Minimization</b> with Inductive Bias We have just demonstrated that the <b>ERM</b> rule might lead to overfitting. Rather than giving up on the <b>ERM</b> paradigm, we will look for ways to rectify it. We will search for conditions under which there is a ...", "dateLastCrawled": "2022-02-03T04:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) <b>Understanding Machine Learning: From Theory</b> to Algorithms | Keep ...", "url": "https://www.academia.edu/40679311/Understanding_Machine_Learning_From_Theory_to_Algorithms", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/40679311/<b>Understanding_Machine_Learning_From_Theory</b>_to_Algorithms", "snippet": "Academia.edu is a platform for academics to share research papers.", "dateLastCrawled": "2022-01-23T01:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Longevity risk and capital markets</b>: The 2019-20 update - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S0167668721000640", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0167668721000640", "snippet": "At the same time, capital markets <b>can</b>, in principle, provide vehicles to hedge longevity <b>risk</b> effectively and transfer the <b>risk</b> from those unwilling or unable to manage it to those willing to invest in this <b>risk</b> in exchange for appropriate <b>risk</b>-adjusted returns or to those who have a counterpoising <b>risk</b> that longevity <b>risk</b> <b>can</b> hedge, e.g., life offices and reinsurers with mortality <b>risk</b> on their books. Many new investment products have been created both by the insurance/reinsurance industry ...", "dateLastCrawled": "2022-01-26T15:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "International Journal of Scientific &amp; Technology Research - IJSTR.ORG", "url": "https://www.ijstr.org/research-paper-publishing.php?month=sep2019", "isFamilyFriendly": true, "displayUrl": "https://www.ijstr.org/research-paper-publishing.php?month=sep2019", "snippet": "This research was aimed <b>to find</b> the susceptibility index value of stress and heritability in the character of tomato varieties in aluminum-<b>treatment</b> with nutrient culture media. This research was conducted in the greenhouse of the Faculty of Agriculture, Universitas Sumatera Utara on December 2017 to February 2018. This research was used factorial Randomized Block Design (RBD) with the first factor of tomato varieties (SL1 = Timoty; SL2 = Fortuna; SL3 = Pandawa Lima; SL4 = Citra Asia; SL5 ...", "dateLastCrawled": "2022-01-30T18:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Cyber Security Awareness Campaigns: Why do</b> they fail to change ...", "url": "https://www.researchgate.net/publication/336676387_Cyber_Security_Awareness_Campaigns_Why_do_they_fail_to_change_behaviour", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/336676387_<b>Cyber_Security_Awareness_Campaigns</b>...", "snippet": "Cultural differences in <b>risk</b> perceptions <b>can</b> also influence the maintenance of a particular way of life. Finally, since the vast majority of behaviours are habitual, the change from existing ...", "dateLastCrawled": "2021-12-24T00:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "college paper writing service - EduHawks.com", "url": "https://eduhawks.com/paper-help/paper-writing-service/college-paper-writing-service/", "isFamilyFriendly": true, "displayUrl": "https://eduhawks.com/paper-help/paper-writing-service/college-paper-writing-service", "snippet": "The process will start as the <b>patient</b> will arrange meeting with the CRM officer and according to the meeting time, the <b>patient</b> will meet the senior <b>doctor</b>. Senior <b>doctor</b> will decide whether the <b>patient</b> has to admit or not. In case of admission, the required medical tests will be taken by the Paramedical staff, the technologist officer will develop a report and then on the advice of Senior and junior doctors, the process of <b>treatment</b> will take place. Following are some of the important ...", "dateLastCrawled": "2021-08-10T01:12:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Understanding Machine Learning Theory Algorithms</b> - Discover the <b>Best</b> ...", "url": "https://www.scribd.com/document/286988649/Understanding-Machine-Learning-Theory-Algorithms", "isFamilyFriendly": true, "displayUrl": "https://www.scribd.com/document/286988649/<b>Understanding-Machine-Learning-Theory-Algorithms</b>", "snippet": "We describe the <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>), Structural <b>Risk</b> <b>Minimization</b> (SRM), and Minimum Description Length (MDL ) learning rules, which shows how <b>can</b> a machine learn. We quantify the amount of data needed for learning using the <b>ERM</b>, SRM, and MDL rules and show how learning might fail by deriving viii. a no-free-lunch theorem. We also discuss how much computation time is required for learning. In the second part of the book we describe various learning algorithms. For some of the ...", "dateLastCrawled": "2022-02-02T23:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Document 11252736", "url": "https://studylib.net/doc/11252736/", "isFamilyFriendly": true, "displayUrl": "https://studylib.net/doc/11252736", "snippet": "The third part of this thesis provides a solution to an open problem regarding the stability of <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>). This algorithm is of central importance in Learning Theory. By studying the suprema of the <b>empirical</b> process, we prove that <b>ERM</b> over Donsker classes of functions is stable in the L1 norm.", "dateLastCrawled": "2021-12-07T23:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "PDF | Machine Learning | Statistics - Discover the <b>Best</b> eBooks ...", "url": "https://www.scribd.com/document/246616207/Understanding-Machine-Learning", "isFamilyFriendly": true, "displayUrl": "https://www.scribd.com/document/246616207", "snippet": "We describe the <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>), Structural <b>Risk</b> <b>Minimization</b> (SRM), and Minimum ... playing the role of the teacher, <b>can</b> be <b>best</b> thought of as passive apples drop, stars shine, and the rain falls without regard to the needs of the learner. We model such learning scenarios by postulating that the training data (or the learners experience) is generated by some random process. This is the basic building block in the branch of statistical learning. Finally, learning also ...", "dateLastCrawled": "2022-01-29T10:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Causality for Machine Learning", "url": "https://ff13.fastforwardlabs.com/", "isFamilyFriendly": true, "displayUrl": "https://ff13.fastforwardlabs.com", "snippet": "The <b>best</b> test accuracy is achieved at epoch 40, after which <b>ERM</b> (<b>empirical</b> <b>risk</b> <b>minimization</b>) begins to overfit. In the case of IRM (invariant <b>risk</b> <b>minimization</b>), we paid a small price in train set accuracy, but achieved much better test results - again, reporting the highest test accuracy achieved in 120 epochs (at epoch 120).", "dateLastCrawled": "2022-02-01T19:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Science topic - ResearchGate | <b>Find</b> and share research", "url": "https://www.researchgate.net/topic/Logistic-Regression/6", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/topic/Logistic-Regression/6", "snippet": "<b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) is a principle in statistical learning theory which defines a family of learning algorithms and is used to give theoretical bounds on the performance of learning ...", "dateLastCrawled": "2022-01-07T02:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Probabilistic Machine Learning: An Introduction [Illustrated] - DOKUMEN.PUB", "url": "https://dokumen.pub/probabilistic-machine-learning-an-introduction-illustrated.html", "isFamilyFriendly": true, "displayUrl": "https://dokumen.pub/probabilistic-machine-learning-an-introduction-illustrated.html", "snippet": "4.3 <b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) 4.3.1 Example: minimizing the misclassification rate 4.3.2 Surrogate loss 4.4 Other estimation methods * 4.4.1 The method of moments 4.4.2 Online (recursive) estimation 4.5 Regularization 4.5.1 Example: MAP estimation for the Bernoulli distribution 4.5.2 Example: MAP estimation for the multivariate Gaussian * 4.5.3 Example: weight decay 4.5.4 Picking the regularizer using a validation set 4.5.5 Cross-validation 4.5.6 Early stopping 4.5.7 Using more data ...", "dateLastCrawled": "2022-02-02T23:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Understanding Machine Learning: From Theory</b> to Algorithms [pdf] [PDF ...", "url": "https://authorzilla.com/JjpMG/understanding-machine-learning-from-theory-to-algorithms-pdf.html", "isFamilyFriendly": true, "displayUrl": "https://authorzilla.com/JjpMG/<b>understanding-machine-learning-from-theory</b>-to-algorithms...", "snippet": "Intuitively, overfitting occurs when our hypothesis fits the training data too well (perhaps like the everyday experience that a person who provides a perfect detailed explanation for each of his single actions may raise suspicion). 2.3 <b>Empirical</b> <b>Risk</b> <b>Minimization</b> with Inductive Bias We have just demonstrated that the <b>ERM</b> rule might lead to overfitting. Rather than giving up on the <b>ERM</b> paradigm, we will look for ways to rectify it. We will search for conditions under which there is a ...", "dateLastCrawled": "2022-02-03T04:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A <b>generic optimising feature extraction method</b> using multiobjective ...", "url": "https://www.researchgate.net/publication/222830249_A_generic_optimising_feature_extraction_method_using_multiobjective_genetic_programming", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/222830249_A_generic_optimising_feature...", "snippet": "We demonstrate the statistical superiority of VRM training over conventional <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) and the well-known C4.5 algorithm, for a range of synthetic and real datasets. We ...", "dateLastCrawled": "2022-01-19T00:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) 246616207-Understanding-Machine-Learning.pdf | moch chamadani ...", "url": "https://www.academia.edu/31654804/246616207_Understanding_Machine_Learning_pdf", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/31654804/246616207_Understanding_Machine_Learning_pdf", "snippet": "246616207-Understanding-Machine-Learning.pdf", "dateLastCrawled": "2022-01-29T09:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "International Journal of Scientific &amp; Technology Research - IJSTR.ORG", "url": "https://www.ijstr.org/research-paper-publishing.php?month=sep2019", "isFamilyFriendly": true, "displayUrl": "https://www.ijstr.org/research-paper-publishing.php?month=sep2019", "snippet": "This research was aimed <b>to find</b> the susceptibility index value of stress and heritability in the character of tomato varieties in aluminum-<b>treatment</b> with nutrient culture media. This research was conducted in the greenhouse of the Faculty of Agriculture, Universitas Sumatera Utara on December 2017 to February 2018. This research was used factorial Randomized Block Design (RBD) with the first factor of tomato varieties (SL1 = Timoty; SL2 = Fortuna; SL3 = Pandawa Lima; SL4 = Citra Asia; SL5 ...", "dateLastCrawled": "2022-01-30T18:57:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Computational and Statistical <b>Learning</b> Theory", "url": "https://home.ttic.edu/~nati/Teaching/TTIC31120/2015/Lecture17.pdf", "isFamilyFriendly": true, "displayUrl": "https://home.ttic.edu/~nati/Teaching/TTIC31120/2015/Lecture17.pdf", "snippet": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) / Sample Average Approximation (SAA): Collect sample z1UYU zm ... SGD for <b>Machine</b> <b>Learning</b> Initialize S 4 L r At iteration t: Draw T \u00e7\u00e1U \u00e71\u00de If U \u00e7 S \u00e7 \u00e1\u00f6 T \u00e7 O s\u00e1 S \u00e7 &gt; 5 Z S \u00e7 E\u00df \u00e7U \u00e7\u00f6 T \u00e7 else: S \u00e7 &gt; 5 Z S \u00e7 Return S % \u00cd L 5 \u00cd \u00c3 \u00cd S \u00e7 \u00e7 @ 5 Draw T 5\u00e1U 5 \u00e1\u00e5\u00e1 T \u00e0 \u00e1U \u00e0 1\u00de Initialize S 4 L r At iteration t: Pick E \u00d0 s\u00e5I at random If U \u00dc S \u00e7 \u00e1\u00f6 T \u00dc O s\u00e1 S \u00e7 &gt; 5 Z S \u00e7 E\u00df \u00e7U \u00dc\u00f6 T \u00dc else: S \u00e7 &gt; 5 Z S \u00e7 S \u00e7 &gt; 5 Z ...", "dateLastCrawled": "2022-01-26T03:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Basics of <b>Machine</b> <b>Learning</b>", "url": "https://courses.cs.duke.edu/spring21/compsci527/slides/s_04_deep_learning.pdf", "isFamilyFriendly": true, "displayUrl": "https://courses.cs.duke.edu/spring21/compsci527/slides/s_04_deep_<b>learning</b>.pdf", "snippet": "This is called <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) COMPSCI 527 \u2014 Computer Vision Basics of <b>Machine</b> <b>Learning</b> 15/26. Loss and <b>Risk</b> <b>Machine</b> <b>Learning</b> and the Statistical <b>Risk</b> <b>ERM</b>: w^ 2argmin w2R m L T(w) In <b>machine</b> <b>learning</b>, we go much farther: We also want h to do well on previously unseen inputs To relate past and future data, assume that all data comes from the same joint probability distribution p(x;y) p is called the generative data model or just model The goal of <b>machine</b> <b>learning</b> is to ...", "dateLastCrawled": "2021-11-06T09:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Statistical <b>Learning</b> Theory and the C-Loss cost function", "url": "http://www.cnel.ufl.edu/courses/EEL6814/closs.pdf", "isFamilyFriendly": true, "displayUrl": "www.cnel.ufl.edu/courses/EEL6814/closs.pdf", "snippet": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) principle Let us consider a <b>learning</b> <b>machine</b> x,d are real r.v. with joint distribution P(x,y). F(x) is a function of some parameters w, i.e. f(x,w). d d. <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) principle How can we find the possible best <b>learning</b> <b>machine</b> that generalizes for unseen data from the same distribution? Define the <b>Risk</b> functional as L(.) is called the Loss function, and minimize it w.r.t. w achieving the best possible loss. But we can not do this ...", "dateLastCrawled": "2022-01-28T20:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Lecture 1: Reinforcement <b>Learning</b>: What and Why?", "url": "https://imada.sdu.dk/~kandemir/rl-lecture1.pdf", "isFamilyFriendly": true, "displayUrl": "https://imada.sdu.dk/~kandemir/rl-lecture1.pdf", "snippet": "<b>machine</b> <b>learning</b> and is referred to as <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>). 3 Challenges of reinforcement <b>learning</b> Consider the cart pole balancing problem, where a cart carrying an unactuated pole \ufb02oats on a straight horizontal track. The cart is actuated by a torque applied either to the right or the left direction. Seeherefor a real cart ...", "dateLastCrawled": "2021-09-30T12:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> and Stochastic Gradient Descent for ...", "url": "http://proceedings.mlr.press/v89/veitch19a/veitch19a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v89/veitch19a/veitch19a.pdf", "snippet": "where F(Sn) is the <b>empirical</b> distribution.2 The <b>ERM</b> dogma is to select the predictor \u03c0\u02c6\u03b8 n given by \u02c6\u03b8 n = argmin\u03b8 R\u02c6(\u03b8,Sn). That is, the objective function that de\ufb01nes <b>learning</b> is the <b>empirical</b> <b>risk</b>. <b>ERM</b> has two useful properties. (1) It provides a prin-cipled framework for de\ufb01ning new <b>machine</b> <b>learning</b> methods. In particular, when ...", "dateLastCrawled": "2021-09-18T06:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and-sample-codes ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/README.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "Loss function 3.1 Categories 3.2 <b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) and Structural <b>risk</b> <b>minimization</b> (SRM) 3.2.1 MLE 3.2.2 MAP 4. Evaluation metrics 4.1 Accuracy, precision, recall and F1 4.2 ROC, AUC 4.3 BLEU 5. Sample projects 6. Classical questions 6.1 Why is logistic regression a generalized linear model ?", "dateLastCrawled": "2021-09-22T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Topics in <b>Machine</b> <b>Learning</b> (TIML-09)", "url": "https://www.cse.iitb.ac.in/~saketh/teaching/cs689.html", "isFamilyFriendly": true, "displayUrl": "https://www.cse.iitb.ac.in/~saketh/teaching/cs689.html", "snippet": "Introduction to Statistical <b>Learning</b> Theory (SLT): Definitions of loss function, <b>risk</b>, <b>empirical</b> <b>risk</b>, motivation for <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) Further Reading, Supplementary: Jan 12: Consistency of <b>ERM</b>, Sufficient condition for <b>ERM</b> as one-sided uniform convergence, Analysis for finite sets of functions and extensions to general case using Symmetrization trick, Shattering Coeff. Further Reading, Supplementary: Jan 15: Shattering coeff., growth function, VC dimension, Annealed Entropy ...", "dateLastCrawled": "2022-01-11T09:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>GitHub</b> - gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "Loss function 3.1 Categories 3.2 <b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) and Structural <b>risk</b> <b>minimization</b> (SRM) 3.2.1 MLE 3.2.2 MAP 4. Evaluation metrics 4.1 Accuracy, precision, recall and F1 4.2 ROC, AUC 4.3 BLEU 5. Sample projects 6. Classical questions 6.1 Why is logistic regression a generalized linear model ?", "dateLastCrawled": "2021-09-12T01:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "[2006.09461] Robust <b>Compressed Sensing using Generative Models</b> - arXiv", "url": "https://arxiv.org/abs/2006.09461", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/abs/2006.09461", "snippet": "Classical recovery approaches such as <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) are guaranteed to succeed when the measurement matrix is sub-Gaussian. However, when the measurement matrix and measurements are heavy-tailed or have outliers, recovery may fail dramatically. In this paper we propose an algorithm inspired by the Median-of-Means (MOM). Our algorithm guarantees recovery for heavy-tailed data, even in the presence of outliers. Theoretically, our results show our novel MOM-based algorithm ...", "dateLastCrawled": "2021-06-27T11:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(PDF) <b>Support Vector Machines: Theory and Applications</b>", "url": "https://www.researchgate.net/publication/221621494_Support_Vector_Machines_Theory_and_Applications", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221621494_Support_Vector_<b>Machine</b>s", "snippet": "The principle used is <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) over a set of possible functions, called hypothesis space. Formally this can be written as minimizing the <b>empirical</b> . error: \u2211 = l. 1 i. x ...", "dateLastCrawled": "2022-02-02T02:47:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "ToyotaTechnologicalInstituteatChicago UniversityofTexasatAustin surbhi ...", "url": "https://arxiv.org/pdf/2005.07652", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/pdf/2005.07652", "snippet": "<b>Just as empirical risk minimization (ERM</b>) is central for non-robust PAC <b>learning</b>, a core component of adversarially robust <b>learning</b> is minimizing the robust empirical risk on a dataset S, \u02c6h \u2208 RERM U(S) ,argmin h\u2208H 1 m Xm i=1 sup z\u2208U(x) 1 [h(z) 6= y]. In this paper, we provide necessary and su\ufb03cient conditions on perturbation sets U, under which the robust empirical risk minimization (RERM) problem is e\ufb03ciently solvable in the realizable setting. We show that an e\ufb03cient ...", "dateLastCrawled": "2021-10-06T13:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Efficiently Learning Adversarially Robust Halfspaces with</b> Noise | DeepAI", "url": "https://deepai.org/publication/efficiently-learning-adversarially-robust-halfspaces-with-noise", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>efficiently-learning-adversarially-robust-halfspaces</b>...", "snippet": "<b>Just as empirical risk minimization (ERM</b>) is central for non-robust PAC <b>learning</b>, a core component of adversarially robust <b>learning</b> is minimizing the robust empirical risk on a dataset S, ^ h \u2208 R E R M U ( S ) \u225c argmin h \u2208 H 1 m m \u2211 i = 1 sup z \u2208 U ( x ) 1 [ h ( z ) \u2260 y ] .", "dateLastCrawled": "2021-12-05T23:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Efficiently <b>Learning</b> Adversarially Robust Halfspaces with Noise", "url": "http://proceedings.mlr.press/v119/montasser20a/montasser20a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v119/montasser20a/montasser20a.pdf", "snippet": "remains a major challenge in <b>machine</b> <b>learning</b>. A line of work has shown that predictors learned by deep neural networks are not robust to adversarial examples (Szegedy et al.,2014;Biggio et al.,2013;Goodfellow et al.,2015). This has led to a long line of research studying different aspects of robustness to adversarial examples. In this paper, we consider the problem of distribution-independent <b>learning</b> of halfspaces that are robust to ad-versarial examples at test time, also referred to as ...", "dateLastCrawled": "2021-11-21T12:03:00.0000000Z", "language": "en", "isNavigational": false}], [], []], "all_bing_queries": ["+(empirical risk minimization (erm))  is like +(doctor trying to find best treatment for patient)", "+(empirical risk minimization (erm)) is similar to +(doctor trying to find best treatment for patient)", "+(empirical risk minimization (erm)) can be thought of as +(doctor trying to find best treatment for patient)", "+(empirical risk minimization (erm)) can be compared to +(doctor trying to find best treatment for patient)", "machine learning +(empirical risk minimization (erm) AND analogy)", "machine learning +(\"empirical risk minimization (erm) is like\")", "machine learning +(\"empirical risk minimization (erm) is similar\")", "machine learning +(\"just as empirical risk minimization (erm)\")", "machine learning +(\"empirical risk minimization (erm) can be thought of as\")", "machine learning +(\"empirical risk minimization (erm) can be compared to\")"]}