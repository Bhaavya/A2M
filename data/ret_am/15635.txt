{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Coursera Deep Learning Module 4</b> Week 2 Notes | XAI - <b>XAI - eXplainable AI</b>", "url": "https://marcossilva.github.io/en/2019/08/04/coursera-deep-learning-module-4-week-2.html", "isFamilyFriendly": true, "displayUrl": "https://marcossilva.github.io/en/2019/08/04/<b>coursera-deep-learning-module-4</b>-week-2.html", "snippet": "Very, very deep neural networks are difficult to train because of vanishing and <b>exploding</b> <b>gradient</b> types of problems. Skip connections allows you to take the activation from one layer and feed it to another layer even much deeper in the neural network. Using that technique that ResNet was built upon enabling it to train very, very deep networks. Residual Block diagram. Why ResNets Work. As we can see below is easier for the network to learn the identity function which means that the network ...", "dateLastCrawled": "2022-01-29T11:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Solved: Hatching looking <b>Solid</b>, but its not a <b>Solid</b> pattern - <b>Autodesk</b> ...", "url": "https://forums.autodesk.com/t5/autocad-forum/hatching-looking-solid-but-its-not-a-solid-pattern/td-p/3735508", "isFamilyFriendly": true, "displayUrl": "https://<b>forums.autodesk.com</b>/t5/autocad-forum/<b>hatch</b>ing-looking-<b>solid</b>-but-its-not-a...", "snippet": "Finally, something may not be related to your question, I normally have the <b>following</b> variable all set to 1; LTSCALE, MSLTCALE and PSLTSCALE. When I work with <b>someone</b> <b>else&#39;s</b> drawing that has these set differently, I have to play around with them to get dashed linetypes and scaling to display correctly. The linetype scale is usually assigned to the lines, etc. when one or more of these variable aren&#39;t set to 1.", "dateLastCrawled": "2022-02-02T20:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "machine learning - Why are neural networks becoming deeper, but not ...", "url": "https://stats.stackexchange.com/questions/222883/why-are-neural-networks-becoming-deeper-but-not-wider", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/222883", "snippet": "For <b>example</b>, if you train a deep convolutional neural <b>network</b> to classify images, you will find that the first layer will train itself to recognize very basic things <b>like</b> edges, the next layer will train itself to recognize collections of edges such as shapes, the next layer will train itself to recognize collections of shapes <b>like</b> eyes or noses, and the next layer will learn even higher-order features <b>like</b> faces.", "dateLastCrawled": "2022-02-03T18:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>A Picture\u2019s Worth</b>", "url": "https://www.blackhat.com/presentations/bh-dc-08/Krawetz/Whitepaper/bh-dc-08-krawetz-WP.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.blackhat.com/presentations/bh-dc-08/Krawetz/Whitepaper/bh-dc-08-krawetz-WP.pdf", "snippet": "\u2022 Luminance <b>Gradient</b> (LG). An analysis method based on light intensity direction. 2 The <b>Problem</b> with Images Images have power. Whether it is the space shuttle <b>exploding</b> during launch, man walking on the moon, or soldiers raising a flag on Iwo Jima during World War II, powerful images influence society. The advent of sophisticated", "dateLastCrawled": "2022-01-28T22:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Simone Scardapane on Twitter: &quot;The field is <b>exploding</b>, too many ...", "url": "https://twitter.com/s_scardapane/status/1405187637900611586", "isFamilyFriendly": true, "displayUrl": "https://<b>twitter.com</b>/s_scardapane/status/1405187637900611586", "snippet": "Verified account Protected Tweets @; Suggested users Verified account Protected Tweets @ Protected Tweets @", "dateLastCrawled": "2022-01-30T04:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Check your english vocabulary for computing</b>", "url": "https://www.slideshare.net/CarmenLucero1/check-your-english-vocabulary-for-computing-79180226", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/CarmenLucero1/<b>check-your-english-vocabulary-for-computing</b>...", "snippet": "A person who illegally accesses somebody <b>else&#39;s</b> computer over the internet is called a _____. a. pirate b. hack c. hacker 2. A website which (in theory) cannot be accessed by a hacker is _____. a. strong b. secure c. clean 3. A website which can only be viewed by authorised people has _____ access. a. reduced b. small c. restricted 4. Unwanted advertising emails are popularly known as _____. a. meatloaf b. spam c. sausages 5. Software which blocks attempts by others to access your computer ...", "dateLastCrawled": "2022-01-31T20:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "New Apostolic Reformation Conspiracy Theory Debunked | God TV", "url": "https://godtv.com/new-apostolic-reformation-conspiracy-theory-debunked/", "isFamilyFriendly": true, "displayUrl": "https://godtv.com/new-apostolic-reformation-conspiracy-theory-debunked", "snippet": "It <b>is like</b> Grape Nuts, it\u2019s not grapes and it\u2019s not nuts, it\u2019s <b>like</b> Christian Science, it\u2019s not Christian and it\u2019s not scientific. Well, the New Apostolic Reformation isn\u2019t new, it isn\u2019t apostolic, and it isn\u2019t a reformation. But it is a rapidly expanding movement being generated by some of the same old troubling false teachers and false leaders that have been around in Charismania for decades, always dishonoring the Holy Spirit, always dishonoring the Scripture, always ...", "dateLastCrawled": "2022-02-02T22:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>02_deep-convolutional-models-case-studies</b> | SnailDove&#39;s blog", "url": "https://snaildove.github.io/2018/05/01/02_deep-convolutional-models-case-studies/", "isFamilyFriendly": true, "displayUrl": "https://snaildove.github.io/2018/05/01/<b>02_deep-convolutional-models-case-studies</b>", "snippet": "You might well be able to take <b>someone</b> <b>else\u2019s</b> neural network architecture and apply that to your <b>problem</b>. And finally, after the next few videos, you\u2019ll be able to read some of the research papers from the theater computer vision and I hope that you might find it satisfying as well. You don\u2019t have to do this as a class but I hope you might find it satisfying to be able to read some of these seminal computer vision research paper and see yourself able to understand them. So with that ...", "dateLastCrawled": "2021-10-14T13:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Which of the algorithm does not have a worst case running time of 0(n\u00b2 ...", "url": "https://www.quora.com/Which-of-the-algorithm-does-not-have-a-worst-case-running-time-of-0-n%C2%B2", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Which-of-the-algorithm-does-not-have-a-worst-case-running-time-of...", "snippet": "Answer (1 of 6): There are many Algorithms. Here is List 1. Merge Sort 2. Searching in AVL Tree (Tree is balanced) 3. Heap Sort 4. Binary Search (logn) There are many Algorithms which have worst case complexity of nlogn, n , logn. I didnt got what exactly you want to ask, do u want list of thes...", "dateLastCrawled": "2022-01-16T11:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What is the best/most classic paper to cite for L2 regularization of ...", "url": "https://www.quora.com/What-is-the-best-most-classic-paper-to-cite-for-L2-regularization-of-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-the-best-most-classic-paper-to-cite-for-L2...", "snippet": "Answer (1 of 2): If believe that regularisation was often framed as \u2018weight decay\u2019 in the older work on neural networks. See for <b>example</b> https://papers.nips.cc ...", "dateLastCrawled": "2022-01-21T04:08:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Coursera Deep Learning Module 4</b> Week 2 Notes | XAI - <b>XAI - eXplainable AI</b>", "url": "https://marcossilva.github.io/en/2019/08/04/coursera-deep-learning-module-4-week-2.html", "isFamilyFriendly": true, "displayUrl": "https://marcossilva.github.io/en/2019/08/04/<b>coursera-deep-learning-module-4</b>-week-2.html", "snippet": "Very, very deep neural networks are difficult to train because of vanishing and <b>exploding</b> <b>gradient</b> types of problems. Skip connections allows you to take the activation from one layer and feed it to another layer even much deeper in the neural network. Using that technique that ResNet was built upon enabling it to train very, very deep networks. Residual Block diagram. Why ResNets Work. As we can see below is easier for the network to learn the identity function which means that the network ...", "dateLastCrawled": "2022-01-29T11:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Biology-Inspired AGI Timelines: The Trick That Never Works - Machine ...", "url": "https://intelligence.org/2021/12/03/biology-inspired-agi-timelines-the-trick-that-never-works/", "isFamilyFriendly": true, "displayUrl": "https://intelligence.org/2021/12/03/biology-inspired-agi-timelines-the-trick-that...", "snippet": "For reference, recall that in 2006, Hinton and Salakhutdinov were just starting to publish that, by training multiple layers of Restricted Boltzmann machines and then unrolling them into a \u201cdeep\u201d neural network, you could get an initialization for the network weights that would avoid the <b>problem</b> of vanishing and <b>exploding</b> gradients and activations. At least so long as you didn\u2019t try to stack too many layers, like a dozen layers or something ridiculous like that. This being the point ...", "dateLastCrawled": "2022-01-25T14:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Yield Thought", "url": "https://yieldthought.com/", "isFamilyFriendly": true, "displayUrl": "https://yieldthought.com", "snippet": "It could be any of: * Hyperparameters need to be more carefully tuned (algorithms can be rather sensitive even within <b>similar</b> domains) * Initialization is incorrect and weights are dropping to zero (vanishing <b>gradient</b> <b>problem</b>) or are becoming unstable (<b>exploding</b> <b>gradient</b> <b>problem</b>) - these at least you can check by producing images of the weights and staring hard at them, like astrologers seeking meaning in the stars. * The input is not preprocessed, normalized or augmented enough. Or it\u2019s", "dateLastCrawled": "2022-01-29T12:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>02_deep-convolutional-models-case-studies</b> | SnailDove&#39;s blog", "url": "https://snaildove.github.io/2018/05/01/02_deep-convolutional-models-case-studies/", "isFamilyFriendly": true, "displayUrl": "https://snaildove.github.io/2018/05/01/<b>02_deep-convolutional-models-case-studies</b>", "snippet": "So, <b>similar</b> to elsewhere in computer vision, a good place to get started might be to use <b>someone</b> <b>else\u2019s</b> open source implementation for how they use data augmentation. But of course, if you want to capture more in variances, then you think <b>someone</b> <b>else\u2019s</b> open source implementation isn\u2019t, it might be reasonable also to use hyperparameters yourself. So with that, I hope that you\u2019re going to use data augmentation, to get your computer vision applications to work better.", "dateLastCrawled": "2021-10-14T13:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Simone Scardapane on Twitter: &quot;The field is <b>exploding</b>, too many ...", "url": "https://twitter.com/s_scardapane/status/1405187637900611586", "isFamilyFriendly": true, "displayUrl": "https://<b>twitter.com</b>/s_scardapane/status/1405187637900611586", "snippet": "Verified account Protected Tweets @; Suggested users Verified account Protected Tweets @ Protected Tweets @", "dateLastCrawled": "2022-01-30T04:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>A Variational Autoencoder on the</b> SVHN dataset - Bounded Rationality", "url": "https://bjlkeng.github.io/posts/a-variational-autoencoder-on-the-svnh-dataset/", "isFamilyFriendly": true, "displayUrl": "https://bjlkeng.github.io/posts/<b>a-variational-autoencoder-on-the</b>-svnh-dataset", "snippet": "Basically the reasoning is that since the <b>gradient</b> is an approximation, you don&#39;t want to use too big of a batch or else you find local minima that are &quot;sharp&quot;, whereas smaller batches converge to &quot;flat&quot; ones. The &quot;flat&quot; parts of the objective function are probably where you want to be instead of spurious &quot;sharp&quot; ones. In my case, I converged to a loss of around -150 or so with a 6300 batch size but was able to get to around -250 with a batch of 1000. So bigger is not always better ...", "dateLastCrawled": "2022-01-31T20:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Physics Simulations in Python", "url": "https://physics.weber.edu/schroeder/scicomp/PythonManual.pdf", "isFamilyFriendly": true, "displayUrl": "https://physics.weber.edu/schroeder/scicomp/PythonManual.pdf", "snippet": "called Octave that is very <b>similar</b> to Matlab.) An earlier version of this manual used the Java programming language, intro-duced by Sun Microsystems (now Oracle) in 1995. Although based on C and C++, Java is easier to learn and use, and comes with standard cross-platform libraries for graphics and other common tasks. Its computational performance is remarkably good, though it isn\u2019t as fast as C or C++ or Fortran. But Java never really caught on with scientists, and its early use for web ...", "dateLastCrawled": "2022-02-02T17:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Major cloth issues in</b> v2019.2.0f1 - Unity Forum", "url": "https://forum.unity.com/threads/major-cloth-issues-in-v2019-2-0f1.730385/", "isFamilyFriendly": true, "displayUrl": "https://forum.unity.com/threads/<b>major-cloth-issues-in</b>-v2019-2-0f1.730385", "snippet": "2. We have just updated our Unity project to version 2019.2.0f1. We have 2 big issues with the update to the cloth physics component, which we use on virtually all of our characters. The first is that all of the painted weights on the cloth component are all over the place (as seen in Cloth_01.JPG). Before the updated the weights were painted ...", "dateLastCrawled": "2022-01-31T08:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "What is a <b>layman&#39;s explanation of computational linguistics</b>? - Quora", "url": "https://www.quora.com/What-is-a-laymans-explanation-of-computational-linguistics", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-a-<b>laymans-explanation-of-computational-linguistics</b>", "snippet": "Answer: When I think about Computational Linguistics as a field of study, I\u2019m reminded of an old TV commercial Hershey did for Reese\u2019s peanut butter cups that had a guy with a chocolate bar and a girl with a jar of peanut butter (I know, weird, right?) crash into each other on a street corner and...", "dateLastCrawled": "2022-01-12T19:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Personality Exam 2</b> Flashcards | <b>Quizlet</b>", "url": "https://quizlet.com/40857212/personality-exam-2-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/40857212/<b>personality-exam-2</b>-flash-cards", "snippet": "A central assessment task in CBT is to establish a clear and comprehensive <b>problem</b> list. The <b>problem</b> list is generated collaboratively between therapist and client. Items on the <b>problem</b> list should be described in simple, descriptive, concrete terms. Persons and Tompkins (1997) recommend including about five to eight items on a <b>problem</b> list.", "dateLastCrawled": "2019-10-07T23:16:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Coursera Deep Learning Module 4</b> Week 2 Notes | XAI - <b>XAI - eXplainable AI</b>", "url": "https://marcossilva.github.io/en/2019/08/04/coursera-deep-learning-module-4-week-2.html", "isFamilyFriendly": true, "displayUrl": "https://marcossilva.github.io/en/2019/08/04/<b>coursera-deep-learning-module-4</b>-week-2.html", "snippet": "Sometimes these training takes several weeks and might take many GPUs and the fact that <b>someone</b> else has done this and gone through the painful high-performance search process, means that you <b>can</b> often download open source ways that took <b>someone</b> else many weeks or months to figure out and use that as a very good initialization for your own neural network. And use transfer learning to sort of transfer knowledge from some of these very large public data sets to your own <b>problem</b>.", "dateLastCrawled": "2022-01-29T11:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Yield <b>Thought</b>", "url": "https://yieldthought.com/", "isFamilyFriendly": true, "displayUrl": "https://yield<b>thought</b>.com", "snippet": "It could be any of: * Hyperparameters need to be more carefully tuned (algorithms <b>can</b> be rather sensitive even within similar domains) * Initialization is incorrect and weights are dropping to zero (vanishing <b>gradient</b> <b>problem</b>) or are becoming unstable (<b>exploding</b> <b>gradient</b> <b>problem</b>) - these at least you <b>can</b> check by producing images of the weights and staring hard at them, like astrologers seeking meaning in the stars. * The input is not preprocessed, normalized or augmented enough. Or it\u2019s", "dateLastCrawled": "2022-01-29T12:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Solved: Hatching looking <b>Solid</b>, but its not a <b>Solid</b> pattern - <b>Autodesk</b> ...", "url": "https://forums.autodesk.com/t5/autocad-forum/hatching-looking-solid-but-its-not-a-solid-pattern/td-p/3735508", "isFamilyFriendly": true, "displayUrl": "https://<b>forums.autodesk.com</b>/t5/autocad-forum/<b>hatch</b>ing-looking-<b>solid</b>-but-its-not-a...", "snippet": "Finally, something may not be related to your question, I normally have the <b>following</b> variable all set to 1; LTSCALE, MSLTCALE and PSLTSCALE. When I work with <b>someone</b> <b>else&#39;s</b> drawing that has these set differently, I have to play around with them to get dashed linetypes and scaling to display correctly. The linetype scale is usually assigned to ...", "dateLastCrawled": "2022-02-02T20:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>02_deep-convolutional-models-case-studies</b> | SnailDove&#39;s blog", "url": "https://snaildove.github.io/2018/05/01/02_deep-convolutional-models-case-studies/", "isFamilyFriendly": true, "displayUrl": "https://snaildove.github.io/2018/05/01/<b>02_deep-convolutional-models-case-studies</b>", "snippet": "You might well be able to take <b>someone</b> <b>else\u2019s</b> neural network architecture and apply that to your <b>problem</b>. And finally, after the next few videos, you\u2019ll be able to read some of the research papers from the theater computer vision and I hope that you might find it satisfying as well. You don\u2019t have to do this as a class but I hope you might find it satisfying to be able to read some of these seminal computer vision research paper and see yourself able to understand them. So with that ...", "dateLastCrawled": "2021-10-14T13:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>A Variational Autoencoder on the</b> SVHN dataset - Bounded Rationality", "url": "https://bjlkeng.github.io/posts/a-variational-autoencoder-on-the-svnh-dataset/", "isFamilyFriendly": true, "displayUrl": "https://bjlkeng.github.io/posts/<b>a-variational-autoencoder-on-the</b>-svnh-dataset", "snippet": "My last post on variational autoencoders showed a simple <b>example</b> on the MNIST dataset but because it was so simple I <b>thought</b> I might have missed some of the subtler points of VAEs -- boy was I right! The fact that I&#39;m not really a computer vision guy nor a deep learning guy didn&#39;t help either. Through this exercise, I picked up some of the basics in the &quot;craft&quot; of computer vision/deep learning area; there are a lot of subtle points that are easy to gloss over if you&#39;re just reading <b>someone</b> ...", "dateLastCrawled": "2022-01-31T20:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Victoria&#39;s ML Implementation Notes - Persagen", "url": "http://persagen.com/files/ml-implementation_notes.html", "isFamilyFriendly": true, "displayUrl": "persagen.com/files/ml-implementation_notes.html", "snippet": "(-) Unfortunately, ReLU units <b>can</b> be fragile during training and <b>can</b> &quot;die&quot;. For <b>example</b>, a large <b>gradient</b> flowing through a ReLU neuron could cause the weights to update in such a way that the neuron will never activate on any datapoint again. If this happens, then the <b>gradient</b> flowing through the unit will forever be zero from that point on ...", "dateLastCrawled": "2022-02-02T12:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>The Intriguing Problem Of The Younger</b> Dryas\u2014What Does It Mean And What ...", "url": "https://wattsupwiththat.com/2012/06/19/the-intriguing-problem-of-the-younger-dryaswhat-does-it-mean-and-what-caused-it/", "isFamilyFriendly": true, "displayUrl": "https://<b>wattsupwiththat.com</b>/2012/06/19/<b>the-intriguing-problem-of-the-younger</b>-dryaswhat...", "snippet": "The progenitor of the Taurid family of objects is <b>thought</b> to have entered the inner solar system, and a very short period elliptical orbit that crossed the orbits of all the planets of the inner solar system sometime between 20,000 and 30,000 YA. The astronomical data on the Taurids is as good as anything you <b>can</b> dig up with a shovel, and trowel. And that evidence indicates the 50 to 100 km wide Taurid Progenitor object immediately began to breakup as soon as it entered the inner solar ...", "dateLastCrawled": "2022-01-30T07:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "New Apostolic Reformation Conspiracy Theory Debunked | God TV", "url": "https://godtv.com/new-apostolic-reformation-conspiracy-theory-debunked/", "isFamilyFriendly": true, "displayUrl": "https://godtv.com/new-apostolic-reformation-conspiracy-theory-debunked", "snippet": "For <b>example</b>, if a politician appeared with somebody in public, and they <b>can</b> find some dirt on that person, the media that\u2019s hostile toward that political candidate, will try to make the dirt of that other person stick to the candidate as well. This happened when the Republican candidates of 2011 started appearing in public with various charismatic leaders. When the reporters did a little bit of research, they didn\u2019t have to dig very far before they started coming across things that these ...", "dateLastCrawled": "2022-02-02T22:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "GEN PSYCH 111 FINAL QUIZLET Flashcards | Quizlet", "url": "https://quizlet.com/649999714/gen-psych-111-final-quizlet-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/649999714/gen-psych-111-final-quizlet-flash-cards", "snippet": "Difference between a theory and a hypothesis and an <b>example</b>. Theory: broad assumptions after research and observation (ex: sleep improves memory) Hypothesis: specific explanation before research and observation (ex: when sleep deprived, people remember less) Introspection. self-reflection; not empirical. Longitudinal design. a group of variables studied in the same people over time. Cross-sectional design. individuals of different ages or developmental stages are directly compared. Pros and ...", "dateLastCrawled": "2021-12-17T20:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Personality Exam 2</b> Flashcards | <b>Quizlet</b>", "url": "https://quizlet.com/40857212/personality-exam-2-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/40857212/<b>personality-exam-2</b>-flash-cards", "snippet": "Persons (1989) recommended using a generic &quot;<b>Thought</b> Record&quot; as a means of collecting cognitive-related information via client homework. To use a <b>thought</b> record, clients are instructed to jot down the <b>following</b> basic information immediately after experiencing a strong emotional response: \u2022Date and time of the emotional response", "dateLastCrawled": "2019-10-07T23:16:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Yield Thought", "url": "http://yieldthought.com/", "isFamilyFriendly": true, "displayUrl": "yieldthought.com", "snippet": "It could be any of: * Hyperparameters need to be more carefully tuned (algorithms <b>can</b> be rather sensitive even within similar domains) * Initialization is incorrect and weights are dropping to zero (vanishing <b>gradient</b> <b>problem</b>) or are becoming unstable (<b>exploding</b> <b>gradient</b> <b>problem</b>) - these at least you <b>can</b> check by producing images of the weights and staring hard at them, like astrologers seeking meaning in the stars. * The input is not preprocessed, normalized or augmented enough. Or it\u2019s", "dateLastCrawled": "2022-01-22T05:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Picture or Words: Predicting Twitter Image Post Popularity with Deep ...", "url": "https://db-event.jpn.org/deim2018/data/papers/365.pdf", "isFamilyFriendly": true, "displayUrl": "https://db-event.jpn.org/deim2018/data/papers/365.pdf", "snippet": "Retweeting is the activity to re-post <b>someone</b> <b>else\u2019s</b> tweet in the retweeting user\u2019s account, while liking is the ac-tivity to click a button on the tweet to indicate admiration, without repeating the tweet. The count of both activities received <b>can</b> indicate the popularity of the tweet. However, liking tends to indicate that the tweet is sentimentally ad-mirable, and retweeting often indicates tweet containing im-portant information, regardless of its sentiment value. In this paper, we ...", "dateLastCrawled": "2022-01-04T22:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>A Variational Autoencoder on the</b> SVHN dataset - Bounded Rationality", "url": "https://bjlkeng.github.io/posts/a-variational-autoencoder-on-the-svnh-dataset/", "isFamilyFriendly": true, "displayUrl": "https://bjlkeng.github.io/posts/<b>a-variational-autoencoder-on-the</b>-svnh-dataset", "snippet": "The key point for this discussion are the two objective functions (i.e. loss functions). The left hand side loss function is the KL divergence which basically ensures that our encoder network is actually generating values that match our prior \\(Z\\) (a standard isotropic Gaussian). The right hand side loss is the log-likelihood of observing \\(X\\) for our given output distribution with parameters generated by our decoder network. We want to minimize the sum of both loss functions to ensure ...", "dateLastCrawled": "2022-01-31T20:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) Image <b>Tweet Popularity Prediction with Convolutional Neural Network</b>", "url": "https://www.researchgate.net/publication/332256497_Image_Tweet_Popularity_Prediction_with_Convolutional_Neural_Network", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/332256497_Image_Tweet_Popularity_Prediction...", "snippet": "re-post <b>someone</b> <b>else\u2019s</b> tweet in the retw eeting user\u2019s account, while liking is the. activity to click a button on the t weet to indicate admiration, without repeating. 2 Zhang and Jatowt. the ...", "dateLastCrawled": "2021-07-31T06:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "machine learning - Why are neural networks becoming deeper, but not ...", "url": "https://stats.stackexchange.com/questions/222883/why-are-neural-networks-becoming-deeper-but-not-wider", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/222883", "snippet": "For <b>example</b>, the number of feature maps in the convolutional layers, or the number of nodes in the fully-connected layers, has remained roughly the same and is still relatively small in magnitude, despite the large increase in the number of layers. From my intuition though, it would seem that increasing the number of parameters per layer would give each layer a richer source of data from which to learn its non-linear function; but this idea seems to have been overlooked in favour of simply ...", "dateLastCrawled": "2022-02-03T18:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) <b>Biometric Recognition Using Deep Learning</b>: A Survey", "url": "https://www.researchgate.net/publication/337703480_Biometric_Recognition_Using_Deep_Learning_A_Survey", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/337703480_Biometric_Recognition_Using_Deep...", "snippet": "ing works on biometric recognition (including face, \ufb01n-. gerprint, iris, palmprint, ear, v oice, signature, and gait. recognition), which deploy deep learning models, and. show their strengths ...", "dateLastCrawled": "2021-12-29T15:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Victoria&#39;s ML Implementation Notes - Persagen", "url": "http://persagen.com/files/ml-implementation_notes.html", "isFamilyFriendly": true, "displayUrl": "persagen.com/files/ml-implementation_notes.html", "snippet": "(-) Unfortunately, ReLU units <b>can</b> be fragile during training and <b>can</b> &quot;die&quot;. For <b>example</b>, a large <b>gradient</b> flowing through a ReLU neuron could cause the weights to update in such a way that the neuron will never activate on any datapoint again. If this happens, then the <b>gradient</b> flowing through the unit will forever be zero from that point on ...", "dateLastCrawled": "2022-02-02T12:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "HCI \u2013 <b>Seluni&#39;s Adventures</b>", "url": "https://selunitoons.wordpress.com/tag/hci/", "isFamilyFriendly": true, "displayUrl": "https://selunitoons.wordpress.com/tag/hci", "snippet": "Some <b>example</b> my professor gave was teaching <b>someone</b> a specific topic, counseling <b>someone</b> who\u2019s struggling emotionally, or debating on a contentious topic. I did this project with Leo, Eddie, and Ethan, and our original vision was to create a chatbot to counsel first years who were feeling lonely/isolated. However, it changed to helping athletes who were having difficulty transitioning to Oxy with a focus on soccer since Ethan is on the soccer team.", "dateLastCrawled": "2022-01-13T11:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "How close to state-of-the-art are the Machine Learning algorithms ...", "url": "https://www.quora.com/How-close-to-state-of-the-art-are-the-Machine-Learning-algorithms-implemented-by-scikit-learn", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/How-close-to-state-of-the-art-are-the-Machine-Learning...", "snippet": "Answer: I guess it just depends on what you mean by state-of-the-art. Here is how I see it. * In terms of efficiency: Logistic regression is never going to change. It was invented/discovered a long time ago in the statistics field. Improvements <b>can</b> be made in how efficiently the algorithm estim...", "dateLastCrawled": "2022-01-23T14:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Can some one provide examples</b> on how to use Shogun library for Hidden ...", "url": "https://www.quora.com/Can-some-one-provide-examples-on-how-to-use-Shogun-library-for-Hidden-markov-models", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>Can-some-one-provide-examples</b>-on-how-to-use-Shogun-library-for...", "snippet": "Answer: Shogun toolbox Provides two implementations of HMMs * CHMM - Hidden Markov Models * CLinearHMM - Markov chains (embedded in ``Linear&#39;&#39; HMMs) After having shogun toolbox and dependencies installed. Below are the codes in Python * Code for CHMM # In this <b>example</b> a hidden markov model ...", "dateLastCrawled": "2022-01-10T18:54:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Exploding</b> Gradients and the <b>Problem</b> with Overshooting \u2013 Populus Press", "url": "https://populuspress.blog/2021/12/24/exploding-gradients-and-the-problem-with-overshooting/", "isFamilyFriendly": true, "displayUrl": "https://populuspress.blog/2021/12/24/<b>exploding</b>-<b>gradients</b>-and-the-<b>problem</b>-with-overshooting", "snippet": "Similar to the vanishing <b>gradient</b>, an <b>exploding</b> <b>gradient</b> can occur when individual layer gradients turn out to be large. When the model multiples these individual gradients together during backpropagation, this can result in a huge <b>gradient</b> since multiplying many large numbers together will cause the product to skyrocket. The thing is, we want our model to make smaller adjustments as time passes. If the model is <b>learning</b> and getting closer and closer to making predictions in line with the ...", "dateLastCrawled": "2022-01-24T21:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Lecture 15: <b>Exploding</b> and Vanishing Gradients", "url": "https://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/readings/L15%20Exploding%20and%20Vanishing%20Gradients.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/readings/L15 <b>Exploding</b> and...", "snippet": "1.1 <b>Learning</b> Goals Understand why gradients explode or vanish, both { in terms of the mechanics of computing the gradients { the functional relationship between the hidden units at di erent time steps Be able to analyze simple examples of iterated functions, including identifying xed points and qualitatively determining the long-term behavior from a given initialization. Know about various methods for dealing with the <b>problem</b>, and why they help: { <b>Gradient</b> clipping { Reversing the input ...", "dateLastCrawled": "2022-01-30T11:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Exploding And Vanishing Gradient Problem: Math Behind</b> The Truth | by ...", "url": "https://becominghuman.ai/exploding-and-vanishing-gradient-problem-math-behind-the-truth-2d17f9bf6a57", "isFamilyFriendly": true, "displayUrl": "https://becominghuman.ai/<b>exploding-and-vanishing-gradient-problem-math-behind</b>-the...", "snippet": "But what if the <b>gradient</b> becomes negligible? When the <b>gradient</b> becomes negligible, subtracting it from original matrix doesn\u2019t makes any sense and hence the model stops <b>learning</b>. This <b>problem</b> is called as Vanishing <b>Gradient</b> <b>Problem</b>. We\u2019ll first visualise the <b>problem</b> practically in our mind. We\u2019ll train a Deep <b>Learning</b> Model with MNIST(you ...", "dateLastCrawled": "2022-01-17T22:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Vanishing gradient</b> and <b>exploding</b> <b>gradient</b> in Neural networks | by Arun ...", "url": "https://medium.com/tech-break/vanishing-gradient-and-exploding-gradient-in-neural-networks-15950664447e", "isFamilyFriendly": true, "displayUrl": "https://medium.com/tech-break/<b>vanishing-gradient</b>-and-<b>exploding</b>-<b>gradient</b>-in-neural...", "snippet": "<b>Vanishing gradient</b> <b>problem</b> is a common <b>problem</b> that we face while training deep neural networks.Gradients of neural networks are found during back propagation. Generally, adding more hidden layers\u2026", "dateLastCrawled": "2022-01-25T21:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "CPSC 540: <b>Machine</b> <b>Learning</b>", "url": "https://www.cs.ubc.ca/~schmidtm/Courses/440-W21/L36.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.ubc.ca/~schmidtm/Courses/440-W21/L36.pdf", "snippet": "\u2022 ^<b>Exploding</b>/vanishing <b>gradient</b> _, initialization is important, slow progress, etc. \u2022<b>Exploding</b>/vanishing <b>gradient</b> <b>problem</b> is now worse: \u2013Parameters are tied across time: \u2022<b>Gradient</b> gets magnified or shrunk exponentially at each step. \u2013Common solutions: \u2022 ^<b>Gradient</b> clipping: limit <b>gradient</b> norm to some maximum value. \u2022Long Short Term Memory (LSTM): make it easier for information to persist. Variations on Recurrent Neural Networks \u2022Bi-directional RNNs: feedforward from past and ...", "dateLastCrawled": "2021-09-01T20:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "What is <b>Vanishing Gradient Problem</b>? - Great <b>Learning</b>", "url": "https://www.mygreatlearning.com/blog/the-vanishing-gradient-problem/", "isFamilyFriendly": true, "displayUrl": "https://www.mygreat<b>learning</b>.com/blog/the-<b>vanishing-gradient-problem</b>", "snippet": "In <b>Machine</b> <b>Learning</b>, the <b>Vanishing Gradient Problem</b> is encountered while training Neural Networks with <b>gradient</b>-based methods (example, Back Propagation). This <b>problem</b> makes it hard to learn and tune the parameters of the earlier layers in the network. The vanishing gradients <b>problem</b> is one example of unstable behaviour that you may encounter when training a deep neural network. It describes the situation where a deep multilayer feed-forward network or a recurrent neural network is unable to ...", "dateLastCrawled": "2022-02-02T21:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine Learning: Text Generation, A Summary</b> \u2013 Alan&#39;s Blog", "url": "https://achungweb.wordpress.com/2017/04/14/machine-learning-text-generation-a-summary/", "isFamilyFriendly": true, "displayUrl": "https://achungweb.wordpress.com/2017/04/14/<b>machine-learning-text-generation-a-summary</b>", "snippet": "The Vanishing (and <b>Exploding</b>!) <b>Gradient</b> <b>Problem</b>. Previously, we stated that the output from the (n-1)th unit is multiplied by some hidden weight matrix H before it gets transferred to the next unit. As a program runs, therefore, a previous piece of information will be multiplied by hundreds of thousands of such matrices as it gets transferred along the RNN. As we know, repeated multiplication has the potential to grow staggering large, and our previous data will become so inflated to the ...", "dateLastCrawled": "2022-01-20T17:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Gradient Descent</b>. It is a slippery slope, but promise it\u2026 | by Hamza ...", "url": "https://towardsdatascience.com/gradient-descent-3a7db7520711", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>gradient-descent</b>-3a7db7520711", "snippet": "tl;dr <b>Gradient Descent</b> is an optimization technique that is used to improve deep <b>learning</b> and neural network-based models by minimizing the cost function.. In our previous post, we talked about activation functions (link here) and where it is used in <b>machine</b> <b>learning</b> models.However, we also heavily used the term \u2018<b>Gradient Descent</b>\u2019 which is a key element in deep <b>learning</b> models, which are going to talk about in this post.", "dateLastCrawled": "2022-01-30T15:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "The <b>Vanishing Gradient</b> <b>Problem</b>. The <b>Problem</b>, Its Causes, Its\u2026 | by Chi ...", "url": "https://towardsdatascience.com/the-vanishing-gradient-problem-69bf08b15484", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/the-<b>vanishing-gradient</b>-<b>problem</b>-69bf08b15484", "snippet": "For shallow network with only a few layers that use these activations, this isn\u2019t a big <b>problem</b>. However, when more layers are used, it can cause the <b>gradient</b> to be too small for training to work effectively. Gradients of neural networks are found using backpropagation. Simply put, backpropagation finds the derivatives of the network by ...", "dateLastCrawled": "2022-02-02T06:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "A Gentle Introduction to <b>Long Short-Term Memory</b> Networks by the Experts", "url": "https://machinelearningmastery.com/gentle-introduction-long-short-term-memory-networks-experts/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/gentle-introduction-<b>long-short-term-memory</b>-networks...", "snippet": "This shortcoming \u2026 referred to in the literature as the vanishing <b>gradient</b> <b>problem</b> \u2026 <b>Long Short-Term Memory</b> (LSTM) is an RNN architecture specifically designed to address the vanishing <b>gradient</b> <b>problem</b>. \u2014 Alex Graves, et al., A Novel Connectionist System for Unconstrained Handwriting Recognition, 2009. The key to the LSTM solution to the technical problems was the specific internal structure of the units used in the model. \u2026 governed by its ability to deal with vanishing and ...", "dateLastCrawled": "2022-01-31T03:31:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(exploding gradient problem)  is like +(following someone else's example)", "+(exploding gradient problem) is similar to +(following someone else's example)", "+(exploding gradient problem) can be thought of as +(following someone else's example)", "+(exploding gradient problem) can be compared to +(following someone else's example)", "machine learning +(exploding gradient problem AND analogy)", "machine learning +(\"exploding gradient problem is like\")", "machine learning +(\"exploding gradient problem is similar\")", "machine learning +(\"just as exploding gradient problem\")", "machine learning +(\"exploding gradient problem can be thought of as\")", "machine learning +(\"exploding gradient problem can be compared to\")"]}