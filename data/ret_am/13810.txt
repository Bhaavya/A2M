{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Loss</b> Functions in Neural Networks", "url": "https://www.theaidream.com/post/loss-functions-in-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://www.theaidream.com/post/<b>loss</b>-functions-in-neural-networks", "snippet": "<b>Loss</b> functions are mainly classified into two different categories Classification <b>loss</b> and Regression <b>Loss</b>. Classification <b>loss</b> is the case where the aim is to predict the output from the different categorical values for example, if we have a dataset of handwritten images and the digit is to be predicted that lies between (0\u20139), in these kinds of scenarios classification <b>loss</b> is <b>used</b>.", "dateLastCrawled": "2022-02-02T10:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Understanding <b>Loss</b> Functions. <b>Loss</b> functions quantify how good or bad ...", "url": "https://medium.com/nerd-for-tech/understanding-loss-functions-cfbb44b5d17f", "isFamilyFriendly": true, "displayUrl": "https://medium.com/nerd-for-tech/understanding-<b>loss</b>-<b>function</b>s-cfbb44b5d17f", "snippet": "Binary Cross Entropy \u2014Also known as <b>Log</b> <b>loss</b>. This is <b>used</b> if your prediction target values are in the set (0, 1). In case of NN output layer should have sigmoid activation or use logits as true.", "dateLastCrawled": "2021-11-23T13:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Introduction to <b>Loss</b> Functions - Algorithmia Blog", "url": "https://www.algorithmia.com/blog/introduction-to-loss-functions", "isFamilyFriendly": true, "displayUrl": "https://www.algorithmia.com/b<b>log</b>/introduction-to-<b>loss</b>-functions", "snippet": "<b>Log</b> <b>loss</b> (cross entropy <b>loss</b>) <b>Log</b> <b>loss</b> is a <b>loss function</b> also <b>used</b> frequently in classification problems, and is one of the most popular measures for Kaggle competitions. It\u2019s just a straightforward modification of the likelihood function with logarithms. This is actually exactly the same formula as the regular likelihood function, but with logarithms added in. You can see that when the actual class is 1, the second half of the function disappears, and when the actual class is 0, the ...", "dateLastCrawled": "2022-02-03T01:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>ML | Common Loss Functions - GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/ml-common-loss-functions/", "isFamilyFriendly": true, "displayUrl": "https://www.geeksforgeeks.org/ml-common-<b>loss</b>-functions", "snippet": "Cross-Entropy <b>Loss</b> also known as Negative <b>Log</b> Likelihood. It is the commonly <b>used</b> <b>loss function</b> for classification. Cross-entropy <b>loss</b> progress as the predicted probability diverges from the actual label. Python3 # Binary <b>Loss</b> . def cross_entropy( y, y_pred ) : return-np.sum( y * np.<b>log</b>( y_pred ) + ( 1-y ) * np.<b>log</b>( 1-y_pred ) ) / np.size( y ) (5) Hinge <b>Loss</b> also known as Multi-class SVM <b>Loss</b>. Hinge <b>loss</b> is applied for maximum-margin classification, prominently for support vector machines ...", "dateLastCrawled": "2022-01-29T08:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Loss</b> Functions Explained. Intuitive explanations of various <b>Loss</b>\u2026 | by ...", "url": "https://medium.com/deep-learning-demystified/loss-functions-explained-3098e8ff2b27", "isFamilyFriendly": true, "displayUrl": "https://medium.com/deep-learning-demystified/<b>loss</b>-functions-explained-3098e8ff2b27", "snippet": "Binary Cross Entropy <b>Loss</b> Graphs. As you can see, there are two separate functions, one for each value of Y. When we need to predict the positive class (Y = 1), we will use. <b>Loss</b> = -<b>log</b>(Y_pred)", "dateLastCrawled": "2022-02-02T21:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "ML | <b>Log Loss</b> <b>and Mean Squared Error - GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/ml-log-loss-and-mean-squared-error/", "isFamilyFriendly": true, "displayUrl": "https://www.geeksforgeeks.org/ml-<b>log-loss</b>-and-mean-squared-error", "snippet": "<b>Log Loss</b>. It is the evaluation measure to check the performance of the classification model. It measures the amount of divergence of predicted probability with the actual label. So lesser the <b>log loss</b> value, more the perfectness of model. For a perfect model, <b>log loss</b> value = 0. For instance, as accuracy is the count of correct predictions i.e ...", "dateLastCrawled": "2022-02-03T09:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "What are the various commonly <b>used</b> <b>loss</b> functions in Machine Learning ...", "url": "https://www.quora.com/What-are-the-various-commonly-used-loss-functions-in-Machine-Learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-are-the-various-commonly-<b>used</b>-<b>loss</b>-functions-in-Machine...", "snippet": "Answer: Here are the most common ones. For classification: (y_i is +1 or -1, f(x_i) is the classifier score) Hinge <b>loss</b> : \\max(0, 1 - y_i f(x_i)) (<b>used</b> in SVMs) <b>Log</b> <b>loss</b> : \\<b>log</b>(1+exp(-y_i f(x_i))) (<b>used</b> in logistic regression and many neural networks). For Regression: (y_i is the target value, f(...", "dateLastCrawled": "2022-01-17T17:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) <b>Loss</b> functions <b>used</b> in the quality theory", "url": "https://www.researchgate.net/publication/228891025_Loss_functions_used_in_the_quality_theory", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/228891025_<b>Loss</b>_functions_<b>used</b>_in_the_quality...", "snippet": "Moreover, a <b>loss</b> is always incurred between the specification limits no matter how small the deviation from the target for the process is when a quadratic quality <b>loss function</b> is <b>used</b>. Due to ...", "dateLastCrawled": "2021-11-18T16:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Mismatch between the definition of the GAN <b>loss function</b> in two papers", "url": "https://ai.stackexchange.com/questions/9990/mismatch-between-the-definition-of-the-gan-loss-function-in-two-papers", "isFamilyFriendly": true, "displayUrl": "https://ai.stackexchange.com/questions/9990/mismatch-between-the-definition-of-the-gan...", "snippet": "<b>The &quot;loss&quot; function</b> of the generator is actually negative, but, for better gradient descent behavior, can be replaced with -<b>log</b>(D(G(z; \u03b8g)), which also has the ideal value for the generator at 0. It is impossible to reach zero <b>loss</b> for both generator and discriminator in the same GAN at the same time. However, the idea of the GAN is not to reach zero <b>loss</b> for any of the game agents (this is actually counterproductive), but to use that &quot;double gradient descent&quot; to &quot;converge&quot; the distribution of", "dateLastCrawled": "2022-02-03T02:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "[D] <b>Why use Exponential term rather than Log</b> term in VAE&#39;s <b>loss function</b>?", "url": "https://www.reddit.com/r/MachineLearning/comments/74dx67/d_why_use_exponential_term_rather_than_log_term/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/.../comments/74dx67/d_<b>why_use_exponential_term_rather_than_log</b>_term", "snippet": "logvar = <b>log</b> (sigma 2) it makes sense that: logvar.exp () = sigma 2. Mathematically it is all equivalent. As to why they end up with logvar as a variable rather than variance (i.e. sigma 2) I&#39;m not sure. It could be numerical stability or ease of calculation. The answer is probably in a line before the one you&#39;ve picked out.", "dateLastCrawled": "2021-08-21T17:42:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Introduction to <b>Loss</b> Functions - Algorithmia Blog", "url": "https://www.algorithmia.com/blog/introduction-to-loss-functions", "isFamilyFriendly": true, "displayUrl": "https://www.algorithmia.com/b<b>log</b>/introduction-to-<b>loss</b>-functions", "snippet": "<b>Log</b> <b>loss</b> (cross entropy <b>loss</b>) <b>Log</b> <b>loss</b> is a <b>loss function</b> also <b>used</b> frequently in classification problems, and is one of the most popular measures for Kaggle competitions. It\u2019s just a straightforward modification of the likelihood function with logarithms. This is actually exactly the same formula as the regular likelihood function, but with logarithms added in. You can see that when the actual class is 1, the second half of the function disappears, and when the actual class is 0, the ...", "dateLastCrawled": "2022-02-03T01:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Loss</b> Functions Explained. Intuitive explanations of various <b>Loss</b>\u2026 | by ...", "url": "https://medium.com/deep-learning-demystified/loss-functions-explained-3098e8ff2b27", "isFamilyFriendly": true, "displayUrl": "https://medium.com/deep-learning-demystified/<b>loss</b>-functions-explained-3098e8ff2b27", "snippet": "Binary Cross Entropy <b>Loss</b> Graphs. As you can see, there are two separate functions, one for each value of Y. When we need to predict the positive class (Y = 1), we will use. <b>Loss</b> = -<b>log</b>(Y_pred)", "dateLastCrawled": "2022-02-02T21:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "What are the various commonly <b>used</b> <b>loss</b> functions in Machine Learning ...", "url": "https://www.quora.com/What-are-the-various-commonly-used-loss-functions-in-Machine-Learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-are-the-various-commonly-<b>used</b>-<b>loss</b>-functions-in-Machine...", "snippet": "Answer: Here are the most common ones. For classification: (y_i is +1 or -1, f(x_i) is the classifier score) Hinge <b>loss</b> : \\max(0, 1 - y_i f(x_i)) (<b>used</b> in SVMs) <b>Log</b> <b>loss</b> : \\<b>log</b>(1+exp(-y_i f(x_i))) (<b>used</b> in logistic regression and many neural networks). For Regression: (y_i is the target value, f(...", "dateLastCrawled": "2022-01-17T17:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Knowledge Informed Machine Learning using a Weibull-based <b>Loss Function</b> ...", "url": "https://deepai.org/publication/knowledge-informed-machine-learning-using-a-weibull-based-loss-function", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/knowledge-informed-machine-learning-using-a-weibull...", "snippet": "The use of the Weibull-based <b>loss function</b> produced statistically significant results and was shown to outperform most models using traditional <b>loss</b> functions. The method excelled on the PRONOSTIA data set. The PRONOSTIA data is seen as less informationally rich, and thus, the machine learner relied on the Weibull-based <b>loss</b> functions more heavily. The Weibull-based <b>loss</b> functions demonstrated mild effectiveness on the IMS data. The IMS data set is seen as more realistic, but is also smaller ...", "dateLastCrawled": "2022-02-03T00:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "machine learning - Objective <b>function</b>, cost <b>function</b>, <b>loss function</b> ...", "url": "https://stats.stackexchange.com/questions/179026/objective-function-cost-function-loss-function-are-they-the-same-thing", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/179026", "snippet": "&quot;The situation [in Clustering] is somewhat <b>similar</b> to the specification of a <b>loss</b> or cost <b>function</b> in prediction problems (supervised learning)&quot;. Maybe these terms exist because they evolved independently in different academic communities. &quot;Objective <b>Function</b>&quot; is an old term <b>used</b> in Operations Research, and Engineering Mathematics. &quot;<b>Loss function</b>&quot; might be more in use among statisticians. But I&#39;m speculating here. Share. Cite. Improve this answer. Follow answered Oct 28 &#39;15 at 8:48. knb knb ...", "dateLastCrawled": "2022-02-02T08:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Journal of <b>Physics</b>: Conference Series PAPER OPEN ACCESS You may also ...", "url": "https://iopscience.iop.org/article/10.1088/1742-6596/1871/1/012071/pdf", "isFamilyFriendly": true, "displayUrl": "https://iopscience.iop.org/article/10.1088/1742-6596/1871/1/012071/pdf", "snippet": "Max and etc., or a multilayer perceptron can be <b>used</b> as the decoder structure, our Readout is Mean for each graph. 2.3. <b>Loss function</b> The <b>loss function</b> of HGNN is ArcFace <b>loss</b> (Additive Angular Margin <b>Loss</b>, Additive Angular Margin <b>Loss</b>) [11] and cross-entropy (CE) <b>loss</b>. Among them, the CE <b>loss</b> is as the equation (3). \ud835\udc3f \u00bc \u00be L", "dateLastCrawled": "2022-01-26T06:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "What is the connection between entropy and cross-entropy <b>loss function</b> ...", "url": "https://www.quora.com/What-is-the-connection-between-entropy-and-cross-entropy-loss-function", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-the-connection-between-entropy-and-cross-entropy-<b>loss</b>...", "snippet": "Answer: Entropy, as defined by Claude Shannon measure the disorder of a probability distribution. Specifically, if p=(p1, p2, \u2026, ) is a vector which represent the probability of being at any specific state, p1 is the probability of state 1, p2 the probability of being at state 2, and so on, then ...", "dateLastCrawled": "2022-01-08T17:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "The Fourier <b>Loss Function</b> | DeepAI", "url": "https://deepai.org/publication/the-fourier-loss-function", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/the-fourier-<b>loss-function</b>", "snippet": "L(\u03a8)\u03bd(\u03bc):=\u03a8(\u03bc,\u03bd). (1) The role of the <b>loss function</b> is to quantify the differences between the measure \u03bc and the target \u03bd, therefore, the function \u03a8 is usually a distance. In the following, we review the commonly <b>used</b> distances between probability measures. The Total Variation distance ( T V ) [ 34] is defined as.", "dateLastCrawled": "2022-01-21T05:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Papers with Code - Applying <b>physics</b>-based <b>loss</b> functions to neural ...", "url": "https://paperswithcode.com/paper/applying-physics-based-loss-functions-to", "isFamilyFriendly": true, "displayUrl": "https://paperswithcode.com/paper/applying-<b>physics</b>-based-<b>loss</b>-functions-to", "snippet": "In this work a new approach to utilizing PIML is discussed that deals with the use of <b>physics</b>-based <b>loss</b> functions. While typical usage of physical equations in the <b>loss function</b> requires complex layers of derivatives and other functions to ensure that the known governing equation is satisfied, here we show that a <b>similar</b> level of enforcement can be found by implementing more simpler <b>loss</b> functions on specific kinds of output data. The ...", "dateLastCrawled": "2022-01-24T05:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Faster R-CNN | ML - GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/faster-r-cnn-ml/", "isFamilyFriendly": true, "displayUrl": "https://www.geeksforgeeks.org/faster-r-cnn-ml", "snippet": "The object detection network <b>used</b> in Faster R-CNN is very much <b>similar</b> to that <b>used</b> in Fast R-CNN. It is also compatible with VGG-16 as a backbone network. It also uses the RoI pooling layer for making region proposal of fixed size and twin layers of softmax classifier and the bounding box regressor is also <b>used</b> in the prediction of the object and its bounding box.", "dateLastCrawled": "2022-01-25T10:47:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "tensorflow - Why is <b>binary cross entropy</b> (or <b>log</b> <b>loss</b>) <b>used</b> in ...", "url": "https://stats.stackexchange.com/questions/394582/why-is-binary-cross-entropy-or-log-loss-used-in-autoencoders-for-non-binary-da", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/394582/why-is-<b>binary-cross-entropy</b>-or-<b>log</b>...", "snippet": "I am working on an autoencoder for non-binary data ranging in [0,1] and while I was exploring existing solutions I noticed that many people (e.g., the keras tutorial on autoencoders, this guy) use <b>binary cross-entropy</b> as the <b>loss function</b> in this scenario.While the autoencoder works, it produces slightly blurry reconstructions, which, among many reasons, might be because <b>binary cross-entropy</b> for non-binary data penalizes errors towards 0 and 1 more than errors towards 0.5 (as nicely ...", "dateLastCrawled": "2022-01-12T15:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>MCQ-ML</b> - Machine Learning Questions &amp;amp; Solutions Question Context A ...", "url": "https://www.studocu.com/in/document/savitribai-phule-pune-university/bsc-computer-science/mcq-ml/11200181", "isFamilyFriendly": true, "displayUrl": "https://www.<b>studocu</b>.com/in/document/savitribai-phule-pune-university/bsc-computer...", "snippet": "If a classifier is confident about an incorrect classification, then <b>log</b>-<b>loss</b> will; penalise it heavily a particular observation, the classifier assigns a very small probability for the; correct class then the corresponding contribution tLower the <b>log</b>-<b>loss</b>, the better is the model. o the <b>log</b>-<b>loss</b> will be very large. A) 1 and 3 B) 2 and 3 C) 1 ...", "dateLastCrawled": "2022-01-30T14:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "What is the most basic explanation of <b>log</b> <b>loss</b>? - Quora", "url": "https://www.quora.com/What-is-the-most-basic-explanation-of-log-loss", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-the-most-basic-explanation-of-<b>log</b>-<b>loss</b>", "snippet": "Answer (1 of 4): <b>Log</b> <b>loss</b> is the gizmo that allows us to match predictive performance in terms of probability to predictive performance in terms of a <b>loss function</b>. When you make a sequence of probabilistic predictions, you <b>can</b> look at what probability you end up assigning to the entire data seq...", "dateLastCrawled": "2022-01-05T14:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Why <b>binary crossentropy</b> <b>can</b> be <b>used</b> as the <b>loss function</b> in autoencoders?", "url": "https://stats.stackexchange.com/questions/370179/why-binary-crossentropy-can-be-used-as-the-loss-function-in-autoencoders", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/370179/why-<b>binary-crossentropy</b>-<b>can</b>-be-<b>used</b>-as...", "snippet": "$\\begingroup$ NOTE FOR CLOSE VOTERS (i.e. claiming this to be duplicate of this question): 1) It&#39;s a very weird decision to close an older question (i.e. this) as a duplicate of a newer question, and 2) Although these two questions have the same title, they attempt to ask different questions: this one asks why BCE works for autoencoders in the first place (and its answer provide a proof), the other one asks why it might be better or worse to use this <b>loss function</b> in autoencoders. $\\endgroup ...", "dateLastCrawled": "2022-02-01T08:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "keras - Confused between optimizer and <b>loss function</b> - Data Science ...", "url": "https://datascience.stackexchange.com/questions/85579/confused-between-optimizer-and-loss-function", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/.../conf<b>used</b>-between-optimizer-and-<b>loss-function</b>", "snippet": "The <b>loss</b> is a way of measuring the difference between your target label (s) and your prediction label (s). There are many ways of doing this, for example mean squared error, squares the difference between target and prediction. Cross entropy is a more complex <b>loss</b> formula related to information theory. Gradient descent algorithms like batch ...", "dateLastCrawled": "2022-02-02T17:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "The <b>loss function of</b> sensorimotor learning | PNAS", "url": "https://www.pnas.org/content/101/26/9839", "isFamilyFriendly": true, "displayUrl": "https://<b>www.pnas.org</b>/content/101/26/9839", "snippet": "Once a <b>loss function</b> is defined, it <b>can</b> be <b>used</b> to determine the overall <b>loss</b> (the sum of individual losses) for a series of movements. A <b>loss function</b> thus specifies whether it is better to perform a series of movements in which the errors <b>can</b> have one of two possible distributions, for example, always 2 cm or alternating between 1 and 3 cm. If the <b>loss function</b> is quadratic in error, \u03c8 \u223c|", "dateLastCrawled": "2021-07-07T13:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "python - What is the <b>loss function</b> that use the DNNRegressor? - Stack ...", "url": "https://stackoverflow.com/questions/39928035/what-is-the-loss-function-that-use-the-dnnregressor", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/39928035", "snippet": "Args: label_name: String, name of the key in label dict. <b>Can</b> be null if label is a tensor (single headed models). weight_column_name: A string defining feature column name representing weights. It is <b>used</b> to down weight or boost examples during training. It will be multiplied by the <b>loss</b> of the example. target_dimension: dimension of the target ...", "dateLastCrawled": "2022-01-07T23:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "python - <b>Costumizing loss function in keras with condition</b> - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/65143966/costumizing-loss-function-in-keras-with-condition", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/65143966", "snippet": "Now for my further plans I need to include the unlabeled data in the training process but I need it to be ignored by the <b>loss function</b>. What I have tried: setting the labels from the unlabeled data to [0,0,0,0] when using categorical_crossentropy as a <b>loss</b>, because i <b>thought</b> then my unlabeled data would be ignored by the <b>loss function</b>. Somehow ...", "dateLastCrawled": "2022-01-17T13:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Target detection in the <b>loss</b> of focus of the introductory guide ...", "url": "https://developpaper.com/target-detection-in-the-loss-of-focus-of-the-introductory-guide/", "isFamilyFriendly": true, "displayUrl": "https://developpaper.com/target-detection-in-the-<b>loss</b>-of-focus-of-the-introductory-guide", "snippet": "For marking convenience, we <b>can</b> define \u221d in the <b>loss function</b> t As follows: CE\uff08p t \uff09= -\u221d t ln ln\uff08p t \uff09 As you <b>can</b> see, this is just an extension of cross entropy. The problem of equilibrium cross entropy. Our experiments will show that the large class imbalance in the dense detector training process outweighs the cross entropy <b>loss</b>. The easily classified negative class accounts for the majority of the losses and dominates the gradient. Although it balances the importance of ...", "dateLastCrawled": "2022-01-27T07:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "thermodynamics - <b>Can</b> the energy of a physical system be described as an ...", "url": "https://physics.stackexchange.com/questions/658645/can-the-energy-of-a-physical-system-be-described-as-an-unconstrained-optimisatio", "isFamilyFriendly": true, "displayUrl": "https://<b>physics</b>.stackexchange.com/questions/658645/<b>can</b>-the-energy-of-a-physical-system...", "snippet": "Sorry if this is something that is well known, not really familiar with modern <b>physics</b> beyond high school / introductory undergrad level. I largely work in deep learning and broadly speaking, you <b>can</b> think of training a deep neural network as an unconstrained optimization (minimization) problem on a <b>loss function</b> whose domain is the space of all parameters that need to be optimized.", "dateLastCrawled": "2022-01-24T08:15:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Loss</b> Functions in Neural Networks", "url": "https://www.theaidream.com/post/loss-functions-in-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://www.theaidream.com/post/<b>loss</b>-functions-in-neural-networks", "snippet": "Cross-Entropy <b>loss</b> is also called logarithmic <b>loss</b>, <b>log</b> <b>loss</b>, or logistic <b>loss</b>. Each predicted class probability is <b>compared</b> to the actual class desired output 0 or 1 and a score/<b>loss</b> is calculated that penalizes the probability based on how far it is from the actual expected value. The penalty is logarithmic in nature yielding a large score for large differences close to 1 and small score for small differences tending to 0. Cross-Entropy is expressed by the equation; Where x represents the ...", "dateLastCrawled": "2022-02-02T10:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "tensorflow - Why is <b>binary cross entropy</b> (or <b>log</b> <b>loss</b>) <b>used</b> in ...", "url": "https://stats.stackexchange.com/questions/394582/why-is-binary-cross-entropy-or-log-loss-used-in-autoencoders-for-non-binary-da", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/394582/why-is-<b>binary-cross-entropy</b>-or-<b>log</b>...", "snippet": "I am working on an autoencoder for non-binary data ranging in [0,1] and while I was exploring existing solutions I noticed that many people (e.g., the keras tutorial on autoencoders, this guy) use <b>binary cross-entropy</b> as the <b>loss function</b> in this scenario.While the autoencoder works, it produces slightly blurry reconstructions, which, among many reasons, might be because <b>binary cross-entropy</b> for non-binary data penalizes errors towards 0 and 1 more than errors towards 0.5 (as nicely ...", "dateLastCrawled": "2022-01-12T15:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>MCQ-ML</b> - Machine Learning Questions &amp;amp; Solutions Question Context A ...", "url": "https://www.studocu.com/in/document/savitribai-phule-pune-university/bsc-computer-science/mcq-ml/11200181", "isFamilyFriendly": true, "displayUrl": "https://www.<b>studocu</b>.com/in/document/savitribai-phule-pune-university/bsc-computer...", "snippet": "If a classifier is confident about an incorrect classification, then <b>log</b>-<b>loss</b> will; penalise it heavily a particular observation, the classifier assigns a very small probability for the; correct class then the corresponding contribution tLower the <b>log</b>-<b>loss</b>, the better is the model. o the <b>log</b>-<b>loss</b> will be very large. A) 1 and 3 B) 2 and 3 C) 1 ...", "dateLastCrawled": "2022-01-30T14:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) USING SQUARED-<b>LOG</b> ERROR <b>LOSS FUNCTION</b> TO ESTIMATE THE SHAPE ...", "url": "https://www.academia.edu/11653344/USING_SQUARED_LOG_ERROR_LOSS_FUNCTION_TO_ESTIMATE_THE_SHAPE_PARAMETER_AND_THE_RELIABILITY_FUNCTION_OF_PARETO_TYPE_I_DISTRIBUTION", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/11653344/USING_SQUARED_<b>LOG</b>_ERROR_<b>LOSS_FUNCTION</b>_TO_ESTIMATE...", "snippet": "USING SQUARED-<b>LOG</b> ERROR <b>LOSS FUNCTION</b> TO ESTIMATE THE SHAPE PARAMETER AND THE RELIABILITY FUNCTION OF PARETO TYPE I DISTRIBUTION. Huda Abdullah. Download Download PDF. Full PDF Package Download Full PDF Package. This Paper. A short summary of this paper. 36 Full PDFs related to this paper . Read Paper. USING SQUARED-<b>LOG</b> ERROR <b>LOSS FUNCTION</b> TO ESTIMATE THE SHAPE PARAMETER AND THE RELIABILITY FUNCTION OF PARETO TYPE I DISTRIBUTION. Download. Related Papers. Bayes Estimators for the Shape ...", "dateLastCrawled": "2022-01-23T01:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "What are the various commonly <b>used</b> <b>loss</b> functions in Machine Learning ...", "url": "https://www.quora.com/What-are-the-various-commonly-used-loss-functions-in-Machine-Learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-are-the-various-commonly-<b>used</b>-<b>loss</b>-functions-in-Machine...", "snippet": "Answer: Here are the most common ones. For classification: (y_i is +1 or -1, f(x_i) is the classifier score) Hinge <b>loss</b> : \\max(0, 1 - y_i f(x_i)) (<b>used</b> in SVMs) <b>Log</b> <b>loss</b> : \\<b>log</b>(1+exp(-y_i f(x_i))) (<b>used</b> in logistic regression and many neural networks). For Regression: (y_i is the target value, f(...", "dateLastCrawled": "2022-01-17T17:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "machine learning - What is the relation between a <b>loss function</b> and an ...", "url": "https://stats.stackexchange.com/questions/409247/what-is-the-relation-between-a-loss-function-and-an-energy-function", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/409247/what-is-the-relation-between-a-<b>loss</b>...", "snippet": "A <b>loss function</b> is a function that measures the distance between the expected value and the actual value of a model (an example of a <b>loss function</b> is the cross entropy).. An energy function <b>can</b> be defined as a function that we want to minimise or maximise and it is a function of the variables of the system. It is referred to as &quot;energy function&quot; because it is often related or <b>compared</b> to the concept of &quot;energy&quot; <b>in physics</b>. These two expression seem to refer to the same concept.", "dateLastCrawled": "2022-01-17T15:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "The Fourier <b>Loss Function</b> | DeepAI", "url": "https://deepai.org/publication/the-fourier-loss-function", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/the-fourier-<b>loss-function</b>", "snippet": "L(\u03a8)\u03bd(\u03bc):=\u03a8(\u03bc,\u03bd). (1) The role of the <b>loss function</b> is to quantify the differences between the measure \u03bc and the target \u03bd, therefore, the function \u03a8 is usually a distance. In the following, we review the commonly <b>used</b> distances between probability measures. The Total Variation distance ( T V ) [ 34] is defined as.", "dateLastCrawled": "2022-01-21T05:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "The <b>loss function of</b> sensorimotor learning | PNAS", "url": "https://www.pnas.org/content/101/26/9839", "isFamilyFriendly": true, "displayUrl": "https://<b>www.pnas.org</b>/content/101/26/9839", "snippet": "Once a <b>loss function</b> is defined, it <b>can</b> be <b>used</b> to determine the overall <b>loss</b> (the sum of individual losses) for a series of movements. A <b>loss function</b> thus specifies whether it is better to perform a series of movements in which the errors <b>can</b> have one of two possible distributions, for example, always 2 cm or alternating between 1 and 3 cm. If the <b>loss function</b> is quadratic in error, \u03c8 \u223c|", "dateLastCrawled": "2021-07-07T13:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Knowledge Informed Machine Learning using a Weibull-based <b>Loss Function</b> ...", "url": "https://deepai.org/publication/knowledge-informed-machine-learning-using-a-weibull-based-loss-function", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/knowledge-informed-machine-learning-using-a-weibull...", "snippet": "Finally, knowledge <b>can</b> be integrated in a machine learner via a learning algorithm. A common approach is to integrate the external knowledge into the <b>loss function</b>. The <b>loss function</b> is then <b>used</b> in the training of a neural network. The knowledge-based <b>loss function</b> acts as a constraint on the machine learner during its training. This method ...", "dateLastCrawled": "2022-02-03T00:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "[<b>D] Physics Informed Neural Networks (PINN</b>) vs Finite Element Method ...", "url": "https://www.reddit.com/r/MachineLearning/comments/hddhtn/d_physics_informed_neural_networks_pinn_vs_finite/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/MachineLearning/comments/hddhtn/d_<b>physics</b>_informed_neural...", "snippet": "Discussion. I am following the development of PINN s (<b>Physics Informed Neural Networks</b>) as a mesh-free method to solve PDEs. PINNs use the expressivity of neural networks to approximate a solution and the PDE (i.e the <b>Physics</b>) is part of the <b>loss function</b> which provides feedback to the optimizer. Although the method is currently in its nascent ...", "dateLastCrawled": "2021-12-15T03:11:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "1 Motivation: <b>log</b> <b>loss</b> in online <b>learning</b>.", "url": "https://www.cs.princeton.edu/courses/archive/spring08/cos511/scribe_notes/0421.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.princeton.edu/courses/archive/spring08/cos511/scribe_notes/0421.pdf", "snippet": "COS 511: Theoretical <b>Machine</b> <b>Learning</b> Lecturer: Rob Schapire Lecture #20 Scribe: Joe Wenjie Jiang April 21, 2008 1 Motivation: <b>log</b> <b>loss</b> in online <b>learning</b>. In the last lecture, we started to discuss the following model of online <b>learning</b>, in which our goal is to minimize the total <b>log</b> <b>loss</b>: Let X be the space of all possible outcomes in any time step. There are N experts from whom we can consult. At each time step t = 1,...,T: \u2022 Each expert i predicts pt,i, which is a distribution over all ...", "dateLastCrawled": "2021-11-29T18:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "TensorFlow is an interface for expressing <b>machine</b> <b>learning</b> algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Choosing and Customizing <b>Loss</b> Functions for Image Processing | by ...", "url": "https://towardsdatascience.com/choosing-and-customizing-loss-functions-for-image-processing-a0e4bf665b0a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/choosing-and-customizing-<b>loss</b>-functions-for-image...", "snippet": "This is what a <b>machine</b> <b>learning</b> (ML) algorithm does during training. More specifically, ... Cross-Entropy (aka <b>log</b> <b>loss</b>): calculates the differences between the predicted class probabilities and those from ground truth across a logarithmic scale. Useful for object detection. Weighted Cross-Entropy: improves on Cross-Entropy accuracy by adding weights to certain aspects (e.g., certain object classes) which are under-represented in the data (e.g., objects occurring in fewer data samples\u00b3 ...", "dateLastCrawled": "2022-01-31T10:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PPT) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> | Colleen Farrelly - Academia.edu", "url": "https://www.academia.edu/30404581/Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/30404581/<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "<b>Machine</b> <b>Learning</b> by <b>Analogy</b> Colleen M. Farrelly Overview of Problem Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics ...", "dateLastCrawled": "2022-01-02T06:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Machined Learnings: ML and OR: An <b>analogy</b> with cost-sensitive ...", "url": "http://www.machinedlearnings.com/2010/07/ml-and-or.html", "isFamilyFriendly": true, "displayUrl": "www.<b>machine</b>d<b>learning</b>s.com/2010/07/ml-and-or.html", "snippet": "Nonetheless I&#39;ve been amusing myself by thinking about it, in particular trying to think about it from a <b>machine</b> <b>learning</b> reduction standpoint. The simplest well-understood reduction that I can think of which is analogous to supplying estimates to a linear program is the reduction of cost-sensitive multiclass classification (CSMC) to regression.", "dateLastCrawled": "2021-12-25T17:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Why and How to use <b>Cross Entropy</b>. The fundamental reasons for ...", "url": "https://towardsdatascience.com/why-and-how-to-use-cross-entropy-4e983cbdd873", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/why-and-how-to-use-<b>cross-entropy</b>-4e983cbdd873", "snippet": "A convenient way to think of <b>log</b> <b>loss</b> is as follows: If the model predicts that an observation should be labeled 1 and assigns a high probability to that prediction, a high penalty will be incurred when the true label is 0. If the model had assigned a lower probability to that prediction, a lower penalty would have been incurred. The reason for taking the <b>log</b> of predicted probabilities goes back to the original formulation of entropy. Information Theory looks at entropy as a measure of ...", "dateLastCrawled": "2022-01-31T14:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine</b> <b>Learning</b> Concepts for Revision | by Raunak Sarada | Medium", "url": "https://raunaksarada-cse21.medium.com/machine-learning-concepts-for-revision-491384952d27", "isFamilyFriendly": true, "displayUrl": "https://raunaksarada-cse21.medium.com/<b>machine</b>-<b>learning</b>-concepts-for-revision-491384952d27", "snippet": "ML Concepts. A.I \u2014 Intelligence showed by machines which is common for humans <b>Machine</b> <b>Learning</b>- Recognize the pattern in data and automatically learn and improve through experience without explicitly being programmed Deep <b>Learning</b>- branch of <b>machine</b> <b>learning</b>.We have to deal with lots of data so in that case problems can\u2019t be solved with simple ML algorithms.", "dateLastCrawled": "2022-01-25T20:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>machine</b> <b>learning</b> - What is the relation between a <b>loss</b> function and an ...", "url": "https://stats.stackexchange.com/questions/409247/what-is-the-relation-between-a-loss-function-and-an-energy-function", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/409247/what-is-the-relation-between-a-<b>loss</b>...", "snippet": "A <b>loss</b> function is a function that measures the distance between the expected value and the actual value of a model (an example of a <b>loss</b> function is the cross entropy).. An energy function can be defined as a function that we want to minimise or maximise and it is a function of the variables of the system. It is referred to as &quot;energy function&quot; because it is often related or compared to the concept of &quot;energy&quot; in physics. These two expression seem to refer to the same concept.", "dateLastCrawled": "2022-01-17T15:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "How To <b>Implement Logistic Regression</b> From Scratch in Python", "url": "https://machinelearningmastery.com/implement-logistic-regression-stochastic-gradient-descent-scratch-python/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>implement-logistic-regression</b>-stochastic-gradient...", "snippet": "Kick-start your project with my new book <b>Machine</b> <b>Learning</b> Algorithms From Scratch, including step-by-step tutorials and the Python source code files for all examples. Let\u2019s get started. Update Jan/2017: Changed the calculation of fold_size in cross_validation_split() to always be an integer. Fixes issues with Python 3. Update Mar/2018: Added alternate link to download the dataset as the original appears to have been taken down. Update Aug/2018: Tested and updated to work with Python 3.6 ...", "dateLastCrawled": "2022-02-02T07:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>All Unit MCQ questions of ML</b> \u2013 TheCodingShef", "url": "https://thecodingshef.com/all-unit-mcq-questions-of-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://thecodingshef.com/<b>all-unit-mcq-questions-of</b>-<b>machine</b>-<b>learning</b>", "snippet": "Correct option is C. Choose the correct option regarding <b>machine</b> <b>learning</b> (ML) and artificial intelligence (AI) ML is a set of techniques that turns a dataset into a software. AI is a software that can emulate the human mind. ML is an alternate way of programming intelligent machines. All of the above. Correct option is D.", "dateLastCrawled": "2022-01-30T22:09:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Predicting the 2019 All-<b>NBA teams with machine learning</b> - <b>Dribble Analytics</b>", "url": "https://dribbleanalytics.blog/2019/03/ml-all-nba-predict/", "isFamilyFriendly": true, "displayUrl": "https://<b>dribbleanalytics</b>.blog/2019/03/ml-all-nba-predict", "snippet": "<b>Log loss is like</b> accuracy, but instead of analyzing the labeled predictions, it analyzes the prediction probabilities. This is particularly important given that we\u2019re more interested in the probabilities than we are in the actual labels. A \u201cperfect\u201d model will have a log loss of 0. The table below shows each model\u2019s log loss. Model Log loss; SVC: 0.416: RF: 0.416: KNN: 0.403: DNN: 0.43: The SVC and RF have the same log loss, while the KNN has the lowest. Next, let\u2019s look at the ...", "dateLastCrawled": "2022-01-04T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "[OC] Predicting the 2019 All-<b>NBA teams with machine learning</b> : nba", "url": "https://www.reddit.com/r/nba/comments/aw51j6/oc_predicting_the_2019_allnba_teams_with_machine/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/.../aw51j6/oc_predicting_the_2019_allnba_teams_with_<b>machine</b>", "snippet": "[OC] Predicting the 2019 All-<b>NBA teams with machine learning</b>. Original Content. This post has a lot of graphs. If you don&#39;t want to click on each one individually, they&#39;re all in an imgur album here. There is a tl;dr and summary infographic at the very end. Introduction . Last year, media members unanimously selected LeBron James to the All-NBA first team, giving him a record 12 All-NBA first team selections. However, given the Lakers recent struggles and LeBron&#39;s absence earlier in the ...", "dateLastCrawled": "2021-10-14T12:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Predicting the 2019 All-NBA teams with <b>machine</b> <b>learning</b>", "url": "https://dribbleanalytics.blogspot.com/2019/03/ml-all-nba-predict.html", "isFamilyFriendly": true, "displayUrl": "https://dribbleanalytics.blogspot.com/2019/03/ml-all-nba-predict.html", "snippet": "Predicting the 2019 All-NBA teams with <b>machine</b> <b>learning</b> Get link; Facebook; Twitter; Pinterest; Email; Other Apps; March 01, 2019 There is a summary at the bottom if you want to skip to the results. Introduction Last year, media members unanimously selected LeBron James to the All-NBA first team, giving him a record 12 All-NBA first team selections. However, given the Lakers recent struggles and LeBron&#39;s absence earlier in the season, LeBron might miss not only the first team but also the ...", "dateLastCrawled": "2021-12-11T07:58:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "What\u2019s considered a good Log <b>Loss</b> in <b>Machine</b> <b>Learning</b> ? | by Federico ...", "url": "https://medium.com/@fzammito/whats-considered-a-good-log-loss-in-machine-learning-a529d400632d", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@fzammito/whats-considered-a-good-log-<b>loss</b>-in-<b>machine</b>-<b>learning</b>-a529...", "snippet": "<b>Log Loss is similar</b> to the Accuracy, but it will favor models that distinguish more strongly the classes. Log <b>Loss</b> it useful to compare models not only on their output but on their probabilistic ...", "dateLastCrawled": "2022-01-30T14:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>What is an intuitive explanation for the log</b> loss function? - Quora", "url": "https://www.quora.com/What-is-an-intuitive-explanation-for-the-log-loss-function", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>What-is-an-intuitive-explanation-for-the-log</b>-loss-function", "snippet": "Answer (1 of 8): To me an intuitive explanation is that minimizing the log loss equals minimizing the Kullback-Leibler divergence (Kullback\u2013Leibler divergence - Wikipedia) between the function you want to optimize (for example a neural network) and the true function that generates the data (from ...", "dateLastCrawled": "2022-01-30T02:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Key techniques for Evaluating <b>Machine</b> <b>Learning</b> models - Data Analytics", "url": "https://vitalflux.com/key-techniques-evaluating-machine-learning-models-performance/", "isFamilyFriendly": true, "displayUrl": "https://vitalflux.com/key-techniques-evaluating-<b>machine</b>-<b>learning</b>-models-performance", "snippet": "Log loss is used to evaluate the performance of classification <b>machine</b> <b>learning</b> models that are built using classification algorithms such as logistic regression, support vector <b>machine</b> (SVM), random forest, and gradient boosting. The idea behind the use of <b>Log loss is similar</b> to taking a base-e exponential or natural logarithm in order to compare model scores from high-value functions which may indicate poor <b>machine</b> <b>learning</b> model performance. The logarithmic loss value is defined as ...", "dateLastCrawled": "2022-01-31T06:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Loss In Machine Learning</b> - 02/2021 - Course f", "url": "https://www.coursef.com/loss-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.coursef.com/<b>loss-in-machine-learning</b>", "snippet": "<b>Log Loss is similar</b> to the Accuracy, but it will favor models that ... Two of the most popular loss functions in <b>machine</b> <b>learning</b> are the 0-1 loss function and the quadratic loss function. The 0-1 loss function is an indicator function that returns 1 when the target and output are not equal and zero otherwise: 0-1 Loss: The quadratic loss is a commonly used symmetric loss \u2026 161 People Used View all course \u203a\u203a Visit Site \u2039 1; 2 \u203a FAQs. Do online classes have tests? Not all online ...", "dateLastCrawled": "2021-02-08T01:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Diagnosing malaria from some symptoms: a <b>machine</b> <b>learning</b> approach and ...", "url": "https://link.springer.com/article/10.1007/s12553-020-00488-5", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s12553-020-00488-5", "snippet": "<b>Machine</b> <b>learning</b> tools have become available in the diagnosis and prediction of diseases, thereby saving costs and improving the likelihood of survivorship, especially in some terminal diseases. In the case of infectious diseases, early diagnosis is highly needed in isolating the subjects to reduce the spread of the disease. Researchers continue to propose new data mining tools that help in the early diagnosis of diseases, reducing the mortality rate, and improving the quality of life of ...", "dateLastCrawled": "2021-12-03T05:54:00.0000000Z", "language": "en", "isNavigational": false}], [], [], []], "all_bing_queries": ["+(log loss)  is like +(the \u201closs function\u201d used in physics)", "+(log loss) is similar to +(the \u201closs function\u201d used in physics)", "+(log loss) can be thought of as +(the \u201closs function\u201d used in physics)", "+(log loss) can be compared to +(the \u201closs function\u201d used in physics)", "machine learning +(log loss AND analogy)", "machine learning +(\"log loss is like\")", "machine learning +(\"log loss is similar\")", "machine learning +(\"just as log loss\")", "machine learning +(\"log loss can be thought of as\")", "machine learning +(\"log loss can be compared to\")"]}