{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Understanding the <b>Universal Approximation Theorem</b> \u2013 Towards AI \u2014 The ...", "url": "https://towardsai.net/p/deep-learning/understanding-the-universal-approximation-theorem", "isFamilyFriendly": true, "displayUrl": "https://towardsai.net/p/<b>deep</b>-learning/understanding-the-<b>universal-approximation-theorem</b>", "snippet": "They\u2019ve been developed further, and today <b>deep</b> <b>neural</b> networks and <b>deep</b> learning achieve outstanding performance on many important problems in computer vision, speech recognition, and natural language processing. That being said, let\u2019s dive into the <b>Universal Approximation Theorem</b>.", "dateLastCrawled": "2022-02-03T15:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A <b>Universal Approximation Theorem</b> of <b>Deep</b> <b>Neural</b> Networks for ...", "url": "https://deepai.org/publication/a-universal-approximation-theorem-of-deep-neural-networks-for-expressing-distributions", "isFamilyFriendly": true, "displayUrl": "https://<b>deep</b>ai.org/publication/a-<b>universal-approximation-theorem</b>-of-<b>deep</b>-<b>neural</b>...", "snippet": "When N 1=N 2=\u22ef=N L=N, we say the <b>network</b> <b>network</b> has width N and depth L. The <b>neural</b> <b>network</b> is said to be a <b>deep</b> <b>neural</b> <b>network</b> (DNN) if L\u22652. The function defined by the <b>deep</b> <b>neural</b> <b>network</b> is denoted by DNN({W \u2113,b\u2113}L+1\u2113=1). Popular choices of activation functions \u03c3. include the rectified linear unit (ReLU) function.", "dateLastCrawled": "2022-02-01T11:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Neural</b> Networks and the <b>Universal Approximation Theorem</b> | by Milind ...", "url": "https://towardsdatascience.com/neural-networks-and-the-universal-approximation-theorem-8a389a33d30a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>neural</b>-<b>networks</b>-and-the-<b>universal-approximation-theorem</b>...", "snippet": "The <b>Universal Approximation Theorem</b>. Mathematically speaking, any <b>neural</b> <b>network</b> architecture aims at finding any mathematical function y= f(x) that can map attributes(x) to output(y). The accuracy of this function i.e. mapping differs depending on the distribution of the dataset and the architecture of the <b>network</b> employed. The function f(x) can be arbitrarily complex. The <b>Universal Approximation Theorem</b> tells us that <b>Neural</b> Networks has a kind of universality i.e. no matter what f(x) is ...", "dateLastCrawled": "2022-02-03T02:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A <b>Universal</b> <b>Approximation</b> <b>Theorem</b> of <b>Deep</b> <b>Neural</b> Networks for ...", "url": "https://proceedings.neurips.cc/paper/2020/file/2000f6325dfc4fc3201fc45ed01c7a5d-Paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/2020/file/2000f6325dfc4fc3201fc45ed01c7a5d-Paper.pdf", "snippet": "A <b>Universal</b> <b>Approximation</b> <b>Theorem</b> of <b>Deep</b> <b>Neural</b> Networks for Expressing Probability Distributions Yulong Lu Department of Mathematics and Statistics University of Massachusetts Amherst Amherst, MA 01003 lu@math.umass.edu Jianfeng Lu Mathematics Department Duke University Durham, NC 27708 jianfeng@math.duke.edu Abstract This paper studies the <b>universal</b> <b>approximation</b> property of <b>deep</b> <b>neural</b> networks for representing probability distributions. Given a target distribution \u02c7and a source ...", "dateLastCrawled": "2022-01-19T15:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Universal</b> <b>Approximation</b> with Quadratic <b>Deep</b> Networks", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7076904/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7076904", "snippet": "Since a ReLU quadratic <b>network</b> can represent any univariate polynomial in a unique and global manner, and by the Weierstrass <b>theorem</b> and the Kolmogorov <b>theorem</b> that multivariate functions can be represented through summation and composition of univariate functions, we can approximate any multivariate function with a well-structured ReLU quadratic <b>neural</b> <b>network</b>, justifying the <b>universal</b> <b>approximation</b> power of the quadratic <b>network</b>. To our best knowledge, our quadratic <b>network</b> is the first-of ...", "dateLastCrawled": "2021-12-16T12:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Universal Approximation Theorem</b>, <b>Neural</b> Nets &amp; Lego Blocks | by ...", "url": "https://medium.com/analytics-vidhya/universal-approximation-theorem-neural-nets-lego-blocks-1f5a7d93542a", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/<b>universal-approximation-theorem</b>-<b>neural</b>-nets-lego...", "snippet": "<b>Deep</b> Learning, which is based on the Multilayer <b>Neural</b> Networks has achieved state-of-the-art results in most of the domains as of today. In this post, we will look at the <b>Universal</b> <b>Approximation</b> ...", "dateLastCrawled": "2022-01-28T22:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Universal Approximation and Depth</b> \u00ab <b>Neural</b> Networks Blog", "url": "https://theneural.wordpress.com/2013/01/07/universal-approximation-and-depth/", "isFamilyFriendly": true, "displayUrl": "https://the<b>neural</b>.wordpress.com/2013/01/07/<b>universal-approximation-and-depth</b>", "snippet": "By being <b>deep</b>, the <b>neural</b> <b>network</b> can represent functions that are computed with several steps of computation. <b>Deep</b> <b>neural</b> networks are best thought of as constant-depth threshold circuits, and these are known to be able to compute a lot of interesting functions. For example, a small 3-hidden layer threshold <b>network</b> can sort N N-bit numbers, add N such numbers, compute their product, their max, compute any analytic function to high precision. And it is this ability of <b>deep</b> <b>neural</b> networks to ...", "dateLastCrawled": "2022-01-29T22:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Neural</b> Networks and the Power of <b>Universal Approximation Theorem</b>. | by ...", "url": "https://medium.com/analytics-vidhya/neural-networks-and-the-power-of-universal-approximation-theorem-9b8790508af2", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/<b>neural</b>-<b>networks</b>-and-the-power-of-<b>universal</b>...", "snippet": "<b>Universal Approximation Theorem</b>. We know we can\u2019t tell what the suitable function is to plot the graph in red (Fig. 4) since that is very complex (<b>like</b> our house). But we know how to plot the ...", "dateLastCrawled": "2021-10-17T13:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "What is the <b>implication of the Universal Approximation</b> <b>Theorem</b> over ...", "url": "https://www.quora.com/What-is-the-implication-of-the-Universal-Approximation-Theorem-over-deep-learning-methodology", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-the-<b>implication-of-the-Universal-Approximation</b>-<b>Theorem</b>...", "snippet": "Answer (1 of 2): The <b>universal</b> <b>approximation</b> <b>theorem</b> (<b>Universal</b> <b>approximation</b> <b>theorem</b> - Wikipedia) is a powerful <b>theorem</b> stating that a <b>neural</b> <b>network</b> is a <b>universal</b> functional approximator. However, it does not say anything about the size of the one hidden-layer <b>neural</b> <b>network</b>. Except that it is...", "dateLastCrawled": "2022-01-11T08:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "IFT 6085 - Lecture <b>10 Expressivity and Universal Approximation Theorems</b> ...", "url": "http://mitliagkas.github.io/ift6085-2020/ift-6085-lecture-10-notes.pdf", "isFamilyFriendly": true, "displayUrl": "mitliagkas.github.io/ift6085-2020/ift-6085-lecture-10-notes.pdf", "snippet": "3 <b>Universal</b> <b>Approximation</b> <b>Theorem</b> The <b>universal</b> <b>approximation</b> <b>theorem</b> states that any continuous function f : [0;1]n! [0;1] can be approximated arbitrarily well by a <b>neural</b> <b>network</b> with at least 1 hidden layer with a \ufb01nite number of weights, which is what we are going to illustrate in the next subsections. 3.1 Visual proof of <b>Universal</b> ...", "dateLastCrawled": "2022-01-30T08:26:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Universal Approximation Theorem</b>. The power of <b>Neural</b> Networks | by ...", "url": "https://medium.com/swlh/universal-approximation-theorem-d1a1a67c1b5b", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/<b>universal-approximation-theorem</b>-d1a1a67c1b5b", "snippet": "<b>Universal Approximation Theorem</b>, in its lose form, states that a feed-forward <b>network</b> with a single hidden layer containing a finite number of neurons can approximate any continuous function. Whoa ...", "dateLastCrawled": "2022-01-28T02:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Illustrative Proof of <b>Universal Approximation Theorem</b> | HackerNoon", "url": "https://hackernoon.com/illustrative-proof-of-universal-approximation-theorem-5845c02822f6", "isFamilyFriendly": true, "displayUrl": "https://hackernoon.com/illustrative-proof-of-<b>universal-approximation-theorem</b>-5845c02822f6", "snippet": "A sigmoid neuron <b>is similar</b> to the perceptron neuron, ... <b>Deep</b> <b>neural</b> <b>network</b> with a certain number of hidden layers should be able to approximate any function that exits between input and output. <b>Universal approximation theorem</b> says: that a feed-forward <b>network</b> with a single hidden layer containing a finite number of neurons can approximate continuous functions on compact subsets of R, under mild assumptions on the activation function \u2014 Wikipedia. In simple terms, UAT says that \u2014 you ...", "dateLastCrawled": "2022-02-01T02:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Universal</b> <b>Approximation</b> with Quadratic <b>Deep</b> Networks", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7076904/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7076904", "snippet": "Since a ReLU quadratic <b>network</b> can represent any univariate polynomial in a unique and global manner, and by the Weierstrass <b>theorem</b> and the Kolmogorov <b>theorem</b> that multivariate functions can be represented through summation and composition of univariate functions, we can approximate any multivariate function with a well-structured ReLU quadratic <b>neural</b> <b>network</b>, justifying the <b>universal</b> <b>approximation</b> power of the quadratic <b>network</b>. To our best knowledge, our quadratic <b>network</b> is the first-of ...", "dateLastCrawled": "2021-12-16T12:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A <b>Universal Approximation Theorem</b> of <b>Deep</b> <b>Neural</b> Networks for ...", "url": "https://deepai.org/publication/a-universal-approximation-theorem-of-deep-neural-networks-for-expressing-distributions", "isFamilyFriendly": true, "displayUrl": "https://<b>deep</b>ai.org/publication/a-<b>universal-approximation-theorem</b>-of-<b>deep</b>-<b>neural</b>...", "snippet": "When N 1=N 2=\u22ef=N L=N, we say the <b>network</b> <b>network</b> has width N and depth L. The <b>neural</b> <b>network</b> is said to be a <b>deep</b> <b>neural</b> <b>network</b> (DNN) if L\u22652. The function defined by the <b>deep</b> <b>neural</b> <b>network</b> is denoted by DNN({W \u2113,b\u2113}L+1\u2113=1). Popular choices of activation functions \u03c3. include the rectified linear unit (ReLU) function.", "dateLastCrawled": "2022-02-01T11:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A <b>Universal</b> <b>Approximation</b> <b>Theorem</b> of <b>Deep</b> <b>Neural</b> Networks for ...", "url": "https://proceedings.neurips.cc/paper/2020/file/2000f6325dfc4fc3201fc45ed01c7a5d-Review.html", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/2020/file/2000f6325dfc4fc3201fc45ed01c7a5d-Review...", "snippet": "A <b>Universal</b> <b>Approximation</b> <b>Theorem</b> of <b>Deep</b> <b>Neural Networks for Expressing Probability Distributions</b>. Review 1 . Summary and Contributions: This paper examines the ability to approximate a distribution constructed by push-forward with a <b>neural</b> <b>network</b>. As generative models, flow models, etc. become more popular, it is very common to construct new distributions by extruding distributions with generators. This paper proves that when a generator is constructed with a <b>neural</b> <b>network</b>, a constant ...", "dateLastCrawled": "2021-11-22T14:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Neural</b> Networks and the Power of <b>Universal Approximation Theorem</b>. | by ...", "url": "https://medium.com/analytics-vidhya/neural-networks-and-the-power-of-universal-approximation-theorem-9b8790508af2", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/<b>neural</b>-<b>networks</b>-and-the-power-of-<b>universal</b>...", "snippet": "<b>Neural</b> <b>Network</b> with 2 hidden layers . So <b>neural</b> <b>network</b> is nothing, but a <b>network</b> of neurons and each neuron tries to learn a simple function and with the help of these simple neurons, the <b>network</b> ...", "dateLastCrawled": "2021-10-17T13:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Universal Approximation and Depth</b> \u00ab <b>Neural</b> Networks Blog", "url": "https://theneural.wordpress.com/2013/01/07/universal-approximation-and-depth/", "isFamilyFriendly": true, "displayUrl": "https://the<b>neural</b>.wordpress.com/2013/01/07/<b>universal-approximation-and-depth</b>", "snippet": "By being <b>deep</b>, the <b>neural</b> <b>network</b> can represent functions that are computed with several steps of computation. <b>Deep</b> <b>neural</b> networks are best thought of as constant-depth threshold circuits, and these are known to be able to compute a lot of interesting functions. For example, a small 3-hidden layer threshold <b>network</b> can sort N N-bit numbers, add N such numbers, compute their product, their max, compute any analytic function to high precision. And it is this ability of <b>deep</b> <b>neural</b> networks to ...", "dateLastCrawled": "2022-01-29T22:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "What is the <b>implication of the Universal Approximation</b> <b>Theorem</b> over ...", "url": "https://www.quora.com/What-is-the-implication-of-the-Universal-Approximation-Theorem-over-deep-learning-methodology", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-the-<b>implication-of-the-Universal-Approximation</b>-<b>Theorem</b>...", "snippet": "Answer (1 of 2): The <b>universal</b> <b>approximation</b> <b>theorem</b> (<b>Universal</b> <b>approximation</b> <b>theorem</b> - Wikipedia) is a powerful <b>theorem</b> stating that a <b>neural</b> <b>network</b> is a <b>universal</b> functional approximator. However, it does not say anything about the size of the one hidden-layer <b>neural</b> <b>network</b>. Except that it is...", "dateLastCrawled": "2022-01-11T08:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "IFT 6085 - Lecture <b>10 Expressivity and Universal Approximation Theorems</b> ...", "url": "http://mitliagkas.github.io/ift6085-2020/ift-6085-lecture-10-notes.pdf", "isFamilyFriendly": true, "displayUrl": "mitliagkas.github.io/ift6085-2020/ift-6085-lecture-10-notes.pdf", "snippet": "3 <b>Universal</b> <b>Approximation</b> <b>Theorem</b> The <b>universal</b> <b>approximation</b> <b>theorem</b> states that any continuous function f : [0;1]n! [0;1] can be approximated arbitrarily well by a <b>neural</b> <b>network</b> with at least 1 hidden layer with a \ufb01nite number of weights, which is what we are going to illustrate in the next subsections. 3.1 Visual proof of <b>Universal</b> ...", "dateLastCrawled": "2022-01-30T08:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Universal approximation theorem vs Fourier series</b> : deeplearning", "url": "https://www.reddit.com/r/deeplearning/comments/9thbut/universal_approximation_theorem_vs_fourier_series/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/<b>deep</b>learning/comments/9thbut/<b>universal</b>_<b>approximation</b>_<b>theorem</b>...", "snippet": "<b>Universal approximation theorem vs Fourier series</b>. Close. 11. Posted by 2 years ago. Archived. <b>Universal approximation theorem vs Fourier series</b> . Why not use Fourier series instead of <b>neural</b> networks for regression? I have a feeling it has to do with the difficulty of finding appropriate coefficients for the Fourier series. 15 comments. share. save. hide. report. 78% Upvoted. This thread is archived. New comments cannot be posted and votes cannot be cast. Sort by. best. level 1. 2 years ago ...", "dateLastCrawled": "2021-04-05T04:20:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Understanding the <b>Universal Approximation Theorem</b> \u2013 Towards AI \u2014 The ...", "url": "https://towardsai.net/p/deep-learning/understanding-the-universal-approximation-theorem", "isFamilyFriendly": true, "displayUrl": "https://towardsai.net/p/<b>deep</b>-learning/understanding-the-<b>universal-approximation-theorem</b>", "snippet": "That being said, let\u2019s dive into the <b>Universal Approximation Theorem</b>. Suppose someone has given you a wiggly function, say f(x) ... Again, that <b>can</b> <b>be thought</b> of as computing a function. Of course, just because we know a <b>neural</b> <b>network</b> exists that <b>can</b> (say) translate Chinese text into English, that doesn\u2019t mean we have good techniques for constructing or even recognizing such a <b>network</b>. This limitation applies also to traditional universality theorems for models such as Boolean circuits ...", "dateLastCrawled": "2022-02-03T15:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Universal Approximation Theorem: Code Refactoring for</b> Software 2.0 ...", "url": "https://towardsdatascience.com/universal-approximation-theorem-code-refactoring-for-software-2-0-20d4bdc3cf48", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>universal-approximation-theorem-code-refactoring-for</b>...", "snippet": "What exactly is <b>Universal</b> <b>Approximation</b> <b>Theorem</b>? Well, put in layman\u2019s terms, UAT just means that giving a one hidden layer <b>neural</b> <b>network</b> with enough neurons. It <b>can</b> approximate(or simulate closely) any continuous function within the given input range. It means that a one hidden layer <b>neural</b> <b>network</b> is an ultimate flexible function approximator. Maybe a little too flexible.", "dateLastCrawled": "2022-01-14T03:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>Universal Approximation Theorems</b> - ResearchGate", "url": "https://www.researchgate.net/publication/336361517_Universal_Approximation_Theorems", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/336361517_<b>Universal_Approximation_Theorems</b>", "snippet": "The <b>universal</b> <b>approximation</b> <b>theorem</b> established the density of specific families of <b>neural</b> networks in the space of continuous functions and in certain Bochner spaces, defined between any two ...", "dateLastCrawled": "2022-01-25T23:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Universal Approximation and Depth</b> \u00ab <b>Neural</b> Networks Blog", "url": "https://theneural.wordpress.com/2013/01/07/universal-approximation-and-depth/", "isFamilyFriendly": true, "displayUrl": "https://the<b>neural</b>.wordpress.com/2013/01/07/<b>universal-approximation-and-depth</b>", "snippet": "By being <b>deep</b>, the <b>neural</b> <b>network</b> <b>can</b> represent functions that are computed with several steps of computation. <b>Deep</b> <b>neural</b> networks are best <b>thought</b> of as constant-depth threshold circuits, and these are known to be able to compute a lot of interesting functions. For example, a small 3-hidden layer threshold <b>network</b> <b>can</b> sort N N-bit numbers, add N such numbers, compute their product, their max, compute any analytic function to high precision. And it is this ability of <b>deep</b> <b>neural</b> networks to ...", "dateLastCrawled": "2022-01-29T22:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Artificial <b>Neural</b> Networks. <b>Universal</b> Function Approximators? | by ...", "url": "https://medium.com/predict/artificial-neural-networks-universal-function-approximators-cf5198224b58", "isFamilyFriendly": true, "displayUrl": "https://medium.com/predict/artificial-<b>neural</b>-<b>networks</b>-<b>universal</b>-function-approximators...", "snippet": "Theories are Great! A great <b>theorem</b> with a large name. Wikipedia says, In the mathematical theory of artificial <b>neural</b> networks, the <b>universal</b> <b>approximation</b> <b>theorem</b> states that a feed-forward ...", "dateLastCrawled": "2022-01-21T10:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "What is the <b>implication of the Universal Approximation</b> <b>Theorem</b> over ...", "url": "https://www.quora.com/What-is-the-implication-of-the-Universal-Approximation-Theorem-over-deep-learning-methodology", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-the-<b>implication-of-the-Universal-Approximation</b>-<b>Theorem</b>...", "snippet": "Answer (1 of 2): The <b>universal</b> <b>approximation</b> <b>theorem</b> (<b>Universal</b> <b>approximation</b> <b>theorem</b> - Wikipedia) is a powerful <b>theorem</b> stating that a <b>neural</b> <b>network</b> is a <b>universal</b> functional approximator. However, it does not say anything about the size of the one hidden-layer <b>neural</b> <b>network</b>. Except that it is...", "dateLastCrawled": "2022-01-11T08:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Universal Approximations of Invariant Maps</b> by <b>Neural</b> Networks ...", "url": "https://link.springer.com/article/10.1007/s00365-021-09546-1", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00365-021-09546-1", "snippet": "Our work <b>can</b> be seen as an extension of results on the <b>universal</b> <b>approximation</b> property of <b>neural</b> networks [ 7, 10, 18, 19, 24, 29, 31, 32] to the setting of group invariant/equivariant maps and/or infinite-dimensional input spaces. Our general results in Sect. 2 are based on classical results of the theory of polynomial invariants [ 16, 17, 46 ].", "dateLastCrawled": "2022-01-23T15:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "How and Why My Article Spurred a Dispute Between Steven Pinker, Yann ...", "url": "https://medium.com/analytics-vidhya/a-deep-learning-twitter-dispute-with-yann-lecun-steven-pinker-gary-marcus-and-my-article-32f1978090b5", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/a-<b>deep</b>-learning-twitter-dispute-with-yann-lecun...", "snippet": "Its thesis was that by the <b>Universal</b> <b>Approximation</b> <b>Theorem</b> (UAT), we <b>can</b> understand the appearance of intelligent behavior in certain problems exhibited by <b>neural</b> networks (e.g. image recognition ...", "dateLastCrawled": "2022-01-29T18:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Do You Know We <b>Can</b> Approximate Any Continuous Function With A Single ...", "url": "https://gowrishankar.info/blog/do-you-know-we-can-approximate-any-continuous-function-with-a-single-hidden-layer-neural-network-a-visual-guide/", "isFamilyFriendly": true, "displayUrl": "https://gowrishankar.info/blog/do-you-know-we-<b>can</b>-approximate-any-continuous-function...", "snippet": "In this post, we shall study the nuances of the <b>Universal</b> <b>Approximation</b> <b>Theorem</b> for <b>Neural</b> Networks, a fundamental property of <b>deep</b> learning systems in detail. Ok, that is neither true nor false. Theoretically speaking, a single hidden layer <b>neural</b> <b>network</b> <b>can</b> approximate any continuous function in the 1-d space with few caveats like a. fail to generalize b. no learnability and c. impossibly large layer size.", "dateLastCrawled": "2021-12-26T00:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Neural Networks as universal problem solvers</b> \u2013 AI goes data-boggling", "url": "https://www.databoggling.com/2020/03/22/neural-networks-as-universal-problem-solvers/", "isFamilyFriendly": true, "displayUrl": "https://www.databoggling.com/2020/03/22/<b>neural-networks-as-universal-problem-solvers</b>", "snippet": "The approximate answer to life, the universe and everything is <b>neural</b> networks. At least this is what the <b>Universal</b> <b>Approximation</b> <b>Theorem</b> (UAT) tells us. Or to put it, in more earthly terms, quoting Ian Goodfellow: \u201cA feedforward <b>network</b> with a single layer is sufficient to represent any function, but the layer may be infeasibly large and may fail to learn and generalize correctly.\u201d But \u2013 I may add \u2013 it is, nonetheless, very useful. Now, other earthlings may ask themselves, why, in ...", "dateLastCrawled": "2021-12-03T23:34:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Universal</b> <b>Approximation</b> with Quadratic <b>Deep</b> Networks", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7076904/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7076904", "snippet": "Since a ReLU quadratic <b>network</b> <b>can</b> represent any univariate polynomial in a unique and global manner, and by the Weierstrass <b>theorem</b> and the Kolmogorov <b>theorem</b> that multivariate functions <b>can</b> be represented through summation and composition of univariate functions, we <b>can</b> approximate any multivariate function with a well-structured ReLU quadratic <b>neural</b> <b>network</b>, justifying the <b>universal</b> <b>approximation</b> power of the quadratic <b>network</b>. To our best knowledge, our quadratic <b>network</b> is the first-of ...", "dateLastCrawled": "2021-12-16T12:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Universal Approximation</b> with <b>Deep</b> Narrow Networks", "url": "http://proceedings.mlr.press/v125/kidger20a/kidger20a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v125/kidger20a/kidger20a.pdf", "snippet": "The classical <b>Universal Approximation</b> <b>Theorem</b> holds for <b>neural</b> networks of arbitrary width and bounded depth. Here we consider the natural \u2018dual\u2019 scenario for networks of bounded width and arbitrary depth. Precisely, let nbe the number of inputs neurons, mbe the number of output neurons, and let \u02c6be any nonaf\ufb01ne continuous function, with a continuous nonzero derivative at some point. Then we show that the class of <b>neural</b> networks of arbitrary depth, width n+ m+ 2, and activation ...", "dateLastCrawled": "2022-01-19T15:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Can</b> <b>neural</b> networks solve any problem? | by Brendan Fortuner | Towards ...", "url": "https://towardsdatascience.com/can-neural-networks-really-learn-any-function-65e106617fc6", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>can</b>-<b>neural</b>-<b>networks</b>-really-learn-any-function-65e106617fc6", "snippet": "The <b>Universal</b> <b>Approximation</b> <b>Theorem</b> states that a <b>neural</b> <b>network</b> with 1 hidden layer <b>can</b> approximate any continuous function for inputs within a specific range. If the function jumps around or has large gaps, we won\u2019t be able to approximate it. Additionally, if we train a <b>network</b> on inputs between 10 and 20, we <b>can</b>\u2019t say for sure whether it will work on inputs between 40 and 50.", "dateLastCrawled": "2022-02-02T08:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A <b>Universal Approximation Theorem</b> of <b>Deep</b> <b>Neural</b> Networks for ...", "url": "https://deepai.org/publication/a-universal-approximation-theorem-of-deep-neural-networks-for-expressing-distributions", "isFamilyFriendly": true, "displayUrl": "https://<b>deep</b>ai.org/publication/a-<b>universal-approximation-theorem</b>-of-<b>deep</b>-<b>neural</b>...", "snippet": "When N 1=N 2=\u22ef=N L=N, we say the <b>network</b> <b>network</b> has width N and depth L. The <b>neural</b> <b>network</b> is said to be a <b>deep</b> <b>neural</b> <b>network</b> (DNN) if L\u22652. The function defined by the <b>deep</b> <b>neural</b> <b>network</b> is denoted by DNN({W \u2113,b\u2113}L+1\u2113=1). Popular choices of activation functions \u03c3. include the rectified linear unit (ReLU) function.", "dateLastCrawled": "2022-02-01T11:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A <b>Universal</b> <b>Approximation</b> <b>Theorem</b> of <b>Deep</b> <b>Neural</b> Networks for ...", "url": "https://proceedings.neurips.cc/paper/2020/file/2000f6325dfc4fc3201fc45ed01c7a5d-Paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/2020/file/2000f6325dfc4fc3201fc45ed01c7a5d-Paper.pdf", "snippet": "A <b>Universal</b> <b>Approximation</b> <b>Theorem</b> of <b>Deep</b> <b>Neural</b> Networks for Expressing Probability Distributions Yulong Lu Department of Mathematics and Statistics University of Massachusetts Amherst Amherst, MA 01003 lu@math.umass.edu Jianfeng Lu Mathematics Department Duke University Durham, NC 27708 jianfeng@math.duke.edu Abstract This paper studies the <b>universal</b> <b>approximation</b> property of <b>deep</b> <b>neural</b> networks for representing probability distributions. Given a target distribution \u02c7and a source ...", "dateLastCrawled": "2022-01-19T15:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Lecture 6: <b>Neural</b> Networks - GitHub Pages", "url": "https://shuaili8.github.io/Teaching/VE445/L6_nn.pdf", "isFamilyFriendly": true, "displayUrl": "https://shuaili8.github.io/Teaching/VE445/L6_nn.pdf", "snippet": "<b>Universal</b> <b>approximation</b> <b>theorem</b> \u2022A feed-forward <b>network</b> with a single hidden layer containing a finite number of neurons <b>can</b> approximate continuous functions 24 Hornik, Kurt, Maxwell Stinchcombe, and Halbert White. &quot;Multilayer feedforward networks are <b>universal</b> approximators.&quot; <b>Neural</b> networks 2.5 (1989): 359-366 1-20-1 NN approximates", "dateLastCrawled": "2022-02-02T01:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "What is the <b>implication of the Universal Approximation</b> <b>Theorem</b> over ...", "url": "https://www.quora.com/What-is-the-implication-of-the-Universal-Approximation-Theorem-over-deep-learning-methodology", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-the-<b>implication-of-the-Universal-Approximation</b>-<b>Theorem</b>...", "snippet": "Answer (1 of 2): The <b>universal</b> <b>approximation</b> <b>theorem</b> (<b>Universal</b> <b>approximation</b> <b>theorem</b> - Wikipedia) is a powerful <b>theorem</b> stating that a <b>neural</b> <b>network</b> is a <b>universal</b> functional approximator. However, it does not say anything about the size of the one hidden-layer <b>neural</b> <b>network</b>. Except that it is...", "dateLastCrawled": "2022-01-11T08:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Universal Approximations of Invariant Maps</b> by <b>Neural</b> Networks ...", "url": "https://link.springer.com/article/10.1007/s00365-021-09546-1", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00365-021-09546-1", "snippet": "Our work <b>can</b> be seen as an extension of results on the <b>universal</b> <b>approximation</b> property of <b>neural</b> networks [ 7, 10, 18, 19, 24, 29, 31, 32] to the setting of group invariant/equivariant maps and/or infinite-dimensional input spaces. Our general results in Sect. 2 are based on classical results of the theory of polynomial invariants [ 16, 17, 46 ].", "dateLastCrawled": "2022-01-23T15:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Universal Approximation and Depth</b> \u00ab <b>Neural</b> Networks Blog", "url": "https://theneural.wordpress.com/2013/01/07/universal-approximation-and-depth/", "isFamilyFriendly": true, "displayUrl": "https://the<b>neural</b>.wordpress.com/2013/01/07/<b>universal-approximation-and-depth</b>", "snippet": "By being <b>deep</b>, the <b>neural</b> <b>network</b> <b>can</b> represent functions that are computed with several steps of computation. <b>Deep</b> <b>neural</b> networks are best thought of as constant-depth threshold circuits, and these are known to be able to compute a lot of interesting functions. For example, a small 3-hidden layer threshold <b>network</b> <b>can</b> sort N N-bit numbers, add N such numbers, compute their product, their max, compute any analytic function to high precision. And it is this ability of <b>deep</b> <b>neural</b> networks to ...", "dateLastCrawled": "2022-01-29T22:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Deep</b> networks vs shallow networks: why do we need depth? - Cross Validated", "url": "https://stats.stackexchange.com/questions/274569/deep-networks-vs-shallow-networks-why-do-we-need-depth", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/274569", "snippet": "The <b>universal</b> <b>approximation</b> <b>theorem</b> states that a feedforward <b>neural</b> <b>network</b> (NN) with a single hidden layer <b>can</b> approximate any function over some compact set, provided that it has enough neurons on that layer.. This suggests that the number of neurons is more important than the number of layers. But in practice <b>deep</b> learning is obviously very successful at various prediction tasks. Why is that? Shouldn&#39;t all <b>deep</b> NNs be equivalent to single layered NNs with enough neurons?", "dateLastCrawled": "2022-01-25T14:32:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Universal Approximation Theorem</b>. The power of Neural Networks | by ...", "url": "https://medium.com/swlh/universal-approximation-theorem-d1a1a67c1b5b", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/<b>universal-approximation-theorem</b>-d1a1a67c1b5b", "snippet": "<b>Universal Approximation Theorem</b>, in its lose form, states that a feed-forward network with a single hidden layer containing a finite number of neurons can approximate any continuous function. Whoa ...", "dateLastCrawled": "2022-01-28T02:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Universal Approximation Theorem</b>, Neural Nets &amp; Lego Blocks | by ...", "url": "https://medium.com/analytics-vidhya/universal-approximation-theorem-neural-nets-lego-blocks-1f5a7d93542a", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/<b>universal-approximation-theorem</b>-neural-nets-lego...", "snippet": "In this post, we will look at the <b>Universal Approximation Theorem</b> \u2014 one of the fundamental theorems on which the entire concept of Deep <b>Learning</b> is based upon. We will make use of lego blocks ...", "dateLastCrawled": "2022-01-28T22:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Learning</b> Complex Functions using <b>Universal</b> Approximate <b>Theorem</b> - Ai Nxt", "url": "https://ainxt.co.in/learning-complex-functions-using-universal-approximate-theorem/", "isFamilyFriendly": true, "displayUrl": "https://ainxt.co.in/<b>learning</b>-complex-functions-using-<b>universal</b>-approximate-<b>theorem</b>", "snippet": "<b>Universal</b> <b>Approximation</b> <b>Theorem</b>. No matter how complex our output logic is, we can use collection of neurons and form Dense Neural Network to approximate our function. This is known as \u201c<b>UNIVERSAL</b> <b>APPROXIMATION</b> <b>THEOREM</b>\u201c. Lets take an example of Two Dimensional data where y = f(x) i.e. y is some function of x. Now, we need to find that ...", "dateLastCrawled": "2022-01-21T05:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Machine</b> <b>Learning</b> 1", "url": "https://uvaml1.github.io/2020/slides/8.2_NeuralNetworksUniversalApproximationTheorem.pdf", "isFamilyFriendly": true, "displayUrl": "https://uvaml1.github.io/2020/slides/8.2_NeuralNetworks<b>UniversalApproximationTheorem</b>.pdf", "snippet": "<b>Theorem</b>: <b>Universal</b> Approximators!f(x) ... <b>Machine</b> <b>Learning</b> 1 (a) (b) (c) (d) ! N=50 datapoints ! MLP: 2 layers, 3 hidden units with tanh activation function. 1 linear output unit. ! Hidden unit outputs: dashed curves 6 Example: Function Approximations Illustration of the ca-pability of a multilayer perceptron to approximate four different func-,(b) |,) is the Heaviside step function. In data points, shown as blue dots, have been sam-over the interval and the corresponding val-evaluated ...", "dateLastCrawled": "2022-01-22T07:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) <b>Universal Approximation Theorems</b> - ResearchGate", "url": "https://www.researchgate.net/publication/336361517_Universal_Approximation_Theorems", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/336361517_<b>Universal_Approximation_Theorems</b>", "snippet": "In the <b>machine</b> <b>learning</b> literature, <b>universal</b> <b>approximation</b> refers to a model class\u2019 ability. to generically approximate any member of a large topological space whose elements are. functions, or ...", "dateLastCrawled": "2022-01-25T23:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Illustrative Proof of <b>Universal Approximation Theorem</b> | HackerNoon", "url": "https://hackernoon.com/illustrative-proof-of-universal-approximation-theorem-5845c02822f6", "isFamilyFriendly": true, "displayUrl": "https://hackernoon.com/illustrative-proof-of-<b>universal-approximation-theorem</b>-5845c02822f6", "snippet": "We will talk about the <b>Universal approximation theorem</b> and we will also prove the <b>theorem</b> graphically. The most commonly used sigmoid function is the logistic function, which has a characteristic of an \u201cS\u201d shaped curve. In real life, we deal with complex functions where the relationship between input and output might be complex. To solve this problem, let&#39;s take an <b>analogy</b> of building a house. The way we are going to create complex functions is that we will combine the sigmoids neurons ...", "dateLastCrawled": "2022-02-01T02:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "ne.neural evol - <b>Universal Approximation Theorem</b> \u2014 Neural Networks ...", "url": "https://cstheory.stackexchange.com/questions/17545/universal-approximation-theorem-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://cstheory.stackexchange.com/questions/17545", "snippet": "<b>Universal approximation theorem</b> states that &quot;the standard multilayer feed-forward network with a single hidden layer, ... There is an advanced result, key to <b>machine</b> <b>learning</b>, known as Kolmogorov&#39;s <b>theorem</b> [1]; I have never seen an intuitive sketch of why it works. This may have to do with the different cultures that approach it. The applied <b>learning</b> crowd regards Kolmogorov&#39;s <b>theorem</b> as an existence <b>theorem</b> that merely indicates that NNs may exist, so at least the structure is not overly ...", "dateLastCrawled": "2022-02-03T04:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "neural networks - <b>Universal Approximation Theorem and high dimension</b> ...", "url": "https://stats.stackexchange.com/questions/298622/universal-approximation-theorem-and-high-dimension-linear-regression", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/298622/<b>universal</b>-<b>approximation</b>-<b>theorem</b>-and...", "snippet": "Cross Validated is a question and answer site for people interested in statistics, <b>machine</b> <b>learning</b>, data analysis, data mining, and data visualization. It only takes a minute to sign up. Sign up to join this community", "dateLastCrawled": "2022-01-17T22:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Machine Learning by Analogy</b> - SlideShare", "url": "https://www.slideshare.net/ColleenFarrelly/machine-learning-by-analogy-59094152", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ColleenFarrelly/<b>machine-learning-by-analogy</b>-59094152", "snippet": "<b>Machine Learning by Analogy</b> 1. Colleen M. Farrelly 2. Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics behind these ...", "dateLastCrawled": "2022-01-31T07:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "TensorFlow is an interface for expressing <b>machine</b> <b>learning</b> algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(universal approximation theorem)  is like +(deep neural network)", "+(universal approximation theorem) is similar to +(deep neural network)", "+(universal approximation theorem) can be thought of as +(deep neural network)", "+(universal approximation theorem) can be compared to +(deep neural network)", "machine learning +(universal approximation theorem AND analogy)", "machine learning +(\"universal approximation theorem is like\")", "machine learning +(\"universal approximation theorem is similar\")", "machine learning +(\"just as universal approximation theorem\")", "machine learning +(\"universal approximation theorem can be thought of as\")", "machine learning +(\"universal approximation theorem can be compared to\")"]}