{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Encoders and <b>Decoders for Neural Machine Translation</b> | <b>Pluralsight</b>", "url": "https://www.pluralsight.com/guides/encoders-and-decoders-for-neural-machine-translation", "isFamilyFriendly": true, "displayUrl": "https://<b>www.pluralsight.com</b>/guides/<b>encoders</b>-and-<b>decoders-for-neural-machine-translation</b>", "snippet": "The <b>encoder</b> is at the feeding end; it understands the sequence and reduces the dimension of the input sequence. The sequence has a fixed size known as the context vector. This context vector acts <b>like</b> input to the decoder, which generates an output sequence when reaching the end token. Hence, you can call these seq2seq models <b>encoder</b>-decoder models. This architecture can handle input and output sequences of variable length. Decoder. If you use LSTM for the <b>encoder</b>, use the same for the ...", "dateLastCrawled": "2022-01-27T02:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Encode Text</b> - Online Text <b>Encoder</b>", "url": "https://www.madeintext.com/encode-text/", "isFamilyFriendly": true, "displayUrl": "https://www.madeintext.com/<b>encode-text</b>", "snippet": "Online Text <b>Encoder</b> Tool Importance of Text Encoding. <b>Like</b> Unicode and URL encoding, different types of encodings have become highly popular because they ensure smooth and safe information sharing. Such types of encoding are used in various aspects <b>like</b> computer files, normalization, and decomposition. How to <b>Encode Text</b>? Unlike many other text encoding tools that make it compulsory for users to download, install, and even purchase different software, this is an online free tool. You do not ...", "dateLastCrawled": "2022-02-02T11:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "How to build an <b>encoder decoder</b> translation model using LSTM with ...", "url": "https://towardsdatascience.com/how-to-build-an-encoder-decoder-translation-model-using-lstm-with-python-and-keras-a31e9d864b9b", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/how-to-build-an-<b>encoder-decoder</b>-translation-model-using...", "snippet": "<b>Encoder Decoder</b> structure. Image by Author. We have split the model into two parts, first, we have an <b>encoder</b> that inputs the Spanish sentence and produces a hidden vector.The <b>encoder</b> is built with an Embedding layer that converts the words into a vector and a recurrent neural network (RNN) that calculates the hidden state, here we will be using Long Short-Term Memory (LSTM) layer.", "dateLastCrawled": "2022-02-02T02:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Binary Encoder &amp; Decoder</b> <b>Translator</b> \u2015 LingoJam", "url": "https://lingojam.com/BinaryEncoder&Decoder", "isFamilyFriendly": true, "displayUrl": "https://lingojam.com/<b>BinaryEncoder&amp;Decoder</b>", "snippet": "<b>Binary Encoder &amp; Decoder</b> <b>Translator</b>. Send. This online tool allows you to encode text into binary and decode binary back to text. Simply paste your text in the left box or your binary data in the right box, and the <b>encoder</b>/decoder will instantly give you the result. What is binary? Information (in its technical sense) is a pattern which holds some meaning. There are many ways to store information, but the most efficient way that computers are able to store information is in a pattern of 2 ...", "dateLastCrawled": "2022-02-02T15:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Neural machine translation with attention</b> | Text | TensorFlow", "url": "https://www.tensorflow.org/text/tutorials/nmt_with_attention", "isFamilyFriendly": true, "displayUrl": "https://www.tensorflow.org/text/tutorials/nmt_with_attention", "snippet": "If you want to sound <b>like</b> a native speaker, you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo. Create a tf.data dataset. From these arrays of strings you can create a tf.data.Dataset of strings that shuffles and batches them efficiently: BUFFER_SIZE = len(inp) BATCH_SIZE = 64 dataset = tf.data.Dataset.from_tensor_slices((inp, targ)).shuffle ...", "dateLastCrawled": "2022-02-03T09:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Best <b>ASCII</b> to Text Converter / <b>Translator</b>", "url": "https://codebeautify.org/ascii-to-text", "isFamilyFriendly": true, "displayUrl": "https://codebeautify.org/<b>ascii</b>-to-text", "snippet": "Click on the URL button, Enter URL and Submit. Users can also convert <b>ASCII</b> File to Text by uploading the file. <b>ASCII</b> Decoder to plain Text Online works well on Windows, MAC, Linux, Chrome, Firefox, Edge, and Safari. It can also <b>ASCII</b> converted JSON, XML, YAML and other data files to Plain Text. It&#39;s also called <b>ASCII</b> <b>translator</b>.", "dateLastCrawled": "2022-02-03T07:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Leet Speak <b>Translator</b> &amp; Generator - 1337 <b>Translator</b>", "url": "https://md5decrypt.net/en/Leet-translator/", "isFamilyFriendly": true, "displayUrl": "https://md5decrypt.net/en/Leet-<b>translator</b>", "snippet": "Leet <b>translator</b> : Leet Speak (1337 5p34k), which means elite speak or eleet speak, is an alternative alphabet that replace usual letters with different ASCII characters. This alphabet is used to translate a text so it can be very hard to read for someone that isn&#39;t used to leet speak. It was firstly used that way to avoid people <b>like</b> lamers ...", "dateLastCrawled": "2022-02-02T23:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Enchantment Table <b>Translator</b> | ArjhanToteck", "url": "https://arjhantoteck.neocities.org/enchantment%20table%20translator.html", "isFamilyFriendly": true, "displayUrl": "https://arjhantoteck.neocities.org/enchantment table <b>translator</b>.html", "snippet": "Enchantment Table <b>Translator</b> Encode some text in the Minecraft enchanting table language. I don&#39;t know how else to describe this.", "dateLastCrawled": "2022-02-03T11:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Caesar Cipher</b> (Shift) - Online Decoder, <b>Encoder</b>, Solver, <b>Translator</b>", "url": "https://www.dcode.fr/caesar-cipher", "isFamilyFriendly": true, "displayUrl": "https://www.dcode.fr/caesar-ciph", "snippet": "Caesar <b>Encoder</b>. Caesar Code plain text. Knowing the shift: Alphabet Encrypt by Caesar Code. See also: ROT ... (<b>like</b> Suetonius) proving that he used this type of substitution to protect his military communications. The exact date of creation and its real author are unknown. Ask a new question. Source code. dCode retains ownership of the &quot;<b>Caesar Cipher</b>&quot; source code. Except explicit open source licence (indicated Creative Commons / free), the &quot;<b>Caesar Cipher</b>&quot; algorithm, the applet or snippet ...", "dateLastCrawled": "2022-02-02T22:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Numbers To Letters</b> (online tool) | Boxentriq", "url": "https://www.boxentriq.com/code-breaking/numbers-to-letters", "isFamilyFriendly": true, "displayUrl": "https://www.boxentriq.com/code-breaking/<b>numbers-to-letters</b>", "snippet": "<b>Numbers To Letters</b> Converter. Convert <b>numbers to letters</b> in various formats. Numbering the letters so A=1, B=2, etc is one of the simplest ways of converting them to numbers. This is called the A1Z26 cipher. However, there are more options such as ASCII codes, tap codes or even the periodic table of elements to decode numbers.", "dateLastCrawled": "2022-02-02T01:40:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Encoders and <b>Decoders for Neural Machine Translation</b> | <b>Pluralsight</b>", "url": "https://www.pluralsight.com/guides/encoders-and-decoders-for-neural-machine-translation", "isFamilyFriendly": true, "displayUrl": "https://<b>www.pluralsight.com</b>/guides/<b>encoders</b>-and-<b>decoders-for-neural-machine-translation</b>", "snippet": "If you use LSTM for the <b>encoder</b>, use the same for the decoder. But it&#39;s slightly more complex than the <b>encoder</b> network. You can say the decoder is in an &quot;aware state.&quot; It knows what words you have generated so far and what the previous hidden state was. The first layer of the decoder is initialized by using the context vector &#39;C&#39; from the <b>encoder</b> network to generate the output. Then a special token is applied at the start to indicate the output generation. It applies a <b>similar</b> token at the ...", "dateLastCrawled": "2022-01-27T02:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Create a Language <b>Translator</b> with RNN - Machine Learning Project with ...", "url": "https://projectgurukul.org/language-translator-project-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://projectgurukul.org/language-<b>translator</b>-project-machine-learning", "snippet": "One RNN layer will act as \u2018<b>encoder</b>\u2019: In this we give our english sentence as an input. And other RNN layer will act as \u2018decoder\u2019: which will give us the output (translated sentence in french) Let\u2019s understand the process that we will be using to implement machine translation: Firstly we will encode the input sequence into state vectors. Then we will start with a target sequence size 1 (just the start-of-sequence character). After this we will feed the state vectors and 1-char ...", "dateLastCrawled": "2022-01-29T01:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Natural language processing (NLP) and its use in machine translation", "url": "https://www.qblocks.cloud/blog/natural-language-processing-machine-translation", "isFamilyFriendly": true, "displayUrl": "https://www.qblocks.cloud/blog/natural-language-processing-machine-translation", "snippet": "As the name suggests Bidirectional <b>Encoder</b> Representation from Transformers is an <b>encoder</b>-based architecture. The BERT model is based on the attention is all you need model, this is the reason \u201cAttention is all you need\u201d is the huge breakthrough in the evolution of deep learning. The BERT model can understand the meaning of complicated human languages in the text and perform various tasks like Machine Translation, Text Summarization, etc with a higher BLEU score and higher accuracy ...", "dateLastCrawled": "2022-01-29T16:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Neural machine translation with attention</b> | Text | TensorFlow", "url": "https://www.tensorflow.org/text/tutorials/nmt_with_attention", "isFamilyFriendly": true, "displayUrl": "https://www.tensorflow.org/text/tutorials/nmt_with_attention", "snippet": "Overall this <b>is similar</b> to the training loop, except that the input to the decoder at each time step is a sample from the decoder&#39;s last prediction. class <b>Translator</b>(tf.Module): def __init__(self, <b>encoder</b>, decoder, input_text_processor, output_text_processor): self.<b>encoder</b> = <b>encoder</b> self.decoder = decoder self.input_text_processor = input_text_processor self.output_text_processor = output_text_processor self.output_token_string_from_index = ( tf.keras.layers.StringLookup( vocabulary=output ...", "dateLastCrawled": "2022-02-03T09:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Build A <b>Simple Machine Translator encoder-decoder framework with</b> lstm ...", "url": "https://6chaoran.github.io/data-story/deep-learning/nlp/build-a-simple-machine-translator-part1/", "isFamilyFriendly": true, "displayUrl": "https://6chaoran.github.io/.../deep-learning/nlp/build-a-simple-machine-<b>translator</b>-part1", "snippet": "Build A <b>Simple Machine Translator encoder-decoder framework with</b> lstm 10 minute read On this page. Introduction. steps to train a seq2seq model: steps to infer a seq2seq model: demo of english-chinese translation; Dataset. clean punucations; tokenize; sequence reprenstation; Model Configuration. <b>Encoder</b>; Decoder; <b>Encoder</b>-Deocder; Model Training; Model Inference. initial states and token; update states and token; Extension; Reference; Introduction. seq2seq model is a general purpose sequence ...", "dateLastCrawled": "2022-02-01T18:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Base64 <b>Translator</b> - Online Toolz", "url": "https://www.online-toolz.com/tools/base64-translator.php", "isFamilyFriendly": true, "displayUrl": "https://www.online-toolz.com/tools/base64-<b>translator</b>.php", "snippet": "Base64 is a group of <b>similar</b> encoding schemes that represent binary data in an ASCII string format by translating it into a radix-64 representation. The Base64 term originates from a specific MIME content transfer encoding. Base64 encoding schemes are commonly used when there is a need to encode binary data that needs be stored and transferred over media that are designed to deal with textual data. This is to ensure that the data remains intact without modification during transport. Base64 ...", "dateLastCrawled": "2022-02-03T05:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Base65536 Encoding</b> Tool Online Free - Better Converter", "url": "https://www.better-converter.com/Encoders-Decoders/Base65536-Encode", "isFamilyFriendly": true, "displayUrl": "https://www.better-converter.com/<b>Encoders</b>-Decoders/<b>Base65536-Encode</b>", "snippet": "<b>base65536</b> encodes data in a <b>similar</b> fashion to base64, but its alphabet, instead of being 64 characters long, is 65536 characters long.This means, one can map 16 bits of data into a single unicode codepoint. It is of course terribly inefficient, if you were to count the outputted bytes (especially when UTF-8 encoded), but if you count just the number of unicode characters, as for example Twitter does for it\u2019s length limit, you can fit double the data per character.", "dateLastCrawled": "2022-02-03T07:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Braingle \u00bb <b>Tap Code</b>", "url": "https://www.braingle.com/brainteasers/codes/tapcode.php", "isFamilyFriendly": true, "displayUrl": "https://www.braingle.com/brainteasers/codes/<b>tapcode</b>.php", "snippet": "The <b>Tap Code</b> is a code (<b>similar</b> to Morse Code ), commonly used by prisoners in jail to communicate with one another. The method of communicating is usually by &quot;tapping&quot; either the metal bars or the walls inside the cell, hence its name. It is a very simple code, not meant to avoid interception, since the messages are sent in cleartext. 1. 2. 3.", "dateLastCrawled": "2022-02-02T10:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>NATO</b> phonetic alphabet online <b>translator</b> \u2014 Cryptii", "url": "https://cryptii.com/pipes/nato-phonetic-alphabet", "isFamilyFriendly": true, "displayUrl": "https://cryptii.com/pipes/<b>nato</b>-phonetic-alphabet", "snippet": "<b>NATO</b> phonetic alphabet online <b>translator</b>. A spelling alphabet is a set of words used to stand for the letters of an alphabet in oral communication. It is used to spell out words when speaking to someone not able to see the speaker, or when the audio channel is not clear. Nihilist cipher Hex to Base32 Affine cipher Base32hex Binary to text Cryptii. Web app offering modular conversion, encoding and encryption online. Translations are done in the browser without any server interaction. Powered ...", "dateLastCrawled": "2022-02-02T21:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "14 Websites Offering Online Translation Jobs(Use Your Bilingual Skills)", "url": "https://crowdworknews.com/14-websites-offering-online-translation-jobs/", "isFamilyFriendly": true, "displayUrl": "https://crowdworknews.com/14-websites-offering-online", "snippet": "Gengo is quite a popular translating platform which has a <b>similar</b> process of signing up like Verbalize. You have to sign up and take a two part test to prove your skills, once you get familiarized by their style guide, you can start working on projects. CyraCom . This company hires interpreters from other countries other than the US. The process of signing up is simple, and they do require a degree in translation. Language Service Associates. Language Service Associates is a famous ...", "dateLastCrawled": "2022-02-02T22:15:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Machine Translation(<b>Encoder-Decoder</b> Model)! | by Shreya Srivastava ...", "url": "https://medium.com/analytics-vidhya/machine-translation-encoder-decoder-model-7e4867377161", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/machine-translation-<b>encoder-decoder</b>-model-7e4867377161", "snippet": "It <b>can</b> <b>be thought</b> of as that the <b>decoder</b> is trained to generate the output based on the information gathered by the <b>encoder</b>. Firstly, we input the START_ so that the <b>decoder</b> starts generating the ...", "dateLastCrawled": "2022-02-01T03:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Seq2seq and <b>Attention</b> - GitHub Pages", "url": "https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html", "isFamilyFriendly": true, "displayUrl": "https://lena-voita.github.io/nlp_course/seq2seq_and_<b>attention</b>.html", "snippet": "Intuitively, Transformer&#39;s <b>encoder</b> <b>can</b> <b>be thought</b> of as a sequence of reasoning steps (layers). At each step, tokens look at each other (this is where we need <b>attention</b> - self-<b>attention</b>), exchange information and try to understand each other better in the context of the whole sentence. This happens in several layers (e.g., 6).", "dateLastCrawled": "2022-02-02T22:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>GitHub</b> - Floki678/<b>Language-translator</b>: using <b>encoder</b> decoder neural net ...", "url": "https://github.com/Floki678/Language-translator", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/Floki678/<b>Language-translator</b>", "snippet": "<b>LANGUAGE TRANSLATOR</b>. Using <b>encoder</b> decoder RNN model to tokenize input text in English language and translate it into French. <b>Encoder</b> model to generate a <b>thought</b> vector and passing the <b>thought</b> vector to decoder model. <b>Thought</b> vector providing initial state to GRU units used in the decoder which computes equivalent word tokens in another language.", "dateLastCrawled": "2021-08-17T21:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Word Level English <b>to Marathi Neural Machine Translation using</b> <b>Encoder</b> ...", "url": "https://towardsdatascience.com/word-level-english-to-marathi-neural-machine-translation-using-seq2seq-encoder-decoder-lstm-model-1a913f2dc4a7", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/word-level-english-to-marathi-neural-machine...", "snippet": "The purpose of this blog post was to give an intuitive explanation on how to build basic level sequence to sequence models using LSTM and not to develop a top quality language <b>translator</b>. So keep in mind that the results are not world class (and you don\u2019t start comparing with google translate) for many reasons. The most important reason being is that the dataset size is very small, only 33000 pairs of sentences (yes these are too few). If you want to improve the quality of translations, I ...", "dateLastCrawled": "2022-01-30T19:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>GitHub</b> - <b>DataDrivenGit/Machine_Translation</b>: English to Spanish ...", "url": "https://github.com/DataDrivenGit/Machine_Translation", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/DataDrivenGit/Machine_Translation", "snippet": "The whole model <b>can</b> <b>be thought</b> of as a seq2seq translation model with an Attention layer between the decoder and <b>encoder</b>. where Seq2Seq models <b>can</b> <b>be thought</b> of as an extention to normal seqence models which <b>can</b> be used when the seqence lengths of inputs and targets differ. Examples for such problems include machine translation, caption generation etc.", "dateLastCrawled": "2021-08-29T08:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Attention in Deep Learning, Part</b> II \u2013 The Bioinformatics Blog", "url": "https://thebioinformaticsblog.wordpress.com/2020/12/10/attention-part-ii/", "isFamilyFriendly": true, "displayUrl": "https://thebioinformaticsblog.wordpress.com/2020/12/10/attention-part-ii", "snippet": "An <b>encoder</b>-decoder model <b>can</b> <b>be thought</b> of as two neural networks stacked in series. The first, the <b>encoder</b>, is designed to take a complex set of input (such as an image or a sentence), and then output an encoded vector which represents this image. So somewhere in the middle of every <b>encoder</b>-decoder model is a simple vector which contains an encoded representation of the model\u2019s input. The second network is the \u201cdecoder.\u201d The decoder is simply a network which takes as input the vector ...", "dateLastCrawled": "2022-01-21T01:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Speech to Speech Translation using <b>Encoder</b> Decoder Architecture", "url": "https://www.irjet.net/archives/V7/i3/IRJET-V7I3779.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.irjet.net/archives/V7/i3/IRJET-V7I3779.pdf", "snippet": "of an <b>Encoder</b> and Decoder both of these are Recurrent Neural Networks and they are connected to <b>Thought</b> Vector as shown in fig. 2.2. The <b>Encoder</b> network here outputs a <b>Thought</b> Vector which is an array of float point numbers roughly between -1 and 1 which summarizes the contents or the meaning or the intentions of the input text. We then use", "dateLastCrawled": "2021-08-26T19:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Literature Survey : Spoken Language Translation</b>", "url": "https://www.cfilt.iitb.ac.in/resources/surveys/Sanket_SurveyPaper_SPKMT.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cfilt.iitb.ac.in/resources/surveys/Sanket_SurveyPaper_SPKMT.pdf", "snippet": "Speech <b>can</b> <b>be thought</b> of as a Markov model for many stochas-tic purposes. HMMs <b>can</b> also be trained automat-ically and are simple computationally. In ASR, the HMM outputs real-valued vectors that consists of cepstral coef\ufb01cients that are obtained by tak- ing the fourier transform of speech and taking the most important coef\ufb01cients. Each word, or more particularly each phoneme has different probabil-ity distribution and we combine lot of such HMMs for different words to get the phoneme ...", "dateLastCrawled": "2022-01-28T18:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "deep learning - How to create a language <b>translator</b> from scratch ...", "url": "https://datascience.stackexchange.com/questions/47278/how-to-create-a-language-translator-from-scratch", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/47278", "snippet": "A large dataset which has pairs of translations ( like English-French ). You <b>can</b> find such a dataset from here. A sequence-to-sequence RNN model. They have <b>Encoder</b>-Decoder architecture which encodes the source sentence into a <b>thought</b> vector and then decode it to form the translation. This image may be helpful.", "dateLastCrawled": "2022-02-02T03:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Fancy Text Converter \u1408 #1 BEST - Fancy Text Generator", "url": "https://www.fancytextconverter.com/", "isFamilyFriendly": true, "displayUrl": "https://www.fancytextconverter.com", "snippet": "The fancy text <b>translator</b> is an advanced online tool that allows you to generate fancy font styles and stylish text designs with different combinations of styles, font colors, and designs. Millions of users are currently using this text generator to make impressive fonts and copy &amp; paste on social media to make their profiles more impressive. The letter converter is incredibly easy to use, and anyone <b>can</b> easily convert their text in a stylish font. To create the cool text, you need to put ...", "dateLastCrawled": "2022-02-02T22:16:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine translation</b> with the <b>seq2seq</b> model: Different approaches | by ...", "url": "https://towardsdatascience.com/machine-translation-with-the-seq2seq-model-different-approaches-f078081aaa37", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>machine-translation</b>-with-the-<b>seq2seq</b>-model-different...", "snippet": "You <b>can</b> create your <b>translator</b> for different languages by simply changing the dataset we are going to use here. We will use the Recurrent Neural Network topic \u2014 <b>seq2seq</b> i.e. the <b>Encoder</b>-Decoder model. In the below article, the <b>seq2seq</b> model is used to build a generative chatbot. Generative chatbots using the <b>seq2seq</b> model! A chatbot is a software that provides a real conversational experience to the user. There are closed domain chatbots\u2026 towardsdatascience.com. <b>Machine translation</b> is ...", "dateLastCrawled": "2022-02-03T06:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "What is Translators? Different type of translators - Computer Notes", "url": "https://ecomputernotes.com/compiler-design/translators-and-its-type", "isFamilyFriendly": true, "displayUrl": "https://ecomputernotes.com/compiler-design/<b>translators</b>-and-its-type", "snippet": "Roles of <b>translator</b> are: \u2022 Translating the high-level language program input into an equivalent machine language program. \u2022 Providing diagnostic messages wherever the programmer violates specification of the high-level language program. Different type of translators. The different types of <b>translator</b> are as follows: Compiler. Compiler is a <b>translator</b> which is used to convert programs in high-level language to low-level language. It translates the entire program and also reports the ...", "dateLastCrawled": "2022-02-02T22:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Build A <b>Simple Machine Translator encoder-decoder framework with</b> lstm ...", "url": "https://6chaoran.github.io/data-story/deep-learning/nlp/build-a-simple-machine-translator-part1/", "isFamilyFriendly": true, "displayUrl": "https://6chaoran.github.io/.../deep-learning/nlp/build-a-simple-machine-<b>translator</b>-part1", "snippet": "initial states and token. The initial states is predicted results from <b>encoder</b>. That <b>can</b> be achieved by enc_model.predict(src_input_seq).The initial token is &lt;s&gt;, I keep track of a triple of (index, token, prediction probability) for each prediction, thus the triple for initial token is ([1],[&#39;&lt;s&gt;&#39;],[1.0]).The following code snippet generate the initial states and token, with the given source sentence.", "dateLastCrawled": "2022-02-01T18:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Hardware Encoding vs. Software Encoding</b> \u2013 Restream Blog", "url": "https://restream.io/blog/hardware-encoding-vs-software-encoding/", "isFamilyFriendly": true, "displayUrl": "https://restream.io/blog/<b>hardware-encoding-vs-software-encoding</b>", "snippet": "The encoders\u2019 dead-center position in a streaming setup is necessary because the <b>encoder</b> is the <b>translator</b> and the communicator. Take it out of the equation and video streaming platforms would have a hard time understanding and reproducing the data you\u2019re sending them. That\u2019s the special role that grants encoders their importance. The two types of encoders: What are the pros and cons of each? The abiding question with encoders is, \u201cWhich ones are better: hardware encoders or software", "dateLastCrawled": "2022-02-02T02:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>GitHub</b> - <b>mshadloo/Neural-Machine-Translation-with-Attention</b>: I ...", "url": "https://github.com/mshadloo/Neural-Machine-Translation-with-Attention", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/<b>mshadloo/Neural-Machine-Translation-with-Attention</b>", "snippet": "The <b>encoder</b> <b>can</b> be a Bidirectional LSTM, a simple LSTM or a GRU, and the decoder <b>can</b> be an LSTM, or a GRU. I have a argument for <b>encoder</b> type (RNN model used in <b>encoder</b>); it <b>can</b> be &#39;bidirectional&#39;, &#39;lstm&#39; or &#39;gru&#39;. When this argument is set to &#39;bidirectional&#39;, the model has Bidirectional LSTM as the enocder a simple LSTM as the decoder. When it is set to &#39;lstm&#39;, the <b>encoder</b> and decoder are both simple LSTMs, and for the &#39;gru&#39; value, they are both GRUs. Thus, I <b>can</b> have different three models.", "dateLastCrawled": "2022-02-03T08:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "transistors - Need <b>for MOSFET in encoder circuit</b>? - Electrical ...", "url": "https://electronics.stackexchange.com/questions/468963/need-for-mosfet-in-encoder-circuit", "isFamilyFriendly": true, "displayUrl": "https://electronics.stackexchange.com/questions/468963/need-<b>for-mosfet-in-encoder-circuit</b>", "snippet": "I would check alignment if this is a setup with a separate <b>encoder</b> disc...those <b>can</b> be really finicky. Also, decoupling caps. \\$\\endgroup\\$ \u2013 DKNguyen. Nov 25 &#39;19 at 16:14 \\$\\begingroup\\$ Will do that ! thankyou @DKNguyen \\$\\endgroup\\$ \u2013 Mr.Sky. Nov 25 &#39;19 at 16:15 | Show 1 more comment. 1 Answer Active Oldest Votes. 5 \\$\\begingroup\\$ Q1 and Q2 MOSFETs are being used as a level <b>translator</b>. The way that type of level <b>translator</b> works is a little unusual <b>compared</b> to how MOSFETs are ...", "dateLastCrawled": "2022-01-22T19:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Roles</b> - guide.encode.moe", "url": "https://guide.encode.moe/overview/roles.html", "isFamilyFriendly": true, "displayUrl": "https://guide.encode.moe/overview/<b>roles</b>.html", "snippet": "Optional: <b>Translator</b>; Optional: Translation Checker; Optional: Karaoke Effects Creator; Optional: Project Leader; In this guide, we will only be providing in-depth guides for the <b>Encoder</b>, Timer, and Typesetter <b>roles</b>. However, Quality Checkers are often expected to be familiar with most or all of the <b>roles</b> in order to recognize errors. This page serves as just an overview of the work various <b>roles</b> will be expected to complete. <b>Encoder</b>. Time commitment per episode: 20 minutes - 2 hours active ...", "dateLastCrawled": "2022-01-29T05:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Encoder</b> and Hall Sensors - FEDEVEL Forum", "url": "https://designhelp.fedevel.com/forum/test/beginners/18687-encoder-and-hall-sensors", "isFamilyFriendly": true, "displayUrl": "https://designhelp.fedevel.com/forum/test/beginners/18687-<b>encoder</b>-and-hall-sensors", "snippet": "My problem is that i&#39;m not sure if i <b>can</b> connect the outputs of <b>Encoder</b> and Hall Sensors directely into the microcontroller or i should use another IC between sensor and microcontroller . So <b>can</b> you help me what&#39;s the best solution or what should i do exactelly ?? Tags: None. Roibert Junior Member Find all posts. View Profile. Close. Roibert. Junior Member. Join Date: Oct 2017; Posts: 18; Share Tweet #2. 12-16-2021, 01:57 AM. Hi. In the datasheet of your <b>encoder</b>, look at VoH min (min voltage ...", "dateLastCrawled": "2022-02-03T07:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "XOR <b>Online</b> Decrypt &amp; <b>Encrypt</b> with our decrypter", "url": "https://md5decrypt.net/en/Xor/", "isFamilyFriendly": true, "displayUrl": "https://md5decrypt.net/en/Xor", "snippet": "That bit will be equal to 1 if the two <b>compared</b> bits were different, 0 if they were equal. Xor encryption is commonly used in several symmetric ciphers (especially AES). A symetric cipher is simply a cipher in which the key is used for xor encryption and decryption process. The XOR operand is so applied to each bit between the text you want to <b>encrypt</b> and the key you&#39;ll choose. Examples are better than words, let&#39;s take the word &quot;xor&quot;. We want to <b>encrypt</b> it with the key &quot;cle&quot;. First we have ...", "dateLastCrawled": "2022-02-03T02:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Letters To Numbers</b> (online tool) | Boxentriq", "url": "https://www.boxentriq.com/code-breaking/letters-to-numbers", "isFamilyFriendly": true, "displayUrl": "https://www.boxentriq.com/code-breaking/<b>letters-to-numbers</b>", "snippet": "<b>Letters To Numbers</b> Converter. Convert <b>letters to numbers</b> in various formats. Numbering the letters so A=1, B=2, etc is one of the simplest ways of converting them to numbers. This is called the A1Z26 cipher. However, there are more options such as ASCII codes and tap codes to decode numbers. This translation tool will help you easily convert ...", "dateLastCrawled": "2022-02-03T07:25:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "9.6. <b>Encoder-Decoder</b> Architecture \u2014 Dive into Deep <b>Learning</b> 0.17.2 ...", "url": "https://d2l.ai/chapter_recurrent-modern/encoder-decoder.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_recurrent-modern/<b>encoder-decoder</b>.html", "snippet": "<b>Encoder-Decoder</b> Architecture \u2014 Dive into Deep <b>Learning</b> 0.17.0 documentation. 9.6. <b>Encoder-Decoder</b> Architecture. As we have discussed in Section 9.5, <b>machine</b> translation is a major problem domain for sequence transduction models, whose input and output are both variable-length sequences. To handle this type of inputs and outputs, we can design ...", "dateLastCrawled": "2022-01-30T23:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Solving Word <b>Analogies: A Machine Learning Perspective</b> | Request PDF", "url": "https://www.researchgate.net/publication/335597029_Solving_Word_Analogies_A_Machine_Learning_Perspective", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/335597029_Solving_Word_Analogies_A_<b>Machine</b>...", "snippet": "We introduce a supervised corpus-based <b>machine</b> <b>learning</b> algorithm for classifying analogous word pairs, and we show that it can solve multiple-choice SAT <b>analogy</b> questions, TOEFL synonym questions ...", "dateLastCrawled": "2021-10-16T21:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and-sample-codes ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/README.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "Usage: In fraud detection, use Auto-<b>Encoder</b> to compress all data to dense vector, then kNN is used to detect outliers; Reinforcement <b>Learning</b> Definitions. Reinforcement <b>learning</b> (RL) is an area of <b>machine</b> <b>learning</b> that focuses on how agent to act in an environment in order to maximize some given reward. Markov Decision Processes (MDPs) Components", "dateLastCrawled": "2021-09-22T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Titanic \u2014 Predicting Survival rates using <b>Machine</b> <b>Learning</b> | by Punith ...", "url": "https://medium.com/codex/titanic-predicting-survival-rates-using-machine-learning-3e83c56af29f", "isFamilyFriendly": true, "displayUrl": "https://medium.com/codex/titanic-predicting-survival-rates-using-<b>machine</b>-<b>learning</b>-3e83...", "snippet": "Label <b>Encoder</b> refers to converting the labels into numeric form so as to convert it into the <b>machine</b> readable form. <b>Machine</b> <b>learning</b> algorithms can then decide in a better way on how those labels ...", "dateLastCrawled": "2022-02-03T01:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Learning to Generate Long-term Future via Hierarchical</b> Prediction", "url": "http://proceedings.mlr.press/v70/villegas17a.html", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v70/villegas17a.html", "snippet": "Our model is built with a combination of LSTM and <b>analogy</b> based <b>encoder</b>-decoder convolutional neural networks, which independently predict the video structure and generate the future frames, respectively. In experiments, our model is evaluated on the Human3.6M and Penn Action datasets on the task of long-term pixel-level video prediction of humans performing actions and demonstrate significantly better results than the state-of-the-art.} } Copy to Clipboard Download. Endnote %0 Conference ...", "dateLastCrawled": "2022-01-29T17:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "The conceptual arithmetics of concepts | by Assaad MOAWAD | DataThings ...", "url": "https://medium.com/datathings/the-conceptual-arithmetics-of-concepts-369df29e4e0f", "isFamilyFriendly": true, "displayUrl": "https://medium.com/datathings/the-conceptual-arithmetics-of-concepts-369df29e4e0f", "snippet": "<b>Machine</b> <b>learning</b> field is an amazing and very fast evolving domain. However, it is still hard to use it in its current state due to its cost and complexity. With time, we will have more and more ...", "dateLastCrawled": "2022-01-04T22:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Transformers in NLP: A beginner friendly explanation | Towards Data Science", "url": "https://towardsdatascience.com/transformers-89034557de14", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>transformer</b>s-89034557de14", "snippet": "<b>Encoder</b>-Decoder Attention: Attention between the input sequence and the output sequence. ... If you are looking for an <b>analogy</b> between self attention and attention, think of z serving the purpose of context vectors and not global alignment weights. The <b>Transformer</b>. \u26a0\ufe0f A word of caution: the contents of this image may appear exponentially more complicated than they are. We will break this scary beast down into small baby beasts and it will all make sense. (I promise #2) (left) The ...", "dateLastCrawled": "2022-02-02T13:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A Gentle Introduction to <b>Long Short-Term Memory</b> Networks by the Experts", "url": "https://machinelearningmastery.com/gentle-introduction-long-short-term-memory-networks-experts/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/gentle-introduction-<b>long-short-term-memory</b>-networks...", "snippet": "<b>Long Short-Term Memory</b> (LSTM) networks are a type of recurrent neural network capable of <b>learning</b> order dependence in sequence prediction problems. This is a behavior required in complex problem domains like <b>machine</b> translation, speech recognition, and more. LSTMs are a complex area of deep <b>learning</b>. It can be hard to get your hands around what LSTMs are, and how terms like bidirectional", "dateLastCrawled": "2022-01-31T03:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>machine</b> <b>learning</b> - What is an <b>autoencoder</b>? - Data Science Stack Exchange", "url": "https://datascience.stackexchange.com/questions/80389/what-is-an-autoencoder", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/80389", "snippet": "I am a student and I am studying <b>machine</b> <b>learning</b>. I am focusing on deep generative models, and in particular to autoencoders and variational autoencoders (VAE).. I am trying to understand the concept, but I am having some problems. So far, I have understood that an <b>autoencoder</b> takes an input, for example an image, and wants to reduce this image into a latent space, which should contain the underlying features of the dataset, with an operation of encoding, then, with an operation of decoding ...", "dateLastCrawled": "2022-01-26T06:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Dive into Deep <b>Learning</b> \u2014 Dive into Deep <b>Learning</b> 0.17.2 documentation", "url": "http://d2l.ai/", "isFamilyFriendly": true, "displayUrl": "d2l.ai", "snippet": "[Dec 2021] We added a new option to run this book for free: check out SageMaker Studio Lab. [Jul 2021] We have improved the content and added TensorFlow implementations up to Chapter 11. To keep track of the latest updates, just follow D2L&#39;s open-source project. [Jan 2021] Check out the brand-new Chapter: Attention Mechanisms.We have also added PyTorch implementations.", "dateLastCrawled": "2022-01-30T00:18:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A Gentle Introduction to <b>LSTM Autoencoders</b> - <b>Machine</b> <b>Learning</b> Mastery", "url": "https://machinelearningmastery.com/lstm-autoencoders/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>lstm-autoencoders</b>", "snippet": "This is challenging because <b>machine</b> <b>learning</b> algorithms, and neural networks in particular, are designed to work with fixed length inputs. Another challenge with sequence data is that the temporal ordering of the observations can make it challenging to extract features suitable for use as input to supervised <b>learning</b> models, often requiring deep expertise in the domain or in the field of signal processing. Finally, many predictive modeling problems involving sequences require a prediction ...", "dateLastCrawled": "2022-02-03T00:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>machine</b> <b>learning</b> - <b>Parameters tuning for auto-encoders</b> - Cross Validated", "url": "https://stats.stackexchange.com/questions/235114/parameters-tuning-for-auto-encoders", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/235114/<b>parameters-tuning-for-auto-encoders</b>", "snippet": "Actually, the cost function of a sparse auto-<b>encoder is like</b>. I tested with my datasets, it seems that all these four parameters have impact on the final results. Are there any general rules of &#39;optimal&#39; settings of these four parameters? When I was using Support Vector <b>Machine</b> based classifier, there is a &#39;grid search&#39; method to optimize the two hyper-parameters of the SVM. Are there any similar method available for (sparse) auto-encoders? As far as I see, grid search is feasible to ...", "dateLastCrawled": "2022-01-28T00:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Log Data Anomaly Detection Using a <b>Machine</b> <b>Learning</b> Model", "url": "https://insights.ltts.com/story/log-data-anomaly-detection-using-a-machine-learning-model/page/1", "isFamilyFriendly": true, "displayUrl": "https://insights.ltts.com/story/log-data-anomaly-detection-using-a-<b>machine</b>-<b>learning</b>...", "snippet": "In this paper, we have explored various <b>machine</b> <b>learning</b> algorithms and an auto encoder to detect anomalies which can help the developers to quickly identify and derive relevant and appropriate information from the logs maintained. &lt;small&gt;An Industry Perspective. System Logs: An Industry Perspective . There are multiple examples of system generated logs in use: Events of logs generated from server application ; A database system maintaining transaction logs which could be used for ...", "dateLastCrawled": "2022-01-26T20:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>The security of machine learning</b> - researchgate.net", "url": "https://www.researchgate.net/publication/220343885_The_security_of_machine_learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/220343885_<b>The_security_of_machine_learning</b>", "snippet": "In particular, self-supervised <b>learning</b> aims to pre-train an encoder using a large amount of unlabeled data. The pre-trained <b>encoder is like</b> an &quot;operating system&quot; of the AI ecosystem. In ...", "dateLastCrawled": "2022-01-12T09:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>machine</b> <b>learning</b> - What is the input for the prior model of VQ-VAE ...", "url": "https://ai.stackexchange.com/questions/17203/what-is-the-input-for-the-prior-model-of-vq-vae", "isFamilyFriendly": true, "displayUrl": "https://ai.stackexchange.com/questions/17203", "snippet": "<b>machine</b>-<b>learning</b> generative-model variational-autoencoder. Share. Improve this question. Follow asked Dec 22 &#39;19 at 6:08. Diego Gomez Diego Gomez. 393 3 3 silver badges 9 9 bronze badges $\\endgroup$ Add a comment | 1 Answer Active Oldest Votes. 0 $\\begingroup$ Some notes about VQ-VAE: In the paper, they used PixelCNN to learn the prior. PixelCNN is trained on images. The discrete latent variables are just the indices of the embedding vectors. For example, you can put your embedding vectors ...", "dateLastCrawled": "2022-01-07T22:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Summary of \u2014 <b>SegNet</b>: <b>A Deep Convolutional Encoder-Decoder</b> Architecture ...", "url": "https://towardsdatascience.com/summary-of-segnet-a-deep-convolutional-encoder-decoder-architecture-for-image-segmentation-75b2805d86f5", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/summary-of-<b>segnet</b>-<b>a-deep-convolutional-encoder-decoder</b>...", "snippet": "Each <b>encoder is like</b> Fig 3. The novelty is in the subsampling stage, Max-pooling is used to achieve translation invariance over small spatial shifts in the image, combine that with Subsampling and it leads to each pixel governing a larger input image context (spatial window). These methods achieve better classification accuracy but reduce the feature map size, this leads to lossy image representation with blurred boundaries which is not ideal for segmentation purpose. It is desired that ...", "dateLastCrawled": "2022-01-30T01:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "[2110.15444] 10 Security and Privacy Problems in Self-Supervised <b>Learning</b>", "url": "https://arxiv.org/abs/2110.15444", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/abs/2110.15444", "snippet": "The pre-trained <b>encoder is like</b> an &quot;operating system&quot; of the AI ecosystem. Specifically, the encoder can be used as a feature extractor for many downstream tasks with little or no labeled training data. Existing studies on self-supervised <b>learning</b> mainly focused on pre-training a better encoder to improve its performance on downstream tasks in non-adversarial settings, leaving its security and privacy in adversarial settings largely unexplored. A security or privacy issue of a pre-trained ...", "dateLastCrawled": "2021-12-28T23:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "probability - why a denoising auto-<b>encoder is like</b> performing ...", "url": "https://math.stackexchange.com/questions/2318301/why-a-denoising-auto-encoder-is-like-performing-stochastic-gradient-this-on-this", "isFamilyFriendly": true, "displayUrl": "https://math.stackexchange.com/questions/2318301", "snippet": "why a denoising auto-<b>encoder is like</b> performing stochastic gradient this on this expression? Ask Question Asked 4 years, 7 months ago. Active 4 years, 7 months ago. Viewed 665 times 2 1 $\\begingroup$ I was reading ...", "dateLastCrawled": "2022-01-24T01:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "[2110.15444v2] 10 Security and Privacy Problems in Self-Supervised <b>Learning</b>", "url": "https://arxiv.org/abs/2110.15444v2", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/abs/2110.15444v2", "snippet": "Self-supervised <b>learning</b> has achieved revolutionary progress in the past several years and is commonly believed to be a promising approach for general-purpose AI. In particular, self-supervised <b>learning</b> aims to pre-train an encoder using a large amount of unlabeled data. The pre-trained <b>encoder is like</b> an &quot;operating system&quot; of the AI ecosystem. Specifically, the encoder can be used as a feature extractor for many downstream tasks with little or no labeled training data. Existing studies on ...", "dateLastCrawled": "2021-11-08T02:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Convolutional Coding</b> - GaussianWaves", "url": "https://www.gaussianwaves.com/2010/06/convolutional-coding-2/", "isFamilyFriendly": true, "displayUrl": "https://www.gaussianwaves.com/2010/06/<b>convolutional-coding</b>-2", "snippet": "Till now the <b>encoder is like</b> a black box to us in the sense that we don\u2019t know how the memory elements are utilized to generate the output bits from the input. To fully understand the encoder structure we need something called \u201cgenerator polynomials\u201d that tell us how the memory elements are linked to achieve encoding. The generator polynomials for a specific convolutional encoder set (n,k,L) are usually found through simulation. The set (n,k,L) along with n generator polynomials ...", "dateLastCrawled": "2022-01-09T00:08:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Categorical Encoding with CatBoost Encoder</b> - GeeksforGeeks", "url": "https://www.geeksforgeeks.org/categorical-encoding-with-catboost-encoder/", "isFamilyFriendly": true, "displayUrl": "https://www.geeksforgeeks.org/<b>categorical-encoding-with-catboost-encoder</b>", "snippet": "Many <b>machine</b> <b>learning</b> algorithms require data to be numeric. So, before training a model, we need to convert categorical data into numeric form. There are various categorical encoding methods available. Catboost is one of them. Catboost is a target-based categorical encoder. It is a supervised encoder that encodes categorical columns according to the target value. It supports binomial and continuous targets. Target encoding is a popular technique used for categorical encoding. It replaces a ...", "dateLastCrawled": "2022-02-03T12:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Intuitively Understanding Variational Autoencoders | by Irhum Shafkat ...", "url": "https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/intuitively-understanding-variational-<b>autoencoder</b>s-1bfe...", "snippet": "The <b>encoder is similar</b>, it is simply is a network that takes in an input and produces a much smaller representation (the encoding), that contains enough information for the next part of the network to process it into the desired output format. Typically, the encoder is trained together with the other parts of the network, optimized via back-propagation, to produce encodings specifically useful for the task at hand. In CNNs, the 1000-dimensional encodings produced are such that they\u2019re ...", "dateLastCrawled": "2022-02-02T13:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Implementing an <b>Autoencoder</b> in TensorFlow 2.0 | by Abien Fred Agarap ...", "url": "https://towardsdatascience.com/implementing-an-autoencoder-in-tensorflow-2-0-5e86126e9f7", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/implementing-an-<b>autoencoder</b>-in-tensorflow-2-0-5e86126e9f7", "snippet": "We deal with huge amount of data in <b>machine</b> <b>learning</b> which naturally leads to more computations. However, we can also just pick the parts of the data that contribute the most to a model\u2019s <b>learning</b>, thus leading to less computations. The process of choosing the important parts of the data is known as feature selection, which is among the number of use cases for an <b>autoencoder</b>. But what exactly is an <b>autoencoder</b>? Well, let\u2019s first recall that a neural network is a computational model that ...", "dateLastCrawled": "2022-02-03T07:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Implementing an <b>Autoencoder</b> in TensorFlow 2.0 - Abien Fred Agarap", "url": "https://afagarap.github.io/2019/03/20/implementing-autoencoder-in-tensorflow-2.0.html", "isFamilyFriendly": true, "displayUrl": "https://afagarap.github.io/2019/03/20/implementing-<b>autoencoder</b>-in-tensorflow-2.0.html", "snippet": "Google announced a major upgrade on the world\u2019s most popular open-source <b>machine</b> <b>learning</b> library, TensorFlow, with a promise of focusing on simplicity and ease of use, eager execution, intuitive high-level APIs, and flexible model building on any platform. This post is a humble attempt to contribute to the body of working TensorFlow 2.0 examples. Specifically, we shall discuss the subclassing API implementation of an <b>autoencoder</b>. To install TensorFlow 2.0, use the following pip install ...", "dateLastCrawled": "2022-01-31T12:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "An Introduction to Generative <b>Deep Learning</b> | by Anil Chandra Naidu ...", "url": "https://medium.com/analytics-vidhya/an-introduction-to-generative-deep-learning-792e93d1c6d4", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/an-introduction-to-generative-<b>deep-learning</b>-792e93...", "snippet": "An autoencoder is a type of ANN used to learn efficient data codings in an unsupervised manner. The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for ...", "dateLastCrawled": "2022-01-29T19:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Encoding</b> <b>categorical</b> variables - Stacked Turtles", "url": "https://kiwidamien.github.io/encoding-categorical-variables.html", "isFamilyFriendly": true, "displayUrl": "https://kiwidamien.github.io/<b>encoding</b>-<b>categorical</b>-variables.html", "snippet": "The way you encode <b>categorical</b> variables changes how effective your <b>machine</b> <b>learning</b> algorithm is. This article will go over some common <b>encoding</b> techniques, as well as their advantages and disadvantages. Some terminology. Levels: A levels of a non-numeric feature are the number of distinct values. The examples listed above are all examples of levels. The number of levels can vary wildly: the number of races for a patient is typically four (asian, black, hispanic, and white), the number of ...", "dateLastCrawled": "2022-01-30T23:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Hands-on with Feature Engineering Techniques</b>: Advanced Methods | by ...", "url": "https://heartbeat.comet.ml/hands-on-with-feature-engineering-advanced-methods-in-python-for-machine-learning-e05bf12da06a", "isFamilyFriendly": true, "displayUrl": "https://heartbeat.comet.ml/<b>hands-on-with-feature-engineering</b>-advanced-methods-in...", "snippet": "This post is a part of a series about <b>feature engineering techniques</b> for <b>machine</b> <b>learning</b> with Python. You can check out the rest of the articles: <b>Hands-on with Feature Engineering Techniques</b>: Broad Introduction. <b>Hands-on with Feature Engineering Techniques</b>: Variable Types. <b>Hands-on with Feature Engineering Techniques</b>: Common Issues in Datasets. <b>Hands-on with Feature Engineering Techniques</b>: Imputing Missing Values. <b>Hands-on with Feature Engineering Techniques</b>: Encoding Categorical Variables ...", "dateLastCrawled": "2022-02-01T14:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Frontiers | Deep <b>Learning</b> for Understanding <b>Satellite Imagery</b>: An ...", "url": "https://www.frontiersin.org/articles/10.3389/frai.2020.534696/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/frai.2020.534696", "snippet": "The left half of the network (<b>encoder) is similar</b> to a CNN, tasked with coming up with a low dimensional dense representation of the input, and the right side (decoder) then up-samples the learned feature representations to the same shape as the input. The shortcut connections let information flow from the encoder to the decoder and help the network keeping spatial information. As the work of Li et al. (2017) has impressively shown, U-Nets benefit greatly from a deeper model architecture. It ...", "dateLastCrawled": "2022-01-31T16:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Fully Convolutional Refined Auto-Encoding Generative Adversarial ...", "url": "https://becominghuman.ai/3d-multi-object-gan-7b7cee4abf80", "isFamilyFriendly": true, "displayUrl": "https://becominghuman.ai/<b>3d-multi-object-gan</b>-7b7cee4abf80", "snippet": "The basic architecture of <b>encoder is similar</b> to discriminator network of 3DGAN[1]. The difference is the last layer which is 1x1x1 fully convolution.-Generator. The basic architecture of generator is also similar to 3DGAN[1] as above figure. The difference is the last layer which has 12 channels and is activated by softmax. Also, the first layer of latent space is flatten. -Discriminator. The basic architecture of discriminator is also similar to 3DGAN[1]. The difference is the activation ...", "dateLastCrawled": "2022-01-26T00:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Deep <b>learning for smart manufacturing: Methods and applications</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0278612518300037", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0278612518300037", "snippet": "Typical <b>machine</b> <b>learning</b> techniques are reviewed in [, ] for intelligent manufacturing, and their strengths and weaknesses are also discussed in a wide range of manufacturing applications. A comparative study of <b>machine</b> <b>learning</b> algorithms including Artificial Neural Network, Support Vector <b>Machine</b>, and Random Forest is performed for machining tool wear prediction. The schemes, techniques and paradigm of developing decision making support systems are reviewed for the monitoring of machining ...", "dateLastCrawled": "2022-02-02T21:23:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Encoder G25 G27 60 Slot - lgpfc.co.uk", "url": "https://lgpfc.co.uk/Encoder-G25-g27-60-Slot", "isFamilyFriendly": true, "displayUrl": "https://lgpfc.co.uk/Encoder-G25-g27-60-Slot", "snippet": "This gameplay is based on the traditional, casino-style slot <b>machine</b>. At the same time, each Online Encoder G25 G27 60 Slot Slots game will have its own unique set of individual rules and characteristics. Before playing any new Online Encoder G25 G27 60 Slot Slots game, you should become familiar with how the game works by trying the free demo version and having a close look at the game\u2019s paytable. Sports. Canada. The Canadian regulatory environment is <b>just as Encoder</b> G25 G27 60 Slot ...", "dateLastCrawled": "2022-01-16T21:44:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Google AI</b> Blog: July 2019", "url": "https://ai.googleblog.com/2019/07/", "isFamilyFriendly": true, "displayUrl": "https://ai.googleblog.com/2019/07", "snippet": "Such a multitask trained <b>encoder can be thought of as</b> <b>learning</b> a latent representation of the input that maintains information about the underlying linguistic content. Overview of the Parrotron model architecture. An input speech spectrogram is passed through encoder and decoder neural networks to generate an output spectrogram in a new voice. Case Studies To demonstrate a proof of concept, we worked with our fellow Google research scientist and mathematician Dimitri Kanevsky, who was born ...", "dateLastCrawled": "2022-01-29T22:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Google AI Blog: Parrotron: New Research into Improving Verbal ...", "url": "https://ai.googleblog.com/2019/07/parrotron-new-research-into-improving.html", "isFamilyFriendly": true, "displayUrl": "https://ai.googleblog.com/2019/07/parrotron-new-research-into-improving.html", "snippet": "Such a multitask trained <b>encoder can be thought of as</b> <b>learning</b> a latent representation of the input that maintains information about the underlying linguistic content. Overview of the Parrotron model architecture. An input speech spectrogram is passed through encoder and decoder neural networks to generate an output spectrogram in a new voice. Case Studies To demonstrate a proof of concept, we worked with our fellow Google research scientist and mathematician Dimitri Kanevsky, who was born ...", "dateLastCrawled": "2022-01-19T04:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Seq2seq and <b>Attention</b> - GitHub Pages", "url": "https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html", "isFamilyFriendly": true, "displayUrl": "https://lena-voita.github.io/nlp_course/seq2seq_and_<b>attention</b>.html", "snippet": "Intuitively, Transformer&#39;s <b>encoder can be thought of as</b> a sequence of reasoning steps (layers). At each step, tokens look at each other (this is where we need <b>attention</b> - self-<b>attention</b>), exchange information and try to understand each other better in the context of the whole sentence. This happens in several layers (e.g., 6).", "dateLastCrawled": "2022-02-02T22:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Using <b>Bidirectional</b> Generative Adversarial Networks to estimate Value ...", "url": "https://towardsdatascience.com/using-bidirectional-generative-adversarial-networks-to-estimate-value-at-risk-for-market-risk-c3dffbbde8dd", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/using-<b>bidirectional</b>-generative-adversarial-networks-to...", "snippet": "Note that given an optimal discriminator, the objective function of the generator and <b>encoder can be thought of as</b> that of an autoencoder, where the generator plays the role of a decoder. The objective function of the generator and encoder is simply to minimize the objective function of the discriminator, i.e., we have not explicitly specified the structure of the reconstruction loss as one might do so with an autoencoder. This implicit minimization of the reconstruction loss is yet another ...", "dateLastCrawled": "2022-01-31T10:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Distributed Coding</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/engineering/distributed-coding", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/engineering/<b>distributed-coding</b>", "snippet": "A Wyner\u2013Ziv <b>encoder can be thought of as</b> a quantizer followed by a Slepian\u2013Wolf encoder. In cases of images and video, existing <b>distributed coding</b> schemes add the Wyner\u2013Ziv encoder into the standard transform coding structure. As with the centralized case, a linear transform is independently applied to each image or video frame. Each transform coefficient is still treated independently, but it is fed into a Wyner\u2013Ziv coder instead of a scalar quantizer and an entropy coder. We refer ...", "dateLastCrawled": "2022-01-04T19:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Parrotron: An End-to-End Speech-to-Speech Conversion Model and its ...", "url": "https://deepai.org/publication/parrotron-an-end-to-end-speech-to-speech-conversion-model-and-its-applications-to-hearing-impaired-speech-and-speech-separation", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/parrotron-an-end-to-end-speech-to-speech-conversion...", "snippet": "We apply more modern <b>machine</b> <b>learning</b> techniques to this problem, and demonstrate that, given sufficient training data, ... Such a multitask trained <b>encoder can be thought of as</b> <b>learning</b> a latent representation of the input that maintains information about the underlying transcript, i.e. one that is closer to the latent representation learned within a TTS sequence-to-sequence network. The decoder input is created by concatenating a 64-dim embedding for the grapheme emitted at the previous ...", "dateLastCrawled": "2022-01-18T09:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Distributed Source Coding: Theory, Algorithms and Applications</b> - PDF ...", "url": "https://epdf.pub/distributed-source-coding-theory-algorithms-and-applications.html", "isFamilyFriendly": true, "displayUrl": "https://epdf.pub/<b>distributed-source-coding-theory-algorithms-and-applications</b>.html", "snippet": "A Wyner\u2013Ziv <b>encoder can be thought of as</b> a quantizer followed by a Slepian\u2013Wolf encoder. In cases of images and video, existing distributed coding schemes add the Wyner\u2013 Ziv encoder into the standard transform coding structure. As with the centralized case, a linear transform is independently applied to each image or video frame. Each transform coef\ufb01cient is still treated independently, but it is fed into a Wyner\u2013Ziv coder instead of a scalar quantizer and an entropy coder. We ...", "dateLastCrawled": "2021-12-28T03:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Hands-On <b>Convolutional Neural Networks with TensorFlow</b>: Solve computer ...", "url": "https://dokumen.pub/hands-on-convolutional-neural-networks-with-tensorflow-solve-computer-vision-problems-with-modeling-in-tensorflow-and-python-9781789132823-1789132827.html", "isFamilyFriendly": true, "displayUrl": "https://dokumen.pub/hands-on-<b>convolutional-neural-networks-with-tensorflow</b>-solve...", "snippet": "In the <b>machine</b> <b>learning</b> stage, all the feature vectors will be given to a <b>machine</b> <b>learning</b> system that creates a model. We hope that this model can generalize and is able to predict the digit for any future images given to the system that it wasn\u2019t trained on. An integral part of an ML system is evaluation. When we evaluate our model, we see how well our model has done in a particular task. In our example, we would look at how accurately it can predict the digit from the image. Accuracy of ...", "dateLastCrawled": "2022-01-24T16:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) Parrotron: An End-to-End Speech-to-Speech Conversion Model and ...", "url": "https://www.researchgate.net/publication/335829307_Parrotron_An_End-to-End_Speech-to-Speech_Conversion_Model_and_its_Applications_to_Hearing-Impaired_Speech_and_Speech_Separation", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/335829307_Parrotron_An_End-to-End_Speech-to...", "snippet": "W.-c. W oo, \u201cConvolutional LSTM network: A <b>machine</b> <b>learning</b> approach for precipitation nowcasting,\u201d in Advances in Neural Information Processing Systems , 2015, pp. 802\u2013810.", "dateLastCrawled": "2022-01-29T14:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Error Diagnosis of Deep Monocular Depth Estimation Models", "url": "http://vision.soic.indiana.edu/papers/errordiagnosis2021iros.pdf", "isFamilyFriendly": true, "displayUrl": "vision.soic.indiana.edu/papers/errordiagnosis2021iros.pdf", "snippet": "<b>Machine</b> <b>learning</b>-based approaches such as Make3D [6], and more recent techniques based on deep <b>learning</b> [7], [8], have shown signi\ufb01cant promise. These techniques take a variety of approaches. For example, instead of directly estimating depth, BTS [9] estimates the parameters of local planes at various scales. The model is trained using only ground truth depth, as the local plane parameters are learned implicitly by the net-work. PlaneRCNN [10], another state-of-the-art technique, estimates ...", "dateLastCrawled": "2021-09-30T12:29:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Automatic <b>Machine</b> Translation Evaluation in Many Languages via Zero ...", "url": "https://aclanthology.org/2020.emnlp-main.8.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/2020.emnlp-main.8.pdf", "snippet": "We frame the task of <b>machine</b> translation evaluation as one of scoring <b>machine</b> transla-tion output with a sequence-to-sequence para-phraser, conditioned on a human reference. We propose training the paraphraser as a multi-lingual NMT system, treating paraphrasing as a zero-shot translation task (e.g., Czech to Czech). This results in the paraphraser\u2019s out-put mode being centered around a copy of the input sequence, which represents the best case scenario where the MT system output matches a ...", "dateLastCrawled": "2022-01-21T14:24:00.0000000Z", "language": "en", "isNavigational": false}]], "all_bing_queries": ["+(encoder)  is like +(translator)", "+(encoder) is similar to +(translator)", "+(encoder) can be thought of as +(translator)", "+(encoder) can be compared to +(translator)", "machine learning +(encoder AND analogy)", "machine learning +(\"encoder is like\")", "machine learning +(\"encoder is similar\")", "machine learning +(\"just as encoder\")", "machine learning +(\"encoder can be thought of as\")", "machine learning +(\"encoder can be compared to\")"]}