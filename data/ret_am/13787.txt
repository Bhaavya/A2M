{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Least</b> <b>squares</b> is a method of <b>fitting</b> a <b>regression</b> <b>line</b> which is robust ...", "url": "https://www.quora.com/Least-squares-is-a-method-of-fitting-a-regression-line-which-is-robust-i-e-safe-from-outliers-True-or-False", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>Least</b>-<b>squares</b>-is-a-method-of-<b>fitting</b>-a-<b>regression</b>-<b>line</b>-which-is...", "snippet": "Answer (1 of 2): This is false. So it is \u201c<b>least</b> <b>squares</b>\u201d - the square of the residual is what you are looking to minimise. Consider your point with the highest residuals and move it some small amount. Consider how your <b>line</b> of best fit will move. Consider for a given small change how much your g...", "dateLastCrawled": "2022-01-08T16:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "The Interesting World of Non-Linear Regressions | by Diego Manfre ...", "url": "https://towardsdatascience.com/the-interesting-world-of-non-linear-regressions-eb0c405fdc97", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/the-interesting-world-of-non-<b>line</b>ar-<b>regressions</b>-eb0c405...", "snippet": "But it keeps getting harder every time I add more <b>points</b> or when the curve I am looking for differs from <b>a straight</b> <b>line</b>. In this case, a curve <b>fitting</b> process can solve all my problems. I admit it is exciting to enter a <b>bunch</b> <b>of points</b> and find a curve that matches the trend \u201cperfectly\u201d. But how does this work? Why <b>fitting</b> a <b>line</b> is not the same as <b>fitting</b> a strange-shaped curve. Everyone is familiar with linear <b>least</b> <b>squares</b> but, what happens when the expression we are trying to match ...", "dateLastCrawled": "2022-01-29T16:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Non-Linear <b>Regression</b>", "url": "https://jblomo.github.io/datamining290/slides/2013-04-19-Nonlinear.pdf", "isFamilyFriendly": true, "displayUrl": "https://jblomo.github.io/datamining290/slides/2013-04-19-Non<b>line</b>ar.pdf", "snippet": "<b>straight</b> <b>line</b> or a flat plane <b>to a bunch</b> of data <b>points</b>. Sometimes the true relationship that you want to model is curved, rather than flat. For example, if something is growing exponentially, which means growing at a steady rate, the relationship between X and Y is curve, <b>like</b> that shown to the right. To fit something <b>like</b> this, you need non-linear <b>regression</b>. Often, you can adapt linear <b>least</b> <b>squares</b> to do this. The method is to create new variables from your data. The new variables are ...", "dateLastCrawled": "2022-02-03T11:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Ordinary least square method</b> (P5.JS code) and Intro to Gradient Descent ...", "url": "https://medium.com/@shaistha24/basic-concepts-you-should-know-before-starting-with-the-neural-networks-nn-2-a4105935201c", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@shaistha24/basic-concepts-you-should-know-before-starting-with-the...", "snippet": "We also know that, the simplest method for <b>fitting</b> a <b>regression</b> <b>line</b> is ordinary <b>least</b>-<b>squares</b> method, which, finds the best-<b>fitting</b> <b>line</b> for the observed data by minimizing the sum of the <b>squares</b> ...", "dateLastCrawled": "2022-01-28T09:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Non-Linear <b>Regression</b> - sambaker.com", "url": "https://sambaker.com/courses/J716/pdf/716-5%20Non-linear%20regression.pdf", "isFamilyFriendly": true, "displayUrl": "https://sambaker.com/courses/J716/pdf/716-5 Non-<b>line</b>ar <b>regression</b>.pdf", "snippet": "<b>straight</b> <b>line</b> or a flat plane <b>to a bunch</b> of data <b>points</b>. Sometimes the true relationship that you want to model is curved, rather than flat. For example, if something is growing exponentially, which means growing at a steady rate, the relationship between X and Y is curve, <b>like</b> that shown to the right. To fit something <b>like</b> this, you need non-linear <b>regression</b>. Often, you can adapt linear <b>least</b> <b>squares</b> to do this. The method is to create new variables from your data. The new variables are ...", "dateLastCrawled": "2021-11-22T05:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "correlation - What is the difference between linear <b>regression</b> on y ...", "url": "https://stats.stackexchange.com/questions/22718/what-is-the-difference-between-linear-regression-on-y-with-x-and-x-with-y", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/22718", "snippet": "$\\begingroup$ Pearson&#39;s correlation isn&#39;t quite <b>fitting</b> a <b>line</b>, @vonjd. It does turn out that it is equivalent to the slope of a fitted <b>least</b> <b>squares</b> <b>line</b> when the data were standardized first. The 1st principal component, when there are only 2 variables &amp; the data were standardized first, is sort of a fitted <b>line</b> that minimizes the orthogonal distances. HTH $\\endgroup$ \u2013 gung - Reinstate Monica. May 5 &#39;18 at 19:35 | Show 4 more comments. 14 $\\begingroup$ I&#39;m going to illustrate the answer ...", "dateLastCrawled": "2022-02-02T18:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Linear Regression in Python</b> using numpy + polyfit (with code base)", "url": "https://data36.com/linear-regression-in-python-numpy-polyfit/", "isFamilyFriendly": true, "displayUrl": "https://data36.com/<b>linear-regression-in-python</b>-numpy-polyfit", "snippet": "If you put all the x\u2013y value pairs <b>on a graph</b>, you\u2019ll get <b>a straight</b> <b>line</b>: The relationship between x and y is linear. Using the equation of this specific <b>line</b> (y = 2 * x + 5), if you change x by 1, y will always change by 2. And it doesn\u2019t matter what a and b values you use, your <b>graph</b> will always show the same characteristics: it will always be <b>a straight</b> <b>line</b>, only its position and slope change. It also means that x and y will always be in linear relationship. In the linear function ...", "dateLastCrawled": "2022-02-02T22:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Statistics for Applications Exam 3 Solution", "url": "https://ocw.mit.edu/courses/mathematics/18-443-statistics-for-applications-spring-2015/exams/MIT18_443S15_Exam3_Sol.pdf", "isFamilyFriendly": true, "displayUrl": "https://<b>ocw.mit.edu</b>/courses/mathematics/18-443-statistics-for-applications-spring-2015/...", "snippet": "Solve for the <b>least</b>-<b>squares</b> <b>line</b> \u2013 Y\u02c6 = \u03b2x.\u02c6 ... The <b>least</b>-<b>squares</b> <b>regression</b> of y on x is given in terms of the standardized values: y\u02c6\u2212y = r. x \u2212 s y s x. A score of 90 on the midterm is (90 \u2212 75)/10 = 1.5 standard deviations above the mean. The predicted score on the \ufb01nal will be r \u00d7 1.5 = .9 standard deviations above the mean \ufb01nal score, which is 75+(.9)\u00d710 = 84. (b). For this case we need to regress the midterm score (x) on (y). The same argument in (a), reversing x ...", "dateLastCrawled": "2022-02-02T02:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Comparing Quadratic and Linear Regression</b>?", "url": "https://www.researchgate.net/post/Comparing_Quadratic_and_Linear_Regression", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/post/<b>Comparing_Quadratic_and_Linear_Regression</b>", "snippet": "When <b>fitting</b> GLMs in R, we need to specify which family function to use from a <b>bunch</b> of options <b>like</b> gaussian, poisson, binomial, quasi, etc. I would <b>like</b> to have your advice regarding how to ...", "dateLastCrawled": "2022-02-03T05:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Fitting</b> 3D <b>points</b> to <b>a straight</b> <b>line</b> - <b>Mathematica Stack Exchange</b>", "url": "https://mathematica.stackexchange.com/questions/123226/fitting-3d-points-to-a-straight-line", "isFamilyFriendly": true, "displayUrl": "https://mathematica.stackexchange.com/questions/123226", "snippet": "I have 3D data - a <b>bunch</b> of triples <b>like</b> { {x1, y1, z1}, {x2, y2, z2}, ...}, and I know they lie on a curve rather than a surface; in fact, I need a <b>least</b> <b>squares</b> fit of these <b>points</b> to a 3D <b>straight</b> <b>line</b>. That is, I am looking for six numbers ax, bx, ay, by, az, bz such that my <b>points</b> are as close as possible to the <b>line</b> {ax*t + bx, ay*t + by ...", "dateLastCrawled": "2022-01-15T10:44:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Curve <b>Fitting using Linear and Nonlinear Regression</b> - Statistics By Jim", "url": "https://statisticsbyjim.com/regression/curve-fitting-linear-nonlinear-regression/", "isFamilyFriendly": true, "displayUrl": "https://statisticsbyjim.com/<b>regression</b>/curve-<b>fitting</b>-<b>line</b>ar-non<b>line</b>ar-<b>regression</b>", "snippet": "Curve <b>Fitting</b> with Nonlinear <b>Regression</b>. Nonlinear <b>regression</b> is a very powerful alternative to linear <b>regression</b>. It provides more flexibility in <b>fitting</b> curves because you can choose from a broad range of nonlinear functions. In fact, there are so many possible functions that the trick becomes finding the function that best fits the particular curve in your data. Most statistical software packages that perform nonlinear <b>regression</b> have a catalog of nonlinear functions. You can use that to ...", "dateLastCrawled": "2022-02-01T19:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "The 10 Algorithms Machine Learning Engineers Need to Know - <b>James Le</b>", "url": "https://jameskle.com/writes/machine-learning", "isFamilyFriendly": true, "displayUrl": "https://jameskle.com/writes/machine-learning", "snippet": "<b>Least</b> <b>squares</b> is a method for performing linear <b>regression</b>. You can think of linear <b>regression</b> as the task of <b>fitting</b> <b>a straight</b> <b>line</b> through a set <b>of points</b>. There are multiple possible strategies to do this, and \u201cordinary <b>least</b> <b>squares</b>\u201d strategy go like this \u2014 You can draw a <b>line</b>, and then for each of the data <b>points</b>, measure the vertical distance between the point and the <b>line</b>, and add these up; the fitted <b>line</b> would be the one where this sum of distances is as small as possible.", "dateLastCrawled": "2022-01-28T10:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "The Interesting World of Non-Linear Regressions | by Diego Manfre ...", "url": "https://towardsdatascience.com/the-interesting-world-of-non-linear-regressions-eb0c405fdc97", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/the-interesting-world-of-non-<b>line</b>ar-<b>regressions</b>-eb0c405...", "snippet": "But it keeps getting harder every time I add more <b>points</b> or when the curve I am looking for differs from <b>a straight</b> <b>line</b>. In this case, a curve <b>fitting</b> process can solve all my problems. I admit it is exciting to enter a <b>bunch</b> <b>of points</b> and find a curve that matches the trend \u201cperfectly\u201d. But how does this work? Why <b>fitting</b> a <b>line</b> is not the same as <b>fitting</b> a strange-shaped curve. Everyone is familiar with linear <b>least</b> <b>squares</b> but, what happens when the expression we are trying to match ...", "dateLastCrawled": "2022-01-29T16:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Comparing Quadratic and Linear Regression</b>?", "url": "https://www.researchgate.net/post/Comparing_Quadratic_and_Linear_Regression", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/post/<b>Comparing_Quadratic_and_Linear_Regression</b>", "snippet": "Finally, note that although a polynomial linear <b>regression</b> can be made to fit better than <b>a straight</b> <b>line</b> linear <b>regression</b>, that may constitute overfitting to the data at hand, which will not ...", "dateLastCrawled": "2022-02-03T05:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Cubic splines to <b>model relationships between continuous variables and</b> ...", "url": "https://www.nature.com/articles/s41409-019-0679-x", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41409-019-0679-x", "snippet": "Within the first and last windows is a simple <b>least</b>-<b>squares</b> <b>regression</b> <b>line</b> <b>fitting</b> the data. Within each of the interior windows is a cubic polynomial fit to the data. But recall that the cubic ...", "dateLastCrawled": "2022-02-03T05:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "What <b>is the power of regression</b>? | Page 1 | Naked Science Forum", "url": "https://www.thenakedscientists.com/forum/index.php?topic=82413.0", "isFamilyFriendly": true, "displayUrl": "https://<b>www.thenakedscientists.com</b>/forum/index.php?topic=82413.0", "snippet": "But two people are likely to draw different lines. - &quot;<b>Least</b>-<b>squares</b>&quot; <b>regression</b> software is able to draw a <b>line</b> that minimises the errors according to the well-known &quot;<b>Least</b>-<b>squares</b>&quot; criterion (which has limitations that are not-so-well known by most people who use it) - Most <b>Regression</b> packages are able to calculate an R 2 measure that gives an idea of how well the data fits the <b>line</b>. - Sometimes the data is not <b>a straight</b> <b>line</b>, and most <b>regression</b> packages allow <b>fitting</b> parabolas ...", "dateLastCrawled": "2022-01-24T14:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "correlation - What is the difference between linear <b>regression</b> on y ...", "url": "https://stats.stackexchange.com/questions/22718/what-is-the-difference-between-linear-regression-on-y-with-x-and-x-with-y", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/22718", "snippet": "$\\begingroup$ Pearson&#39;s correlation isn&#39;t quite <b>fitting</b> a <b>line</b>, @vonjd. It does turn out that it is equivalent to the slope of a fitted <b>least</b> <b>squares</b> <b>line</b> when the data were standardized first. The 1st principal component, when there are only 2 variables &amp; the data were standardized first, is sort of a fitted <b>line</b> that minimizes the orthogonal distances. HTH $\\endgroup$ \u2013 gung - Reinstate Monica. May 5 &#39;18 at 19:35 | Show 4 more comments. 14 $\\begingroup$ I&#39;m going to illustrate the answer ...", "dateLastCrawled": "2022-02-02T18:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Linear Regression in Python</b> using numpy + polyfit (with code base)", "url": "https://data36.com/linear-regression-in-python-numpy-polyfit/", "isFamilyFriendly": true, "displayUrl": "https://data36.com/<b>linear-regression-in-python</b>-numpy-polyfit", "snippet": "If you put all the x\u2013y value pairs <b>on a graph</b>, you\u2019ll get <b>a straight</b> <b>line</b>: The relationship between x and y is linear. Using the equation of this specific <b>line</b> (y = 2 * x + 5), if you change x by 1, y will always change by 2. And it doesn\u2019t matter what a and b values you use, your <b>graph</b> will always show the same characteristics: it will always be <b>a straight</b> <b>line</b>, only its position and slope change. It also means that x and y will always be in linear relationship. In the linear function ...", "dateLastCrawled": "2022-02-02T22:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Statistics for Applications Exam 3 Solution", "url": "https://ocw.mit.edu/courses/mathematics/18-443-statistics-for-applications-spring-2015/exams/MIT18_443S15_Exam3_Sol.pdf", "isFamilyFriendly": true, "displayUrl": "https://<b>ocw.mit.edu</b>/courses/mathematics/18-443-statistics-for-applications-spring-2015/...", "snippet": "Solve for the <b>least</b>-<b>squares</b> <b>line</b> \u2013 Y\u02c6 = \u03b2x.\u02c6 ... The <b>least</b>-<b>squares</b> <b>regression</b> of y on x is given in terms of the standardized values: y\u02c6\u2212y = r. x \u2212 s y s x. A score of 90 on the midterm is (90 \u2212 75)/10 = 1.5 standard deviations above the mean. The predicted score on the \ufb01nal will be r \u00d7 1.5 = .9 standard deviations above the mean \ufb01nal score, which is 75+(.9)\u00d710 = 84. (b). For this case we need to regress the midterm score (x) on (y). The same argument in (a), reversing x ...", "dateLastCrawled": "2022-02-02T02:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "If your <b>data is strongly non-linear, then how</b> should be it treated?", "url": "https://www.researchgate.net/post/If_your_data_is_strongly_non-linear_then_how_should_be_it_treated", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/post/If_your_<b>data_is_strongly_non-linear_then_how</b>_should...", "snippet": "When a relationship seems nonlinear, it is often that one is just thinking of &quot;linear&quot; as <b>a straight</b> <b>line</b> <b>on a graph</b>. A polynomial linear <b>regression</b>, even just a quadratic linear <b>regression</b> may be ...", "dateLastCrawled": "2022-02-02T23:33:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "7 Classical Assumptions of Ordinary <b>Least</b> <b>Squares</b> (OLS) Linear <b>Regression</b>", "url": "https://statisticsbyjim.com/regression/ols-linear-regression-assumptions/", "isFamilyFriendly": true, "displayUrl": "https://statisticsbyjim.com/<b>regression</b>/ols-<b>line</b>ar-<b>regression</b>-assumptions", "snippet": "Ordinary <b>Least</b> <b>Squares</b> is the most common estimation method for linear models\u2014and that\u2019s true for a good reason.As long as your model satisfies the OLS assumptions for linear <b>regression</b>, you <b>can</b> rest easy knowing that you\u2019re getting the best possible estimates.. <b>Regression</b> is a powerful analysis that <b>can</b> analyze multiple variables simultaneously to answer complex research questions. However, if you don\u2019t satisfy the OLS assumptions, you might not be able to trust the results.", "dateLastCrawled": "2022-02-03T09:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>The Method of Least Squares</b> | Aleksandr Hovhannisyan", "url": "https://www.aleksandrhovhannisyan.com/blog/the-method-of-least-squares/", "isFamilyFriendly": true, "displayUrl": "https://www.aleksandrhovhannisyan.com/blog/<b>the-method-of-least-squares</b>", "snippet": "But clearly, we <b>can</b> draw a best-fit <b>line</b> that at <b>least</b> gets as close to all of the <b>points</b> as possible: Spoiler : The solution is y = 0.59459459 x + 1.2972973 y = 0.59459459x + 1.2972973 y = 0 . 5 9 4 5 9 4 5 9 x + 1 . 2 9 7 2 9 7 3 .", "dateLastCrawled": "2022-01-04T00:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Curve <b>Fitting using Linear and Nonlinear Regression</b> - Statistics By Jim", "url": "https://statisticsbyjim.com/regression/curve-fitting-linear-nonlinear-regression/", "isFamilyFriendly": true, "displayUrl": "https://statisticsbyjim.com/<b>regression</b>/curve-<b>fitting</b>-<b>line</b>ar-non<b>line</b>ar-<b>regression</b>", "snippet": "It\u2019s interesting to me that I <b>can</b> use Linear <b>Regression</b> for curve-<b>fitting</b>. I <b>thought</b> I need to learn Nonlinear <b>Regression</b>. Reply. Jim Frost says. July 12, 2021 at 5:27 pm. Hi Yujin, The naming <b>can</b> be confusing! Nonlinear <b>regression</b> <b>can</b> fit a wider variety of curve shapes but often linear <b>regression</b> <b>can</b> fit your curve. I always recommend starting with linear <b>regression</b> because it\u2019s easier and see if that works for your data. Reply. Julian says. May 20, 2021 at 5:57 am. Hello Jim! very ...", "dateLastCrawled": "2022-02-01T19:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "statistics - <b>Straight line through data by eye - least</b> <b>squares</b> ...", "url": "https://math.stackexchange.com/questions/204517/straight-line-through-data-by-eye-least-squares", "isFamilyFriendly": true, "displayUrl": "https://math.stackexchange.com/questions/204517", "snippet": "<b>Straight line through data by eye - least</b> <b>squares</b>? [closed] Ask Question Asked 9 years, 3 months ago. Active 9 years, 3 months ago. Viewed 278 times 4 1 $\\begingroup$ Closed. This question is off-topic. It is not currently accepting answers. Want to improve this question? Update the question so it&#39;s on-topic for Mathematics Stack Exchange. Closed 9 years ago. Improve this question I heard an interesting fact a while ago about how people draw a <b>line</b> through a cloud <b>of points</b> on a scatter plot ...", "dateLastCrawled": "2022-01-15T02:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Chapter 13 - <b>Regression</b> | The Effect", "url": "https://theeffectbook.net/ch-StatisticalAdjustment.html", "isFamilyFriendly": true, "displayUrl": "https://theeffectbook.net/ch-StatisticalAdjustment.html", "snippet": "Estimating this <b>line</b> using ordinary <b>least</b> <b>squares</b> (standard, linear <b>regression</b>) will select the <b>line</b> that minimizes the sum of squared residuals, which is what you get if you take the prediction errors from the <b>line</b>, square them, and add them up. Linear <b>regression</b>, for example, gives us the best linear approximation of the relationship between \\(X\\) and \\(Y\\). The quality of that approximation depends in part on how linear the true model is. Pro: Uses variation efficiently; Pro: A shape is ...", "dateLastCrawled": "2022-01-30T12:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Linear Regression in Python</b> using numpy + polyfit (with code base)", "url": "https://data36.com/linear-regression-in-python-numpy-polyfit/", "isFamilyFriendly": true, "displayUrl": "https://data36.com/<b>linear-regression-in-python</b>-numpy-polyfit", "snippet": "If you put all the x\u2013y value pairs <b>on a graph</b>, you\u2019ll get <b>a straight</b> <b>line</b>: The relationship between x and y is linear. Using the equation of this specific <b>line</b> (y = 2 * x + 5), if you change x by 1, y will always change by 2. And it doesn\u2019t matter what a and b values you use, your <b>graph</b> will always show the same characteristics: it will always be <b>a straight</b> <b>line</b>, only its position and slope change. It also means that x and y will always be in linear relationship. In the linear function ...", "dateLastCrawled": "2022-02-02T22:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Why is <b>the regression line the best fit</b>? - Quora", "url": "https://www.quora.com/Why-is-the-regression-line-the-best-fit", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Why-is-<b>the-regression-line-the-best-fit</b>", "snippet": "Answer (1 of 8): The <b>Regression</b> <b>Line</b> is a function of the independent variable which takes the value of E{Y;x} for any value of x. It may be <b>a straight</b> <b>line</b> or a \u201cnot <b>straight</b>\u201d <b>line</b>. If we know the probability density function f(y;x) for all x, we <b>can</b> find the <b>Regression</b> <b>Line</b> from that. When we ...", "dateLastCrawled": "2022-01-23T15:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "correlation - What is the difference between linear <b>regression</b> on y ...", "url": "https://stats.stackexchange.com/questions/22718/what-is-the-difference-between-linear-regression-on-y-with-x-and-x-with-y", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/22718", "snippet": "$\\begingroup$ Pearson&#39;s correlation isn&#39;t quite <b>fitting</b> a <b>line</b>, @vonjd. It does turn out that it is equivalent to the slope of a fitted <b>least</b> <b>squares</b> <b>line</b> when the data were standardized first. The 1st principal component, when there are only 2 variables &amp; the data were standardized first, is sort of a fitted <b>line</b> that minimizes the orthogonal distances. HTH $\\endgroup$ \u2013 gung - Reinstate Monica. May 5 &#39;18 at 19:35 | Show 4 more comments. 14 $\\begingroup$ I&#39;m going to illustrate the answer ...", "dateLastCrawled": "2022-02-02T18:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Can a Regression Model with a Small</b> R-squared Be Useful? - The Analysis ...", "url": "https://www.theanalysisfactor.com/small-r-squared/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>theanalysisfactor</b>.com/small-r-squared", "snippet": "I have a question from my assignment that says to explain why the <b>regression</b> <b>line</b> (below) without referring to the numerical results cannot be the <b>least</b> <b>squares</b> <b>line</b> of best fit . Stature= -11.68 + 4.167 x Metacarpal length. The 2 variables measured were:-Metacarpal bone length (mm) \u2013 Stature- a persons standing height (cm) Reply. Dom says. July 4, 2016 at 10:52 pm. Hi all, I\u2019m having unexpected problems with my analysis. All correlation indicator such as R square etc. indicate there ...", "dateLastCrawled": "2022-02-03T15:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How <b>good is &quot;Coefficient of Determination (R-Square</b>)&quot; in case of non ...", "url": "https://www.researchgate.net/post/How_good_is_Coefficient_of_Determination_R-Square_in_case_of_non-linear_regression", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/post/How_<b>good_is_Coefficient_of_Determination_R-Square</b>_in...", "snippet": "Assuming that you are <b>fitting</b> the non-linear model by <b>Least</b> <b>Squares</b> i.e. minimizing the sum of the squared residuals, then I would say that R2 is equally informative in the non-linear case as in ...", "dateLastCrawled": "2022-01-17T07:51:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "7 Classical Assumptions of Ordinary <b>Least</b> <b>Squares</b> (OLS) Linear <b>Regression</b>", "url": "https://statisticsbyjim.com/regression/ols-linear-regression-assumptions/", "isFamilyFriendly": true, "displayUrl": "https://statisticsbyjim.com/<b>regression</b>/ols-<b>line</b>ar-<b>regression</b>-assumptions", "snippet": "Ordinary <b>Least</b> <b>Squares</b> is the most common estimation method for linear models\u2014and that\u2019s true for a good reason.As long as your model satisfies the OLS assumptions for linear <b>regression</b>, you <b>can</b> rest easy knowing that you\u2019re getting the best possible estimates.. <b>Regression</b> is a powerful analysis that <b>can</b> analyze multiple variables simultaneously to answer complex research questions. However, if you don\u2019t satisfy the OLS assumptions, you might not be able to trust the results.", "dateLastCrawled": "2022-02-03T09:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Comparing Quadratic and Linear Regression</b>?", "url": "https://www.researchgate.net/post/Comparing_Quadratic_and_Linear_Regression", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/post/<b>Comparing_Quadratic_and_Linear_Regression</b>", "snippet": "Finally, note that although a polynomial linear <b>regression</b> <b>can</b> be made to fit better than <b>a straight</b> <b>line</b> linear <b>regression</b>, that may constitute overfitting to the data at hand, which will not ...", "dateLastCrawled": "2022-02-03T05:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Least</b> <b>squares</b> is a method of <b>fitting</b> a <b>regression</b> <b>line</b> which is robust ...", "url": "https://www.quora.com/Least-squares-is-a-method-of-fitting-a-regression-line-which-is-robust-i-e-safe-from-outliers-True-or-False", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>Least</b>-<b>squares</b>-is-a-method-of-<b>fitting</b>-a-<b>regression</b>-<b>line</b>-which-is...", "snippet": "Answer (1 of 2): This is false. So it is \u201c<b>least</b> <b>squares</b>\u201d - the square of the residual is what you are looking to minimise. Consider your point with the highest residuals and move it some small amount. Consider how your <b>line</b> of best fit will move. Consider for a given small change how much your g...", "dateLastCrawled": "2022-01-08T16:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Understanding Linear <b>Regression</b> vs. <b>Multiple Regression</b>", "url": "https://www.investopedia.com/ask/answers/060315/what-difference-between-linear-regression-and-multiple-regression.asp", "isFamilyFriendly": true, "displayUrl": "https://<b>www.investopedia.com</b>/ask/answers/060315/what-difference-between-<b>line</b>ar...", "snippet": "The <b>least</b>-<b>squares</b> criterion is a method of measuring the accuracy of a <b>line</b> in depicting the data that was used to generate it. That is, the formula determines the <b>line</b> of best fit. That is, the ...", "dateLastCrawled": "2022-02-03T02:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Integrity of the linear regression</b> | Request PDF", "url": "https://www.researchgate.net/publication/261587123_Integrity_of_the_linear_regression", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/261587123_<b>Integrity_of_the_linear_regression</b>", "snippet": "The <b>least</b> <b>squares</b> linear <b>regression</b> used for two hundred years is an incomplete solution for <b>fitting</b> a <b>line</b> to a set <b>of points</b>. In most cases it will produce two separate and unrelated solutions ...", "dateLastCrawled": "2021-10-01T16:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Choosing the Correct Type of <b>Regression</b> Analysis - Statistics By Jim", "url": "https://statisticsbyjim.com/regression/choosing-regression-analysis/", "isFamilyFriendly": true, "displayUrl": "https://statisticsbyjim.com/<b>regression</b>/choosing-<b>regression</b>-analysis", "snippet": "OLS produces the fitted <b>line</b> that minimizes the sum of the squared differences between the data <b>points</b> and the <b>line</b>. Linear <b>regression</b>, also known as ordinary <b>least</b> <b>squares</b> and linear <b>least</b> <b>squares</b>, is the real workhorse of the <b>regression</b> world.Use linear <b>regression</b> to understand the mean change in a dependent variable given a one-unit change in each independent variable.", "dateLastCrawled": "2022-02-03T01:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "correlation - What is the difference between linear <b>regression</b> on y ...", "url": "https://stats.stackexchange.com/questions/22718/what-is-the-difference-between-linear-regression-on-y-with-x-and-x-with-y", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/22718", "snippet": "$\\begingroup$ Pearson&#39;s correlation isn&#39;t quite <b>fitting</b> a <b>line</b>, @vonjd. It does turn out that it is equivalent to the slope of a fitted <b>least</b> <b>squares</b> <b>line</b> when the data were standardized first. The 1st principal component, when there are only 2 variables &amp; the data were standardized first, is sort of a fitted <b>line</b> that minimizes the orthogonal distances. HTH $\\endgroup$ \u2013 gung - Reinstate Monica. May 5 &#39;18 at 19:35 | Show 4 more comments. 14 $\\begingroup$ I&#39;m going to illustrate the answer ...", "dateLastCrawled": "2022-02-02T18:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Advantages and Disadvantages of Linear Regression</b>", "url": "https://iq.opengenus.org/advantages-and-disadvantages-of-linear-regression/", "isFamilyFriendly": true, "displayUrl": "https://iq.opengenus.org/<b>advantages-and-disadvantages-of-linear-regression</b>", "snippet": "Linear <b>Regression</b> is a very simple algorithm that <b>can</b> be implemented very easily to give satisfactory results.Furthermore, these models <b>can</b> be trained easily and efficiently even on systems with relatively low computational power when <b>compared</b> to other complex algorithms.Linear <b>regression</b> has a considerably lower time complexity when <b>compared</b> to some of the other machine learning algorithms.The mathematical equations of Linear <b>regression</b> are also fairly easy to understand and interpret.Hence ...", "dateLastCrawled": "2022-02-02T08:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Piecewise Linear <b>Regression</b> Model. What Is It and When <b>Can</b> We Use It ...", "url": "https://towardsdatascience.com/piecewise-linear-regression-model-what-is-it-and-when-can-we-use-it-93286cfee452", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/piecewise-<b>line</b>ar-<b>regression</b>-model-what-is-it-and-when...", "snippet": "Piecewise linear <b>regression</b> takes the best aspects of linear <b>regression</b> and solves complex problems that we wouldn\u2019t be able to solve with a simple linear <b>regression</b>. The most awesome part of this simple algorithm is that it allows you easily understand your data by solving multiple linear regressions, so if you have data that doesn\u2019t fit a single <b>line</b>, piecewise linear <b>regression</b> <b>can</b> help you. Furthermore, we saw that the PWLF library is a fairly simple way to implement piecewise linear ...", "dateLastCrawled": "2022-01-30T15:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Statistics for Applications Exam 3 Solution", "url": "https://ocw.mit.edu/courses/mathematics/18-443-statistics-for-applications-spring-2015/exams/MIT18_443S15_Exam3_Sol.pdf", "isFamilyFriendly": true, "displayUrl": "https://<b>ocw.mit.edu</b>/courses/mathematics/18-443-statistics-for-applications-spring-2015/...", "snippet": "Solve for the <b>least</b>-<b>squares</b> <b>line</b> \u2013 Y\u02c6 = \u03b2x.\u02c6 ... The <b>least</b>-<b>squares</b> <b>regression</b> of y on x is given in terms of the standardized values: y\u02c6\u2212y = r. x \u2212 s y s x. A score of 90 on the midterm is (90 \u2212 75)/10 = 1.5 standard deviations above the mean. The predicted score on the \ufb01nal will be r \u00d7 1.5 = .9 standard deviations above the mean \ufb01nal score, which is 75+(.9)\u00d710 = 84. (b). For this case we need to regress the midterm score (x) on (y). The same argument in (a), reversing x ...", "dateLastCrawled": "2022-02-02T02:54:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "CS 189/289A: Introduction to <b>Machine</b> <b>Learning</b>", "url": "https://people.eecs.berkeley.edu/~jrs/189s21/", "isFamilyFriendly": true, "displayUrl": "https://people.eecs.berkeley.edu/~jrs/189s21", "snippet": "LDA vs. logistic <b>regression</b>: advantages and disadvantages. ROC curves. Weighted <b>least</b>-<b>squares</b> <b>regression</b>. <b>Least</b>-<b>squares</b> polynomial <b>regression</b>. Read ISL, Sections 4.4.3, 7.1, 9.3.3; ESL, Section 4.4.1. Optional: here is a fine short discussion of ROC curves\u2014but skip the incoherent question at the top and jump straight to the answer.", "dateLastCrawled": "2022-01-31T03:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>CS 189/289A</b>: Introduction to <b>Machine</b> <b>Learning</b>", "url": "https://people.eecs.berkeley.edu/~jrs/189/", "isFamilyFriendly": true, "displayUrl": "https://people.eecs.berkeley.edu/~jrs/189", "snippet": "<b>regression</b>: <b>least</b>-<b>squares</b> linear <b>regression</b>, logistic <b>regression</b>, polynomial <b>regression</b>, ridge <b>regression</b>, Lasso; density estimation: maximum likelihood estimation (MLE); dimensionality reduction: principal components analysis (PCA), random projection; and clustering: k-means clustering, hierarchical clustering, spectral graph clustering. Useful Links. Access the <b>CS 189/289A</b> Piazza discussion group. If you want an instructional account, you can get one online. Go to the same link if you ...", "dateLastCrawled": "2022-02-02T17:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Neurath&#39;s Speedboat</b>: <b>Least squares as springs</b>", "url": "https://joshualoftus.com/posts/2020-11-23-least-squares-as-springs/", "isFamilyFriendly": true, "displayUrl": "https://joshualoftus.com/posts/2020-11-23-<b>least-squares-as-springs</b>", "snippet": "(This is also called total <b>least</b> <b>squares</b> or a special case of Deming <b>regression</b>.) Model complexity/elasticity: <b>machine</b> <b>learning</b> or AI. We can keep building on this <b>analogy</b> by using it to understand more complex modeling methods with another very simple idea: elasticity of the model object itself. Instead of a rigid body like a line (or ...", "dateLastCrawled": "2022-02-03T04:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "A difficult <b>regression</b> parameter estimation problem is posed when the data sample is hypothesized to have been generated by more than a single <b>regression</b> model. To find the best-fitting number and ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "LSEbA: <b>least squares regression and estimation by analogy</b> in a semi ...", "url": "https://link.springer.com/article/10.1007/s10664-010-9128-6", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10664-010-9128-6", "snippet": "In this study, we indicatively applied the ordinary <b>least</b> <b>squares</b> <b>regression</b> and the estimation by <b>analogy</b> technique for the computation of the parametric and non-parametric part, respectively. However, there are lots of other well-known methods that can substitute the abovementioned methods and can be used for evaluation of these components. For example, practitioners may use a robust <b>regression</b> in the computation of the parametric portion of the proposed model in order to have a model less ...", "dateLastCrawled": "2021-12-03T23:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Machine Learning by Analogy</b> - SlideShare", "url": "https://www.slideshare.net/ColleenFarrelly/machine-learning-by-analogy-59094152", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ColleenFarrelly/<b>machine-learning-by-analogy</b>-59094152", "snippet": "<b>Machine Learning by Analogy</b> 1. Colleen M. Farrelly 2. Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics behind these ...", "dateLastCrawled": "2022-01-31T07:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A Big Problem with Linear <b>Regression</b> and How to Solve It | Towards Data ...", "url": "https://towardsdatascience.com/robust-regression-23b633e5d6a5", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/robust-<b>regression</b>-23b633e5d6a5", "snippet": "Introduction to Robust <b>Regression</b> in <b>Machine</b> <b>Learning</b>. Hussein Abdulrahman . Just now \u00b7 7 min read. The idea behind classic linear <b>regression</b> is simple: draw a \u201cbest-fit\u201d line across the data points that minimizes the mean squared errors: Classic linear <b>regression</b> with ordinary <b>least</b> <b>squares</b>. (Image by author) Looks good. But we don\u2019t always get such clean, well behaved data in real life. Instead, we may get something like this: Same algorithm as above, but now performing poorly due ...", "dateLastCrawled": "2022-02-01T17:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PPT) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> | Colleen Farrelly - Academia.edu", "url": "https://www.academia.edu/30404581/Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/30404581/<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "<b>Machine</b> <b>Learning</b> by <b>Analogy</b> Colleen M. Farrelly Overview of Problem Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics ...", "dateLastCrawled": "2022-01-02T06:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Introduction to Machine Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/24/introduction-to-machine-learning-2/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/24/<b>introduction-to-machine-learning</b>-2", "snippet": "<b>Machine</b> <b>learning</b> terminology for model building and validation There seems to be an <b>analogy</b> between statistical modeling and <b>machine</b> <b>learning</b> that we will cover in subsequent chapters in depth. However, a quick view has been provided as follows: in statistical modeling, linear <b>regression</b> with two independent variables is trying to fit the best plane with\u2026", "dateLastCrawled": "2022-01-17T05:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>machine</b> <b>learning</b> - Do we actually take random line in first step of ...", "url": "https://stats.stackexchange.com/questions/556085/do-we-actually-take-random-line-in-first-step-of-linear-regression", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/556085/do-we-actually-take-random-line-in...", "snippet": "As a simple <b>analogy</b> to show the difference between a closed form solution and an algorithm: if I were to give you a mathematical equation, ... (to which the exact solution to linear <b>least</b> <b>squares</b> <b>regression</b> is extremely sensitive). Share. Cite. Improve this answer. Follow answered Dec 17 &#39;21 at 12:54. Roger Vadim Roger Vadim. 1,314 6 6 silver badges 16 16 bronze badges $\\endgroup$ Add a comment | Your Answer Thanks for contributing an answer to Cross Validated! Please be sure to answer the ...", "dateLastCrawled": "2022-02-02T23:48:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Bayesian <b>Learning</b> - Rebellion Research", "url": "https://www.rebellionresearch.com/bayesian-learning", "isFamilyFriendly": true, "displayUrl": "https://www.rebellionresearch.com/bayesian-<b>learning</b>", "snippet": "Linear Regression example of <b>machine learning Least Squares Regression can be thought of as</b> a very limited <b>learning</b> algorithm, where the training set consists of a number of x and y data pairs. The task would be trying to predict the y value, and the performance measure would be the sum of the squared differences between the predicted and actual y\u2019s.", "dateLastCrawled": "2022-01-19T02:15:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(least squares regression)  is like +(fitting a straight line to a bunch of points on a graph)", "+(least squares regression) is similar to +(fitting a straight line to a bunch of points on a graph)", "+(least squares regression) can be thought of as +(fitting a straight line to a bunch of points on a graph)", "+(least squares regression) can be compared to +(fitting a straight line to a bunch of points on a graph)", "machine learning +(least squares regression AND analogy)", "machine learning +(\"least squares regression is like\")", "machine learning +(\"least squares regression is similar\")", "machine learning +(\"just as least squares regression\")", "machine learning +(\"least squares regression can be thought of as\")", "machine learning +(\"least squares regression can be compared to\")"]}