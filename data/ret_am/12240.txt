{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Seq2Seq models and the Attention mechanism</b> - Mattia ... - Mattia Mancassola", "url": "https://mett29.github.io/posts/2019/12/seq2seq_and_attention/", "isFamilyFriendly": true, "displayUrl": "https://mett29.github.io/posts/2019/12/seq2seq_and_attention", "snippet": "<b>Sequence to Sequence</b> <b>Learning</b>. As we can see from the below image, there are different sequential data problems: Credits Andrej Karpathy. In this post I want to discuss how a RNN can be trained to map an input sequence to an output sequence which is not necessarily of the same length. This model is at the core of many different tasks, <b>like</b> speech recognition, machine translation etc. The first authors who proposed this new model (Cho et al. (2014) and Sutskever et al. (2014)) called it ...", "dateLastCrawled": "2022-01-01T23:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "deepchar | Transliteration with <b>sequence-to-sequence</b> models and ...", "url": "https://deepchar.github.io/", "isFamilyFriendly": true, "displayUrl": "https://deepchar.github.io", "snippet": "You can <b>read</b> more about what makes this problem non-trivial at Automatic transliteration with LSTM and Interpreting ... and inspired by similar approaches to transliteration and other <b>sequence-to-sequence</b> tasks <b>like</b> grammar correction, we rely primarily on data generation. See the deepchar/data repo. Benchmarks. See the deepchar/benchmarks repo. Results. TODO. References. Sockeye. Sockeye: A Toolkit for Neural Machine Translation 2017/2018 arXiv Felix Hieber, Tobias Domhan, Michael Denkowski ...", "dateLastCrawled": "2021-12-30T20:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Sequence-to-Sequence Models</b> \u00b7 Tensorflow document", "url": "https://haosdent.gitbooks.io/tensorflow-document/content/tutorials/seq2seq/", "isFamilyFriendly": true, "displayUrl": "https://haosdent.gitbooks.io/tensorflow-document/content/tutorials/seq2seq", "snippet": "In many applications of <b>sequence-to-sequence models</b>, the output of the decoder at time t is fed back and becomes the input of the decoder at time t+1. At test time, when decoding a sequence, this is how the sequence is constructed. During training, on the other hand, it is common to provide the correct input to the decoder at every time-step, even if the decoder made a mistake before. Functions in", "dateLastCrawled": "2022-01-31T01:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Machine Translation With <b>Sequence To Sequence</b> Models And Dot Attention ...", "url": "https://blog.paperspace.com/nlp-machine-translation-with-keras/", "isFamilyFriendly": true, "displayUrl": "https://blog.paperspace.com/nlp-machine-translation-with-keras", "snippet": "While working with <b>sequence to sequence</b> models, it sometimes becomes significant to distinguish between the essential components in a particular <b>task</b>. I will state an example for a computer vision and NLP <b>task</b>. For a computer vision <b>task</b>, let us consider an image of a dog walking on the ground. The <b>sequence to sequence</b> model can identify the following fairly easily. But with the help of the attention mechanism, we can add more weightage to the essential component in the image, which is the ...", "dateLastCrawled": "2022-01-30T03:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "How to implement <b>Seq2Seq</b> LSTM Model in Keras | by Akira Takezawa ...", "url": "https://towardsdatascience.com/how-to-implement-seq2seq-lstm-model-in-keras-shortcutnlp-6f355f3e5639", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/how-to-implement-<b>seq2seq</b>-lstm-model-in-keras-shortcut...", "snippet": "Keras: Deep <b>Learning</b> for Python Why do you need <b>to read</b> this? At the first time when I tried to implement <b>seq2seq</b> for Chatbot <b>Task</b>, I got stuck a lot of times especially about the Dimension of Input Data and Input layer of Neural Network Architecture. Now I understand that unless you have a deep understanding of linear algebra concepts such as matrix and tensor, or how Keras API works, you\u2019ll get errors continuously(and that\u2019s so miserable!). So in this article, I will explain the ...", "dateLastCrawled": "2022-01-29T05:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>GitHub</b> - deepchar/deepchar.<b>github</b>.io: Transliteration with sequence-to ...", "url": "https://github.com/deepchar/deepchar.github.io", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/deepchar/deepchar.<b>github</b>.io", "snippet": "Another flavour of this <b>task</b> is transliteration of named entities. Instead of mapping many inputs to one output (n:1), it maps one input to many outputs (1:n). You can <b>read</b> more about that in deepchar/entities. Our approach. Our baseline approach is to train character-level <b>sequence-to-sequence</b> models on generated data.", "dateLastCrawled": "2021-09-10T18:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "An <b>introduction to sequence-to-sequence learning</b> - Loren Lugosch", "url": "https://lorenlugosch.github.io/posts/2019/02/seq2seq/", "isFamilyFriendly": true, "displayUrl": "https://lorenlugosch.github.io/posts/2019/02/seq2seq", "snippet": "An <b>introduction to sequence-to-sequence learning</b>. Published: February 19, 2019. Many interesting problems in artificial intelligence can be described in the following way: Map a sequence of inputs $\\mathbf{x}$ to the correct sequence of outputs $\\mathbf{y}$. Speech recognition is one example: the goal is to map an audio signal $\\mathbf{x}$ (a sequence of real-valued audio samples) to the correct text transcript $\\mathbf{y}$ (a sequence of letters). Other examples are machine translation ...", "dateLastCrawled": "2022-02-02T00:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Sequence-to-sequence learning with Transducers</b> - Loren Lugosch", "url": "https://lorenlugosch.github.io/posts/2020/11/transducer/", "isFamilyFriendly": true, "displayUrl": "https://lorenlugosch.github.io/posts/2020/11/transducer", "snippet": "<b>Sequence-to-sequence learning with Transducers</b>. Published: November 16, 2020 The Transducer (sometimes called the \u201cRNN Transducer\u201d or \u201cRNN-T\u201d, though it need not use RNNs) is a <b>sequence-to-sequence</b> model proposed by Alex Graves in \u201cSequence Transduction with Recurrent Neural Networks\u201d. The paper was published at the ICML 2012 Workshop on Representation <b>Learning</b>.Graves showed that the Transducer was a sensible model to use for speech recognition, achieving good results on a small ...", "dateLastCrawled": "2022-01-31T02:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "3 Types of <b>Sequence</b> Prediction Problems | by Jingles (Hong Jing ...", "url": "https://towardsdatascience.com/3-types-of-sequence-prediction-problems-97f22e946318", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/3-types-of-<b>sequence</b>-prediction-problems-97f22e946318", "snippet": "<b>Sequence</b> prediction is a popular machine <b>learning</b> <b>task</b>, which consists of predicting the next symbol(s) based on the previously observed <b>sequence</b> of symbols. These symbols could be a number, an alphabet, a word, an event, or an object <b>like</b> a webpage or product. For example: A <b>sequence</b> of words or characters in a text; A <b>sequence</b> of products bought by a customer; A <b>sequence</b> of events observed on logs; <b>Sequence</b> prediction is different from other types of supervised <b>learning</b> problems, as it ...", "dateLastCrawled": "2022-01-30T18:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Sequencing events in reading and writing</b>: A Complete Guide for Students ...", "url": "https://literacyideas.com/teaching-sequencing-in-english/", "isFamilyFriendly": true, "displayUrl": "https://literacyideas.com/teaching-sequencing-in-english", "snippet": "Students should <b>write</b> the events of the story on each step of the stairs in the order they occur, starting with the first event on the first step and with each event that follows written on the next step above. This is also a useful way for students to represent nonlinear narratives, such as in medias res. This organizer is a helpful means to unravel more complex chronologies. The finished chart helps the student to see each of the events in the story in the order that they occurred. 101 ...", "dateLastCrawled": "2022-02-03T03:02:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "paper review: \u201cBART: Denoising <b>Sequence-to-Sequence</b> Pre-training for ...", "url": "https://medium.com/mlearning-ai/paper-summary-bart-denoising-sequence-to-sequence-pre-training-for-natural-language-generation-69e41dfbb7fe", "isFamilyFriendly": true, "displayUrl": "https://medium.com/m<b>learning</b>-ai/paper-summary-bart-denoising-<b>sequence-to-sequence</b>-pre...", "snippet": "<b>Similar</b> approach to [CLS] token output in BERT but difference is that in BART we use the final token\u2019s output so that the output token is a result of attending to all previous input tokens.", "dateLastCrawled": "2022-02-01T16:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "deepchar | Transliteration with <b>sequence-to-sequence</b> models and ...", "url": "https://deepchar.github.io/", "isFamilyFriendly": true, "displayUrl": "https://deepchar.github.io", "snippet": "Another flavour of this <b>task</b> is transliteration of named entities. Instead of mapping many inputs to one output (n:1), it maps one input to many outputs (1:n). You can <b>read</b> more about that in deepchar/entities. Our approach. Our baseline approach is to train character-level <b>sequence-to-sequence</b> models on generated data.", "dateLastCrawled": "2021-12-30T20:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Sequence-to-Sequence Models</b> \u00b7 Tensorflow document", "url": "https://haosdent.gitbooks.io/tensorflow-document/content/tutorials/seq2seq/", "isFamilyFriendly": true, "displayUrl": "https://haosdent.gitbooks.io/tensorflow-document/content/tutorials/seq2seq", "snippet": "In many applications of <b>sequence-to-sequence models</b>, the output of the decoder at time t is fed back and becomes the input of the decoder at time t+1. At test time, when decoding a sequence, this is how the sequence is constructed. During training, on the other hand, it is common to provide the correct input to the decoder at every time-step, even if the decoder made a mistake before. Functions in", "dateLastCrawled": "2022-01-31T01:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "AdapterHub - <b>Adapters for Generative and Seq2Seq Models in</b> NLP", "url": "https://adapterhub.ml/blog/2021/04/adapters-for-generative-and-seq2seq-models-in-nlp/", "isFamilyFriendly": true, "displayUrl": "https://adapterhub.ml/blog/2021/04/<b>adapters-for-generative-and-seq2seq-models-in</b>-nlp", "snippet": "On a neural machine translation <b>task</b>, they achieved <b>similar</b> results with adapters as compared to a fully finetuned model. ... To test the BART model on <b>sequence-to-sequence</b> tasks, we evaluated the model on the CNN/Daily Mail dataset (Hermann et al. (2015); See et al., 2017) and the extreme summary dataset (XSum) dataset (Narayan et al., 2018). Both tasks have the objective to summarize newspaper articles. The main difference is that XSum requires the model to output short one sentence ...", "dateLastCrawled": "2022-02-03T16:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>GitHub</b> - deepchar/deepchar.<b>github</b>.io: Transliteration with sequence-to ...", "url": "https://github.com/deepchar/deepchar.github.io", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/deepchar/deepchar.<b>github</b>.io", "snippet": "Another flavour of this <b>task</b> is transliteration of named entities. Instead of mapping many inputs to one output (n:1), it maps one input to many outputs (1:n). You can <b>read</b> more about that in deepchar/entities. Our approach. Our baseline approach is to train character-level <b>sequence-to-sequence</b> models on generated data.", "dateLastCrawled": "2021-09-10T18:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Sequence to Sequence Learning with Neural Networks</b>", "url": "https://wandb.ai/authors/seq2seq/reports/Sequence-to-Sequence-Learning-with-Neural-Networks--Vmlldzo0Mzg0MTI", "isFamilyFriendly": true, "displayUrl": "https://wandb.ai/authors/seq2seq/reports/<b>Sequence-to-Sequence-Learning-with</b>-Neural...", "snippet": "<b>Task</b>. The <b>task</b> is to emulate a translator. We will be given a sentence in a given language which we need to translate into another language. Prior to this paper, this <b>task</b> was viewed as a mapping <b>task</b>, where systems had to map <b>similar</b> contextual phrases in different languages. This way of approaching the problem was too tough for a <b>learning</b> system.", "dateLastCrawled": "2022-01-13T19:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A Tour of Attention-Based Architectures", "url": "https://machinelearningmastery.com/a-tour-of-attention-based-architectures/", "isFamilyFriendly": true, "displayUrl": "https://machine<b>learning</b>mastery.com/a-t", "snippet": "Taken from \u201c<b>Sequence to Sequence</b> <b>Learning</b> with Neural Networks ... It consists of a neural network controller that takes an input to produce an output, and performs <b>read</b> <b>and write</b> operations to memory. Neural Turing Machine Architecture Taken from \u201cNeural Turing Machines\u201c The operation performed by the <b>read</b> head <b>is similar</b> to the attention mechanism employed for seq2seq tasks, where an attention weight indicates the importance of the vector under consideration in forming the output. A ...", "dateLastCrawled": "2022-01-31T05:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Transformers BART Model Explained for Text Summarization", "url": "https://www.projectpro.io/article/transformers-bart-model-explained/553", "isFamilyFriendly": true, "displayUrl": "https://www.projectpro.io/article/transformers-bart-model-explained/553", "snippet": "The original Transformer is based on an encoder-decoder architecture and is a classic <b>sequence-to-sequence</b> model. The model\u2019s input and output are in the form of a sequence (text), and the encoder learns a high-dimensional representation of the input,which is then mapped to the output by the decoder. This architecture introduced a new form of <b>learning</b> for language-related tasks and, thus, the models spawned from it achieve outstanding results overtaking the existing deep neural network ...", "dateLastCrawled": "2022-02-03T05:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Abstractive Text Summarization with Deep Learning</b> - Times Internet ...", "url": "https://timesinternet.in/blog/abstractive-text-summarization-with-deep-learning/", "isFamilyFriendly": true, "displayUrl": "https://timesinternet.in/blog/<b>abstractive-text-summarization-with-deep-learning</b>", "snippet": "BART is a denoising autoencoder for pretraining <b>sequence-to-sequence</b> models. It is trained by firstly corrupting text with an arbitrary noising function, and then <b>learning</b> a model to reconstruct the original text. It uses a standard Transformer-based neural machine translation architecture. It uses a standard seq2seq/NMT architecture with a bidirectional encoder and a left-to-right decoder. The corruption schemes used for BART [10] pre-training are as follows:", "dateLastCrawled": "2022-01-27T13:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "[D] <b>Order Matters: Sequence to sequence for sets</b> : MachineLearning", "url": "https://www.reddit.com/r/MachineLearning/comments/85yjzo/d_order_matters_sequence_to_sequence_for_sets/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/Machine<b>Learning</b>/comments/85yjzo/d_order_matters_sequence_to...", "snippet": "Hi all, I am reading the paper <b>Order Matters: Sequence to sequence for sets</b> and I am trying to implement ... The paper mentioned that the exact equations and implementation of the <b>read</b>, process, <b>and write</b> block will be released in an appendix. However, I cannot find the appendix online. I saw another paper, Neural Combinatorial Optimization with Reinforcement <b>Learning</b>, using a <b>similar</b> approach to obtain an embedding for a set of inputs. They use the last hidden state of the encoder as the ...", "dateLastCrawled": "2021-08-07T13:05:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Seq2Seq models and the Attention mechanism</b> - Mattia ... - Mattia Mancassola", "url": "https://mett29.github.io/posts/2019/12/seq2seq_and_attention/", "isFamilyFriendly": true, "displayUrl": "https://mett29.github.io/posts/2019/12/seq2seq_and_attention", "snippet": "<b>Sequence to Sequence</b> <b>Learning</b>. As we <b>can</b> see from the below image, there are different sequential data problems: ... and that also interacts with the memory through selective <b>read</b> <b>and write</b> operations. a memory bank. The main issue is: how <b>can</b> we make the whole differentiable? We want the <b>read</b> <b>and write</b> to be differentiable w.r.t. the location we <b>read</b> from or we <b>write</b> to, but this is not an easy <b>task</b>, since usually a single element in memory is addressed, like happens in Turing machine or in ...", "dateLastCrawled": "2022-01-01T23:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Sequence to Sequence Learning with Neural Networks</b>", "url": "https://wandb.ai/authors/seq2seq/reports/Sequence-to-Sequence-Learning-with-Neural-Networks--Vmlldzo0Mzg0MTI", "isFamilyFriendly": true, "displayUrl": "https://wandb.ai/authors/seq2seq/reports/<b>Sequence-to-Sequence-Learning-with</b>-Neural...", "snippet": "Introduction. In the age of attention and transformers, I <b>thought</b> writing a simple report on <b>sequence to sequence</b> modelling thinking it would be a good starting point for a lot of people. In this article I will try declassifying the paper <b>Sequence to Sequence Learning with Neural Networks</b> by Ilya Sutskever et. al. Here in this paper, the authors have presented an end to end <b>learning</b> system that helps in translating one language into another.", "dateLastCrawled": "2022-01-13T19:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Encoder-<b>Decoder</b> <b>Seq2Seq</b> Models, Clearly Explained!! | by Kriz Moses ...", "url": "https://medium.com/analytics-vidhya/encoder-decoder-seq2seq-models-clearly-explained-c34186fbf49b", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/encoder-<b>decoder</b>-<b>seq2seq</b>-models-clearly-explained-c...", "snippet": "<b>Sequence to Sequence</b> <b>Learning</b> with Neural Networks [1] This was the motivation behind coming up with an architecture that <b>can</b> solve general <b>sequence-to-sequence</b> problems and so encoder-<b>decoder</b> ...", "dateLastCrawled": "2022-02-03T06:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Understanding <b>Encoder</b>-Decoder <b>Sequence to Sequence</b> Model | by Simeon ...", "url": "https://towardsdatascience.com/understanding-encoder-decoder-sequence-to-sequence-model-679e04af4346", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-<b>encoder</b>-decoder-<b>sequence-to-sequence</b>...", "snippet": "Feb 5, 2019 \u00b7 5 min <b>read</b>. In this article, I will try to give a short and concise explanation of the <b>sequence to sequence</b> model which have recently achieved significant results on pretty complex tasks like machine translation, video captioning, question answering etc. Prerequisites: the reader should already be familiar with neura l networks and, in particular, recurrent neural networks (RNNs). In addition, knowledge of LSTM or GRU models is preferable. If you are not yet familiar with RNNs ...", "dateLastCrawled": "2022-02-02T00:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Neural <b>Machine Translation</b>: Inner Workings, Seq2Seq, and Transformers ...", "url": "https://towardsdatascience.com/neural-machine-translation-inner-workings-seq2seq-and-transformers-229faff5895b", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/neural-<b>machine-translation</b>-inner-workings-seq2seq-and...", "snippet": "We <b>can</b> formally <b>write</b> this goal as in Eq. 1, that is we want to find the output sentence y that maximizes the ... Seq2seq paper by Sutskever et al. \u201c<b>Sequence-to-sequence</b> <b>learning</b> with neural networks\u201d [1]. Attention mechanism by Bahdanau et al. \u201cNeural <b>machine translation</b> by jointly <b>learning</b> to align and translate\u201d [2]. BPE and subword units by Sennrich et al. \u201c Neural <b>machine translation</b> of rare words with subword units.\u201d [4]. The Transformer by Vaswani et al. \u201cAttention is ...", "dateLastCrawled": "2022-01-29T08:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Transformers BART Model Explained for Text Summarization", "url": "https://www.projectpro.io/article/transformers-bart-model-explained/553", "isFamilyFriendly": true, "displayUrl": "https://www.projectpro.io/article/transformers-bart-model-explained/553", "snippet": "The original Transformer is based on an encoder-decoder architecture and is a classic <b>sequence-to-sequence</b> model. The model\u2019s input and output are in the form of a sequence (text), and the encoder learns a high-dimensional representation of the input,which is then mapped to the output by the decoder. This architecture introduced a new form of <b>learning</b> for language-related tasks and, thus, the models spawned from it achieve outstanding results overtaking the existing deep neural network ...", "dateLastCrawled": "2022-02-03T05:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "An <b>introduction to sequence-to-sequence learning</b> - Loren Lugosch", "url": "https://lorenlugosch.github.io/posts/2019/02/seq2seq/", "isFamilyFriendly": true, "displayUrl": "https://lorenlugosch.github.io/posts/2019/02/seq2seq", "snippet": "An <b>introduction to sequence-to-sequence learning</b>. Published: February 19, 2019. Many interesting problems in artificial intelligence <b>can</b> be described in the following way: Map a sequence of inputs $\\mathbf{x}$ to the correct sequence of outputs $\\mathbf{y}$. Speech recognition is one example: the goal is to map an audio signal $\\mathbf{x}$ (a sequence of real-valued audio samples) to the correct text transcript $\\mathbf{y}$ (a sequence of letters). Other examples are machine translation ...", "dateLastCrawled": "2022-02-02T00:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Neural Machine Translation</b> | CS-677 - GitHub Pages", "url": "https://pantelis.github.io/cs677/docs/common/lectures/nlp/nmt/", "isFamilyFriendly": true, "displayUrl": "https://pantelis.github.io/cs677/docs/common/lectures/nlp/nmt", "snippet": "<b>Sequence-to-sequence</b>, or \u201cSeq2Seq\u201d, is a relatively new paradigm, ... The encoder network\u2019s job is <b>to read</b> the input sequence to our Seq2Seq model and generate a fixed-dimensional context vector $\\phi$ for the sequence. To do that we use an RNN (LSTM) that mathematically, it evolves its hidden state as we have seen as, $$\\mathbf h_t = f(\\mathbf x_t, \\mathbf h_{t-1})$$ and the context vector $\\mathbf \\phi = q(\\mathbf h_1, \u2026, \\mathbf h_{Tx})$ is generated in general from the sequence ...", "dateLastCrawled": "2021-11-18T18:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "is <b>the Sequence to Sequence learning right</b>? \u00b7 Issue #395 \u00b7 keras-team ...", "url": "https://github.com/keras-team/keras/issues/395", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/keras-team/keras/issues/395", "snippet": "Assume we are trying to learn a <b>sequence to sequence</b> map. For this we <b>can</b> use Recurrent and TimeDistributedDense layers. Now assume that the sequences have different lengths. We should pad both input and desired sequences with zeros, rig...", "dateLastCrawled": "2022-01-29T00:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Sequencing events in reading and writing</b>: A Complete Guide for Students ...", "url": "https://literacyideas.com/teaching-sequencing-in-english/", "isFamilyFriendly": true, "displayUrl": "https://literacyideas.com/teaching-sequencing-in-english", "snippet": "Students should <b>write</b> the events of the story on each step of the stairs in the order they occur, starting with the first event on the first step and with each event that follows written on the next step above. This is also a useful way for students to represent nonlinear narratives, such as in medias res. This organizer is a helpful means to unravel more complex chronologies. The finished chart helps the student to see each of the events in the story in the order that they occurred. 101 ...", "dateLastCrawled": "2022-02-03T03:02:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "paper review: \u201cBART: Denoising <b>Sequence-to-Sequence</b> Pre-training for ...", "url": "https://medium.com/mlearning-ai/paper-summary-bart-denoising-sequence-to-sequence-pre-training-for-natural-language-generation-69e41dfbb7fe", "isFamilyFriendly": true, "displayUrl": "https://medium.com/m<b>learning</b>-ai/paper-summary-bart-denoising-<b>sequence-to-sequence</b>-pre...", "snippet": "This configuration is to show that a pretrained BART model itself as a whole <b>can</b> be utilized by adding the small front encoder for machine translation <b>task</b> on a new language.", "dateLastCrawled": "2022-02-01T16:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Neural <b>Machine Translation</b>: Inner Workings, Seq2Seq, and Transformers ...", "url": "https://towardsdatascience.com/neural-machine-translation-inner-workings-seq2seq-and-transformers-229faff5895b", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/neural-<b>machine-translation</b>-inner-workings-seq2seq-and...", "snippet": "We <b>can</b> formally <b>write</b> this goal as in Eq. 1, that is we want to find the output sentence y that maximizes the ... Seq2seq paper by Sutskever et al. \u201c<b>Sequence-to-sequence</b> <b>learning</b> with neural networks\u201d [1]. Attention mechanism by Bahdanau et al. \u201cNeural <b>machine translation</b> by jointly <b>learning</b> to align and translate\u201d [2]. BPE and subword units by Sennrich et al. \u201c Neural <b>machine translation</b> of rare words with subword units.\u201d [4]. The Transformer by Vaswani et al. \u201cAttention is ...", "dateLastCrawled": "2022-01-29T08:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Order Matters: Sequence to sequence for sets</b> | DeepAI", "url": "https://deepai.org/publication/order-matters-sequence-to-sequence-for-sets", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>order-matters-sequence-to-sequence-for-sets</b>", "snippet": "Sequences have become first class citizens in supervised <b>learning</b> thanks to the resurgence of recurrent neural networks.Many complex tasks that require mapping from or to a sequence of observations <b>can</b> now be formulated with the <b>sequence-to-sequence</b> (seq2seq) framework which employs the chain rule to efficiently represent the joint probability of sequences. In many cases, however, variable sized inputs and/or outputs might not be naturally expressed as sequences.", "dateLastCrawled": "2021-12-18T17:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Why do Transformers yield Superior <b>Sequence to Sequence</b>(Seq2Seq)Results ...", "url": "https://medium.com/saarthi-ai/transformers-attention-based-seq2seq-machine-translation-a28940aaa4fe", "isFamilyFriendly": true, "displayUrl": "https://medium.com/saarthi-ai/<b>transformer</b>s-attention-based-seq2seq-machine-translation...", "snippet": "The long-range dependency of RNN. Self-Attention. Now, let\u2019s discuss the idea of self-attention. Self-attention. also known as intra-attention, is an attention operation of a single sequence in ...", "dateLastCrawled": "2022-01-28T10:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Transformers BART Model Explained for Text Summarization", "url": "https://www.projectpro.io/article/transformers-bart-model-explained/553", "isFamilyFriendly": true, "displayUrl": "https://www.projectpro.io/article/transformers-bart-model-explained/553", "snippet": "The original Transformer is based on an encoder-decoder architecture and is a classic <b>sequence-to-sequence</b> model. The model\u2019s input and output are in the form of a sequence (text), and the encoder learns a high-dimensional representation of the input,which is then mapped to the output by the decoder. This architecture introduced a new form of <b>learning</b> for language-related tasks and, thus, the models spawned from it achieve outstanding results overtaking the existing deep neural network ...", "dateLastCrawled": "2022-02-03T05:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Neural Programmer-Interpreters</b> | DeepAI", "url": "https://deepai.org/publication/neural-programmer-interpreters", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>neural-programmer-interpreters</b>", "snippet": "By <b>learning</b> to compose lower-level programs to express higher-level programs, NPI reduces sample complexity and increases generalization ability <b>compared</b> to <b>sequence-to-sequence</b> LSTMs. The program memory allows efficient <b>learning</b> of additional tasks by building on existing programs. NPI <b>can</b> also harness the environment (e.g. a scratch", "dateLastCrawled": "2021-12-29T10:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Order Matters: Sequence to sequence for sets</b>", "url": "https://www.researchgate.net/publication/284476538_Order_Matters_Sequence_to_sequence_for_sets", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/284476538_<b>Order_Matters_Sequence_to_sequence</b>...", "snippet": "A set-to-sequence (Vinyals et al., 2016) model was developed for investigation on the importance of order in various machine <b>learning</b> problems. This framework learns a holistic representation of ...", "dateLastCrawled": "2022-01-23T14:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Behind Tacotron 2: Google&#39;s Incredibly Real <b>Text To Speech System</b>", "url": "https://analyticsindiamag.com/tacotron-2-google-ai-text-to-speech-system/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/tacotron-2-google-ai-<b>text-to-speech-system</b>", "snippet": "The <b>sequence-to-sequence</b> model that generates mel spectrograms has been borrowed from Tacotron, ... This means that Tacotron 2 <b>can</b> pronounce words like <b>read</b>, sewer and lead, according to their usage in a sentence. The audio sample below demonstrates this ability through the vocal rendering of the sentence, \u201c Don\u2019t desert me here in the desert!\u201d Mastery Over Prosody. Another area which Tacotron 2 seems to have mastered is prosody. Prosody is a distinguishing feature of human speech ...", "dateLastCrawled": "2022-02-03T04:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Department of Computer Science, University of Oxford: Publication ...", "url": "https://www.cs.ox.ac.uk/publications/publication10338-abstract.html", "isFamilyFriendly": true, "displayUrl": "https://www.cs.ox.ac.uk/publications/publication10338-abstract.html", "snippet": "By <b>learning</b> to compose lower-level programs to express higher-level programs, NPI reduces sample complexity and increases generalization ability <b>compared</b> to <b>sequence-to-sequence</b> LSTMs. The program memory allows efficient <b>learning</b> of additional tasks by building on existing programs. NPI <b>can</b> also harness the environment (e.g. a scratch pad with <b>read</b>-<b>write</b> pointers) to cache intermediate results of computation, lessening the long-term memory burden on recurrent hidden units. In this work we ...", "dateLastCrawled": "2021-08-10T20:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Neural Programmer-Interpreters</b> - VideoLectures.NET", "url": "http://videolectures.net/iclr2016_reed_neural_programmer/", "isFamilyFriendly": true, "displayUrl": "videolectures.net/iclr2016_reed_neural_programmer", "snippet": "By <b>learning</b> to compose lower-level programs to express higher-level programs, NPI reduces sample complexity and increases generalization ability <b>compared</b> to <b>sequence-to-sequence</b> LSTMs. The program memory allows efficient <b>learning</b> of additional tasks by building on existing programs. NPI <b>can</b> also harness the environment (e.g. a scratch pad with <b>read</b>-<b>write</b> pointers) to cache intermediate results of computation, lessening the long-term memory burden on recurrent hidden units. In this work we ...", "dateLastCrawled": "2021-12-18T03:48:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Is the race over for <b>Seq2Seq</b> models? | by Thushan Ganegedara | Towards ...", "url": "https://towardsdatascience.com/is-the-race-over-for-seq2seq-models-adef2b24841c", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/is-the-race-over-for-<b>seq2seq</b>-models-adef2b24841c", "snippet": "This goes for any <b>machine</b> <b>learning</b> <b>task</b>, be it <b>machine</b> translation, dependency parsing or language modelling. Self-attention layer enables to transformer to exactly do that. While processing the word \u201cits\u201d, the model can look at all the other words and decide for itself which words are important to \u201c mix \u201d into the output, so that the transformer can solve the <b>task</b> effectively.", "dateLastCrawled": "2022-02-02T02:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "The language of <b>proteins: NLP, machine learning &amp; protein sequences</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S2001037021000945", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2001037021000945", "snippet": "Popular deep-<b>learning</b> architectures are long short-term memory (LSTM) , <b>sequence-to-sequence</b> (seq2seq) and attention . In seq2seq models, a text is transformed using an encoder component, then a separate decoder uses the encoded representation to solve some <b>task</b> (e.g. translating between English and French). Attention models use attention layers (also called attention heads) that allow the network to concentrate on specific tokens in the text", "dateLastCrawled": "2022-02-03T02:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "9.7. <b>Sequence to Sequence</b> <b>Learning</b> \u2014 Dive into Deep <b>Learning</b> 0.17.2 ...", "url": "https://d2l.ai/chapter_recurrent-modern/seq2seq.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_recurrent-modern/seq2seq.html", "snippet": "As we have seen in Section 9.5, in <b>machine</b> translation both the input and output are a variable-length <b>sequence</b>.To address this type of problem, we have designed a general encoder-decoder architecture in Section 9.6.In this section, we will use two RNNs to design the encoder and the decoder of this architecture and apply it to <b>sequence to sequence</b> <b>learning</b> for <b>machine</b> translation [Sutskever et al., 2014] [Cho et al., 2014b].. Following the design principle of the encoder-decoder architecture ...", "dateLastCrawled": "2022-01-26T00:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Introduction to Transformers in Machine Learning</b>", "url": "https://www.machinecurve.com/index.php/2020/12/28/introduction-to-transformers-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/.../12/28/<b>introduction-to-transformers-in-machine-learning</b>", "snippet": "When you talk about <b>Machine</b> <b>Learning</b> in Natural Language Processing these days, all you hear is one thing \u2013 Transformers. Models based on this Deep <b>Learning</b> architecture have taken the NLP world by storm since 2017. In fact, they are the go-to approach today, and many of the approaches build on top of the original Transformer, one way or another. Transformers are however not simple. The original Transformer architecture is quite complex and the same is true for many of the spin-off ...", "dateLastCrawled": "2022-02-03T03:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Geometric deep <b>learning</b> on molecular representations | Nature <b>Machine</b> ...", "url": "https://www.nature.com/articles/s42256-021-00418-8", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s42256-021-00418-8", "snippet": "In <b>analogy</b> to some popular pre-deep <b>learning</b> ... which can be cast as a <b>sequence-to-sequence</b> translation <b>task</b> in which the string representations of the reactants are mapped to those of the ...", "dateLastCrawled": "2022-01-29T04:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Benefits of AI and Deep <b>Learning</b> - <b>Machine</b> <b>Learning</b> Company ...", "url": "https://www.folio3.ai/blog/advantages-of-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://www.folio3.ai/blog/<b>advantages-of-neural-networks</b>", "snippet": "<b>Sequence-To-Sequence</b> models are mainly applied in question answering, <b>machine</b> translations systems, and chatbots. What Are The <b>Advantages of Neural Networks</b> . There are various <b>advantages of neural networks</b>, some of which are discussed below: 1) Store information on the entire network. Just like it happens in traditional programming where information is stored on the network and not on a database. If a few pieces of information disappear from one place, it does not stop the whole network ...", "dateLastCrawled": "2022-02-02T09:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine Learning Gist</b> \u00b7 GitHub", "url": "https://gist.github.com/sgoyal1012/b30d70d12b6efad88bb285e8e709b161", "isFamilyFriendly": true, "displayUrl": "https://gist.github.com/sgoyal1012/b30d70d12b6efad88bb285e8e709b161", "snippet": "Week 3: <b>Sequence to sequence</b> architectures. <b>Sequence to sequence</b> models Language translation for example; Image captioning, caption an image; Picking the most likely model <b>Machine</b> Transation Model Split into a model encoding the sentence; and then a language model. Calculate the probability of an English sentence conditioned on a French sentence.", "dateLastCrawled": "2022-01-29T03:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A Gentle Introduction to <b>Long Short-Term Memory</b> Networks by the Experts", "url": "https://machinelearningmastery.com/gentle-introduction-long-short-term-memory-networks-experts/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/gentle-introduction-<b>long-short-term-memory</b>-networks...", "snippet": "<b>Long Short-Term Memory</b> (LSTM) networks are a type of recurrent neural network capable of <b>learning</b> order dependence in sequence prediction problems. This is a behavior required in complex problem domains like <b>machine</b> translation, speech recognition, and more. LSTMs are a complex area of deep <b>learning</b>. It can be hard to get your hands around what LSTMs are, and how terms like bidirectional", "dateLastCrawled": "2022-01-31T03:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Sequence Classification with LSTM Recurrent Neural Networks in Python ...", "url": "https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/sequence-classification-", "snippet": "Sequence Classification with LSTM Recurrent Neural Networks in Python with Keras. Sequence classification is a predictive modeling problem where you have some sequence of inputs over space or time and the <b>task</b> is to predict a category for the sequence. What makes this problem difficult is that the sequences can vary in length, be comprised of a ...", "dateLastCrawled": "2022-02-02T22:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "deep-<b>learning</b>-coursera/Week 1 Quiz - Introduction to deep <b>learning</b>.md ...", "url": "https://github.com/Kulbear/deep-learning-coursera/blob/master/Neural%20Networks%20and%20Deep%20Learning/Week%201%20Quiz%20-%20Introduction%20to%20deep%20learning.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/Kulbear/deep-<b>learning</b>-coursera/blob/master/Neural Networks and Deep...", "snippet": "Week 1 Quiz - Introduction to deep <b>learning</b>. What does the <b>analogy</b> \u201cAI is the new electricity\u201d refer to? AI is powering personal devices in our homes and offices, similar to electricity. Through the \u201csmart grid\u201d, AI is delivering a new wave of electricity. AI runs on computers and is thus powered by electricity, but it is letting computers do things not possible before. Similar to electricity starting about 100 years ago, AI is transforming multiple industries. Note: Andrew ...", "dateLastCrawled": "2022-02-03T05:16:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(sequence-to-sequence task)  is like +(learning to read and write)", "+(sequence-to-sequence task) is similar to +(learning to read and write)", "+(sequence-to-sequence task) can be thought of as +(learning to read and write)", "+(sequence-to-sequence task) can be compared to +(learning to read and write)", "machine learning +(sequence-to-sequence task AND analogy)", "machine learning +(\"sequence-to-sequence task is like\")", "machine learning +(\"sequence-to-sequence task is similar\")", "machine learning +(\"just as sequence-to-sequence task\")", "machine learning +(\"sequence-to-sequence task can be thought of as\")", "machine learning +(\"sequence-to-sequence task can be compared to\")"]}