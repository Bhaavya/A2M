{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "An <b>introduction to sequence-to-sequence learning</b> - Loren Lugosch", "url": "https://lorenlugosch.github.io/posts/2019/02/seq2seq/", "isFamilyFriendly": true, "displayUrl": "https://lorenlugosch.github.io/posts/2019/02/seq2seq", "snippet": "We will train a <b>sequence-to-sequence</b> model to take as input a vowel-less <b>sentence</b> (\u201cMst ppl\u201d) and output the <b>sentence</b> with the correct vowels re-inserted (\u201cMost people\u201d). This toy <b>task</b> is a bit easier to work with than a <b>task</b> <b>like</b> speech recognition or translation, in which you need lots of labelled data and lots of tricks to get something to work. A tutorial which covers the actual useful <b>task</b> of translating from French to English using PyTorch can be found here. Running the ...", "dateLastCrawled": "2022-02-02T00:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Sequence-to-sequence learning with Transducers</b> - Loren Lugosch", "url": "https://lorenlugosch.github.io/posts/2020/11/transducer/", "isFamilyFriendly": true, "displayUrl": "https://lorenlugosch.github.io/posts/2020/11/transducer", "snippet": "<b>Sequence-to-sequence learning with Transducers</b>. Published: November 16, 2020 The Transducer (sometimes called the \u201cRNN Transducer\u201d or \u201cRNN-T\u201d, though it need not use RNNs) is a <b>sequence-to-sequence</b> model proposed by Alex Graves in \u201cSequence Transduction with Recurrent Neural Networks\u201d. The paper was published at the ICML 2012 Workshop on Representation Learning.Graves showed that the Transducer was a sensible model to use for speech recognition, achieving good results on a small ...", "dateLastCrawled": "2022-01-31T02:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Differences between Autoregressive, Autoencoding and Sequence</b>-to ...", "url": "https://www.machinecurve.com/index.php/2020/12/29/differences-between-autoregressive-autoencoding-and-sequence-to-sequence-models-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.machinecurve.com/index.php/2020/12/29/differences-between-autoregressive...", "snippet": "This sequence can for example be a <b>sentence</b> written in English. Adding an encoder and a decoder allows us to build models that can transduce (i.e. map without losing semantics) \u2018one way\u2019 into \u2018another\u2019, e.g. German into English. By training the encoder and decoder together, we have created what is known as a <b>sequence-to-sequence</b> model ...", "dateLastCrawled": "2022-02-02T01:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "The 7 <b>NLP</b> Techniques That Will Change How You Communicate in the Future ...", "url": "https://heartbeat.comet.ml/the-7-nlp-techniques-that-will-change-how-you-communicate-in-the-future-part-i-f0114b2f0497", "isFamilyFriendly": true, "displayUrl": "https://heartbeat.comet.ml/the-7-<b>nlp</b>-techniques-that-will-change-how-you-communicate...", "snippet": "It uses the <b>sequence-to-sequence</b> framework. The model converses by predicting <b>the next</b> <b>sentence</b> given the previous <b>sentence</b>(s) in a conversation. The strength of the model is such it can be trained end-to-end and thus requires much fewer hand-crafted rules. The model can generate simple conversations given a large conversational training ...", "dateLastCrawled": "2022-01-23T13:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "STATE-OF-THE-ARTAUTOMATEDIMAGE DESCRIPTIONSYSTEMS", "url": "https://www.inf.uni-hamburg.de/en/inst/ab/lt/teaching/independent-studies/201807-fkarl-soa-image.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.inf.uni-hamburg.de/en/inst/ab/lt/<b>teaching</b>/independent-studies/201807-fkarl...", "snippet": "ideas from <b>Sequence-to-Sequence</b> (Seq2Seq), used most famously for machine transla-tion, were considered state-of-the-art. Image descriptions are learned in an end-to-end fashion and report state-of-the-art performance for most <b>word</b> n-gram based scores <b>like</b> BLEU (Papineni, Roukos, Ward, &amp; Zhu, 2002) or ROUGE (Lin, 2004). Both models, together with a brief history of AID, are explained and compared in this survey. In order to test the performance of automated end-to-end image description ...", "dateLastCrawled": "2021-12-25T22:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "How to Develop an Encoder-Decoder Model for <b>Sequence-to-Sequence</b> ...", "url": "https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence-prediction-keras/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence...", "snippet": "Here you can see how the recursive use of the model can be used to build up output sequences. During prediction, the inference_encoder model is used to encode the input sequence once which returns states that are used to initialize the inference_decoder model. From that point, the inference_decoder model is used to generate predictions step by step.. The function below named <b>predict</b>_sequence() can be used after the model is trained to generate a target sequence given a source sequence.", "dateLastCrawled": "2022-01-29T13:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Tutorial - Sequence Modelling | Attention Models", "url": "https://www.analyticsvidhya.com/blog/2019/01/sequence-models-deeplearning/", "isFamilyFriendly": true, "displayUrl": "https://www.analyticsvidhya.com/blog/2019/01/sequence-models-deeplearning", "snippet": "The input to the model is \u201cI want a glass of orange\u201d and we want the model to <b>predict</b> <b>the next</b> <b>word</b>. We will first learn the embeddings of each of the words in the sequence using a pre trained <b>word</b> embedding matrix and then pass those embeddings to a neural network which will have a softmax classifier at the end to <b>predict</b> <b>the next</b> <b>word</b>.", "dateLastCrawled": "2022-02-02T03:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>How I Used Deep Learning To Train A Chatbot</b> To Talk <b>Like</b> Me (Sorta ...", "url": "https://adeshpande3.github.io/How-I-Used-Deep-Learning-to-Train-a-Chatbot-to-Talk-Like-Me", "isFamilyFriendly": true, "displayUrl": "https://adeshpande3.github.io/<b>How-I-Used-Deep-Learning-to-Train-a-Chatbot</b>-to-Talk-<b>Like</b>-Me", "snippet": "A <b>sequence to sequence</b> model is composed of 2 main components, an encoder RNN and a decoder RNN (If you\u2019re a little shaky on RNNs, check out my previous blog post for a refresher). From a high level, the encoder\u2019s job is to encapsulate the information of the input text into a fixed representation. The decoder\u2019s is to take that representation, and generate a variable length text that best responds to it.", "dateLastCrawled": "2022-01-30T02:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Sequence Models by Andrew Ng \u2014 11 Lessons Learned | by Ryan Shrott ...", "url": "https://towardsdatascience.com/sequence-models-by-andrew-ng-11-lessons-learned-c62fb1d3485b", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/sequence-models-by-andrew-ng-11-lessons-learned-c62fb1d...", "snippet": "These systems can be used to <b>predict</b> when a person says \u201cAlexa\u201d or <b>predict</b> the timing of financial trigger events. ... the previous inputs are inherently important in predicting <b>the next</b> output. For example, if you were predicting <b>the next</b> <b>word</b> in a stream of text, you would want to know at least a couple of words before the target <b>word</b>. Traditional neural networks require the input and output sequence lengths to be constant across all predictions. As discussed in lesson 2, sequence ...", "dateLastCrawled": "2022-01-29T09:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Self Supervised Representation Learning in NLP</b>", "url": "https://amitness.com/2020/05/self-supervised-learning-nlp/", "isFamilyFriendly": true, "displayUrl": "https://amitness.com/2020/05/self-supervised-learning-nlp", "snippet": "Compared to the auto-regressive formulation, in this <b>task</b>, we <b>predict</b> only a small subset of masked words and so the amount of things learned from each <b>sentence</b> is lower. 6. <b>Next</b> <b>Sentence</b> Prediction. In this formulation, we take two consecutive sentences present in a document and another <b>sentence</b> from a random location in the same document or a ...", "dateLastCrawled": "2022-02-02T13:54:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) STEP: <b>Sequence-to-Sequence</b> Transformer Pre-training for Document ...", "url": "https://www.researchgate.net/publication/340475742_STEP_Sequence-to-Sequence_Transformer_Pre-training_for_Document_Summarization", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/340475742_STEP_<b>Sequence-to-Sequence</b>...", "snippet": "Specifically, STEP is pre-trained using three different tasks, namely <b>sentence</b> reordering, <b>next</b> <b>sentence</b> generation, and masked document generation. Experiments on two summarization datasets show ...", "dateLastCrawled": "2022-01-09T01:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "How <b>to deal with a vocabulary</b>? - <b>Sequence to sequence</b> tasks | <b>Coursera</b>", "url": "https://www.coursera.org/lecture/language-processing/how-to-deal-with-a-vocabulary-mvV6t", "isFamilyFriendly": true, "displayUrl": "https://<b>www.coursera.org</b>/lecture/language-processing/how-<b>to-deal-with-a-vocabulary</b>-mvV6t", "snippet": "Nearly any <b>task</b> in NLP can be formulates as a <b>sequence to sequence</b> <b>task</b>: machine translation, summarization, question answering, and many more. In this module we will learn a general encoder-decoder-attention architecture that can be used to solve them. We will cover machine translation in more details and you will see how attention technique ...", "dateLastCrawled": "2022-01-18T13:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "STATE-OF-THE-ARTAUTOMATEDIMAGE DESCRIPTIONSYSTEMS", "url": "https://www.inf.uni-hamburg.de/en/inst/ab/lt/teaching/independent-studies/201807-fkarl-soa-image.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.inf.uni-hamburg.de/en/inst/ab/lt/<b>teaching</b>/independent-studies/201807-fkarl...", "snippet": "ideas from <b>Sequence-to-Sequence</b> (Seq2Seq), used most famously for machine transla-tion, were considered state-of-the-art. Image descriptions are learned in an end-to-end fashion and report state-of-the-art performance for most <b>word</b> n-gram based scores like BLEU (Papineni, Roukos, Ward, &amp; Zhu, 2002) or ROUGE (Lin, 2004). Both models, together with a brief history of AID, are explained and compared in this survey. In order to test the performance of automated end-to-end image description ...", "dateLastCrawled": "2021-12-25T22:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>How I Used Deep Learning To Train A Chatbot</b> To Talk Like Me (Sorta ...", "url": "https://adeshpande3.github.io/How-I-Used-Deep-Learning-to-Train-a-Chatbot-to-Talk-Like-Me", "isFamilyFriendly": true, "displayUrl": "https://adeshpande3.github.io/<b>How-I-Used-Deep-Learning-to-Train-a-Chatbot</b>-to-Talk-Like-Me", "snippet": "A <b>sequence to sequence</b> model is composed of 2 main components, an encoder RNN and a decoder RNN (If you\u2019re a little shaky on RNNs, check out my previous blog post for a refresher). From a high level, the encoder\u2019s job is to encapsulate the information of the input text into a fixed representation. The decoder\u2019s is to take that representation, and generate a variable length text that best responds to it.", "dateLastCrawled": "2022-01-30T02:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "How to Develop an Encoder-Decoder Model for <b>Sequence-to-Sequence</b> ...", "url": "https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence-prediction-keras/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence...", "snippet": "Here you can see how the recursive use of the model can be used to build up output sequences. During prediction, the inference_encoder model is used to encode the input sequence once which returns states that are used to initialize the inference_decoder model. From that point, the inference_decoder model is used to generate predictions step by step.. The function below named <b>predict</b>_sequence() can be used after the model is trained to generate a target sequence given a source sequence.", "dateLastCrawled": "2022-01-29T13:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Self Supervised Representation Learning in NLP</b>", "url": "https://amitness.com/2020/05/self-supervised-learning-nlp/", "isFamilyFriendly": true, "displayUrl": "https://amitness.com/2020/05/self-supervised-learning-nlp", "snippet": "3. Neighbor <b>Sentence</b> Prediction. In this formulation, we take three consecutive sentences and design a <b>task</b> in which given the center <b>sentence</b>, we need to generate the previous <b>sentence</b> and <b>the next</b> <b>sentence</b>. It <b>is similar</b> to the previous skip-gram method but applied to sentences instead of words.", "dateLastCrawled": "2022-02-02T13:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "How to easily do Handwriting Recognition using Deep Learning", "url": "https://nanonets.com/blog/handwritten-character-recognition/", "isFamilyFriendly": true, "displayUrl": "https://nanonets.com/blog/handwritten-character-recognition", "snippet": "This paper proposes an attention based <b>sequence-to-sequence</b> model for handwritten <b>word</b> recognition. The proposed architecture has three main parts: an encoder, consisting of a CNN and a bi-directional GRU, an attention mechanism devoted to focus on the pertinent features and a decoder formed by a one-directional GRU, able to spell the corresponding <b>word</b>, character by character. The encoder uses a CNN to extract visual features. A pre-trained VGG-19-BN architecture is used as a feature ...", "dateLastCrawled": "2022-02-03T01:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Sequence Classification with LSTM Recurrent Neural Networks in Python ...", "url": "https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/sequence-classification-", "snippet": "Sequence classification is a predictive modeling problem where you have some sequence of inputs over space or time and the <b>task</b> is to <b>predict</b> a category for the sequence. What makes this problem difficult is that the sequences can vary in length, be comprised of a very large vocabulary of input symbols and may require the model to learn the long-term", "dateLastCrawled": "2022-02-02T22:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Sequence Models by Andrew Ng \u2014 11 Lessons Learned | by Ryan Shrott ...", "url": "https://towardsdatascience.com/sequence-models-by-andrew-ng-11-lessons-learned-c62fb1d3485b", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/sequence-models-by-andrew-ng-11-lessons-learned-c62fb1d...", "snippet": "These systems can be used to <b>predict</b> when a person says \u201cAlexa\u201d or <b>predict</b> the timing of financial trigger events. ... the previous inputs are inherently important in predicting <b>the next</b> output. For example, if you were predicting <b>the next</b> <b>word</b> in a stream of text, you would want to know at least a couple of words before the target <b>word</b>. Traditional neural networks require the input and output sequence lengths to be constant across all predictions. As discussed in lesson 2, sequence ...", "dateLastCrawled": "2022-01-29T09:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "The 7 <b>NLP</b> Techniques That Will Change How You Communicate in the Future ...", "url": "https://heartbeat.comet.ml/the-7-nlp-techniques-that-will-change-how-you-communicate-in-the-future-part-i-f0114b2f0497", "isFamilyFriendly": true, "displayUrl": "https://heartbeat.comet.ml/the-7-<b>nlp</b>-techniques-that-will-change-how-you-communicate...", "snippet": "In Word2vec, we have a large corpus of text in which every <b>word</b> in a fixed vocabulary is represented by a vector. We then go through each position t in the text, which has a center <b>word</b> c and context words o.<b>Next</b>, we use the similarity of the <b>word</b> vectors for c and o to calculate the probability of o given c (or vice versa). We keep adjusting the <b>word</b> vectors to maximize this probability.", "dateLastCrawled": "2022-01-23T13:18:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "An <b>introduction to sequence-to-sequence learning</b> - Loren Lugosch", "url": "https://lorenlugosch.github.io/posts/2019/02/seq2seq/", "isFamilyFriendly": true, "displayUrl": "https://lorenlugosch.github.io/posts/2019/02/seq2seq", "snippet": "We will train a <b>sequence-to-sequence</b> model to take as input a vowel-less <b>sentence</b> (\u201cMst ppl\u201d) and output the <b>sentence</b> with the correct vowels re-inserted (\u201cMost people\u201d). This toy <b>task</b> is a bit easier to work with than a <b>task</b> like speech recognition or translation, in which you need lots of labelled data and lots of tricks to get something to work. A tutorial which covers the actual useful <b>task</b> of translating from French to English using PyTorch <b>can</b> be found here. Running the ...", "dateLastCrawled": "2022-02-02T00:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Sequence Models by Andrew Ng \u2014 11 Lessons Learned | by Ryan Shrott ...", "url": "https://towardsdatascience.com/sequence-models-by-andrew-ng-11-lessons-learned-c62fb1d3485b", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/sequence-models-by-andrew-ng-11-lessons-learned-c62fb1d...", "snippet": "<b>Word</b> embeddings <b>can</b> <b>be thought</b> of as the vector representation of a given <b>word</b>. They <b>can</b> be trained using the Word2Vec, Negative Sampling or Glove algorithms. <b>Word</b> embedding models may be trained on a very large text corpus (say 100B words) and <b>can</b> then be used on a sequence prediction <b>task</b> with a smaller number of training example (say 10,000 words). For example, sentiment classification may use <b>word</b> embeddings to greatly reduce the number of training examples required to generate an ...", "dateLastCrawled": "2022-01-29T09:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Differences between Autoregressive, Autoencoding and Sequence</b>-to ...", "url": "https://www.machinecurve.com/index.php/2020/12/29/differences-between-autoregressive-autoencoding-and-sequence-to-sequence-models-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.machinecurve.com/index.php/2020/12/29/differences-between-autoregressive...", "snippet": "A <b>sequence-to-sequence</b> model is capable of ingesting a sequence of a particular kind and outputting another sequence of another kind. In general, it\u2019s the model architecture visualized above. Such models are also called Seq2Seq models. There are many applications of performing <b>sequence-to-sequence</b> learning. <b>Sequence to sequence</b> learning has been successful in many tasks such as machine translation, speech recognition (\u2026) and text summarization (\u2026) amongst others. Gehring et al. (2017 ...", "dateLastCrawled": "2022-02-02T01:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "The 7 <b>NLP</b> Techniques That Will Change How You Communicate in the Future ...", "url": "https://heartbeat.comet.ml/the-7-nlp-techniques-that-will-change-how-you-communicate-in-the-future-part-i-f0114b2f0497", "isFamilyFriendly": true, "displayUrl": "https://heartbeat.comet.ml/the-7-<b>nlp</b>-techniques-that-will-change-how-you-communicate...", "snippet": "It uses the <b>sequence-to-sequence</b> framework. The model converses by predicting <b>the next</b> <b>sentence</b> given the previous <b>sentence</b>(s) in a conversation. The strength of the model is such it <b>can</b> be trained end-to-end and thus requires much fewer hand-crafted rules. The model <b>can</b> generate simple conversations given a large conversational training ...", "dateLastCrawled": "2022-01-23T13:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Knet.jl/rnn.md at master \u00b7 denizyuret/Knet.jl \u00b7 <b>GitHub</b>", "url": "https://github.com/denizyuret/Knet.jl/blob/master/docs/src/rnn.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/denizyuret/Knet.jl/blob/master/docs/src/rnn.md", "snippet": "<b>Sequence-to-sequence</b> mapping: given a sequence, produce another, not necessarily parallel, ... The state h_t <b>can</b> <b>be thought</b> of as analogous to a memory device storing variables in <b>a computer</b> program. In fact, RNNs have been proven to be Turing complete machines (however see this and this for a discussion). At each time step, the RNN processes the current input x_t using the &quot;program&quot; specified by parameters w and the internal &quot;variables&quot; specified by h_{t-1}. The program stores new values in ...", "dateLastCrawled": "2021-12-04T13:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "How to Develop an Encoder-Decoder Model for <b>Sequence-to-Sequence</b> ...", "url": "https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence-prediction-keras/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence...", "snippet": "Here you <b>can</b> see how the recursive use of the model <b>can</b> be used to build up output sequences. During prediction, the inference_encoder model is used to encode the input sequence once which returns states that are used to initialize the inference_decoder model. From that point, the inference_decoder model is used to generate predictions step by step.. The function below named <b>predict</b>_sequence() <b>can</b> be used after the model is trained to generate a target sequence given a source sequence.", "dateLastCrawled": "2022-01-29T13:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Self Supervised Representation Learning in NLP</b>", "url": "https://amitness.com/2020/05/self-supervised-learning-nlp/", "isFamilyFriendly": true, "displayUrl": "https://amitness.com/2020/05/self-supervised-learning-nlp", "snippet": "This formulation has been used in the Skip-<b>Thought</b> Vectors paper. 4. Auto-regressive Language Modeling. In this formulation, we take large corpus of unlabeled text and setup a <b>task</b> to <b>predict</b> <b>the next</b> <b>word</b> given the previous words. Since we already know what <b>word</b> should come <b>next</b> from the corpus, we don\u2019t need manually-annotated labels. For example, we could setup the <b>task</b> as left-to-right language modeling by predicting <b>next</b> words given the previous words. We <b>can</b> also formulate this as ...", "dateLastCrawled": "2022-02-02T13:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "The Main Approaches to Natural Language Processing Tasks", "url": "https://www.kdnuggets.com/2018/10/main-approaches-natural-language-processing-tasks.html", "isFamilyFriendly": true, "displayUrl": "https://www.<b>kdnuggets</b>.com/2018/10/main-approaches-natural-language-processing-<b>tasks</b>.html", "snippet": "There are no hard lines between these <b>task</b> types; however, many are fairly well-defined at this point. A given macro NLP <b>task</b> may include a variety of sub-tasks. We first outlined the main approaches, since the technologies are often focused on for beginners, but it&#39;s good to have a concrete idea of what types of NLP tasks there are. Below are the main categories of NLP tasks. 1. Text Classification Tasks. Representation: bag of words (does not preserve <b>word</b> order) Goal: <b>predict</b> tags ...", "dateLastCrawled": "2022-01-30T19:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Is there a way of <b>teaching</b> a neural network using a string as input, a ...", "url": "https://www.quora.com/Is-there-a-way-of-teaching-a-neural-network-using-a-string-as-input-a-string-as-expected-output-and-to-teach-it-to-output-strings-when-given-a-string-as-input", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Is-there-a-way-of-<b>teaching</b>-a-neural-network-using-a-string-as...", "snippet": "Answer (1 of 3): Yes. Such networks are called <b>Sequence to Sequence</b> models. You already know that neural networks or any other model only understands numbers. So we have to find a way to represent these strings by numbers ( vectors to be more precise). For this, there are lot of approaches follow...", "dateLastCrawled": "2022-01-23T04:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How to <b>train GPT-2 to Create JSOn</b>? : LanguageTechnology", "url": "https://www.reddit.com/r/LanguageTechnology/comments/j1iaxq/how_to_train_gpt2_to_create_json/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/LanguageTechnology/comments/j1iaxq/how_to_train_gpt2_to...", "snippet": "So you&#39;re describing a <b>sequence to sequence</b> (seq2seq) <b>task</b>, which is a bit tricky with GPT-2 in huggingface. Most seq2seq tasks use a encoder-decoder setup where the encoder takes your inputs and encodes it into an intermediate representation and a decoder decodes into a target language. I don&#39;t think there&#39;s a straightforward way (e.g. using existing training scripts) in huggingface but I could be wrong. The combiners api", "dateLastCrawled": "2022-01-22T07:20:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Sequence-to-sequence learning with Transducers</b> - Loren Lugosch", "url": "https://lorenlugosch.github.io/posts/2020/11/transducer/", "isFamilyFriendly": true, "displayUrl": "https://lorenlugosch.github.io/posts/2020/11/transducer", "snippet": "<b>Sequence-to-sequence learning with Transducers</b>. Published: November 16, 2020 The Transducer (sometimes called the \u201cRNN Transducer\u201d or \u201cRNN-T\u201d, though it need not use RNNs) is a <b>sequence-to-sequence</b> model proposed by Alex Graves in \u201cSequence Transduction with Recurrent Neural Networks\u201d. The paper was published at the ICML 2012 Workshop on Representation Learning.Graves showed that the Transducer was a sensible model to use for speech recognition, achieving good results on a small ...", "dateLastCrawled": "2022-01-31T02:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "The 7 <b>NLP</b> Techniques That Will Change How You Communicate in the Future ...", "url": "https://heartbeat.comet.ml/the-7-nlp-techniques-that-will-change-how-you-communicate-in-the-future-part-i-f0114b2f0497", "isFamilyFriendly": true, "displayUrl": "https://heartbeat.comet.ml/the-7-<b>nlp</b>-techniques-that-will-change-how-you-communicate...", "snippet": "It uses the <b>sequence-to-sequence</b> framework. The model converses by predicting <b>the next</b> <b>sentence</b> given the previous <b>sentence</b>(s) in a conversation. The strength of the model is such it <b>can</b> be trained end-to-end and thus requires much fewer hand-crafted rules. The model <b>can</b> generate simple conversations given a large conversational training ...", "dateLastCrawled": "2022-01-23T13:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "STATE-OF-THE-ARTAUTOMATEDIMAGE DESCRIPTIONSYSTEMS", "url": "https://www.inf.uni-hamburg.de/en/inst/ab/lt/teaching/independent-studies/201807-fkarl-soa-image.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.inf.uni-hamburg.de/en/inst/ab/lt/<b>teaching</b>/independent-studies/201807-fkarl...", "snippet": "ideas from <b>Sequence-to-Sequence</b> (Seq2Seq), used most famously for machine transla-tion, were considered state-of-the-art. Image descriptions are learned in an end-to-end fashion and report state-of-the-art performance for most <b>word</b> n-gram based scores like BLEU (Papineni, Roukos, Ward, &amp; Zhu, 2002) or ROUGE (Lin, 2004). Both models, together with a brief history of AID, are explained and <b>compared</b> in this survey. In order to test the performance of automated end-to-end image description ...", "dateLastCrawled": "2021-12-25T22:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) STEP: <b>Sequence-to-Sequence</b> Transformer Pre-training for Document ...", "url": "https://www.researchgate.net/publication/340475742_STEP_Sequence-to-Sequence_Transformer_Pre-training_for_Document_Summarization", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/340475742_STEP_<b>Sequence-to-Sequence</b>...", "snippet": "Pre-training of STEP. The document (x 1 , x 2 , \u00b7 \u00b7 \u00b7 , x 8 ) contains three sentences (i.e., SENT. 1, SENT. 2 and SENT. 3). STEP adopts the SEQ2SEQ Transformer architecture. It takes the ...", "dateLastCrawled": "2022-01-09T01:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Self Supervised Representation Learning in NLP</b>", "url": "https://amitness.com/2020/05/self-supervised-learning-nlp/", "isFamilyFriendly": true, "displayUrl": "https://amitness.com/2020/05/self-supervised-learning-nlp", "snippet": "In this formulation, words in a text are randomly masked and the <b>task</b> is to <b>predict</b> them. <b>Compared</b> to the auto-regressive formulation, we <b>can</b> use context from both previous and <b>next</b> words when predicting the masked <b>word</b>. This formulation has been used in the BERT, RoBERTa and ALBERT papers. <b>Compared</b> to the auto-regressive formulation, in this <b>task</b>, we <b>predict</b> only a small subset of masked words and so the amount of things learned from each <b>sentence</b> is lower. 6. <b>Next</b> <b>Sentence</b> Prediction. In ...", "dateLastCrawled": "2022-02-02T13:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "How to easily do Handwriting Recognition using Deep Learning", "url": "https://nanonets.com/blog/handwritten-character-recognition/", "isFamilyFriendly": true, "displayUrl": "https://nanonets.com/blog/handwritten-character-recognition", "snippet": "This paper proposes an attention based <b>sequence-to-sequence</b> model for handwritten <b>word</b> recognition. The proposed architecture has three main parts: an encoder, consisting of a CNN and a bi-directional GRU, an attention mechanism devoted to focus on the pertinent features and a decoder formed by a one-directional GRU, able to spell the corresponding <b>word</b>, character by character. The encoder uses a CNN to extract visual features. A pre-trained VGG-19-BN architecture is used as a feature ...", "dateLastCrawled": "2022-02-03T01:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "How to Develop an Encoder-Decoder Model for <b>Sequence-to-Sequence</b> ...", "url": "https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence-prediction-keras/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence...", "snippet": "Here you <b>can</b> see how the recursive use of the model <b>can</b> be used to build up output sequences. During prediction, the inference_encoder model is used to encode the input sequence once which returns states that are used to initialize the inference_decoder model. From that point, the inference_decoder model is used to generate predictions step by step.. The function below named <b>predict</b>_sequence() <b>can</b> be used after the model is trained to generate a target sequence given a source sequence.", "dateLastCrawled": "2022-01-29T13:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "The language of <b>proteins: NLP, machine learning &amp; protein sequences</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S2001037021000945", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2001037021000945", "snippet": "The skip-gram architecture is exactly the opposite: the model uses the current <b>word</b> to <b>predict</b> the surrounding window of context words. FastText combines words with subword information when learning representations, to better handle unknown or syntactically similar words. Low-dimensional embeddings are popular in NLP due to the huge vocabulary (often &gt;100 k of words) of natural languages. In proteins we have only ~20 AAs. While we <b>can</b> embed AAs onto a lower-dimensional space, it is not as ...", "dateLastCrawled": "2022-02-03T02:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Is there a way of <b>teaching</b> a neural network using a string as input, a ...", "url": "https://www.quora.com/Is-there-a-way-of-teaching-a-neural-network-using-a-string-as-input-a-string-as-expected-output-and-to-teach-it-to-output-strings-when-given-a-string-as-input", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Is-there-a-way-of-<b>teaching</b>-a-neural-network-using-a-string-as...", "snippet": "Answer (1 of 3): Yes. Such networks are called <b>Sequence to Sequence</b> models. You already know that neural networks or any other model only understands numbers. So we have to find a way to represent these strings by numbers ( vectors to be more precise). For this, there are lot of approaches follow...", "dateLastCrawled": "2022-01-23T04:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(PDF) <b>Artificial Intelligence A Modern Approach</b> (4th Edition) | Fahim ...", "url": "https://www.academia.edu/45126798/Artificial_Intelligence_A_Modern_Approach_4th_Edition_", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/45126798/<b>Artificial_Intelligence_A_Modern_Approach</b>_4th_Edition_", "snippet": "Artificial Intelligence (AI) is a big field, and this is a big book. We have tried to explore the full breadth of the field, which encompasses logic, probability, and continuous mathematics; perception, reasoning, learning, and action; fairness,", "dateLastCrawled": "2022-02-02T22:50:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "9.7. <b>Sequence to Sequence</b> <b>Learning</b> \u2014 Dive into Deep <b>Learning</b> 0.17.2 ...", "url": "https://d2l.ai/chapter_recurrent-modern/seq2seq.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_recurrent-modern/seq2seq.html", "snippet": "As we have seen in Section 9.5, in <b>machine</b> translation both the input and output are a variable-length <b>sequence</b>.To address this type of problem, we have designed a general encoder-decoder architecture in Section 9.6.In this section, we will use two RNNs to design the encoder and the decoder of this architecture and apply it to <b>sequence to sequence</b> <b>learning</b> for <b>machine</b> translation [Sutskever et al., 2014] [Cho et al., 2014b].. Following the design principle of the encoder-decoder architecture ...", "dateLastCrawled": "2022-01-26T00:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Is the race over for <b>Seq2Seq</b> models? | by Thushan Ganegedara | Towards ...", "url": "https://towardsdatascience.com/is-the-race-over-for-seq2seq-models-adef2b24841c", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/is-the-race-over-for-<b>seq2seq</b>-models-adef2b24841c", "snippet": "This goes for any <b>machine</b> <b>learning</b> <b>task</b>, be it <b>machine</b> translation, dependency parsing or language modelling. Self-attention layer enables to transformer to exactly do that. While processing the word \u201cits\u201d, the model can look at all the other words and decide for itself which words are important to \u201c mix \u201d into the output, so that the transformer can solve the <b>task</b> effectively.", "dateLastCrawled": "2022-02-02T02:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "The language of <b>proteins: NLP, machine learning &amp; protein sequences</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S2001037021000945", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2001037021000945", "snippet": "Popular deep-<b>learning</b> architectures are long short-term memory (LSTM) , <b>sequence-to-sequence</b> (seq2seq) and attention . In seq2seq models, a text is transformed using an encoder component, then a separate decoder uses the encoded representation to solve some <b>task</b> (e.g. translating between English and French). Attention models use attention layers (also called attention heads) that allow the network to concentrate on specific tokens in the text", "dateLastCrawled": "2022-02-03T02:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Introduction to Transformers in Machine Learning</b>", "url": "https://www.machinecurve.com/index.php/2020/12/28/introduction-to-transformers-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/.../12/28/<b>introduction-to-transformers-in-machine-learning</b>", "snippet": "When you talk about <b>Machine</b> <b>Learning</b> in Natural Language Processing these days, all you hear is one thing \u2013 Transformers. Models based on this Deep <b>Learning</b> architecture have taken the NLP world by storm since 2017. In fact, they are the go-to approach today, and many of the approaches build on top of the original Transformer, one way or another. Transformers are however not simple. The original Transformer architecture is quite complex and the same is true for many of the spin-off ...", "dateLastCrawled": "2022-02-03T03:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A Gentle Introduction to <b>Long Short-Term Memory</b> Networks by the Experts", "url": "https://machinelearningmastery.com/gentle-introduction-long-short-term-memory-networks-experts/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/gentle-introduction-<b>long-short-term-memory</b>-networks...", "snippet": "<b>Long Short-Term Memory</b> (LSTM) networks are a type of recurrent neural network capable of <b>learning</b> order dependence in sequence prediction problems. This is a behavior required in complex problem domains like <b>machine</b> translation, speech recognition, and more. LSTMs are a complex area of deep <b>learning</b>. It can be hard to get your hands around what LSTMs are, and how terms like bidirectional", "dateLastCrawled": "2022-01-31T03:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Machine Learning Gist</b> \u00b7 GitHub", "url": "https://gist.github.com/sgoyal1012/b30d70d12b6efad88bb285e8e709b161", "isFamilyFriendly": true, "displayUrl": "https://gist.github.com/sgoyal1012/b30d70d12b6efad88bb285e8e709b161", "snippet": "Week 3: <b>Sequence to sequence</b> architectures. <b>Sequence to sequence</b> models Language translation for example; Image captioning, caption an image; Picking the most likely model <b>Machine</b> Transation Model Split into a model encoding the sentence; and then a language model. Calculate the probability of an English sentence conditioned on a French sentence.", "dateLastCrawled": "2022-01-29T03:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Geometric deep <b>learning</b> on molecular representations | Nature <b>Machine</b> ...", "url": "https://www.nature.com/articles/s42256-021-00418-8", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s42256-021-00418-8", "snippet": "In <b>analogy</b> to some popular pre-deep <b>learning</b> ... which can be cast as a <b>sequence-to-sequence</b> translation <b>task</b> in which the string representations of the reactants are mapped to those of the ...", "dateLastCrawled": "2022-01-29T04:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Benefits of AI and Deep <b>Learning</b> - <b>Machine</b> <b>Learning</b> Company ...", "url": "https://www.folio3.ai/blog/advantages-of-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://www.folio3.ai/blog/<b>advantages-of-neural-networks</b>", "snippet": "<b>Sequence-To-Sequence</b> models are mainly applied in question answering, <b>machine</b> translations systems, and chatbots. What Are The <b>Advantages of Neural Networks</b> . There are various <b>advantages of neural networks</b>, some of which are discussed below: 1) Store information on the entire network. Just like it happens in traditional programming where information is stored on the network and not on a database. If a few pieces of information disappear from one place, it does not stop the whole network ...", "dateLastCrawled": "2022-02-02T09:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "deep-<b>learning</b>-coursera/Week 1 Quiz - Introduction to deep <b>learning</b>.md ...", "url": "https://github.com/Kulbear/deep-learning-coursera/blob/master/Neural%20Networks%20and%20Deep%20Learning/Week%201%20Quiz%20-%20Introduction%20to%20deep%20learning.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/Kulbear/deep-<b>learning</b>-coursera/blob/master/Neural Networks and Deep...", "snippet": "Week 1 Quiz - Introduction to deep <b>learning</b>. What does the <b>analogy</b> \u201cAI is the new electricity\u201d refer to? AI is powering personal devices in our homes and offices, similar to electricity. Through the \u201csmart grid\u201d, AI is delivering a new wave of electricity. AI runs on computers and is thus powered by electricity, but it is letting computers do things not possible before. Similar to electricity starting about 100 years ago, AI is transforming multiple industries. Note: Andrew ...", "dateLastCrawled": "2022-02-03T05:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Sequence Classification with LSTM Recurrent Neural Networks in Python ...", "url": "https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/sequence-classification-", "snippet": "Sequence Classification with LSTM Recurrent Neural Networks in Python with Keras. Sequence classification is a predictive modeling problem where you have some sequence of inputs over space or time and the <b>task</b> is to predict a category for the sequence. What makes this problem difficult is that the sequences can vary in length, be comprised of a ...", "dateLastCrawled": "2022-02-02T22:19:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(sequence-to-sequence task)  is like +(teaching a computer how to predict the next word in a sentence)", "+(sequence-to-sequence task) is similar to +(teaching a computer how to predict the next word in a sentence)", "+(sequence-to-sequence task) can be thought of as +(teaching a computer how to predict the next word in a sentence)", "+(sequence-to-sequence task) can be compared to +(teaching a computer how to predict the next word in a sentence)", "machine learning +(sequence-to-sequence task AND analogy)", "machine learning +(\"sequence-to-sequence task is like\")", "machine learning +(\"sequence-to-sequence task is similar\")", "machine learning +(\"just as sequence-to-sequence task\")", "machine learning +(\"sequence-to-sequence task can be thought of as\")", "machine learning +(\"sequence-to-sequence task can be compared to\")"]}