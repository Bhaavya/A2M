{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Understanding and Identifying Unfairness in Machine Learning | by ...", "url": "https://cannon-m-bray.medium.com/understanding-and-identifying-unfairness-in-machine-learning-80178a16357c", "isFamilyFriendly": true, "displayUrl": "https://cannon-m-bray.medium.com/understanding-and-identifying-unfairness-in-machine...", "snippet": "<b>Predictive</b> <b>Parity</b>. Next, let\u2019s look at <b>predictive</b> <b>parity</b>, also known as the outcome test. Unlike group fairness, this metric takes into account the predicted outcomes and the actual outcomes. According to <b>predictive</b> <b>parity</b>, a classifier is considered fair if each level of a protected class has the same PPV, also known as precision.", "dateLastCrawled": "2022-01-21T10:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "The accuracy, fairness, and limits of predicting recidivism", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5777393/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC5777393", "snippet": "Specifically, it is argued that the COMPAS score is not biased against blacks because the likelihood of recidivism among high-risk offenders is the same regardless of race (<b>predictive</b> <b>parity</b>), it can discriminate between recidivists and nonrecidivists equally well for white and black defendants as measured with the area under the curve of the receiver operating characteristic, AUC-ROC (accuracy equity), and the likelihood of recidivism for any given score is the same regardless of race ...", "dateLastCrawled": "2022-02-02T05:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Foundations of machine learning Fairness and machine learning", "url": "https://maxkasy.github.io/home/files/teaching/ML_Oxford_2022/fairness_slides.pdf", "isFamilyFriendly": true, "displayUrl": "https://maxkasy.github.io/home/files/teaching/ML_Oxford_2022/fairness_slides.pdf", "snippet": "This is called \u201c<b>predictive</b> <b>parity</b>\u201d in machine learning, the \u201chit rate test\u201d for \u201ctaste based discrimination\u201d in economics. \u201cFairness in machine learning\u201d literature: Constrained optimization. w()=argmax w() E[w(X)(m(X) c)] subject to p =0: 5/24. Fairness and D\u2019s objective Observation Suppose that W;M are binary (\u201cclassi\u2022cation\u201d), and that 1. m(X)=M (perfect predictability), and 2. w(x)=1(m(X)&gt;c) (unconstrained maximization of D\u2019s objective m). Then w(x) satis\u2022es ...", "dateLastCrawled": "2021-11-19T10:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Removing <b>Bias from Predictive Modeling</b> - Knowledge@Wharton", "url": "https://knowledge.wharton.upenn.edu/article/removing-bias-from-predictive-modeling/", "isFamilyFriendly": true, "displayUrl": "https://knowledge.wharton.upenn.edu/article/removing-<b>bias-from-predictive-modeling</b>", "snippet": "There is this notion called statistical <b>parity</b> or demographic <b>parity</b>, and that is the idea that you would have certain protected groups. Those protected variables might be things <b>like</b> sex or race ...", "dateLastCrawled": "2022-02-01T19:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Measuring and Mitigating Algorithmic Bias \u00b7 Notes", "url": "https://jsinkers.github.io/notes/notebooks/machine_learning/03_fairness.html", "isFamilyFriendly": true, "displayUrl": "https://jsinkers.github.io/notes/notebooks/machine_learning/03_fairness.html", "snippet": "i.e. classifier should make similar predictions for <b>humans</b> and martians with truly good credit scores; accounts for ground truth; same limitation as <b>predictive</b> <b>parity</b>; Criterion 4: Individual Fairness. rather than balancing by group (human, martian) compare individuals directly", "dateLastCrawled": "2022-01-28T09:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Why it&#39;s <b>fair | Predictive Hire</b> | Removing unconscious bias", "url": "https://www.predictivehire.com/hiring-fair/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>predictive</b>hire.com/hiring-fair", "snippet": "<b>Humans</b> are prone to unconscious bias. When a recruiter first screens a resume, they do so for ~6 seconds. So what is it that they are seeking? A job role can attract thousands of applicants. When recruiters initially screen applicants, they are looking for \u2018short-hand\u2019 clues to confirm a pre-existing judgement that \u2018predicts\u2019 success, <b>like</b> a particular degree or score. Without knowing it, this method adversely affects the ability to hire the best candidates. There is a better way ...", "dateLastCrawled": "2022-01-29T16:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "What is A.I.? Intro to Artificial Intelligence - Salesforce India", "url": "https://www.salesforce.com/in/products/einstein/ai-deep-dive/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.salesforce.com</b>/in/products/einstein/<b>ai-deep-dive</b>", "snippet": "Artificial Intelligence (AI) is the concept of having machines \u201cthink <b>like</b> <b>humans</b>\u201d \u2014 in other words, perform tasks <b>like</b> reasoning, planning, learning, and understanding language. While no one is expecting <b>parity</b> with human intelligence today or in the near future, AI has big implications in how we live our lives. The brains behind artificial intelligence is a technology called machine learning, which is designed to make our jobs easier and more productive.", "dateLastCrawled": "2022-02-01T13:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A Field Guide to Everything You Need To Know", "url": "https://www.salesforce.com/content/dam/web/en_in/www/documents/pdf/AI-for-CRM.pdf", "isFamilyFriendly": true, "displayUrl": "https://<b>www.salesforce.com</b>/content/dam/web/en_in/www/documents/pdf/AI-for-CRM.pdf", "snippet": "5 Artificial Intelligence (AI) is the concept of having machines \u201cthink <b>like</b> <b>humans</b>\u201d \u2014 in other words, perform tasks <b>like</b> reasoning, planning, learning, and understanding language. While no one is expecting <b>parity</b> with human intelligence today or in the near future, AI has big implications in how we live our lives.", "dateLastCrawled": "2022-01-29T10:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Recommended Reading Material <b>Predictive</b> Analytics - #CWBlog", "url": "https://conferenziaworld.com/blog/recommended-reading-material/", "isFamilyFriendly": true, "displayUrl": "https://conferenziaworld.com/blog/recommended-reading-material", "snippet": "Published on February 5, 2018 July 15, 2019 by Corporate <b>Parity</b>. Ahead of our Global <b>Predictive</b> Analytics Forum this week, we would <b>like</b> to share some reading material in the area of analytics &amp; business development. The books listed below cover a range of expertise from beginner right through to re-thinking your leadership style. \u201c<b>Humans</b> Need Not Apply\u201d \u2013 Jerry Kaplan (2015) In the past few years, the media has focused on publishing the most \u2018exciting\u2019 developments in AI ...", "dateLastCrawled": "2022-01-10T08:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Inspecting Algorithms for Bias</b> | <b>MIT Technology Review</b>", "url": "https://www.technologyreview.com/2017/06/12/105804/inspecting-algorithms-for-bias/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.technologyreview.com</b>/2017/06/12/105804/<b>inspecting-algorithms-for-bias</b>", "snippet": "<b>Inspecting Algorithms for Bias</b>. Courts, banks, and other institutions are using automated data analysis systems to make decisions about your life. Let\u2019s not leave it up to the algorithm makers ...", "dateLastCrawled": "2022-01-30T23:55:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "What is fair machine learning? Depends on your definition of fair ...", "url": "https://blogs.ischool.berkeley.edu/w231/2021/07/09/what-is-fair-machine-learning-depends-on-your-definition-of-fair/", "isFamilyFriendly": true, "displayUrl": "https://blogs.ischool.berkeley.edu/w231/2021/07/09/what-is-fair-machine-learning...", "snippet": "Another definition known as <b>predictive</b> <b>parity</b> would ensure <b>similar</b> fractions of correct acceptances from each gender category (i.e. if 40% of the accepted female applicants were true positives, a <b>similar</b> percentage of true positives should be observed among accepted applicants in each gender category).", "dateLastCrawled": "2021-12-27T02:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Machine learning systems</b>, fair or biased, reflect our moral standards ...", "url": "https://www.scmp.com/tech/innovation/article/3090961/machine-learning-systems-fair-or-biased-reflect-our-moral-standards", "isFamilyFriendly": true, "displayUrl": "https://<b>www.scmp.com</b>/tech/innovation/article/3090961/<b>machine-learning-systems</b>-fair-or...", "snippet": "<b>Predictive</b> (rate) <b>parity</b>: the fairness metric is met when the acceptance rate of a given classifier, e.g. race, is equivalent for each subgroup. This is also referred to as <b>predictive</b> <b>parity</b>. This ...", "dateLastCrawled": "2021-05-28T23:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "The accuracy, fairness, and limits of predicting recidivism", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5777393/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC5777393", "snippet": "Specifically, it is argued that the COMPAS score is not biased against blacks because the likelihood of recidivism among high-risk offenders is the same regardless of race (<b>predictive</b> <b>parity</b>), it can discriminate between recidivists and nonrecidivists equally well for white and black defendants as measured with the area under the curve of the receiver operating characteristic, AUC-ROC (accuracy equity), and the likelihood of recidivism for any given score is the same regardless of race ...", "dateLastCrawled": "2022-02-02T05:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Measuring and Mitigating Algorithmic Bias \u00b7 Notes", "url": "https://jsinkers.github.io/notes/notebooks/machine_learning/03_fairness.html", "isFamilyFriendly": true, "displayUrl": "https://jsinkers.github.io/notes/notebooks/machine_learning/03_fairness.html", "snippet": "Criterion 2: <b>Predictive</b> <b>Parity</b>. all groups shall have same PPV (precision): i.e. probability predicted positive is truly positive; for classifier, this means we want: \\[P(Y=1|\\hat Y=1,A=m) = P(Y=1|\\hat Y = 1, A=h)\\] chance to correctly get positive credit score should be the same for both human and martian applicants; now ground truth is taken into account; subtle limitation: assumes ground truth is fair if ground truth is unfair in dataset, this impacts the predictions we make; e.g. <b>humans</b> ...", "dateLastCrawled": "2022-01-28T09:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Testing for Algorithmic Bias | Watt AI", "url": "https://watt-ai.github.io/blog/Testing_for_Algorithmic_Bias", "isFamilyFriendly": true, "displayUrl": "https://watt-ai.github.io/blog/Testing_for_Algorithmic_Bias", "snippet": "<b>Predictive</b> <b>Parity</b>: A classifier achieves <b>predictive</b> <b>parity</b> if both protected and unprotected groups have equal Precision \u2013 the probability of a subject with positive <b>predictive</b> value to truly belong to the positive class. Take the example of an algorithm that predicts whether loan applicants have a good or bad credit score. The tool would have <b>predictive</b> <b>parity</b> if for both male and female applicants, the probability of an applicant with a good predicted credit score to actually have a good ...", "dateLastCrawled": "2021-10-24T20:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Human Parity</b> - DES-SHOW", "url": "https://www.des-show.com/human-parity/", "isFamilyFriendly": true, "displayUrl": "https://www.des-show.com/<b>human-parity</b>", "snippet": "The rate of times that team of <b>humans</b> fails to recognise a phrase from other <b>humans</b> is higher than we would have if we put an Artificial Intelligence to recognise them. <b>Human parity</b> in voice recognition in English was already surpassed in 2017. <b>Similar</b> exercises have been going on over the past three years. It was as early as 2015 that ...", "dateLastCrawled": "2022-01-31T17:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Foundations of machine learning Fairness and machine learning", "url": "https://maxkasy.github.io/home/files/teaching/ML_Oxford_2022/fairness_slides.pdf", "isFamilyFriendly": true, "displayUrl": "https://maxkasy.github.io/home/files/teaching/ML_Oxford_2022/fairness_slides.pdf", "snippet": "This is called \u201c<b>predictive</b> <b>parity</b>\u201d in machine learning, the \u201chit rate test\u201d for \u201ctaste based discrimination\u201d in economics. \u201cFairness in machine learning\u201d literature: Constrained optimization. w()=argmax w() E[w(X)(m(X) c)] subject to p =0: 5/24. Fairness and D\u2019s objective Observation Suppose that W;M are binary (\u201cclassi\u2022cation\u201d), and that 1. m(X)=M (perfect predictability), and 2. w(x)=1(m(X)&gt;c) (unconstrained maximization of D\u2019s objective m). Then w(x) satis\u2022es ...", "dateLastCrawled": "2021-11-19T10:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Humans</b> Against the Machines: Is <b>Predictive</b> Coding Really Better Than ...", "url": "https://www.natlawreview.com/article/humans-against-machines-predictive-coding-really-better-humans-part-1", "isFamilyFriendly": true, "displayUrl": "https://<b>www.natlawreview.com</b>/article/<b>humans</b>-against-machines-<b>predictive</b>-coding-really...", "snippet": "<b>Predictive</b> coding is a type of technology-assisted review that employs algorithms to help classify documents (based on relevancy to the subject matter at hand, or screening for privileged attorney ...", "dateLastCrawled": "2022-02-02T19:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Why it&#39;s <b>fair | Predictive Hire</b> | Removing unconscious bias", "url": "https://www.predictivehire.com/hiring-fair/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>predictive</b>hire.com/hiring-fair", "snippet": "<b>Humans</b> are prone to unconscious bias. When a recruiter first screens a resume, they do so for ~6 seconds. So what is it that they are seeking? A job role can attract thousands of applicants. When recruiters initially screen applicants, they are looking for \u2018short-hand\u2019 clues to confirm a pre-existing judgement that \u2018predicts\u2019 success, like a particular degree or score. Without knowing it, this method adversely affects the ability to hire the best candidates. There is a better way ...", "dateLastCrawled": "2022-01-29T16:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Inspecting Algorithms for Bias</b> | <b>MIT Technology Review</b>", "url": "https://www.technologyreview.com/2017/06/12/105804/inspecting-algorithms-for-bias/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.technologyreview.com</b>/2017/06/12/105804/<b>inspecting-algorithms-for-bias</b>", "snippet": "In part because the recidivism rates for blacks and whites do in fact differ, it is mathematically likely that the positive <b>predictive</b> values for people in each group will be <b>similar</b> while the ...", "dateLastCrawled": "2022-01-30T23:55:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Removing <b>Bias from Predictive Modeling</b> - Knowledge@Wharton", "url": "https://knowledge.wharton.upenn.edu/article/removing-bias-from-predictive-modeling/", "isFamilyFriendly": true, "displayUrl": "https://knowledge.wharton.upenn.edu/article/removing-<b>bias-from-predictive-modeling</b>", "snippet": "We <b>thought</b> a pretty good selling point of this is that overall the predictions are still quite good. Well, they are as good as they <b>can</b> be on the data. But we are able to accomplish this ...", "dateLastCrawled": "2022-02-01T19:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Advancing health equity with artificial intelligence", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8607970/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8607970", "snippet": "<b>Predictive</b> <b>parity</b>: No difference in positive <b>predictive</b> value rates across all groups: Demographic <b>parity</b>: No difference in positive outcome rates across all groups : Validation (AI lifecycle) Evaluation of model performance prior to formal implementation: Interpretability: The degree to which the decision process of AI is understandable to <b>humans</b>: Continuously learning AI: AI that <b>can</b> update in real-time to learn from incoming data: Development. Approaches to bias mitigation in the ...", "dateLastCrawled": "2021-12-23T22:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Understanding and Identifying Unfairness in Machine Learning | by ...", "url": "https://cannon-m-bray.medium.com/understanding-and-identifying-unfairness-in-machine-learning-80178a16357c", "isFamilyFriendly": true, "displayUrl": "https://<b>can</b>non-m-bray.medium.com/understanding-and-identifying-unfairness-in-machine...", "snippet": "<b>Predictive</b> <b>Parity</b>. Next, let\u2019s look at <b>predictive</b> <b>parity</b>, also known as the outcome test. Unlike group fairness, this metric takes into account the predicted outcomes and the actual outcomes. According to <b>predictive</b> <b>parity</b>, a classifier is considered fair if each level of a protected class has the same PPV, also known as precision.", "dateLastCrawled": "2022-01-21T10:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "The limits of human predictions of recidivism", "url": "https://www.science.org/doi/10.1126/sciadv.aaz0652", "isFamilyFriendly": true, "displayUrl": "https://www.science.org/doi/10.1126/sciadv.aaz0652", "snippet": "Second, our results suggest that people <b>can</b> predict recidivism as well as statistical models if only a few simple <b>predictive</b> factors are specified as inputs, as was the case in Dressel and Farid\u2019s study. In this context of streamlined inputs, the accuracy of models and <b>humans</b> (without feedback) was largely interchangeable. In contrast, when inputs were enriched with additional <b>predictive</b> factors, models outperformed human judgment. This was not because the additional risk information ...", "dateLastCrawled": "2022-02-03T00:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Human <b>creativity, evolutionary algorithms, and predictive</b> ...", "url": "https://link.springer.com/article/10.3758/s13423-014-0743-x", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.3758/s13423-014-0743-x", "snippet": "Any <b>predictive</b> processing, therefore, <b>can</b> only proceed on a step by step basis, and by using the same operator as the real system (Downing, 2009). The explicit system, on the other hand, <b>can</b> abstract information from context. In so doing, simulation increases generalizability and cognitive flexibility in the imagined future. Naturally, there is a tradeoff. Given the added transformative detail it contains, emulation might be more accurate and powerful (Pezzulo et al.,", "dateLastCrawled": "2021-12-09T14:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "How to <b>Recognize and Change Human &amp; Machine Discrimination</b>", "url": "https://citizensandtech.org/2020/02/recognize-and-change-human-machine-discrimination/", "isFamilyFriendly": true, "displayUrl": "https://citizensandtech.org/2020/02/<b>recognize-and-change-human-machine-discrimination</b>", "snippet": "In in 2010, Sendhil would have <b>thought</b> that (a) <b>humans</b> are biased, and (b) the more rules we put in place, the more we <b>can</b> protect ourselves from ourselves. Algorithms would have seemed like a great way to solve this problem. Next, Sendhil tells us about a recent projecy with Obermeyer Powers and Vogeli to study health systems designed to help potentially \u201cheavy users\u201d with chronic conditions. To provide this program, healthcare providers needed to find out who the heavy users would be ...", "dateLastCrawled": "2021-12-26T18:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Humans</b> Against the Machines: Is <b>Predictive</b> Coding Really Better Than ...", "url": "https://www.natlawreview.com/article/humans-against-machines-predictive-coding-really-better-humans-part-1", "isFamilyFriendly": true, "displayUrl": "https://<b>www.natlawreview.com</b>/article/<b>humans</b>-against-machines-<b>predictive</b>-coding-really...", "snippet": "<b>Predictive</b> coding is a type of technology-assisted review that employs algorithms to help classify documents (based on relevancy to the subject matter at hand, or screening for privileged attorney ...", "dateLastCrawled": "2022-02-02T19:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Reprogramming Fairness: Affirmative Action in Algorithmic Criminal</b> ...", "url": "http://hrlr.law.columbia.edu/hrlr-online/reprogramming-fairness-affirmative-action-in-algorithmic-criminal-sentencing/", "isFamilyFriendly": true, "displayUrl": "hrlr.law.columbia.edu/hrlr-online/<b>reprogramming-fairness-affirmative-action-in</b>...", "snippet": "<b>Predictive</b> accuracy is certainly an important value for both legislatures and courts. Employing algorithmic risk assessments that satisfy <b>predictive</b> <b>parity</b> <b>can</b> adequately fulfill our commitment to that value. And yet, the founding ideals of this nation remind us of the moral imperative to equalize the odds in our criminal justice system.", "dateLastCrawled": "2022-02-01T07:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Mental Health <b>Parity</b> - An Environmental Scan", "url": "https://www.slideshare.net/kapaligners/mental-health-parity-an-environmental-scan", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/kapaligners/mental-health-<b>parity</b>-an-environmental-s<b>can</b>", "snippet": "The three departments responsible for <b>parity</b> <b>can</b> act as an arbiter to enhance public/private partnerships, convene a summit of stakeholder groups, provide education and advocacy for consumers and their families, provide direct oversight, and enforce accountability. The Federal government <b>can</b> also assist stakeholders by continuing to involve behavioral health in the health care reform process, maintaining open communication. Other strategic considerations our respondents noted include the ...", "dateLastCrawled": "2022-01-31T10:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Parity</b> - <b>An Environmental Scan- AHP Behavioral Health</b> Series White Pa\u2026", "url": "https://www.slideshare.net/kalexandrei/parity-an-environmental-scan-ahp-behavioral-health-series-white-paper-1", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/kalexandrei/<b>parity</b>-<b>an-environmental-scan-ahp-behavioral</b>...", "snippet": "<b>Thought</b> leaders from the public sector included Single State Authorities, leaders in professional organizations, advocates for and leaders of consumer groups. Their responses reflect national, regional and state-level perspectives. Each interview lasted between 30-45 minutes. The format was composed of open-ended questions regarding the scope of implementation, resources required in the behavioral health system, perceived impacts on consumers, and the major challenges to implementing <b>parity</b> ...", "dateLastCrawled": "2022-01-18T08:32:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "The accuracy, fairness, and limits of predicting recidivism", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5777393/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC5777393", "snippet": "Specifically, it is argued that the COMPAS score is not biased against blacks because the likelihood of recidivism among high-risk offenders is the same regardless of race (<b>predictive</b> <b>parity</b>), it <b>can</b> discriminate between recidivists and nonrecidivists equally well for white and black defendants as measured with the area under the curve of the receiver operating characteristic, AUC-ROC (accuracy equity), and the likelihood of recidivism for any given score is the same regardless of race ...", "dateLastCrawled": "2022-02-02T05:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Understanding and Identifying Unfairness in Machine Learning | by ...", "url": "https://cannon-m-bray.medium.com/understanding-and-identifying-unfairness-in-machine-learning-80178a16357c", "isFamilyFriendly": true, "displayUrl": "https://<b>can</b>non-m-bray.medium.com/understanding-and-identifying-unfairness-in-machine...", "snippet": "<b>Predictive</b> <b>Parity</b>. Next, let\u2019s look at <b>predictive</b> <b>parity</b>, also known as the outcome test. Unlike group fairness, this metric takes into account the predicted outcomes and the actual outcomes. According to <b>predictive</b> <b>parity</b>, a classifier is considered fair if each level of a protected class has the same PPV, also known as precision.", "dateLastCrawled": "2022-01-21T10:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Beyond the first trimester screen: <b>can</b> we predict who will choose ...", "url": "https://pubmed.ncbi.nlm.nih.gov/21415760/", "isFamilyFriendly": true, "displayUrl": "https://pubmed.ncbi.nlm.nih.gov/21415760", "snippet": "Purpose: The purpose of this study is to determine what factors, in addition to a positive first trimester aneuploidy screen, correlate with a pregnant patient&#39;s decision to undergo invasive prenatal testing. Methods: We conducted a retrospective cohort study of singleton pregnancies referred to the Johns Hopkins Prenatal Diagnosis and Treatment Center between 2001 and 2009 with an indication of positive first trimester screen. We <b>compared</b> demographic factors and numerical first trimester ...", "dateLastCrawled": "2021-09-10T07:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Can</b> We <b>Automate Fairness? Professor Alexandra Chouldechova on Machine</b> ...", "url": "https://www.heinz.cmu.edu/media/2017/january/automate-fairness-machine-learning-discrimination", "isFamilyFriendly": true, "displayUrl": "https://www.heinz.cmu.edu/media/2017/january/automate-fairness-machine-learning...", "snippet": "Chouldechova says risk assessments are traditionally held to the same standards of bias as psychological and educational tests, something called \u201c<b>predictive</b> <b>parity</b>.\u201d While that may seem reasonable, she says it\u2019s not adequate in all contexts. Following the ProPublica-Northpointe feud, Chouldechova performed her own analysis, which she presented at the Fairness, Accountability, and Transparency in Machine Learning", "dateLastCrawled": "2021-12-21T01:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) <b>Predictive</b> adaptive responses in perspective", "url": "https://www.researchgate.net/publication/5522982_Predictive_adaptive_responses_in_perspective", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/5522982_<b>Predictive</b>_adaptive_responses_in...", "snippet": "<b>humans</b> <b>can</b> contribute to later disease if there is a dis- <b>parity</b> (\u2018mismatch\u2019) between nutritional experience at different phases of life [2] , but he revives a debate [3,4]", "dateLastCrawled": "2022-01-30T01:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Towards Automatic Bias Detection in Knowledge Graphs", "url": "https://aclanthology.org/2021.findings-emnlp.321.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/2021.findings-emnlp.321.pdf", "snippet": "Hrelated <b>to humans</b>, towards which we want to detect any biases. For <b>Predictive</b> and Demographic <b>Parity</b>, we also assume a classi\ufb01cation task and a classi\ufb01er which takes en- tity embeddings as input, together with a relation and predicts the corresponding tails. Lastly, we re-fer sas a possibly sensitive relation in the following de\ufb01nitions, and take it as binary for simplicity. Demographic <b>Parity</b> A classi\ufb01er satis\ufb01es demo-graphic <b>parity</b> with respect to a sensitive relation s, if the ...", "dateLastCrawled": "2022-01-16T13:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "How to <b>Recognize and Change Human &amp; Machine Discrimination</b>", "url": "https://citizensandtech.org/2020/02/recognize-and-change-human-machine-discrimination/", "isFamilyFriendly": true, "displayUrl": "https://citizensandtech.org/2020/02/<b>recognize-and-change-human-machine-discrimination</b>", "snippet": "If your goal is to improve the lives of disadvantaged people, how should he think about algorithms <b>compared</b> <b>to humans</b>? If a firm comes to Sendhil asking if they should use an algorithm to review resumes? Could he tell them to go back to biased <b>humans</b>? To answer the question, Sendhil says, we need to find ways to compare human bias to algorithmic bias. He suggests three ways to compare them: First, how <b>can</b> we detect bias? With <b>humans</b>, we typically have to set up audit studies that gives us ...", "dateLastCrawled": "2021-12-26T18:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Reprogramming Fairness: Affirmative Action in Algorithmic Criminal</b> ...", "url": "http://hrlr.law.columbia.edu/hrlr-online/reprogramming-fairness-affirmative-action-in-algorithmic-criminal-sentencing/", "isFamilyFriendly": true, "displayUrl": "hrlr.law.columbia.edu/hrlr-online/<b>reprogramming-fairness-affirmative-action-in</b>...", "snippet": "<b>Predictive</b> accuracy is certainly an important value for both legislatures and courts. Employing algorithmic risk assessments that satisfy <b>predictive</b> <b>parity</b> <b>can</b> adequately fulfill our commitment to that value. And yet, the founding ideals of this nation remind us of the moral imperative to equalize the odds in our criminal justice system.", "dateLastCrawled": "2022-02-01T07:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Machine learning fairness notions: Bridging the</b> gap with real-world ...", "url": "https://www.sciencedirect.com/science/article/pii/S0306457321001321", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0306457321001321", "snippet": "For completeness, we define such notion and give it the name negative <b>predictive</b> <b>parity</b>. Definition 5.1. Negative <b>predictive</b> <b>parity</b> holds iff all sub-groups have the same N P V = T N F N + T N: (8) P (Y = 1 \u2223 Y \u02c6 = 0, A = 0) = P (Y = 1 \u2223 Y \u02c6 = 0, A = 1) The following proposition states formally the relationship between conditional use ...", "dateLastCrawled": "2022-02-03T07:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "The accuracy, fairness, and limits of predicting recidivism", "url": "https://www.science.org/doi/pdf/10.1126/sciadv.aao5580", "isFamilyFriendly": true, "displayUrl": "https://www.science.org/doi/pdf/10.1126/sciadv.aao5580", "snippet": "one subset of the defendants, the median accuracies of each subset <b>can</b> reasonably be assumed to be independent. Therefore, the participant performance on the 20 subsets <b>can</b> be directly <b>compared</b> to the COMPAS performance on the same 20 subsets. A one-sided t test re-veals that the average of the 20 medi an participant accuracies of 62.8%", "dateLastCrawled": "2022-01-29T23:07:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine learning</b> and bias \u2013 IBM Developer", "url": "https://developer.ibm.com/articles/machine-learning-and-bias/", "isFamilyFriendly": true, "displayUrl": "https://developer.ibm.com/articles/<b>machine-learning</b>-and-bias", "snippet": "<b>Machine learning</b> has shown great promise in powering self-driving cars, accurately recognizing cancer in radiographs, and predicting our interests based upon past behavior (to name just a few). But with the benefits from <b>machine learning</b>, there are also challenges. One key challenge is the presence of bias in the classifications and predictions of <b>machine learning</b>. These biases are not benign. They have consequences based upon the decisions resulting from a <b>machine learning</b> model. Therefore ...", "dateLastCrawled": "2022-02-02T20:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "RStudio AI Blog: Starting to think about AI Fairness", "url": "https://blogs.rstudio.com/ai/posts/2021-07-15-ai-fairness/", "isFamilyFriendly": true, "displayUrl": "https://blogs.rstudio.com/ai/posts/2021-07-15-ai-fairness", "snippet": "Heidari et al. have written a paper comparing the three criteria \u2013 demographic <b>parity</b>, equality of opportunity, and <b>predictive</b> <b>parity</b> \u2013 to egalitarianism, equality of opportunity (EOP) in the Rawlsian sense, and EOP seen through the glass of luck egalitarianism, respectively. While the <b>analogy</b> is fascinating, it too assumes that we may take what is in the data at face value. In their likening <b>predictive</b> <b>parity</b> to luck egalitarianism, they have to go to especially great lengths, in ...", "dateLastCrawled": "2022-01-31T09:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Explaining <b>Machine</b> <b>Learning</b> Decisions | Philosophy of Science ...", "url": "https://www.cambridge.org/core/journals/philosophy-of-science/article/explaining-machine-learning-decisions/8E20E695A0ADBB2DEC78D0568B78CDF5", "isFamilyFriendly": true, "displayUrl": "https://www.cambridge.org/core/journals/philosophy-of-science/article/explaining...", "snippet": "<b>Machine</b> <b>learning</b> is a form of data processing that identifies statistical patterns from large quantities of information. Instead of being programmed with predetermined responses to a set of conditions\u2014the dominant approach to AI up until fairly recently\u2014an ML system is set up to \u201clearn\u201d its own suitable responses to those conditions under a training regime. Many tasks for which no straightforward sequence of \u201cif-then\u201d rules can be formulated may be handled more efficiently, and ...", "dateLastCrawled": "2022-01-31T14:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Starting to think about AI Fairness - Adolfo Eliaz\u00e0t - Artificial ...", "url": "https://adolfoeliazat.com/2022/01/01/starting-to-think-about-ai-fairness-2/", "isFamilyFriendly": true, "displayUrl": "https://adolfoeliazat.com/2022/01/01/starting-to-think-about-ai-fairness-2", "snippet": "() have written a paper comparing the three criteria \u2013 demographic <b>parity</b>, equality of opportunity, and <b>predictive</b> <b>parity</b> \u2013 to egalitarianism, equality of opportunity (EOP) in the Rawlsian sense, and EOP seen through the glass of luck egalitarianism, respectively. While the <b>analogy</b> is fascinating, it too assumes that we may take what is in the data at face value. In their likening <b>predictive</b> <b>parity</b> to luck egalitarianism, they have to go to especially great lengths, in assuming that the", "dateLastCrawled": "2022-02-01T05:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Classification - Fairness and <b>machine</b> <b>learning</b>", "url": "https://fairmlbook.org/classification.html", "isFamilyFriendly": true, "displayUrl": "https://fairmlbook.org/classification.html", "snippet": "Simply put, the goal of classification is to determine a plausible value for an unknown variable Y given an observed variable X.For example, we might try to predict whether a loan applicant will pay back her loan by looking at various characteristics such as credit history, income, and net worth. Classification also applies in situations where the variable Y does not refer to an event that lies in the future. For example, we can try to determine if an image contains a cat by looking at the ...", "dateLastCrawled": "2022-02-03T07:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "The <b>measure and mismeasure of fairness: a critical review</b> of fair ...", "url": "https://blog.acolyer.org/2020/02/03/measure-mismeasure-fairness/", "isFamilyFriendly": true, "displayUrl": "https://blog.acolyer.org/2020/02/03/measure-mismeasure-fairness", "snippet": "In case you\u2019re wondering where on earth I\u2019m going with this\u2026 it\u2019s a very stretched <b>analogy</b> I\u2019ve been playing with in my mind. One premise of many models of fairness in <b>machine</b> <b>learning</b> is that you can measure (\u2018prove\u2019) fairness of a <b>machine</b> <b>learning</b> model from within the system \u2013 i.e. from properties of the model itself and perhaps the data it is trained on. Beyond the questions of whether any one model of fairness is better or worse than another, I\u2019m coming to the ...", "dateLastCrawled": "2022-01-30T11:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Predicting phosphorescence energies and inferring wavefunction ...", "url": "https://pubs.rsc.org/en/content/articlelanding/2021/sc/d1sc02136b#!", "isFamilyFriendly": true, "displayUrl": "https://pubs.rsc.org/en/content/articlelanding/2021/sc/d1sc02136b#!", "snippet": "Phosphorescence is commonly utilized for applications including light-emitting diodes and photovoltaics. <b>Machine</b> <b>learning</b> (ML) approaches trained on ab initio datasets of singlet\u2013triplet energy gaps may expedite the discovery of phosphorescent compounds with the desired emission energies. However, we show that standard ML approaches for modeling potential energy surfaces inaccurately predict singlet\u2013triplet energy gaps due to the failure to account for spatial localities of spin transitions.", "dateLastCrawled": "2022-01-31T06:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Mitigating Unwanted Biases with Adversarial Learning</b>", "url": "https://www.aies-conference.com/2018/contents/papers/main/AIES_2018_paper_162.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.aies-conference.com/2018/contents/papers/main/AIES_2018_paper_162.pdf", "snippet": "<b>Machine</b> <b>learning</b> is a tool for building models that accurately represent input training data. When undesired biases concern- ing demographic groups are in the training data, well-trained models will re\ufb02ect those biases. We present a framework for mitigating such biases by including a variable for the group of interest and simultaneously <b>learning</b> a predictor and an ad-versary. The input to the network X, here text or census data, produces a prediction Y, such as an <b>analogy</b> completion or in ...", "dateLastCrawled": "2021-12-17T22:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "On the integration of molecular dynamics, data science, and experiments ...", "url": "https://www.sciencedirect.com/science/article/pii/S2211339822000065", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2211339822000065", "snippet": "<b>Machine</b> <b>learning</b> (ML) models have been used for efficiently identifying ternary platinum alloys ... and the slope of a <b>parity</b> line between predicted and experimentally measured values. When trained on mixtures of water and dioxane, a linear regression model was accurate, obtaining an RMSE of 0.23 and <b>parity</b> line slope of 0.89; however, this model was not as accurate when including mixtures with other cosolvents (gamma-valerolactone or tetrahydrofuran), obtaining an RMSE of 0.58 and ...", "dateLastCrawled": "2022-02-03T03:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>All Unit MCQ questions of ML</b> \u2013 TheCodingShef", "url": "https://thecodingshef.com/all-unit-mcq-questions-of-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://thecodingshef.com/<b>all-unit-mcq-questions-of</b>-<b>machine</b>-<b>learning</b>", "snippet": "Correct option is C. Choose the correct option regarding <b>machine</b> <b>learning</b> (ML) and artificial intelligence (AI) ML is a set of techniques that turns a dataset into a software. AI is a software that can emulate the human mind. ML is an alternate way of programming intelligent machines. All of the above. Correct option is D.", "dateLastCrawled": "2022-01-30T22:09:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(predictive parity)  is like +(humans)", "+(predictive parity) is similar to +(humans)", "+(predictive parity) can be thought of as +(humans)", "+(predictive parity) can be compared to +(humans)", "machine learning +(predictive parity AND analogy)", "machine learning +(\"predictive parity is like\")", "machine learning +(\"predictive parity is similar\")", "machine learning +(\"just as predictive parity\")", "machine learning +(\"predictive parity can be thought of as\")", "machine learning +(\"predictive parity can be compared to\")"]}