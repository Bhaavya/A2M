{"src_spec_res": [[], [], [], []], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Batch, <b>Mini Batch</b> &amp; <b>Stochastic</b> <b>Gradient Descent</b> | by Sushant Patrikar ...", "url": "https://towardsdatascience.com/batch-mini-batch-stochastic-gradient-descent-7a62ecba642a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/batch-<b>mini-batch</b>-<b>stochastic</b>-<b>gradient-descent</b>-7a62ecba642a", "snippet": "<b>Mini Batch</b> <b>Gradient Descent</b>. We have seen the Batch <b>Gradient Descent</b>. We have also seen the <b>Stochastic</b> <b>Gradient Descent</b>. Batch <b>Gradient Descent</b> can be used for smoother curves. SGD can be used when the dataset is large. Batch <b>Gradient Descent</b> converges directly to minima. SGD converges faster for larger datasets. But, since in SGD we use only ...", "dateLastCrawled": "2022-02-02T11:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Batch vs <b>Mini-batch</b> vs <b>Stochastic Gradient Descent</b> with Code Examples ...", "url": "https://medium.datadriveninvestor.com/batch-vs-mini-batch-vs-stochastic-gradient-descent-with-code-examples-cd8232174e14", "isFamilyFriendly": true, "displayUrl": "https://medium.datadriveninvestor.com/batch-vs-<b>mini-batch</b>-vs-<b>stochastic</b>-<b>gradient</b>...", "snippet": "It is possible to use only the <b>Mini-batch</b> <b>Gradient Descent</b> code to implement all versions of <b>Gradient Descent</b>, you just need to set the <b>mini_batch</b>_size equals one to <b>Stochastic</b> GD or the number of training examples to Batch GD. Thus, the main difference between Batch, <b>Mini-batch</b>, and <b>Stochastic Gradient Descent</b> is the number of examples used for each epoch and the time and effort necessary to reach the global minimum value of the Cost Function.", "dateLastCrawled": "2022-01-27T13:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Batch vs <b>Mini-batch vs Stochastic Gradient Descent</b> with Code Examples", "url": "https://edgeaiguru.com/Batch-vs-Mini-batch-vs-Stochastic-Gradient-Descent", "isFamilyFriendly": true, "displayUrl": "https://edgeaiguru.com/Batch-vs-<b>Mini-batch-vs-Stochastic-Gradient-Descent</b>", "snippet": "It is possible to use only the <b>Mini-batch</b> <b>Gradient</b> <b>Descent</b> code to implement all versions of <b>Gradient</b> <b>Descent</b>, you just need to set the <b>mini_batch</b>_size equals one to <b>Stochastic</b> GD or to the number of training examples to Batch GD. Thus, the main difference between Batch, <b>Mini-batch</b> and <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> is the number of examples used for each epoch and the time and effort necessary to reach the global minimum value of the Cost Function.", "dateLastCrawled": "2022-01-27T23:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "13.6 <b>Stochastic and mini-batch gradient descent</b>", "url": "https://kenndanielso.github.io/mlrefined/blog_posts/13_Multilayer_perceptrons/13_6_Stochastic_and_minibatch_gradient_descent.html", "isFamilyFriendly": true, "displayUrl": "https://kenndanielso.github.io/.../13_6_<b>Stochastic_and_minibatch_gradient_descent</b>.html", "snippet": "It is noteworthy that because moderately accurate solutions (provided by a moderate amount of minimization of a cost function) tend to perform reasonably well in <b>machine</b> <b>learning</b> applications, and because with large datasets a random initialization will tend to lie far from a convergent point, in many cases even a single iteration of <b>stochastic</b>/<b>mini-batch</b> <b>gradient</b> <b>descent</b> can provide a good solution.", "dateLastCrawled": "2022-02-02T00:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Gradient</b> <b>Descent</b>: A Quick, Simple Introduction | Built In", "url": "https://builtin.com/data-science/gradient-descent", "isFamilyFriendly": true, "displayUrl": "https://builtin.com/data-science/<b>gradient</b>-<b>descent</b>", "snippet": "Types of <b>gradient</b> <b>descent</b>: batch, <b>stochastic</b>, <b>mini-batch</b>; Introduction to <b>Gradient</b> <b>Descent</b>. <b>Gradient</b> <b>descent</b> is an optimization algorithm that&#39;s used when training a <b>machine</b> <b>learning</b> model. It&#39;s based on a convex function and tweaks its parameters iteratively to minimize a given function to its local minimum. What is <b>Gradient</b> <b>Descent</b>? <b>Gradient</b> <b>Descent</b> is an optimization algorithm for finding a local minimum of a differentiable function. <b>Gradient</b> <b>descent</b> is simply used in <b>machine</b> <b>learning</b> to ...", "dateLastCrawled": "2022-02-02T07:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Understanding the 3 Primary <b>Types of Gradient Descent</b> | by ODSC - Open ...", "url": "https://medium.com/odscjournal/understanding-the-3-primary-types-of-gradient-descent-987590b2c36", "isFamilyFriendly": true, "displayUrl": "https://medium.com/odscjournal/understanding-the-3-primary-<b>types-of-gradient-descent</b>...", "snippet": "<b>Mini Batch</b> <b>Gradient Descent</b> is commonly used for deep <b>learning</b> problems. Conclusion This article should give you the basic motivation for the <b>gradient descent</b> process in <b>machine</b> <b>learning</b>.", "dateLastCrawled": "2022-02-03T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Stochastic Gradient Descent</b> \u2014 Clearly Explained !! | by Aishwarya V ...", "url": "https://towardsdatascience.com/stochastic-gradient-descent-clearly-explained-53d239905d31", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>stochastic-gradient-descent</b>-clearly-explained-53d239905d31", "snippet": "<b>Stochastic gradient descent</b> is a very popular and common algorithm used in various <b>Machine</b> <b>Learning</b> algorithms, most importantly forms the basis of Neural Networks. In this article, I have tried my\u2026 Get started. Open in app. Sign in. Get started. Follow. 618K Followers \u00b7 Editors&#39; Picks Features Deep Dives Grow Contribute. About. Get started. Open in app. <b>Stochastic Gradient Descent</b> \u2014 Clearly Explained !! Aishwarya V Srinivasan. Sep 7, 2019 \u00b7 4 min read. <b>Stochastic gradient descent</b> is a ...", "dateLastCrawled": "2022-02-02T12:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Gradient Descent in Logistic Regression [Explained for Beginners</b> ...", "url": "https://www.upgrad.com/blog/gradient-descent-in-logistic-regression/", "isFamilyFriendly": true, "displayUrl": "https://www.upgrad.com/blog/<b>gradient-descent-in-logistic-regression</b>", "snippet": "<b>Mini-Batch</b> <b>Gradient</b> <b>Descent</b> Algorithm. <b>Mini-Batch</b> <b>Gradient</b> <b>Descent</b> is another slight modification of the <b>Gradient</b> <b>Descent</b> Algorithm. It is somewhat in between Normal <b>Gradient</b> <b>Descent</b> and <b>Stochastic</b> <b>Gradient</b> <b>Descent</b>. <b>Mini-Batch</b> <b>Gradient</b> <b>Descent</b> is just taking a smaller batch of the entire dataset, and then minimizing the loss on it. This process is more efficient than both the above two <b>Gradient</b> <b>Descent</b> Algorithms. Now the batch size can be of-course anything you want. But researchers have ...", "dateLastCrawled": "2022-01-28T16:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Introduction to Machine Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/24/introduction-to-machine-learning-2/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/24/<b>introduction-to-machine-learning</b>-2", "snippet": "<b>Mini batch</b> <b>gradient</b> <b>descent</b> (about 30 training observations or more for each and every iteration): This is a trade-off between huge computational costs and a quick method of updating weights. In this method, at each iteration, about 30 observations will be selected at random and gradients calculated to update the model weights. Here, a question many can ask is, why the minimum 30 and not any other number? If we look into statistical basics, 30 observations required to be considering in order ...", "dateLastCrawled": "2022-01-17T05:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How To <b>Implement Logistic Regression</b> From Scratch in Python", "url": "https://machinelearningmastery.com/implement-logistic-regression-stochastic-gradient-descent-scratch-python/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>implement-logistic-regression</b>-<b>stochastic</b>-<b>gradient</b>...", "snippet": "Batch <b>Stochastic</b> <b>Gradient</b> <b>Descent</b>. Change the <b>stochastic</b> <b>gradient</b> <b>descent</b> algorithm to accumulate updates across each epoch and only update the coefficients in a batch at the end of the epoch. Additional Classification Problems. Apply the technique to other binary (2 class) classification problems on the UCI <b>machine</b> <b>learning</b> repository. Did you explore any of these extensions? Let me know about it in the comments below. Review. In this tutorial, you discovered how to implement logistic ...", "dateLastCrawled": "2022-02-02T07:08:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(mini-batch stochastic gradient descent)  is like +(bank robber choosing a teller window)", "+(mini-batch stochastic gradient descent) is similar to +(bank robber choosing a teller window)", "+(mini-batch stochastic gradient descent) can be thought of as +(bank robber choosing a teller window)", "+(mini-batch stochastic gradient descent) can be compared to +(bank robber choosing a teller window)", "machine learning +(mini-batch stochastic gradient descent AND analogy)", "machine learning +(\"mini-batch stochastic gradient descent is like\")", "machine learning +(\"mini-batch stochastic gradient descent is similar\")", "machine learning +(\"just as mini-batch stochastic gradient descent\")", "machine learning +(\"mini-batch stochastic gradient descent can be thought of as\")", "machine learning +(\"mini-batch stochastic gradient descent can be compared to\")"]}