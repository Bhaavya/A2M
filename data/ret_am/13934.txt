{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "How to easily do Handwriting Recognition using Deep Learning", "url": "https://nanonets.com/blog/handwritten-character-recognition/", "isFamilyFriendly": true, "displayUrl": "https://nanonets.com/blog/handwritten-character-recognition", "snippet": "Nevertheless it&#39;s a crucial problem to solve for <b>multiple</b> industries <b>like</b> healthcare, insurance and banking. Source : ... In this work the authors proposed usage of a transformer based architecture using multi-headed attention <b>self-attention</b> layers at both visual and text stages and thus can learn both character recognition as well as language-related dependencies of the character sequences to be decoded. Since the language knowledge is embedded into the model itself, there is no need for ...", "dateLastCrawled": "2022-02-03T01:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>BEHRT: Transformer for Electronic Health Records</b> | Scientific Reports", "url": "https://www.nature.com/articles/s41598-020-62922-y", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41598-020-62922-y", "snippet": "By depicting diagnoses as words, each visit as a sentence, and a patient\u2019s entire medical history as <b>a document</b>, we facilitate the use of <b>multi-head</b> <b>self-attention</b>, positional encoding, and ...", "dateLastCrawled": "2022-02-02T17:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "The <b>Illustrated Transformer</b> \u2013 Jay Alammar \u2013 Visualizing machine ...", "url": "http://jalammar.github.io/illustrated-transformer/", "isFamilyFriendly": true, "displayUrl": "jalammar.github.io/<b>illustrated-transformer</b>", "snippet": "The word at each position passes through a <b>self-attention</b> process. Then, <b>they</b> each pass through a feed-forward neural network -- the exact same network with each vector flowing through it separately. <b>Self-Attention</b> at a High Level. Don\u2019t be fooled by me throwing around the word \u201c<b>self-attention</b>\u201d <b>like</b> it\u2019s a concept everyone should be familiar with. I had personally never came across the concept until <b>reading</b> the Attention is All You Need paper. Let us distill how it works. Say the ...", "dateLastCrawled": "2022-02-03T07:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Uncategorized \u2013 prettyandnerdy", "url": "https://pretteyandnerdy.wordpress.com/category/uncategorized/", "isFamilyFriendly": true, "displayUrl": "https://pretteyandnerdy.wordpress.com/category/uncategorized", "snippet": "The first is a <b>multi-head</b> <b>self-attention</b> mechanism in the encoder, and the second is the original encoder-decoder attention in the decoder, which performs <b>multi-head</b> attention over the output of the encoder stack The decoder stack also has a <b>self-attention</b> layer but it is only allowed to attend to earlier positions in the output sequence. This is done by masking future positions by setting them to negative infinity before the Softmax step. Residual Connection. Six layers is pretty deep. When ...", "dateLastCrawled": "2021-12-20T11:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Text Document Summarization using Word Embedding</b> | Request PDF", "url": "https://www.researchgate.net/publication/336248932_Text_Document_Summarization_using_Word_Embedding", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/336248932_Text_<b>Document</b>_Summarization_using...", "snippet": "The sequences are then fed into the parallel structure of the <b>multi-head</b> <b>self-attention</b> module and the BILSTM neural network module, respectively. By splicing the output of the neural network ...", "dateLastCrawled": "2022-01-31T03:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Deep Learning Based</b> <b>Text Classification</b>: A Comprehensive Review \u2013 arXiv ...", "url": "https://www.arxiv-vanity.com/papers/2004.03705/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2004.03705", "snippet": "Abstract. <b>Deep learning based</b> models have surpassed classical machine learning based approaches in various <b>text classification</b> tasks, including sentiment analysis, news categorization, question answering, and natural language inference. In this work, we provide a detailed review of more than 150 <b>deep learning based</b> models for <b>text classification</b> developed in recent years, and discuss their technical contributions, similarities, and strengths. We also provide a summary of more than 40 popular ...", "dateLastCrawled": "2022-02-03T05:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A Survey of <b>Document</b> Grounded Dialogue Systems (DGDS) - arxiv-vanity.com", "url": "https://www.arxiv-vanity.com/papers/2004.13818/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2004.13818", "snippet": "Dialogue system (DS) attracts great attention from industry and academia because of its wide application prospects. Researchers usually divide the DS according to the function. However, many conversations require the DS to switch between different functions. For example, movie discussion can change from chit-chat to QA, the conversational recommendation can transform from chit-chat to recommendation, etc. Therefore, classification according to functions may not be enough to help us ...", "dateLastCrawled": "2021-12-03T17:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "[R] Stanford NLP just released a model for question -&gt; <b>document</b> ...", "url": "https://www.reddit.com/r/MachineLearning/comments/dnzcy5/r_stanford_nlp_just_released_a_model_for_question/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/MachineLearning/comments/dnzcy5/r_stanford_nlp_just_released...", "snippet": "The only video I have seen that actually motivates the idea of <b>self-attention</b>. CodeEmporium\u2019s video ... Video and article by Jay Alammar - Nice visuals. Goes in-depth into the computations. Very useful for understanding <b>multi head</b> attention. Try <b>reading</b> the paper again now; it will, hopefully, make a lot more sense. [2] Code. Tensorflow code example . Pytorch code example by Aladdin Persson [3] Theory (for pros) See MathChief\u2019s comment below. I hope you understand the paper \ud83d\ude42. Edit 1 ...", "dateLastCrawled": "2021-09-11T13:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Python Machine Learning: Machine Learning and Deep Learning with Python ...", "url": "https://ebin.pub/python-machine-learning-machine-learning-and-deep-learning-with-python-scikit-learn-and-tensorflow-2-3rd-edition-3nbsped-9781789955750-1789955750.html", "isFamilyFriendly": true, "displayUrl": "https://ebin.pub/python-machine-learning-machine-learning-and-deep-learning-with...", "snippet": "McCulloch and Pitts described such a nerve cell as a simple logic gate with binary outputs; <b>multiple</b> signals arrive at the dendrites, <b>they</b> are then integrated into the cell body, and, if the accumulated signal exceeds a certain threshold, an output signal is generated that will be passed on by the axon. Only a few years later, Frank Rosenblatt published the first concept of the perceptron learning rule based on the MCP neuron model (The Perceptron: A Perceiving and Recognizing Automaton, F ...", "dateLastCrawled": "2022-01-31T17:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Abstracts of BERT-Related Papers", "url": "https://ayaka14732.github.io/bert-related-paper-abstracts/", "isFamilyFriendly": true, "displayUrl": "https://ayaka14732.github.io/bert-related-paper-abstracts", "snippet": "Interpretable multi-hop <b>reading</b> comprehension (RC) over <b>multiple</b> documents is a challenging problem because it demands reasoning over <b>multiple</b> information sources and explaining the answer prediction by providing supporting evidences. In this paper, we propose an effective and interpretable Select, Answer and Explain (SAE) system to solve the multi-<b>document</b> RC problem. Our system first filters out answer-unrelated documents and thus reduce the amount of distraction information. This is ...", "dateLastCrawled": "2022-01-30T18:45:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "How to easily do Handwriting Recognition using Deep Learning", "url": "https://nanonets.com/blog/handwritten-character-recognition/", "isFamilyFriendly": true, "displayUrl": "https://nanonets.com/blog/handwritten-character-recognition", "snippet": "This result is then passed to a <b>Multi-Head</b> Language <b>Self-Attention</b> module which <b>is similar</b> to attention module in Visual encoder. The text features generated along the visual features from visual encoder are passed to a mutual-attention module whose task is to align and combine the learned features from both images and the text inputs. The output is passed through a softmax function to get the final result.", "dateLastCrawled": "2022-02-03T01:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>BEHRT: Transformer for Electronic Health Records</b> | Scientific Reports", "url": "https://www.nature.com/articles/s41598-020-62922-y", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41598-020-62922-y", "snippet": "By depicting diagnoses as words, each visit as a sentence, and a patient\u2019s entire medical history as <b>a document</b>, we facilitate the use of <b>multi-head</b> <b>self-attention</b>, positional encoding, and ...", "dateLastCrawled": "2022-02-02T17:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "The <b>Illustrated Transformer</b> \u2013 Jay Alammar \u2013 Visualizing machine ...", "url": "http://jalammar.github.io/illustrated-transformer/", "isFamilyFriendly": true, "displayUrl": "jalammar.github.io/<b>illustrated-transformer</b>", "snippet": "The encoder\u2019s inputs first flow through a <b>self-attention</b> layer \u2013 a layer that helps the encoder look at other words in the input sentence as it encodes a specific word. We\u2019ll look closer at <b>self-attention</b> later in the post. The outputs of the <b>self-attention</b> layer are fed to a feed-forward neural network. The exact same feed-forward network is independently applied to each position. The decoder has both those layers, but between them is an attention layer that helps the decoder focus on ...", "dateLastCrawled": "2022-02-03T07:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "transformer-cnn-emotion-recognition/Parallel_is_All_You_Want.py at main ...", "url": "https://github.com/IliaZenkov/transformer-cnn-emotion-recognition/blob/main/Parallel_is_All_You_Want.py", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/IliaZenkov/transformer-cnn-emotion-recognition/blob/main/Parallel...", "snippet": "I could have used LSTM-RNNs to learn the sequence of the spectrogram for each emotion, but the network would only learn to predict frequency changes according to adjacent time steps; in contrast, the <b>multi-head</b> <b>self-attention</b> layers of the transformer enable the network to look at <b>multiple</b> previous time steps when predicting the next. This made sense to me because emotions colour the entire sequence of frequencies, not just at one timestep.", "dateLastCrawled": "2021-12-03T19:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Enhanced Speaker-aware Multi-party Multi-turn Dialogue Comprehension ...", "url": "https://deepai.org/publication/enhanced-speaker-aware-multi-party-multi-turn-dialogue-comprehension", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/enhanced-speaker-aware-multi-party-multi-turn-dialogue...", "snippet": "The mainstream work of machine <b>reading</b> comprehension on <b>multiple</b> multi-turn dialogues commonly adopts the pre-trained language model (PrLM) Devlin et al. as an encoder to represent the dialogue contexts coarsely, <b>taking</b> the pairwise dialogue passage and question as a whole Qu et al. ; Gu et al. ; Li et al. . Recent researches about modeling speaker-aware information for dialogue MRC proved to be effective Gu et al. ; Liu et al. . However, there are still unmatched attentions over key speaker ...", "dateLastCrawled": "2021-11-21T02:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Deep Learning Based</b> <b>Text Classification</b>: A Comprehensive Review \u2013 arXiv ...", "url": "https://www.arxiv-vanity.com/papers/2004.03705/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2004.03705", "snippet": "Abstract. <b>Deep learning based</b> models have surpassed classical machine learning based approaches in various <b>text classification</b> tasks, including sentiment analysis, news categorization, question answering, and natural language inference. In this work, we provide a detailed review of more than 150 <b>deep learning based</b> models for <b>text classification</b> developed in recent years, and discuss their technical contributions, similarities, and strengths. We also provide a summary of more than 40 popular ...", "dateLastCrawled": "2022-02-03T05:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Python Machine Learning: Machine Learning and Deep Learning with Python ...", "url": "https://ebin.pub/python-machine-learning-machine-learning-and-deep-learning-with-python-scikit-learn-and-tensorflow-2-3rd-edition-3nbsped-9781789955750-1789955750.html", "isFamilyFriendly": true, "displayUrl": "https://ebin.pub/python-machine-learning-machine-learning-and-deep-learning-with...", "snippet": "McCulloch and Pitts described such a nerve cell as a simple logic gate with binary outputs; <b>multiple</b> signals arrive at the dendrites, <b>they</b> are then integrated into the cell body, and, if the accumulated signal exceeds a certain threshold, an output signal is generated that will be passed on by the axon. Only a few years later, Frank Rosenblatt published the first concept of the perceptron learning rule based on the MCP neuron model (The Perceptron: A Perceiving and Recognizing Automaton, F ...", "dateLastCrawled": "2022-01-31T17:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A Survey of <b>Document</b> Grounded Dialogue Systems (DGDS) - arxiv-vanity.com", "url": "https://www.arxiv-vanity.com/papers/2004.13818/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2004.13818", "snippet": "Dialogue system (DS) attracts great attention from industry and academia because of its wide application prospects. Researchers usually divide the DS according to the function. However, many conversations require the DS to switch between different functions. For example, movie discussion can change from chit-chat to QA, the conversational recommendation can transform from chit-chat to recommendation, etc. Therefore, classification according to functions may not be enough to help us ...", "dateLastCrawled": "2021-12-03T17:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Paper Digest: <b>Recent Papers on Speech Recognition \u2013 Paper Digest</b>", "url": "https://www.paperdigest.org/2020/06/recent-papers-on-speech-recognition/", "isFamilyFriendly": true, "displayUrl": "https://www.paperdigest.org/2020/06/<b>recent-papers-on-speech-recognition</b>", "snippet": "Paper Digest Team extracted all recent Speech Recognition related papers on our radar, and generated highlight sentences for them. The results are then sorted by relevance &amp; date. In addition to this \u2018static\u2019 page, we also provide a real-time version of this article, which has more coverage and is updated in real time to include the most recent updates on this topic. Based in New York, Paper Digest is dedicated to producing high-quality text analysis results that <b>people</b> can acturally use ...", "dateLastCrawled": "2022-01-31T23:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Abstracts of BERT-Related Papers", "url": "https://ayaka14732.github.io/bert-related-paper-abstracts/", "isFamilyFriendly": true, "displayUrl": "https://ayaka14732.github.io/bert-related-paper-abstracts", "snippet": "Interpretable multi-hop <b>reading</b> comprehension (RC) over <b>multiple</b> documents is a challenging problem because it demands reasoning over <b>multiple</b> information sources and explaining the answer prediction by providing supporting evidences. In this paper, we propose an effective and interpretable Select, Answer and Explain (SAE) system to solve the multi-<b>document</b> RC problem. Our system first filters out answer-unrelated documents and thus reduce the amount of distraction information. This is ...", "dateLastCrawled": "2022-01-30T18:45:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Master&#39;s theses</b> \u2013 Seminar for Statistics | ETH Zurich", "url": "https://math.ethz.ch/sfs/research/master-theses.html", "isFamilyFriendly": true, "displayUrl": "https://math.ethz.ch/sfs/research/master-theses.html", "snippet": "We propose a new Transformer-based framework for the SASIE problem, in which the intra-/inter-image <b>multi-head</b> <b>self-attention</b> blocks are developed for intra-/inter-knowledge transfer. The content of the edited areas is synthesized according to the given semantic label, while the style of the edited areas is inherited from the reference image. Extensive experiments on different datasets prove the effectiveness of our proposed framework for semantically editing the images and stylizing the ...", "dateLastCrawled": "2022-02-03T02:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "transformer-cnn-emotion-recognition/Parallel_is_All_You_Want.py at main ...", "url": "https://github.com/IliaZenkov/transformer-cnn-emotion-recognition/blob/main/Parallel_is_All_You_Want.py", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/IliaZenkov/transformer-cnn-emotion-recognition/blob/main/Parallel...", "snippet": "I could have used LSTM-RNNs to learn the sequence of the spectrogram for each emotion, but the network would only learn to predict frequency changes according to adjacent time steps; in contrast, the <b>multi-head</b> <b>self-attention</b> layers of the transformer enable the network to look at <b>multiple</b> previous time steps when predicting the next. This made sense to me because emotions colour the entire sequence of frequencies, not just at one timestep.", "dateLastCrawled": "2021-12-03T19:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Text Document Summarization using Word Embedding</b> | Request PDF", "url": "https://www.researchgate.net/publication/336248932_Text_Document_Summarization_using_Word_Embedding", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/336248932_Text_<b>Document</b>_Summarization_using...", "snippet": "The sequences are then fed into the parallel structure of the <b>multi-head</b> <b>self-attention</b> module and the BILSTM neural network module, respectively. By splicing the output of the neural network ...", "dateLastCrawled": "2022-01-31T03:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Deep Learning Based</b> <b>Text Classification</b>: A Comprehensive Review \u2013 arXiv ...", "url": "https://www.arxiv-vanity.com/papers/2004.03705/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2004.03705", "snippet": "Abstract. <b>Deep learning based</b> models have surpassed classical machine learning based approaches in various <b>text classification</b> tasks, including sentiment analysis, news categorization, question answering, and natural language inference. In this work, we provide a detailed review of more than 150 <b>deep learning based</b> models for <b>text classification</b> developed in recent years, and discuss their technical contributions, similarities, and strengths. We also provide a summary of more than 40 popular ...", "dateLastCrawled": "2022-02-03T05:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Machine-Learning-and-Artificial-Intelligence/Deep Learning.md at master ...", "url": "https://github.com/SaqibMamoon/Machine-Learning-and-Artificial-Intelligence/blob/master/Deep%20Learning.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/SaqibMamoon/Machine-Learning-and-Artificial-Intelligence/blob/master...", "snippet": "Resources to learn more about Machine Learning and Artificial Intelligence - Machine-Learning-and-Artificial-Intelligence/Deep Learning.md at master \u00b7 SaqibMamoon ...", "dateLastCrawled": "2021-12-07T17:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Uncategorized \u2013 prettyandnerdy", "url": "https://pretteyandnerdy.wordpress.com/category/uncategorized/", "isFamilyFriendly": true, "displayUrl": "https://pretteyandnerdy.wordpress.com/category/uncategorized", "snippet": "The first is a <b>multi-head</b> <b>self-attention</b> mechanism in the encoder, and the second is the original encoder-decoder attention in the decoder, which performs <b>multi-head</b> attention over the output of the encoder stack The decoder stack also has a <b>self-attention</b> layer but it is only allowed to attend to earlier positions in the output sequence. This is done by masking future positions by setting them to negative infinity before the Softmax step. Residual Connection. Six layers is pretty deep. When ...", "dateLastCrawled": "2021-12-20T11:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Progress in Neural NLP: Modeling, Learning, and Reasoning</b>", "url": "https://www.researchgate.net/publication/338426630_Progress_in_Neural_NLP_Modeling_Learning_and_Reasoning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/338426630_<b>Progress_in_Neural_NLP_Modeling</b>...", "snippet": "<b>self-attention</b> network, a sentence-ending symbol, &lt;/S&gt;, <b>can</b> be added, and its hidden state (the blue one) <b>can</b> be used as the representation of the whole sentence.", "dateLastCrawled": "2021-12-21T23:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Master thesis by asck - Issuu", "url": "https://issuu.com/asck/docs/multilingual_detection_ofoffensive_speech_in_socia", "isFamilyFriendly": true, "displayUrl": "https://issuu.com/asck/docs/multilingual_detection_ofoffensive_speech_in_socia", "snippet": "Both the Scaled Dot-Product Attention and the <b>Multi-Head</b> Attention <b>can</b> be seen in figure 10. Figure 10: To the left the <b>self-attention</b> module is depicted, and to the right the <b>Multi-Head</b> Attention ...", "dateLastCrawled": "2022-02-02T05:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Abstracts of BERT-Related Papers", "url": "https://ayaka14732.github.io/bert-related-paper-abstracts/", "isFamilyFriendly": true, "displayUrl": "https://ayaka14732.github.io/bert-related-paper-abstracts", "snippet": "Interpretable multi-hop <b>reading</b> comprehension (RC) over <b>multiple</b> documents is a challenging problem because it demands reasoning over <b>multiple</b> information sources and explaining the answer prediction by providing supporting evidences. In this paper, we propose an effective and interpretable Select, Answer and Explain (SAE) system to solve the multi-<b>document</b> RC problem. Our system first filters out answer-unrelated documents and thus reduce the amount of distraction information. This is ...", "dateLastCrawled": "2022-01-30T18:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Python Machine Learning: Machine Learning and Deep Learning with Python ...", "url": "https://ebin.pub/python-machine-learning-machine-learning-and-deep-learning-with-python-scikit-learn-and-tensorflow-2-3rd-edition-3nbsped-9781789955750-1789955750.html", "isFamilyFriendly": true, "displayUrl": "https://ebin.pub/python-machine-learning-machine-learning-and-deep-learning-with...", "snippet": "Each state <b>can</b> be associated with a positive or negative reward, and a reward <b>can</b> be defined as accomplishing an overall goal, such as winning or losing a game of chess. For instance, in chess, the outcome of each move <b>can</b> <b>be thought</b> of as a different state of the environment. To explore the chess example further, let&#39;s think of visiting ...", "dateLastCrawled": "2022-01-31T17:24:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "How to easily do Handwriting Recognition using Deep Learning", "url": "https://nanonets.com/blog/handwritten-character-recognition/", "isFamilyFriendly": true, "displayUrl": "https://nanonets.com/blog/handwritten-character-recognition", "snippet": "In this work the authors proposed usage of a transformer based architecture using multi-headed attention <b>self-attention</b> layers at both visual and text stages and thus <b>can</b> learn both character recognition as well as language-related dependencies of the character sequences to be decoded. Since the language knowledge is embedded into the model itself, there is no need for any additional post-processing step using a language model and hence has the capability to predicts outputs which are not ...", "dateLastCrawled": "2022-02-03T01:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>BEHRT: Transformer for Electronic Health Records</b> | Scientific Reports", "url": "https://www.nature.com/articles/s41598-020-62922-y", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41598-020-62922-y", "snippet": "By depicting diagnoses as words, each visit as a sentence, and a patient\u2019s entire medical history as <b>a document</b>, we facilitate the use of <b>multi-head</b> <b>self-attention</b>, positional encoding, and ...", "dateLastCrawled": "2022-02-02T17:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Recent Advances in Natural Language Processing via Large Pre-Trained ...", "url": "https://www.arxiv-vanity.com/papers/2111.01243/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2111.01243", "snippet": "The Transformer\u2019s <b>multi-head</b> <b>self-attention</b> mechanism allows every word to attend to all previous words or every word except the target, allowing the model to efficiently capture long-range dependencies without the expensive recurrent computation in LSTMs. <b>Multiple</b> layers of <b>multi-head</b> <b>self-attention</b> allow for increasingly more expressive representations, useful for a range of NLP problems. As a result, nearly all popular language models, including GPT, BERT, BART (lewis-etal-2020-bart ...", "dateLastCrawled": "2021-12-23T23:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Master&#39;s theses</b> \u2013 Seminar for Statistics | ETH Zurich", "url": "https://math.ethz.ch/sfs/research/master-theses.html", "isFamilyFriendly": true, "displayUrl": "https://math.ethz.ch/sfs/research/master-theses.html", "snippet": "We propose a new Transformer-based framework for the SASIE problem, in which the intra-/inter-image <b>multi-head</b> <b>self-attention</b> blocks are developed for intra-/inter-knowledge transfer. The content of the edited areas is synthesized according to the given semantic label, while the style of the edited areas is inherited from the reference image. Extensive experiments on different datasets prove the effectiveness of our proposed framework for semantically editing the images and stylizing the ...", "dateLastCrawled": "2022-02-03T02:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Paper Digest: <b>ICASSP 2020 Highlights \u2013 Paper Digest</b>", "url": "https://www.paperdigest.org/2020/04/icassp-2020-highlights/", "isFamilyFriendly": true, "displayUrl": "https://www.paperdigest.org/2020/04/icassp-2020-highlights", "snippet": "Speech Enhancement Using Self-Adaptation and <b>Multi-Head</b> <b>Self-Attention</b>: Y. Koizumi, K. Yaiabe, M. Delcroix, Y. Maxuxama and D. Takeuchi: This paper investigates a self-adaptation method for speech enhancement using auxiliary speaker-aware features; we extract a speaker representation used for adaptation directly from the test utterance. 38", "dateLastCrawled": "2022-01-30T12:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Python Machine Learning: Machine Learning and Deep Learning with Python ...", "url": "https://ebin.pub/python-machine-learning-machine-learning-and-deep-learning-with-python-scikit-learn-and-tensorflow-2-3rd-edition-3nbsped-9781789955750-1789955750.html", "isFamilyFriendly": true, "displayUrl": "https://ebin.pub/python-machine-learning-machine-learning-and-deep-learning-with...", "snippet": "A basic version of <b>self-attention</b>.....Page 643 Parameterizing the <b>self-attention</b> mechanism with query, key, and value weights.....Page 645 <b>Multi-head</b> attention and the Transformer block.....Page 646 Summary.....Page 647 Chapter 17: Generative Adversarial Networks for Synthesizing New Data.....Page 648 Starting with autoencoders.....Page 649 Generative models for synthesizing new data.....Page 652 Generating new samples with GANs.....Page 653 Understanding the loss functions of the generator ...", "dateLastCrawled": "2022-01-31T17:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Retrospective Reader for Machine Reading Comprehension</b>", "url": "https://www.researchgate.net/publication/338853526_Retrospective_Reader_for_Machine_Reading_Comprehension", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/338853526_Retrospective_<b>Read</b>er_for_Machine...", "snippet": "Inspired by how humans solve. <b>reading</b> comprehension questions, we proposed a. retrospective reader (Retro-Reader) that integrates. two stages of <b>reading</b> and veri\ufb01cation strategies: 1) sketchy ...", "dateLastCrawled": "2022-01-21T21:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Hands-On <b>Python Natural Language Processing: Explore tools</b> and ...", "url": "https://dokumen.pub/hands-on-python-natural-language-processing-explore-tools-and-techniques-to-analyze-and-process-text-with-a-view-to-building-real-world-nlp-applications-9781838982584-1838982582.html", "isFamilyFriendly": true, "displayUrl": "https://dokumen.pub/hands-on-<b>python-natural-language-processing-explore-tools</b>-and...", "snippet": "A small note on masked <b>self-attention</b> Another important thing to understand is that inside the decoder, the <b>multi-head</b> attention mechanism is masked, meaning that it is only allowed to use representations from positions toward the left. This is because, while predicting the token for a particular position, only tokens from the left side would be available. As a result, all embeddings from the right are multiplied with 0 to mask them and the representations created <b>can</b> only be influenced by ...", "dateLastCrawled": "2022-02-01T06:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Progress in Neural NLP: Modeling, Learning, and Reasoning</b>", "url": "https://www.researchgate.net/publication/338426630_Progress_in_Neural_NLP_Modeling_Learning_and_Reasoning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/338426630_<b>Progress_in_Neural_NLP_Modeling</b>...", "snippet": "<b>self-attention</b> network, a sentence-ending symbol, &lt;/S&gt;, <b>can</b> be added, and its hidden state (the blue one) <b>can</b> be used as the representation of the whole sentence.", "dateLastCrawled": "2021-12-21T23:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Paper Digest: <b>Recent Papers on Speech Recognition \u2013 Paper Digest</b>", "url": "https://www.paperdigest.org/2020/06/recent-papers-on-speech-recognition/", "isFamilyFriendly": true, "displayUrl": "https://www.paperdigest.org/2020/06/<b>recent-papers-on-speech-recognition</b>", "snippet": "Paper Digest Team extracted all recent Speech Recognition related papers on our radar, and generated highlight sentences for them. The results are then sorted by relevance &amp; date. In addition to this \u2018static\u2019 page, we also provide a real-time version of this article, which has more coverage and is updated in real time to include the most recent updates on this topic. Based in New York, Paper Digest is dedicated to producing high-quality text analysis results that <b>people</b> <b>can</b> acturally use ...", "dateLastCrawled": "2022-01-31T23:19:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Introduction to Transformers in Machine Learning</b>", "url": "https://www.machinecurve.com/index.php/2020/12/28/introduction-to-transformers-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/.../12/28/<b>introduction-to-transformers-in-machine-learning</b>", "snippet": "The masked <b>multi-head</b> attention segment, which performs <b>multi-head</b> <b>self-attention</b> on the outputs, but does so in a masked way, so that positions depend on the past only. The <b>multi-head</b> attention segment , which performs <b>multi-head</b> <b>self-attention</b> on a combination of the ( encoded ) inputs and the outputs, so that the model learns to correlate encoded inputs with desired outputs.", "dateLastCrawled": "2022-02-03T03:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "10.5. <b>Multi-Head Attention</b> \u2014 Dive into Deep <b>Learning</b> 0.17.2 documentation", "url": "https://www.d2l.ai/chapter_attention-mechanisms/multihead-attention.html", "isFamilyFriendly": true, "displayUrl": "https://www.d2l.ai/chapter_attention-mechanisms/<b>multihead-attention</b>.html", "snippet": "<b>Multi-Head Attention</b> \u2014 Dive into Deep <b>Learning</b> 0.17.0 documentation. 10.5. <b>Multi-Head Attention</b>. In practice, given the same set of queries, keys, and values we may want our model to combine knowledge from different behaviors of the same attention mechanism, such as capturing dependencies of various ranges (e.g., shorter-range vs. longer ...", "dateLastCrawled": "2022-01-30T04:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Machine</b> <b>Learning</b> Glossary: Language Evaluation | Google Developers", "url": "https://developers.google.com/machine-learning/glossary/language", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine</b>-<b>learning</b>/glossary/language", "snippet": "Refer also to <b>self-attention</b> and <b>multi-head</b> <b>self-attention</b>, which are the building blocks of Transformers. B. bag of words. #language. A representation of the words in a phrase or passage, irrespective of order. For example, bag of words represents the following three phrases identically: the dog jumps; jumps the dog; dog jumps the; Each word is mapped to an index in a sparse vector, where the vector has an index for every word in the vocabulary. For example, the phrase the dog jumps is ...", "dateLastCrawled": "2022-01-29T02:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Attending to Attention. A summary of a revolutionary paper\u2026 | by Akash ...", "url": "https://towardsdatascience.com/attending-to-attention-eba798f0e940", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/attending-to-attention-eba798f0e940", "snippet": "The first is a <b>multi-head</b> <b>self-attention</b> mechanism, and the second is a simple, position-wise fully connected feed-forward network. We employ a residual connection around each of the two sub-layers, followed by layer normalization. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of ...", "dateLastCrawled": "2022-01-25T10:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "11. Attention Mechanisms \u2014 Dive into Deep <b>Learning</b> 0.17.0 documentation", "url": "http://preview.d2l.ai/d2l-en/master/chapter_attention-mechanisms/index.html", "isFamilyFriendly": true, "displayUrl": "preview.d2l.ai/d2l-en/master/chapter_attention-mechanisms/index.html", "snippet": "In the end, equipped with the more recent <b>multi-head</b> attention and <b>self-attention</b> designs, we will describe the transformer architecture based solely on attention mechanisms. Since their proposal in 2017, transformers have been pervasive in modern deep <b>learning</b> applications, such as in areas of language, vision, speech, and reinforcement <b>learning</b>.", "dateLastCrawled": "2022-01-18T14:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Intuitive Introduction to BERT \u2013 MachineCurve</b>", "url": "https://www.machinecurve.com/index.php/2021/01/04/intuitive-introduction-to-bert/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/index.php/2021/01/04/<b>intuitive-introduction-to-bert</b>", "snippet": "From our article about GPT: \u201cThe input is then served to a masked <b>multi-head</b> attention segment, which computes <b>self-attention</b> in a unidirectional way.Here, the residual is added and the result is layer normalized.\u201d Indeed, GPT (which uses the Transformer decoder segment autoregressively during pretraining) and the original Transformer (which performs Seq2Seq), apply a mask in one of the attention modules \u2013 the masked <b>multi-head</b> <b>self-attention</b> subsegment in the decoder segment.. For any ...", "dateLastCrawled": "2022-01-30T22:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Capturing Multi-Resolution Context by Dilated <b>Self-Attention</b>", "url": "https://www.merl.com/publications/docs/TR2021-036.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.merl.com/publications/docs/TR2021-036.pdf", "snippet": "to <b>machine</b> translation or language modeling, where close-by words are more likely to have a dependent relationship, while only a few distant words or word groups are relevant to trace the semantic con-text and syntax of a sentence [15]. This hypothesis is investigated in this work by combining re-stricted (or time-restricted) <b>self-attention</b> with a dilation mechanism, whereby a high <b>self-attention</b> resolution for neighboring frames and a lower <b>self-attention</b> resolution for distant information ...", "dateLastCrawled": "2021-12-02T00:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "CoAtNet: how to perfectly combine CNNs and Transformers | by Leonardo ...", "url": "https://medium.com/codex/coatnet-how-to-perfectly-combine-cnns-and-transformer-9632e187ecbf", "isFamilyFriendly": true, "displayUrl": "https://medium.com/codex/coatnet-how-to-perfectly-combine-cnns-and-transformer-9632e...", "snippet": "The <b>multi-head</b> attention block computes <b>self-attention</b> several times with different weight matrices and then concatenates the results together, which are resized to the embedding dimension using ...", "dateLastCrawled": "2022-01-26T06:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Dilated Residual Network with Multi-head</b> <b>Self-attention</b> for Speech ...", "url": "https://www.researchgate.net/publication/332791636_Dilated_Residual_Network_with_Multi-head_Self-attention_for_Speech_Emotion_Recognition", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/332791636_<b>Dilated_Residual_Network_with_Multi</b>...", "snippet": "While Li et al. [23] proposed the combining use of Dilated Residual Network and <b>Multi-head</b> <b>Self-attention</b> for feature <b>learning</b> in speech emotion recognition. <b>Multi-head</b> <b>Self-attention</b> models ...", "dateLastCrawled": "2021-12-21T23:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Transformers in NLP: A beginner friendly explanation | Towards Data Science", "url": "https://towardsdatascience.com/transformers-89034557de14", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>transformer</b>s-89034557de14", "snippet": "If you are looking for an <b>analogy</b> between <b>self attention</b> and attention, think of z serving the purpose of context vectors and not global alignment weights. The <b>Transformer</b> . \u26a0\ufe0f A word of caution: the contents of this image may appear exponentially more complicated than they are. We will break this scary beast down into small baby beasts and it will all make sense. (I promise #2) (left) The <b>Transformer</b> architecture. Source: paper. (right) An abstracted version of the same for better ...", "dateLastCrawled": "2022-02-02T13:19:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(multi-head self-attention)  is like +(having multiple people reading a document and taking notes on what they read)", "+(multi-head self-attention) is similar to +(having multiple people reading a document and taking notes on what they read)", "+(multi-head self-attention) can be thought of as +(having multiple people reading a document and taking notes on what they read)", "+(multi-head self-attention) can be compared to +(having multiple people reading a document and taking notes on what they read)", "machine learning +(multi-head self-attention AND analogy)", "machine learning +(\"multi-head self-attention is like\")", "machine learning +(\"multi-head self-attention is similar\")", "machine learning +(\"just as multi-head self-attention\")", "machine learning +(\"multi-head self-attention can be thought of as\")", "machine learning +(\"multi-head self-attention can be compared to\")"]}