{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>RNN</b> (Recurrent Neural Network) Tutorial: TensorFlow Example", "url": "https://www.guru99.com/rnn-tutorial.html", "isFamilyFriendly": true, "displayUrl": "https://www.guru99.com/<b>rnn</b>-tutorial.html", "snippet": "What is a Recurrent Neural Network (<b>RNN</b>)? A Recurrent Neural Network (<b>RNN</b>) is a class of Artificial Neural Network in which the connection between different nodes forms a directed graph to give a temporal dynamic behavior. It helps to model sequential data that are derived from feedforward networks. It works similarly to human <b>brains</b> to deliver predictive results.", "dateLastCrawled": "2022-02-02T21:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Are we all Bayesian? <b>Our</b> <b>Brains</b> Think So | by Ron Ozminkowski, PhD ...", "url": "https://towardsdatascience.com/are-we-all-bayesian-our-brains-think-so-555cedaffed9", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/are-we-all-bayesian-<b>our</b>-<b>brains</b>-think-so-555cedaffed9", "snippet": "The use of prior information in this prediction process suggests Bayesian roots to the <b>RNN</b>, and this is the subject of my post today. The Brain as a \u201cPrediction Machine\u201d Ananthaswamy (2021) says <b>our</b> <b>brains</b> are \u201cprediction machines,\u201d gobbling up input from <b>our</b> five senses to assess why things happen. We then update these predictions as we receive new information. \u201cThrough predictive processing,\u201d he states, \u201cthe brain uses its prior knowledge of the world to make inferences or ...", "dateLastCrawled": "2022-01-29T08:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Tutorial: Recurrent neural networks for cognitive neuroscience | The ...", "url": "https://cbmm.mit.edu/video/tutorial-recurrent-neural-networks-cognitive-neuroscience", "isFamilyFriendly": true, "displayUrl": "https://cbmm.mit.edu/video/tutorial-recurrent-neural-networks-cognitive-neuroscience", "snippet": "But for many modelers <b>like</b> us, you want these networks not just as tools, but also really as computational models of <b>brains</b> or different parts of <b>brains</b>. And they&#39;re convenient compared to traditional models in several ways. One is that they help you model complex behavior more easily. So we&#39;ll see some examples. In the past, if you have a complex behavior that you want to model, then you have to build and design a very clever model to do it, whereas here, you can let-- do some machine ...", "dateLastCrawled": "2022-01-27T17:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>RNN or Recurrent Neural Network for Noobs</b> | by Debarko De \ud83e\udd81 ...", "url": "https://medium.com/hackernoon/rnn-or-recurrent-neural-network-for-noobs-a9afbb00e860", "isFamilyFriendly": true, "displayUrl": "https://medium.com/hackernoon/<b>rnn-or-recurrent-neural-network-for-noobs</b>-a9afbb00e860", "snippet": "This is very much <b>like</b> how we as humans take decisions in <b>our</b> life. We combine the present data with recent past to take a call on a particular problem at hand. This example is excessively ...", "dateLastCrawled": "2021-02-27T21:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>RNN</b> or Recurrent Neural Network for Noobs | Bitcoin Insider", "url": "https://www.bitcoininsider.org/article/30372/rnn-or-recurrent-neural-network-noobs", "isFamilyFriendly": true, "displayUrl": "https://www.bitcoininsider.org/article/30372", "snippet": "Just to draw a correlation with humans in general, we also don\u2019t take in place decisions. We also base <b>our</b> decisions on previous knowledge on the subject. (**over simplified, hard to say I understand even 0.1% of human <b>brains</b>**) Where to use a <b>RNN</b>? RNNs can be used in a lot of different places. Following are a few examples where a lot of RNNs ...", "dateLastCrawled": "2022-01-19T18:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "What is a Neural Network?", "url": "https://madanswer.com/44593/neural-network", "isFamilyFriendly": true, "displayUrl": "https://madanswer.com/44593/neural-network", "snippet": "Neural Networks replicate the way humans learn, inspired by how the neurons in <b>our</b> <b>brains</b> fire, only much simpler. Neural Network. The most common Neural Networks consist of three network layers: An input layer. A hidden layer (this is the most important layer where feature extraction takes place, and adjustments are made to train faster and function better) An output layer. Each sheet contains neurons called \u201cnodes,\u201d performing various operations. Neural Networks are used in deep ...", "dateLastCrawled": "2022-02-03T00:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>RNN</b> or Recurrent Neural Network for Noobs | HackerNoon", "url": "https://hackernoon.com/rnn-or-recurrent-neural-network-for-noobs-a9afbb00e860", "isFamilyFriendly": true, "displayUrl": "https://hackernoon.com/<b>rnn</b>-or-recurrent-neural-network-for-noobs-a9afbb00e860", "snippet": "Unrolling a <b>RNN</b>. We take a <b>RNN</b>\u2019s hidden units and replicate it for every time step. Each replication through time step <b>is like</b> a layer in a feed-forward network. Each time step t layer connects to all possible layers in the time step t+1. Thus we randomly initialise the weights, unroll the network and then use backpropagation to optimise the ...", "dateLastCrawled": "2022-02-02T13:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Long Short-Term Memory Networks. Introduction | by Vinithavn ...", "url": "https://medium.com/analytics-vidhya/long-short-term-memory-networks-23119598b66b", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/long-short-term-memory-networks-23119598b66b", "snippet": "<b>Like</b> Backpropagation, here also the weights of the neural network get updated through the gradients. But in <b>RNN</b>, we backpropagate through time and the layers. If the sequence is very large (many ...", "dateLastCrawled": "2022-02-02T15:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "How is <b>RNN related to deep learning</b>? - Quora", "url": "https://www.quora.com/How-is-RNN-related-to-deep-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/How-is-<b>RNN-related-to-deep-learning</b>", "snippet": "Answer (1 of 5): <b>RNN</b>, commonly known as Recurrent Neural Network is a very popular Deep Learning model which is used to carry out a number of Deep Learning tasks <b>like</b> Time Series prediction, Image Captioning, Google auto complete feature, etc. <b>RNN</b> as the name suggests, uses recursion technique t...", "dateLastCrawled": "2022-01-17T11:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Are <b>recurrent neural networks (RNNs) considered a generative</b> model in ...", "url": "https://www.quora.com/Are-recurrent-neural-networks-RNNs-considered-a-generative-model-in-Machine-Learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Are-<b>recurrent-neural-networks-RNNs-considered-a-generative</b>-model...", "snippet": "Answer (1 of 2): <b>RNN</b> v.s. \u201cnot <b>RNN</b>\u201d and generative v.s. discriminative are orthogonal categories. Basically, an <b>RNN</b> can be generative, but not all RNNs are generative. <b>RNN</b> is a particular way of organizing your network, i.e. some of the output gets fed back as input. The generative v.s. discrim...", "dateLastCrawled": "2022-01-21T15:11:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>RNN</b> (Recurrent Neural Network) Tutorial: TensorFlow Example", "url": "https://www.guru99.com/rnn-tutorial.html", "isFamilyFriendly": true, "displayUrl": "https://www.guru99.com/<b>rnn</b>-tutorial.html", "snippet": "A Recurrent Neural Network (<b>RNN</b>) is a class of Artificial Neural Network in which the connection between different nodes forms a directed graph to give a temporal dynamic behavior. It helps to model sequential data that are derived from feedforward networks. It works similarly to human <b>brains</b> to deliver predictive results.", "dateLastCrawled": "2022-02-02T21:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Researchers study recurrent neural network structure in the brain ...", "url": "https://www.sciencedaily.com/releases/2021/09/210921172655.htm", "isFamilyFriendly": true, "displayUrl": "https://<b>www.sciencedaily.com</b>/releases/2021/09/210921172655.htm", "snippet": "&quot;Two, it will help uncover other <b>similar</b> RNNs in other parts of the brain. It will help researchers use computational simulations to predict how <b>our</b> brain codes short-term memory, and how can it ...", "dateLastCrawled": "2022-02-01T17:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Recurrent Neural Networks (<b>RNN</b>): What It Is &amp; How It Works | Built In", "url": "https://builtin.com/data-science/recurrent-neural-networks-and-lstm", "isFamilyFriendly": true, "displayUrl": "https://builtin.com/data-science/recurrent-neural-networks-and-lstm", "snippet": "Recurrent neural networks (<b>RNN</b>) are the state of the art algorithm for sequential data and are used by Apple&#39;s Siri and and Google&#39;s voice search. It is the first algorithm that remembers its input, due to an internal memory, which makes it perfectly suited for machine learning problems that involve sequential data.", "dateLastCrawled": "2022-02-01T09:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Are we all Bayesian? <b>Our</b> <b>Brains</b> Think So | by Ron Ozminkowski, PhD ...", "url": "https://towardsdatascience.com/are-we-all-bayesian-our-brains-think-so-555cedaffed9", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/are-we-all-bayesian-<b>our</b>-<b>brains</b>-think-so-555cedaffed9", "snippet": "The use of prior information in this prediction process suggests Bayesian roots to the <b>RNN</b>, and this is the subject of my post today. The Brain as a \u201cPrediction Machine\u201d Ananthaswamy (2021) says <b>our</b> <b>brains</b> are \u201cprediction machines,\u201d gobbling up input from <b>our</b> five senses to assess why things happen. We then update these predictions as we receive new information. \u201cThrough predictive processing,\u201d he states, \u201cthe brain uses its prior knowledge of the world to make inferences or ...", "dateLastCrawled": "2022-01-29T08:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Tutorial: Recurrent neural networks for cognitive neuroscience | The ...", "url": "https://cbmm.mit.edu/video/tutorial-recurrent-neural-networks-cognitive-neuroscience", "isFamilyFriendly": true, "displayUrl": "https://cbmm.mit.edu/video/tutorial-recurrent-neural-networks-cognitive-neuroscience", "snippet": "It&#39;s also very <b>similar</b> to the evolutionary perspective, which is in biology. And it&#39;s just a different way of thinking about the problem. And it can be satisfying. OK, so in particular, so here, today, I will talk about recurrent neural network for cognitive neuroscience. And this really has a very long tradition. So it&#39;s dated at least back to the &#39;80s. But there is this iconic paper from Mante, Sussillo that I recommend you to check out if you&#39;re interested. If you haven&#39;t read it and if ...", "dateLastCrawled": "2022-01-27T17:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A simple Word_Predictor using <b>RNN</b> | by Sreelakshmi V B | Analytics ...", "url": "https://medium.com/analytics-vidhya/a-simple-word-predictor-using-rnn-460884c97e6c", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/a-simple-word-predictor-using-<b>rnn</b>-460884c97e6c", "snippet": "In here, we are trying to do the word prediction using Recurrent Neural Network or <b>RNN</b>. Neural Network is actually said to be a design, inspired by how human <b>brains</b> work. They recognize patterns ...", "dateLastCrawled": "2022-01-31T13:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Next-generation recurrent network models for cognitive neuroscience ...", "url": "https://cbmm.mit.edu/video/next-generation-recurrent-network-models-cognitive-neuroscience", "isFamilyFriendly": true, "displayUrl": "https://cbmm.mit.edu/video/next-generation-recurrent-network-models-cognitive-neuroscience", "snippet": "In this talk, I will discuss some of these challenges, and several recent steps that we took to partly address them and to build next-generation <b>RNN</b> models for cognitive neuroscience. PRESENTER: Robert Young, whom I&#39;m introducing today, would be joining MIT as an assistant professor in the <b>Brains</b> and Cognitive Science Department and with a joint appointment in the Schwartzmann College of Computing starting next week.", "dateLastCrawled": "2022-01-26T03:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "How is <b>RNN related to deep learning</b>? - Quora", "url": "https://www.quora.com/How-is-RNN-related-to-deep-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/How-is-<b>RNN-related-to-deep-learning</b>", "snippet": "Answer (1 of 5): <b>RNN</b>, commonly known as Recurrent Neural Network is a very popular Deep Learning model which is used to carry out a number of Deep Learning tasks like Time Series prediction, Image Captioning, Google auto complete feature, etc. <b>RNN</b> as the name suggests, uses recursion technique t...", "dateLastCrawled": "2022-01-17T11:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Deep Learning for Natural Language Processing: Part</b> 2 \u2014 <b>RNN</b> | by Iris ...", "url": "https://medium.com/gumgum-tech/deep-learning-for-natural-language-processing-part-2-rnn-ed2e7bd4d017", "isFamilyFriendly": true, "displayUrl": "https://medium.com/gumgum-tech/<b>deep-learning-for-natural-language-processing-part</b>-2...", "snippet": "<b>Deep Learning for Natural Language Processing: Part</b> 2 \u2014 <b>RNN</b>. This is a continuation from Part 1 on word embeddings. If you haven\u2019t read it, I would encourage going through it just so that this ...", "dateLastCrawled": "2021-07-07T17:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Top <b>Deep Learning Interview Questions</b> &amp; Answers for 2022 | Simplilearn", "url": "https://www.simplilearn.com/tutorials/deep-learning-tutorial/deep-learning-interview-questions", "isFamilyFriendly": true, "displayUrl": "https://www.simplilearn.com/tutorials/deep-learning-tutorial/deep-learning-interview...", "snippet": "While training an <b>RNN</b>, your slope can become either too small or too large; this makes the training difficult. When the slope is too small, the problem is known as a \u201cVanishing Gradient.\u201d When the slope tends to grow exponentially instead of decaying, it\u2019s referred to as an \u201cExploding Gradient.\u201d Gradient problems lead to long training times, poor performance, and low accuracy.", "dateLastCrawled": "2022-02-02T12:28:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Are we all Bayesian? <b>Our</b> <b>Brains</b> Think So | by Ron Ozminkowski, PhD ...", "url": "https://towardsdatascience.com/are-we-all-bayesian-our-brains-think-so-555cedaffed9", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/are-we-all-bayesian-<b>our</b>-<b>brains</b>-think-so-555cedaffed9", "snippet": "An <b>RNN</b> is a deep learning process involving multiple layers of neurons. If the neuroscientists\u2019 line of reasoning is correct, then the mathematical neurons created in their deep learning models resemble the real-life nerve cells animals use to make sense of things. Since humans are animals too, <b>our</b> thinking processes may be generating predictions the same way RNNs do. Errors in those predictions <b>can</b> be either perpetuated or corrected once more sensory input is collected and analyzed; this ...", "dateLastCrawled": "2022-01-29T08:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Researchers Study Recurrent Neural Network Structure in the Brain ...", "url": "https://neurosciencenews.com/recurrent-neural-network-frontal-cortex-19348/", "isFamilyFriendly": true, "displayUrl": "https://neurosciencenews.com/recurrent-neural-network-frontal-cortex-19348", "snippet": "And the two scientists learned that a recurrent neural network structure, or <b>RNN</b>, is responsible for those functions. \u201cThis <b>RNN</b> receives inputs from emotional regions of the brain and sends outputs to the motor cortex, the part of the brain responsible for voluntary movement,\u201d says Qian-Quan Sun, a UW professor of zoology and physiology. \u201cIn the artificial intelligence field, computer scientists have designed various artificial neural networks, including RNNs, which effectively solve ...", "dateLastCrawled": "2022-02-01T22:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Tutorial: Recurrent neural networks for cognitive neuroscience | The ...", "url": "https://cbmm.mit.edu/video/tutorial-recurrent-neural-networks-cognitive-neuroscience", "isFamilyFriendly": true, "displayUrl": "https://cbmm.mit.edu/video/tutorial-recurrent-neural-networks-cognitive-neuroscience", "snippet": "So many recurrent networks <b>can</b> <b>be thought</b> of this way. And sequence to sequence, there are many, many one liners. You <b>can</b> generate LSTM this way. This is an LMN, <b>RNN</b>, Gated Recurrent Unit, GRU, or even multiheaded tension that is used in transformers. And they are very popular in natural language processing. It&#39;s sequence to sequence. So they all generate output that is sequence length, batch size, and some dimension. But today we will use not these recurrent neural networks that use the ...", "dateLastCrawled": "2022-01-27T17:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Researchers study recurrent neural network structure in the brain ...", "url": "https://www.sciencedaily.com/releases/2021/09/210921172655.htm", "isFamilyFriendly": true, "displayUrl": "https://<b>www.sciencedaily.com</b>/releases/2021/09/210921172655.htm", "snippet": "However, Wang&#39;s data not only showed that the <b>RNN</b> does exist in the most important part of the brain -- the frontal cortex -- but additionally, this network is less complex than we <b>thought</b> and ...", "dateLastCrawled": "2022-02-01T17:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "UW Researchers Study Recurrent Neural Network Structure in the Brain ...", "url": "https://www.uwyo.edu/uw/news/2021/09/uw-researchers-study-recurrent-neural-network-structure-in-the-brain.html", "isFamilyFriendly": true, "displayUrl": "https://<b>www.uwyo.edu</b>/uw/news/2021/09/uw-researchers-study-recurrent-neural-network...", "snippet": "\u201cThe biggest surprise is that RNNs not only exist in <b>our</b> brain, but they are constructed with much more delicate function and, yet, highly efficient in processing sequential inputs,\u201d Sun says. \u201cIn general, cortical neurons are spatially reciprocal and intermingle with each other. However, Wang\u2019s data not only showed that the <b>RNN</b> does exist in the most important part of the brain -- the frontal cortex -- but additionally, this network is less complex than we <b>thought</b> and mostly ...", "dateLastCrawled": "2021-12-22T21:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Brain-Computer Interface: Advancement and Challenges", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8433803/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8433803", "snippet": "<b>RNN</b> <b>can</b> be considered a more powerful version of hidden Markov models (HMM), which classifies EEG correctly . LSTM is a kind of <b>RNN</b> with a unique architecture that allows it to acquire long-term dependencies despite the difficulties that RNNs confront. It contains a discrete memory cell, a type of node. To manage the flow of data, LSTM employs an architecture with a series of \u201cgates\u201d. When it comes to modeling time series of tasks such as writing and voice recognition, <b>RNN</b> and LSTM have ...", "dateLastCrawled": "2022-02-03T04:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A simple Word_Predictor using <b>RNN</b> | by Sreelakshmi V B | Analytics ...", "url": "https://medium.com/analytics-vidhya/a-simple-word-predictor-using-rnn-460884c97e6c", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/a-simple-word-predictor-using-<b>rnn</b>-460884c97e6c", "snippet": "In here, we are trying to do the word prediction using Recurrent Neural Network or <b>RNN</b>. Neural Network is actually said to be a design, inspired by how human <b>brains</b> work. They recognize patterns ...", "dateLastCrawled": "2022-01-31T13:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Next-generation recurrent network models for cognitive neuroscience ...", "url": "https://cbmm.mit.edu/video/next-generation-recurrent-network-models-cognitive-neuroscience", "isFamilyFriendly": true, "displayUrl": "https://cbmm.mit.edu/video/next-generation-recurrent-network-models-cognitive-neuroscience", "snippet": "Here you <b>can</b> see that you <b>can</b> decode the stimulus very well from synapses. And some networks, you cannot decode down from neurons at all towards the end of the delay period. And all of these networks, they <b>can</b> do the task just fine. So what this is telling us is this recurrent network, when endowed with short term plasticity, <b>can</b> solve delayed match to sample task with a silent working memory mechanism where at the end of the delay period the network is silent. There is no neural activity ...", "dateLastCrawled": "2022-01-26T03:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A <b>New Brain Implant Turns Thoughts Into Text</b> With 90 Percent Accuracy", "url": "https://singularityhub.com/2021/05/18/a-new-brain-implant-turns-thoughts-into-text-with-90-percent-accuracy/", "isFamilyFriendly": true, "displayUrl": "https://singularityhub.com/2021/05/18/a-<b>new-brain-implant-turns-thoughts-into-text</b>...", "snippet": "Over time, the <b>RNN</b> was able to decode neural signals and translate them into letters, which were displayed on a computer screen. It\u2019s fast: within half a second, the algorithm could guess what letter T5 was attempting to write, with 94.1 percent accuracy. Add in some run-of-the-mill autocorrect function that\u2019s in everyone\u2019s smartphones, and the accuracy bumped up to over 99 percent.", "dateLastCrawled": "2022-01-27T16:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How is <b>RNN related to deep learning</b>? - Quora", "url": "https://www.quora.com/How-is-RNN-related-to-deep-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/How-is-<b>RNN-related-to-deep-learning</b>", "snippet": "Answer (1 of 5): <b>RNN</b>, commonly known as Recurrent Neural Network is a very popular Deep Learning model which is used to carry out a number of Deep Learning tasks like Time Series prediction, Image Captioning, Google auto complete feature, etc. <b>RNN</b> as the name suggests, uses recursion technique t...", "dateLastCrawled": "2022-01-17T11:46:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Recurrent Neural Networks (<b>RNN</b>): What It Is &amp; How It Works | Built In", "url": "https://builtin.com/data-science/recurrent-neural-networks-and-lstm", "isFamilyFriendly": true, "displayUrl": "https://builtin.com/data-science/recurrent-neural-networks-and-lstm", "snippet": "Recurrent neural networks <b>can</b> form a much deeper understanding of a sequence and its context <b>compared</b> to other algorithms. What is a Recurrent Neural Network (<b>RNN</b>)? Recurrent neural networks (<b>RNN</b>) are a class of neural networks that are helpful in modeling sequence data. Derived from feedforward networks, RNNs exhibit similar behavior to how human <b>brains</b> function. Simply put: recurrent neural networks produce predictive results in sequential data that other algorithms <b>can</b>\u2019t. But when do ...", "dateLastCrawled": "2022-02-01T09:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>RNN or Recurrent Neural Network for Noobs</b> | by Debarko De \ud83e\udd81 ...", "url": "https://medium.com/hackernoon/rnn-or-recurrent-neural-network-for-noobs-a9afbb00e860", "isFamilyFriendly": true, "displayUrl": "https://medium.com/hackernoon/<b>rnn-or-recurrent-neural-network-for-noobs</b>-a9afbb00e860", "snippet": "We also base <b>our</b> decisions on previous knowledge on the subject. (**over simplified, hard to say I understand even 0.1% of human <b>brains</b>**) Where to use a <b>RNN</b>? RNNs <b>can</b> be used in a lot of ...", "dateLastCrawled": "2021-02-27T21:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>RNN</b> or Recurrent Neural Network for Noobs | HackerNoon", "url": "https://hackernoon.com/rnn-or-recurrent-neural-network-for-noobs-a9afbb00e860", "isFamilyFriendly": true, "displayUrl": "https://hackernoon.com/<b>rnn</b>-or-recurrent-neural-network-for-noobs-a9afbb00e860", "snippet": "We also base <b>our</b> decisions on previous knowledge on the subject. (**over simplified, hard to say I understand even 0.1% of human <b>brains</b>**) Where to use a <b>RNN</b>? RNNs <b>can</b> be used in a lot of different places. Following are a few examples where a lot of RNNs are used. 1. Language Modelling and Generating Text. Given a sequence of word, here we try to predict the likelihood of the next word. This is useful for translation since the most likely sentence would be the one that is correct. 2. Machine ...", "dateLastCrawled": "2022-02-02T13:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "UW Researchers Study Recurrent Neural Network Structure in the Brain ...", "url": "https://www.uwyo.edu/uw/news/2021/09/uw-researchers-study-recurrent-neural-network-structure-in-the-brain.html", "isFamilyFriendly": true, "displayUrl": "https://<b>www.uwyo.edu</b>/uw/news/2021/09/uw-researchers-study-recurrent-neural-network...", "snippet": "\u201cThe biggest surprise is that RNNs not only exist in <b>our</b> brain, but they are constructed with much more delicate function and, yet, highly efficient in processing sequential inputs,\u201d Sun says. \u201cIn general, cortical neurons are spatially reciprocal and intermingle with each other. However, Wang\u2019s data not only showed that the <b>RNN</b> does exist in the most important part of the brain -- the frontal cortex -- but additionally, this network is less complex than we thought and mostly ...", "dateLastCrawled": "2021-12-22T21:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Researchers study recurrent neural network structure in the brain ...", "url": "https://www.sciencedaily.com/releases/2021/09/210921172655.htm", "isFamilyFriendly": true, "displayUrl": "https://<b>www.sciencedaily.com</b>/releases/2021/09/210921172655.htm", "snippet": "A recurrent neural network structure exists in the most important part of the brain -- the frontal cortex -- and this network is less complex than has been thought and mostly unidirectional, new ...", "dateLastCrawled": "2022-02-01T17:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "CNN vs. <b>RNN: How are they different</b>? - SearchEnterpriseAI", "url": "https://www.techtarget.com/searchenterpriseai/feature/CNN-vs-RNN-How-they-differ-and-where-they-overlap", "isFamilyFriendly": true, "displayUrl": "https://www.techtarget.com/searchenterpriseai/feature/CNN-vs-<b>RNN</b>-How-they-differ-and...", "snippet": "Taking a hint from how the neurons in <b>our</b> <b>brains</b> work, neural network architecture introduced an algorithm that allowed the computer to fine-tune its decision-making -- in other words, to learn. An artificial neural network, or ANN, consists of many perceptrons. In its simplest form, a perceptron consists of a function that takes two inputs, multiplies them by two random weights, adds them together with a bias value, passes the results through an activation function and prints the results ...", "dateLastCrawled": "2022-01-28T05:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>RNN</b> or Recurrent Neural Network for Noobs | Bitcoin Insider", "url": "https://www.bitcoininsider.org/article/30372/rnn-or-recurrent-neural-network-noobs", "isFamilyFriendly": true, "displayUrl": "https://www.bitcoininsider.org/article/30372", "snippet": "We also base <b>our</b> decisions on previous knowledge on the subject. (**over simplified, hard to say I understand even 0.1% of human <b>brains</b>**) Where to use a <b>RNN</b>? RNNs <b>can</b> be used in a lot of different places. Following are a few examples where a lot of RNNs are used. 1. Language Modelling and Generating Text Given a sequence of word, here we try to predict the likelihood of the next word. This is useful for translation since the most likely sentence would be the one that is correct. 2. Machine ...", "dateLastCrawled": "2022-01-19T18:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Long Short-Term Memory Networks. Introduction | by Vinithavn ...", "url": "https://medium.com/analytics-vidhya/long-short-term-memory-networks-23119598b66b", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/long-short-term-memory-networks-23119598b66b", "snippet": "Long Short-Term Memory networks or LSTMs are specifically designed to overcome the disadvantages of <b>RNN</b>. LSTMs <b>can</b> preserve information for longer periods when <b>compared</b> to <b>RNN</b>. LSTMs <b>can</b> also ...", "dateLastCrawled": "2022-02-02T15:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "What is <b>RNN</b> in a neural network? - Quora", "url": "https://www.quora.com/What-is-RNN-in-a-neural-network", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>RNN</b>-in-a-neural-network", "snippet": "Answer (1 of 2): Iqbal\u2019s answer is right, but \u2026. Recurrent neural networks are *any* neural network that contains a loop. Such networks are able to give much more complex (= interesting) behaviours because they have a memory of previous inputs, which non-recurrent (also known as feedforward) net...", "dateLastCrawled": "2022-01-02T14:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Comparing Transformers and RNNs on predicting human sentence processing ...", "url": "https://deepai.org/publication/comparing-transformers-and-rnns-on-predicting-human-sentence-processing-data", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/comparing-transformers-and-<b>rnn</b>s-on-predicting-human...", "snippet": "They <b>compared</b> SRNs, LSTMs and GRUs; three <b>RNN</b> types differing in how they integrate their \u2018memory\u2019 of past input with the next input in the sequence, on human reading data from three psycholinguistic experiments. Despite the GRU and LSTM generally outperforming the SRN on NLP tasks, Aurnhammer and Frank found no difference in how well the models\u2019 surprisal predicted human processing effort in the self-paced reading, eye-tracking and EEG experiments. The Transformer has to the best of ...", "dateLastCrawled": "2021-12-17T13:30:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A Tour of <b>Recurrent Neural Network Algorithms for Deep Learning</b>", "url": "https://machinelearningmastery.com/recurrent-neural-network-algorithms-for-deep-learning/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>recurrent-neural-network-algorithms-for-deep-learning</b>", "snippet": "RNNs stand out from other <b>machine</b> <b>learning</b> methods for their ability to learn and carry out complicated transformations of data over extended periods of time. Moreover, it is known that RNNs are Turing-Complete and therefore have the capacity to simulate arbitrary procedures, if properly wired. The capabilities of standard RNNs are extended to simplify the solution of algorithmic tasks. This enrichment is primarily via a large, addressable memory, so, by <b>analogy</b> to Turing\u2019s enrichment of ...", "dateLastCrawled": "2022-02-02T22:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Mathematical understanding of <b>RNN</b> and its variants - <b>GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/mathematical-understanding-of-rnn-and-its-variants/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/mathematical-understanding-of-<b>rnn</b>-and-its-variants", "snippet": "<b>RNN</b> is suitable for such work thanks to their capability of <b>learning</b> the context. Other applications include speech to text conversion, building virtual assistance, time-series stocks forecasting, sentimental analysis, language modelling and <b>machine</b> translation. On the other hand, a feed-forward neural network produces an output which only depends on the current input. Examples for such are image classification task, image segmentation or object detection task. One such type of such network ...", "dateLastCrawled": "2022-01-29T04:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Deep Learning: Models for Sequence Data</b> (<b>RNN</b> and LSTM)", "url": "https://cse.iitk.ac.in/users/piyush/courses/ml_autumn16/771A_lec24_slides.pdf", "isFamilyFriendly": true, "displayUrl": "https://cse.iitk.ac.in/users/piyush/courses/ml_autumn16/771A_lec24_slides.pdf", "snippet": "Filters are like basis/dictionary (PCA <b>analogy</b>) Each lter is convolved over entire input to produce a feature map Nonlinearity and pooling and applied after each convolution layer Last layer (one that connects to outputs) is fully connected <b>Machine</b> <b>Learning</b> (CS771A) <b>Deep Learning: Models for Sequence Data</b> (<b>RNN</b> and LSTM) 3. Recap: Convolutional Neural Network Special type of feedforward neural nets (local connectivity + weight sharing) Each layer uses a set of \\ lters&quot; (basically, weights to ...", "dateLastCrawled": "2022-01-17T20:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Machine</b> <b>Learning</b> (ML) and Neural Networks (NN)\u2026 An Intuitive ...", "url": "https://medium.com/visionary-hub/machine-learning-ml-and-neural-networks-nn-an-intuitive-walkthrough-76bdaba8b0e3", "isFamilyFriendly": true, "displayUrl": "https://medium.com/visionary-hub/<b>machine</b>-<b>learning</b>-ml-and-neural-networks-nn-an...", "snippet": "A better <b>analogy</b> for unsupervised <b>learning</b>, and one that\u2019s more commonly used, is separating a group of blocks by colour. Suppose we have 10 blocks, each with different coloured faces. In the ...", "dateLastCrawled": "2022-01-30T17:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Chapter 8 Recurrent Neural Networks</b> | Deep <b>Learning</b> and its Applications", "url": "https://frcs.github.io/4C16-LectureNotes/recurrent-neural-networks.html", "isFamilyFriendly": true, "displayUrl": "https://frcs.github.io/4C16-LectureNotes/recurrent-neural-networks.html", "snippet": "In its simplest form, the inner structure of the hidden layer block is simply a dense layer of neurons with \\(\\mathrm{tanh}\\) activation. This is called a simple <b>RNN</b> architecture or Elman network.. We usually take a \\(\\mathrm{tanh}\\) activation as it can produce positive or negative values, allowing for increases and decreases of the state values. Also \\(\\mathrm{tanh}\\) bounds the state values between -1 and 1, and thus avoids a potential explosion of the state values.. The equations for ...", "dateLastCrawled": "2022-02-02T05:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Python <b>RNN</b>: Recurrent Neural Networks for Time Series Forecasting | by ...", "url": "https://towardsdatascience.com/temporal-loops-intro-to-recurrent-neural-networks-for-time-series-forecasting-in-python-b0398963dc1f", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/temporal-loops-intro-to-recurrent-neural-networks-for...", "snippet": "We have put a relatively fine-toothed comb to the <b>learning</b> rate, 0.001, and the epochs, 300, in our setup of the <b>RNN</b> model. We could also play with the dropout parameter (to make the <b>RNN</b> try out various subsets of nodes during training); and with the size of the hidden state (a higher hidden dimension value increases the <b>RNN</b>\u2019s capability to deal with more intricate patterns over longer time frames). A tuning algorithm could tweak them while rerunning the fitting process to try to achieve ...", "dateLastCrawled": "2022-02-02T15:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Sentiment Analysis</b> from Tweets using Recurrent Neural Networks | by ...", "url": "https://medium.com/@gabriel.mayers/sentiment-analysis-from-tweets-using-recurrent-neural-networks-ebf6c202b9d5", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@gabriel.mayers/<b>sentiment-analysis</b>-from-tweets-using-recurrent...", "snippet": "LSTM Architeture. This is a variation from <b>RNN</b> and very powerful alternative when you need that your network is able to memorize information for a longer period of time. LSTM is based in gates ...", "dateLastCrawled": "2022-01-23T01:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A Gentle Introduction to <b>Long Short-Term Memory</b> Networks by the Experts", "url": "https://machinelearningmastery.com/gentle-introduction-long-short-term-memory-networks-experts/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/gentle-introduction-<b>long-short-term-memory</b>-networks...", "snippet": "<b>Long Short-Term Memory</b> (LSTM) networks are a type of recurrent neural network capable of <b>learning</b> order dependence in sequence prediction problems. This is a behavior required in complex problem domains like <b>machine</b> translation, speech recognition, and more. LSTMs are a complex area of deep <b>learning</b>. It can be hard to get your hands around what LSTMs are, and how terms like bidirectional", "dateLastCrawled": "2022-01-31T03:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Coursera: Neural Networks and Deep Learning</b> (Week 1) Quiz [MCQ Answers ...", "url": "https://www.apdaga.com/2019/03/coursera-neural-networks-and-deep-learning-week-1-quiz.html", "isFamilyFriendly": true, "displayUrl": "https://www.apdaga.com/2019/03/<b>coursera-neural-networks-and-deep-learning</b>-week-1-quiz.html", "snippet": "Recommended <b>Machine</b> <b>Learning</b> Courses: ... edX: <b>Machine</b> <b>Learning</b>; Fast.ai: Introduction to <b>Machine</b> <b>Learning</b> for Coders; What does the <b>analogy</b> \u201cAI is the new electricity\u201d refer to? AI runs on computers and is thus powered by electricity, but it is letting computers do things not possible before. Similar to electricity starting about 100 years ago, AI is transforming multiple industries. Correct. Yes. AI is transforming many fields from the car industry to agriculture to supply-chain ...", "dateLastCrawled": "2022-01-30T01:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "deep-<b>learning</b>-coursera/Week 1 Quiz - Introduction to deep <b>learning</b>.md ...", "url": "https://github.com/Kulbear/deep-learning-coursera/blob/master/Neural%20Networks%20and%20Deep%20Learning/Week%201%20Quiz%20-%20Introduction%20to%20deep%20learning.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/Kulbear/deep-<b>learning</b>-coursera/blob/master/Neural Networks and Deep...", "snippet": "Why is an <b>RNN</b> (Recurrent Neural Network) used for <b>machine</b> translation, say translating English to French? (Check all that apply.) It can be trained as a supervised <b>learning</b> problem. It is strictly more powerful than a Convolutional Neural Network (CNN). It is applicable when the input/output is a sequence (e.g., a sequence of words).", "dateLastCrawled": "2022-02-03T05:16:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Introduction to Recurrent Neural Networks | <b>Machine</b> <b>Learning</b> lab", "url": "https://en.mlab.ai/blog/introduction-recurrent-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://en.mlab.ai/blog/introduction-recurrent-neural-networks", "snippet": "The <b>Machine</b> <b>Learning</b> Blog. 09/27/2018. Introduction to Recurrent Neural Networks In this article, I will explain what are Recurrent Neural Networks (RNN), how they work and what you can do with them. I will also show a very cool example of music generation using artificial intelligence. However, before discussing RNN, we need to explain the concept of sequence data. Sequence Data As the name indicates, sequence data is a collection of data in different states through time so it can form ...", "dateLastCrawled": "2022-02-01T17:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Machine Learning for NLP</b> - Aurelie Herbelot", "url": "http://aurelieherbelot.net/resources/slides/teaching/RNNs.pdf", "isFamilyFriendly": true, "displayUrl": "aurelieherbelot.net/resources/slides/teaching/RNNs.pdf", "snippet": "An RNN, step by step Now we backpropagate through time. We need to compute gradients for three matrices: Why, Whh and Wxh. The gradient of matrix Why is straightforward \u2013 it is simply the sum", "dateLastCrawled": "2021-09-18T14:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Notes on Recurrent Neural Networks</b> \u2013 humblesoftwaredev", "url": "https://humblesoftwaredev.wordpress.com/2016/12/04/notes-on-recurrent-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://humblesoftwaredev.wordpress.com/2016/12/04/<b>notes-on-recurrent-neural-networks</b>", "snippet": "Recurrent neural nets have states, unlike feed-forward networks. An analogy for RNN is the C strtok function, where calling it with the same parameter typically yields a different value (but of course, unlike strtok, RNN does not modify the input). An analogy for feed-forward networks is a function in the mathematical sense, where y=f(x) regardless of how many times\u2026", "dateLastCrawled": "2022-01-14T08:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "State-of-the-art in artificial <b>neural network applications</b>: A survey ...", "url": "https://www.sciencedirect.com/science/article/pii/S2405844018332067", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2405844018332067", "snippet": "Unlike a recurrent neural network, an <b>RNN is like</b> a hierarchical network where the input need processing hierarchically in the form of a tree because there is no time to the input sequence. 2.4. Deep <b>learning</b>. Artificial intelligence (AI) has existed over many decades, and the field is wide. AI can be view as a set that contains <b>machine</b> <b>learning</b> (ML), and deep <b>learning</b> (DL). The ML is a subset of AI, meanwhile, DL, in turn, a subset of ML. That is DL is an aspect of AI; the term deep ...", "dateLastCrawled": "2022-01-27T17:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>NLP - Transformers</b> | Blog Posts | Lumenci", "url": "https://www.lumenci.com/post/nlp-transformers", "isFamilyFriendly": true, "displayUrl": "https://www.lumenci.com/post/<b>nlp-transformers</b>", "snippet": "Thus, because weights are shared across time, <b>RNN is like</b> a state <b>machine</b> that takes actions temporally based on its historical sequential information. For example, RNN can be trained on a sequence of characters to generate the next character correctly. RNN - The activation at each time step is feedback to the next time step. For many years, RNN and its gated variants were the most popular architectures used for NLP. However, one of the main problems with RNN is the vanishing gradient ...", "dateLastCrawled": "2022-01-26T07:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Very simple example of RNN</b>? : learnmachinelearning", "url": "https://www.reddit.com/r/learnmachinelearning/comments/84bk5r/very_simple_example_of_rnn/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/learn<b>machinelearning</b>/comments/84bk5r/<b>very_simple_example_of_rnn</b>", "snippet": "basically, an <b>RNN is like</b> a regular layer (the dense layer where all neurons are connected to the next layer&#39;s neurons), except that it takes as an additional paramenter its own output from the previous training iteration.", "dateLastCrawled": "2021-01-08T07:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Deep <b>Learning Approaches for Phantom Movement Recognition</b>", "url": "https://www.researchgate.net/publication/336367291_Deep_Learning_Approaches_for_Phantom_Movement_Recognition", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/336367291_Deep_<b>Learning</b>_Approaches_for...", "snippet": "<b>RNN is, like</b> MLP, only. have good results for T A WD while other region successes are. far behind other algorithms. For <b>machine</b> <b>learning</b> algorithms, cross validation (k=10) is used to split the ...", "dateLastCrawled": "2022-01-04T05:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Artificial intelligence in drug design: algorithms, applications ...", "url": "https://www.future-science.com/doi/full/10.4155/fdd-2020-0028", "isFamilyFriendly": true, "displayUrl": "https://www.future-science.com/doi/full/10.4155/fdd-2020-0028", "snippet": "The discovery paradigm of drugs is rapidly growing due to advances in <b>machine</b> <b>learning</b> (ML) and artificial intelligence (AI). This review covers myriad faces of AI and ML in drug design. There is a plethora of AI algorithms, the most common of which are summarized in this review. In addition, AI is fraught with challenges that are highlighted along with plausible solutions to them. Examples are provided to illustrate the use of AI and ML in drug discovery and in predicting drug properties ...", "dateLastCrawled": "2022-01-29T02:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "State-of-the-art <b>in artificial neural network applications: A</b> survey", "url": "https://www.researchgate.net/publication/329149409_State-of-the-art_in_artificial_neural_network_applications_A_survey", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/329149409_State-of-the-art_in_artificial...", "snippet": "ANNs are one type of model for <b>machine</b> <b>learning</b> (ML) and has become . relatively competitive to conventional regression and stat istical models regarding. usefulness [1]. Currently, arti \ufb01 cial ...", "dateLastCrawled": "2022-01-29T22:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>The future of AI music is Magenta</b> | DataDrivenInvestor", "url": "https://www.datadriveninvestor.com/2020/04/25/the-future-of-ai-music-is-magenta/", "isFamilyFriendly": true, "displayUrl": "https://www.datadriveninvestor.com/2020/04/25/<b>the-future-of-ai-music-is-magenta</b>", "snippet": "<b>The future of AI music is Magenta</b>. Music seems to be one of the fields that, at a surface level at least, AI just can\u2019t seem to penetrate. AI is rapidly taking over so many fields, and there\u2019s huge progress in music too! There are so many awesome developments (check out the app Transformer) and progress is moving at a breakneck pace.", "dateLastCrawled": "2022-01-28T07:09:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "End to end <b>machine</b> <b>learning</b> for fault detection and classification in ...", "url": "https://www.sciencedirect.com/science/article/pii/S0378779621004119", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0378779621004119", "snippet": "The training process for <b>RNN is similar</b> to traditional ANNs. However, since the parameters are shared among time instances in RNNs, the back-propagation algorithm for RNNs is termed as Backpropagation through time (BPTT) . As the number of time steps increase in RNN, it faces a problem termed as \u201cvanishing gradients\u201d due to which it cannot retain long term dependencies. Description can be seen in 39,40]. This phenomenon makes RNNs difficult to train and render them impractical in most of ...", "dateLastCrawled": "2021-12-14T22:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "2_tensorflow_lstm", "url": "http://ethen8181.github.io/machine-learning/deep_learning/rnn/2_tensorflow_lstm.html", "isFamilyFriendly": true, "displayUrl": "ethen8181.github.io/<b>machine</b>-<b>learning</b>/deep_<b>learning</b>/rnn/2_tensorflow_lstm.html", "snippet": "Training a <b>RNN is similar</b> to training a traditional Neural Network, we also use the backpropagation algorithm, but with a little twist. Because the parameters are shared by all time steps in the network, the gradient at each output depends not only on the calculations of the current time step, but also the previous time steps. For example, in order to calculate the gradient at t=4 we would need to backpropagate 3 steps and sum up the gradients. This is called Backpropagation Through Time ...", "dateLastCrawled": "2022-02-03T13:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Deep <b>Recurrent Neural Networks</b> with Keras | <b>Paperspace Blog</b>", "url": "https://blog.paperspace.com/advanced-recurrent-neural-networks-deep-rnns/", "isFamilyFriendly": true, "displayUrl": "https://blog.paperspace.com/advanced-<b>recurrent-neural-networks</b>-deep-rnns", "snippet": "The training of a deep <b>RNN is similar</b> to the Backpropagation Through Time (BPTT) algorithm, as in an RNN but with additional hidden units. Now that you\u2019ve got an idea of what a deep RNN is, in the next section we&#39;ll build a music generator using a deep RNN and Keras. Generating Music Using a Deep RNN. Music is the ultimate language. We have been creating and rendering beautiful melodies since time unknown. In this context, do you think a computer can generate musical notes comparable to ...", "dateLastCrawled": "2022-02-03T18:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Machine</b> <b>Learning</b> - <b>Kbeznak Parmatonic</b>", "url": "https://sites.google.com/view/kbeznak-parmatonic-guru-of-ml/home", "isFamilyFriendly": true, "displayUrl": "https://<b>sites.google.com</b>/view/<b>kbeznak-parmatonic</b>-guru-of-ml/home", "snippet": "Backpropagation in <b>RNN is similar</b> to Neural Network, but we have to take care of the weight with respect to all the time steps. So, the gradient has to be calculated for all those steps going backwards, this is called Backpropagation Through Time(BPTT). Software and Tools: <b>Kbeznak Parmatonic</b> prefers Tensorflow and Caffe2 for deeplearning, and keras would help you lot in the initial stages. Author <b>Kbeznak Parmatonic</b>: Dr. <b>Kbeznak Parmatonic</b>, was a chief scientist at NASA and was well deserved ...", "dateLastCrawled": "2021-12-23T18:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A Motor-Imagery BCI System Based on Deep <b>Learning</b> Networks and Its ...", "url": "https://www.intechopen.com/chapters/60241", "isFamilyFriendly": true, "displayUrl": "https://www.intechopen.com/chapters/60241", "snippet": "Training an <b>RNN is similar</b> to training a traditional neural network (TNN). Because RNNs trained by TNN\u2019s style have difficulties in <b>learning</b> long-term dependencies due to the vanishing and exploding gradient problem. LSTMs do not have a fundamentally different architecture from RNNs, but they use a different function to calculate the states in hidden layer. The memory in LSTMs is called cells and can be thought as black boxes that take as input the previous state and current input ...", "dateLastCrawled": "2022-02-02T11:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Review of Vibration-Based Structural Health Monitoring Using Deep <b>Learning</b>", "url": "https://www.mdpi.com/2076-3417/10/5/1680/htm", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/2076-3417/10/5/1680/htm", "snippet": "An <b>RNN is similar</b> to recurrent neural networks in that it is good at dealing with sequential data. Recurrent neural networks are also called RNNs in the literature; to distinguish between the architectures, only the recursive neural network is abbreviated as RNN in this paper. An RNN models hierarchical structures in a tree fashion, which is overly time-consuming and costly. This has led to a lack of attention being given to RNNs. Because an RNN processes all information of the input ...", "dateLastCrawled": "2022-01-12T00:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Deep Neural Network</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/computer-science/deep-neural-network", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/computer-science/<b>deep-neural-network</b>", "snippet": "This dataset is designed for <b>machine</b> <b>learning</b> classification tasks and includes 60,000 training and 10,000 test gray scale images composed of 28-by-28 pixels. Every training and test case is related to one of ten labels (0\u20139). Zalando\u2019s new dataset is mainly the same as the original handwritten digits data. But instead of having images of the digits 0\u20139, Zalando\u2019s data involves images with 10 different fashion products. Hence the dataset is named fashion-MNIST dataset and can be ...", "dateLastCrawled": "2022-01-30T06:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Deep Learning</b> - SlideShare", "url": "https://www.slideshare.net/JunWang5/deep-learning-61493694", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/JunWang5/<b>deep-learning</b>-61493694", "snippet": "\u2022 ClockWork-<b>RNN is similar</b> to a simple RNN with an input, output and hidden layer \u2022 Difference lies in \u2013 The hidden layer is partitioned into g modules each with its own clock rate \u2013 Neurons in faster module are connected to neurons in a slower module RNN applications: time series Koutnik, Jan, et al. &quot;A clockwork rnn.&quot; arXiv preprint arXiv:1402.3511 (2014). A Clockwork RNN Figure 1. CW-RNN architecture is similar to a simple RNN with an input, output and hidden layer. The hidden ...", "dateLastCrawled": "2022-01-31T18:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Deep Learning</b> for Geophysics: Current and Future Trends - Yu - 2021 ...", "url": "https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2021RG000742", "isFamilyFriendly": true, "displayUrl": "https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2021RG000742", "snippet": "Different from traditional model-driven methods, <b>machine</b> <b>learning</b> (ML) is a type of data-driven approach that trains a regression or classification model through a complex nonlinear mapping with adjustable parameters based on a training data set. The comparison of model-driven and data-driven approaches is summarized in Figure 1. For decades, ML methods have been widely adopted in various geophysical applications, such as exploration geophysics (Huang et al., 2006; Helmy et al., 2010; Jia ...", "dateLastCrawled": "2022-01-31T08:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Different Architecture of Deep <b>Learning</b> Algorithms Extensive number of ...", "url": "https://www.researchgate.net/figure/Different-Architecture-of-Deep-Learning-Algorithms-Extensive-number-of-deep-learning_fig1_324149367", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/figure/Different-Architecture-of-Deep-<b>Learning</b>-Algorithms...", "snippet": "Unlike classical <b>machine</b> <b>learning</b> (support vector <b>machine</b>, k-nearest neighbour, k-mean, etc.) that require a human engineered feature to perform optimally (LeCun, et al., 2015). Over the years ...", "dateLastCrawled": "2022-01-29T15:47:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Towards deep entity resolution via soft schema matching - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S0925231221016635", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0925231221016635", "snippet": "Technically, TLM is a new fundamental architecture for deep ER, <b>just as RNN</b>. Our work and TLM based approaches falls into different lines of deep ER research, which are orthogonal and complementary to each other. Our major contribution is proposing soft schema mapping and incorporating it into (RNN based) deep ER models, which does not require huge amounts of NLP corpora for pre-training, while TLM based approaches exploit the deeper language understanding capability from tremendously pre ...", "dateLastCrawled": "2022-01-21T02:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Positional encoding, residual connections, padding masks</b>: covering the ...", "url": "https://data-science-blog.com/blog/2021/04/22/positional-encoding-residual-connections-padding-masks-all-the-details-of-transformer-model/", "isFamilyFriendly": true, "displayUrl": "https://data-science-blog.com/blog/2021/04/22/positional-encoding-residual-connections...", "snippet": "Transformer decoder also predicts the output sequences autoregressively one token at a time step, <b>just as RNN</b> decoders. I think it easy to understand this process because RNN decoder generates tokens just as you connect RNN cells one after another, like connecting rings to a chain. In this way it is easy to make sure that generating of one token in only affected by the former tokens. On the other hand, during training Transformer decoders, you input the whole sentence at once. That means ...", "dateLastCrawled": "2022-01-30T16:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Machine</b> <b>Learning</b> Archives - Data Science Blog", "url": "https://data-science-blog.com/blog/category/main-category/machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://data-science-blog.com/blog/category/main-category/<b>machine</b>-<b>learning</b>", "snippet": "Most <b>machine</b> <b>learning</b> algorithms covered by major introductory textbooks tend to be too deterministic and dependent on the size of data. Many of those algorithms have another \u201cparallel world,\u201d where you can handle inaccuracy in better ways. I hope I can also write about them, and I might prepare another trilogy for such PCA. But I will not disappoint you, like \u201cThe Phantom Menace.\u201d Appendix: making a model of a bunch of grape with ellipsoid berries. If you can control quadratic ...", "dateLastCrawled": "2022-01-05T04:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "1561982779 | PDF | Equity Crowdfunding | Investor", "url": "https://www.scribd.com/document/550868164/1878586842-1561982779", "isFamilyFriendly": true, "displayUrl": "https://www.scribd.com/document/550868164/1878586842-1561982779", "snippet": "Scribd is the world&#39;s largest social reading and publishing site.", "dateLastCrawled": "2022-01-25T03:48:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Recurrent Neural Networks and LSTM explained", "url": "https://dhrubajitdas44.blogspot.com/2018/10/recurrent-neural-networks-and-lstm.html", "isFamilyFriendly": true, "displayUrl": "https://dhrubajitdas44.blogspot.com/2018/10/recurrent-neural-networks-and-lstm.html", "snippet": "A <b>RNN can be thought of as</b> multiple copies of the same network , each passing message to . the next. Because of their internal memory, RNN\u2019s are able to remember important things about the input they received, which enables them to be very precise in predicting what\u2019s coming next. This is the reason why they are the preferred algorithm for sequential data like time series, speech, text, financial data, audio, video, weather and much more because they can form a much deeper understanding ...", "dateLastCrawled": "2022-01-10T10:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Introduction to Sequence Learning Models</b>: RNN, LSTM, GRU", "url": "https://www.researchgate.net/publication/350950396_Introduction_to_Sequence_Learning_Models_RNN_LSTM_GRU", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/350950396_Introduction_to_Sequence_<b>Learning</b>...", "snippet": "an <b>RNN can be thought of as</b> multiple copies (in t ime) of the same network, ... In International conference on <b>machine</b> <b>learning</b> (pp. 1310-1318). [13] Williams, R. J., &amp; Zipser, D. (1989). A ...", "dateLastCrawled": "2022-02-03T13:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "PHC6937-<b>Machine</b> <b>Learning</b>-Guest Lecture", "url": "https://slides.com/benh-hu/phc6937machinelearning", "isFamilyFriendly": true, "displayUrl": "https://slides.com/benh-hu/phc6937<b>machinelearning</b>", "snippet": "<b>Machine</b> <b>learning</b> is predicated on this idea of <b>learning</b> from example ... A <b>RNN can be thought of as</b> the addition of loops to the archetecture of a standard feedforward NN - the output of the network may feedback as an input to the network with the next input vector, and so on The recurrent connections add state or memory to the network and allow it to learn broader abstractions from the input sequences; Reading. PHC6937-<b>Machine</b> <b>Learning</b>-Guest Lecture. By Hui Hu. PHC6937-<b>Machine</b> <b>Learning</b> ...", "dateLastCrawled": "2022-01-25T13:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Decoding Your Genes</b>. Can Neural Networks Unravel The Secrets\u2026 | by ...", "url": "https://towardsdatascience.com/decoding-your-genes-4a23e89aba98", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>decoding-your-genes</b>-4a23e89aba98", "snippet": "Conceptually, an <b>RNN can be thought of as</b> a connected sequence of feed-forward networks with information passed between them. The information being passed is the hidden-state which represents all the previous inputs to the network. At each step of the RNN, the hidden state generated from the previous step is passed in, as well as the next sequence input. This then returns an output as well as the new hidden state to be passed on again. This allows the RNN to retain a \u2018memory\u2019 of the ...", "dateLastCrawled": "2022-01-26T03:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Using RNNs for <b>Machine Translation</b> | by Aryan Misra | Towards Data Science", "url": "https://towardsdatascience.com/using-rnns-for-machine-translation-11ddded78ddf", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/using-rnns-for-<b>machine-translation</b>-11ddded78ddf", "snippet": "3. Sequence to Sequence. The RNN takes in an input sequence and outputs a sequence. <b>Machine Translation</b>: an RNN reads a sentence in one language and then outputs it in another. This should help you get a high-level understanding of RNNs, if you want to learn more about the math behind the operations an RNN performs, I recommend you check out ...", "dateLastCrawled": "2022-02-01T23:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "[DL] 11. RNN <b>2(Bidirectional, Deep RNN, Long term connection</b>) | by Jun ...", "url": "https://medium.com/jun-devpblog/dl-11-rnn-2-bidirectional-deep-rnn-long-term-connection-8a836a7f2260", "isFamilyFriendly": true, "displayUrl": "https://medium.com/jun-devpblog/dl-11-rnn-<b>2-bidirectional-deep-rnn-long-term</b>...", "snippet": "Basically, Bidirectional <b>RNN can be thought of as</b> two RNNs in a network, one is moving forwards in time and the other one is moving backward and both are contributing to producing output ...", "dateLastCrawled": "2021-08-12T14:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Time series prediction of COVID-19 transmission in America using LSTM ...", "url": "https://www.sciencedirect.com/science/article/pii/S2211379721005775", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2211379721005775", "snippet": "The <b>machine</b> <b>learning</b> algorithm XGBoost was employed to build the models to predict the criticality , mortality , and ... RNNs can use their internal state (memory) to process variable length sequences of inputs. A <b>RNN can be thought of as</b> multiple copies of the same network, each passing a message to a successor (see Fig. 4). They might be able to connect previous information to the present task. However, as that gap grows, RNNs become unable to learn to connect the information. The short ...", "dateLastCrawled": "2022-01-24T06:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Convolutional Neural Network and RNN</b> for OCR problem.", "url": "https://www.slideshare.net/vishalmishra982/convolutional-neural-network-and-rnn-for-ocr-problem-86087045", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/vishalmishra982/<b>convolutional-neural-network-and-rnn</b>-for...", "snippet": "Sequence-to-Sequence <b>Learning</b> using Deep <b>Learning</b> for Optical Character Recognition. ... <b>RNN can be thought of as</b> multiple copies of the same network, each passing a message to a successor. An unrolled RNN is shown below. \u2022 In fast last few years, there have been incredible success applying RNNs to a variety of problems: speech recognition, language modeling, translation, image captioning\u2026. The list goes on. An Unrolled RNN 44. DRAWBACK OF AN RNN \u2022 RNN has a problem of long term ...", "dateLastCrawled": "2022-01-17T23:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "[DL] 11. RNN 2(Bidirectional, Deep RNN, Long term connection) | by temp ...", "url": "https://medium.com/temp08050309-devpblog/dl-11-rnn-2-bidirectional-deep-rnn-long-term-connection-8a836a7f2260", "isFamilyFriendly": true, "displayUrl": "https://medium.com/temp08050309-devpblog/dl-11-rnn-2-bidirectional-deep-rnn-long-term...", "snippet": "The typical RNN structures we\u2019ve looked at in the last chapter are causal. The term causal means that the output of RNN at time step t only regards the information of input from 0 to t\u20131. The ...", "dateLastCrawled": "2021-12-30T10:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Training Spiking Neural Networks Using Lessons From Deep <b>Learning</b> | DeepAI", "url": "https://deepai.org/publication/training-spiking-neural-networks-using-lessons-from-deep-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/.../training-spiking-neural-networks-using-lessons-from-deep-<b>learning</b>", "snippet": "The brain is the perfect place to look for inspiration to develop more efficient neural networks.The inner workings of our synapses and neurons provide a glimpse at what the future of deep <b>learning</b> might look like. This paper serves as a tutorial and perspective showing how to apply the lessons learnt from several decades of research in deep <b>learning</b>, gradient descent, backpropagation and neuroscience to biologically plausible spiking neural neural networks. We also explore the delicate ...", "dateLastCrawled": "2022-01-18T15:49:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(rnn)  is like +(our brains)", "+(rnn) is similar to +(our brains)", "+(rnn) can be thought of as +(our brains)", "+(rnn) can be compared to +(our brains)", "machine learning +(rnn AND analogy)", "machine learning +(\"rnn is like\")", "machine learning +(\"rnn is similar\")", "machine learning +(\"just as rnn\")", "machine learning +(\"rnn can be thought of as\")", "machine learning +(\"rnn can be compared to\")"]}