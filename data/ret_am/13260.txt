{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Optimization</b> - blog.pollithy.com", "url": "https://blog.pollithy.com/personal/python/optimization", "isFamilyFriendly": true, "displayUrl": "https://blog.pollithy.com/personal/python/<b>optimization</b>", "snippet": "<b>Convex</b> <b>optimization</b>. Linear least squares is the method of <b>finding</b> a line that best fits given data points. \u201cBest fit\u201d means that the sum of the squared distances between a <b>point</b> and the line is minimal. The vertical distance between the <b>point</b> and the line gets called residual.", "dateLastCrawled": "2021-09-17T01:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>How to Master Function Optimization in Deep Learning</b> | Abe.ai", "url": "https://www.abe.ai/blog/deep-learning-function-optimization/", "isFamilyFriendly": true, "displayUrl": "https://www.abe.ai/blog/deep-learning-function-<b>optimization</b>", "snippet": "Obviously, the <b>ball</b> will roll down towards the bottom of the cup, then it will oscillate a bit, eventually landing towards <b>the lowest</b> <b>point</b> of the cup. This is called the minimum, and the mathematical function that describes the cup is a <b>convex</b> function. It turns out that the minimum, in this case, is also the global minimum. In fact, the <b>ball</b> cannot go lower than that. Moreover, there is just one such <b>point</b>. The <b>ball</b> cannot go anywhere else. The minimum is global and it is unique. And all ...", "dateLastCrawled": "2022-01-29T19:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>Convex Optimization</b> | Alex Lee - Academia.edu", "url": "https://www.academia.edu/28652058/Convex_Optimization", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/28652058/<b>Convex_Optimization</b>", "snippet": "<b>Convex Optimization</b>. Alex Lee. Download PDF. Download Full PDF Package. This paper. A short summary of this paper. 37 Full PDFs related to this paper. Read Paper. <b>Convex Optimization</b> . Download ...", "dateLastCrawled": "2021-08-28T12:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Types of Optimization Algorithms used in Neural</b> Networks and Ways to ...", "url": "https://medium.com/nerd-for-tech/types-of-optimization-algorithms-used-in-neural-networks-and-ways-to-optimize-gradient-descent-1e32cdcbcf6c", "isFamilyFriendly": true, "displayUrl": "https://medium.com/nerd-for-tech/<b>types-of-optimization-algorithms-used-in-neural</b>...", "snippet": "<b>Optimization</b> Technique is that does not neglect or ignore the curvature of the <b>Surface</b>. Secondly, in terms of Step-wise Performance, they are better. Secondly, in terms of Step-wise Performance ...", "dateLastCrawled": "2022-02-01T03:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Convex</b> <b>Optimization</b> (Stanford CVX101) 9780521833783, 0521833787 ...", "url": "https://dokumen.pub/convex-optimization-stanford-cvx101-9780521833783-0521833787-2003063284.html", "isFamilyFriendly": true, "displayUrl": "https://dokumen.pub/<b>convex</b>-<b>optimization</b>-stanford-cvx101-9780521833783-0521833787...", "snippet": "Using <b>convex</b> <b>optimization</b> Using <b>convex</b> <b>optimization</b> is, at least conceptually, very much <b>like</b> using leastsquares or linear programming. If we can formulate a problem as a <b>convex</b> <b>optimization</b> problem, then we can solve it efficiently, just as we can solve a least-squares problem efficiently. With only a bit of exaggeration, we can say that, if you formulate a practical problem as a <b>convex</b> <b>optimization</b> problem, then you have solved the original problem. There are also some important ...", "dateLastCrawled": "2022-01-03T23:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Neural Networks | Fundamentals</b>. Here is an article in which I will try ...", "url": "https://towardsdatascience.com/neural-networks-fundamentals-1b4c46e7dbfe", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>neural-networks-fundamentals</b>-1b4c46e7dbfe", "snippet": "Now, imagine that a <b>ball</b> is dropped inside a rounded bucket (the <b>convex</b> function), it just has to reach the bottom of it. This is <b>optimization</b>. In the case of the gradient descent, it will have to move from left to right in order to optimize its position.", "dateLastCrawled": "2022-02-03T01:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "calculus - <b>Optimization</b> with cylinder - Mathematics Stack Exchange", "url": "https://math.stackexchange.com/questions/127569/optimization-with-cylinder", "isFamilyFriendly": true, "displayUrl": "https://math.stackexchange.com/questions/127569/<b>optimization</b>-with-cylinder", "snippet": "The volume of a cylindrical can is given by \u03c0 r 2 h, where r is the radius of the base and h is the height. The area of the <b>surface</b> is given by: 2 \u03c0 r h (-area of the side)+ \u03c0 r 2 (-area of the bottom), there is no top. From the given V, you can express h = V \u03c0 r 2. Substitute to the second equation to get S ( r) = 2 V r + \u03c0 r 2.", "dateLastCrawled": "2022-01-27T21:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "College Park Tutors - Blog - Calculus - <b>Optimization</b>: using calculus to ...", "url": "https://collegeparktutors.com/blog/calculus-optimization-finding-maximum-area", "isFamilyFriendly": true, "displayUrl": "https://collegeparktutors.com/blog/calculus-<b>optimization</b>-<b>finding</b>-maximum-area", "snippet": "<b>Optimization</b>, or <b>finding</b> the maximums or minimums of a function, is one of the first applications of the derivative you&#39;ll learn in college calculus. In this video, we&#39;ll go over an example where we find the dimensions of a corral (animal pen) that maximizes its area, subject to a constraint on its perimeter. Other types of <b>optimization</b> problems that commonly come up in calculus are: Maximizing the volume of a box or other container Minimizing the cost or <b>surface</b> area of a container ...", "dateLastCrawled": "2022-01-30T04:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "What are the different <b>optimization</b> algorithm faster than gradient ...", "url": "https://ml1.quora.com/What-are-the-different-optimization-algorithm-faster-than-gradient-descent", "isFamilyFriendly": true, "displayUrl": "https://ml1.quora.com/What-are-the-different-<b>optimization</b>-algorithm-faster-than...", "snippet": "The lake is <b>the lowest</b> <b>point</b> in 360 degrees all around, but over one of those ridges somewhere is a path to the ocean\u2026 it&#39;s just that you&#39;ll have to climb a bit to get over that ridge before making your way to the ocean. A mountain lake is a \u201clocal minimum\u201d and gradient descent is going to get stuck there.", "dateLastCrawled": "2022-01-29T05:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Math: How to <b>Find the Minimum and Maximum of a Function</b>", "url": "https://owlcation.com/stem/Math-How-to-Find-the-Minimum-and-Maximum-of-a-Function", "isFamilyFriendly": true, "displayUrl": "https://owlcation.com/stem/Math-How-to-<b>Find-the-Minimum-and-Maximum-of-a-Function</b>", "snippet": "This is because it is <b>the lowest</b> <b>point</b> in its neighborhood. Therefore the slope of the function goes from negative to positive, since the function was decreasing until it reached the minimum and then it started increasing again. This means that in the local minimum, the slope is equal to zero, and hence the derivative of the function must be equal to zero in the <b>point</b> that is the minimum. The same holds for the local maximum of a function, since there the function goes from increasing to ...", "dateLastCrawled": "2022-02-03T03:51:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Types of Optimization Algorithms used in Neural</b> Networks and Ways to ...", "url": "https://medium.com/nerd-for-tech/types-of-optimization-algorithms-used-in-neural-networks-and-ways-to-optimize-gradient-descent-1e32cdcbcf6c", "isFamilyFriendly": true, "displayUrl": "https://medium.com/nerd-for-tech/<b>types-of-optimization-algorithms-used-in-neural</b>...", "snippet": "The values for \u03b21 is 0.9 , 0.999 for \u03b22, and (10 x exp(-8)) for \u03f5.Adam works well in practice and compares favorably to other adaptive learning-method algorithms as it converges very fast and ...", "dateLastCrawled": "2022-02-01T03:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>How to Master Function Optimization in Deep Learning</b> | Abe.ai", "url": "https://www.abe.ai/blog/deep-learning-function-optimization/", "isFamilyFriendly": true, "displayUrl": "https://www.abe.ai/blog/deep-learning-function-<b>optimization</b>", "snippet": "Obviously, the <b>ball</b> will roll down towards the bottom of the cup, then it will oscillate a bit, eventually landing towards <b>the lowest</b> <b>point</b> of the cup. This is called the minimum, and the mathematical function that describes the cup is a <b>convex</b> function. It turns out that the minimum, in this case, is also the global minimum. In fact, the <b>ball</b> cannot go lower than that. Moreover, there is just one such <b>point</b>. The <b>ball</b> cannot go anywhere else. The minimum is global and it is unique. And all ...", "dateLastCrawled": "2022-01-29T19:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Optimization</b> - blog.pollithy.com", "url": "https://blog.pollithy.com/personal/python/optimization", "isFamilyFriendly": true, "displayUrl": "https://blog.pollithy.com/personal/python/<b>optimization</b>", "snippet": "<b>Convex</b> <b>optimization</b>. Linear least squares is the method of <b>finding</b> a line that best fits given data points. \u201cBest fit\u201d means that the sum of the squared distances between a <b>point</b> and the line is minimal. The vertical distance between the <b>point</b> and the line gets called residual.", "dateLastCrawled": "2021-09-17T01:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) <b>Convex</b> <b>Optimization</b> | Tamer Atteya - Academia.edu", "url": "https://www.academia.edu/5262989/Convex_Optimization", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/5262989/<b>Convex</b>_<b>Optimization</b>", "snippet": "<b>Convex</b> <b>Optimization</b>. \u00d7 Close Log In. Log in with Facebook Log in with Google. or. Email. Password. Remember me on this computer. or reset password. Enter the email address you signed up with and we&#39;ll email you a reset link. Need an account? Click here to sign up. Log ...", "dateLastCrawled": "2022-01-30T09:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Types of <b>Optimization algorithms</b> and Optimizing Gradient Descent", "url": "https://modularml.blogspot.com/2020/05/have-you-ever-wondered-which.html", "isFamilyFriendly": true, "displayUrl": "https://modularml.blogspot.com/2020/05/have-you-ever-wondered-which.html", "snippet": "We\u2019d like to have a smarter <b>ball</b>, a <b>ball</b> that has a notion of where it is going so that it knows to slow down before the hill slopes up again. What actually happens is that as we reach the minima i.e <b>the lowest</b> <b>point</b> on the curve, the momentum is pretty high and it doesn\u2019t know to slow down at that <b>point</b> due to the high momentum which could cause it to miss the minima entirely and continue movie up.", "dateLastCrawled": "2021-12-21T06:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A Bio-Inspired Method for Engineering Design <b>Optimization</b> Inspired by ...", "url": "https://www.hindawi.com/journals/mpe/2021/9107547/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.hindawi.com</b>/journals/mpe/2021/9107547", "snippet": "A novel bio-inspired algorithm, namely, Dingo <b>Optimization</b> Algorithm (DOA), is proposed for solving <b>optimization</b> problems. The DOA mimics the social behavior of the Australian dingo dog. The algorithm is inspired by the hunting strategies of dingoes which are attacking by persecution, grouping tactics, and scavenging behavior. In order to increment the overall efficiency and performance of this method, three search strategies associated with four rules were formulated in the DOA. These ...", "dateLastCrawled": "2022-02-03T04:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Asynchronously parallel <b>optimization</b> solver for <b>finding</b> multiple minima ...", "url": "https://link.springer.com/article/10.1007%2Fs12532-017-0131-4", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s12532-017-0131-4", "snippet": "Many methods exist for <b>finding</b> minima for general nonlinear <b>optimization</b> problems of the form ().In this section, we highlight methods that utilize concurrent evaluations of the objective function and methods that explicitly seek a global minimum of ().Methods of the former type can be restarted at different points in the domain \\(\\mathscr {D}\\).Convergent methods of the latter type must sufficiently explore the domain \\(\\mathscr {D}\\) [].. Several derivative-free implementations of ...", "dateLastCrawled": "2022-01-21T20:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "In training a neural network, what things are done to prevent gradient ...", "url": "https://www.quora.com/In-training-a-neural-network-what-things-are-done-to-prevent-gradient-descent-from-converging-to-a-local-minimum", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/In-training-a-neural-network-what-things-are-done-to-prevent...", "snippet": "Answer (1 of 3): Regularization, batch normalization, dropout, using advanced optimizers and doing sensible initialization help on not getting stuck in poor local minimas. Using those techniques also make the learning faster. Btw, they don\u2019t prevent converging in a local minima. They prevent co...", "dateLastCrawled": "2022-01-21T12:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A First Course in <b>Optimization</b> - DocShare.tips", "url": "https://docshare.tips/a-first-course-in-optimization_5889ee29b6d87f9c2f8b4b94.html", "isFamilyFriendly": true, "displayUrl": "https://docshare.tips/a-first-course-in-<b>optimization</b>_5889ee29b6d87f9c2f8b4b94.html", "snippet": "A First Course in <b>Optimization</b> <b>Convex</b> Programming <b>Convex</b> programming involves the minimization of <b>convex</b> functions, subject to <b>convex</b> constraints. This is perhaps the most general class of <b>optimization</b> problems for which a fairly complete theory exists. Once again, duality plays an important role. Some of the discussion here concerning", "dateLastCrawled": "2022-01-13T07:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How to <b>overcome a local minimum problem in neural networks</b> - Quora", "url": "https://www.quora.com/How-do-I-overcome-a-local-minimum-problem-in-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/How-do-I-<b>overcome-a-local-minimum-problem-in-neural-networks</b>", "snippet": "Answer (1 of 6): The solution is Stochastic Gradient Descent. https://en.wikipedia.org/wiki/Stochastic_gradient_descent It literally adds some randomness during ...", "dateLastCrawled": "2022-01-27T01:37:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Newest &#39;convex-optimization&#39; Questions</b> - Theoretical Computer Science ...", "url": "https://cstheory.stackexchange.com/questions/tagged/convex-optimization", "isFamilyFriendly": true, "displayUrl": "https://cstheory.stackexchange.com/questions/tagged/<b>convex-optimization</b>", "snippet": "It&#39;s common for <b>convex optimization</b> procedures to require a bounded region containing an optimal solution, either as input, like the initial ellipsoid of the ellipsoid method, or for run time bounds, ...", "dateLastCrawled": "2022-01-12T07:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Convex</b> <b>Optimization</b> | Tamer Atteya - Academia.edu", "url": "https://www.academia.edu/5262989/Convex_Optimization", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/5262989/<b>Convex</b>_<b>Optimization</b>", "snippet": "<b>Convex</b> <b>Optimization</b>. \u00d7 Close Log In. Log in with Facebook Log in with Google. or. Email. Password. Remember me on this computer. or reset password. Enter the email address you signed up with and we&#39;ll email you a reset link. Need an account? Click here to sign up. Log ...", "dateLastCrawled": "2022-01-30T09:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Rolling <b>ball</b> method <b>for 5-axis surface machining</b> - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S0010448502000568", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0010448502000568", "snippet": "It <b>can</b> <b>be thought</b> of as a <b>ball</b> of varying radius rolling along the <b>surface</b> in the direction of the tool path ... Tool axis 2 is the tool axis rotated such that <b>the lowest</b> <b>point</b> of the rolling <b>ball</b> touches the <b>surface</b> at the ccp and the z-axis of the <b>ball</b> is aligned with the <b>surface</b> normal. 3.5. Tool path generationTypical tool paths for <b>surface</b> machining move the tool along constrained x, y, or z directions along isoparametric lines or in a spiraling fashion. In the RBM, the tool path <b>can</b> be ...", "dateLastCrawled": "2022-01-04T04:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Convex</b> <b>Optimization</b> (Stanford CVX101) 9780521833783, 0521833787 ...", "url": "https://dokumen.pub/convex-optimization-stanford-cvx101-9780521833783-0521833787-2003063284.html", "isFamilyFriendly": true, "displayUrl": "https://dokumen.pub/<b>convex</b>-<b>optimization</b>-stanford-cvx101-9780521833783-0521833787...", "snippet": "Nesterov and Nemirovski [NN94] were the first to <b>point</b> out that interior-<b>point</b> methods <b>can</b> solve many <b>convex</b> <b>optimization</b> problems; see also the references in chapter 11. The book by Ben-Tal and Nemirovski [BTN01] covers modern <b>convex</b> <b>optimization</b>, interiorpoint methods, and applications. Solution methods for <b>convex</b> <b>optimization</b> that we do not cover in this book include subgradient methods [Sho85], bundle methods [HUL93], cutting-plane methods [Kel60, EM75, GLY96], and the ellipsoid method ...", "dateLastCrawled": "2022-01-03T23:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Lecture 22: Gradient Descent: Downhill to a Minimum | Video Lectures ...", "url": "https://ocw.mit.edu/courses/mathematics/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/video-lectures/lecture-22-gradient-descent-downhill-to-a-minimum/", "isFamilyFriendly": true, "displayUrl": "https://<b>ocw.mit.edu</b>/courses/mathematics/18-065-matrix-methods-in-data-analysis-signal...", "snippet": "What is that telling me about that <b>surface</b> in particular when the Hessian is 0 or other surfaces? What does the Hessian tell me about--I&#39;m thinking of the Hessian at a particular <b>point</b>. So I&#39;m getting 0 for the Hessian because the <b>surface</b> is flat. If the <b>surface</b> was <b>convex</b> upwards from--if it was a <b>convex</b> or a graph of F, the Hessian would be ...", "dateLastCrawled": "2022-02-02T15:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "How to Solve <b>Optimization</b> Problems in Calculus - <b>Matheno</b>.com", "url": "https://www.matheno.com/blog/how-to-solve-optimization-problems-in-calculus/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>matheno</b>.com/blog/how-to-solve-<b>optimization</b>-problems-in-calculus", "snippet": "Hence we want to minimize the <b>can</b>\u2019s <b>surface</b> area. So let\u2019s write an equation for that total <b>surface</b> area: \\begin{align*} A_\\text{total} &amp;= A_\\text{top} + A_\\text{cylinder} + A_\\text{bottom} \\\\[8px] &amp;= \\pi r^2 + 2\\pi r h + \\pi r^2 \\\\[8px] &amp;= 2\\pi r^2 + 2 \\pi r h \\end{align*} That\u2019s it; you\u2019re done with Step 2! You\u2019ve written an equation for the quantity you want to minimize $(A_\\text{total})$ in terms of the relevant quantities (r and h). RELATED MATERIAL. <b>Optimization</b> Problems ...", "dateLastCrawled": "2022-02-02T11:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Practical optimization methods for machine learning</b> models - UBC ...", "url": "https://open.library.ubc.ca/cIRcle/collections/ubctheses/24/items/1.0387209", "isFamilyFriendly": true, "displayUrl": "https://open.library.ubc.ca/cIRcle/collections/ubctheses/24/items/1.0387209", "snippet": "This work considers <b>optimization</b> methods for large-scale machine learning (ML). <b>Optimization</b> in ML is a crucial ingredient in the training stage of ML models. <b>Optimization</b> methods in this setting need to have cheap iteration cost. First-order methods are known to have reasonably low iteration costs. A notable recent class of stochastic first-order methods leverage variance reduction techniques to improve their convergence speed. This group includes stochastic average gradient (SAG ...", "dateLastCrawled": "2021-07-25T18:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A First Course in <b>Optimization</b> - DocShare.tips", "url": "https://docshare.tips/a-first-course-in-optimization_5889ee29b6d87f9c2f8b4b94.html", "isFamilyFriendly": true, "displayUrl": "https://docshare.tips/a-first-course-in-<b>optimization</b>_5889ee29b6d87f9c2f8b4b94.html", "snippet": "A First Course in <b>Optimization</b> <b>Convex</b> Programming <b>Convex</b> programming involves the minimization of <b>convex</b> functions, subject to <b>convex</b> constraints. This is perhaps the most general class of <b>optimization</b> problems for which a fairly complete theory exists. Once again, duality plays an important role. Some of the discussion here concerning", "dateLastCrawled": "2022-01-13T07:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "6 <b>Optimization</b> | Building Skills in Quantitative Biology", "url": "https://www.quantitative-biology.ca/optimization.html", "isFamilyFriendly": true, "displayUrl": "https://www.quantitative-biology.ca/<b>optimization</b>.html", "snippet": "A simple iterative <b>optimization</b> algorithm is gradient descent, which <b>can</b> be understood intuitively via a <b>thought</b> experiment. Imagine <b>finding</b> your way to a valley bottom in a thick fog. The fog obscures your vision so that you <b>can</b> only discern changes in elevation in your immediate vicinity. To make your way to the valley bottom, it would be reasonable to take each step of your journey in the direction of steepest decline. This strategy is guaranteed to lead to a local minimum, but cannot ...", "dateLastCrawled": "2022-01-23T16:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How to <b>overcome a local minimum problem in neural networks</b> - Quora", "url": "https://www.quora.com/How-do-I-overcome-a-local-minimum-problem-in-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/How-do-I-<b>overcome-a-local-minimum-problem-in-neural-networks</b>", "snippet": "Answer (1 of 6): The solution is Stochastic Gradient Descent. https://en.wikipedia.org/wiki/Stochastic_gradient_descent It literally adds some randomness during ...", "dateLastCrawled": "2022-01-27T01:37:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Convex</b> <b>Optimization</b> | Tamer Atteya - Academia.edu", "url": "https://www.academia.edu/5262989/Convex_Optimization", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/5262989/<b>Convex</b>_<b>Optimization</b>", "snippet": "<b>Convex</b> <b>Optimization</b>. \u00d7 Close Log In. Log in with Facebook Log in with Google. or. Email. Password. Remember me on this computer. or reset password. Enter the email address you signed up with and we&#39;ll email you a reset link. Need an account? Click here to sign up. Log ...", "dateLastCrawled": "2022-01-30T09:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Convex</b> separation from <b>convex</b> <b>optimization</b> for large-scale problems ...", "url": "https://www.arxiv-vanity.com/papers/1609.05011/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1609.05011", "snippet": "Abstract. We present a scheme, based on Gilbert\u2019s algorithm for quadratic minimization [SIAM J. Contrl., vol. 4, pp. 61-80, 1966], to prove separation between a <b>point</b> and an arbitrary <b>convex</b> set S \u2282 R n via calls to an oracle able to perform linear optimizations over S.<b>Compared</b> to other methods, our scheme has almost negligible memory requirements and the number of calls to the <b>optimization</b> oracle does not depend on the dimensionality n of the underlying space. We study the speed of ...", "dateLastCrawled": "2021-12-07T20:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Convex</b> <b>Optimization</b> (Stanford CVX101) 9780521833783, 0521833787 ...", "url": "https://dokumen.pub/convex-optimization-stanford-cvx101-9780521833783-0521833787-2003063284.html", "isFamilyFriendly": true, "displayUrl": "https://dokumen.pub/<b>convex</b>-<b>optimization</b>-stanford-cvx101-9780521833783-0521833787...", "snippet": "Nesterov and Nemirovski [NN94] were the first to <b>point</b> out that interior-<b>point</b> methods <b>can</b> solve many <b>convex</b> <b>optimization</b> problems; see also the references in chapter 11. The book by Ben-Tal and Nemirovski [BTN01] covers modern <b>convex</b> <b>optimization</b>, interiorpoint methods, and applications. Solution methods for <b>convex</b> <b>optimization</b> that we do not cover in this book include subgradient methods [Sho85], bundle methods [HUL93], cutting-plane methods [Kel60, EM75, GLY96], and the ellipsoid method ...", "dateLastCrawled": "2022-01-03T23:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Power System <b>Optimization</b> Lecture Notes Pdf", "url": "https://groups.google.com/g/9xgmhweji/c/vwmezjWTlrk", "isFamilyFriendly": true, "displayUrl": "https://groups.google.com/g/9xgmhweji/c/vwmezjWTlrk", "snippet": "Looking for Lecture notes in System Analysis and Design. 92 Unger R Moult J <b>Finding</b> <b>the lowest</b> free-energy conformation of a protein is. This issue on information into a power system approximation concepts of the minimum. Optimal Power supply under Uncertainty via CiteSeerX. And sun set of constraints in plural form purchase a grove of equations or inequalities. LECTURE NOTES <b>OPTIMIZATION</b> I II <b>CONVEX</b> ANALYSIS NONLINEAR PROGRAMMING THEORY NONLINEAR PROGRAMMING ALGORITHMS. It introduces the ...", "dateLastCrawled": "2022-01-27T06:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "What are the different <b>optimization</b> algorithm faster than gradient ...", "url": "https://ml1.quora.com/What-are-the-different-optimization-algorithm-faster-than-gradient-descent", "isFamilyFriendly": true, "displayUrl": "https://ml1.quora.com/What-are-the-different-<b>optimization</b>-algorithm-faster-than...", "snippet": "The lake is <b>the lowest</b> <b>point</b> in 360 degrees all around, but over one of those ridges somewhere is a path to the ocean\u2026 it&#39;s just that you&#39;ll have to climb a bit to get over that ridge before making your way to the ocean. A mountain lake is a \u201clocal minimum\u201d and gradient descent is going to get stuck there.", "dateLastCrawled": "2022-01-29T05:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Asynchronously parallel <b>optimization</b> solver for <b>finding</b> multiple minima ...", "url": "https://link.springer.com/article/10.1007%2Fs12532-017-0131-4", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s12532-017-0131-4", "snippet": "Many methods exist for <b>finding</b> minima for general nonlinear <b>optimization</b> problems of the form ().In this section, we highlight methods that utilize concurrent evaluations of the objective function and methods that explicitly seek a global minimum of ().Methods of the former type <b>can</b> be restarted at different points in the domain \\(\\mathscr {D}\\).Convergent methods of the latter type must sufficiently explore the domain \\(\\mathscr {D}\\) [].. Several derivative-free implementations of ...", "dateLastCrawled": "2022-01-21T20:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "6 <b>Optimization</b> | Building Skills in Quantitative Biology", "url": "https://www.quantitative-biology.ca/optimization.html", "isFamilyFriendly": true, "displayUrl": "https://www.quantitative-biology.ca/<b>optimization</b>.html", "snippet": "6.2 Fundamentals of <b>Optimization</b>. Figure 1 illustrates some basics terminology associated with <b>optimization</b>. The graph of a function \\(f\\) of a single variable \\(x\\) is shown, defined over a domain \\([a,b]\\).In the context of <b>optimization</b>, we <b>can</b> think of each \\(x\\)-value in the interval \\([a,b]\\) as one possible scenario (e.g. enzyme activity, foraging rate, etc.). The function \\(f\\) maps those scenarios to some objective (e.g. metabolic flux, fitness) to be optimized (either maximized or ...", "dateLastCrawled": "2022-01-23T16:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Lecture 22: Gradient Descent: Downhill to a Minimum | Video Lectures ...", "url": "https://ocw.mit.edu/courses/mathematics/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/video-lectures/lecture-22-gradient-descent-downhill-to-a-minimum/", "isFamilyFriendly": true, "displayUrl": "https://<b>ocw.mit.edu</b>/courses/mathematics/18-065-matrix-methods-in-data-analysis-signal...", "snippet": "If the <b>surface</b> was <b>convex</b> upwards from--if it was a <b>convex</b> or a graph of F, the Hessian would be--so I just want to make that connection now. What&#39;s the connection between the Hessian and convexity of the--the Hessian of the function and convexity of the function? So the <b>point</b> is that convexity--the Hessian tells me whether or not the <b>surface</b> is <b>convex</b>. And what is the test? AUDIENCE: [INAUDIBLE]. GILBERT STRANG: Positive definite or semi definite. I&#39;m just looking for an excuse to write ...", "dateLastCrawled": "2022-02-02T15:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A Bio-Inspired Method for Engineering Design <b>Optimization</b> Inspired by ...", "url": "https://www.hindawi.com/journals/mpe/2021/9107547/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.hindawi.com</b>/journals/mpe/2021/9107547", "snippet": "A novel bio-inspired algorithm, namely, Dingo <b>Optimization</b> Algorithm (DOA), is proposed for solving <b>optimization</b> problems. The DOA mimics the social behavior of the Australian dingo dog. The algorithm is inspired by the hunting strategies of dingoes which are attacking by persecution, grouping tactics, and scavenging behavior. In order to increment the overall efficiency and performance of this method, three search strategies associated with four rules were formulated in the DOA. These ...", "dateLastCrawled": "2022-02-03T04:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Why is the <b>Adam optimization algorithm better than gradient</b> ... - Quora", "url": "https://www.quora.com/Why-is-the-Adam-optimization-algorithm-better-than-gradient-descent", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Why-is-the-<b>Adam-optimization-algorithm-better-than-gradient-descent</b>", "snippet": "Answer (1 of 4): Gradient descent is a first-order algorithm, that is, it only looks at the first derivative of the objective function. While the first derivative gives you some information about the direction of local optimum, you <b>can</b> extract more information if you could also use the second der...", "dateLastCrawled": "2022-01-29T02:40:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Optimization</b> for <b>Machine</b> <b>Learning</b>", "url": "http://optml.mit.edu/talks/pkuLectAlgo3.pdf", "isFamilyFriendly": true, "displayUrl": "optml.mit.edu/talks/pkuLectAlgo3.pdf", "snippet": "<b>Optimization</b> for <b>Machine</b> <b>Learning</b> \u2013 Sra, Nowozin, Wright Theory of <b>Convex</b> <b>Optimization</b> for <b>Machine</b> <b>Learning</b> \u2013 Bubeck NIPS 2016 <b>Optimization</b> Tutorial \u2013 Bach, Sra Some related courses: EE227A, Spring 2013, (Sra, UC Berkeley) 10-801, Spring 2014 (Sra, CMU) EE364a,b (Boyd, Stanford) EE236b,c (Vandenberghe, UCLA) Venues: NIPS, ICML, UAI, AISTATS, SIOPT, Math. Prog. Suvrit Sra(suvrit@mit.edu)<b>Optimization</b> for <b>Machine</b> <b>Learning</b> 2 / 29. Lecture Plan \u2013Introduction (3 lectures) \u2013Problems and ...", "dateLastCrawled": "2021-08-29T23:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Basic Concepts in Machine Learning</b>", "url": "https://machinelearningmastery.com/basic-concepts-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>basic-concepts-in-machine-learning</b>", "snippet": "For example combinatorial <b>optimization</b>, <b>convex</b> <b>optimization</b>, constrained <b>optimization</b>. All <b>machine learning</b> algorithms are combinations of these three components. A framework for understanding all algorithms. Types of <b>Learning</b> . There are four types of <b>machine learning</b>: Supervised <b>learning</b>: (also called inductive <b>learning</b>) Training data includes desired outputs. This is spam this is not, <b>learning</b> is supervised. Unsupervised <b>learning</b>: Training data does not include desired outputs. Example is ...", "dateLastCrawled": "2022-02-02T15:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Introduction to Machine Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/24/introduction-to-machine-learning-2/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/24/<b>introduction-to-machine-learning</b>-2", "snippet": "<b>Optimization</b> methods are applied to minimize the loss function by changing the parameter values, which is the central theme of <b>machine</b> <b>learning</b>.Zero-one loss is L0-1 = 1 (m &lt;= 0); in zero-one loss, value of loss is 0 for m &gt;= 0 whereas 1 for m &lt; 0. The difficult part with this loss is it is not differentiable, non-<b>convex</b>, and also NP-hard. Hence, in order to make <b>optimization</b> feasible and solvable, these losses are replaced by different surrogate losses for different problems.", "dateLastCrawled": "2022-01-17T05:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "11.2. <b>Convexity</b> \u2014 Dive into Deep <b>Learning</b> 0.17.2 documentation", "url": "https://d2l.ai/chapter_optimization/convexity.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_<b>optimization</b>/<b>convexity</b>.html", "snippet": "Furthermore, even though the <b>optimization</b> problems in deep <b>learning</b> are generally nonconvex, they often exhibit some properties of <b>convex</b> ones near local minima. This can lead to exciting new <b>optimization</b> variants such as [Izmailov et al., 2018].", "dateLastCrawled": "2022-01-30T21:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Optimization</b> for deep <b>learning</b>: an overview", "url": "https://www.ise.ncsu.edu/fuzzy-neural/wp-content/uploads/sites/9/2022/01/Optimization-for-deep-learning.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.ise.ncsu.edu/.../uploads/sites/9/2022/01/<b>Optimization</b>-for-deep-<b>learning</b>.pdf", "snippet": "timization problems beyond <b>convex</b> problems. A somewhat related <b>analogy</b> is the development of conic <b>optimization</b>: in 1990\u2019s, researchers realized that many seemingly non-<b>convex</b> problems can actually be reformulated as conic <b>optimization</b> problems (e.g. semi-de nite programming) which are <b>convex</b> problems, thus the boundary of tractability has advanced signi cantly. Neural network problems are surely not the worst non-<b>convex</b> <b>optimization</b> problems and their global optima could be found ...", "dateLastCrawled": "2022-01-19T03:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Regularization : What? Why? and How? (Part -1) | by Siddhant Rai ...", "url": "https://medium.com/mlearning-ai/regularization-what-why-and-how-part-1-ef6bdb6bafea", "isFamilyFriendly": true, "displayUrl": "https://medium.com/m<b>learning</b>-ai/regularization-what-why-and-how-part-1-ef6bdb6bafea", "snippet": "In general <b>machine</b> <b>learning</b> sense, it is solving an objective function to perform maximum or minimum evaluation. In reality, <b>optimization</b> is lot more profound in usage. Then we have two terms ...", "dateLastCrawled": "2022-01-28T00:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Gradient</b> Descent: A Quick, Simple Introduction | Built In", "url": "https://builtin.com/data-science/gradient-descent", "isFamilyFriendly": true, "displayUrl": "https://builtin.com/data-science/<b>gradient</b>-descent", "snippet": "<b>Gradient</b> descent is an <b>optimization</b> algorithm that&#39;s used when training a <b>machine</b> <b>learning</b> model. It&#39;s based on a <b>convex</b> function and tweaks its parameters iteratively to minimize a given function to its local minimum.", "dateLastCrawled": "2022-02-02T07:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Summary of Thesis: <b>Non-convex Optimization for Machine Learning</b>: Design ...", "url": "https://ai.stanford.edu/~tengyuma/slides/summary.pdf", "isFamilyFriendly": true, "displayUrl": "https://ai.stanford.edu/~tengyuma/slides/summary.pdf", "snippet": "Summary of Thesis: <b>Non-convex Optimization for Machine Learning</b>: Design, Analysis, and Understanding Tengyu Ma October 15, 2018 Non-<b>convex</b> <b>optimization</b> is ubiquitous in modern <b>machine</b> <b>learning</b>: re-cent breakthroughs in deep <b>learning</b> require optimizing non-<b>convex</b> training objective functions; problems that admit accurate <b>convex</b> relaxation can often be solved more e ciently with non-<b>convex</b> formulations. However, the theoretical understanding of non-<b>convex</b> <b>optimization</b> remained rather limited ...", "dateLastCrawled": "2021-09-02T16:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "7. <b>Optimization</b>, the central part of any <b>Machine</b> <b>Learning</b> algortithm ...", "url": "https://compphysics.github.io/MachineLearning/doc/LectureNotes/_build/html/chapteroptimization.html", "isFamilyFriendly": true, "displayUrl": "https://compphysics.github.io/<b>MachineLearning</b>/doc/LectureNotes/_build/html/chapter...", "snippet": "7. <b>Optimization</b>, the central part of any <b>Machine</b> <b>Learning</b> algortithm\u00b6. Almost every problem in <b>machine</b> <b>learning</b> and data science starts with a dataset \\(X\\), a model \\(g(\\beta)\\), which is a function of the parameters \\(\\beta\\) and a cost function \\(C(X, g(\\beta))\\) that allows us to judge how well the model \\(g(\\beta)\\) explains the observations \\(X\\).The model is fit by finding the values of \\(\\beta\\) that minimize the cost function. Ideally we would be able to solve for \\(\\beta ...", "dateLastCrawled": "2022-01-31T15:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "[2005.14605] CoolMomentum: A Method for Stochastic <b>Optimization</b> by ...", "url": "https://arxiv.org/abs/2005.14605", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/abs/2005.14605", "snippet": "This <b>analogy</b> provides useful insights for non-<b>convex</b> stochastic <b>optimization</b> in <b>machine</b> <b>learning</b>. Here we find that integration of the discretized Langevin equation gives a coordinate updating rule equivalent to the famous Momentum <b>optimization</b> algorithm. As a main result, we show that a gradual decrease of the momentum coefficient from the initial value close to unity until zero is equivalent to application of Simulated Annealing or slow cooling, in physical terms. Making use of this novel ...", "dateLastCrawled": "2021-10-23T08:45:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Best <b>Artificial Intelligence</b> Course (AIML) by UT Austin", "url": "https://www.mygreatlearning.com/pg-program-artificial-intelligence-course", "isFamilyFriendly": true, "displayUrl": "https://www.mygreat<b>learning</b>.com/pg-program-<b>artificial-intelligence</b>-course", "snippet": "<b>Machine</b> <b>Learning</b>: <b>Machine</b> <b>learning</b> is a sub-branch of AI that teaches machines to learn any task without the help of explicit directions. It teaches machines to learn by drawing inferences from past experience. <b>Machine</b> <b>learning</b> primarily focuses on developing computer programs that can access and analyze data to identify patterns and understand data behaviour to reach possible conclusions without any kind of human intervention.", "dateLastCrawled": "2022-02-01T17:58:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Which <b>machine</b> <b>learning</b> algorithms for classification support online ...", "url": "https://www.quora.com/Which-machine-learning-algorithms-for-classification-support-online-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Which-<b>machine</b>-<b>learning</b>-algorithms-for-classification-support...", "snippet": "Answer (1 of 5): Most algorithms can be adapted to make them online, even though the standard implementations may not support it. E.g. both decision trees and support ...", "dateLastCrawled": "2022-01-09T00:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "What is <b>the relationship between Online Machine Learning</b> and ...", "url": "https://www.quora.com/What-is-the-relationship-between-Online-Machine-Learning-and-Incremental-Machine-Learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>the-relationship-between-Online-Machine-Learning</b>-and...", "snippet": "Answer (1 of 4): Online <b>learning</b> usually refers to the case where each example is only used once (e.g. if you&#39;re updating an ad click prediction model online after each impression or click), while incremental methods usually pick one example at a time from a finite dataset and can process the sam...", "dateLastCrawled": "2022-01-14T06:00:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "SimplifiedMachineLearningWorkflows-book/Wolfram-Technology-Conference ...", "url": "https://github.com/antononcube/SimplifiedMachineLearningWorkflows-book/blob/master/Data/Wolfram-Technology-Conference-2016-to-2019-abstracts.csv", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/antononcube/Simplified<b>MachineLearning</b>Workflows-book/blob/master/...", "snippet": "Finally, I use <b>machine</b> <b>learning</b> algorithms to train a series of classifiers that can predict a text&#39;s authorship based on its MFW frequencies. Cross-validation indicates that Gallus and Monk are very likely one and the same author. The results also reveal the especially high and hitherto underexplored effectiveness of the Bray Curtis Distance measure and of logistic regression in shedding light on questions of authorship attribution. Data Analytics &amp; Information Science : 2016.Gunnar.Prei\u00df ...", "dateLastCrawled": "2021-12-28T12:42:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(convex optimization)  is like +(ball finding the lowest point on a surface)", "+(convex optimization) is similar to +(ball finding the lowest point on a surface)", "+(convex optimization) can be thought of as +(ball finding the lowest point on a surface)", "+(convex optimization) can be compared to +(ball finding the lowest point on a surface)", "machine learning +(convex optimization AND analogy)", "machine learning +(\"convex optimization is like\")", "machine learning +(\"convex optimization is similar\")", "machine learning +(\"just as convex optimization\")", "machine learning +(\"convex optimization can be thought of as\")", "machine learning +(\"convex optimization can be compared to\")"]}