{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Interpretable Machine Learning: What is <b>Interpretability</b>? \u2014 Andrew Silva", "url": "https://www.andrew-silva.com/blog/what-is-interpretability-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.andrew-silva.com/blog/what-is-<b>interpretability</b>-in-machine-learning", "snippet": "Even though each individual decision is clear, nobody is going to memorize a thousand nodes and combinations to get a full mental <b>map</b> of the model. Explanations. An alternate approach to <b>interpretability</b> is to provide explanations with outputs from a machine learning system. These explanations can take a variety of forms, with one of the most ...", "dateLastCrawled": "2022-01-31T23:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Interpretability</b> in the medical field: A systematic mapping and review ...", "url": "https://www.sciencedirect.com/science/article/pii/S1568494621011522", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1568494621011522", "snippet": "ML research has focused on global <b>interpretability</b> to help users <b>map</b> possible model inputs to the space of the predictions made by the model , . Perhaps, since most of the global techniques provide rules explanations (RQ5), doctors prefer IF-THEN rules because they are easy to understand and close to the patient language. Moreover, almost all studies that investigated global <b>interpretability</b> were evaluated for a diagnosis task, which shows the necessity of acquiring new global rules or ...", "dateLastCrawled": "2022-01-23T08:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Certi\ufb01ed <b>Interpretability</b> Robustness for Class Activation Mapping", "url": "https://ml4ad.github.io/files/papers2020/Certified%20Interpretability%20Robustness%20for%20Class%20Activation%20Mapping.pdf", "isFamilyFriendly": true, "displayUrl": "https://ml4ad.github.io/files/papers2020/Certified <b>Interpretability</b> Robustness for...", "snippet": "<b>interpretability</b> <b>map</b> for each image showing the effect of each input pixel on the overall prediction. Unfortunately, recent work has shown that these <b>interpretability</b> methods are also prone to adversarial perturbations [24] [1] [6]. All three works successfully generate visually imperceptible perturbations for which the network correctly classi\ufb01es the image, but shows a nonsensical <b>interpretability</b> <b>map</b>. While providing certi\ufb01able guarantees for adversarial attacks on a neural network ...", "dateLastCrawled": "2021-12-27T10:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "6 \u2013 <b>Interpretability</b> \u2013 Machine Learning Blog | ML@CMU | Carnegie Mellon ...", "url": "https://blog.ml.cmu.edu/2020/08/31/6-interpretability/", "isFamilyFriendly": true, "displayUrl": "https://blog.ml.cmu.edu/2020/08/31/6-<b>interpretability</b>", "snippet": "But just <b>like</b> <b>interpretability</b>, trust is difficult to define. One way to define trust is how comfortable we are with deploying the model in the real world. We might feel more at ease with a model that is better-understood, especially in high stakes decision making (e.g. finance or medicine), but this comfort does not necessarily reflect how accurate or effective a model is. In the case of trust, we care about where models make mistakes and not just how many. We especially desire that the ...", "dateLastCrawled": "2022-02-03T05:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "How <b>should you interpret your deep learning model</b>? | by Mara Graziani ...", "url": "https://medium.com/research-at-medgift/how-should-you-interpret-your-deep-learning-model-a266fcf3ab48", "isFamilyFriendly": true, "displayUrl": "https://medium.com/research-at-medgift/how-<b>should-you-interpret-your-deep-learning</b>...", "snippet": "A <b>map</b> to choose the deep learning <b>interpretability</b> technique most appropriate to our needs. If <b>interpretability</b> is needed to understand each component in the deep learning model, <b>like</b> the ...", "dateLastCrawled": "2021-02-02T13:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Uncertainty and interpretability in convolutional neural networks</b> for ...", "url": "https://www.sciencedirect.com/science/article/pii/S1361841519301574", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1361841519301574", "snippet": "The <b>interpretability</b> <b>map</b> in Fig. 5 (d) indicates that there are two regions of importance in the input image, ... Upsampling in FCNs is performed using a fixed upsampling approach, <b>like</b> bi-linear or nearest neighbor interpolation, or by learning the upsampling procedure as part of the model optimization via transposed convolutions. Learned upsampling filters add additional parameters to the network architecture, but tend to provide better overall results (Shelhamer et al., 2017). Upsampling ...", "dateLastCrawled": "2022-01-07T07:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Interpretability vs Explainability: The Black</b> Box of Machine Learning ...", "url": "https://www.bmc.com/blogs/machine-learning-interpretability-vs-explainability/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.bmc.com</b>/blogs/machine-learning-<b>interpretability</b>-vs-explainability", "snippet": "<b>Interpretability</b> means that the cause and effect can be determined. If a model can take the inputs, and routinely get the same outputs, the model is interpretable: If you overeat your pasta at dinnertime and you always have troubles sleeping, the situation is interpretable. If all 2016 polls showed a Democratic win and the Republican candidate took office, all those models showed low <b>interpretability</b>. If the pollsters\u2019 goal is to have a good model, which the institution of journalism is ...", "dateLastCrawled": "2022-01-30T19:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Model Performance \u2013 accuracy v/s <b>interpretability</b> | Dr.Bharatheesh Jaysimha", "url": "https://bjsimha.wordpress.com/2013/04/14/model-performance-accuracy-vs-interpretability/", "isFamilyFriendly": true, "displayUrl": "https://bjsimha.wordpress.com/2013/04/14/model-performance-accuracy-vs-<b>interpretability</b>", "snippet": "Though it is a subjective opinion, I have tried to put my thoughts as a cognitive <b>map</b> for ease of explanation. Fig.1 Factors affecting Model performance. One of the utilities of predictive modeling is to get insights into the factors affecting a particular behavior of interest. However, when it comes to applicability of the model to justify ROI, higher accuracies in prediction is essential. Not surprisingly, accuracy and <b>interpretability</b> always do not go together. It has been observed that ...", "dateLastCrawled": "2022-02-01T02:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Guide to Interpretable Machine Learning | by Matthew Stewart, PhD ...", "url": "https://towardsdatascience.com/guide-to-interpretable-machine-learning-d40e8a64b6cf", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/guide-to-interpretable-machine-learning-d40e8a64b6cf", "snippet": "For something <b>like</b> linear regression, the models are very well understood and highly interpretable. When we move to something <b>like</b> a support vector machine (SVM) or a random forest model, things get a bit more difficult. In this sense, there is no white or black box algorithm in machine learning, the <b>interpretability</b> exists as a spectrum or a \u2018gray box\u2019 of varying grayness.", "dateLastCrawled": "2022-01-29T20:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "10.1 Learned Features | Interpretable Machine Learning", "url": "https://christophm.github.io/interpretable-ml-book/cnn-features.html", "isFamilyFriendly": true, "displayUrl": "https://christophm.github.io/interpretable-ml-book/cnn-features.html", "snippet": "Rotation reduces <b>interpretability</b>, i.e. the number of channels aligned with a concept decreases. The rotation was designed to keep the performance of the model the same. The first conclusion: <b>Interpretability</b> of CNNs is axis-dependent. This means that random combinations of channels are less likely to detect unique concepts. The second conclusion: <b>Interpretability</b> is independent of discriminative power. The channels can be transformed with orthogonal transformations while the discriminative ...", "dateLastCrawled": "2022-01-30T04:23:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Certi\ufb01ed <b>Interpretability</b> Robustness for Class Activation Mapping", "url": "https://ml4ad.github.io/files/papers2020/Certified%20Interpretability%20Robustness%20for%20Class%20Activation%20Mapping.pdf", "isFamilyFriendly": true, "displayUrl": "https://ml4ad.github.io/files/papers2020/Certified <b>Interpretability</b> Robustness for...", "snippet": "<b>interpretability</b> <b>map</b> for each image showing the effect of each input pixel on the overall prediction. Unfortunately, recent work has shown that these <b>interpretability</b> methods are also prone to adversarial perturbations [24] [1] [6]. All three works successfully generate visually imperceptible perturbations for which the network correctly classi\ufb01es the image, but shows a nonsensical <b>interpretability</b> <b>map</b>. While providing certi\ufb01able guarantees for adversarial attacks on a neural network ...", "dateLastCrawled": "2021-12-27T10:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Interpretability</b> in the medical field: A systematic mapping and review ...", "url": "https://www.sciencedirect.com/science/article/pii/S1568494621011522", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1568494621011522", "snippet": "Methods: This review was carried out according to the well-known systematic <b>map</b> and review process to analyze the literature on <b>interpretability</b> techniques when applied in the medical field with regard to different aspects: publication venues and publication year, contribution and empirical types, medical and ML disciplines and objectives, ML black-box techniques interpreted, <b>interpretability</b> techniques investigated, their performance and the best performing techniques, and lastly, the ...", "dateLastCrawled": "2022-01-23T08:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>What is Interpretability</b>? - LessWrong 2.0 viewer", "url": "https://www.greaterwrong.com/posts/rSMbGFfsLMB3GWZtX/what-is-interpretability", "isFamilyFriendly": true, "displayUrl": "https://www.greaterwrong.com/posts/rSMbGFfsLMB3GWZtX/<b>what-is-interpretability</b>", "snippet": "Many terms are used to mean many <b>similar</b> things in the <b>interpretability</b> research literature. We think of ... However, we think it\u2019s just as important to consider what the role of the human is in an <b>interpretability</b> method\u2019s use. A <b>Map</b> of Methods. These dimensions aren\u2019t an operationalisation of <b>interpretability</b>, and don\u2019t present necessary and sufficient conditions, and we\u2019re uncertain whether it\u2019d be useful to have a strict dividing line. We do think that the framing is useful ...", "dateLastCrawled": "2021-11-26T21:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "6 \u2013 <b>Interpretability</b> \u2013 Machine Learning Blog | ML@CMU | Carnegie Mellon ...", "url": "https://blog.ml.cmu.edu/2020/08/31/6-interpretability/", "isFamilyFriendly": true, "displayUrl": "https://blog.ml.cmu.edu/2020/08/31/6-<b>interpretability</b>", "snippet": "<b>Interpretability</b> takes many forms and can be difficult to define; we first explore general frameworks and sets of definitions in which model <b>interpretability</b> can be evaluated and compared (Lipton 2016, Doshi-Velez &amp; Kim 2017). Next, we analyze several well-known examples of <b>interpretability</b> methods\u2013LIME (Ribeiro et al. 2016), SHAP (Lundberg &amp; Lee 2017), and convolutional neural network visualization (Olah et al. 2018)\u2013in the context of this framework. Model <b>interpretability</b> has no ...", "dateLastCrawled": "2022-02-03T05:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Interpretability vs Explainability: The Black</b> Box of Machine Learning ...", "url": "https://www.bmc.com/blogs/machine-learning-interpretability-vs-explainability/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.bmc.com</b>/blogs/machine-learning-<b>interpretability</b>-vs-explainability", "snippet": "<b>Interpretability</b> means that the cause and effect can be determined. If a model can take the inputs, and routinely get the same outputs, the model is interpretable: If you overeat your pasta at dinnertime and you always have troubles sleeping, the situation is interpretable. If all 2016 polls showed a Democratic win and the Republican candidate took office, all those models showed low <b>interpretability</b>. If the pollsters\u2019 goal is to have a good model, which the institution of journalism is ...", "dateLastCrawled": "2022-01-30T19:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Saliency maps for model interpretability in Python</b> - Value ML", "url": "https://valueml.com/saliency-maps-for-model-interpretability-in-python/", "isFamilyFriendly": true, "displayUrl": "https://valueml.com/<b>saliency-maps-for-model-interpretability-in-python</b>", "snippet": "Seaborn <b>is similar</b> to matplotlib used for visualising graphs. tf.keras_vis is used for visualisation of deep learning. Please take a look at these libraries to know more about their functions using the documentation. TRAINING THE MODEL. As I have explained in the introduction, please look at my previous tutorial on using transfer learning for training pneumonia dataset. The link is here. I have detailly explained the procedure for training the dataset. We will use the same DenseNet model for ...", "dateLastCrawled": "2022-02-01T06:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Neural Net <b>Interpretability</b> Visualization Techniques | by Manish ...", "url": "https://medium.com/analytics-vidhya/neural-net-interpretability-visualization-techniques-15c2b24597ff", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/neural-net-<b>interpretability</b>-visualization...", "snippet": "<b>Similar</b> to the process for constructing a saliency <b>map</b>, you can compute the gradients for mid level neurons in a network with respect to the input pixels. Guided backpropagation looks at each ...", "dateLastCrawled": "2021-09-29T09:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Guide to Interpretable Machine Learning | by Matthew Stewart, PhD ...", "url": "https://towardsdatascience.com/guide-to-interpretable-machine-learning-d40e8a64b6cf", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/guide-to-interpretable-machine-learning-d40e8a64b6cf", "snippet": "A <b>similar</b> technique to saliency mapping for discerning the importance of pixels in an image\u2019s prediction is occlusion mapping. In occlusion mapping, we are still developing a <b>map</b> related to an image\u2019s output. However, this time we are interested in how blocking out part of the image affects the prediction output of the image.", "dateLastCrawled": "2022-01-29T20:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Assessing Interpretable Models | Practical Cheminformatics", "url": "https://patwalters.github.io/practicalcheminformatics/jupyter/ml/interpretability/2021/06/03/interpretable.html", "isFamilyFriendly": true, "displayUrl": "https://patwalters.github.io/practicalcheminformatics/jupyter/ml/<b>interpretability</b>/2021/...", "snippet": "In this paper, the authors lay out benchmark datasets and evaluation metrics for model <b>interpretability</b>. In this post, we&#39;ll build a simple machine learning model and use some techniques implemented in the RDKit to evaluate the contributions of specific atoms in a molecule to a particular activity. One of the things I like about the paper by Matveieva and Polishchuk is that it defines some simple cases where the answer is known, and the activity should be explainable. We will consider the ...", "dateLastCrawled": "2022-02-03T01:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "10.1 Learned Features | Interpretable Machine Learning", "url": "https://christophm.github.io/interpretable-ml-book/cnn-features.html", "isFamilyFriendly": true, "displayUrl": "https://christophm.github.io/interpretable-ml-book/cnn-features.html", "snippet": "Rotation reduces <b>interpretability</b>, i.e. the number of channels aligned with a concept decreases. The rotation was designed to keep the performance of the model the same. The first conclusion: <b>Interpretability</b> of CNNs is axis-dependent. This means that random combinations of channels are less likely to detect unique concepts. The second conclusion: <b>Interpretability</b> is independent of discriminative power. The channels can be transformed with orthogonal transformations while the discriminative ...", "dateLastCrawled": "2022-01-30T04:23:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "6 \u2013 <b>Interpretability</b> \u2013 Machine Learning Blog | ML@CMU | Carnegie Mellon ...", "url": "https://blog.ml.cmu.edu/2020/08/31/6-interpretability/", "isFamilyFriendly": true, "displayUrl": "https://blog.ml.cmu.edu/2020/08/31/6-<b>interpretability</b>", "snippet": "\u201cMythos of Model <b>Interpretability</b>\u201d (Lipton 2016) lists the desiderata for which we desire <b>interpretability</b>, and describes the criteria by which the <b>interpretability</b> of models <b>can</b> be analyzed. We use this framework to discuss recent advances in <b>interpretability</b> research \u2013 LIME, SHAP, and the Olah method \u2013 and the tradeoffs that interpretable models present. Though <b>interpretability</b> research has made large advances, much more remains to be done as machine learning models are deployed ...", "dateLastCrawled": "2022-02-03T05:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Interpretability vs Explainability: The Black</b> Box of Machine Learning ...", "url": "https://www.bmc.com/blogs/machine-learning-interpretability-vs-explainability/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.bmc.com</b>/blogs/machine-learning-<b>interpretability</b>-vs-explainability", "snippet": "<b>Interpretability</b> means that the cause and effect <b>can</b> be determined. If a model <b>can</b> take the inputs, and routinely get the same outputs, the model is interpretable: If you overeat your pasta at dinnertime and you always have troubles sleeping, the situation is interpretable. If all 2016 polls showed a Democratic win and the Republican candidate took office, all those models showed low <b>interpretability</b>. If the pollsters\u2019 goal is to have a good model, which the institution of journalism is ...", "dateLastCrawled": "2022-01-30T19:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Visual <b>Interpretability</b> for <b>Convolutional</b> Neural Networks | by Himanshu ...", "url": "https://towardsdatascience.com/visual-interpretability-for-convolutional-neural-networks-2453856210ce", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/visual-<b>interpretability</b>-for-<b>convolutional</b>-neural...", "snippet": "A deconvnet <b>can</b> <b>be thought</b> of as a convnet model that uses the same components (filtering, pooling) but in reverse, so instead of mapping pixels to features, deconvnets projects the feature activations (convolution outputs) back to the input pixel space. A deconvnet layer (left) attached to a convnet layer (right). [1] To visualize a convnet, a deconvnet is attached to each of its layers, providing a continuous path back to image pixels. To start, an input image is presented to the convnet ...", "dateLastCrawled": "2022-01-30T19:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "What everyone <b>needs to know about interpretability in machine learning</b> ...", "url": "https://dallascard.medium.com/what-everyone-needs-to-know-about-interpretability-in-machine-learning-d5ce16730407", "isFamilyFriendly": true, "displayUrl": "https://dallascard.medium.com/what-everyone-<b>needs-to-know-about-interpretability</b>-in...", "snippet": "There is now essentially a whole subfield of research devoted to <b>interpretability</b> in machine learning, so there\u2019s no chance of covering all of that here. However, given how much confusion seems to be taking place, I <b>thought</b> it would be useful to outline a few essential ideas that everyone should know about this area. 1. Machine learning systems make predictions based on a set of input features (i.e. a bunch of numbers). This point is in some sense so obvious that it\u2019s rarely discussed ...", "dateLastCrawled": "2022-01-12T04:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Machine Learning MCQ questions and answers - PhDTalks", "url": "https://phdtalks.org/2021/10/machine-learning-mcq.html", "isFamilyFriendly": true, "displayUrl": "https://phdtalks.org/2021/10/machine-learning-mcq.html", "snippet": "26. After training an SVM, we <b>can</b> discard all examples which are not support vectors and <b>can</b> still classify new examples. A) TRUE. B) FALSE. 27. Suppose you are dealing with 3 class classification problem and you want to train a SVM model on the data for that you are using One-vs-all method. How many times we need to train our SVM model in such case? A) 1. B) 2. C) 3. D) 4. 28. What is/are true about kernel in SVM? 1. Kernel function <b>map</b> low dimensional data to high dimensional space. 2. It ...", "dateLastCrawled": "2022-02-01T04:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Cartographic Design", "url": "https://saylordotorg.github.io/text_essentials-of-geographic-information-systems/s13-03-cartographic-design.html", "isFamilyFriendly": true, "displayUrl": "https://saylordotorg.github.io/text_essentials-of-geographic-information-systems/s13...", "snippet": "Insets A <b>map</b> within a <b>map</b>. <b>can</b> <b>be thought</b> of as secondary <b>map</b> areas, each encased within their own <b>neat line</b>. These neat lines should be of different thickness or type than other line features on the <b>map</b> to adequately demarcate them from other <b>map</b> features. Insets often display the primary mapped area in relation to a larger area. For example, if the primary <b>map</b> shows the locales of national parks with a county, an inset displaying the location of that county within the larger state boundary ...", "dateLastCrawled": "2022-01-30T00:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Interpretability</b> of Deep Learning Models: A Survey of Results", "url": "https://discovery.ucl.ac.uk/id/eprint/10059575/1/Chakraborty_Interpretability%20of%20deep%20learning%20models.pdf", "isFamilyFriendly": true, "displayUrl": "https://discovery.ucl.ac.uk/id/eprint/10059575/1/Chakraborty_<b>Interpretability</b> of deep...", "snippet": "the human <b>thought</b> process in deep learning models is often referred to as <b>interpretability</b> [2]. One may argue that the above justi\ufb01cation should be in terms of the low-level machine parameters and their sequential updates due to a learning algorithm. However, a closer inspec-tion of even the human <b>thought</b> process reveals that we do not actually interpret the working of our brain in terms of its low-level parameters. We do not justify our predictions based on the learning algorithm used by ...", "dateLastCrawled": "2022-01-30T15:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Demystifying Hidden Units in Neural Networks through Network Dissection ...", "url": "https://medium.com/analytics-vidhya/demystifying-hidden-units-in-neural-networks-through-network-dissection-7d3ac657c428", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/demystifying-hidden-units-in-neural-networks...", "snippet": "The <b>interpretability</b> of individual units is quantified by measuring the alignment between a hidden unit\u2019s response and a set of visual concepts. Human-interpretable concepts include low-level ...", "dateLastCrawled": "2022-01-30T02:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Thought</b> Flow Nets: From Single Predictions to Trains of Model <b>Thought</b> ...", "url": "https://deepai.org/publication/thought-flow-nets-from-single-predictions-to-trains-of-model-thought", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>thought</b>-flow-nets-from-single-predictions-to-trains-of...", "snippet": "Contrary to this, today&#39;s neural classification models are mostly trained to <b>map</b> an input to one single and fixed output. In this paper, we investigate how we <b>can</b> give models the opportunity of a second, third and k-th <b>thought</b>. We take inspiration from Hegel&#39;s dialectics and propose a method that turns an existing classifier&#39;s class prediction (such as the image class forest) into a sequence of predictions (such as forest \u2192 tree \u2192 mushroom). Concretely, we propose a correction module ...", "dateLastCrawled": "2021-12-11T04:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Self-Explaining Neural Networks: A Review</b> - Omar\u2019s Blog", "url": "https://omarelb.github.io/self-explaining-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://omarelb.github.io/<b>self-explaining-neural-networks</b>", "snippet": "For many applications, understanding why a predictive model makes a certain prediction <b>can</b> be of crucial importance. In the paper \u201cTowards Robust <b>Interpretability</b> with <b>Self-Explaining Neural Networks</b>\u201d, David Alvarez-Melis and Tommi Jaakkola propose a neural network model that takes <b>interpretability</b> of predictions into account by design. In this post, we will look at how this model works, how reproducible the paper\u2019s results are, and how the framework <b>can</b> be extended.", "dateLastCrawled": "2022-01-29T06:31:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "6 \u2013 <b>Interpretability</b> \u2013 Machine Learning Blog | ML@CMU | Carnegie Mellon ...", "url": "https://blog.ml.cmu.edu/2020/08/31/6-interpretability/", "isFamilyFriendly": true, "displayUrl": "https://blog.ml.cmu.edu/2020/08/31/6-<b>interpretability</b>", "snippet": "<b>Interpretability</b> takes many forms and <b>can</b> be difficult to define; we first explore general frameworks and sets of definitions in which model <b>interpretability</b> <b>can</b> be evaluated and <b>compared</b> (Lipton 2016, Doshi-Velez &amp; Kim 2017). Next, we analyze several well-known examples of <b>interpretability</b> methods\u2013LIME (Ribeiro et al. 2016), SHAP (Lundberg &amp; Lee 2017), and convolutional neural network visualization (Olah et al. 2018)\u2013in the context of this framework. Model <b>interpretability</b> has no ...", "dateLastCrawled": "2022-02-03T05:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Interpretability</b> in the medical field: A systematic mapping and review ...", "url": "https://www.sciencedirect.com/science/article/pii/S1568494621011522", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1568494621011522", "snippet": "<b>Interpretability</b> <b>can</b> be defined as the degree to which a human <b>can</b> understand the cause of a decision ... which <b>can</b> be explained by the fact that the results <b>can</b> <b>be compared</b> with other techniques evaluated on the same datasets and the availability of public datasets in the medical domain. Additionally, it is preferred to evaluate and interpret ML models on historical data before using real-time evaluation. 23% of the qualified studies were empirically evaluated using case study methods that ...", "dateLastCrawled": "2022-01-23T08:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Local computational methods to improve the interpretability</b> and ...", "url": "https://www.nature.com/articles/s41467-021-21509-5", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41467-021-21509-5", "snippet": "A <b>Map</b> obtained by LocSpiral approach (left) <b>compared</b> with the <b>map</b> as deposited in EMDB for EMD-21375. B B -factor maps to be used for sharpening (slope of the local Guinier plot multiplied by 4 ...", "dateLastCrawled": "2022-01-26T08:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) <b>Interpretability</b> in the medical field: A systematic mapping and ...", "url": "https://www.researchgate.net/publication/357475684_Interpretability_in_the_medical_field_A_systematic_mapping_and_review_study", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/357475684_<b>Interpretability</b>_in_the_medical...", "snippet": "Methods: This review was carried out according to the well-known systematic <b>map</b> and review process to analyze the literature on <b>interpretability</b> techniques when applied in the medical field with ...", "dateLastCrawled": "2022-01-26T13:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Interpretability vs Explainability: The Black</b> Box of Machine Learning ...", "url": "https://www.bmc.com/blogs/machine-learning-interpretability-vs-explainability/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.bmc.com</b>/blogs/machine-learning-<b>interpretability</b>-vs-explainability", "snippet": "<b>Interpretability</b> has to do with how accurate a machine learning model <b>can</b> associate a cause to an effect. Explainability has to do with the ability of the parameters, often hidden in Deep Nets, to justify the results. This is a long article. Hang in there and, by the end, you will understand: How <b>interpretability</b> is different from explainability.", "dateLastCrawled": "2022-01-30T19:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Interpretability</b> in Machine Learning: An Overview", "url": "https://thegradient.pub/interpretability-in-ml-a-broad-overview/", "isFamilyFriendly": true, "displayUrl": "https://thegradient.pub/<b>interpretability</b>-in-ml-a-broad-overview", "snippet": "Even once we get improved notions of <b>interpretability</b> with intuitive properties, it remains to be seen if we <b>can</b> use them to achieve the benefits I listed out in the very beginning. While it certainly seems more challenging to formalize <b>interpretability</b> than to use it well, I&#39;m glad that empirical tests are already being done; they <b>can</b> hopefully also guide where the research goes next.", "dateLastCrawled": "2022-02-01T15:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Radiomics Feature Activation Maps as</b> a New Tool for Signature ...", "url": "https://pubmed.ncbi.nlm.nih.gov/33364192/", "isFamilyFriendly": true, "displayUrl": "https://pubmed.ncbi.nlm.nih.gov/33364192", "snippet": "Introduction: In the field of personalized medicine, radiomics has shown its potential to support treatment decisions. However, the limited feature <b>interpretability</b> hampers its introduction into the clinics. Here, we propose a new methodology to create radiomics feature activation maps, which allows to identify the spatial-anatomical locations responsible for signature activation based on local radiomics.", "dateLastCrawled": "2021-01-11T03:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "On <b>the Connection Between Adversarial Robustness and</b> Saliency <b>Map</b> ...", "url": "https://deepai.org/publication/on-the-connection-between-adversarial-robustness-and-saliency-map-interpretability", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/on-<b>the-connection-between-adversarial-robustness-and</b>...", "snippet": "On <b>the Connection Between Adversarial Robustness and Saliency Map Interpretability</b>. 05/10/2019 . \u2219 . by Christian Etmann, et al. \u2219. University of Cambridge \u2219. Universit\u00e4t Bremen \u2219. 0 \u2219. share Recent studies on the adversarial vulnerability of neural networks have shown that models trained to be more robust to adversarial attacks exhibit more interpretable saliency maps than their non-robust counterparts. We aim to quantify this behavior by considering the alignment between input ...", "dateLastCrawled": "2022-01-21T19:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>RPGAN: GANs Interpretability via Random Routing</b> | DeepAI", "url": "https://deepai.org/publication/rpgan-gans-interpretability-via-random-routing", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>rpgan-gans-interpretability-via-random-routing</b>", "snippet": "Thus, the generator defines a <b>map</b> from the Cartesian product m 1 \u00d7 m 2 \u00d7 \u22ef \u00d7 m n to the image space. Note that we <b>can</b> take an arbitrary existing GAN model, group its generator layers into buckets and replicate them into multiple blocks. In these terms, the original model <b>can</b> be treated as the RPGAN model with a single block in each bucket and random input noise. Note that during image generation we perform the same number of operations as in the standard GAN generator. For each", "dateLastCrawled": "2022-01-15T16:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(PDF) <b>Radiomics Feature Activation Maps as</b> a New Tool for Signature ...", "url": "https://www.researchgate.net/publication/347399029_Radiomics_Feature_Activation_Maps_as_a_New_Tool_for_Signature_Interpretability", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/347399029_Radiomics_Feature_Activation_<b>Map</b>s...", "snippet": "A binary activation <b>map</b> was created for each patient using the median global feature value from the training. The ratios of activated/non-activated patches of GTV and rim regions were <b>compared</b> ...", "dateLastCrawled": "2022-01-14T17:13:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Interpretability</b> in <b>Machine</b> <b>Learning</b>: An Overview", "url": "https://thegradient.pub/interpretability-in-ml-a-broad-overview/", "isFamilyFriendly": true, "displayUrl": "https://thegradient.pub/<b>interpretability</b>-in-ml-a-broad-overview", "snippet": "First, <b>interpretability</b> in <b>machine</b> <b>learning</b> is useful because it can aid in trust. As humans, we may be reluctant to rely on <b>machine</b> <b>learning</b> models for certain critical tasks, e.g., medical diagnosis, unless we know &quot;how they work.&quot; There&#39;s often a fear of the unknown when trusting in something opaque, which we see when people confront new technology, and this can slow down adoption. Approaches to <b>interpretability</b> that focus on transparency could help mitigate some of these fears.", "dateLastCrawled": "2022-02-01T15:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Towards Analogy-Based Explanations in Machine Learning</b> | DeepAI", "url": "https://deepai.org/publication/towards-analogy-based-explanations-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>towards-analogy-based-explanations-in-machine-learning</b>", "snippet": "More specifically, we take the view that an <b>analogy</b>-based approach is a viable alternative to existing approaches in the realm of explainable AI and interpretable <b>machine</b> <b>learning</b>, and that <b>analogy</b>-based explanations of the predictions produced by a <b>machine</b> <b>learning</b> algorithm can complement similarity-based explanations in a meaningful way. To corroborate these claims, we outline the basic idea of an <b>analogy</b>-based explanation and illustrate its potential usefulness by means of some examples.", "dateLastCrawled": "2022-01-10T12:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Interpretability</b> in <b>Machine</b> <b>Learning</b>", "url": "https://www.cl.cam.ac.uk/teaching/1819/P230/IWML-Lecture-4.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cl.cam.ac.uk/teaching/1819/P230/IWML-Lecture-4.pdf", "snippet": "This provides a novel <b>analogy</b> between data compression and regularization. Qualitative and quantitative state-of-the-art results on three datasets. 20 / 33. Interpretable Lens Variable Model (ILVM) 21 / 33. Interactive <b>Interpretability</b> via Active <b>Learning</b> Interactive \u2018human-in-the-loop\u2019 <b>interpretability</b> Choose the point with index j that maximizes : ^j = argmax jI(s ; ) = H(s ) E q \u02da(z js)[H(s jz j)] = Z p(s j)log p(s j)ds + E q \u02da(z js) Z p (s jjz)log p (s jjz)ds : (5) Choose the point ...", "dateLastCrawled": "2022-01-19T17:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Towards <b>Analogy</b>-Based Explanations in <b>Machine</b> <b>Learning</b>", "url": "https://link.springer.com/chapter/10.1007/978-3-030-57524-3_17", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/chapter/10.1007/978-3-030-57524-3_17", "snippet": "More specifically, we take the view that an <b>analogy</b>-based approach is a viable alternative to existing approaches in the realm of explainable AI and interpretable <b>machine</b> <b>learning</b>, and that <b>analogy</b>-based explanations of the predictions produced by a <b>machine</b> <b>learning</b> algorithm can complement similarity-based explanations in a meaningful way. To corroborate these claims, we outline the basic idea of an <b>analogy</b>-based explanation and illustrate its potential usefulness by means of some examples.", "dateLastCrawled": "2022-01-16T03:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Towards <b>Analogy</b>-Based Explanations in <b>Machine</b> <b>Learning</b>", "url": "https://arxiv.org/abs/2005.12800", "isFamilyFriendly": true, "displayUrl": "https://<b>arxiv</b>.org/abs/2005.12800", "snippet": "Principles of analogical reasoning have recently been applied in the context of <b>machine</b> <b>learning</b>, for example to develop new methods for classification and preference <b>learning</b>. In this paper, we argue that, while analogical reasoning is certainly useful for constructing new <b>learning</b> algorithms with high predictive accuracy, is is arguably not less interesting from an <b>interpretability</b> and explainability point of view. More specifically, we take the view that an <b>analogy</b>-based approach is a ...", "dateLastCrawled": "2021-10-24T20:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Economic Methodology Meets Interpretable <b>Machine</b> <b>Learning</b> ...", "url": "https://bcmullins.github.io/economic_methodology_interpretable_ml_intro/", "isFamilyFriendly": true, "displayUrl": "https://bcmullins.github.io/economic_methodology_interpretable_ml_intro", "snippet": "In this series of posts, we will develop an <b>analogy</b> between the realistic assumptions debate in economic methodology and the current discussion over <b>interpretability</b> when using <b>machine</b> <b>learning</b> models in the wild. While this connection may seem fuzzy at first, the past seventy years or so of economic methodology offers many lessons for <b>machine</b> <b>learning</b> theorists and practitioners to avoid analysis paralysis and make progress on the <b>interpretability</b> issue - one way or the other. But first ...", "dateLastCrawled": "2022-01-05T13:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "6 \u2013 <b>Interpretability</b> \u2013 <b>Machine</b> <b>Learning</b> Blog | ML@CMU | Carnegie Mellon ...", "url": "https://blog.ml.cmu.edu/2020/08/31/6-interpretability/", "isFamilyFriendly": true, "displayUrl": "https://blog.ml.cmu.edu/2020/08/31/6-<b>interpretability</b>", "snippet": "Figure 1: <b>Interpretability</b> for <b>machine</b> <b>learning</b> models bridges the concrete objectives models optimize for and the real-world (and less easy to define) desiderata that ML applications aim to achieve. Introduction . The objectives <b>machine</b> <b>learning</b> models optimize for do not always reflect the actual desiderata of the task at hand. <b>Interpretability</b> in models allows us to evaluate their decisions and obtain information that the objective alone cannot confer. <b>Interpretability</b> takes many forms ...", "dateLastCrawled": "2022-02-03T05:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Economic Methodology Meets Interpretable Machine Learning</b> - Part I ...", "url": "https://bcmullins.github.io/economic_methodology_interpretable_ml_blackboxes/", "isFamilyFriendly": true, "displayUrl": "https://bcmullins.github.io/economic_methodology_interpretable_ml_blackboxes", "snippet": "In this series of posts, we will develop an <b>analogy</b> between the realistic assumptions debate in economic methodology and the current discussion over <b>interpretability</b> when using <b>machine</b> <b>learning</b> models in the wild. While this connection may seem fuzzy at first, the past seventy years or so of economic methodology offers many lessons for <b>machine</b> <b>learning</b> theorists and practitioners to avoid analysis paralysis and make progress on the <b>interpretability</b> issue - one way or the other. Intro - Part ...", "dateLastCrawled": "2022-01-22T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Confusion Matrices</b> &amp; <b>Interpretable ML</b> | by andrea b | high stakes ...", "url": "https://medium.com/high-stakes-design/interpretability-techniques-explained-in-simple-terms-f5e1573674f3", "isFamilyFriendly": true, "displayUrl": "https://medium.com/high-stakes-design/<b>interpretability</b>-techniques-explained-in-simple...", "snippet": "The best [<b>analogy</b>] I can think of is an indicator light in your car \u2014 [and the] <b>machine</b> that you plug in to tell you more about the readout. ANDREA: Do you see <b>interpretability</b>, primarily, as ...", "dateLastCrawled": "2021-03-22T05:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Analogies between Biology and Deep <b>Learning</b> [rough note] -- colah&#39;s blog", "url": "http://colah.github.io/notes/bio-analogies/", "isFamilyFriendly": true, "displayUrl": "colah.github.io/notes/bio-analogies", "snippet": "Neuroscience \u2194 <b>Interpretability</b>. <b>Analogy</b>: model=brain. Artificial neural networks are historically inspired by neuroscience, but I used to be pretty skeptical that the connection was anything more than superficial. I&#39;ve since come around: I now think this is a very deep connection. The thing that personally persuaded me was that, in my own investigations of what goes on inside neural networks, we kept finding things that were previously discovered by neuroscientists. The most recent ...", "dateLastCrawled": "2022-01-30T16:04:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Chris Olah on what the hell is going on inside neural networks - 80,000 ...", "url": "https://80000hours.org/podcast/episodes/chris-olah-interpretability-research/", "isFamilyFriendly": true, "displayUrl": "https://80000hours.org/podcast/episodes/chris-olah-interpretability-research", "snippet": "Chris is a <b>machine</b> <b>learning</b> researcher currently focused on neural network interpretability. Until last December he led OpenAI\u2019s interpretability team but along with some colleagues he recently moved on to help start a new AI lab focussed on large models and safety called Anthropic. Rob Wiblin: Before OpenAI he spent 4 years at Google Brain developing tools to visualize what\u2019s going on in neural networks. Chris was hugely impactful at Google Brain. He was second author on the launch of ...", "dateLastCrawled": "2022-02-01T04:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Marketing AI: Interpretability and Explainability</b> - Christopher S. Penn ...", "url": "https://www.christopherspenn.com/2021/03/marketing-ai-interpretability-and-explainability/", "isFamilyFriendly": true, "displayUrl": "https://www.christopherspenn.com/2021/03/<b>marketing-ai-interpretability-and-explainability</b>", "snippet": "<b>Interpretability is like</b> inspecting the baker\u2019s recipe for the cake. We look at the list of ingredients and the steps taken to bake the cake, and we verify that the recipe makes sense and the ingredients were good. This is a much more rigorous way of validating our results, but it\u2019s the most complete \u2013 and if we\u2019re in a high-stakes situation where we need to remove all doubt, this is the approach we take. Interpretability in AI is like that \u2013 we step through the code itself that ...", "dateLastCrawled": "2022-01-29T12:09:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Causal <b>Learning</b> From Predictive Modeling for Observational Data", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7931928/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7931928", "snippet": "Given the recent success of <b>machine</b> <b>learning</b>, specifically deep <b>learning</b>, in several applications (Goodfellow et al., ... This statistical <b>interpretability is similar</b> in spirit to traditional interpretability. This allows to answer questions, such as \u201cdoes BMI influence susceptibility to Covid?\u201d Moreover, it has been argued that developing an effective CBN for practical applications requires expert knowledge when data collection is cumbersome (Fenton and Neil, 2012). This applies to ...", "dateLastCrawled": "2021-12-09T23:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Optimal <b>Predictive Clustering</b> - Dimitris Bertsimas", "url": "https://dbertsim.mit.edu/pdfs/papers/2020-sobiesk-optimal-predictive-clustering.pdf", "isFamilyFriendly": true, "displayUrl": "https://dbertsim.mit.edu/pdfs/papers/2020-sobiesk-optimal-<b>predictive-clustering</b>.pdf", "snippet": "Table 1 Comparison of major <b>machine</b> <b>learning</b> methods relative to each other across the metrics of performance (out-of-sample R2), scalability and interpretability. 1 is the best, while 5 is the worst. Optimal <b>Predictive Clustering</b> 3 From Table 1, we observe all existing methods have weakness in at least one category. We therefore seek to design a method that has strong performance in all three categories at the same time. Optimal <b>Predictive Clustering</b> (OPC) is an algorithm that uses mixed ...", "dateLastCrawled": "2021-11-24T20:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Reviewing Challenges of Predicting Protein Melting Temperature Change ...", "url": "https://link.springer.com/article/10.1007/s12033-021-00349-0", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s12033-021-00349-0", "snippet": "Predicting the effects of mutations on protein stability is a key problem in fundamental and applied biology, still unsolved even for the relatively simple case of small, soluble, globular, monomeric, two-state-folder proteins. Many articles discuss the limitations of prediction methods and of the datasets used to train them, which result in low reliability for actual applications despite globally capturing trends. Here, we review these and other issues by analyzing one of the most detailed ...", "dateLastCrawled": "2022-02-03T02:54:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Interpretable Machine Learning: Advantages and Disadvantages</b> | by ...", "url": "https://towardsdatascience.com/interpretable-machine-learning-advantages-and-disadvantages-901769f48c43", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>interpretable-machine-learning-advantages-and</b>...", "snippet": "In my view, a shor t coming of interpretable <b>machine</b> <b>learning</b> is that it assumes to a degree that the data being fed into the model is always going to be suitable for human interpretation. This is not necessarily the case. For instance, let\u2019s say that a company is trying to implement interpretable <b>machine</b> <b>learning</b> to devise a credit scoring model, whereby prospective credit card applications are classified as approved or rejected based on numerous features. It is often the case that such ...", "dateLastCrawled": "2022-01-19T03:58:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Breaking the interpretability barrier - a</b> method for interpreting deep ...", "url": "http://www.di.uniba.it/~loglisci/NFMCP2019/NFMCP/nfMCP2019_paper_17.pdf", "isFamilyFriendly": true, "displayUrl": "www.di.uniba.it/~loglisci/NFMCP2019/NFMCP/nfMCP2019_paper_17.pdf", "snippet": "Last, but not least, <b>interpretability can be thought of as</b> a useful tool for understanding and correcting model errors. In general, we are faced with a trade-o between performance and inter-pretability. Graph classi cation is normally a domain which requires the ap-plication of complex <b>learning</b> models, such as deep neural networks, which are not interpretable by nature. Several relevant attempts have been made to in-terpret complex models post-hoc (brie y reviewed in section2). However, most ...", "dateLastCrawled": "2021-09-22T17:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Discovering Discriminative Nodes for Classi\ufb01cation with Deep Graph ...", "url": "http://muresanlab.tins.ro/publications/preprints/Palcu_et_al_LNAI_2020.pdf", "isFamilyFriendly": true, "displayUrl": "muresanlab.tins.ro/publications/preprints/Palcu_et_al_LNAI_2020.pdf", "snippet": "Last, but not least, <b>interpretability can be thought of as</b> a useful tool for understanding and correcting model errors. In general, we are faced with a trade-o\ufb00 between performance and inter-pretability. Graph classi\ufb01cation is normally a domain which requires the appli-cation of complex <b>learning</b> models, such as deep neural networks, which are not interpretable by nature. Several relevant attempts have been made to interpret complex models post-hoc (brie\ufb02y reviewed in Sect.2). However ...", "dateLastCrawled": "2021-09-02T02:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Confronting Abusive Language Online: A Survey from the Ethical and ...", "url": "https://www.arxiv-vanity.com/papers/2012.12305/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2012.12305", "snippet": "The pervasiveness of abusive content on the internet can lead to severe psychological and physical harm. Significant effort in Natural Language Processing (NLP) research has been devoted to addressing this problem through abusive content detection and related sub-areas, such as the detection of hate speech, toxicity, cyberbullying, etc. Although current technologies achieve high classification performance in research studies, it has been observed that the real-life application of this ...", "dateLastCrawled": "2021-10-13T19:21:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(interpretability)  is like +(map)", "+(interpretability) is similar to +(map)", "+(interpretability) can be thought of as +(map)", "+(interpretability) can be compared to +(map)", "machine learning +(interpretability AND analogy)", "machine learning +(\"interpretability is like\")", "machine learning +(\"interpretability is similar\")", "machine learning +(\"just as interpretability\")", "machine learning +(\"interpretability can be thought of as\")", "machine learning +(\"interpretability can be compared to\")"]}