{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "What is <b>an intuitive explanation of stochastic gradient descent</b>? - Quora", "url": "https://www.quora.com/What-is-an-intuitive-explanation-of-stochastic-gradient-descent", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>an-intuitive-explanation-of-stochastic-gradient-descent</b>", "snippet": "Answer (1 of 10): Let us say you have gone on a road trip. You get lost just 25 KM (KiloMetres) before the destination. There are people walking all along the highway..you can ask them for directions. Let us say you encounter around 10 people for each KM (one every 100 meters). Most of them know...", "dateLastCrawled": "2022-01-26T17:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Gradient</b> <b>descent</b> - Hashnode", "url": "https://hashnode.com/post/gradient-descent-ckw0ewd5f00xo0as10hy2971i", "isFamilyFriendly": true, "displayUrl": "https://hashnode.com/post/<b>gradient</b>-<b>descent</b>-ckw0ewd5f00xo0as10hy2971i", "snippet": "<b>Gradient</b> <b>Descent</b> is an optimization algorithm for finding a local minimum of a differentiable function. <b>Gradient</b> <b>descent</b> is simply used in machine learning to find the values of a function\u2019s parameters (coefficients) that minimize a cost function as far as possible. Imagine a blindfolded man who wants to climb to the top of a hill with the fewest steps along the way as possible. He might start <b>climbing</b> the hill by taking really big steps in the steepest direction, which he can do as long ...", "dateLastCrawled": "2022-01-21T03:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 2, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Gradient descent</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Gradient_descent", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Gradient_descent</b>", "snippet": "<b>Gradient descent</b> is based on the observation that if the multi-variable function is defined and differentiable in a neighborhood of a point , then () decreases fastest if one goes from in the direction of the negative <b>gradient</b> of at , ().It follows that, if + = for a small enough step size or learning rate +, then (+).In other words, the term () is subtracted from because we want to move against the <b>gradient</b>, toward the local minimum. With this observation in mind, one starts with a guess ...", "dateLastCrawled": "2022-02-03T03:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Popular Optimization Algorithms In Deep Learning", "url": "https://dataaspirant.com/optimization-algorithms-deep-learning/", "isFamilyFriendly": true, "displayUrl": "https://dataaspirant.com/optimization-algorithms-deep-learning", "snippet": "<b>Stochastic</b> <b>gradient</b> <b>descent</b> \u2014 many parameters . In deep neural networks, every weight is a parameter. As deep learning models are higher dimensional, there could be millions or even more parameters. Still, <b>Stochastic</b> <b>gradient</b> <b>descent</b> works the same just you need to compute the partial derivatives of the given function.", "dateLastCrawled": "2022-02-02T16:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Gradient Descent</b> in Python. When you venture into machine learning ...", "url": "https://towardsdatascience.com/gradient-descent-in-python-a0d07285742f", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>gradient-descent</b>-in-python-a0d07285742f", "snippet": "To explain in brief about <b>gradient descent</b>, imagine that you are on a <b>mountain</b> and are blindfolded and your task is to come down from the <b>mountain</b> to the flat land without assistance. The only assistance you have is a gadget which tells you the height from sea-level. What would be your approach be. You would start to descend in some random direction and then ask the gadget what is the height now. If the gadget tells you that height and it is more than the initial height then you know you ...", "dateLastCrawled": "2022-02-02T17:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Can hinge loss be optimized with <b>stochastic</b> <b>gradient</b> <b>descent</b>? - Quora", "url": "https://www.quora.com/Can-hinge-loss-be-optimized-with-stochastic-gradient-descent", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Can-hinge-loss-be-optimized-with-<b>stochastic</b>-<b>gradient</b>-<b>descent</b>", "snippet": "Answer (1 of 2): Yes. I&#39;ve coauthored a paper where we look at this, and specifically at the influence of minibatch size on the convergence rate: http://proceedings ...", "dateLastCrawled": "2022-01-22T10:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>SGD</b> - programmersought.com", "url": "https://www.programmersought.com/article/61415716542/", "isFamilyFriendly": true, "displayUrl": "https://www.programmersought.com/article/61415716542", "snippet": "Because of my low level of knowledge, the following explanations may be more detailed or simple. If there are errors, please be sure to correct them. \u2014\u201420170729 Part of the co", "dateLastCrawled": "2021-07-12T17:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "optimization - Why is the <b>gradient</b> the best direction to move in ...", "url": "https://stats.stackexchange.com/questions/207139/why-is-the-gradient-the-best-direction-to-move-in", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/207139", "snippet": "$\\begingroup$ The <b>gradient</b> is not necessarily the best direction to move in. Newton or Quasi-Newton methods are based on a quadratic approximation of the objective function, using the exact or approximate Hessian for the Taylor expansion. <b>Gradient</b> <b>descent</b> is in effect based on this same quadratic approximation, except using the identity matrix in place of the Hessian, which is therefore a less accurate quadratic approximation, and thereby often resulting in an inferior direction. $\\endgroup ...", "dateLastCrawled": "2022-01-18T23:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "multivariable calculus - Why is <b>gradient</b> the direction of <b>steepest</b> ...", "url": "https://math.stackexchange.com/questions/223252/why-is-gradient-the-direction-of-steepest-ascent", "isFamilyFriendly": true, "displayUrl": "https://math.stackexchange.com/questions/223252", "snippet": "$\\begingroup$ I <b>like</b> this anwer a lot, and my intuition also was, that the <b>gradient</b> points in the direction of greatest change. But that is not the same as ascent. E.g. in the <b>gradient</b> <b>descent</b> algorithm one always uses the negative <b>gradient</b>, suggesting ascent but not <b>descent</b>.", "dateLastCrawled": "2022-02-03T06:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "\u51f8\u4f18\u5316\u7b97\u6cd5-\u65e0\u7ea6\u675f\u95ee\u9898-\u4e0b\u964d\u6cd5\uff08<b>Descent</b> Methods\uff09_\u6280\u672f\u5e76\u4e0d\u662f\u6211\u4eec\u7684\u5168\u90e8-CSDN\u535a\u5ba2", "url": "https://blog.csdn.net/MadJieJie/article/details/118728129", "isFamilyFriendly": true, "displayUrl": "https://blog.csdn.net/MadJieJie/article/details/118728129", "snippet": "Directory1. <b>Descent</b> Methods2. <b>Gradient</b> <b>Descent</b> MethodInterpretation of GDMDifferential\uff081\uff09 the differential of one variable\uff082\uff09the differential of variables\uff083\uff09Gradient3. Batch <b>gradient</b> <b>descent</b> Method4. <b>Stochastic</b> <b>gradient</b> <b>descent</b> Method5. Mini-Batch Gradie..", "dateLastCrawled": "2021-12-14T22:31:00.0000000Z", "language": "zh_chs", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "What is <b>an intuitive explanation of stochastic gradient descent</b>? - Quora", "url": "https://www.quora.com/What-is-an-intuitive-explanation-of-stochastic-gradient-descent", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>an-intuitive-explanation-of-stochastic-gradient-descent</b>", "snippet": "Answer (1 of 10): Let us say you have gone on a road trip. You get lost just 25 KM (KiloMetres) before the destination. There are people walking all along the highway..you can ask them for directions. Let us say you encounter around 10 people for each KM (one every 100 meters). Most of them know...", "dateLastCrawled": "2022-01-26T17:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Popular Optimization Algorithms In Deep Learning", "url": "https://dataaspirant.com/optimization-algorithms-deep-learning/", "isFamilyFriendly": true, "displayUrl": "https://dataaspirant.com/optimization-algorithms-deep-learning", "snippet": "<b>Stochastic</b> <b>gradient</b> <b>descent</b> \u2014 many parameters . In deep neural networks, every weight is a parameter. As deep learning models are higher dimensional, there could be millions or even more parameters. Still, <b>Stochastic</b> <b>gradient</b> <b>descent</b> works the same just you need to compute the partial derivatives of the given function.", "dateLastCrawled": "2022-02-02T16:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 2, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Gradient descent</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Gradient_descent", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Gradient_descent</b>", "snippet": "<b>Gradient descent</b> is based on the observation that if the multi-variable function is defined and differentiable in a neighborhood of a point , then () decreases fastest if one goes from in the direction of the negative <b>gradient</b> of at , ().It follows that, if + = for a small enough step size or learning rate +, then (+).In other words, the term () is subtracted from because we want to move against the <b>gradient</b>, toward the local minimum. With this observation in mind, one starts with a guess ...", "dateLastCrawled": "2022-02-03T03:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Gradient Descent</b> in Python. When you venture into machine learning ...", "url": "https://towardsdatascience.com/gradient-descent-in-python-a0d07285742f", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>gradient-descent</b>-in-python-a0d07285742f", "snippet": "To explain in brief about <b>gradient descent</b>, imagine that you are on a <b>mountain</b> and are blindfolded and your task is to come down from the <b>mountain</b> to the flat land without assistance. The only assistance you have is a gadget which tells you the height from sea-level. What would be your approach be. You would start to descend in some random direction and then ask the gadget what is the height now. If the gadget tells you that height and it is more than the initial height then you know you ...", "dateLastCrawled": "2022-02-02T17:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Can hinge loss be optimized with <b>stochastic</b> <b>gradient</b> <b>descent</b>? - Quora", "url": "https://www.quora.com/Can-hinge-loss-be-optimized-with-stochastic-gradient-descent", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Can-hinge-loss-be-optimized-with-<b>stochastic</b>-<b>gradient</b>-<b>descent</b>", "snippet": "Answer (1 of 2): Yes. I&#39;ve coauthored a paper where we look at this, and specifically at the influence of minibatch size on the convergence rate: http://proceedings ...", "dateLastCrawled": "2022-01-22T10:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>SGD</b> - programmersought.com", "url": "https://www.programmersought.com/article/61415716542/", "isFamilyFriendly": true, "displayUrl": "https://www.programmersought.com/article/61415716542", "snippet": "Because of my low level of knowledge, the following explanations may be more detailed or simple. If there are errors, please be sure to correct them. \u2014\u201420170729 Part of the co", "dateLastCrawled": "2021-07-12T17:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Susceptibility assessment of earthquake-induced landslides</b> using ...", "url": "https://www.researchgate.net/publication/251507276_Susceptibility_assessment_of_earthquake-induced_landslides_using_Bayesian_network_A_case_study_in_Beichuan_China", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/251507276_Susceptibility_assessment_of...", "snippet": "The main purpose of this study was to compare the performance of Support Vector Machines (SVM), <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> (<b>SGD</b>), and Bayesian Logistic Regression (BLR) algorithms for landslide ...", "dateLastCrawled": "2022-01-27T18:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Hands-On Transfer Learning with Python: Implement advanced deep ...", "url": "https://dokumen.pub/hands-on-transfer-learning-with-python-implement-advanced-deep-learning-and-neural-network-models-using-tensorflow-and-keras-1788831306-9781788831307.html", "isFamilyFriendly": true, "displayUrl": "https://<b>dokumen.pub</b>/hands-on-transfer-learning-with-python-implement-advanced-deep...", "snippet": "Advances in optimization algorithms that are used to train neural networks: Traditionally, there was only one algorithm used to learn the weights in a neural network, <b>gradient</b> <b>descent</b> or <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> (<b>SGD</b>). <b>SGD</b> has a few limitations, such as getting stuck in a local minima and slow convergence, that are overcome by the newer algorithms. We will discuss these algorithms in detail in the later sections on Neural network basics.", "dateLastCrawled": "2021-12-15T01:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "\u51f8\u4f18\u5316\u7b97\u6cd5-\u65e0\u7ea6\u675f\u95ee\u9898-\u4e0b\u964d\u6cd5\uff08<b>Descent</b> Methods\uff09_\u6280\u672f\u5e76\u4e0d\u662f\u6211\u4eec\u7684\u5168\u90e8-CSDN\u535a\u5ba2", "url": "https://blog.csdn.net/MadJieJie/article/details/118728129", "isFamilyFriendly": true, "displayUrl": "https://blog.csdn.net/MadJieJie/article/details/118728129", "snippet": "Directory1. <b>Descent</b> Methods2. <b>Gradient</b> <b>Descent</b> MethodInterpretation of GDMDifferential\uff081\uff09 the differential of one variable\uff082\uff09the differential of variables\uff083\uff09Gradient3. Batch <b>gradient</b> <b>descent</b> Method4. <b>Stochastic</b> <b>gradient</b> <b>descent</b> Method5. Mini-Batch Gradie..", "dateLastCrawled": "2021-12-14T22:31:00.0000000Z", "language": "zh_chs", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(PDF) <b>Adaptive Computation and Machine Learning series- Deep learning</b> ...", "url": "https://www.academia.edu/38223830/Adaptive_Computation_and_Machine_Learning_series_Deep_learning_The_MIT_Press_2016_pdf", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/38223830/<b>Adaptive_Computation_and_Machine_Learning_series</b>...", "snippet": "<b>Adaptive Computation and Machine Learning series- Deep learning</b>-The MIT Press (2016).pdf", "dateLastCrawled": "2022-02-02T06:14:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Popular Optimization Algorithms In Deep Learning", "url": "https://dataaspirant.com/optimization-algorithms-deep-learning/", "isFamilyFriendly": true, "displayUrl": "https://dataaspirant.com/optimization-algorithms-deep-learning", "snippet": "<b>Stochastic</b> <b>gradient</b> <b>descent</b> \u2014 many parameters . In deep neural networks, every weight is a parameter. As deep learning models are higher dimensional, there could be millions or even more parameters. Still, <b>Stochastic</b> <b>gradient</b> <b>descent</b> works the same just you need to compute the partial derivatives of the given function.", "dateLastCrawled": "2022-02-02T16:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 1, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Gradient descent</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Gradient_descent", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Gradient_descent</b>", "snippet": "<b>Gradient descent</b> is based on the observation that if the multi-variable function is defined and differentiable in a neighborhood of a point , then () decreases fastest if one goes from in the direction of the negative <b>gradient</b> of at , ().It follows that, if + = for a small enough step size or learning rate +, then (+).In other words, the term () is subtracted from because we want to move against the <b>gradient</b>, toward the local minimum. With this observation in mind, one starts with a guess ...", "dateLastCrawled": "2022-02-03T03:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "optimization - Why is the <b>gradient</b> the best direction to move in ...", "url": "https://stats.stackexchange.com/questions/207139/why-is-the-gradient-the-best-direction-to-move-in", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/207139", "snippet": "$\\begingroup$ The <b>gradient</b> is not necessarily the best direction to move in. Newton or Quasi-Newton methods are based on a quadratic approximation of the objective function, using the exact or approximate Hessian for the Taylor expansion. <b>Gradient</b> <b>descent</b> is in effect based on this same quadratic approximation, except using the identity matrix in place of the Hessian, which is therefore a less accurate quadratic approximation, and thereby often resulting in an inferior direction. $\\endgroup ...", "dateLastCrawled": "2022-01-18T23:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "multivariable calculus - Why is <b>gradient</b> the direction of <b>steepest</b> ...", "url": "https://math.stackexchange.com/questions/223252/why-is-gradient-the-direction-of-steepest-ascent", "isFamilyFriendly": true, "displayUrl": "https://math.stackexchange.com/questions/223252", "snippet": "This means that the <b>gradient</b> will always point in the direction of the <b>steepest</b> <b>descent</b> (nb: which is of course not a proof but a hand-waving indication of its behaviour to give some intuition only!) For a little bit of background and the code for creating the animation see here: Why <b>Gradient</b> <b>Descent</b> Works (and How To Animate 3D-Functions in R) .", "dateLastCrawled": "2022-02-03T06:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Deep Learning for Computer Vision with Python: ImageNet Bundle ...", "url": "https://ebin.pub/deep-learning-for-computer-vision-with-python-imagenet-bundle-1722487860-9781722487867-d-7948098.html", "isFamilyFriendly": true, "displayUrl": "https://ebin.pub/deep-learning-for-computer-vision-with-python-imagenet-bundle...", "snippet": "In fact, this automatic training procedure formed the basis of <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> (<b>SGD</b>) which is still used to train very deep neural networks today. During this time period, Perceptron-based techniques were all the rage in the neural network community. However, a 1969 publication by Minsky and Papert [14] effectively stagnated neural network research for nearly a decade. Their work demonstrated that a Perceptron with a linear activation function (regardless of depth) was merely a ...", "dateLastCrawled": "2022-01-20T03:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) Python Machine Learning Third Edition Machine Learning and Deep ...", "url": "https://www.academia.edu/45601598/Python_Machine_Learning_Third_Edition_Machine_Learning_and_Deep_Learning_with_Python_scikit_learn_and_TensorFlow_2", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/45601598/Python_Machine_Learning_Third_Edition_Machine...", "snippet": "Python Machine Learning Third Edition Machine Learning and Deep Learning with Python, scikit-learn, and TensorFlow 2", "dateLastCrawled": "2022-01-27T14:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Deep <b>learning: adaptive computation and machine learning 0262035618</b> ...", "url": "https://dokumen.pub/deep-learning-adaptive-computation-and-machine-learning-0262035618-9780262035613.html", "isFamilyFriendly": true, "displayUrl": "https://dokumen.pub/deep-<b>learning-adaptive-computation-and-machine-learning-0262035618</b>...", "snippet": "The training algorithm used to adapt the weights of the ADALINE was a special case of an algorithm called <b>stochastic</b> <b>gradient</b> <b>descent</b>. Slightly modified versions of the <b>stochastic</b> <b>gradient</b> <b>descent</b> algorithm remain the dominant training algorithms for deep learning models today. Models based on the f (x, w) used by the perceptron and ADALINE are called linear models. These models remain some of the most widely used machine learning models, though in many cases they are trained in different ...", "dateLastCrawled": "2022-01-24T19:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Hands-On Transfer Learning with Python: Implement advanced deep ...", "url": "https://dokumen.pub/hands-on-transfer-learning-with-python-implement-advanced-deep-learning-and-neural-network-models-using-tensorflow-and-keras-1788831306-9781788831307.html", "isFamilyFriendly": true, "displayUrl": "https://<b>dokumen.pub</b>/hands-on-transfer-learning-with-python-implement-advanced-deep...", "snippet": "Advances in optimization algorithms that are used to train neural networks: Traditionally, there was only one algorithm used to learn the weights in a neural network, <b>gradient</b> <b>descent</b> or <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> (<b>SGD</b>). <b>SGD</b> has a few limitations, such as getting stuck in a local minima and slow convergence, that are overcome by the newer algorithms. We will discuss these algorithms in detail in the later sections on Neural network basics.", "dateLastCrawled": "2021-12-15T01:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) <b>Adaptive Computation and Machine Learning series- Deep learning</b> ...", "url": "https://www.academia.edu/38223830/Adaptive_Computation_and_Machine_Learning_series_Deep_learning_The_MIT_Press_2016_pdf", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/38223830/<b>Adaptive_Computation_and_Machine_Learning_series</b>...", "snippet": "<b>Adaptive Computation and Machine Learning series- Deep learning</b>-The MIT Press (2016).pdf", "dateLastCrawled": "2022-02-02T06:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Machine Learning: A Probabilistic Perspective</b> - PDF Free Download - Donuts", "url": "https://epdf.pub/machine-learning-a-probabilistic-perspective.html", "isFamilyFriendly": true, "displayUrl": "https://epdf.pub/<b>machine-learning-a-probabilistic-perspective</b>.html", "snippet": "This <b>can</b> <b>be thought</b> of as an unsupervised version of (multi-output) linear regression, where we observe the high-dimensional response y, but not the low-dimensional \u201ccause\u201d z. Thus the model has the form z \u2192 y; we have to \u201cinvert the arrow\u201d, and infer the latent low-dimensional z from the observed high-dimensional y. See Section 12.1 for details. Dimensionality reduction, and PCA in particular, has been applied in many different areas. Some examples include the following: \u2022 In ...", "dateLastCrawled": "2021-12-03T05:11:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Can</b> hinge loss be optimized with <b>stochastic</b> <b>gradient</b> <b>descent</b>? - Quora", "url": "https://www.quora.com/Can-hinge-loss-be-optimized-with-stochastic-gradient-descent", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>Can</b>-hinge-loss-be-optimized-with-<b>stochastic</b>-<b>gradient</b>-<b>descent</b>", "snippet": "Answer (1 of 2): Yes. I&#39;ve coauthored a paper where we look at this, and specifically at the influence of minibatch size on the convergence rate: http://proceedings ...", "dateLastCrawled": "2022-01-22T10:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Popular Optimization Algorithms In Deep Learning", "url": "https://dataaspirant.com/optimization-algorithms-deep-learning/", "isFamilyFriendly": true, "displayUrl": "https://dataaspirant.com/optimization-algorithms-deep-learning", "snippet": "<b>Stochastic</b> <b>gradient</b> <b>descent</b> \u2014 many parameters . In deep neural networks, every weight is a parameter. As deep learning models are higher dimensional, there could be millions or even more parameters. Still, <b>Stochastic</b> <b>gradient</b> <b>descent</b> works the same just you need to compute the partial derivatives of the given function. You think of <b>Stochastic</b> <b>gradient</b> <b>descent</b> as a Travelling Salesman Problem (TSP), local search, or hill <b>climbing</b>. An example of walking down the <b>mountain</b> step by step to ...", "dateLastCrawled": "2022-02-02T16:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "What is <b>an intuitive explanation of stochastic gradient descent</b>? - Quora", "url": "https://www.quora.com/What-is-an-intuitive-explanation-of-stochastic-gradient-descent", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>an-intuitive-explanation-of-stochastic-gradient-descent</b>", "snippet": "Answer (1 of 10): Let us say you have gone on a road trip. You get lost just 25 KM (KiloMetres) before the destination. There are people walking all along the highway..you <b>can</b> ask them for directions. Let us say you encounter around 10 people for each KM (one every 100 meters). Most of them know...", "dateLastCrawled": "2022-01-26T17:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>SGD</b> - programmersought.com", "url": "https://www.programmersought.com/article/61415716542/", "isFamilyFriendly": true, "displayUrl": "https://www.programmersought.com/article/61415716542", "snippet": "Because of my low level of knowledge, the following explanations may be more detailed or simple. If there are errors, please be sure to correct them. \u2014\u201420170729 Part of the co", "dateLastCrawled": "2021-07-12T17:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Gradient</b> Energy Penalty Term", "url": "https://groups.google.com/g/kxhwph/c/FcM1o-2gAhA", "isFamilyFriendly": true, "displayUrl": "https://groups.google.com/g/kxhwph/c/FcM1o-2gAhA", "snippet": "All groups and messages ... ...", "dateLastCrawled": "2022-01-16T21:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Hands-On Transfer Learning with Python: Implement advanced deep ...", "url": "https://dokumen.pub/hands-on-transfer-learning-with-python-implement-advanced-deep-learning-and-neural-network-models-using-tensorflow-and-keras-1788831306-9781788831307.html", "isFamilyFriendly": true, "displayUrl": "https://<b>dokumen.pub</b>/hands-on-transfer-learning-with-python-implement-advanced-deep...", "snippet": "Advances in optimization algorithms that are used to train neural networks: Traditionally, there was only one algorithm used to learn the weights in a neural network, <b>gradient</b> <b>descent</b> or <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> (<b>SGD</b>). <b>SGD</b> has a few limitations, such as getting stuck in a local minima and slow convergence, that are overcome by the newer algorithms. We will discuss these algorithms in detail in the later sections on Neural network basics.", "dateLastCrawled": "2021-12-15T01:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Deep <b>learning: adaptive computation and machine learning 0262035618</b> ...", "url": "https://dokumen.pub/deep-learning-adaptive-computation-and-machine-learning-0262035618-9780262035613.html", "isFamilyFriendly": true, "displayUrl": "https://dokumen.pub/deep-<b>learning-adaptive-computation-and-machine-learning-0262035618</b>...", "snippet": "The training algorithm used to adapt the weights of the ADALINE was a special case of an algorithm called <b>stochastic</b> <b>gradient</b> <b>descent</b>. Slightly modified versions of the <b>stochastic</b> <b>gradient</b> <b>descent</b> algorithm remain the dominant training algorithms for deep learning models today. Models based on the f (x, w) used by the perceptron and ADALINE are called linear models. These models remain some of the most widely used machine learning models, though in many cases they are trained in different ...", "dateLastCrawled": "2022-01-24T19:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Python Machine Learning Machine Learning and Deep Learning With Python ...", "url": "https://www.scribd.com/document/526461154/Python-Machine-Learning-Machine-Learning-and-Deep-Learning-With-Python-Scikit-learn-And-TensorFlow-by-Sebastian-Raschka-Vahid-Mirjalili-Z-lib-org", "isFamilyFriendly": true, "displayUrl": "https://www.scribd.com/document/526461154/Python-Machine-Learning-Machine-Learning-and...", "snippet": "As illustrated in the following figure, we <b>can</b> describe the main idea behind <b>gradient</b> <b>descent</b> as <b>climbing</b> down a hill until a local or global cost minimum is reached. In each iteration, we take a step in the opposite direction of the <b>gradient</b> where the step size is determined by the value of the learning rate, as well as the slope of the <b>gradient</b>: Using <b>gradient</b> <b>descent</b>, we <b>can</b> now update the weights by taking a step in the opposite direction of the <b>gradient</b> \u2207J ( w ) of our cost function J ...", "dateLastCrawled": "2022-01-14T02:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 8, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Gradient descent</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Gradient_descent", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Gradient_descent</b>", "snippet": "<b>Gradient descent</b> is based on the observation that if the multi-variable function is defined and differentiable in a neighborhood of a point , then () decreases fastest if one goes from in the direction of the negative <b>gradient</b> of at , ().It follows that, if + = for a small enough step size or learning rate +, then (+).In other words, the term () is subtracted from because we want to move against the <b>gradient</b>, toward the local minimum. With this observation in mind, one starts with a guess ...", "dateLastCrawled": "2022-02-03T03:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(PDF) <b>Adaptive Computation and Machine Learning series- Deep learning</b> ...", "url": "https://www.academia.edu/38223830/Adaptive_Computation_and_Machine_Learning_series_Deep_learning_The_MIT_Press_2016_pdf", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/38223830/<b>Adaptive_Computation_and_Machine_Learning_series</b>...", "snippet": "<b>Adaptive Computation and Machine Learning series- Deep learning</b>-The MIT Press (2016).pdf", "dateLastCrawled": "2022-02-02T06:14:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Stochastic Gradient Descent</b> \u2014 Clearly Explained !! | by Aishwarya V ...", "url": "https://towardsdatascience.com/stochastic-gradient-descent-clearly-explained-53d239905d31", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>stochastic-gradient-descent</b>-clearly-explained-53d239905d31", "snippet": "<b>Stochastic gradient descent</b> is a very popular and common algorithm used in various <b>Machine</b> <b>Learning</b> algorithms, most importantly forms the basis of Neural Networks. In this article, I have tried my\u2026 Get started. Open in app. Sign in. Get started. Follow. 618K Followers \u00b7 Editors&#39; Picks Features Deep Dives Grow Contribute. About. Get started. Open in app. <b>Stochastic Gradient Descent</b> \u2014 Clearly Explained !! Aishwarya V Srinivasan. Sep 7, 2019 \u00b7 4 min read. <b>Stochastic gradient descent</b> is a ...", "dateLastCrawled": "2022-02-02T12:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Stochastic gradient descent</b> - The <b>Learning</b> <b>Machine</b>", "url": "https://the-learning-machine.com/article/optimization/stochastic-gradient-descent", "isFamilyFriendly": true, "displayUrl": "https://the-<b>learning</b>-<b>machine</b>.com/article/optimization/<b>stochastic-gradient-descent</b>", "snippet": "<b>Stochastic gradient descent</b> (<b>SGD</b>) is an approach for unconstrained optimization.<b>SGD</b> is the workhorse of optimization for <b>machine</b> <b>learning</b> approaches. It is used as a faster alternative for training support vector machines and is the preferred optimization routine for deep <b>learning</b> approaches.. In this article, we will motivate the formulation for <b>stochastic gradient descent</b> and provide interactive demos over multiple univariate and multivariate functions to show it in action.", "dateLastCrawled": "2022-01-26T05:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Stochastic</b> <b>Gradient</b> <b>Descent</b> in Theory and Practice", "url": "https://ai.stanford.edu/~optas/data/stanford_qual_exams.pdf", "isFamilyFriendly": true, "displayUrl": "https://ai.stanford.edu/~optas/data/stanford_qual_exams.pdf", "snippet": "<b>Stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>) is the most widely used optimization method in the <b>machine</b> <b>learning</b> community. Researchers in both academia and industry have put considerable e ort to optimize <b>SGD</b>\u2019s runtime performance and to develop a theoretical framework for its empirical success. For example, recent advancements in deep neural networks have been largely achieved because, surprisingly, <b>SGD</b> has been found adequate to train them. Here we present three works highlighting desirable ...", "dateLastCrawled": "2022-01-30T21:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Gradient</b> <b>Descent</b> for <b>Machine</b> <b>Learning</b> <b>Gradient</b> <b>Descent</b> for <b>Machine</b> <b>Learning</b>", "url": "http://www.bel.utcluj.ro/dce/didactic/eai/04_GradientDescent_ML.pdf", "isFamilyFriendly": true, "displayUrl": "www.bel.utcluj.ro/dce/didactic/eai/04_<b>GradientDescent</b>_ML.pdf", "snippet": "<b>Gradient</b> <b>Descent</b> for <b>Machine</b> <b>Learning</b> Elements of Artificial Intelligence G. Oltean BGD vs. <b>SGD</b> The summation part is important, especially with the concept of batch <b>gradient</b> <b>descent</b> (BGD) vs. <b>stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>). In Batch <b>Gradient</b> <b>Descent</b>, all the training data is taken into consideration to take a single step (one training epoch ...", "dateLastCrawled": "2022-02-03T10:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Adam, <b>Momentum and Stochastic Gradient Descent</b> - <b>Machine</b> <b>Learning</b> From ...", "url": "https://mlfromscratch.com/optimizers-explained/", "isFamilyFriendly": true, "displayUrl": "https://mlfromscratch.com/optimizers-explained", "snippet": "The basic difference between batch <b>gradient</b> <b>descent</b> (BGD) and <b>stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>), is that we only calculate the cost of one example for each step in <b>SGD</b>, but in BGD, we have to calculate the cost for all training examples in the dataset. Trivially, this speeds up neural networks greatly. Exactly this is the motivation behind <b>SGD</b>. The equation for <b>SGD</b> is used to update parameters in a neural network \u2013 we use the equation to update parameters in a backwards pass, using ...", "dateLastCrawled": "2022-02-02T18:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Scikit Learn - Stochastic Gradient Descent</b>", "url": "https://www.tutorialspoint.com/scikit_learn/scikit_learn_stochastic_gradient_descent.htm", "isFamilyFriendly": true, "displayUrl": "https://<b>www.tutorialspoint.com</b>/scikit_learn/<b>scikit_learn_stochastic_gradient_descent</b>.htm", "snippet": "<b>Stochastic</b> <b>Gradient</b> <b>Descent</b> (<b>SGD</b>) is a simple yet efficient optimization algorithm used to find the values of parameters/coefficients of functions that minimize a cost function. In other words, it is used for discriminative <b>learning</b> of linear classifiers under convex loss functions such as SVM and Logistic regression. It has been successfully applied to large-scale datasets because the update to the coefficients is performed for each training instance, rather than at the end of instances.", "dateLastCrawled": "2022-02-03T02:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Gradient</b> <b>Descent</b>: One of <b>Machine</b> <b>Learning</b>\u2019s Most Popular Algorithms ...", "url": "https://urmiparekh.medium.com/gradient-descent-one-of-machine-learnings-most-popular-algorithms-c31963d1e67f", "isFamilyFriendly": true, "displayUrl": "https://urmiparekh.medium.com/<b>gradient</b>-<b>descent</b>-one-of-<b>machine</b>-<b>learning</b>s-most-popular...", "snippet": "<b>Gradient</b> <b>descent</b> is an optimization algorithm which is used to train a <b>machine</b> <b>learning</b> model. It is an optimization algorithm to find a local minimum of a differential function. It is used to find the values of a function\u2019s coefficients that minimize a cost function as much as possible. Source: Here. It i s a first-order iterative ...", "dateLastCrawled": "2022-01-17T15:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Batch, Mini Batch &amp; <b>Stochastic</b> <b>Gradient Descent</b> | by Sushant Patrikar ...", "url": "https://towardsdatascience.com/batch-mini-batch-stochastic-gradient-descent-7a62ecba642a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/batch-mini-batch-<b>stochastic</b>-<b>gradient-descent</b>-7a62ecba642a", "snippet": "In Batch <b>Gradient Descent</b> we were considering all the examples for every step of <b>Gradient Descent</b>. But what if our dataset is very huge. Deep <b>learning</b> models crave for data. The more the data the more chances of a model to be good. Suppose our dataset has 5 million examples, then just to take one step the model will have to calculate the gradients of all the 5 million examples. This does not seem an efficient way. To tackle this problem we have <b>Stochastic</b> <b>Gradient Descent</b>. In <b>Stochastic</b> ...", "dateLastCrawled": "2022-02-02T11:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Gradient Descent With Momentum from Scratch</b> - <b>Machine</b> <b>Learning</b> Mastery", "url": "https://machinelearningmastery.com/gradient-descent-with-momentum-from-scratch/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>gradient-descent-with-momentum-from-scratch</b>", "snippet": "<b>Gradient</b> <b>descent</b> is an optimization algorithm that follows the negative <b>gradient</b> of an objective function in order to locate the minimum of the function. A problem with <b>gradient</b> <b>descent</b> is that it can bounce around the search space on optimization problems that have large amounts of curvature or noisy gradients, and it can get stuck in flat spots in the search", "dateLastCrawled": "2022-01-26T05:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Gradient Descent in Logistic Regression [Explained for Beginners</b> ...", "url": "https://www.upgrad.com/blog/gradient-descent-in-logistic-regression/", "isFamilyFriendly": true, "displayUrl": "https://www.upgrad.com/blog/<b>gradient-descent-in-logistic-regression</b>", "snippet": "It\u2019s massive, and hence there was a need for a slightly modified <b>Gradient</b> <b>Descent</b> Algorithm, namely \u2013 <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> Algorithm (<b>SGD</b>). The only difference <b>SGD</b> has with Normal <b>Gradient</b> <b>Descent</b> is that, in <b>SGD</b>, we don\u2019t deal with the entire training instance at a single time. In <b>SGD</b>, we compute the <b>gradient</b> of the cost function for just a single random example at each iteration. Now, doing so brings down the time taken for computations by a huge margin especially for large ...", "dateLastCrawled": "2022-01-28T16:49:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Gossip <b>Learning</b> as a Decentralized Alternative to Federated <b>Learning</b>", "url": "http://publicatio.bibl.u-szeged.hu/15824/1/dais19a.pdf", "isFamilyFriendly": true, "displayUrl": "publicatio.bibl.u-szeged.hu/15824/1/dais19a.pdf", "snippet": "Federated <b>learning</b> is adistributed <b>machine</b> <b>learning</b> approach for computing models over data collected by edge devices. Most impor-tantly, the data itself is not collected centrally, but a master-worker ar-chitecture is applied where a master node performs aggregation and the edge devices are the workers, not unlike the parameter server approach. Gossip <b>learning</b> also assumes that the data remains at the edge devices, but it requires no aggregation server or any central component. In this ...", "dateLastCrawled": "2022-01-27T14:06:00.0000000Z", "language": "en", "isNavigational": false}], [], [], []], "all_bing_queries": ["+(stochastic gradient descent (sgd))  is like +(climbing a steep mountain)", "+(stochastic gradient descent (sgd)) is similar to +(climbing a steep mountain)", "+(stochastic gradient descent (sgd)) can be thought of as +(climbing a steep mountain)", "+(stochastic gradient descent (sgd)) can be compared to +(climbing a steep mountain)", "machine learning +(stochastic gradient descent (sgd) AND analogy)", "machine learning +(\"stochastic gradient descent (sgd) is like\")", "machine learning +(\"stochastic gradient descent (sgd) is similar\")", "machine learning +(\"just as stochastic gradient descent (sgd)\")", "machine learning +(\"stochastic gradient descent (sgd) can be thought of as\")", "machine learning +(\"stochastic gradient descent (sgd) can be compared to\")"]}