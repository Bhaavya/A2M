{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>True Positive Rate</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/computer-science/true-positive-rate", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/computer-science/<b>true-positive-rate</b>", "snippet": "It uses the Cartesian coordinate system with the y-axis representing the <b>TPR</b> (<b>true positive rate</b>) and the x-axis representing the RPR (false <b>positive</b> <b>rate</b>). The two axes make the ROC\u2019s plane as the single point on the plane that can visualize the fundamental tradeoff between the <b>true</b> <b>positive</b> and false <b>positive</b>, representing the discrete classifier performance. Similarly, a single point of operation in a scoring classifier set is also established as a point on the ROC plane. The <b>TPR</b> of 1 ...", "dateLastCrawled": "2022-02-02T19:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "What is <b>Sensitivity, Specificity, False positive, False negative</b>?", "url": "https://microbenotes.com/sensitivity-specificity-false-positive-false-negative/", "isFamilyFriendly": true, "displayUrl": "https://microbenotes.com/<b>sensitivity-specificity-false-positive-false-negative</b>", "snippet": "It is also known as the <b>True</b> <b>Positive</b> <b>Rate</b> (<b>TPR</b>), i.e. the <b>percentage</b> of sick persons who are correctly identified as having the condition. Therefore sensitivity is the extent to which actual positives are not overlooked. For example, a test that correctly identifies all <b>positive</b> samples in a panel is a very sensitive test while a test that only detects 80 % of the <b>true</b> <b>positive</b> samples and 20% of the samples are undetected, hence false negatives in the panel. This test will be termed to ...", "dateLastCrawled": "2022-02-02T20:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Understanding <b>Classification</b> Thresholds Using Isocurves", "url": "https://druce.ai/2019/10/understanding-classification-thresholds-using-isocurves", "isFamilyFriendly": true, "displayUrl": "https://druce.ai/2019/10/understanding-<b>classification</b>-thresholds-using-isocurves", "snippet": "The <b>true</b>-<b>positive</b> <b>rate</b> (<b>TPR</b>) is the number of <b>true</b> positives / ground truth positives (also called recall or sensitivity). Ground truth positives = <b>true</b> positives + false negatives: \\[<b>TPR</b> = \\frac{tp}{tp+fn}\\] A false <b>positive</b> is a false observation incorrectly predicted to be <b>true</b>. The false-<b>positive</b> <b>rate</b> (FPR) is the number of false positives / ground truth negatives (1 \u2014 FPR is the specificity). Ground truth negatives = <b>true</b> negatives + false positives: \\[FPR = \\frac{fp}{tn + fp}\\] The ...", "dateLastCrawled": "2022-01-19T15:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Model Evaluation Metrics in Machine Learning - KDnuggets", "url": "https://www.kdnuggets.com/2020/05/model-evaluation-metrics-machine-learning.html", "isFamilyFriendly": true, "displayUrl": "https://www.kdnuggets.com/2020/05/model-evaluation-metrics-machine-learning.html", "snippet": "<b>True</b> <b>Positive</b> <b>Rate</b> = TP/actual yes Recall gives us the <b>true</b> <b>positive</b> <b>rate</b> (<b>TPR</b>), which is the ratio of <b>true</b> positives to everything <b>positive</b>. In the case of the 99/1 split between classes A and B, the model that classifies everything as A would have a recall of 0% for the <b>positive</b> class, B (precision would be undefined \u2014 0/0). Precision and ...", "dateLastCrawled": "2022-01-29T20:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Confusion Matrix</b> for Your Multi-Class Machine Learning Model | by ...", "url": "https://towardsdatascience.com/confusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>confusion-matrix</b>-for-your-multi-class-machine-learning...", "snippet": "Recall: It tells you what fraction of all <b>positive</b> samples were correctly predicted as <b>positive</b> by the classifier. It is also known as <b>True</b> <b>Positive</b> <b>Rate</b> (<b>TPR</b>), Sensitivity, Probability of Detection. To calculate Recall, use the following formula: TP/(TP+FN).", "dateLastCrawled": "2022-01-30T03:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "python - Scikit-learn: How to obtain <b>True Positive</b>, <b>True</b> Negative ...", "url": "https://stackoverflow.com/questions/31324218/scikit-learn-how-to-obtain-true-positive-true-negative-false-positive-and-fal", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/31324218", "snippet": "I make the <b>predictions</b> and obtain the accuracy &amp; confusion matrix of that fold. After this, I would <b>like</b> to obtain the <b>True Positive</b>(TP), <b>True</b> Negative(TN), False <b>Positive</b>(FP) and False Negative(FN) values. I&#39;ll use these parameters to obtain the Sensitivity and Specificity. Finally, I would use this to put in HTML in order to show a chart with the TPs of each label. Code: The variables I have for the moment: trainList #It is a list with all the data of my dataset in JSON form labelList #It ...", "dateLastCrawled": "2022-01-26T12:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Accuracy, Precision, Recall &amp; F1-Score - Python</b> Examples - <b>Data Analytics</b>", "url": "https://vitalflux.com/accuracy-precision-recall-f1-score-python-example/", "isFamilyFriendly": true, "displayUrl": "https://vitalflux.com/<b>accuracy-precision-recall-f1-score-python</b>-example", "snippet": "The following confusion matrix is printed:. Fig 1. Confusion Matrix representing <b>predictions</b> vs Actuals on Test Data. The predicted data results in the above diagram could be read in the following manner given 1 represents malignant cancer (<b>positive</b>).. <b>True</b> <b>Positive</b> (TP): <b>True</b> <b>positive</b> represents the value <b>of correct</b> <b>predictions</b> of positives out of actual <b>positive</b> cases.Out of 107 actual <b>positive</b>, 104 is correctly predicted <b>positive</b>.", "dateLastCrawled": "2022-02-02T22:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Confusion Matrix</b>, Accuracy, Precision, Recall, F1 Score | by ...", "url": "https://medium.com/analytics-vidhya/confusion-matrix-accuracy-precision-recall-f1-score-ade299cf63cd", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/<b>confusion-matrix</b>-accuracy-precision-recall-f1...", "snippet": "Accuracy represents the number of correctly classified data instances over the total number of data instances. In this example, Accuracy = (55 + 30)/(55 + 5 + 30 + 10 ) = 0.85 and in <b>percentage</b> ...", "dateLastCrawled": "2022-02-02T15:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "logistic regression - Roc curve and cut off point. <b>Python</b> - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/28719067/roc-curve-and-cut-off-point-python", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/28719067", "snippet": "The optimal cut off point is 0.317628, so anything above this can be labeled as 1 else 0. You can see from the output/chart that where <b>TPR</b> is crossing 1-FPR the <b>TPR</b> is 63%, FPR is 36% and <b>TPR</b>- (1-FPR) is nearest to zero in the current example.", "dateLastCrawled": "2022-01-28T10:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Glossary \u2014 ML Glossary documentation", "url": "https://ml-cheatsheet.readthedocs.io/en/latest/glossary.html", "isFamilyFriendly": true, "displayUrl": "https://ml-cheatsheet.readthedocs.io/en/latest/glossary.html", "snippet": "<b>Percentage</b> <b>of correct</b> <b>predictions</b> <b>made</b> by the model. <b>Algorithm</b> A method, function, or series of instructions used to generate a <b>machine learning</b> model. Examples include linear regression, decision trees, support vector machines, and neural networks. Attribute A quality describing an observation (e.g. color, size, weight). In Excel terms, these are column headers. Bias metric. What is the average difference between your <b>predictions</b> and the <b>correct</b> value for that observation? Low bias could ...", "dateLastCrawled": "2022-01-28T21:08:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>True Positive Rate</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/computer-science/true-positive-rate", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/computer-science/<b>true-positive-rate</b>", "snippet": "The hit <b>rate</b> (<b>true positive rate</b>, <b>TPR</b> i) is defined as rater i&#39;s <b>positive</b> response when the <b>correct</b> answer is <b>positive</b> (X ik = 1 and Z k = 1), and the false alarm <b>rate</b> (false <b>positive</b> <b>rate</b>, FPR i) is defined as a <b>positive</b> response when the <b>correct</b> answer is negative (X ik = 1 and Z k = 0). Competence can be calculated from the hit and false alarm rates or from the <b>true</b> <b>positive</b> and <b>true</b> negative rates: D i = <b>TPR</b> i \u2212 FPR i = <b>TPR</b> i + TNR i \u2212 1. Bias can be calculated from <b>true</b> <b>positive</b> and ...", "dateLastCrawled": "2022-02-02T19:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Confusion Matrix</b> for Your Multi-Class Machine Learning Model | by ...", "url": "https://towardsdatascience.com/confusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>confusion-matrix</b>-for-your-multi-class-machine-learning...", "snippet": "Recall: It tells you what fraction of all <b>positive</b> samples were correctly predicted as <b>positive</b> by the classifier. It is also known as <b>True</b> <b>Positive</b> <b>Rate</b> (<b>TPR</b>), Sensitivity, Probability of Detection. To calculate Recall, use the following formula: TP/(TP+FN).", "dateLastCrawled": "2022-01-30T03:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Must-know Machine Learning Questions \u2013 <b>Logistic Regression</b>", "url": "https://www.upgrad.com/blog/machine-learning-interview-questions-answers-logistic-regression/", "isFamilyFriendly": true, "displayUrl": "https://www.upgrad.com/blog/<b>machine-learning-interview-questions-answers</b>-logistic...", "snippet": "A CRV consists of the <b>true</b> <b>positive</b> <b>rate</b> or the <b>percentage</b> of positives correctly classified on the Y-axis and the <b>percentage</b> of the population targeted on the X-axis. It is important to note that the <b>percentage</b> of the population will be ranked by the model in descending order (either the probabilities or the expected values). If the model is good, then by targeting a top portion of the ranked list, all high percentages of positives will be captured. As with the ROC curve, there will be a ...", "dateLastCrawled": "2022-02-03T08:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Top <b>10 Evaluation Metrics for Classification</b> Models", "url": "https://www.explorium.ai/blog/top-10-evaluation-metrics-for-classification-models/", "isFamilyFriendly": true, "displayUrl": "https://www.explorium.ai/blog/top-<b>10-evaluation-metrics-for-classification</b>-models", "snippet": "This metric basically shows the number <b>of correct</b> <b>positive</b> class <b>predictions</b> <b>made</b> as a proportion of all of the <b>predictions</b> <b>made</b>. Detection <b>Rate</b> = TP / TP + FP + FN + TN. 4. Logarithmic loss. Also known as log loss, logarithmic loss basically functions by penalizing all false/incorrect classifications. The classifier must assign a specific probability to each class for all samples while working with this metric. The formula for calculating log loss is as follows: Yij - Indicates if sample i ...", "dateLastCrawled": "2022-01-25T01:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "How to evaluate my <b>Classification</b> Model results | by Songhao Wu ...", "url": "https://towardsdatascience.com/top-5-metrics-for-evaluating-classification-model-83ede24c7584", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/top-5-metrics-for-evaluating-<b>classification</b>-model-83ede...", "snippet": "Each point on the curve represents a different cut-off point between <b>TPR</b> and FPR. For example, if I cannot tolerate any false <b>positive</b> <b>rate</b> (&lt;0.01), then the <b>true</b> <b>positive</b> <b>rate</b> I can reach is around 0.6(see the green dot on the graph). If I loosen my criteria a bit and I only need to control FPR below 0.1, then my <b>TPR</b> can reach 0.9(see the red ...", "dateLastCrawled": "2022-02-02T13:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Confusion Matrix</b>, Accuracy, Precision, Recall, F1 Score | by ...", "url": "https://medium.com/analytics-vidhya/confusion-matrix-accuracy-precision-recall-f1-score-ade299cf63cd", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/<b>confusion-matrix</b>-accuracy-precision-recall-f1...", "snippet": "Figure 4. False Negative. What we desire is <b>TRUE</b> <b>POSITIVE</b> and <b>TRUE</b> NEGATIVE but due to the misclassifications, we may also end up in FALSE <b>POSITIVE</b> and FALSE NEGATIVE.So there is a confusion in ...", "dateLastCrawled": "2022-02-02T15:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "python - Scikit-learn: How to obtain <b>True Positive</b>, <b>True</b> Negative ...", "url": "https://stackoverflow.com/questions/31324218/scikit-learn-how-to-obtain-true-positive-true-negative-false-positive-and-fal", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/31324218", "snippet": "They are not <b>correct</b>, because in the first answer, False <b>Positive</b> should be where actual is 0, but the predicted is 1, not the opposite. It is also same for False Negative. And, if we use the second answer, the results are computed as follows: FP: 3 FN: 1 TP: 4 TN: 3. <b>True Positive</b> and <b>True</b> Negative numbers are not <b>correct</b>, they should be opposite.", "dateLastCrawled": "2022-01-26T12:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "How to Calculate Precision, Recall, and F-Measure for Imbalanced ...", "url": "https://machinelearningmastery.com/precision-recall-and-f-measure-for-imbalanced-classification/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/precision-recall-and-f-measure-for-", "snippet": "Precision is a metric that quantifies the number <b>of correct</b> <b>positive</b> <b>predictions</b> <b>made</b>. Precision, therefore, calculates the accuracy for the minority class. It is calculated as the ratio of correctly predicted <b>positive</b> examples divided by the total number of <b>positive</b> examples that were predicted. Precision evaluates the fraction <b>of correct</b> classified instances among the ones classified as <b>positive</b> \u2026 \u2014 Page 52, Learning from Imbalanced Data Sets, 2018. Precision for Binary Classification ...", "dateLastCrawled": "2022-02-03T02:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "logistic regression - Roc curve and cut off point. <b>Python</b> - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/28719067/roc-curve-and-cut-off-point-python", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/28719067", "snippet": "The optimal cut off point is 0.317628, so anything above this can be labeled as 1 else 0. You can see from the output/chart that where <b>TPR</b> is crossing 1-FPR the <b>TPR</b> is 63%, FPR is 36% and <b>TPR</b>- (1-FPR) is nearest to zero in the current example.", "dateLastCrawled": "2022-01-28T10:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Evaluating a <b>Classification Model</b> | Machine Learning, Deep Learning ...", "url": "https://www.ritchieng.com/machine-learning-evaluate-classification-model/", "isFamilyFriendly": true, "displayUrl": "https://www.ritchieng.com/machine-learning-evaluate-<b>classification-model</b>", "snippet": "Classification accuracy: <b>percentage</b> <b>of correct</b> <b>predictions</b>. In [7]: # calculate accuracy from sklearn import metrics print (metrics. accuracy_score (y_test, y_pred_class)) 0.692708333333 Classification accuracy is 69%. Null accuracy: accuracy that could be achieved by always predicting the most frequent class. We must always compare with this; In [8]: # examine the class distribution of the testing set (using a Pandas Series method) y_test. value_counts Out[8]: 0 130 1 62 Name: label, dtype ...", "dateLastCrawled": "2022-02-03T00:57:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "What is <b>Sensitivity, Specificity, False positive, False negative</b>?", "url": "https://microbenotes.com/sensitivity-specificity-false-positive-false-negative/", "isFamilyFriendly": true, "displayUrl": "https://microbenotes.com/<b>sensitivity-specificity-false-positive-false-negative</b>", "snippet": "It is also known as the <b>True</b> <b>Positive</b> <b>Rate</b> (<b>TPR</b>), i.e. the <b>percentage</b> of sick persons who are correctly identified as having the condition. Therefore sensitivity is the extent to which actual positives are not overlooked. For example, a test that correctly identifies all <b>positive</b> samples in a panel is a very sensitive test while a test that only detects 80 % of the <b>true</b> <b>positive</b> samples and 20% of the samples are undetected, hence false negatives in the panel. This test will be termed to ...", "dateLastCrawled": "2022-02-02T20:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Entry 23: Scoring Classification Models - Theory</b> - Data Science Diaries", "url": "https://julielinx.github.io/blog/23_class_score_theory/", "isFamilyFriendly": true, "displayUrl": "https://julielinx.github.io/blog/23_class_score_theory", "snippet": "This <b>can</b> <b>be thought</b> of as: The prediction was <b>correct</b> = <b>True</b> <b>Positive</b> and <b>True</b> Negative (main diagonal) The prediction was incorrect = False <b>Positive</b> and False Negative (off-diagonal) Incorrect <b>predictions</b> each have their own label. Type I error: False <b>positive</b>; Type II error: False negative; Being able to differentiate between these types of ...", "dateLastCrawled": "2022-01-29T03:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>GitHub</b> - <b>iamtodor/data-science-interview-questions-and-answers</b>: Data ...", "url": "https://github.com/iamtodor/data-science-interview-questions-and-answers", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/<b>iamtodor/data-science-interview-questions-and-answers</b>", "snippet": "What percent of <b>positive</b> <b>predictions</b> were <b>correct</b>? You answer: ... It is also called Sensitivity or the <b>True</b> <b>Positive</b> <b>Rate</b>. Recall <b>can</b> <b>be thought</b> of as a measure of a classifiers completeness. A low recall indicates many False Negatives. F1 Score (or F-score): A weighted average of precision and recall. I would also advise you to take a look at the following: Kappa (or Cohen\u2019s kappa): Classification accuracy normalized by the imbalance of the classes in the data. ROC Curves: Like precision ...", "dateLastCrawled": "2022-02-03T02:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Gentle Introduction to Bayes Theorem for <b>Machine Learning</b>", "url": "https://machinelearningmastery.com/bayes-theorem-for-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/bayes-theorem-for-<b>machine-learning</b>", "snippet": "<b>True</b> <b>Positive</b> <b>Rate</b> (<b>TPR</b>) = TP / (TP + FN) False <b>Positive</b> <b>Rate</b> (FPR) = FP / (FP + TN) <b>True</b> Negative <b>Rate</b> (TNR) = TN / (TN + FP) False Negative <b>Rate</b> (FNR) = FN / (FN + TP) These terms are called rates, but they <b>can</b> also be interpreted as probabilities. Also, it might help to notice: <b>TPR</b> + FNR = 1.0, or: FNR = 1.0 \u2013 <b>TPR</b>; <b>TPR</b> = 1.0 \u2013 FNR; TNR + FPR = 1.0, or: TNR = 1.0 \u2013 FPR; FPR = 1.0 \u2013 TNR; Recall that in a previous section that we calculated the false <b>positive</b> <b>rate</b> given the ...", "dateLastCrawled": "2022-01-28T19:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Bootstrap estimated true and false positive</b> rates and ROC curve ...", "url": "https://www.researchgate.net/publication/23629765_Bootstrap_estimated_true_and_false_positive_rates_and_ROC_curve", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/23629765_<b>Bootstrap_estimated_true_and_false</b>...", "snippet": "Recall or sensitivity (<b>true</b> <b>positive</b> <b>rate</b>, <b>TPR</b>) and missing <b>rate</b> (false negative <b>rate</b>, FNR) are the rates at which shadow areas are correctly and incorrectly segmented, respectively, while the ...", "dateLastCrawled": "2021-10-23T04:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Top 70+ <b>Data Science Interview Questions and Answers</b> for 2022", "url": "https://intellipaat.com/blog/interview-question/data-science-interview-questions/", "isFamilyFriendly": true, "displayUrl": "https://intellipaat.com/blog/interview-question/data-science-interview-", "snippet": "<b>True</b> <b>positive</b> <b>rate</b>: In Machine Learning, <b>true</b>-<b>positive</b> rates, which are also referred to as sensitivity or recall, are used to measure the <b>percentage</b> of actual positives which are correctly identified. Formula: <b>True</b> <b>Positive</b> <b>Rate</b> = <b>True</b> Positives/Positives False <b>positive</b> <b>rate</b>: False <b>positive</b> <b>rate</b> is basically the probability of falsely rejecting the null hypothesis for a particular test. The false-<b>positive</b> <b>rate</b> is calculated as the ratio between the number of negative events wrongly ...", "dateLastCrawled": "2022-02-03T02:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Receiver Operating Characteristic (ROC) Curves</b>", "url": "https://www.researchgate.net/publication/229703193_Receiver_Operating_Characteristic_ROC_Curves", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/229703193_Receiver_Operating_Characteristic...", "snippet": "[4] [5][6] It indicates the relationship between the <b>true</b> <b>positive</b> <b>rate</b> (<b>TPR</b>) and the false <b>positive</b> <b>rate</b> (FPR) of the test, as the threshold used to distinguish disease cases from noncases varies ...", "dateLastCrawled": "2022-01-03T13:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "python - Scikit-learn: How to obtain <b>True Positive</b>, <b>True</b> Negative ...", "url": "https://stackoverflow.com/questions/31324218/scikit-learn-how-to-obtain-true-positive-true-negative-false-positive-and-fal", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/31324218", "snippet": "They are not <b>correct</b>, because in the first answer, False <b>Positive</b> should be where actual is 0, but the predicted is 1, not the opposite. It is also same for False Negative. And, if we use the second answer, the results are computed as follows: FP: 3 FN: 1 TP: 4 TN: 3. <b>True Positive</b> and <b>True</b> Negative numbers are not <b>correct</b>, they should be opposite.", "dateLastCrawled": "2022-01-26T12:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "logistic regression - Roc curve and cut off point. <b>Python</b> - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/28719067/roc-curve-and-cut-off-point-python", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/28719067", "snippet": "The optimal cut off point is 0.317628, so anything above this <b>can</b> be labeled as 1 else 0. You <b>can</b> see from the output/chart that where <b>TPR</b> is crossing 1-FPR the <b>TPR</b> is 63%, FPR is 36% and <b>TPR</b>- (1-FPR) is nearest to zero in the current example.", "dateLastCrawled": "2022-01-28T10:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "IT Support <b>Ticket</b> Classification and Deployment using Machine Learning ...", "url": "https://towardsdatascience.com/it-support-ticket-classification-and-deployment-using-machine-learning-and-aws-lambda-8ef8b82643b6", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/it-support-<b>ticket</b>-classification-and-deployment-using...", "snippet": "A recurrent neural network <b>can</b> <b>be thought</b> of as multiple copies of the same network, each passing a message to a successor. Consider what happens if we unroll the loop: LSTM Networks. Long Short Term Memory networks \u2014 usually just called \u201cLSTMs\u201d \u2014 are a special kind of RNN, capable of learning long-term dependencies. They were introduced by Hochreiter &amp; Schmidhuber (1997), and were refined and popularized by many people in following work.1 They work tremendously well on a large ...", "dateLastCrawled": "2022-02-03T02:40:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>True Positive Rate</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/computer-science/true-positive-rate", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/computer-science/<b>true-positive-rate</b>", "snippet": "The hit <b>rate</b> (<b>true positive rate</b>, <b>TPR</b> i) is defined as rater i&#39;s <b>positive</b> response when the <b>correct</b> answer is <b>positive</b> (X ik = 1 and Z k = 1), and the false alarm <b>rate</b> (false <b>positive</b> <b>rate</b>, FPR i) is defined as a <b>positive</b> response when the <b>correct</b> answer is negative (X ik = 1 and Z k = 0). Competence <b>can</b> be calculated from the hit and false alarm rates or from the <b>true</b> <b>positive</b> and <b>true</b> negative rates: D i = <b>TPR</b> i \u2212 FPR i = <b>TPR</b> i + TNR i \u2212 1. Bias <b>can</b> be calculated from <b>true</b> <b>positive</b> and ...", "dateLastCrawled": "2022-02-02T19:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Understanding <b>Classification</b> Thresholds Using Isocurves | by Druce ...", "url": "https://towardsdatascience.com/understanding-classification-thresholds-using-isocurves-9e5e7e00e5a2", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-<b>classification</b>-<b>threshold</b>s-using-isocurves...", "snippet": "The <b>true</b>-<b>positive</b> <b>rate</b> (<b>TPR</b>) is the number of <b>true</b> positives / ground truth positives (also called recall or sensitivity). Ground truth positives = <b>true</b> positives + false negatives: <b>TPR</b> = tp / (tp+fn) A false <b>positive</b> is a false observation incorrectly predicted to be <b>true</b>. The false-<b>positive</b> <b>rate</b> (FPR) is the number of false positives / ground truth negatives (1 \u2014 FPR is the specificity). Ground truth negatives = <b>true</b> negatives + false positives: FPR = fp / (tn + fp) The best point to be ...", "dateLastCrawled": "2022-02-02T19:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "What is <b>Sensitivity, Specificity, False positive, False negative</b>?", "url": "https://microbenotes.com/sensitivity-specificity-false-positive-false-negative/", "isFamilyFriendly": true, "displayUrl": "https://microbenotes.com/<b>sensitivity-specificity-false-positive-false-negative</b>", "snippet": "It is also known as the <b>True</b> <b>Positive</b> <b>Rate</b> (<b>TPR</b>), i.e. the <b>percentage</b> of sick persons who are correctly identified as having the condition. Therefore sensitivity is the extent to which actual positives are not overlooked. For example, a test that correctly identifies all <b>positive</b> samples in a panel is a very sensitive test while a test that only detects 80 % of the <b>true</b> <b>positive</b> samples and 20% of the samples are undetected, hence false negatives in the panel. This test will be termed to ...", "dateLastCrawled": "2022-02-02T20:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Classification: <b>ROC</b> Curve and AUC | Machine Learning Crash Course ...", "url": "https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/machine-learning/crash-course/classification/<b>roc</b>-and-auc", "snippet": "<b>Predictions</b> ranked in ascending order of logistic regression score. AUC represents the probability that a random <b>positive</b> (green) example is positioned to the right of a random negative (red) example. AUC ranges in value from 0 to 1. A model whose <b>predictions</b> are 100% wrong has an AUC of 0.0; one whose <b>predictions</b> are 100% <b>correct</b> has an AUC of ...", "dateLastCrawled": "2022-02-02T09:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Model Evaluation Metrics in Machine Learning - KDnuggets", "url": "https://www.kdnuggets.com/2020/05/model-evaluation-metrics-machine-learning.html", "isFamilyFriendly": true, "displayUrl": "https://www.kdnuggets.com/2020/05/model-evaluation-metrics-machine-learning.html", "snippet": "<b>True</b> <b>Positive</b> <b>Rate</b> = TP/actual yes Recall gives us the <b>true</b> <b>positive</b> <b>rate</b> (<b>TPR</b>), which is the ratio of <b>true</b> positives to everything <b>positive</b>. In the case of the 99/1 split between classes A and B, the model that classifies everything as A would have a recall of 0% for the <b>positive</b> class, B (precision would be undefined \u2014 0/0). Precision and ...", "dateLastCrawled": "2022-01-29T20:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Evaluate AutoML experiment results - <b>Azure</b> Machine Learning | Microsoft ...", "url": "https://docs.microsoft.com/en-us/azure/machine-learning/how-to-understand-automated-ml", "isFamilyFriendly": true, "displayUrl": "https://docs.microsoft.com/en-us/<b>azure</b>/machine-learning/how-to-understand-automated-ml", "snippet": "The receiver operating characteristic (ROC) curve plots the relationship between <b>true</b> <b>positive</b> <b>rate</b> (<b>TPR</b>) and false <b>positive</b> <b>rate</b> (FPR) as the decision threshold changes. The ROC curve <b>can</b> be less informative when training models on datasets with high class imbalance, as the majority class <b>can</b> drown out contributions from minority classes. The area under the curve (AUC) <b>can</b> be interpreted as the proportion of correctly classified samples. More precisely, the AUC is the probability that the ...", "dateLastCrawled": "2022-02-02T18:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Classification: <b>True</b> vs. False and <b>Positive</b> vs. Negative | Machine ...", "url": "https://developers.google.com/machine-learning/crash-course/classification/true-false-positive-negative", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/.../crash-course/classification/<b>true</b>-false-<b>positive</b>-negative", "snippet": "A <b>true</b> <b>positive</b> is an outcome where the model correctly predicts the <b>positive</b> class. Similarly, a <b>true</b> negative is an outcome where the model correctly predicts the negative class.. A false <b>positive</b> is an outcome where the model incorrectly predicts the <b>positive</b> class. And a false negative is an outcome where the model incorrectly predicts the negative class.. In the following sections, we&#39;ll look at how to evaluate classification models using metrics derived from these four outcomes.", "dateLastCrawled": "2022-02-02T23:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Performance</b> Metrics for Machine Learning Models | by Sachin D N ...", "url": "https://medium.com/analytics-vidhya/performance-metrics-for-machine-learning-models-80d7666b432e", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/<b>performance</b>-metrics-for-machine-learning-models-80...", "snippet": "Accuracy in terms of the confusion matrix given by in classification problems is the number <b>of correct</b> <b>predictions</b> <b>made</b> by the ... the <b>True</b> <b>Positive</b> <b>Rate</b> and <b>True</b> Negative Should be high, and the ...", "dateLastCrawled": "2022-01-30T13:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "roc - Given <b>true</b> <b>positive</b>, false negative rates, <b>can</b> you calculate ...", "url": "https://stats.stackexchange.com/questions/61829/given-true-positive-false-negative-rates-can-you-calculate-false-positive-tru", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/61829", "snippet": "We cannot however directly derive the false <b>positive</b> <b>rate</b> from either the <b>true</b> <b>positive</b> or false negative rates because they provide no information on the specificity, i.e., how the test behaves when \u201cnot A\u201d is the <b>correct</b> answer. The answer to your question would therefore be \u201cno, it&#39;s not possible\u201d because you have no information on the right column of the confusion matrix.", "dateLastCrawled": "2022-02-02T16:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Confusion Matrix</b> in <b>Machine Learning</b> with EXAMPLE", "url": "https://www.guru99.com/confusion-matrix-machine-learning-example.html", "isFamilyFriendly": true, "displayUrl": "https://www.guru99.com/<b>confusion-matrix</b>-<b>machine-learning</b>-example.html", "snippet": "The total <b>of correct</b> <b>predictions</b> of each class. The total of incorrect <b>predictions</b> of each class. After that, these numbers are organized in the below-given methods: Every row of the matrix links to a predicted class. Every column of the matrix corresponds with an actual class. The total counts <b>of correct</b> and incorrect classification are entered into the table. The sum <b>of correct</b> <b>predictions</b> for a class go into the predicted column and expected row for that class value. The sum of incorrect ...", "dateLastCrawled": "2022-02-02T20:30:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine Learning</b> <b>Evaluation Metrics</b> - GitHub Pages", "url": "https://kevalnagda.github.io/evaluation-metrics", "isFamilyFriendly": true, "displayUrl": "https://kevalnagda.github.io/<b>evaluation-metrics</b>", "snippet": "It essentially shows the <b>True</b> <b>Positive</b> <b>Rate</b> (<b>TPR</b>) against the False <b>Positive</b> <b>Rate</b> (FPR) for various threshold values. AUC The Area Under the Curve (AUC), is an aggregated measure of performance of a binary classifier on all possible threshold values (and therefore it is threshold invariant). AUC calculates the area under the ROC curve, and therefore it is between 0 and 1. One way of interpreting AUC is the probability that the model ranks a random <b>positive</b> example more highly than a random ...", "dateLastCrawled": "2021-10-13T19:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) A Comparison of Various <b>Machine</b> <b>Learning</b> Algorithms in a ...", "url": "https://www.academia.edu/68902781/A_Comparison_of_Various_Machine_Learning_Algorithms_in_a_Distributed_Denial_of_Service_Intrusion", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/68902781/A_Comparison_of_Various_<b>Machine</b>_<b>Learning</b>_Algorithms...", "snippet": "2) <b>True</b> <b>Positive</b> <b>Rate</b> (<b>TPR</b>) 4) Decision Tree (DT) This metric calculates how often the model is able to predict a This algorithm uses a tree structure <b>analogy</b> to represent a <b>positive</b> result correctly. Similar to Accuracy, but difference is series of rules that lead to a class or value [16]. It starts with a it only takes <b>positive</b> observation. root node, which is the best predictor. Then, it progresses <b>TPR</b>:: \ud835\udc47\ud835\udc43 through branch nodes to other predictors. Ultimately it reaches \ud835\udc47\ud835\udc43 ...", "dateLastCrawled": "2022-02-05T20:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Evaluation Metric Special ROC-AUC Candra\u2019s blog", "url": "https://saltfarmer.github.io/blog/machine%20learning/Evaluation-Metrics-Special-ROCAUC/", "isFamilyFriendly": true, "displayUrl": "https://saltfarmer.github.io/blog/<b>machine</b> <b>learning</b>/Evaluation-Metrics-Special-ROCAUC", "snippet": "The ROC curve is created by plotting the <b>true</b> <b>positive</b> <b>rate</b> (<b>TPR</b>) against the false <b>positive</b> <b>rate</b> (FPR) at various threshold settings. ROC is a probability curve and AUC represents degree or measure of separability. It tells how much model is capable of distinguishing between classes. Higher the AUC, better the model is at predicting 0s as 0s ...", "dateLastCrawled": "2022-02-03T01:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Evaluation Metrics. when it comes to unsupervised <b>learning</b>\u2026 | by Khalid ...", "url": "https://khalidgharib.medium.com/evaluation-metrics-69f3905880b", "isFamilyFriendly": true, "displayUrl": "https://khalidgharib.medium.com/evaluation-metrics-69f3905880b", "snippet": "Recall also known as sensitivity or <b>True</b> <b>Positive</b> <b>Rate</b>(<b>TPR</b>), is saying that when the actual number of positives is 5, ... in other words, the higher the AUC, the better the model is at predicting 0s as 0s and 1s as 1s. By <b>analogy</b>, the Higher the AUC, the better the model is at distinguishing between patients with the disease and no disease. you can see as I mentioned earlier depending on where your threshold or criterion value is placed you can reduce the number of FP but will inevitably ...", "dateLastCrawled": "2022-01-31T06:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "More Performance Evaluation Metrics for Classification Problems You ...", "url": "https://www.kdnuggets.com/2020/04/performance-evaluation-metrics-classification.html", "isFamilyFriendly": true, "displayUrl": "https://www.<b>kdnuggets</b>.com/2020/04/performance-evaluation-metrics-classification.html", "snippet": "A ROC curve plots the <b>true</b> <b>positive</b> <b>rate</b> (<b>tpr</b>) versus the false <b>positive</b> <b>rate</b> (fpr) as a function of the model\u2019s threshold for classifying a <b>positive</b>. Given that c is a constant known as decision threshold, the below ROC curve suggests that by default c=0.5, when c=0.2, both <b>tpr</b> and fpr increase.", "dateLastCrawled": "2022-01-26T05:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Evaluation metric for Supervised <b>Learning</b>: | by Anuganti Suresh | Medium", "url": "https://anugantisuresh.medium.com/evaluation-metric-for-supervised-learning-ba063f1bb1af", "isFamilyFriendly": true, "displayUrl": "https://anugantisuresh.medium.com/evaluation-metric-for-supervised-<b>learning</b>-ba063f1bb1af", "snippet": "A higher <b>TPR</b> and a lower FNR is desirable since we want to correctly classify the <b>positive</b> class. The area under the curve represents the area under the curve when the false <b>positive</b> <b>rate</b> is plotted against the <b>True</b> <b>positive</b> <b>rate</b> as below. AUC ranges between 0 and 1. A value of 0 means 100% prediction of the model is incorrect. A value of 1 ...", "dateLastCrawled": "2022-01-06T20:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Understanding <b>AUC</b> - ROC Curve | by Sarang Narkhede | Towards Data Science", "url": "https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-<b>auc</b>-roc-curve-68b2303cc9c5", "snippet": "In <b>Machine</b> <b>Learning</b>, performance measurement is an essential task. So when it comes to a classification problem, we can count on an <b>AUC</b> - ROC Curve. When we need to check or visualize the performance\u2026 Get started. Open in app. Sign in. Get started. Follow. 617K Followers \u00b7 Editors&#39; Picks Features Deep Dives Grow Contribute. About. Get started. Open in app. Understanding <b>AUC</b> - ROC Curve. Sarang Narkhede. Jun 26, 2018 \u00b7 5 min read. Understanding <b>AUC</b> - ROC Curve [Image 1] (Image courtesy ...", "dateLastCrawled": "2022-01-30T12:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "What is the AUC \u2014 <b>ROC</b> Curve?. AUC-<b>ROC</b> CURVE | CONFUSION MATRIX |\u2026 | by ...", "url": "https://medium.com/computer-architecture-club/what-is-the-auc-roc-curve-47fbdcbf7a4a", "isFamilyFriendly": true, "displayUrl": "https://medium.com/computer-architecture-club/what-is-the-auc-<b>roc</b>-curve-47fbdcbf7a4a", "snippet": "By <b>analogy</b>, Higher the AUC, ... Sensitivity / <b>TPR</b> (<b>True</b> <b>Positive</b> <b>Rate</b>) / Recall. Sensitivity tells us what proportion of the <b>positive</b> class got correctly classified. A simple example would be to ...", "dateLastCrawled": "2022-01-26T00:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Understanding <b>Classification</b> Thresholds Using Isocurves | by Druce ...", "url": "https://towardsdatascience.com/understanding-classification-thresholds-using-isocurves-9e5e7e00e5a2", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-<b>classification</b>-<b>threshold</b>s-using-isocurves...", "snippet": "The <b>true</b>-<b>positive</b> <b>rate</b> (<b>TPR</b>) is the number of <b>true</b> positives / ground truth positives (also called recall or sensitivity). Ground truth positives = <b>true</b> positives + false negatives: <b>TPR</b> = tp / (tp+fn) A false <b>positive</b> is a false observation incorrectly predicted to be <b>true</b>. The false-<b>positive</b> <b>rate</b> (FPR) is the number of false positives / ground truth negatives (1 \u2014 FPR is the specificity). Ground truth negatives = <b>true</b> negatives + false positives: FPR = fp / (tn + fp) The best point to be ...", "dateLastCrawled": "2022-02-02T19:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Learning from positive</b> and unlabeled data: a survey - <b>Machine</b> <b>Learning</b>", "url": "https://link.springer.com/article/10.1007/s10994-020-05877-5", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10994-020-05877-5", "snippet": "<b>Learning from positive</b> and unlabeled data or PU <b>learning</b> is the setting where a learner only has access to <b>positive</b> examples and unlabeled data. The assumption is that the unlabeled data can contain both <b>positive</b> and negative examples. This setting has attracted increasing interest within the <b>machine</b> <b>learning</b> literature as this type of data naturally arises in applications such as medical diagnosis and knowledge base completion. This article provides a survey of the current state of the art ...", "dateLastCrawled": "2022-02-02T03:04:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "How <b>to calculate the image accuracy through ROC method</b>?", "url": "https://www.researchgate.net/post/How_to_calculate_the_image_accuracy_through_ROC_method", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/post/How_<b>to_calculate_the_image_accuracy_through_ROC_method</b>", "snippet": "<b>True Positive Rate (TPR) is like</b> a recall and is defined as mathematically . TPR = (TP/TP+FN) False Positive Rate (FPR) is defined as mathematically . FPR = (FP/FP+TN) An ROC curve plots TPR vs ...", "dateLastCrawled": "2022-01-17T03:34:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], []], "all_bing_queries": ["+(true positive rate (tpr))  is like +(percentage of correct predictions made by the algorithm)", "+(true positive rate (tpr)) is similar to +(percentage of correct predictions made by the algorithm)", "+(true positive rate (tpr)) can be thought of as +(percentage of correct predictions made by the algorithm)", "+(true positive rate (tpr)) can be compared to +(percentage of correct predictions made by the algorithm)", "machine learning +(true positive rate (tpr) AND analogy)", "machine learning +(\"true positive rate (tpr) is like\")", "machine learning +(\"true positive rate (tpr) is similar\")", "machine learning +(\"just as true positive rate (tpr)\")", "machine learning +(\"true positive rate (tpr) can be thought of as\")", "machine learning +(\"true positive rate (tpr) can be compared to\")"]}