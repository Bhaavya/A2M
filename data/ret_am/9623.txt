{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Nuit Blanche: <b>Convex</b> <b>Optimization</b> Approaches for Blind Sensor ...", "url": "https://nuit-blanche.blogspot.com/2013/08/convex-optimization-approaches-for.html", "isFamilyFriendly": true, "displayUrl": "https://nuit-blanche.blogspot.com/2013/08/<b>convex</b>-<b>optimization</b>-approaches-for.html", "snippet": "<b>Convex</b> <b>Optimization</b> Approaches for Blind Sensor Calibration using Sparsity /A Conjugate Gradient Algorithm for Blind Senor Calibration in Sparse Recovery Here is a follow-up to Blind Sensor Calibration in Sparse Recovery Using <b>Convex</b> <b>Optimization</b> and Analysis Based Blind Compressive Sensing. One of the most important finding in recent years besides the idea of compressive sensing is the idea that there are sharp phase transition between parameters where a system works and doesn&#39;t work. In ...", "dateLastCrawled": "2022-01-13T15:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Calibrating</b> Least Squares Semidefinite Programming with Equality and ...", "url": "https://epubs.siam.org/doi/10.1137/080727075", "isFamilyFriendly": true, "displayUrl": "https://epubs.siam.org/doi/10.1137/080727075", "snippet": "Computational <b>Optimization</b> and Applications 78:1, 1-42. (2020) A Novel Posture Positioning Method for Multi-Joint Manipulators. IEEE Sensors Journal 20:23, 14310-14316. (2020) A multi-stage <b>convex</b> relaxation approach to noisy structured low-rank matrix recovery. Mathematical Programming Computation 12:4, 569-602. (2020) Feasibility and a fast algorithm for Euclidean distance matrix <b>optimization</b> with ordinal constraints. Computational <b>Optimization</b> and Applications 76:2, 535-569. (2020 ...", "dateLastCrawled": "2022-01-28T09:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Calibrating Optimal PMSM Torque Control with Field-Weakening</b> Using ...", "url": "https://www.mathworks.com/company/newsletters/articles/calibrating-optimal-pmsm-torque-control-with-field-weakening-using-model-based-calibration.html", "isFamilyFriendly": true, "displayUrl": "https://www.mathworks.com/company/newsletters/articles/<b>calibrating</b>-optimal-pmsm-torque...", "snippet": "It is an industry-proven, automated workflow that uses statistical modeling and numeric <b>optimization</b> to optimally calibrate complex nonlinear systems. It can be used in a wide range of applications and is well known for being adopted in internal combustion engine control calibration. When applied to e-motor control calibration, the model-based calibration workflow can help motor control engineers achieve optimal torque and field-weakening control for PMSMs. PMSM Characterization and ...", "dateLastCrawled": "2022-01-28T15:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Advantages and Disadvantages of Support Vector Machines (SVMs) Compared ...", "url": "https://www.diw.de/documents/publikationen/73/diw_01.c.88369.de/dp811.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.diw.de/documents/publikationen/73/diw_01.c.88369.de/dp811.pdf", "snippet": "methods <b>like</b> linear Discriminant Analysis (DA) and Logit or Probit Models and non-parametric statistical models <b>like</b> Neural Networks. SVMs are a new promising non-linear, non-parametric classification tech- nique, which already showed good results in the medical diagnostics, optical character recognition, elec-tric load forecasting and other fields. Applied to solvency analysis, the common objective of all these clas-1 Deutsche Bundesbank, Georgplatz 5, 30159 Hannover. 2 German Institute for ...", "dateLastCrawled": "2022-01-26T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Quantum algorithm for quicker clinical prognostic analysis: an ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8323083/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8323083", "snippet": "<b>Optimization</b> and Gibbs Sampling . Enhance learning algorithms <b>like</b> Bayesian networks , Tensors, and search. QML\u2019s ability to deal with large-scale biased datasets yield faster complexity factors for major classical computing and <b>machine</b> learning tasks, consuming thereby less space and time. Effectively, it uses quantum annealers and tunneling ...", "dateLastCrawled": "2022-01-26T10:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "mathematical <b>optimization</b> - Issue with ConvexOptimization over Matrices ...", "url": "https://mathematica.stackexchange.com/questions/257165/issue-with-convexoptimization-over-matrices-with-mathematica-12-2", "isFamilyFriendly": true, "displayUrl": "https://mathematica.stackexchange.com/questions/257165/issue-with-<b>convexoptimization</b>...", "snippet": "With Mathematica 12.2, ConvexOptimization was introduced, which, as its name suggests, can solve arbitrary <b>convex</b> <b>optimization</b> problems over matrices [Language Reference]. Using ConvexOptimization can help me get around reformulating my problem into SemidefiniteOptimization, a step that is cumbersome but otherwise works fine.", "dateLastCrawled": "2022-01-09T15:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine learning</b> and <b>optimization</b> for production rescheduling in ...", "url": "https://link.springer.com/article/10.1007/s00170-020-05850-5", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00170-020-05850-5", "snippet": "Support Vector <b>Machine</b> (SVM): input vectors are mapped to high dimensional feature space and a linear decision surface is constructed in the space . It does not require any parameter tuning since it can find good parameter settings automatically . It delivers a unique solution because the <b>optimization</b> problem is <b>convex</b>.", "dateLastCrawled": "2022-01-27T22:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A Gentle Introduction to Jensen&#39;s Inequality - <b>Machine</b> Learning Mastery", "url": "https://machinelearningmastery.com/a-gentle-introduction-to-jensens-inequality/", "isFamilyFriendly": true, "displayUrl": "https://<b>machine</b>learningmastery.com/a-gentle-introduction-to-jensens-inequality", "snippet": "The mean of a <b>convex</b> function of a variable is always greater than the function of the mean variable, called Jensen\u2019s Inequality. A common application of the inequality is in the comparison of arithmetic and geometric means when averaging the financial returns for a time interval. Kick-start your project with my new book Probability for <b>Machine</b> Learning, including step-by-step tutorials and the Python source code files for all examples. Let\u2019s get started. Update Oct/2019: Fixed minor ...", "dateLastCrawled": "2022-01-29T13:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "How to Implement <b>Bayesian Optimization</b> from Scratch in Python", "url": "https://machinelearningmastery.com/what-is-bayesian-optimization/", "isFamilyFriendly": true, "displayUrl": "https://<b>machine</b>learningmastery.com/what-is-<b>bayesian-optimization</b>", "snippet": "<b>Bayesian Optimization</b> is often used in applied <b>machine</b> learning to tune the hyperparameters of a given well-performing model on a validation dataset. After completing this tutorial, you will know: Global <b>optimization</b> is a challenging problem that involves black box and often non-<b>convex</b>, non-linear, noisy, and computationally expensive objective functions.", "dateLastCrawled": "2022-02-03T04:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How to use Jax <b>to streamline machine learning optimization</b> | by Sam ...", "url": "https://medium.com/utility-machine-learning/using-jax-to-streamline-machine-learning-optimization-d0da2f53a9fb", "isFamilyFriendly": true, "displayUrl": "https://medium.com/.../using-jax-<b>to-streamline-machine-learning-optimization</b>-d0da2f53a9fb", "snippet": "<b>Optimization</b> is an essential part of making any <b>machine</b> learning problem feasible. Derivative-based <b>optimization</b> methods are by far the most common and reliable approaches used, but require that ...", "dateLastCrawled": "2021-09-30T11:29:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Nuit Blanche: <b>Convex</b> <b>Optimization</b> Approaches for Blind Sensor ...", "url": "https://nuit-blanche.blogspot.com/2013/08/convex-optimization-approaches-for.html", "isFamilyFriendly": true, "displayUrl": "https://nuit-blanche.blogspot.com/2013/08/<b>convex</b>-<b>optimization</b>-approaches-for.html", "snippet": "<b>Convex</b> <b>Optimization</b> Approaches for Blind Sensor Calibration using Sparsity /A Conjugate Gradient Algorithm for Blind Senor Calibration in Sparse Recovery Here is a follow-up to Blind Sensor Calibration in Sparse Recovery Using <b>Convex</b> <b>Optimization</b> and Analysis Based Blind Compressive Sensing. One of the most important finding in recent years besides the idea of compressive sensing is the idea that there are sharp phase transition between parameters where a system works and doesn&#39;t work. In ...", "dateLastCrawled": "2022-01-13T15:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Convex</b> <b>Optimization</b> Approaches for Blind Sensor Calibration using Sparsity", "url": "https://www.arxiv-vanity.com/papers/1308.5354/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1308.5354", "snippet": "We investigate a compressive sensing framework in which the sensors introduce a distortion to the measurements in the form of unknown gains. We focus on blind calibration, using measures performed on multiple unknown (but sparse) signals and formulate the joint recovery of the gains and the sparse signals as a <b>convex</b> <b>optimization</b> problem. We divide this problem in 3 subproblems with different conditions on the gains, specifially (i) gains with different amplitude and the same phase, (ii ...", "dateLastCrawled": "2021-10-21T18:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Combination treatment <b>optimization</b> using a pan-cancer pathway model", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8747684/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8747684", "snippet": "We adapt non-<b>convex</b> <b>optimization</b> techniques and use an efficient parallelization scheme which enables the analysis of dozens of cell lines and combinations of 7 anti-cancer agents at low cost. Three different treatment scenarios targeting single as well as multiple cell lines at once are formalized as <b>optimization</b> problems and simulations studies are conducted. Our simulations identified a set of treatment candidates in the form of drug combinations that achieve better predicted treatment ...", "dateLastCrawled": "2022-01-12T07:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) <b>Machine</b> Learning in Marketing: Overview, Learning Strategies ...", "url": "https://www.researchgate.net/publication/344000369_Machine_Learning_in_Marketing_Overview_Learning_Strategies_Applications_and_Future_Developments", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/344000369_<b>Machine</b>_Learning_in_Marketing...", "snippet": "iterative <b>optimization</b> procedure <b>similar</b> to the Concave-<b>Convex</b> procedure to update parameters 26 corresponding to each taste, and shuffling products between tastes.", "dateLastCrawled": "2022-02-01T20:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Quantum algorithm for quicker clinical prognostic analysis: an ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8323083/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8323083", "snippet": "The performance was evident because of the efficiency of quantum simulation and faster convergence property solving for an <b>optimization</b> problem for network training particularly for large-scale biased image classification task. The model run-time observed on quantum optimized hardware was 52 min, while on K80 GPU hardware it was 1 h 30 min for <b>similar</b> sample size. The simulation shows that QNN outperforms DNN, CNN, 2D CNN by more than 2.92% in gain in accuracy measure with an average recall ...", "dateLastCrawled": "2022-01-26T10:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Machine learning</b> and <b>optimization</b> for production rescheduling in ...", "url": "https://link.springer.com/article/10.1007/s00170-020-05850-5", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00170-020-05850-5", "snippet": "Along with the fourth industrial revolution, different tools coming from <b>optimization</b>, Internet of Things, data science, and artificial intelligence fields are creating new opportunities in production management. While manufacturing processes are stochastic and rescheduling decisions need to be made under uncertainty, it is still a complicated task to decide whether a rescheduling is worthwhile, which is often addressed in practice on a greedy basis. To find a tradeoff between rescheduling ...", "dateLastCrawled": "2022-01-27T22:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "An Efficient Method <b>for Convex Constrained Rank Minimization Problems</b> ...", "url": "https://www.hindawi.com/journals/mpe/2016/7473041/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.hindawi.com</b>/journals/mpe/2016/7473041", "snippet": "The constrained rank minimization problem has various applications in many fields including <b>machine</b> learning, control, and signal processing. In this paper, we consider the <b>convex</b> constrained rank minimization problem. By introducing a new variable and penalizing an equality constraint to objective function, we reformulate the <b>convex</b> objective function with a rank constraint as a difference of <b>convex</b> functions based on the closed-form solutions, which can be reformulated as DC programming. A ...", "dateLastCrawled": "2022-01-18T23:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "How to Implement <b>Bayesian Optimization</b> from Scratch in Python", "url": "https://machinelearningmastery.com/what-is-bayesian-optimization/", "isFamilyFriendly": true, "displayUrl": "https://<b>machine</b>learningmastery.com/what-is-<b>bayesian-optimization</b>", "snippet": "<b>Bayesian Optimization</b> is often used in applied <b>machine</b> learning to tune the hyperparameters of a given well-performing model on a validation dataset. After completing this tutorial, you will know: Global <b>optimization</b> is a challenging problem that involves black box and often non-<b>convex</b>, non-linear, noisy, and computationally expensive objective functions. <b>Bayesian Optimization</b> provides a probabilistically principled method for global <b>optimization</b>. How to implement <b>Bayesian Optimization</b> from ...", "dateLastCrawled": "2022-02-03T04:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A Gentle Introduction to Expectation-Maximization (EM Algorithm)", "url": "https://machinelearningmastery.com/expectation-maximization-em-algorithm/", "isFamilyFriendly": true, "displayUrl": "https://<b>machine</b>learningmastery.com/expectation-maximization-em-algorithm", "snippet": "Maximum Likelihood Estimation involves treating the problem as an <b>optimization</b> or search problem, ... but the data is combined and the distributions are <b>similar</b> enough that it is not obvious to which distribution a given point may belong. The processes used to generate the data point represents a latent variable, e.g. process 0 and process 1. It influences the data but is not observable. As such, the EM algorithm is an appropriate approach to use to estimate the parameters of the ...", "dateLastCrawled": "2022-02-03T01:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "A comparative study among <b>machine</b> learning and numerical models for ...", "url": "https://www.nature.com/articles/s41598-020-60698-9", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41598-020-60698-9", "snippet": "<b>Similar</b> to the <b>machine</b> learning models, the generalization ability of numerical model was also evaluated by calculating GA values. The GA values calculated from groundwater level and streamflow ...", "dateLastCrawled": "2022-02-01T21:52:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Blind calibration for compressed sensing by convex optimization</b> ...", "url": "https://www.researchgate.net/publication/51960431_Blind_calibration_for_compressed_sensing_by_convex_optimization", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/51960431_Blind_calibration_for_compressed...", "snippet": "Gribonval et al. [29] showed that a BDC problem <b>can</b> be posed as a <b>convex</b> <b>optimization</b> problem and a solution <b>can</b> be achieved by using off-the-shelf <b>optimization</b> solvers. However, identifiability ...", "dateLastCrawled": "2021-12-21T11:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Blind Sensor Calibration in Sparse Recovery Using <b>Convex</b> <b>Optimization</b> ...", "url": "https://nuit-blanche.blogspot.com/2013/05/blind-sensor-calibration-in-sparse.html", "isFamilyFriendly": true, "displayUrl": "https://nuit-blanche.blogspot.com/2013/05/blind-sensor-calibration-in-sparse.html", "snippet": "The simultaneous recovery of the gains and the sparse signals is formulated as a <b>convex</b> <b>optimization</b> problem which <b>can</b> be solved easily using off-the-shelf algorithms. Numerical simulations demonstrate that the proposed approach is effective provided that suf\ufb01ciently many (unknown, but sparse) <b>calibrating</b> signals are provided, especially when the sign or phase of the unknown gains are not completely random. Let us note in a different realm version 2 of this paper: Analysis Based Blind ...", "dateLastCrawled": "2022-01-22T00:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Nuit Blanche: <b>Convex</b> <b>Optimization</b> Approaches for Blind Sensor ...", "url": "https://nuit-blanche.blogspot.com/2013/08/convex-optimization-approaches-for.html", "isFamilyFriendly": true, "displayUrl": "https://nuit-blanche.blogspot.com/2013/08/<b>convex</b>-<b>optimization</b>-approaches-for.html", "snippet": "<b>Convex</b> <b>Optimization</b> Approaches for Blind Sensor Calibration using Sparsity /A Conjugate Gradient Algorithm for Blind Senor Calibration in Sparse Recovery Here is a follow-up to Blind Sensor Calibration in Sparse Recovery Using <b>Convex</b> <b>Optimization</b> and Analysis Based Blind Compressive Sensing. One of the most important finding in recent years besides the idea of compressive sensing is the idea that there are sharp phase transition between parameters where a system works and doesn&#39;t work. In ...", "dateLastCrawled": "2022-01-13T15:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Combination treatment <b>optimization</b> using a pan-cancer pathway model", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8747684/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8747684", "snippet": "We adapt non-<b>convex</b> <b>optimization</b> techniques and use an efficient parallelization scheme which enables the analysis of dozens of cell lines and combinations of 7 anti-cancer agents at low cost. Three different treatment scenarios targeting single as well as multiple cell lines at once are formalized as <b>optimization</b> problems and simulations studies are conducted. Our simulations identified a set of treatment candidates in the form of drug combinations that achieve better predicted treatment ...", "dateLastCrawled": "2022-01-12T07:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Learning Convex Optimization Control Policies</b> | Request PDF", "url": "https://www.researchgate.net/publication/338116200_Learning_Convex_Optimization_Control_Policies", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/338116200_Learning_<b>Convex</b>_<b>Optimization</b>...", "snippet": "Common examples of such <b>convex</b> <b>optimization</b> control policies (COCPs) include the linear quadratic regulator (LQR), <b>convex</b> model predictive control (MPC), and <b>convex</b> control-Lyapunov or approximate ...", "dateLastCrawled": "2022-01-22T08:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>optimization</b> - What is the canonical reference for Minimum Variance ...", "url": "https://quant.stackexchange.com/questions/4568/what-is-the-canonical-reference-for-minimum-variance-portfolios-uniqueness", "isFamilyFriendly": true, "displayUrl": "https://quant.stackexchange.com/questions/4568/what-is-the-<b>can</b>onical-reference-for...", "snippet": "For academic references, you will likely have to look in the very early <b>optimization</b> literature. Uniqueness of the MV portfolio follows immediately from the lemma that a strictly <b>convex</b> function on a <b>convex</b> set has no local minima. The standard textbook reference is <b>Convex</b> <b>Optimization</b> by Boyd and Vandenberghe. See section 4.2.2 in particular.", "dateLastCrawled": "2022-01-24T08:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A Gentle Introduction to Bayes Theorem for <b>Machine Learning</b>", "url": "https://machinelearningmastery.com/bayes-theorem-for-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/bayes-theorem-for-<b>machine-learning</b>", "snippet": "The notion of testing different models on a dataset in applied <b>machine learning</b> <b>can</b> <b>be thought</b> of as estimating the probability of each hypothesis (h1, h2, h3, \u2026 in H) being true given the observed data. The <b>optimization</b> or seeking the hypothesis with the maximum posterior probability in modeling is called maximum a posteriori or MAP for short. Any such maximally probable hypothesis is called a maximum a posteriori (MAP) hypothesis. We <b>can</b> determine the MAP hypotheses by using Bayes ...", "dateLastCrawled": "2022-01-28T19:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A <b>Gentle Introduction to Bayes Theorem for Machine Learning</b> \u3010Get ...", "url": "https://tutorials.one/a-gentle-introduction-to-bayes-theorem-for-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://tutorials.one/a-<b>gentle-introduction-to-bayes-theorem-for-machine-learning</b>", "snippet": "The notion of testing different models on a dataset in applied <b>machine</b> learning <b>can</b> <b>be thought</b> of as estimating the probability of each hypothesis (h1, h2, h3, \u2026 in H) being true given the observed data. The <b>optimization</b> or seeking the hypothesis with the maximum posterior probability in modeling is called maximum a posteriori or MAP for short. Any such maximally probable hypothesis is called a maximum a posteriori (MAP) hypothesis. We <b>can</b> determine the MAP hypotheses by using Bayes ...", "dateLastCrawled": "2021-12-17T23:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Most efficient scipy <b>optimization</b>/root finding function : quant", "url": "https://www.reddit.com/r/quant/comments/h7uz1c/most_efficient_scipy_optimizationroot_finding/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/quant/comments/h7uz1c/most_efficient_scipy_<b>optimization</b>root...", "snippet": "I\u2019m looking for a more efficient root finding method in scipy\u2019s <b>optimization</b> module. The purpose is solving for volatility used in a calibration objective function. Currently I\u2019ve been switching between brent and fsolve, but I\u2019m kind of disappointed in their performance. They are both pretty slow and I <b>can</b>\u2019t reduce the tolerance enough without sacrificing accuracy. Are there any other methods that are used that give better performance? I would prefer to stick with scipy, but if ...", "dateLastCrawled": "2021-01-21T13:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What should I do when my <b>neural network</b> doesn&#39;t learn? - Cross Validated", "url": "https://stats.stackexchange.com/questions/352036/what-should-i-do-when-my-neural-network-doesnt-learn", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/352036", "snippet": "Non-<b>convex</b> <b>optimization</b> is hard. The objective function of a <b>neural network</b> is only <b>convex</b> when there are no hidden units, all activations are linear, and the design matrix is full-rank -- because this configuration is identically an ordinary regression problem. In all other cases, the <b>optimization</b> problem is non-<b>convex</b>, and non-<b>convex</b> <b>optimization</b> is hard. The challenges of training neural networks are well-known (see: Why is it hard to train deep neural networks?). Additionally, neural ...", "dateLastCrawled": "2022-01-30T03:31:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Blind calibration for compressed sensing by convex optimization</b> ...", "url": "https://www.researchgate.net/publication/51960431_Blind_calibration_for_compressed_sensing_by_convex_optimization", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/51960431_Blind_calibration_for_compressed...", "snippet": "Gribonval et al. [29] showed that a BDC problem <b>can</b> be posed as a <b>convex</b> <b>optimization</b> problem and a solution <b>can</b> be achieved by using off-the-shelf <b>optimization</b> solvers. However, identifiability ...", "dateLastCrawled": "2021-12-21T11:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Convex</b> <b>Optimization</b> Approaches for Blind Sensor Calibration using Sparsity", "url": "https://www.arxiv-vanity.com/papers/1308.5354/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1308.5354", "snippet": "We investigate a compressive sensing framework in which the sensors introduce a distortion to the measurements in the form of unknown gains. We focus on blind calibration, using measures performed on multiple unknown (but sparse) signals and formulate the joint recovery of the gains and the sparse signals as a <b>convex</b> <b>optimization</b> problem. We divide this problem in 3 subproblems with different conditions on the gains, specifially (i) gains with different amplitude and the same phase, (ii ...", "dateLastCrawled": "2021-10-21T18:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A robust and <b>convex</b> metric for unconstrained <b>optimization</b> in ...", "url": "https://www.researchgate.net/publication/332915326_A_robust_and_convex_metric_for_unconstrained_optimization_in_statistical_model_calibration-probability_residual_PR", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/332915326_A_robust_and_<b>convex</b>_metric_for...", "snippet": "DGAN <b>can</b> construct a predictive model through <b>machine</b> learning using only actual test data, improve the prediction accuracy of an actual test model, and present design variables that affect the ...", "dateLastCrawled": "2022-01-16T14:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>A convex relaxation optimization algorithm for multi</b>-camera calibration ...", "url": "https://www.sciencedirect.com/science/article/pii/S0925231216306464", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0925231216306464", "snippet": "The core of the proposed method is LMI-based <b>convex</b> relaxation <b>optimization</b> for the intrinsic parameters. First, the constraints on the intrinsic parameters of each camera in the multi-camera system are analyzed, and a cost function for the IAC corresponding to each camera is constructed. Second, <b>a convex relaxation optimization algorithm</b> based on an LMI is presented to solve the IAC, and the intrinsic parameters of each camera are determined. Finally, the extrinsic parameters are computed ...", "dateLastCrawled": "2021-11-09T10:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "On Accelerating Distributed <b>Convex</b> Optimizations | DeepAI", "url": "https://deepai.org/publication/on-accelerating-distributed-convex-optimizations", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/on-accelerating-distributed-<b>convex</b>-<b>optimizations</b>", "snippet": "The proposed algorithm rigorously extends that idea of iterative pre-conditioning to general <b>convex</b> <b>optimization</b> problems (1). Using real-world datasets, we empirically show that the proposed algorithm converges in fewer iterations <b>compared</b> to the aforementioned state-of-the-art methods for solving the distributed <b>convex</b> <b>optimization</b> problem (1 ...", "dateLastCrawled": "2022-01-14T18:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A Priori Calibration of Transient Kinetics Data via <b>Machine</b> Learning ...", "url": "https://deepai.org/publication/a-priori-calibration-of-transient-kinetics-data-via-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/a-priori-calibration-of-transient-kinetics-data-via...", "snippet": "A novel method is developed for treating and <b>calibrating</b> the data without reliance on previous inert experiments while utilizing the relationships of the reactor physics, e.g., the expected distribution of the outlet flux and statistics. More specifically, <b>machine</b> learning via <b>convex</b> <b>optimization</b> with physics-based constraints is used to automatically preprocess the raw data into chemical quantities. The baseline correction and calibration are performed through examining the relationships ...", "dateLastCrawled": "2022-01-09T06:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Advantages and Disadvantages of Support Vector Machines (SVMs) <b>Compared</b> ...", "url": "https://www.diw.de/documents/publikationen/73/diw_01.c.88369.de/dp811.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.diw.de/documents/publikationen/73/diw_01.c.88369.de/dp811.pdf", "snippet": "sample is reduced. It <b>can</b> be demonstrated that C is linked to the width of the margin. The smaller is C, the wider is the margin, the more and larger in-sample classification errors are permitted. Solving the above mentioned constrained <b>optimization</b> problem of <b>calibrating</b> an SVM means searching for the minimum of the following Lagrange function: 4", "dateLastCrawled": "2022-01-26T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "mathematical <b>optimization</b> - Issue with ConvexOptimization over Matrices ...", "url": "https://mathematica.stackexchange.com/questions/257165/issue-with-convexoptimization-over-matrices-with-mathematica-12-2", "isFamilyFriendly": true, "displayUrl": "https://mathematica.stackexchange.com/questions/257165/issue-with-<b>convexoptimization</b>...", "snippet": "With Mathematica 12.2, ConvexOptimization was introduced, which, as its name suggests, <b>can</b> solve arbitrary <b>convex</b> <b>optimization</b> problems over matrices [Language Reference]. Using ConvexOptimization <b>can</b> help me get around reformulating my problem into SemidefiniteOptimization, a step that is cumbersome but otherwise works fine.", "dateLastCrawled": "2022-01-09T15:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "The Curious Case of <b>Convex</b> Neural Networks | SpringerLink", "url": "https://link.springer.com/chapter/10.1007/978-3-030-86486-6_45", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/chapter/10.1007/978-3-030-86486-6_45", "snippet": "We draw three valuable insights: (a) Input Output <b>Convex</b> Neural Networks (IOC-NNs) self regularize and significantly reduce the problem of overfitting; (b) Although heavily constrained, they outperform the base multi layer perceptrons and achieve similar performance as <b>compared</b> to base convolutional architectures and (c) IOC-NNs show robustness to noise in train labels. We demonstrate the efficacy of the proposed idea using thorough experiments and ablation studies on six commonly used image ...", "dateLastCrawled": "2021-12-31T15:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "An Efficient Method <b>for Convex Constrained Rank Minimization Problems</b> ...", "url": "https://www.hindawi.com/journals/mpe/2016/7473041/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.hindawi.com</b>/journals/mpe/2016/7473041", "snippet": "The constrained rank minimization problem has various applications in many fields including <b>machine</b> learning, control, and signal processing. In this paper, we consider the <b>convex</b> constrained rank minimization problem. By introducing a new variable and penalizing an equality constraint to objective function, we reformulate the <b>convex</b> objective function with a rank constraint as a difference of <b>convex</b> functions based on the closed-form solutions, which <b>can</b> be reformulated as DC programming. A ...", "dateLastCrawled": "2022-01-18T23:00:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Optimization</b> for <b>Machine</b> <b>Learning</b>", "url": "http://optml.mit.edu/talks/pkuLectAlgo3.pdf", "isFamilyFriendly": true, "displayUrl": "optml.mit.edu/talks/pkuLectAlgo3.pdf", "snippet": "<b>Optimization</b> for <b>Machine</b> <b>Learning</b> \u2013 Sra, Nowozin, Wright Theory of <b>Convex</b> <b>Optimization</b> for <b>Machine</b> <b>Learning</b> \u2013 Bubeck NIPS 2016 <b>Optimization</b> Tutorial \u2013 Bach, Sra Some related courses: EE227A, Spring 2013, (Sra, UC Berkeley) 10-801, Spring 2014 (Sra, CMU) EE364a,b (Boyd, Stanford) EE236b,c (Vandenberghe, UCLA) Venues: NIPS, ICML, UAI, AISTATS, SIOPT, Math. Prog. Suvrit Sra(suvrit@mit.edu)<b>Optimization</b> for <b>Machine</b> <b>Learning</b> 2 / 29. Lecture Plan \u2013Introduction (3 lectures) \u2013Problems and ...", "dateLastCrawled": "2021-08-29T23:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Basic Concepts in Machine Learning</b>", "url": "https://machinelearningmastery.com/basic-concepts-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>basic-concepts-in-machine-learning</b>", "snippet": "For example combinatorial <b>optimization</b>, <b>convex</b> <b>optimization</b>, constrained <b>optimization</b>. All <b>machine learning</b> algorithms are combinations of these three components. A framework for understanding all algorithms. Types of <b>Learning</b> . There are four types of <b>machine learning</b>: Supervised <b>learning</b>: (also called inductive <b>learning</b>) Training data includes desired outputs. This is spam this is not, <b>learning</b> is supervised. Unsupervised <b>learning</b>: Training data does not include desired outputs. Example is ...", "dateLastCrawled": "2022-02-02T15:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Introduction to Machine Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/24/introduction-to-machine-learning-2/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/24/<b>introduction-to-machine-learning</b>-2", "snippet": "<b>Optimization</b> methods are applied to minimize the loss function by changing the parameter values, which is the central theme of <b>machine</b> <b>learning</b>.Zero-one loss is L0-1 = 1 (m &lt;= 0); in zero-one loss, value of loss is 0 for m &gt;= 0 whereas 1 for m &lt; 0. The difficult part with this loss is it is not differentiable, non-<b>convex</b>, and also NP-hard. Hence, in order to make <b>optimization</b> feasible and solvable, these losses are replaced by different surrogate losses for different problems.", "dateLastCrawled": "2022-01-17T05:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "11.2. <b>Convexity</b> \u2014 Dive into Deep <b>Learning</b> 0.17.2 documentation", "url": "https://d2l.ai/chapter_optimization/convexity.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_<b>optimization</b>/<b>convexity</b>.html", "snippet": "Furthermore, even though the <b>optimization</b> problems in deep <b>learning</b> are generally nonconvex, they often exhibit some properties of <b>convex</b> ones near local minima. This can lead to exciting new <b>optimization</b> variants such as [Izmailov et al., 2018].", "dateLastCrawled": "2022-01-30T21:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Gradient</b> Descent: A Quick, Simple Introduction | Built In", "url": "https://builtin.com/data-science/gradient-descent", "isFamilyFriendly": true, "displayUrl": "https://builtin.com/data-science/<b>gradient</b>-descent", "snippet": "<b>Gradient</b> descent is an <b>optimization</b> algorithm that&#39;s used when training a <b>machine</b> <b>learning</b> model. It&#39;s based on a <b>convex</b> function and tweaks its parameters iteratively to minimize a given function to its local minimum.", "dateLastCrawled": "2022-02-02T07:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Introduction to <b>Machine</b> <b>Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/30/introduction-to-machine-learning-3/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/30/introduction-to-<b>machine</b>-<b>learning</b>-3", "snippet": "Unsupervised <b>learning</b>: Similar to the teacher-student <b>analogy</b>, in which the instructor does not present and provide feedback to the student and who needs to prepare on his/her own. Unsupervised <b>learning</b> does not have as many are in supervised <b>learning</b>: Principal component analysis (PCA) K-means clustering; Reinforcement <b>learning</b>: This is the scenario in which multiple decisions need to be taken by an agent prior to reaching the target and it provides a reward, either +1 or -1, rather than ...", "dateLastCrawled": "2022-01-28T18:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>ICML Tutorial</b> \u2013 Parameter-free <b>Learning</b> and <b>Optimization</b> Algorithms", "url": "https://parameterfree.com/icml-tutorial/", "isFamilyFriendly": true, "displayUrl": "https://parameterfree.com/<b>icml-tutorial</b>", "snippet": "In particular, stochastic and batch <b>optimization</b> have become core skills for applied and theoretical <b>machine</b> <b>learning</b> researchers. This is clear from the amount of tutorials related to <b>optimization</b> in the latest years in ICML and NeurIPS. Yet, most of the advanced tutorials focused only on <b>optimization</b> under very strong assumptions, e.g. strong convexity, or finite-sum regimes, while the tutorials with a broad focus only briefly touched the basic concepts. In particular, the necessity of ...", "dateLastCrawled": "2022-02-02T03:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Regularization : What? Why? and How? (Part -1) | by Siddhant Rai ...", "url": "https://medium.com/mlearning-ai/regularization-what-why-and-how-part-1-ef6bdb6bafea", "isFamilyFriendly": true, "displayUrl": "https://medium.com/m<b>learning</b>-ai/regularization-what-why-and-how-part-1-ef6bdb6bafea", "snippet": "In general <b>machine</b> <b>learning</b> sense, it is solving an objective function to perform maximum or minimum evaluation. In reality, <b>optimization</b> is lot more profound in usage. Then we have two terms ...", "dateLastCrawled": "2022-01-28T00:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "7. <b>Optimization</b>, the central part of any <b>Machine</b> <b>Learning</b> algortithm ...", "url": "https://compphysics.github.io/MachineLearning/doc/LectureNotes/_build/html/chapteroptimization.html", "isFamilyFriendly": true, "displayUrl": "https://compphysics.github.io/<b>MachineLearning</b>/doc/LectureNotes/_build/html/chapter...", "snippet": "7. <b>Optimization</b>, the central part of any <b>Machine</b> <b>Learning</b> algortithm\u00b6. Almost every problem in <b>machine</b> <b>learning</b> and data science starts with a dataset \\(X\\), a model \\(g(\\beta)\\), which is a function of the parameters \\(\\beta\\) and a cost function \\(C(X, g(\\beta))\\) that allows us to judge how well the model \\(g(\\beta)\\) explains the observations \\(X\\).The model is fit by finding the values of \\(\\beta\\) that minimize the cost function. Ideally we would be able to solve for \\(\\beta ...", "dateLastCrawled": "2022-01-31T15:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "[2005.14605] CoolMomentum: A Method for Stochastic <b>Optimization</b> by ...", "url": "https://arxiv.org/abs/2005.14605", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/abs/2005.14605", "snippet": "This <b>analogy</b> provides useful insights for non-<b>convex</b> stochastic <b>optimization</b> in <b>machine</b> <b>learning</b>. Here we find that integration of the discretized Langevin equation gives a coordinate updating rule equivalent to the famous Momentum <b>optimization</b> algorithm. As a main result, we show that a gradual decrease of the momentum coefficient from the initial value close to unity until zero is equivalent to application of Simulated Annealing or slow cooling, in physical terms. Making use of this novel ...", "dateLastCrawled": "2021-10-23T08:45:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Best <b>Artificial Intelligence</b> Course (AIML) by UT Austin", "url": "https://www.mygreatlearning.com/pg-program-artificial-intelligence-course", "isFamilyFriendly": true, "displayUrl": "https://www.mygreat<b>learning</b>.com/pg-program-<b>artificial-intelligence</b>-course", "snippet": "<b>Machine</b> <b>Learning</b>: <b>Machine</b> <b>learning</b> is a sub-branch of AI that teaches machines to learn any task without the help of explicit directions. It teaches machines to learn by drawing inferences from past experience. <b>Machine</b> <b>learning</b> primarily focuses on developing computer programs that can access and analyze data to identify patterns and understand data behaviour to reach possible conclusions without any kind of human intervention.", "dateLastCrawled": "2022-02-01T17:58:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "What is <b>the relationship between Online Machine Learning</b> and ...", "url": "https://www.quora.com/What-is-the-relationship-between-Online-Machine-Learning-and-Incremental-Machine-Learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>the-relationship-between-Online-Machine-Learning</b>-and...", "snippet": "Answer (1 of 4): Online <b>learning</b> usually refers to the case where each example is only used once (e.g. if you&#39;re updating an ad click prediction model online after each impression or click), while incremental methods usually pick one example at a time from a finite dataset and can process the sam...", "dateLastCrawled": "2022-01-14T06:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Which <b>machine</b> <b>learning</b> algorithms for classification support online ...", "url": "https://www.quora.com/Which-machine-learning-algorithms-for-classification-support-online-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Which-<b>machine</b>-<b>learning</b>-algorithms-for-classification-support...", "snippet": "Answer (1 of 5): Most algorithms can be adapted to make them online, even though the standard implementations may not support it. E.g. both decision trees and support ...", "dateLastCrawled": "2022-01-09T00:45:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "SimplifiedMachineLearningWorkflows-book/Wolfram-Technology-Conference ...", "url": "https://github.com/antononcube/SimplifiedMachineLearningWorkflows-book/blob/master/Data/Wolfram-Technology-Conference-2016-to-2019-abstracts.csv", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/antononcube/Simplified<b>MachineLearning</b>Workflows-book/blob/master/...", "snippet": "Finally, I use <b>machine</b> <b>learning</b> algorithms to train a series of classifiers that can predict a text&#39;s authorship based on its MFW frequencies. Cross-validation indicates that Gallus and Monk are very likely one and the same author. The results also reveal the especially high and hitherto underexplored effectiveness of the Bray Curtis Distance measure and of logistic regression in shedding light on questions of authorship attribution. Data Analytics &amp; Information Science : 2016.Gunnar.Prei\u00df ...", "dateLastCrawled": "2021-12-28T12:42:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(convex optimization)  is like +(calibrating a machine)", "+(convex optimization) is similar to +(calibrating a machine)", "+(convex optimization) can be thought of as +(calibrating a machine)", "+(convex optimization) can be compared to +(calibrating a machine)", "machine learning +(convex optimization AND analogy)", "machine learning +(\"convex optimization is like\")", "machine learning +(\"convex optimization is similar\")", "machine learning +(\"just as convex optimization\")", "machine learning +(\"convex optimization can be thought of as\")", "machine learning +(\"convex optimization can be compared to\")"]}