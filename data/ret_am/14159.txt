{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Learning to Drive Smoothly in Minutes</b> | by Antonin RAFFIN | Towards ...", "url": "https://towardsdatascience.com/learning-to-drive-smoothly-in-minutes-450a7cdb35f4", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>learning-to-drive-smoothly-in-minutes</b>-450a7cdb35f4", "snippet": "<b>Learning</b> <b>to Drive</b> in a Day \u2014 Key Elements of Wayve.ai Approach . Wayve.ai describes a method to train a self-driving <b>car</b> in the real world on a simple road. This method is composed of several key elements. Wayve.ai approach: <b>learning</b> <b>to drive</b> in a day. First, they train a feature extractor (here a Variational Auto-Encoder or VAE) to compress the image to a lower dimensional space. The model is trained to reconstruct the input image but contains a bottleneck that forces it to compress the ...", "dateLastCrawled": "2022-01-31T08:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Reinforcement learning in Python to</b> teach a virtual <b>car</b> to avoid ...", "url": "https://blog.coast.ai/reinforcement-learning-in-python-to-teach-a-virtual-car-to-avoid-obstacles-part-2-93e614fcd238", "isFamilyFriendly": true, "displayUrl": "https://blog.coast.ai/<b>reinforcement-learning-in-python-to</b>-teach-a-virtual-<b>car</b>-to-avoid...", "snippet": "<b>Buffer</b> (experience <b>replay</b>): 10,000, 50,000, 500,000; Gamma: 0.9, 0.95; After running 250,000 frames of training for each of the 60 combinations of the first three params (plus a random sample to test the gamma), some interesting patterns emerged. Interesting patterns. The smaller the <b>buffer</b> size, the lower the loss but greater the variance. At a large 500,000 <b>replay</b> <b>buffer</b>, there\u2019s a tiny amount of variance but very little <b>learning</b>. 10,000 just looks <b>like</b> a highly variant 50,000. So 50,000 ...", "dateLastCrawled": "2022-02-03T07:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Self-Driving <b>Car</b> with Deep Reinforcement <b>Learning</b> in a Real ... - <b>GitHub</b>", "url": "https://github.com/MichaelBosello/Self-Driving-Car", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/MichaelBosello/Self-Driving-<b>Car</b>", "snippet": "<b>car</b>_env.py is the Reinforcement <b>Learning</b> environment. rl_<b>car</b>_driver.py contains the training cycle. dqn.py includes the CNN. state.py creates states by pre-processing and merging of frames. It also provides the compression to save RAM. <b>replay</b>.py manage the samples and the <b>replay</b> <b>buffer</b>. Release section", "dateLastCrawled": "2022-01-26T08:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Reinforcement <b>Learning</b> : Train your Own Agent using Deep Q Networks ...", "url": "https://anurag2das.medium.com/reinforcement-learning-train-your-own-agent-using-deep-q-networks-9b2c02528596", "isFamilyFriendly": true, "displayUrl": "https://anurag2das.medium.com/reinforcement-<b>learning</b>-train-your-own-agent-using-deep-q...", "snippet": "<b>Replay</b> <b>Buffer</b>: &lt;State,Action,Reward,Next_State&gt; Agent performs an action leading to a change in the state and in turn the agent is assigned with some rewards or penalties. In our case a reward of 0 is given to the agent (<b>car</b>) if it reaches to the target (location 0.5) or a penalty of -1 if location is less than 0.5 . Living penalties such as this makes sure that the agent tries to take actions with minimal number of steps and reach the final goal. When we design our environments, designing ...", "dateLastCrawled": "2022-01-19T22:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Reinforcement learning and deep neural network for autonomous driving</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/B9780128166376000099", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/B9780128166376000099", "snippet": "A <b>replay</b> <b>buffer</b> is basically a <b>buffer</b> in which the collected data of the agent (state, action, reward) are stored. After storing enough data, the <b>buffer</b> is shuffled batchwise and only then data is taken from it batchwise for training. The <b>replay</b> <b>buffer</b> has a fixed size and when this size is reached older data will be overwritten with new data. This kind of training provides a better convergence, because the network is prevented from constantly training with successive data.", "dateLastCrawled": "2022-01-24T22:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Deep imitation reinforcement <b>learning</b> for self\u2010driving by vision - Zou ...", "url": "https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/cit2.12025", "isFamilyFriendly": true, "displayUrl": "https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/cit2.12025", "snippet": "Equation allows the <b>car</b> <b>to drive</b> in the center of the lane by the angle \u03b8 ... Initialize <b>replay</b> <b>buffer</b> R. for episode = 1, M do. Initialize a random process N for action exploration. Receive initial observation state s 1. Get the low-dimensional characteristics s 1 \u2032 \u2190 P (s 1 | \u03b8 P) For t = 1, T do. Select action a t = \u03bc (s t \u2032 | \u03b8 \u03bc) + N t according to the current policy and exploration noise. Execute action a t and observe reward r t and get new characteristic s t + 1 \u2032 \u2190 P ...", "dateLastCrawled": "2022-01-30T12:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Why are correlated samples bad in DQN</b> ? : reinforcementlearning", "url": "https://www.reddit.com/r/reinforcementlearning/comments/alua6f/why_are_correlated_samples_bad_in_dqn/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/reinforcement<b>learning</b>/comments/alua6f/why_are_correlated...", "snippet": "The reason you randomly sample from the <b>replay</b> <b>buffer</b> is to reduce the sequential aspect of each s,a,r,s&#39; pair. Imagine an agent <b>learning</b> <b>to drive</b> <b>a car</b>; If you train a network based on 100 samples of only uphill driving, the gradient of the training step over compensates for pushing on the accelerator (a given action).", "dateLastCrawled": "2021-04-12T11:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Human-like autonomous car-following model with deep reinforcement learning</b>", "url": "https://www.sciencedirect.com/science/article/pii/S0968090X1830055X", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0968090X1830055X", "snippet": "Human-<b>like</b> driving will (1) provide passengers with comfortable riding, and confidence that the <b>car</b> can <b>drive</b> independently, and (2) ... The current study is the first to use a deep reinforcement <b>learning</b> to model <b>car</b>-following behavior. Of the other recent data-driven <b>car</b>-following models, the RNN, nonparametric Loess, and conventional neural network-based (designated hereafter as NNa) models were selected for comparison. 2.3. Reinforcement <b>learning</b>. By setting up simulations in which an ...", "dateLastCrawled": "2022-02-03T19:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "How can we make the <b>training loss decrease in reinforcement learning</b>", "url": "https://www.researchgate.net/post/How-can-we-make-the-training-loss-decrease-in-reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/post/How-can-we-make-the-training-loss-decrease-in...", "snippet": "The goal is <b>to drive</b> at a desired speed without crashing into other cars The state contains the velocities and positions of the agent&#39;s <b>car</b> and the surrounding cars", "dateLastCrawled": "2022-01-22T23:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "[Q] Do vanilla DQN&#39;s always have a <b>replay</b> <b>buffer</b> and use experience ...", "url": "https://www.reddit.com/r/reinforcementlearning/comments/mfomcr/q_do_vanilla_dqns_always_have_a_replay_buffer_and/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/.../mfomcr/q_do_vanilla_dqns_always_have_a_<b>replay</b>_<b>buffer</b>_and", "snippet": "I just read Minh et. al (2016) Paper on the <b>replay</b> <b>buffer</b>, where they specifically use the <b>replay</b> <b>buffer</b> to combat &quot;catastrophic forgetfulness&quot;, a phenomenon in which a DQN reaches a high score but then drastically drops in performance, abandoning the &quot;learned&quot; strategy so to speak. This made me think, that this must be an improvement on previous DQNs in which catastrophic forgetfulness occurs &quot;more&quot; catastrophically. However I cannot find a lot of resources implementing a DQN without a ...", "dateLastCrawled": "2021-03-29T12:36:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Human-like autonomous car-following model with deep reinforcement learning</b>", "url": "https://www.sciencedirect.com/science/article/pii/S0968090X1830055X", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0968090X1830055X", "snippet": "Human-like driving will (1) provide passengers with comfortable riding, and confidence that the <b>car</b> can <b>drive</b> independently, and (2) ... To enable stable and robust <b>learning</b>, DDPG deploys experience <b>replay</b> and target network, as in DQN: (1) Experience <b>replay</b>. A <b>replay</b> <b>buffer</b> is applied to address the issue that training samples in RL are not independently and identically distributed because they are generated by sequential explorations in an environment. The <b>replay</b> <b>buffer</b> is a cache D of ...", "dateLastCrawled": "2022-02-03T19:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A unifying, game-theoretic framework for imitation <b>learning</b>", "url": "https://aihub.org/2021/08/13/a-unifying-game-theoretic-framework-for-imitation-learning/", "isFamilyFriendly": true, "displayUrl": "https://aihub.org/2021/08/13/a-unifying-game-theoretic-framework-for-imitation-<b>learning</b>", "snippet": "Online: We repurpose the <b>replay</b> <b>buffer</b> of an off-policy RL algorithm as the discriminator by assigning negative rewards to actions that don\u2019t directly match the expert. We impute a reward of +1 for expert behavior and -1/k for learner behavior from a past round, where k is the round number. The slow-moving append-only <b>replay</b> <b>buffer</b> implements a no-regret algorithm against a policy that best-responds via RL at each round. We term this approach Adversarial Reward-moment IL, or", "dateLastCrawled": "2022-01-30T20:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Crystal Clear <b>Reinforcement</b> <b>Learning</b> | by Baijayanta Roy | Towards Data ...", "url": "https://towardsdatascience.com/crystal-clear-reinforcement-learning-7e6c1541365e", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/crystal-clear-<b>reinforcement</b>-<b>learning</b>-7e6c1541365e", "snippet": "Experience <b>replay</b>: We run the agent and store say the last 100 thousand transitions (or video frames) into a <b>buffer</b> and sample a mini-batch of samples of size 512 from this <b>buffer</b> to train the deep network. As we randomly sample from the <b>replay</b> <b>buffer</b>, the data is more independent of each other and closer to i.i.d (independent and identically distributed). This makes raining stable.", "dateLastCrawled": "2022-01-29T12:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Deep imitation reinforcement <b>learning</b> for self\u2010driving by vision - Zou ...", "url": "https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/cit2.12025", "isFamilyFriendly": true, "displayUrl": "https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/cit2.12025", "snippet": "Equation allows the <b>car</b> <b>to drive</b> in the center of the lane by the angle \u03b8 ... Initialize <b>replay</b> <b>buffer</b> R. for episode = 1, M do. Initialize a random process N for action exploration. Receive initial observation state s 1. Get the low-dimensional characteristics s 1 \u2032 \u2190 P (s 1 | \u03b8 P) For t = 1, T do. Select action a t = \u03bc (s t \u2032 | \u03b8 \u03bc) + N t according to the current policy and exploration noise. Execute action a t and observe reward r t and get new characteristic s t + 1 \u2032 \u2190 P ...", "dateLastCrawled": "2022-01-30T12:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "How can we make the <b>training loss decrease in reinforcement learning</b>", "url": "https://www.researchgate.net/post/How-can-we-make-the-training-loss-decrease-in-reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/post/How-can-we-make-the-training-loss-decrease-in...", "snippet": "The goal is <b>to drive</b> at a desired speed without crashing into other cars The state contains the velocities and positions of the agent&#39;s <b>car</b> and the surrounding cars", "dateLastCrawled": "2022-01-22T23:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Exploring Data Aggregation in Policy Learning</b> for Vision-Based Urban ...", "url": "https://openaccess.thecvf.com/content_CVPR_2020/papers/Prakash_Exploring_Data_Aggregation_in_Policy_Learning_for_Vision-Based_Urban_Autonomous_CVPR_2020_paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://openaccess.thecvf.com/content_CVPR_2020/papers/Prakash_Exploring_Data...", "snippet": "vision-based policy <b>learning</b> within a training environment, e.g., <b>learning</b> <b>to drive</b> in a speci\ufb01c simulation condition. However, as on-policy data is sequentially sampled and added in an iterative manner, the policy can specialize and over\ufb01t to the training conditions. For real-world ap-plications, it is useful for the learned policy to general-", "dateLastCrawled": "2022-01-31T12:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Deep Q Learning and Deep Q Networks</b> | AI Summer", "url": "https://theaisummer.com/Deep_Q_Learning/", "isFamilyFriendly": true, "displayUrl": "https://theaisummer.com/Deep_Q_<b>Learning</b>", "snippet": "The goal is to make <b>a car</b> <b>drive</b> up a hill. The <b>car</b>&#39;s engine is not strong enough to climb the hill in a single pass. Therefore, the only way to succeed is <b>to drive</b> back and forth to build up momentum. I will explain more about Deep Q Networks alongside with the code. First we should build our Agent as a Neural Network with 3 Dense layers and we are goint to train it using Adam optimization. class DQNAgent: def __init__ (self, state_size, action_size): self. state_size = state_size. self ...", "dateLastCrawled": "2022-01-29T04:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Reinforcement <b>Learning</b>: A Deep Dive | Toptal", "url": "https://www.toptal.com/machine-learning/deep-dive-into-reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://www.toptal.com/machine-<b>learning</b>/<b>deep-dive-into-reinforcement-learning</b>", "snippet": "The agent perceives an environment through an observation: the <b>car</b>\u2019s X position and velocity. If we want our <b>car</b> <b>to drive</b> on top of the mountain, we define the reward in a convenient way: The agent gets -1 to its reward for every step in which it hasn\u2019t reached the goal. When it reaches the goal, the episode ends. So, in fact, the agent is punished for not being in a position we want it to be. The faster he reaches it, the better for him. The agent\u2019s goal is to maximize the total ...", "dateLastCrawled": "2022-01-30T12:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>GitHub</b> - trishmapow/rf-jam-<b>replay</b>: Jam and <b>replay</b> attack on vehicle ...", "url": "https://github.com/trishmapow/rf-jam-replay", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/trishmapow/rf-jam-<b>replay</b>", "snippet": "The Python <b>replay</b> program was run simultaneously with rpitx, and resulted in the <b>car</b> not locking or unlocking. However, as expected, the signal was captured by the Yard Stick One, and could be replayed at any time to unlock the <b>car</b>. In the interest of responsible disclosure, photographic evidence of the device unlocking the vehicle will not be released as this could compromise the security of many vehicles still in use.", "dateLastCrawled": "2022-01-26T00:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "[D] State of the art Deep-RL still struggles to solve Mountain <b>Car</b> ...", "url": "https://www.reddit.com/r/reinforcementlearning/comments/axp63j/d_state_of_the_art_deeprl_still_struggles_to/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/reinforcement<b>learning</b>/comments/axp63j/d_state_of_the_art...", "snippet": "- First generate diverse trajectory with a memory-based policy (<b>similar</b> to novelty search) called Goal Exploration Process (GEP) - Then transfer the trajectories to the <b>replay</b> <b>buffer</b> of DDPG. My thoughts on the subject: - My work, in the same way as novelty search, requires a behavioral space to describe the trajectory. Diversity is generated ...", "dateLastCrawled": "2021-07-22T04:09:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Deep Reinforcement Learning for Simulated Autonomous Vehicle</b> Control", "url": "http://cs231n.stanford.edu/reports/2016/pdfs/112_Report.pdf", "isFamilyFriendly": true, "displayUrl": "cs231n.stanford.edu/reports/2016/pdfs/112_Report.pdf", "snippet": "everyday camera to enable <b>a car</b> <b>to drive</b> itself. Our main question was whether we could learn simple driving poli-cies from video alone. Current autonomous driving imple- mentations have shied away from the computer vision tech-niques because of a lack of robustness. The inaccuracies with vision-based autonomous driving systems lie mostly in the dif\ufb01culty of compressing the input image into a com-pact but representative feature vector. There are currently two approaches to this problem ...", "dateLastCrawled": "2022-01-25T23:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "How <b>can</b> we make the <b>training loss decrease in reinforcement learning</b>", "url": "https://www.researchgate.net/post/How-can-we-make-the-training-loss-decrease-in-reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/post/How-<b>can</b>-we-make-the-training-loss-decrease-in...", "snippet": "I wish to implement Q-<b>learning</b> for the CartPole RL problem using Neural network function approximator with tensorflow on Open AI Gym. My code has been giving me troubles and I <b>can</b> not debug it. I ...", "dateLastCrawled": "2022-01-22T23:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>Continuous control with deep reinforcement learning</b>", "url": "https://www.researchgate.net/publication/281670459_Continuous_control_with_deep_reinforcement_learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/281670459_Continuous_control_with_deep...", "snippet": "Classic reinforcement <b>learning</b> algorithms generate experiences by the agent&#39;s constant trial and error, which leads to a large number of failure experiences stored in the <b>replay</b> <b>buffer</b>. As a ...", "dateLastCrawled": "2022-02-03T13:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Crystal Clear <b>Reinforcement</b> <b>Learning</b> | by Baijayanta Roy | Towards Data ...", "url": "https://towardsdatascience.com/crystal-clear-reinforcement-learning-7e6c1541365e", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/crystal-clear-<b>reinforcement</b>-<b>learning</b>-7e6c1541365e", "snippet": "For example, winning the chess game at less time or <b>drive</b> the <b>car</b> at average speed but without any collision (<b>car</b> <b>can</b> speed up in empty high-way while <b>drive</b> slowly in the busy road). Reward tells what is good in an immediate sense. For example, in the Atari game, every time step agent gets points or loses points. Discount Factor. Reward now is more valuable than reward in the future. The discount factor, usually denoted as \u03b3, is a factor multiplying the future expected reward and varies on ...", "dateLastCrawled": "2022-01-29T12:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Q <b>Learning and Deep Q Networks</b> - Experfy Insights", "url": "https://resources.experfy.com/ai-ml/q-learning-and-deep-q-networks/", "isFamilyFriendly": true, "displayUrl": "https://resources.experfy.com/ai-ml/q-<b>learning-and-deep-q-networks</b>", "snippet": "As you <b>can</b> see is the exact same process with the Q-table example, with the difference that the next action comes by the DQN prediction and not by the Q-table. As a result, it <b>can</b> be applied to unknown states. That\u2019s the magic of Neural Networks. You just created an agent that learns <b>to drive</b> the <b>car</b> up the hill. Awesome. And what is more ...", "dateLastCrawled": "2022-01-19T16:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Deep <b>Learning</b> in a <b>Nutshell: Reinforcement Learning</b> | NVIDIA Developer Blog", "url": "https://developer.nvidia.com/blog/deep-learning-nutshell-reinforcement-learning/", "isFamilyFriendly": true, "displayUrl": "https://developer.nvidia.com/blog/deep-<b>learning</b>-<b>nutshell-reinforcement-learning</b>", "snippet": "This post is Part 4 of the Deep <b>Learning</b> in a Nutshell series, in which I\u2019ll dive into reinforcement <b>learning</b>, a type of machine <b>learning</b> in which agents take actions in an environment aimed at maximizing their cumulative reward.. Deep <b>Learning</b> in a Nutshell posts offer a high-level overview of essential concepts in deep <b>learning</b>. The posts aim to provide an understanding of each concept rather than its mathematical and theoretical details.", "dateLastCrawled": "2022-01-30T23:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Improving Outcome of Psychosocial Treatments by Enhancing Memory and ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4276345/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC4276345", "snippet": "A great deal of research on memory and <b>learning</b> in cognitive psychology and education research has tried to understand how memory and <b>learning</b> work and <b>can</b> be improved, but these literatures do not tend to consider clinical settings. We begin by defining concepts that are key to this paper and we highlight relevant theory from a review of the cognitive psychology and education research literatures. We then describe specific cognitive support strategies and tools therapists could use to ...", "dateLastCrawled": "2021-12-13T12:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "What is Automatic Processing? | Controlled Thinking &amp; Automatic <b>Thought</b> ...", "url": "https://study.com/learn/lesson/what-is-automatic-thought-processing.html", "isFamilyFriendly": true, "displayUrl": "https://study.com/learn/lesson/what-is-automatic-<b>thought</b>-processing.html", "snippet": "Here are a few <b>thought</b> process examples that are automatic: Riding a bike: At first, it is difficult to ride a bike without falling or crashing into things unless one pays careful attention to ...", "dateLastCrawled": "2022-01-31T05:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Controlled vs <b>Automatic</b> <b>Processing</b>: Definition &amp; Difference - Video ...", "url": "https://study.com/academy/lesson/controlled-vs-automatic-processing-definition-difference.html", "isFamilyFriendly": true, "displayUrl": "https://study.com/academy/lesson/controlled-vs-<b>automatic</b>-<b>processing</b>-definition...", "snippet": "For example, as an experienced bike rider, you may be able to do many bike-riding tasks (i.e. shifting the gears of the bike, braking, and steering) automatically without giving it much <b>thought</b> ...", "dateLastCrawled": "2022-02-03T06:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Ball Tracking with OpenCV - PyImageSearch", "url": "https://www.pyimagesearch.com/2015/09/14/ball-tracking-with-opencv/", "isFamilyFriendly": true, "displayUrl": "https://www.pyimagesearch.com/2015/09/14/ball-tracking-with-opencv", "snippet": "That said, to make the 100th blog post special, I <b>thought</b> I would do something a fun \u2014 ball tracking with ... to make a few basic tasks (like resizing) much easier. If you don\u2019t already have imutils installed on your system, you <b>can</b> grab the source from GitHub or just use pip to install it: $ pip install --upgrade imutils From there, Lines 11-16 handle parsing our command line arguments. The first switch, --video is the (optional) path to our example video file. If this switch is ...", "dateLastCrawled": "2022-01-30T07:48:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Learning to Drive Smoothly in Minutes</b> | by Antonin RAFFIN | Towards ...", "url": "https://towardsdatascience.com/learning-to-drive-smoothly-in-minutes-450a7cdb35f4", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>learning-to-drive-smoothly-in-minutes</b>-450a7cdb35f4", "snippet": "This policy is updated after each episode. One important aspect of the algorithm is that it has a memory, called <b>replay</b> <b>buffer</b>, where its interactions with its environment are recorded and <b>can</b> be \u201creplayed\u201d afterward. So, even when the <b>car</b> does not interact with the world, it <b>can</b> sample experience from this <b>buffer</b> to update its policy.", "dateLastCrawled": "2022-01-31T08:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Safe, <b>efficient, and comfortable velocity control</b> based on ...", "url": "https://www.sciencedirect.com/science/article/pii/S0968090X20305775", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0968090X20305775", "snippet": "Experience <b>replay</b>. A <b>replay</b> <b>buffer</b> was applied to avoid <b>learning</b> from sequentially generated, correlated experience samples. The <b>replay</b> <b>buffer</b> is a finite-sized cache D that stores transitions (s t, a t, r t, s t+1) sampled from the environment. The <b>replay</b> <b>buffer</b> is continually updated by replacing old samples with new ones. At each time step ...", "dateLastCrawled": "2022-01-27T14:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Human-Like Autonomous Car-Following Model with Deep Reinforcement Learning</b>", "url": "https://www.researchgate.net/publication/330132675_Human-Like_Autonomous_Car-Following_Model_with_Deep_Reinforcement_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/330132675_<b>Human-Like_Autonomous_Car-Following</b>...", "snippet": "that th e <b>car</b> <b>can</b> <b>drive</b> independently, and 2) ... <b>replay</b> <b>buffer</b>. The oldest samples ... updating speeds, stable <b>learning</b> <b>can</b> be achieved. This study pr oposed to model <b>car</b>-following behavior with ...", "dateLastCrawled": "2022-01-25T18:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Crystal Clear <b>Reinforcement</b> <b>Learning</b> | by Baijayanta Roy | Towards Data ...", "url": "https://towardsdatascience.com/crystal-clear-reinforcement-learning-7e6c1541365e", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/crystal-clear-<b>reinforcement</b>-<b>learning</b>-7e6c1541365e", "snippet": "Experience <b>replay</b>: We run the agent and store say the last 100 thousand transitions (or video frames) into a <b>buffer</b> and sample a mini-batch of samples of size 512 from this <b>buffer</b> to train the deep network. As we randomly sample from the <b>replay</b> <b>buffer</b>, the data is more independent of each other and closer to i.i.d (independent and identically distributed). This makes raining stable.", "dateLastCrawled": "2022-01-29T12:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Human-like autonomous car-following model with deep reinforcement learning</b>", "url": "https://www.sciencedirect.com/science/article/pii/S0968090X1830055X", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0968090X1830055X", "snippet": "Human-like driving will (1) provide passengers with comfortable riding, and confidence that the <b>car</b> <b>can</b> <b>drive</b> independently, and (2) enable surrounding drivers to better understand and predict autonomous vehicles\u2019 behavior so that they <b>can</b> interact with it naturally (Wei et al., 2010, Wang et al., 2016). To achieve human-like driving, it is useful to introduce a driver model that reproduces individual drivers\u2019 behaviors and trajectories.", "dateLastCrawled": "2022-02-03T19:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Optimizing hyperparameters of deep reinforcement <b>learning</b> for ...", "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0252754", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0252754", "snippet": "The DDPG algorithm eventually receives the <b>replay</b> <b>buffer</b> feature from ... and the results show that the optimized parameters found by the GA <b>can</b> lead to better performance. The <b>learning</b> agent <b>can</b> run faster and <b>can</b> reach the maximum success rate faster. The results show changes when only two parameters (out of the five target parameters) are being optimized. The results from optimizing all five parameters justify that GA found parameters outperformed the original parameters of the DDPG ...", "dateLastCrawled": "2022-01-04T19:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "The A2C algorithm | TensorFlow Reinforcement <b>Learning</b> Quick Start Guide", "url": "https://subscription.packtpub.com/book/big-data-and-business-intelligence/9781789533583/6/ch06lvl1sec52/the-a3c-algorithm", "isFamilyFriendly": true, "displayUrl": "https://subscription.packtpub.com/book/big-data-and-business-intelligence/...", "snippet": "<b>Learning</b> about <b>replay</b> <b>buffer</b>; Getting introduced to the Atari environment; Summary; Questions; Further reading; 4. Double DQN, Dueling Architectures, and Rainbow . Double DQN, Dueling Architectures, and Rainbow; Technical requirements; Understanding Double DQN; Understanding dueling network architectures; Understanding Rainbow networks; Running a Rainbow network on Dopamine; Summary; Questions; Further reading; 5. Deep Deterministic Policy Gradient. Deep Deterministic Policy Gradient ...", "dateLastCrawled": "2021-11-02T15:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Literature Review - Presentation on Relevant work for RL4AD capstone", "url": "https://www.slideshare.net/mgmayank18/literature-review-presentation-on-relevant-work-for-rl4ad-capstone", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/mgmayank18/literature-review-presentation-on-relevant-work...", "snippet": "One of the major contribution is that they use <b>replay</b> <b>buffer</b> to break the correlations between samples. 4) Also, the target changes(non stationary) here which causes instability in <b>learning</b>. Targets networks hold the parameters fixed and avoid the changing targets problem. 5) But still one problem remains. The target is estimated. What if the target is wrong? It leads to maximization bias; 1) Q <b>learning</b> is good for discrete actions. But what if the actions are large/countinous? 2) Policy ...", "dateLastCrawled": "2021-12-25T01:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "tensorflow - <b>DQN - Q-Loss not converging</b> - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/47036246/dqn-q-loss-not-converging", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/47036246", "snippet": "The goal is <b>to drive</b> at a desired speed without crashing into other cars; The state contains the velocities and positions of the agent&#39;s <b>car</b> and the surrounding cars ; Rewards: -100 for crashing into other cars, positive reward according to the absolute difference to the desired speed (+50 if driving at desired speed) I have already adjusted some hyperparameters (network architecture, exploration, <b>learning</b> rate) which gave me some descent results, but still not as good as it should/could be ...", "dateLastCrawled": "2022-01-27T23:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>GitHub</b> - trishmapow/rf-jam-<b>replay</b>: Jam and <b>replay</b> attack on vehicle ...", "url": "https://github.com/trishmapow/rf-jam-replay", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/trishmapow/rf-jam-<b>replay</b>", "snippet": "A PKE system <b>can</b> be theoretically compromised by a jam and <b>replay</b> attack, however the algorithm for the \u201cresponse\u201d code given the \u201cchallenge\u201d from the vehicle must be reverse-engineered. An even simpler \u2018relay\u2019 attack requires an attacker to stand near the vehicle and amplify the LF signals, then transmit this to another attacker who is within close range of the owner\u2019s key fob. The valid response from the key fob <b>can</b> then be transmitted back to the first attacker to unlock the ...", "dateLastCrawled": "2022-01-26T00:21:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "DeepMind\u2019s Idea to Build Neural Networks that can <b>Replay</b> Past ...", "url": "https://medium.com/dataseries/deepminds-idea-to-build-neural-networks-that-can-replay-past-experiences-just-like-humans-do-f9d7721473ac", "isFamilyFriendly": true, "displayUrl": "https://medium.com/dataseries/deepminds-idea-to-build-neural-networks-that-can-<b>replay</b>...", "snippet": "In this case, the <b>replay</b> <b>buffer</b> will <b>replay</b> the sequence e: \u201cwater, vase, dog\u201d in that exact order. Architecturally, our model will use an offline learner agent to <b>replay</b> those experiences.", "dateLastCrawled": "2021-12-09T06:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Towards continual task <b>learning</b> in artificial neural networks: current ...", "url": "https://deepai.org/publication/towards-continual-task-learning-in-artificial-neural-networks-current-approaches-and-insights-from-neuroscience", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/towards-continual-task-<b>learning</b>-in-artificial-neural...", "snippet": "Figure 2: A) Schematic of the <b>analogy</b> between synaptic consolidation (left) and the regularisation of EWC (right), ... including a straightforward experience <b>replay</b> <b>buffer</b> of all prior events for a reinforcement <b>learning</b> agent (Rolnick et al., 2018). This method, called CLEAR, attempts to address the stability-plasticity tradeoff of sequential task <b>learning</b>, using off-policy <b>learning</b> and <b>replay</b>-based behavioural cloning to enhance stability, while maintaining plasticity via on-policy ...", "dateLastCrawled": "2022-01-29T14:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Recreating Imagination: DeepMind Builds Neural Networks</b> ... - KDnuggets", "url": "https://www.kdnuggets.com/2019/10/recreating-imagination-deepmind-builds-neural-networks-spontaneously-replay-past-experiences.html", "isFamilyFriendly": true, "displayUrl": "https://www.kdnuggets.com/2019/10/<b>recreating-imagination-deepmind-builds-neural</b>...", "snippet": "Most solutions in the space relied on an additional <b>replay</b> <b>buffer</b> that records the experiences learned by the agent and plays them back at specific times. Some architectures choose to <b>replay</b> the experiences randomly while others use a specific preferred order that will optimize the <b>learning</b> experiences of the agent. The way in which experiences are replayed in a reinforcement <b>learning</b> model play a key role in the <b>learning</b> experience of an AI agent. At the moment, two of the most actively ...", "dateLastCrawled": "2022-01-14T20:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>DeepMind Creates AI That Replays Memories Like The Hippocampus</b> - Unite.AI", "url": "https://www.unite.ai/deepmind-creates-ai-that-replays-memories-like-the-hippocampus/", "isFamilyFriendly": true, "displayUrl": "https://www.unite.ai/<b>deepmind-creates-ai-that-replays-memories-like-the-hippocampus</b>", "snippet": "DeepMind added the replaying of experiences to a reinforcement <b>learning</b> algorithm using a <b>replay</b> <b>buffer</b> that would playback memories/recorded experiences to the system at specific times. Some versions of the system had the experiences played back in random orders while other models had pre-selected playback orders. While the researchers experimented with the order of playback for the reinforcement agents, they also experimented with different methods of replaying the experiences themselves ...", "dateLastCrawled": "2022-02-01T15:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "BRAIN LIKE <b>REPLAY</b> <b>FOR CONTINUAL LEARNING WITH ARTIFICIAL NEURAL NETWORKS</b>", "url": "https://baicsworkshop.github.io/pdf/BAICS_8.pdf", "isFamilyFriendly": true, "displayUrl": "https://baicsworkshop.github.io/pdf/BAICS_8.pdf", "snippet": "Published as a workshop paper at \u201cBridging AI and Cognitive Science\u201d (ICLR 2020) BRAIN-LIKE <b>REPLAY</b> <b>FOR CONTINUAL LEARNING WITH ARTIFICIAL NEURAL NETWORKS</b> Gido M. van de Ven 1;2, Hava T. Siegelmann3 &amp; Andreas S. Tolias 4 1 Center for Neuroscience and Arti\ufb01cial Intelligence, Baylor College of Medicine, Houston, US 2 Department of Engineering, University of Cambridge, UK 3 College of Computer and Information Sciences, University of Massachusetts Amherst, US 4 Department of Electrical and ...", "dateLastCrawled": "2022-01-21T19:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "DQN Algorithm: A father-son tale. The Deep Q-Network (DQN ...", "url": "https://medium.com/analytics-vidhya/dqn-algorithm-a-father-son-tale-b4bf6ff1ae2f", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/dqn-algorithm-a-father-son-tale-b4bf6ff1ae2f", "snippet": "The Deep Q-Network (DQN) Reinforcement <b>learning</b> algorithm has a surprisingly simple and real life <b>analogy</b> with which it can be explained. It helps understand the sequence of operations involved by\u2026", "dateLastCrawled": "2022-01-13T23:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Brain-inspired replay for continual learning with</b> artificial neural ...", "url": "https://www.nature.com/articles/s41467-020-17866-2", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41467-020-17866-2", "snippet": "Recent evidence indicates that depending on how a continual <b>learning</b> problem is set up, <b>replay</b> might even be unavoidable 21,22,23,24.Typically, continual <b>learning</b> is studied in a task-incremental ...", "dateLastCrawled": "2022-01-30T01:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "reinforcement <b>learning</b> - Hindsight Experience <b>Replay</b>: what the reward w ...", "url": "https://datascience.stackexchange.com/questions/36872/hindsight-experience-replay-what-the-reward-w-r-t-to-sample-goal-means", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/36872", "snippet": "R : <b>replay</b> <b>buffer</b> All other symbols with a dash indicate that they were sampled in addition to the actual current goal within the current episode. It means (as long as I understand) that for the sampled goals (g&#39;) the reward is now a function of action taken in state given the sampled goal.", "dateLastCrawled": "2022-01-15T00:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "[D] What <b>are some relatively simple problems that current</b> ML methods ...", "url": "https://www.reddit.com/r/MachineLearning/comments/ijtolv/d_what_are_some_relatively_simple_problems_that/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/<b>MachineLearning</b>/comments/ijtolv/d_what_are_some_relatively...", "snippet": "DL in particular is super forgetful, requiring i.i.d. samples to work. Experience <b>replay</b> uses crazy amounts of memory and compute while still forgetting eventually (at the latest when the <b>buffer</b> doesn&#39;t cover everything anymore). (Related) low compute <b>learning</b>. DL is super compute hungry, and is nowhere near the lower bound of needed compute on basically any task. DL generally doesn&#39;t even support branched execution (only some parts of the network used at a time), because that hurts ...", "dateLastCrawled": "2021-03-04T11:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Realizing Continual Learning through Modeling</b> a <b>Learning</b> System as a ...", "url": "https://deepai.org/publication/realizing-continual-learning-through-modeling-a-learning-system-as-a-fiber-bundle", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>realizing-continual-learning-through-modeling</b>-a...", "snippet": "<b>Realizing Continual Learning through Modeling</b> a <b>Learning</b> System as a Fiber Bundle. 02/16/2019 \u2219 by Zhenfeng Cao, et al. \u2219 0 \u2219 share . A human brain is capable of continual <b>learning</b> by nature; however the current mainstream deep neural networks suffer from a phenomenon named catastrophic forgetting (i.e., <b>learning</b> a new set of patterns suddenly and completely would result in fully forgetting what has already been learned). In this paper we propose a generic <b>learning</b> model, which regards ...", "dateLastCrawled": "2021-12-10T00:19:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A <b>review On reinforcement learning: Introduction and applications</b> in ...", "url": "https://www.sciencedirect.com/science/article/pii/S0098135420300557", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0098135420300557", "snippet": "The sub-components of <b>machine</b> <b>learning</b>. 2.5.1. Dynamic programming. Dynamic programming refers to a set of algorithms with the ability to find optimal policies assuming a perfect model is available. DP algorithms are in general not widely used due to their very high computational cost for non-trivial problems. The two most popular methods in DP are policy iteration and value iteration. On a high level, policy iteration searches for the optimal policy by iterating through many policies, \u03c0\u03c0 ...", "dateLastCrawled": "2022-01-14T17:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Accelerating Online Reinforcement <b>Learning</b> with <b>Offline</b> Datasets | DeepAI", "url": "https://deepai.org/publication/accelerating-online-reinforcement-learning-with-offline-datasets", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/accelerating-online-reinforcement-<b>learning</b>-with-<b>offline</b>...", "snippet": "Accelerating Online Reinforcement <b>Learning</b> with <b>Offline</b> Datasets. 06/16/2020 \u2219 by Ashvin Nair, et al. \u2219 berkeley college \u2219 0 \u2219 share . Reinforcement <b>learning</b> provides an appealing formalism for <b>learning</b> control policies from experience. However, the classic active formulation of reinforcement <b>learning</b> necessitates a lengthy active exploration process for each behavior, making it difficult to apply in real-world settings.", "dateLastCrawled": "2021-11-22T12:59:00.0000000Z", "language": "en", "isNavigational": false}], [], [], []], "all_bing_queries": ["+(replay buffer)  is like +(learning to drive a car)", "+(replay buffer) is similar to +(learning to drive a car)", "+(replay buffer) can be thought of as +(learning to drive a car)", "+(replay buffer) can be compared to +(learning to drive a car)", "machine learning +(replay buffer AND analogy)", "machine learning +(\"replay buffer is like\")", "machine learning +(\"replay buffer is similar\")", "machine learning +(\"just as replay buffer\")", "machine learning +(\"replay buffer can be thought of as\")", "machine learning +(\"replay buffer can be compared to\")"]}