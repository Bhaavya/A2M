{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "[2022] What Is <b>Sequence-to-Sequence</b> Keras <b>Learning</b> and How To Perform ...", "url": "https://proxet.com/blog/how-to-perform-sequence-to-sequence-learning-in-keras/", "isFamilyFriendly": true, "displayUrl": "https://proxet.com/blog/how-to-perform-<b>sequence-to-sequence</b>-<b>learning</b>-in-keras", "snippet": "This is exactly how the algorithm of seq2seq <b>learning</b> looks <b>like</b>. Let\u2019s see what applications this model can be used for and the examples of <b>sequence-to-sequence</b> <b>learning</b> in Keras code. What is <b>Sequence-to-Sequence</b> <b>Learning</b>. <b>Sequence to Sequence</b> (also called seq2seq) models is a special class of Recurrent Neural Network architectures that is usually used for machine translation, question answering, development of chatbots, text summarization, and so on. How GPU Acceleration and Sequence ...", "dateLastCrawled": "2022-01-30T02:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Seq2seq and <b>Attention</b> - GitHub Pages", "url": "https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html", "isFamilyFriendly": true, "displayUrl": "https://lena-voita.github.io/nlp_course/seq2seq_and_<b>attention</b>.html", "snippet": "The most popular <b>sequence-to-sequence</b> <b>task</b> is translation: usually, from one natural <b>language</b> to another. In the last couple of years, commercial systems became surprisingly good at machine translation - check out, for example, Google Translate, Yandex Translate, DeepL Translator, Bing Microsoft Translator.Today we will learn about the core part of these systems.", "dateLastCrawled": "2022-02-02T22:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "tensorflow - <b>sequence to sequence</b> <b>learning</b> for <b>language</b> translation ...", "url": "https://stackoverflow.com/questions/46962582/sequence-to-sequence-learning-for-language-translation-what-about-unseen-words", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/46962582", "snippet": "<b>sequence to sequence</b> <b>learning</b> is a powerful mechanism for <b>language</b> translation, especially using it locally in a context specific case. I am following this pytorch tutorial for the <b>task</b>. However, the tutorial did not split the data into training and testing. You might think its not a big deal, just split it up, use one chunk for training and ...", "dateLastCrawled": "2022-01-16T12:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A ten-minute introduction to <b>sequence-to-sequence</b> <b>learning</b> in Keras", "url": "https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html", "isFamilyFriendly": true, "displayUrl": "https://blog.keras.io/a-ten-minute-introduction-to-<b>sequence-to-sequence</b>-<b>learning</b>-in...", "snippet": "<b>Sequence-to-sequence</b> <b>learning</b> (Seq2Seq) is about training models to convert sequences from one domain (e.g. sentences in English) to sequences in another domain (e.g. the same sentences translated to French). &quot;the cat sat on the mat&quot;-&gt; [Seq2Seq model]-&gt; &quot;le chat etait assis sur le tapis&quot; This can be used for machine translation or for free-from question answering (generating a natural <b>language</b> answer given a natural <b>language</b> question) -- in general, it is applicable any time you need to ...", "dateLastCrawled": "2022-01-29T00:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Sequence-to-Sequence</b> <b>Language</b> Grounding of Non-Markovian <b>Task</b> Speci\ufb01cations", "url": "https://nakulgopalan.github.io/docs/sequence-sequence-language.pdf", "isFamilyFriendly": true, "displayUrl": "https://nakulgopalan.github.io/docs/sequence-sequence-<b>language</b>.pdf", "snippet": "neural <b>sequence-to-sequence</b> <b>learning</b> models can successfully ground <b>language</b> to this semantic representation, we also provide analysis that highlights generalization to novel, unseen logical forms as an open problem for this class of model. We evaluate our system within two simulated robot domains as well as on a physical robot, demonstrating accurate <b>language</b> grounding alongside a signi\ufb01cant expansion in the space of interpretable robot behaviors. I. INTRODUCTION The broad spectrum of ...", "dateLastCrawled": "2022-01-27T00:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "paper review: \u201cBART: Denoising <b>Sequence-to-Sequence</b> Pre-training for ...", "url": "https://medium.com/mlearning-ai/paper-summary-bart-denoising-sequence-to-sequence-pre-training-for-natural-language-generation-69e41dfbb7fe", "isFamilyFriendly": true, "displayUrl": "https://medium.com/m<b>learning</b>-ai/paper-summary-bart-denoising-<b>sequence-to-sequence</b>-pre...", "snippet": "This configuration is to show that a pretrained BART model itself as a whole can be utilized by adding the small front encoder for machine translation <b>task</b> on <b>a new</b> <b>language</b>.", "dateLastCrawled": "2022-02-01T16:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "An <b>introduction to sequence-to-sequence learning</b> - Loren Lugosch", "url": "https://lorenlugosch.github.io/posts/2019/02/seq2seq/", "isFamilyFriendly": true, "displayUrl": "https://lorenlugosch.github.io/posts/2019/02/seq2seq", "snippet": "An <b>introduction to sequence-to-sequence learning</b>. Published: February 19, 2019. Many interesting problems in artificial intelligence can be described in the following way: Map a sequence of inputs $\\mathbf{x}$ to the correct sequence of outputs $\\mathbf{y}$. Speech recognition is one example: the goal is to map an audio signal $\\mathbf{x}$ (a sequence of real-valued audio samples) to the correct text transcript $\\mathbf{y}$ (a sequence of letters). Other examples are machine translation ...", "dateLastCrawled": "2022-02-02T00:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Natural <b>Language</b> Processing with Deep <b>Learning</b> CS224N/Ling284", "url": "https://web.stanford.edu/class/cs224n/slides/cs224n-2022-lecture07-nmt.pdf", "isFamilyFriendly": true, "displayUrl": "https://web.stanford.edu/class/cs224n/slides/cs224n-2022-lecture07-nmt.pdf", "snippet": "Natural <b>Language</b> Processing with Deep <b>Learning</b> CS224N/Ling284 Christopher Manning Lecture 7: Machine Translation, <b>Sequence-to-Sequence</b> and Attention. Lecture Plan Today we will: 1.Introduce <b>a new</b> <b>task</b>: Machine Translation [15 mins], which is a major use-case of 2.<b>A new</b> neural architecture: <b>sequence-to-sequence</b>[45 mins],which is improved by 3.<b>A new</b> neural technique: attention [20 mins] \u2022Announcements \u2022Please accept your Azure Lab Assignment (to get GPU)! Today!!! See Ed. \u2022Assignment 3 ...", "dateLastCrawled": "2022-02-01T04:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Learning</b> to Progressively Recognize <b>New</b> Named Entities with Sequence to ...", "url": "https://aclanthology.org/C18-1185.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/C18-1185.pdf", "snippet": "<b>Learning</b> to Progressively Recognize <b>New</b> Named Entities with <b>Sequence to Sequence</b> Models Lingzhen Chen University of Trento Povo, Italy lingzhen.chen@unitn.it Alessandro Moschitti Amazon Manhattan Beach, CA, USA amosch@amazon.com Abstract In this paper, we propose to use a <b>sequence to sequence</b> model for Named Entity Recognition (NER) and we explore the effectiveness of such model in a progressive NER setting \u2013 a Transfer <b>Learning</b> (TL) setting. We train an initial model on source data and ...", "dateLastCrawled": "2022-02-02T18:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Classical Structured Prediction Losses for Sequence to Sequence</b> <b>Learning</b>", "url": "https://aclanthology.org/N18-1033/", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/N18-1033", "snippet": "\ufeff%0 Conference Proceedings %T <b>Classical Structured Prediction Losses for Sequence to Sequence</b> <b>Learning</b> %A Edunov, Sergey %A Ott, Myle %A Auli, Michael %A Grangier, David %A Ranzato, Marc\u2019Aurelio %S Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human <b>Language</b> Technologies, Volume 1 (Long Papers) %D 2018 %8 jun %I Association for Computational Linguistics %C <b>New</b> Orleans, Louisiana %F edunov-etal-2018-classical %X There ...", "dateLastCrawled": "2022-01-19T08:40:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "[2022] What Is <b>Sequence-to-Sequence</b> Keras <b>Learning</b> and How To Perform ...", "url": "https://proxet.com/blog/how-to-perform-sequence-to-sequence-learning-in-keras/", "isFamilyFriendly": true, "displayUrl": "https://proxet.com/blog/how-to-perform-<b>sequence-to-sequence</b>-<b>learning</b>-in-keras", "snippet": "An equally important <b>task</b> for any successful data scientist or analyst is to identify and create <b>new</b> tasks that can be solved analytically. The latter is a very different exercise and does not need a lot of coding experience or mathematical background. All you need to know is what is possible and what is not, using a given tool. \u201d \u2014 Tavish Srivastava, Co-Founder and Chief Strategy Officer of Analytics. Here\u2019s an interesting example of the LSTM model in Keras \u2014 the development of a ...", "dateLastCrawled": "2022-01-30T02:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Multi-<b>task</b> <b>Sequence to Sequence Learning</b> | DeepAI", "url": "https://deepai.org/publication/multi-task-sequence-to-sequence-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/multi-<b>task</b>-<b>sequence-to-sequence-learning</b>", "snippet": "Despite the popularity of multi-<b>task</b> <b>learning</b> and <b>sequence to sequence learning</b>, there has been little work in combining MTL with seq2seq <b>learning</b>. To the best of our knowledge, there is only one recent publication by Dong et al. which applies a seq2seq models for machine translation, where the goal is to translate from one <b>language</b> to multiple languages. In this work, we propose three MTL approaches that complement one another: (a) the one-to-many approach \u2013 for tasks that can have an ...", "dateLastCrawled": "2022-02-02T23:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "paper review: \u201cBART: Denoising <b>Sequence-to-Sequence</b> Pre-training for ...", "url": "https://medium.com/mlearning-ai/paper-summary-bart-denoising-sequence-to-sequence-pre-training-for-natural-language-generation-69e41dfbb7fe", "isFamilyFriendly": true, "displayUrl": "https://medium.com/m<b>learning</b>-ai/paper-summary-bart-denoising-<b>sequence-to-sequence</b>-pre...", "snippet": "This configuration is to show that a pretrained BART model itself as a whole can be utilized by adding the small front encoder for machine translation <b>task</b> on <b>a new</b> <b>language</b>.", "dateLastCrawled": "2022-02-01T16:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Attention mechanism</b> - <b>Sequence to sequence</b> tasks | <b>Coursera</b>", "url": "https://www.coursera.org/lecture/language-processing/attention-mechanism-1nQaG", "isFamilyFriendly": true, "displayUrl": "https://<b>www.coursera.org</b>/lecture/<b>language</b>-processing/<b>attention-mechanism</b>-1nQaG", "snippet": "Nearly any <b>task</b> in NLP can be formulates as a <b>sequence to sequence</b> <b>task</b>: machine translation, summarization, question answering, and many more. In this module we will learn a general encoder-decoder-attention architecture that can be used to solve them. We will cover machine translation in more details and you will see how attention technique resembles word alignment <b>task</b> in traditional pipeline.", "dateLastCrawled": "2022-01-30T19:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A Review of the Neural <b>History of Natural Language Processing</b> - AYLIEN ...", "url": "https://aylien.com/blog/a-review-of-the-recent-history-of-natural-language-processing", "isFamilyFriendly": true, "displayUrl": "https://aylien.com/blog/a-review-of-the-recent-<b>history-of-natural-language-processing</b>", "snippet": "<b>Sequence-to-sequence</b> <b>learning</b> can even be applied to structured prediction tasks common in NLP where the output has a particular structure. For simplicity, the output is linearized as can be seen for constituency parsing in Figure 10 below. Neural networks have demonstrated the ability to directly learn to produce such a linearized output given sufficient amount of training data for constituency parsing (Vinyals et al, 2015), and named entity recognition (Gillick et al., 2016), among others.", "dateLastCrawled": "2022-01-30T00:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Sequence to Sequence</b> Coreference Resolution", "url": "https://aclanthology.org/2020.crac-1.5.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/2020.crac-1.5.pdf", "snippet": "paper we present <b>a new</b> approach to face coreference resolution as a <b>sequence to sequence</b> <b>task</b> based on the Transformer architecture. This approach is simple and universal, compatible with any <b>language</b> or dataset (regardless of singletons) and easier to integrate with current <b>language</b> models architectures. We test it on the ARRAU corpus, where we get 65.6 F1 CoNLL. We see this approach not as a \ufb01nal goal, but a means to pretrain <b>sequence to sequence</b> <b>language</b> models (T5) on coreference ...", "dateLastCrawled": "2021-11-20T14:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "CLIP vs Vision <b>Language</b> Pre-training Vs VisionEncoderDecoder", "url": "https://analyticsindiamag.com/clip-vs-vision-language-pre-training-vs-visionencoderdecoder/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/clip-vs-vision-<b>language</b>-pre-training-vs-visionencoderdecoder", "snippet": "Released in January last year, Contrastive <b>Language</b>\u2013Image Pre-training, or CLIP, is built on a large body of work on zero-shot transfer, natural <b>language</b> supervision, and multimodal <b>learning</b>. OpenAI showed that scaling a simple pre-training <b>task</b> is sufficient to achieve competitive zero-shot performance on a wide range of image classification datasets. This method uses available sources of supervision \u2013 the text paired with images found on the internet. The data is used to proxy training ...", "dateLastCrawled": "2022-02-01T16:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Learning</b> to Progressively Recognize <b>New</b> Named Entities with Sequence to ...", "url": "https://aclanthology.org/C18-1185.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/C18-1185.pdf", "snippet": "<b>Learning</b> to Progressively Recognize <b>New</b> Named Entities with <b>Sequence to Sequence</b> Models Lingzhen Chen University of Trento Povo, Italy lingzhen.chen@unitn.it Alessandro Moschitti Amazon Manhattan Beach, CA, USA amosch@amazon.com Abstract In this paper, we propose to use a <b>sequence to sequence</b> model for Named Entity Recognition (NER) and we explore the effectiveness of such model in a progressive NER setting \u2013 a Transfer <b>Learning</b> (TL) setting. We train an initial model on source data and ...", "dateLastCrawled": "2022-02-02T18:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "AI Quantification of <b>Language</b> Puzzle to <b>Language</b> <b>Learning</b> ...", "url": "http://www.cs.sjsu.edu/faculty/pollett/masters/Semesters/Fall19/harita/Project_report.pdf", "isFamilyFriendly": true, "displayUrl": "www.cs.sjsu.edu/faculty/pollett/masters/Semesters/Fall19/harita/Project_report.pdf", "snippet": "quantifying the effectiveness of these games in <b>learning</b> <b>a new</b> <b>language</b>. Secondarily my goal for this project is to measure the effectiveness of exercises for transfer <b>learning</b> in machine translation. As part of CS297, I worked on projects involving different artificial intelligence topics. Mainly the focus was on different types of neural networks and their implementations. <b>Language</b> modeling and <b>sequence-to-sequence</b> <b>learning</b> were topics providing possible insight into the end goal of the ...", "dateLastCrawled": "2021-11-13T22:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Convolutional Sequence to Sequence Learning</b>", "url": "http://proceedings.mlr.press/v70/gehring17a/gehring17a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v70/gehring17a/gehring17a.pdf", "snippet": "English-Romanian translation we achieve <b>a new</b> state of the art, outperforming the previous best result by 1.9 BLEU. On WMT\u201914 English-German we outperform the strong LSTM setup of Wu et al. (2016)by0.5BLEUandonWMT\u201914 English-French we outperform the likelihood trained system of Wu et al. (2016)by1.6BLEU.Furthermore,ourmodelcan translate unseen sentences at an order of magnitude faster speed than Wu et al. (2016)onGPUandCPUhardware(\u00a74,\u00a75). 2. Recurrent <b>Sequence to Sequence</b> <b>Learning</b> ...", "dateLastCrawled": "2022-02-02T20:36:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Multi-task Sequence to Sequence Learning</b> | DeepMind", "url": "https://deepmind.com/research/publications/2019/multi-task-sequence-sequence-learning", "isFamilyFriendly": true, "displayUrl": "https://deepmind.com/research/publications/2019/multi-<b>task</b>-sequence-sequence-<b>learning</b>", "snippet": "<b>Sequence to sequence learning</b> has recently emerged as <b>a new</b> paradigm in supervised <b>learning</b>. To date, most of its applications focused on only one <b>task</b> and not much work explored this framework for multiple tasks. This paper examines three settings to <b>multi-task sequence to sequence learning</b>: (a) the one-to-many setting - where the encoder is shared between several tasks such as machine translation and syntactic parsing, (b) the many-to-one setting - useful when only the decoder <b>can</b> be ...", "dateLastCrawled": "2021-12-07T23:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Seq2seq and <b>Attention</b> - GitHub Pages", "url": "https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html", "isFamilyFriendly": true, "displayUrl": "https://lena-voita.github.io/nlp_course/seq2seq_and_<b>attention</b>.html", "snippet": "The most popular <b>sequence-to-sequence</b> <b>task</b> is translation: usually, from one natural <b>language</b> to another. In the last couple of years, commercial systems became surprisingly good at machine translation - check out, for example, Google Translate, Yandex Translate, DeepL Translator, Bing Microsoft Translator.Today we will learn about the core part of these systems.", "dateLastCrawled": "2022-02-02T22:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "MIT Presents <b>New</b> Approach for <b>Sequence-to-Sequence</b> <b>Learning</b> with Latent ...", "url": "https://medium.com/syncedreview/mit-presents-new-approach-for-sequence-to-sequence-learning-with-latent-neural-grammars-60cd5dfe8f32", "isFamilyFriendly": true, "displayUrl": "https://medium.com/syncedreview/mit-presents-<b>new</b>-approach-for-<b>sequence-to-sequence</b>...", "snippet": "<b>Sequence to sequence</b> modelling (seq2seq) with neural networks has become the de facto standard for sequence prediction tasks such as those found in <b>language</b> modelling and machine translation.", "dateLastCrawled": "2021-11-21T23:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "[1511.06114] Multi-<b>task</b> <b>Sequence to Sequence Learning</b>", "url": "https://arxiv.org/abs/1511.06114", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/abs/1511.06114", "snippet": "<b>Sequence to sequence learning</b> has recently emerged as <b>a new</b> paradigm in supervised <b>learning</b>. To date, most of its applications focused on only one <b>task</b> and not much work explored this framework for multiple tasks. This paper examines three multi-<b>task</b> <b>learning</b> (MTL) settings for <b>sequence to sequence</b> models: (a) the oneto-many setting - where the encoder is shared between several tasks such as machine translation and syntactic parsing, (b) the many-to-one setting - useful when only the decoder ...", "dateLastCrawled": "2021-07-27T17:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Sequence to Sequence Learning with Neural Networks</b>", "url": "https://wandb.ai/authors/seq2seq/reports/Sequence-to-Sequence-Learning-with-Neural-Networks--Vmlldzo0Mzg0MTI", "isFamilyFriendly": true, "displayUrl": "https://wandb.ai/authors/seq2seq/reports/<b>Sequence-to-Sequence-Learning-with</b>-Neural...", "snippet": "Introduction. In the age of attention and transformers, I <b>thought</b> writing a simple report on <b>sequence to sequence</b> modelling thinking it would be a good starting point for a lot of people. In this article I will try declassifying the paper <b>Sequence to Sequence Learning with Neural Networks</b> by Ilya Sutskever et. al. Here in this paper, the authors have presented an end to end <b>learning</b> system that helps in translating one <b>language</b> into another.", "dateLastCrawled": "2022-01-13T19:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Making Predictions with Sequences - Machine <b>Learning</b> Mastery", "url": "https://machinelearningmastery.com/sequence-prediction/", "isFamilyFriendly": true, "displayUrl": "https://machine<b>learning</b>mastery.com/sequence-prediction", "snippet": "<b>Sequence to Sequence</b> Prediction; Sequence. Often we deal with sets in applied machine <b>learning</b> such as a train or test sets of samples. Each sample in the set <b>can</b> <b>be thought</b> of as an observation from the domain. In a set, the order of the observations is not important. A sequence is different. The sequence imposes an explicit order on the ...", "dateLastCrawled": "2022-02-02T04:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "MULTI <b>TASK</b> <b>SEQUENCE TO SEQUENCE</b> <b>LEARNING</b>", "url": "https://nlp.stanford.edu/pubs/luong2016iclr_multi.pdf", "isFamilyFriendly": true, "displayUrl": "https://nlp.stanford.edu/pubs/luong2016iclr_multi.pdf", "snippet": "less in terms of perplexities but more on BLEU scores comparedto skip-<b>thought</b>. 1 INTRODUCTION Multi-<b>task</b> <b>learning</b> (MTL) is an important machine <b>learning</b> paradigm that aims at improving the generalization performance of a <b>task</b> using other related tasks. Such framework has been widelystudiedbyThrun(1996);Caruana(1997);Evgeniou&amp; Pontil(2004);Ando &amp; Zhang(2005); Argyriouet al. (2007); Kumar &amp; III (2012), among many others. In the context of deep neural net-works, MTL has been applied ...", "dateLastCrawled": "2021-11-23T15:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Differences between Autoregressive, Autoencoding and Sequence</b>-to ...", "url": "https://www.machinecurve.com/index.php/2020/12/29/differences-between-autoregressive-autoencoding-and-sequence-to-sequence-models-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.machinecurve.com/index.php/2020/12/29/differences-between-autoregressive...", "snippet": "<b>Sequence to sequence</b> <b>learning</b> has been successful in many tasks such as machine translation, speech recognition ... An autoregressive model <b>can</b> therefore be seen as a model that utilizes its previous predictions for generating <b>new</b> ones. In doing so, it <b>can</b> continue infinitely, or \u2013 in the case of NLP models \u2013 until a stop signal is predicted. Autoregressive Transformers. The GPT architecture (based on Radford et al., 2018) After studying the original Transformer proposed by Vaswani et al ...", "dateLastCrawled": "2022-02-02T01:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Understanding <b>Encoder</b>-Decoder <b>Sequence to Sequence</b> Model | by Simeon ...", "url": "https://towardsdatascience.com/understanding-encoder-decoder-sequence-to-sequence-model-679e04af4346", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-<b>encoder</b>-decoder-<b>sequence-to-sequence</b>...", "snippet": "A <b>sequence to sequence</b> model lies behind numerous systems which you face on a daily basis. For instance, seq2seq model powers applications like Google Translate, voice-enabled devices and online chatbots. Generally speaking, these applications are composed of: Machine translation \u2014 a 2016 paper from Google shows how the seq2seq model\u2019s translation quality \u201capproaches or surpasses all currently published results\u201d. Speech recognition \u2014 another Google paper compares the existing ...", "dateLastCrawled": "2022-02-02T00:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "[1511.06114v1] <b>Multi-task Sequence to Sequence Learning</b>", "url": "https://arxiv.org/abs/1511.06114v1", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/abs/1511.06114v1", "snippet": "Abstract: <b>Sequence to sequence learning</b> has recently emerged as <b>a new</b> paradigm in supervised <b>learning</b>. To date, most of its applications focused on only one <b>task</b> and not much work explored this framework for multiple tasks. This paper examines three settings to <b>multi-task sequence to sequence learning</b>: (a) the one-to-many setting - where the encoder is shared between several tasks such as machine translation and syntactic parsing, (b) the many-to-one setting - useful when only the decoder ...", "dateLastCrawled": "2022-01-24T08:28:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Multi-<b>task</b> <b>Sequence to Sequence Learning</b> | DeepAI", "url": "https://deepai.org/publication/multi-task-sequence-to-sequence-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/multi-<b>task</b>-<b>sequence-to-sequence-learning</b>", "snippet": "Despite the popularity of multi-<b>task</b> <b>learning</b> and <b>sequence to sequence learning</b>, there has been little work in combining MTL with seq2seq <b>learning</b>. To the best of our knowledge, there is only one recent publication by Dong et al. which applies a seq2seq models for machine translation, where the goal is to translate from one <b>language</b> to multiple languages. In this work, we propose three MTL approaches that complement one another: (a) the one-to-many approach \u2013 for tasks that <b>can</b> have an ...", "dateLastCrawled": "2022-02-02T23:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "MIT Presents <b>New</b> Approach for <b>Sequence-to-Sequence</b> <b>Learning</b> with Latent ...", "url": "https://medium.com/syncedreview/mit-presents-new-approach-for-sequence-to-sequence-learning-with-latent-neural-grammars-60cd5dfe8f32", "isFamilyFriendly": true, "displayUrl": "https://medium.com/syncedreview/mit-presents-<b>new</b>-approach-for-<b>sequence-to-sequence</b>...", "snippet": "<b>A new</b> MIT CSAIL paper presents an alternative, hierarchical approach to <b>sequence-to-sequence</b> <b>learning</b> with quasi-synchronous grammars to improve <b>sequence-to-sequence</b> models\u2019 compositional ...", "dateLastCrawled": "2021-11-21T23:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Multi-task Sequence to Sequence Learning</b> | DeepMind", "url": "https://deepmind.com/research/publications/2019/multi-task-sequence-sequence-learning", "isFamilyFriendly": true, "displayUrl": "https://deepmind.com/research/publications/2019/multi-<b>task</b>-sequence-sequence-<b>learning</b>", "snippet": "<b>Sequence to sequence learning</b> has recently emerged as <b>a new</b> paradigm in supervised <b>learning</b>. To date, most of its applications focused on only one <b>task</b> and not much work explored this framework for multiple tasks. This paper examines three settings to <b>multi-task sequence to sequence learning</b>: (a) the one-to-many setting - where the encoder is shared between several tasks such as machine translation and syntactic parsing, (b) the many-to-one setting - useful when only the decoder <b>can</b> be ...", "dateLastCrawled": "2021-12-07T23:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "paper review: \u201cBART: Denoising <b>Sequence-to-Sequence</b> Pre-training for ...", "url": "https://medium.com/mlearning-ai/paper-summary-bart-denoising-sequence-to-sequence-pre-training-for-natural-language-generation-69e41dfbb7fe", "isFamilyFriendly": true, "displayUrl": "https://medium.com/m<b>learning</b>-ai/paper-summary-bart-denoising-<b>sequence-to-sequence</b>-pre...", "snippet": "This configuration is to show that a pretrained BART model itself as a whole <b>can</b> be utilized by adding the small front encoder for machine translation <b>task</b> on <b>a new</b> <b>language</b>.", "dateLastCrawled": "2022-02-01T16:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Compositional generalization through meta sequence-to-sequence</b> <b>learning</b>", "url": "https://cims.nyu.edu/~brenden/papers/Lake2019NeurIPS.pdf", "isFamilyFriendly": true, "displayUrl": "https://cims.nyu.edu/~brenden/papers/Lake2019NeurIPS.pdf", "snippet": "<b>sequence-to-sequence</b> <b>learning</b> Brenden M. Lake <b>New</b> York University Facebook AI Reasearch brenden@nyu.edu Abstract People <b>can</b> learn <b>a new</b> concept and use it compositionally, understanding how to \u201cblicket twice\u201d after <b>learning</b> how to \u201cblicket.\u201d In contrast, powerful <b>sequence-to-sequence</b> (seq2seq) neural networks fail such tests of compositionality, especially when composing <b>new</b> concepts together with existing concepts. In this paper, I show how memory-augmented neural networks <b>can</b> be ...", "dateLastCrawled": "2022-01-25T11:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "[1511.06114] Multi-<b>task</b> <b>Sequence to Sequence Learning</b>", "url": "https://arxiv.org/abs/1511.06114", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/abs/1511.06114", "snippet": "This paper examines three multi-<b>task</b> <b>learning</b> (MTL) settings for <b>sequence to sequence</b> models: (a) the oneto-many setting - where the encoder is shared between several tasks such as machine translation and syntactic parsing, (b) the many-to-one setting - useful when only the decoder <b>can</b> be shared, as in the case of translation and image caption generation, and (c) the many-to-many setting - where multiple encoders and decoders are shared, which is the case with unsupervised objectives and ...", "dateLastCrawled": "2021-07-27T17:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Neural <b>Machine Translation</b>: Inner Workings, Seq2Seq, and Transformers ...", "url": "https://towardsdatascience.com/neural-machine-translation-inner-workings-seq2seq-and-transformers-229faff5895b", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/neural-<b>machine-translation</b>-inner-workings-seq2seq-and...", "snippet": "Figure 2: <b>Sequence-to-sequence</b> architecture [1] We <b>can</b> utilize the <b>sequence-to-sequence</b> architecture in many scenarios such as. Summarization: Given a long passage (e.g., news, chapter of a book, an article), generate its summary. <b>Machine Translation</b> (duh!): Given a sentence from a source <b>language</b>, generate the corresponding sentence in another ...", "dateLastCrawled": "2022-01-29T08:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Progress in Neural NLP: Modeling, <b>Learning</b>, and Reasoning", "url": "https://www.sciencedirect.com/science/article/pii/S2095809919304928", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2095809919304928", "snippet": "Many NLP tasks <b>can</b> be formulated as a <b>sequence-to-sequence</b> <b>task</b>, such as MT (i.e., given the source <b>language</b> word sequence, generate the target <b>language</b> word sequence), QA (i.e., given the word sequence of a question, generate the word sequence of an answer), and dialog (i.e., given the word sequence of user input, generate the word sequence of response). 2.2.2. Encoder\u2013decoder framework. Ref. proposed an encoder\u2013decoder framework for <b>sequence-to-sequence</b> modeling. As shown in Fig. 5 ...", "dateLastCrawled": "2022-01-30T04:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "AI Quantification of <b>Language</b> Puzzle to <b>Language</b> <b>Learning</b> ...", "url": "http://www.cs.sjsu.edu/faculty/pollett/masters/Semesters/Fall19/harita/Project_report.pdf", "isFamilyFriendly": true, "displayUrl": "www.cs.sjsu.edu/faculty/pollett/masters/Semesters/Fall19/harita/Project_report.pdf", "snippet": "quantifying the effectiveness of these games in <b>learning</b> <b>a new</b> <b>language</b>. Secondarily my goal for this project is to measure the effectiveness of exercises for transfer <b>learning</b> in machine translation. As part of CS297, I worked on projects involving different artificial intelligence topics. Mainly the focus was on different types of neural networks and their implementations. <b>Language</b> modeling and <b>sequence-to-sequence</b> <b>learning</b> were topics providing possible insight into the end goal of the ...", "dateLastCrawled": "2021-11-13T22:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Convolutional <b>Sequence to Sequence</b> <b>Learning</b>", "url": "https://proceedings.mlr.press/v70/gehring17a/gehring17a.pdf", "isFamilyFriendly": true, "displayUrl": "https://proceedings.mlr.press/v70/gehring17a/gehring17a.pdf", "snippet": "Convolutional <b>Sequence to Sequence</b> <b>Learning</b> Jonas Gehring 1Michael Auli David Grangier Denis Yarats 1Yann N. Dauphin Abstract The prevalent approach to <b>sequence to sequence</b> <b>learning</b> maps an input sequence to a variable length output sequence via recurrent neural networks. We introduce an architecture based entirely on convolutional neural networks. <b>Compared</b> to recurrent models, computations over all elements <b>can</b> be fully parallelized during training to better exploit the GPU hardware and ...", "dateLastCrawled": "2022-02-02T16:43:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "The language of <b>proteins: NLP, machine learning &amp; protein sequences</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S2001037021000945", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2001037021000945", "snippet": "Popular deep-<b>learning</b> architectures are long short-term memory (LSTM) , <b>sequence-to-sequence</b> (seq2seq) and attention . In seq2seq models, a text is transformed using an encoder component, then a separate decoder uses the encoded representation to solve some <b>task</b> (e.g. translating between English and French). Attention models use attention layers (also called attention heads) that allow the network to concentrate on specific tokens in the text", "dateLastCrawled": "2022-02-03T02:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Is the race over for <b>Seq2Seq</b> models? | by Thushan Ganegedara | Towards ...", "url": "https://towardsdatascience.com/is-the-race-over-for-seq2seq-models-adef2b24841c", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/is-the-race-over-for-<b>seq2seq</b>-models-adef2b24841c", "snippet": "This goes for any <b>machine</b> <b>learning</b> <b>task</b>, be it <b>machine</b> translation, dependency parsing or language modelling. Self-attention layer enables to transformer to exactly do that. While processing the word \u201cits\u201d, the model can look at all the other words and decide for itself which words are important to \u201c mix \u201d into the output, so that the transformer can solve the <b>task</b> effectively.", "dateLastCrawled": "2022-02-02T02:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Introduction to Transformers in Machine Learning</b>", "url": "https://www.machinecurve.com/index.php/2020/12/28/introduction-to-transformers-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/.../12/28/<b>introduction-to-transformers-in-machine-learning</b>", "snippet": "When you talk about <b>Machine</b> <b>Learning</b> in Natural Language Processing these days, all you hear is one thing \u2013 Transformers. Models based on this Deep <b>Learning</b> architecture have taken the NLP world by storm since 2017. In fact, they are the go-to approach today, and many of the approaches build on top of the original Transformer, one way or another. Transformers are however not simple. The original Transformer architecture is quite complex and the same is true for many of the spin-off ...", "dateLastCrawled": "2022-02-03T03:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "9.7. <b>Sequence to Sequence</b> <b>Learning</b> \u2014 Dive into Deep <b>Learning</b> 0.17.2 ...", "url": "https://d2l.ai/chapter_recurrent-modern/seq2seq.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_recurrent-modern/seq2seq.html", "snippet": "As we have seen in Section 9.5, in <b>machine</b> translation both the input and output are a variable-length <b>sequence</b>.To address this type of problem, we have designed a general encoder-decoder architecture in Section 9.6.In this section, we will use two RNNs to design the encoder and the decoder of this architecture and apply it to <b>sequence to sequence</b> <b>learning</b> for <b>machine</b> translation [Sutskever et al., 2014] [Cho et al., 2014b].. Following the design principle of the encoder-decoder architecture ...", "dateLastCrawled": "2022-01-26T00:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Geometric deep <b>learning</b> on molecular representations | Nature <b>Machine</b> ...", "url": "https://www.nature.com/articles/s42256-021-00418-8", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s42256-021-00418-8", "snippet": "In <b>analogy</b> to some popular pre-deep <b>learning</b> ... which can be cast as a <b>sequence-to-sequence</b> translation <b>task</b> in which the string representations of the reactants are mapped to those of the ...", "dateLastCrawled": "2022-01-29T04:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Machine Learning Gist</b> \u00b7 GitHub", "url": "https://gist.github.com/sgoyal1012/b30d70d12b6efad88bb285e8e709b161", "isFamilyFriendly": true, "displayUrl": "https://gist.github.com/sgoyal1012/b30d70d12b6efad88bb285e8e709b161", "snippet": "Week 3: <b>Sequence to sequence</b> architectures. <b>Sequence to sequence</b> models Language translation for example; Image captioning, caption an image; Picking the most likely model <b>Machine</b> Transation Model Split into a model encoding the sentence; and then a language model. Calculate the probability of an English sentence conditioned on a French sentence.", "dateLastCrawled": "2022-01-29T03:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Benefits of AI and Deep <b>Learning</b> - <b>Machine</b> <b>Learning</b> Company ...", "url": "https://www.folio3.ai/blog/advantages-of-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://www.folio3.ai/blog/<b>advantages-of-neural-networks</b>", "snippet": "<b>Sequence-To-Sequence</b> models are mainly applied in question answering, <b>machine</b> translations systems, and chatbots. What Are The <b>Advantages of Neural Networks</b> . There are various <b>advantages of neural networks</b>, some of which are discussed below: 1) Store information on the entire network. Just like it happens in traditional programming where information is stored on the network and not on a database. If a few pieces of information disappear from one place, it does not stop the whole network ...", "dateLastCrawled": "2022-02-02T09:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A Gentle Introduction to <b>Long Short-Term Memory</b> Networks by the Experts", "url": "https://machinelearningmastery.com/gentle-introduction-long-short-term-memory-networks-experts/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/gentle-introduction-<b>long-short-term-memory</b>-networks...", "snippet": "<b>Long Short-Term Memory</b> (LSTM) networks are a type of recurrent neural network capable of <b>learning</b> order dependence in sequence prediction problems. This is a behavior required in complex problem domains like <b>machine</b> translation, speech recognition, and more. LSTMs are a complex area of deep <b>learning</b>. It can be hard to get your hands around what LSTMs are, and how terms like bidirectional", "dateLastCrawled": "2022-01-31T03:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Sequence Classification with LSTM Recurrent Neural Networks in Python ...", "url": "https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/sequence-classification-", "snippet": "Sequence Classification with LSTM Recurrent Neural Networks in Python with Keras. Sequence classification is a predictive modeling problem where you have some sequence of inputs over space or time and the <b>task</b> is to predict a category for the sequence. What makes this problem difficult is that the sequences can vary in length, be comprised of a ...", "dateLastCrawled": "2022-02-02T22:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "deep-<b>learning</b>-coursera/Week 1 Quiz - Introduction to deep <b>learning</b>.md ...", "url": "https://github.com/Kulbear/deep-learning-coursera/blob/master/Neural%20Networks%20and%20Deep%20Learning/Week%201%20Quiz%20-%20Introduction%20to%20deep%20learning.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/Kulbear/deep-<b>learning</b>-coursera/blob/master/Neural Networks and Deep...", "snippet": "Week 1 Quiz - Introduction to deep <b>learning</b>. What does the <b>analogy</b> \u201cAI is the new electricity\u201d refer to? AI is powering personal devices in our homes and offices, similar to electricity. Through the \u201csmart grid\u201d, AI is delivering a new wave of electricity. AI runs on computers and is thus powered by electricity, but it is letting computers do things not possible before. Similar to electricity starting about 100 years ago, AI is transforming multiple industries. Note: Andrew ...", "dateLastCrawled": "2022-02-03T05:16:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(sequence-to-sequence task)  is like +(learning a new language)", "+(sequence-to-sequence task) is similar to +(learning a new language)", "+(sequence-to-sequence task) can be thought of as +(learning a new language)", "+(sequence-to-sequence task) can be compared to +(learning a new language)", "machine learning +(sequence-to-sequence task AND analogy)", "machine learning +(\"sequence-to-sequence task is like\")", "machine learning +(\"sequence-to-sequence task is similar\")", "machine learning +(\"just as sequence-to-sequence task\")", "machine learning +(\"sequence-to-sequence task can be thought of as\")", "machine learning +(\"sequence-to-sequence task can be compared to\")"]}