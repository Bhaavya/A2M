{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A Tutorial on <b>Fairness</b> in Machine Learning | by Ziyuan Zhong | Towards ...", "url": "https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-tutorial-on-<b>fairness</b>-in-machine-learning-3ff8ba1040cb", "snippet": "These laws typically evaluate the <b>fairness</b> of a decision making <b>process</b> using two distinct notions (Barocas and Selbst, 2016): disparate treatment and disparate impact. A decision making <b>process</b> suffers from disparate treatment if its decisions are (partly) based on the subject\u2019s sensitive attribute, and it has disparate impact if its outcomes disproportionately hurt (or benefit) people with certain sensitive attribute values (e.g., females, blacks). These two definitions, however, are too ...", "dateLastCrawled": "2022-02-01T18:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "[PDF] <b>Fairness</b>-Aware Learning with <b>Restriction</b> of Universal Dependency ...", "url": "https://www.semanticscholar.org/paper/Fairness-Aware-Learning-with-Restriction-of-using-Fukuchi-Sakuma/4084534766fd743eb105b218ce937a9a09426e0b", "isFamilyFriendly": true, "displayUrl": "https://www.semanticscholar.org/paper/<b>Fairness</b>-Aware-Learning-with-<b>Restriction</b>-of...", "snippet": "The existing <b>fairness</b>-aware learning algorithms employ different dependency measures, and each algorithm is specifically designed for a particular one. Such diversity makes it difficult to theoretically analyze and compare them. In this paper, we propose a general framework for <b>fairness</b>-aware learning that uses f-divergences and that covers most of the dependency measures employed in the existing methods. We introduce a way to estimate the f-divergences that allows us to give a unified ...", "dateLastCrawled": "2021-11-14T12:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "CONFAIR: Configurable and Interpretable Algorithmic <b>Fairness</b> | DeepAI", "url": "https://deepai.org/publication/confair-configurable-and-interpretable-algorithmic-fairness", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/confair-configurable-and-interpretable-algorithmic-<b>fairness</b>", "snippet": "The different definitions of statistical <b>fairness</b> lead to the evaluation of an algorithm\u2019s performance in fundamentally different ways. For instance, the disparate impact measures the ratio of true positive rates across groups while equal opportunity measures the difference between the true positive rates and false positive rates across groups with the <b>restriction</b> that they are less than a specified threshold. It has been shown that the different <b>fairness</b> metrics are often incompatible ...", "dateLastCrawled": "2022-01-22T16:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Differentially Private Fair Learning", "url": "https://par.nsf.gov/servlets/purl/10105700", "isFamilyFriendly": true, "displayUrl": "https://par.nsf.gov/servlets/purl/10105700", "snippet": "and the <b>restriction</b> on the collection or use of protected at-tributes on the other \u2014 present technical conundrums, since the most straightforward methods for ensuring <b>fairness</b> gen-erally require knowing or using the attribute being protected. It seems dif\ufb01cult to guarantee that a trained model is not discriminating against (say) a racial group if we cannot even identify members of that group in the data. A recent line of work (Veale &amp; Binns ,2017; Kilbertus et al. 2018) made these cogent ...", "dateLastCrawled": "2021-12-29T18:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Freedom of Testation &amp; <b>the Fairness of the Methods of its Curtailment</b> ...", "url": "https://www.theunitutor.com/critical-analysis-freedom-testation-fairness-methods-curtailment/", "isFamilyFriendly": true, "displayUrl": "https://www.theunitutor.com/critical-analysis-freedom-testation-<b>fairness</b>-methods...", "snippet": "Customary obligation is an unreasonable <b>restriction</b>, firstly because it is vague, secondly, because the intent of supporting one\u2019s close relative may be better served through other means or method <b>like</b> a legislative enactment created for such purpose and thirdly because the method itself offers not the least restrictive means of the curtailment of the testator\u2019s freedom.", "dateLastCrawled": "2022-01-22T10:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Legal, Ethical and Contractual constraints in the Media</b> Industry", "url": "https://www.slideshare.net/Mattwattsmedia/legal-ethical-and-contractual-constraints-in-the-media-industry-13456879", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/Mattwattsmedia/<b>legal-ethical-and-contractual-constraints-in</b>...", "snippet": "<b>Fairness</b> \u201cOur output will be based on <b>fairness</b>, openness, honesty and straight dealing. Contributors and audiences will be treated with respect\u201d The BBC look to be open, honest, straightforward and fair in all dealings with contributors and audiences unless there is a clear public interest in doing otherwise, or they need to consider important issues such as legal matters, safety, or confidentiality. The BBC employ different tactics by giving contributors and audiences the right to ...", "dateLastCrawled": "2022-01-27T09:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Differentially Private Fair Learning", "url": "http://proceedings.mlr.press/v97/jagielski19a/jagielski19a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v97/jagielski19a/jagielski19a.pdf", "snippet": "and the <b>restriction</b> on the collection or use of protected at-tributes on the other \u2014 present technical conundrums, since the most straightforward methods for ensuring <b>fairness</b> gen-erally require knowing or using the attribute being protected. It seems dif\ufb01cult to guarantee that a trained model is not discriminating against (say) a racial group if we cannot even identify members of that group in the data. A recent line of work (Veale &amp; Binns ,2017;Kilbertus et al. 2018) made these cogent ...", "dateLastCrawled": "2022-01-22T22:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "The Landscape and Gaps in Open Source <b>Fairness</b> Toolkits", "url": "https://dl.acm.org/doi/fullHtml/10.1145/3411764.3445261", "isFamilyFriendly": true, "displayUrl": "https://dl.acm.org/doi/fullHtml/10.1145/3411764.3445261", "snippet": "Augment model-building <b>process</b> with <b>fairness</b> in mind (e.g. debiasing or <b>constraint</b> during <b>training</b>). The group identified some initial high-level gaps in the open source tool landscape, which we futher validate with in-depth, semi-structured interviews. In the focus group, only between one and five people assessed each toolkit, limiting external validity due to the subjective nature of the scoring, hence using the interviews to validate the toolkit assessments. In rating the toolkits ...", "dateLastCrawled": "2022-01-16T20:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Maintaining Discrimination and <b>Fairness</b> in Class Incremental Learning ...", "url": "https://deepai.org/publication/maintaining-discrimination-and-fairness-in-class-incremental-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/maintaining-discrimination-and-<b>fairness</b>-in-class...", "snippet": "Then, to further maintain the <b>fairness</b> between old classes and new classes, we propose Weight Aligning (WA) that corrects the biased weights in the FC layer after normal <b>training</b> <b>process</b>. Unlike previous work, WA does not require any extra parameters or a validation set in advance, as it utilizes the information provided by the biased weights themselves. The proposed method is evaluated on", "dateLastCrawled": "2021-11-28T15:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(PDF) SaDe: Learning Models that Provably Satisfy Domain Constraints", "url": "https://www.researchgate.net/publication/356710886_SaDe_Learning_Models_that_Provably_Satisfy_Domain_Constraints", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/356710886_SaDe_Learning_Models_that_Provably...", "snippet": "ther as substitutes for <b>training</b> data, <b>like</b> in (Diligenti, Gori, and Sacca 2017), or to provide structured knowledge to the model prediction to learn more \u2018coherent\u2019 solutions, <b>like</b> the", "dateLastCrawled": "2022-01-12T09:38:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A Tutorial on <b>Fairness</b> in Machine Learning | by Ziyuan Zhong | Towards ...", "url": "https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-tutorial-on-<b>fairness</b>-in-machine-learning-3ff8ba1040cb", "snippet": "These laws typically evaluate the <b>fairness</b> of a decision making <b>process</b> using two distinct notions (Barocas and Selbst, 2016): disparate treatment and disparate impact. A decision making <b>process</b> suffers from disparate treatment if its decisions are (partly) based on the subject\u2019s sensitive attribute, and it has disparate impact if its outcomes disproportionately hurt (or benefit) people with certain sensitive attribute values (e.g., females, blacks). These two definitions, however, are too ...", "dateLastCrawled": "2022-02-01T18:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "[PDF] <b>Fairness</b>-Aware Learning with <b>Restriction</b> of Universal Dependency ...", "url": "https://www.semanticscholar.org/paper/Fairness-Aware-Learning-with-Restriction-of-using-Fukuchi-Sakuma/4084534766fd743eb105b218ce937a9a09426e0b", "isFamilyFriendly": true, "displayUrl": "https://www.semanticscholar.org/paper/<b>Fairness</b>-Aware-Learning-with-<b>Restriction</b>-of...", "snippet": "The existing <b>fairness</b>-aware learning algorithms employ different dependency measures, and each algorithm is specifically designed for a particular one. Such diversity makes it difficult to theoretically analyze and compare them. In this paper, we propose a general framework for <b>fairness</b>-aware learning that uses f-divergences and that covers most of the dependency measures employed in the existing methods. We introduce a way to estimate the f-divergences that allows us to give a unified ...", "dateLastCrawled": "2021-11-14T12:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Freedom of Testation &amp; <b>the Fairness of the Methods of its Curtailment</b> ...", "url": "https://www.theunitutor.com/critical-analysis-freedom-testation-fairness-methods-curtailment/", "isFamilyFriendly": true, "displayUrl": "https://www.theunitutor.com/critical-analysis-freedom-testation-<b>fairness</b>-methods...", "snippet": "<b>Fairness</b> of the limitation may be judged in more ways than one. One of such measures is a scrutiny based on the degree of <b>restriction</b> imposed, the soundness of the legislative intent sought to be achieved and the reasonableness of the means used to achieve the legislative purpose. SCRUTINY OF THE <b>RESTRICTION</b>: FORCED HEIRSHIP", "dateLastCrawled": "2022-01-22T10:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Fairness-aware Classification: Criterion, Convexity, and Bounds</b> | DeepAI", "url": "https://deepai.org/publication/fairness-aware-classification-criterion-convexity-and-bounds", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>fairness-aware-classification-criterion-convexity-and</b>...", "snippet": "In this paper, we propose a general framework for <b>fairness</b>-aware classification which addresses all above issues. The framework formulates various commonly-used <b>fairness</b> metrics (risk difference, risk ratio, equal odds, etc.) as convex constraints that are then directly incorporated into classic classification models.Within the framework, we present a <b>constraint</b>-free criterion <b>on the training</b> data which ensures that any classifier learned from the data will be fair.", "dateLastCrawled": "2021-12-02T00:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Fair and Interpretable Decision Rules for Binary Classi\ufb01cation</b>", "url": "http://www.opt-ml.org/papers/2020/paper_41.pdf", "isFamilyFriendly": true, "displayUrl": "www.opt-ml.org/papers/2020/paper_41.pdf", "snippet": "k 2 for all k2K:<b>Constraint</b> (6) provides the bound on complexity of the \ufb01nal rule set. Finally, constraints (8) and (9) bound the maximum allowed unfairness, denoted by in (2) by a speci\ufb01ed constant 1 0. If 1 is chosen to be 0, then the <b>fairness</b> <b>constraint</b> is imposed strictly. Depending on the application, 1 can also be", "dateLastCrawled": "2021-09-19T19:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Maintaining Discrimination and <b>Fairness</b> in Class Incremental Learning ...", "url": "https://deepai.org/publication/maintaining-discrimination-and-fairness-in-class-incremental-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/maintaining-discrimination-and-<b>fairness</b>-in-class...", "snippet": "Then, to further maintain the <b>fairness</b> between old classes and new classes, we propose Weight Aligning (WA) that corrects the biased weights in the FC layer after normal <b>training</b> <b>process</b>. Unlike previous work, WA does not require any extra parameters or a validation set in advance, as it utilizes the information provided by the biased weights themselves. The proposed method is evaluated on", "dateLastCrawled": "2021-11-28T15:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) On the <b>Fairness</b> of Causal Algorithmic Recourse - researchgate.net", "url": "https://www.researchgate.net/publication/344639341_On_the_Fairness_of_Causal_Algorithmic_Recourse", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/344639341_On_the_<b>Fairness</b>_of_Causal...", "snippet": "Content may be subject to copyright. On the <b>Fairness</b> of Causal Algorithmic Recourse. Julius von K\u00fcgelgen 1,2 Umang Bhatt 2Amir-Hossein Karimi 1, 3. Isabel V alera 1,4Adrian Weller 2,5Bernhard ...", "dateLastCrawled": "2021-09-18T18:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "The Landscape and Gaps in Open Source <b>Fairness</b> Toolkits", "url": "https://dl.acm.org/doi/fullHtml/10.1145/3411764.3445261", "isFamilyFriendly": true, "displayUrl": "https://dl.acm.org/doi/fullHtml/10.1145/3411764.3445261", "snippet": "Augment model-building <b>process</b> with <b>fairness</b> in mind (e.g. debiasing or <b>constraint</b> during <b>training</b>). The group identified some initial high-level gaps in the open source tool landscape, which we futher validate with in-depth, semi-structured interviews. In the focus group, only between one and five people assessed each toolkit, limiting external validity due to the subjective nature of the scoring, hence using the interviews to validate the toolkit assessments. In rating the toolkits ...", "dateLastCrawled": "2022-01-16T20:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) SaDe: Learning Models that Provably Satisfy Domain Constraints", "url": "https://www.researchgate.net/publication/356710886_SaDe_Learning_Models_that_Provably_Satisfy_Domain_Constraints", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/356710886_SaDe_Learning_Models_that_Provably...", "snippet": "on <b>training</b> data). In <b>constraint</b> satisfaction, the way to rep- resent such constraints is by using universal quanti\ufb01ers (@) over the data domain. These constraints are crucial in many. domains ...", "dateLastCrawled": "2022-01-12T09:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "New submissions for Tue, 14 Dec 21 \u00b7 Issue #194 \u00b7 zoq/arxiv-updates ...", "url": "https://github.com/zoq/arxiv-updates/issues/194", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/zoq/arxiv-updates/issues/194", "snippet": "In the system model, we propose a parallel <b>training</b> <b>process</b> of multiple jobs, and construct a cost model based <b>on the training</b> time and the data <b>fairness</b> of various devices during the <b>training</b> <b>process</b> of diverse jobs. We propose a reinforcement learning-based method and a Bayesian optimization-based method to schedule devices for multiple jobs while minimizing the cost. We conduct extensive experimentation with multiple jobs and datasets. The experimental results show that our proposed ...", "dateLastCrawled": "2022-01-29T10:05:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Accuracy and <b>Fairness</b> Trade-o s in Machine Learning: A Stochastic Multi ...", "url": "https://engineering.lehigh.edu/sites/engineering.lehigh.edu/files/_DEPARTMENTS/ise/pdf/tech-papers/20/20T_016.pdf", "isFamilyFriendly": true, "displayUrl": "https://engineering.lehigh.edu/sites/engineering.lehigh.edu/files/_DEPARTMENTS/ise/pdf/...", "snippet": "fronts, and by doing so we <b>can</b> handle <b>training</b> data arriving in a streaming way. 1. Introduction Machine learning (ML) plays an increasingly signicant role in data-driven decision making, e.g., credit scoring, col-lege admission, hiring decisions, and criminal justice. As the learning models became more and more sophisticated, concern regarding <b>fairness</b> started receiving more and more attention. In 2014, the Obama Administration&#39;s Big Data Report (Podesta et al.,2014) claimed that ...", "dateLastCrawled": "2022-01-28T20:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "ACCURACY AND <b>FAIRNESS</b> TRADE-OFFS IN MACHINE APPROACH", "url": "https://www.mat.uc.pt/preprints/ps/p2037.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.mat.uc.pt/preprints/ps/p2037.pdf", "snippet": "monly used strategy in fair machine learning is to include <b>fairness</b> as a <b>constraint</b> or a penalization term in the minimization of the prediction loss, which ultimately limits the information given to decision-makers. In this paper, we introduce a new approach to handle <b>fairness</b> by formulating a stochastic multi-objective optimiza-tion problem for which the corresponding Pareto fronts uniquely and comprehen-sively de ne the accuracy-<b>fairness</b> trade-o s. We have then applied a stochastic ...", "dateLastCrawled": "2022-01-09T06:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Accuracy and <b>Fairness</b> Trade-offs in Machine Learning: A Stochastic ...", "url": "http://www.optimization-online.org/DB_FILE/2020/08/7950.pdf", "isFamilyFriendly": true, "displayUrl": "www.optimization-online.org/DB_FILE/2020/08/7950.pdf", "snippet": "fronts, and by doing so we <b>can</b> handle <b>training</b> data arriving in a streaming way. 1. Introduction Machine learning (ML) plays an increasingly signi\ufb01cant role in data-driven decision making, e.g., credit scoring, col-lege admission, hiring decisions, and criminal justice. As the learning models became more and more sophisticated, concern regarding <b>fairness</b> started receiving more and more attention. In 2014, the Obama Administration\u2019s Big Data Report (Podesta et al.,2014) claimed that ...", "dateLastCrawled": "2021-11-22T20:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Legal, Ethical and Contractual constraints in the Media</b> Industry", "url": "https://www.slideshare.net/Mattwattsmedia/legal-ethical-and-contractual-constraints-in-the-media-industry-13456879", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/Mattwattsmedia/<b>legal-ethical-and-contractual-constraints-in</b>...", "snippet": "<b>Fairness</b> \u201cOur output will be based on <b>fairness</b>, openness, honesty and straight dealing. Contributors and audiences will be treated with respect\u201d The BBC look to be open, honest, straightforward and fair in all dealings with contributors and audiences unless there is a clear public interest in doing otherwise, or they need to consider important issues such as legal matters, safety, or confidentiality. The BBC employ different tactics by giving contributors and audiences the right to ...", "dateLastCrawled": "2022-01-27T09:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Mirror Mirror", "url": "https://shiraamitchell.github.io/fairness/", "isFamilyFriendly": true, "displayUrl": "https://shiraamitchell.github.io/<b>fairness</b>", "snippet": "Procedure Selection (<b>Fairness</b> as <b>Constraint</b>): after imposing a definition of <b>fairness</b>, we could try to choose the best procedure by some other criterion. Procedure Selection (<b>Fairness</b> As Objective): assuming the degree of deviation from <b>fairness</b> <b>can</b> be quantified, we <b>can</b> attempt to minimize this deviation.", "dateLastCrawled": "2022-01-06T19:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>A CRITICAL ASSESSMENT OF JOHN RAWLS</b>\u2019S <b>THEORY OF JUSTICE AS FAIRNESS</b>", "url": "https://www.academia.edu/39832793/A_CRITICAL_ASSESSMENT_OF_JOHN_RAWLS_S_THEORY_OF_JUSTICE_AS_FAIRNESS", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/39832793/<b>A_CRITICAL_ASSESSMENT_OF_JOHN_RAWLS</b>_S_THEORY_OF...", "snippet": "This dissertation is a critical analysis of John Rawls\u2019s theory of justice in its historical and philosophical context. To that end, his works from A Theory of Justice (1971) to Justice as <b>Fairness</b>: A Restatement (2001) are examined. Not only Rawls\u2019s", "dateLastCrawled": "2022-01-29T00:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Fairness in Machine Learning with Tractable Models</b> | DeepAI", "url": "https://deepai.org/publication/fairness-in-machine-learning-with-tractable-models", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>fairness-in-machine-learning-with-tractable-models</b>", "snippet": "The approaches <b>can</b> be broadly partitioned into two categories: \u2018intra-processing\u2019 regularisation techniques which aim to minimise a measure of <b>fairness</b> whilst <b>training</b> is underway (e.g. (Learning_Fair_Reps, ; Disparate_Impact__Historical, ; tick_cross_paper, )), and \u2018pre-processing\u2019 techniques, where the data is modified in the hope of removing any dependence on the protected attribute prior to <b>training</b> (Counterfactual_<b>Fairness</b>, ; Calders_first_latent_attempt, ; Causal_Reasoning, ).", "dateLastCrawled": "2021-11-27T09:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "The Impact of <b>Fairness</b> on Decision Making - An Analysis of Dierent ...", "url": "https://www.researchgate.net/publication/242098367_The_Impact_of_Fairness_on_Decision_Making_-_An_Analysis_of_Dierent_Video_Experiments", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/242098367_The_Impact_of_<b>Fairness</b>_on_Decision...", "snippet": "The nature of <b>fairness</b> <b>can</b> be viewed as consisting of different facets or components. The ones we study are allocation aspects, power asymmetry, cultural background, the interplay between <b>fairness</b> ...", "dateLastCrawled": "2021-12-29T17:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A Reductions Approach to Fair Classification - arxiv-vanity.com", "url": "https://www.arxiv-vanity.com/papers/1803.02453/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1803.02453", "snippet": "We present a systematic approach for achieving <b>fairness</b> in a binary classification setting. While we focus on two well-known quantitative definitions of <b>fairness</b>, our ...", "dateLastCrawled": "2021-10-15T11:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(PDF) Blackbox Post-Processing for Multiclass <b>Fairness</b>", "url": "https://www.researchgate.net/publication/357790861_Blackbox_Post-Processing_for_Multiclass_Fairness", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/357790861_Blackbox_Post-<b>Process</b>ing_for_Multi...", "snippet": "proach for in-<b>process</b> <b>training</b> based on adversarial learning, with the discriminator distinguishing between the distribu- tion of the model\u2019s current predictions, the true label, and ...", "dateLastCrawled": "2022-01-25T23:22:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Accuracy and <b>Fairness</b> Trade-o s in Machine Learning: A Stochastic Multi ...", "url": "https://engineering.lehigh.edu/sites/engineering.lehigh.edu/files/_DEPARTMENTS/ise/pdf/tech-papers/20/20T_016.pdf", "isFamilyFriendly": true, "displayUrl": "https://engineering.lehigh.edu/sites/engineering.lehigh.edu/files/_DEPARTMENTS/ise/pdf/...", "snippet": "to include <b>fairness</b> as a <b>constraint</b> or a penal-ization term in the minimization of the predic-tion loss, which ultimately limits the information given to decision-makers. In this paper, we in- troduce a new approach to handle <b>fairness</b> by for-mulating a stochastic multi-objective optimiza-tion problem for which the corresponding Pareto fronts uniquely and comprehensively dene the accuracy-<b>fairness</b> trade-offs. We have then ap-plied a stochastic approximation-type method to efciently obtain ...", "dateLastCrawled": "2022-01-28T20:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "CONFAIR: Configurable and Interpretable Algorithmic <b>Fairness</b> | DeepAI", "url": "https://deepai.org/publication/confair-configurable-and-interpretable-algorithmic-fairness", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/confair-configurable-and-interpretable-algorithmic-<b>fairness</b>", "snippet": "The different definitions of statistical <b>fairness</b> lead to the evaluation of an algorithm\u2019s performance in fundamentally different ways. For instance, the disparate impact measures the ratio of true positive rates across groups while equal opportunity measures the difference between the true positive rates and false positive rates across groups with the <b>restriction</b> that they are less than a specified threshold. It has been shown that the different <b>fairness</b> metrics are often incompatible ...", "dateLastCrawled": "2022-01-22T16:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Accuracy and <b>Fairness</b> Trade-offs in Machine Learning: A Stochastic ...", "url": "http://www.optimization-online.org/DB_FILE/2020/08/7950.pdf", "isFamilyFriendly": true, "displayUrl": "www.optimization-online.org/DB_FILE/2020/08/7950.pdf", "snippet": "to include <b>fairness</b> as a <b>constraint</b> or a penal-ization term in the minimization of the predic-tion loss, which ultimately limits the information given to decision-makers. In this paper, we in- troduce a new approach to handle <b>fairness</b> by for-mulating a stochastic multi-objective optimiza-tion problem for which the corresponding Pareto fronts uniquely and comprehensively de\ufb01ne the accuracy-<b>fairness</b> trade-offs. We have then ap-plied a stochastic approximation-type method to ef\ufb01ciently ...", "dateLastCrawled": "2021-11-22T20:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "ACCURACY AND <b>FAIRNESS</b> TRADE-OFFS IN MACHINE APPROACH", "url": "https://www.mat.uc.pt/preprints/ps/p2037.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.mat.uc.pt/preprints/ps/p2037.pdf", "snippet": "monly used strategy in fair machine learning is to include <b>fairness</b> as a <b>constraint</b> or a penalization term in the minimization of the prediction loss, which ultimately limits the information given to decision-makers. In this paper, we introduce a new approach to handle <b>fairness</b> by formulating a stochastic multi-objective optimiza-tion problem for which the corresponding Pareto fronts uniquely and comprehen-sively de ne the accuracy-<b>fairness</b> trade-o s. We have then applied a stochastic ...", "dateLastCrawled": "2022-01-09T06:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) <b>Fairness</b>-Aware Learning <b>with Restriction of Universal Dependency</b> ...", "url": "https://www.researchgate.net/publication/279310002_Fairness-Aware_Learning_with_Restriction_of_Universal_Dependency_using_f-Divergences", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/279310002_<b>Fairness</b>-Aware_Learning_with...", "snippet": "PDF | <b>Fairness</b>-aware learning is a novel framework for classification tasks. Like regular empirical risk minimization (ERM), it aims to learn a... | Find, read and cite all the research you need ...", "dateLastCrawled": "2021-11-09T02:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Differentially Private Fair Learning", "url": "http://proceedings.mlr.press/v97/jagielski19a/jagielski19a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v97/jagielski19a/jagielski19a.pdf", "snippet": "and the <b>restriction</b> on the collection or use of protected at-tributes on the other \u2014 present technical conundrums, since the most straightforward methods for ensuring <b>fairness</b> gen-erally require knowing or using the attribute being protected. It seems dif\ufb01cult to guarantee that a trained model is not discriminating against (say) a racial group if we cannot even identify members of that group in the data. A recent line of work (Veale &amp; Binns ,2017;Kilbertus et al. 2018) made these cogent ...", "dateLastCrawled": "2022-01-22T22:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Mirror Mirror", "url": "https://shiraamitchell.github.io/fairness/", "isFamilyFriendly": true, "displayUrl": "https://shiraamitchell.github.io/<b>fairness</b>", "snippet": "Procedure Selection (<b>Fairness</b> as <b>Constraint</b>): after imposing a definition of <b>fairness</b>, we could try to choose the best procedure by some other criterion. Procedure Selection (<b>Fairness</b> As Objective): assuming the degree of deviation from <b>fairness</b> <b>can</b> be quantified, we <b>can</b> attempt to minimize this deviation.", "dateLastCrawled": "2022-01-06T19:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Multiaccuracy: Black-Box <b>Post-Processing</b> for <b>Fairness</b> in Classi cation", "url": "https://cs.stanford.edu/~mpkim/pubs/multiaccuracy.pdf", "isFamilyFriendly": true, "displayUrl": "https://cs.stanford.edu/~mpkim/pubs/multiaccuracy.pdf", "snippet": "revealed that the system was signi cantly less accurate on female subjects <b>compared</b> to males and on dark-skinned individuals <b>compared</b> to light-skinned. Worse yet, this discrepancy in accuracy compounded when comparing dark-skinned females to light-skinned males; classi cation accuracy di ered between these groups by as much as 34%! This study con rmed empirically the intuition that machine-learned classi ers may optimize predictions to perform well on the majority popula-tion, inadvertently ...", "dateLastCrawled": "2021-12-22T09:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Freedom of Testation &amp; <b>the Fairness of the Methods of its Curtailment</b> ...", "url": "https://www.theunitutor.com/critical-analysis-freedom-testation-fairness-methods-curtailment/", "isFamilyFriendly": true, "displayUrl": "https://www.theunitutor.com/critical-analysis-freedom-testation-<b>fairness</b>-methods...", "snippet": "We will accept legal <b>constraint</b> grudgingly, acknowledging that it is better than the alternative. But in a perfect world, we would prefer that our liberty remain unrestricted. On the second reading, however, the law is what instructs us in excellence and what makes real joy possible. One becomes a freer player of golf the more one manages to internalize the laws of the game; one becomes increasingly rangy and free in the articulation of English the more one adapts to the laws of syntax ...", "dateLastCrawled": "2022-01-22T10:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Maintaining Discrimination and <b>Fairness</b> in Class Incremental Learning ...", "url": "https://deepai.org/publication/maintaining-discrimination-and-fairness-in-class-incremental-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/maintaining-discrimination-and-<b>fairness</b>-in-class...", "snippet": "Then, to further maintain the <b>fairness</b> between old classes and new classes, we propose Weight Aligning (WA) that corrects the biased weights in the FC layer after normal <b>training</b> <b>process</b>. Unlike previous work, WA does not require any extra parameters or a validation set in advance, as it utilizes the information provided by the biased weights themselves. The proposed method is evaluated on", "dateLastCrawled": "2021-11-28T15:57:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Classification - <b>Fairness</b> and <b>machine</b> <b>learning</b>", "url": "https://fairmlbook.org/classification.html", "isFamilyFriendly": true, "displayUrl": "https://fairmlbook.org/classification.html", "snippet": "Dwork et al. Dwork et al., \u201c<b>Fairness</b> Through Awareness.\u201d argued that the independence criterion was inadequate as a <b>fairness</b> <b>constraint</b>. The separation criterion appeared under the name equalized odds , Hardt, Price, and Srebro, \u201cEquality of Opportunity in Supervised <b>Learning</b>.\u201d alongside the relaxation to equal false negative rates, called equality of opportunity.", "dateLastCrawled": "2022-02-03T07:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A Human-centric Approach to <b>Fairness</b> in AI", "url": "https://www.timlrx.com/blog/a-human-centric-approach-to-fairness-in-ai", "isFamilyFriendly": true, "displayUrl": "https://www.timlrx.com/blog/a-human-centric-approach-to-<b>fairness</b>-in-ai", "snippet": "A lot of what is discussed in the <b>machine</b> <b>learning</b> literature touches on <b>fairness</b> (or rather equivalence in certain outcomes) between groups, yet this narrowly constricts <b>fairness</b> to the notion of equality. Of course, we should think about <b>fairness</b> in the context of prejudiced groups, but we should also ask whether it is fair to an individual. Adding constraints in models might lead to worse outcomes for other individuals. If the decision making processs has serious consequences e.g. a fraud ...", "dateLastCrawled": "2022-02-03T02:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A Survey on Bias and <b>Fairness in Machine Learning</b> | DeepAI", "url": "https://deepai.org/publication/a-survey-on-bias-and-fairness-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/a-survey-on-bias-and-<b>fairness-in-machine-learning</b>", "snippet": "A Survey on Bias and <b>Fairness in Machine Learning</b>. 08/23/2019 \u2219 by Ninareh Mehrabi, et al. \u2219 1 \u2219 share. With the widespread use of AI systems and applications in our everyday lives, it is important to take <b>fairness</b> issues into consideration while designing and engineering these types of systems. Such systems can be used in many sensitive ...", "dateLastCrawled": "2022-01-22T23:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "dimensions of <b>machine</b> Causality and the normative", "url": "https://maxkasy.github.io/home/files/other/ML_inequality_conference/slides_loftus.pdf", "isFamilyFriendly": true, "displayUrl": "https://maxkasy.github.io/home/files/other/ML_inequality_conference/slides_loftus.pdf", "snippet": "dimensions of <b>machine</b> <b>learning</b> Joshua Loftus (LSE Statistics) High level intro Causality, what is it good for? Causal <b>fairness</b> In prediction and ranking tasks, and with intersectionality Designing interventions Optimal fair policies, causal interference Concluding thoughts 2 / 27. Tech solutionism, using ML/AI in every situation 3 / 27. Imagination Albert Einstein: Imagination is more important than knowledge. For knowledge is limited, whereas imagination [...] stimulat[es] progress, giving ...", "dateLastCrawled": "2022-01-11T12:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "PAC-Bayes and <b>Fairness</b>: Risk and <b>Fairness</b> Bounds on Distribution ...", "url": "https://www.esann.org/sites/default/files/proceedings/legacy/es2019-77.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.esann.org/sites/default/files/proceedings/legacy/es2019-77.pdf", "snippet": "Several notions of <b>fairness</b> and associated <b>learning</b> methods have been introduced in <b>machine</b> <b>learning</b> in the past few years, including Demographic Parity [2], Equal Odds and Equal Opportunities [3], Disparate Treatment, Im-pact, and Mistreatment [4]. The underlying idea behind such notions is to balance decisions of a classi er among the di erent sensitive groups and label sets. Contemporary, it is well known that combining the output of several clas-si ers results in much better ...", "dateLastCrawled": "2021-11-22T03:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Is Bias in Machine Learning all Bad</b>? - KDnuggets", "url": "https://www.kdnuggets.com/2019/07/bias-machine-learning-bad.html", "isFamilyFriendly": true, "displayUrl": "https://www.kdnuggets.com/2019/07/bias-<b>machine</b>-<b>learning</b>-bad.html", "snippet": "<b>Analogy</b> with previously learned generalizations; If a system is <b>learning</b> a collection of related concepts, or generalizations, then a possible <b>constraint</b> on generalizing any one of them is to consider successful generalization of others. For example, consider a task of <b>learning</b> structural descriptions of blocks-world objects, such as \u201darch ...", "dateLastCrawled": "2022-01-22T18:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Ethical and legal considerations of artificial intelligence and ...", "url": "https://link.springer.com/article/10.1057/s41272-019-00225-2", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1057/s41272-019-00225-2", "snippet": "The transition to <b>machine</b> <b>learning</b> as the basis for algorithmic modeling requires a substantial increase in available data for training, validation, and testing (Heath 2018). Yet raw data used for training reflect human bias\u2014whether explicit or by omission. A Google image search for \u201cparents\u201d yields photos of predominantly heterosexual couples, and a similar search for \u201cnurse\u201d results in a preponderance of women (Steimer", "dateLastCrawled": "2022-02-02T23:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "On word analogies and negative results in NLP - Hacking semantics", "url": "https://hackingsemantics.xyz/2019/analogies/", "isFamilyFriendly": true, "displayUrl": "https://hackingsemantics.xyz/2019/analogies", "snippet": "Share of BATS <b>analogy</b> questions in which the vector the closest to the predicted vector is one of the source vectors (a,a&#39;, b), the target vector b&#39;, or some other vector. In most cases the result is simply the vector b (&quot;woman&quot;). If in most cases the predicted vector is the closest to the source vector, it means that the vector offset is simply too small to induce a meaning shift on its own. And that means that adding it will not get you somewhere significantly different. Which means you ...", "dateLastCrawled": "2021-06-02T05:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Fairness</b> Through Awareness", "url": "https://www.cs.toronto.edu/~zemel/documents/fairAwareItcs2012.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.toronto.edu/~zemel/documents/fairAwareItcs2012.pdf", "snippet": "<b>Fairness</b> Through Awareness Cynthia Dwork Moritz Hardty Toniann Pitassiz Omer Reingoldx Richard Zemel{November 29, 2011 Abstract We study <b>fairness</b> in classi\ufb01cation, where individuals are classi\ufb01ed, e.g., admitted to a uni- versity, and the goal is to prevent discrimination against individuals based on their membership in some group, while maintaining utility for the classi\ufb01er (the university). The main conceptual contribution of this paper is a framework for fair classi\ufb01cation ...", "dateLastCrawled": "2022-01-29T03:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "VERIFICATION AND VALIDATION OF ETHICAL DECISION-MAKING IN AUTONOMOUS ...", "url": "http://scs.org/wp-content/uploads/2017/06/1_Final_Manuscript-3.pdf", "isFamilyFriendly": true, "displayUrl": "scs.org/wp-content/uploads/2017/06/1_Final_Manuscript-3.pdf", "snippet": "analogical reasoning, <b>constraint</b> satisfaction, decision-theory, and (social) <b>learning</b>. In (Kurland 1995), the role of theory of planned behavior and reasoned action in predicting ethical intentions towards others was demonstrated in experiments with human subjects. Besides planning and goal-directed behavior, deductive logic has been proposed (Saptawijaya, Pereira, et al. 2012) to infer judgments and their ethical implications. In (Arkoudas, Bringsjord, and Bello 2005), authors demonstrate ...", "dateLastCrawled": "2021-11-28T13:10:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(fairness constraint)  is like +(restriction on the training process)", "+(fairness constraint) is similar to +(restriction on the training process)", "+(fairness constraint) can be thought of as +(restriction on the training process)", "+(fairness constraint) can be compared to +(restriction on the training process)", "machine learning +(fairness constraint AND analogy)", "machine learning +(\"fairness constraint is like\")", "machine learning +(\"fairness constraint is similar\")", "machine learning +(\"just as fairness constraint\")", "machine learning +(\"fairness constraint can be thought of as\")", "machine learning +(\"fairness constraint can be compared to\")"]}