{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Concept of <b>Gradient</b> <b>Descent</b> in Machine Learning. | by VARSHITHA ...", "url": "https://varshithagudimalla.medium.com/concept-of-gradient-descent-algorithm-in-machine-learning-44f587ac16ac", "isFamilyFriendly": true, "displayUrl": "https://varshithagudimalla.medium.com/concept-of-<b>gradient</b>-<b>descent</b>-algorithm-in-machine...", "snippet": "It is quite simple to understand once we know Batch and <b>Stochastic</b> <b>Gradient</b> <b>Descent</b>: at each step, instead of computing the gradients based on the full training set or based on just one instance, <b>Mini-batch</b> GD computes the gradients on small random sets of instances called mini-batches. The main advantage of <b>Mini-batch</b> GD over <b>Stochastic</b> GD is that we can get a performance boost from hardware optimization of matrix operations.", "dateLastCrawled": "2022-01-26T08:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Optimization Algorithms in Neural Networks - <b>KDnuggets</b>", "url": "https://www.kdnuggets.com/2020/12/optimization-algorithms-neural-networks.html", "isFamilyFriendly": true, "displayUrl": "https://www.<b>kdnuggets</b>.com/2020/12/optimization-algorithms-neural-networks.html", "snippet": "<b>Mini-batch</b> <b>Gradient</b> <b>Descent</b> is relatively more stable than <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> (SGD) but does have oscillations as <b>gradient</b> steps are being taken in the direction of a sample of the training set and not the entire set as in BGD. It is observed that in SGD the updates take more number iterations compared to <b>gradient</b> <b>descent</b> to reach ...", "dateLastCrawled": "2022-01-31T07:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>All about Gradient Descent and its variants</b> | by Anjana Yadav ...", "url": "https://medium.com/analytics-vidhya/all-about-gradient-descent-and-its-variants-d095be1a833b", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/<b>all-about-gradient-descent-and-its-variants</b>-d095be...", "snippet": "This is what <b>Mini-Batch</b> <b>Gradient</b> <b>Descent</b> does. It samples a batch of data points, computes the loss for those points and finally updates the weights. Thus if the batch size is k data points then ...", "dateLastCrawled": "2022-01-27T13:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "An Easy Guide to <b>Gradient Descent in Machine Learning</b>", "url": "https://www.mygreatlearning.com/blog/gradient-descent/", "isFamilyFriendly": true, "displayUrl": "https://www.mygreatlearning.com/blog/<b>gradient</b>-<b>descent</b>", "snippet": "Instead, we prefer to use <b>stochastic</b> <b>gradient</b> <b>descent</b> or <b>mini-batch</b> <b>gradient</b> <b>descent</b> which is discussed next. <b>Mini Batch</b> <b>gradient</b> <b>descent</b>: This is a type of <b>gradient</b> <b>descent</b> which works faster than both batch <b>gradient</b> <b>descent</b> and <b>stochastic</b> <b>gradient</b> <b>descent</b>. Neither we use all the dataset all at once nor we use the single example at a time. We ...", "dateLastCrawled": "2022-01-30T17:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine Learning: Linear Regression</b> | by Shubham singh | Medium", "url": "https://shubhamsindal0098.medium.com/machine-learning-linear-regression-7b2e04454368", "isFamilyFriendly": true, "displayUrl": "https://shubhamsindal0098.medium.com/<b>machine-learning-linear-regression</b>-7b2e04454368", "snippet": "<b>Mini batch</b> <b>gradient</b> <b>descent</b> finds a balance between Batch <b>gradient</b> <b>descent</b> and <b>stochastic</b> <b>gradient</b>. It splits the dataset into several small batches and then updates the model parameters by calculating the <b>gradient</b> of small batches. The advantage of using <b>mini-batch</b> <b>gradient</b> <b>descent</b> is that it uses matrix operations to speed up the calculation. Since we are using mini-batches in the training process, the fluctuation of the cost function is a bit <b>like</b> <b>stochastic</b> <b>gradient</b> <b>descent</b>. However ...", "dateLastCrawled": "2022-01-31T21:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A Gentle Introduction to Optimizers | by Shivam Singh | Medium", "url": "https://sshivam-singh96.medium.com/a-gentle-introduction-to-optimizers-f9ef4e26f688", "isFamilyFriendly": true, "displayUrl": "https://sshivam-singh96.medium.com/a-gentle-introduction-to-optimizers-f9ef4e26f688", "snippet": "<b>Mini-batch</b> <b>gradient</b> <b>descent</b>. MBGD uses a small batch of samples, that is, n samples to calculate each time. In this way, it can reduce the variance when the parameters are updated, and the convergence is more stable. It can make full use of the highly optimized matrix operations in the deep learning library for more efficient <b>gradient</b> calculations.", "dateLastCrawled": "2022-01-16T14:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>How neural networks are trained</b> - GitHub Pages", "url": "https://ml4a.github.io/ml4a/how_neural_networks_are_trained/", "isFamilyFriendly": true, "displayUrl": "https://ml4a.github.io/ml4a/<b>how_neural_networks_are_trained</b>", "snippet": "<b>Stochastic</b>, batch, and <b>mini-batch</b> <b>gradient</b> <b>descent</b>. Besides for local minima, \u201cvanilla\u201d <b>gradient</b> <b>descent</b> has another major problem: it\u2019s too slow. A neural net may have hundreds of millions of parameters; this means a single example from our dataset requires hundreds of millions of operations to evaluate. Subsequently, <b>gradient</b> <b>descent</b> evaluated over all of the points in our dataset \u2013 also known as \u201cbatch <b>gradient</b> <b>descent</b>\u201d \u2013 is a very expensive and slow operation. Moreover ...", "dateLastCrawled": "2022-02-02T22:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Linear regression from scratch in Python. | by Joanna Trojak | Medium", "url": "https://joannatrojak.medium.com/linear-regression-from-scratch-in-python-6b5eabccbeab", "isFamilyFriendly": true, "displayUrl": "https://joannatrojak.medium.com/linear-regression-from-scratch-in-python-6b5eabccbeab", "snippet": "<b>Mini-batch</b> <b>gradient</b> <b>descent</b> computes the <b>gradient</b>-based on small sets of instances called mini-batches. The main advantage over <b>stochastic</b> <b>gradient</b> <b>descent</b> is the performance boost. You can optimize the performance of the algorithm especially using GPU\u2019s. The algorithm progress is less erratic than SGD especially with big mini-batches but it may be harder for it to escape the local minimum.", "dateLastCrawled": "2022-02-02T21:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Optimizers in Machine Learning and Deep Learning.-InsideAIML", "url": "https://www.insideaiml.com/blog/Optimizers-in-Machine-Learning-and-Deep-Learning.-1048", "isFamilyFriendly": true, "displayUrl": "https://www.insideaiml.com/blog/Optimizers-in-Machine-Learning-and-Deep-Learning.-1048", "snippet": "<b>Mini-batch</b> <b>gradient</b> <b>descent</b>. In this article, I will not go into the details of the above optimizers. Later I will try to write a separate article on each topic. Other Types of Optimizers. As we already know, how popular <b>gradient</b> <b>descent</b> algorithm is, and how it\u2019s used in machine learning and even up to complex neural networks in deep learning problems. (Backpropagation is basically <b>gradient</b> <b>descent</b> implemented on a network). There are some other types of optimizers available and used ...", "dateLastCrawled": "2022-01-28T20:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "4. Training Models - Hands-On Machine Learning with Scikit-Learn and ...", "url": "https://www.oreilly.com/library/view/hands-on-machine-learning/9781491962282/ch04.html", "isFamilyFriendly": true, "displayUrl": "https://www.oreilly.com/library/view/hands-on-machine-learning/9781491962282/ch04.html", "snippet": "The last <b>Gradient</b> <b>Descent</b> algorithm we will look at is called <b>Mini-batch</b> <b>Gradient</b> <b>Descent</b>. It is quite simple to understand once you know Batch and <b>Stochastic</b> <b>Gradient</b> <b>Descent</b>: at each step, instead of computing the gradients based on the full training set (as in Batch GD) or based on just one instance (as in <b>Stochastic</b> GD), <b>Mini-batch</b> GD computes the gradients on small random sets of instances called mini-batches .", "dateLastCrawled": "2022-01-29T19:12:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "An Easy Guide to <b>Gradient Descent in Machine Learning</b>", "url": "https://www.mygreatlearning.com/blog/gradient-descent/", "isFamilyFriendly": true, "displayUrl": "https://www.mygreatlearning.com/blog/<b>gradient</b>-<b>descent</b>", "snippet": "Instead, we prefer to use <b>stochastic</b> <b>gradient</b> <b>descent</b> or <b>mini-batch</b> <b>gradient</b> <b>descent</b> which is discussed next. <b>Mini Batch</b> <b>gradient</b> <b>descent</b>: This is a type of <b>gradient</b> <b>descent</b> which works faster than both batch <b>gradient</b> <b>descent</b> and <b>stochastic</b> <b>gradient</b> <b>descent</b>. Neither we use all the dataset all at once nor we use the single example at a time. We ...", "dateLastCrawled": "2022-01-30T17:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A visual understanding of <b>Gradient</b> <b>Descent</b> | by Sai Kiran Putta | Medium", "url": "https://medium.com/@saikiran48/a-visual-understanding-of-gradient-descent-6ec54e75d8ba", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@saikiran48/a-visual-understanding-of-<b>gradient</b>-<b>descent</b>-6ec54e75d8ba", "snippet": "<b>Mini-batch</b> <b>Gradient</b> <b>Descent</b> : <b>Mini-batch</b> <b>Gradient</b> <b>Descent</b> is a hybrid of Batch <b>Gradient</b> <b>Descent</b> and <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> and uses a small, randomized subset of training data to make an ...", "dateLastCrawled": "2021-11-19T06:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Optimization Algorithms in Neural Networks - <b>KDnuggets</b>", "url": "https://www.kdnuggets.com/2020/12/optimization-algorithms-neural-networks.html", "isFamilyFriendly": true, "displayUrl": "https://www.<b>kdnuggets</b>.com/2020/12/optimization-algorithms-neural-networks.html", "snippet": "<b>Mini-batch</b> <b>Gradient</b> <b>Descent</b> is relatively more stable than <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> (SGD) but does have oscillations as <b>gradient</b> steps are being taken in the direction of a sample of the training set and not the entire set as in BGD. It is observed that in SGD the updates take more number iterations compared to <b>gradient</b> <b>descent</b> to reach ...", "dateLastCrawled": "2022-01-31T07:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>All about Gradient Descent and its variants</b> | by Anjana Yadav ...", "url": "https://medium.com/analytics-vidhya/all-about-gradient-descent-and-its-variants-d095be1a833b", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/<b>all-about-gradient-descent-and-its-variants</b>-d095be...", "snippet": "<b>Stochastic</b> <b>Gradient</b> <b>Descent</b> with N = 5. This problem with <b>stochastic</b> <b>gradient</b> <b>descent</b> arises because the model calculates the gradients for complete dataset using one data point. This <b>is similar</b> ...", "dateLastCrawled": "2022-01-27T13:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Gradient</b> <b>descent</b> Algorithm: In-Depth explanation-InsideAIML", "url": "https://www.insideaiml.com/blog/Gradient-descent-Algorithm%3A-In-Depth-explanation-1047", "isFamilyFriendly": true, "displayUrl": "https://www.insideaiml.com/blog/<b>Gradient</b>-<b>descent</b>-Algorithm:-In-Depth-explanation-1047", "snippet": "SGD (<b>Stochastic</b> <b>gradient</b> <b>descent</b>) 3. <b>Mini-batch</b> <b>gradient</b> <b>descent</b>. Let\u2019s see how they differ from each other\u2019s. Batch <b>gradient</b> <b>descent</b>/ Vanilla <b>gradient</b> <b>descent</b>. <b>Gradient</b> update rule: BGD uses the data of the entire training set to calculate the <b>gradient</b> of the cost function to the parameters: Disadvantages: Because this method calculates the <b>gradient</b> for the entire data set in one update, the calculation is very slow, it will be very tricky to encounter a large number of data sets, and ...", "dateLastCrawled": "2022-02-03T15:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Machine Learning: Linear Regression</b> | by Shubham singh | Medium", "url": "https://shubhamsindal0098.medium.com/machine-learning-linear-regression-7b2e04454368", "isFamilyFriendly": true, "displayUrl": "https://shubhamsindal0098.medium.com/<b>machine-learning-linear-regression</b>-7b2e04454368", "snippet": "<b>Mini batch</b> <b>gradient</b> <b>descent</b> finds a balance between Batch <b>gradient</b> <b>descent</b> and <b>stochastic</b> <b>gradient</b>. It splits the dataset into several small batches and then updates the model parameters by calculating the <b>gradient</b> of small batches. The advantage of using <b>mini-batch</b> <b>gradient</b> <b>descent</b> is that it uses matrix operations to speed up the calculation. Since we are using mini-batches in the training process, the fluctuation of the cost function is a bit like <b>stochastic</b> <b>gradient</b> <b>descent</b>. However ...", "dateLastCrawled": "2022-01-31T21:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Extensions to Gradient Descent: from momentum to</b> AdaBound", "url": "https://www.machinecurve.com/index.php/2019/11/03/extensions-to-gradient-descent-from-momentum-to-adabound/", "isFamilyFriendly": true, "displayUrl": "https://www.machinecurve.com/index.php/2019/11/03/<b>extensions-to-gradient-descent-from</b>...", "snippet": "<b>Stochastic</b> <b>gradient</b> <b>descent</b>, which optimizes your model each time a sample is fed forward, based on the loss generated for this sample. Although it\u2019s blazing fast, especially compared to batch <b>gradient</b> <b>descent</b>, it is much less accurate. Imagine what happens when a statistical outlier is fed forward \u2013 your model will swing away from its path to the minimum. <b>Minibatch</b> <b>gradient</b> <b>descent</b>, which lies somewhere in between: your model is optimized based on a weights change determined by mini ...", "dateLastCrawled": "2022-01-21T09:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "4. Training Models - Hands-On Machine Learning with Scikit-Learn and ...", "url": "https://www.oreilly.com/library/view/hands-on-machine-learning/9781491962282/ch04.html", "isFamilyFriendly": true, "displayUrl": "https://www.oreilly.com/library/view/hands-on-machine-learning/9781491962282/ch04.html", "snippet": "The last <b>Gradient</b> <b>Descent</b> algorithm we will look at is called <b>Mini-batch</b> <b>Gradient</b> <b>Descent</b>. It is quite simple to understand once you know Batch and <b>Stochastic</b> <b>Gradient</b> <b>Descent</b>: at each step, instead of computing the gradients based on the full training set (as in Batch GD) or based on just one instance (as in <b>Stochastic</b> GD), <b>Mini-batch</b> GD computes the gradients on small random sets of instances called mini-batches .", "dateLastCrawled": "2022-01-29T19:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "The ins and outs of <b>Gradient Descent</b> | by Jack Leitch | Towards Data ...", "url": "https://towardsdatascience.com/the-ins-and-outs-of-gradient-descent-1cf23dc90f83", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/the-ins-and-outs-of-<b>gradient-descent</b>-1cf23dc90f83", "snippet": "A good strategy to get down the <b>mountain</b> is to feel the ground in every direction and take a step in the direction in which the ground is <b>descending</b> the fastest. Repeating this you should end up at the bottom of the <b>mountain</b> (although you might also end up at something that looks like the bottom that isn&#39;t \u2014 more on this later). This is exactly what <b>gradient descent</b> does: it measures the local <b>gradient</b> of the cost function J(\u03c9) (parametrized by model parameters \u03c9), and moves in the ...", "dateLastCrawled": "2022-01-31T05:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Gradient Descent</b> in Python. When you venture into machine learning ...", "url": "https://towardsdatascience.com/gradient-descent-in-python-a0d07285742f", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>gradient-descent</b>-in-python-a0d07285742f", "snippet": "To explain in brief about <b>gradient descent</b>, imagine that you are on <b>a mountain</b> and are blindfolded and your task is to come down from the <b>mountain</b> to the flat land without assistance. The only assistance you have is a gadget which tells you the height from sea-level. What would be your approach be. You would start to descend in some random direction and then ask the gadget what is the height now. If the gadget tells you that height and it is more than the initial height then you know you ...", "dateLastCrawled": "2022-02-02T17:02:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>GRADIENT</b> <b>DESCENT</b>, A QUICK, SIMPLE INTRODUCTION TO HEART OF MACHINE ...", "url": "https://blurcode.in/blog/gradient-descent-a-quick-simple-introduction-to-heart-of-machine-learning-algorithms/", "isFamilyFriendly": true, "displayUrl": "https://blurcode.in/blog/<b>gradient</b>-<b>descent</b>-a-quick-simple-introduction-to-heart-of...", "snippet": "<b>Mini Batch</b> <b>Gradient</b> <b>Descent</b>; <b>Mini-batch</b> <b>gradient</b> <b>descent</b> is the go-to method since it\u2019s a combination of the concepts of SGD and batch <b>gradient</b> <b>descent</b>. It simply splits the training dataset into small batches and performs an update for each of those batches. This creates a balance between the robustness of <b>stochastic</b> <b>gradient</b> <b>descent</b> and the ...", "dateLastCrawled": "2022-01-30T18:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Gradient</b> <b>Descent</b> : A Quick, Simple Introduction to heart of Machine ...", "url": "https://medium.com/swlh/gradient-descent-a-quick-simple-introduction-to-heart-of-machine-learning-algorithms-ae34f8a627f0?source=post_internal_links---------6----------------------------", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/<b>gradient</b>-<b>descent</b>-a-quick-simple-introduction-to-heart-of...", "snippet": "<b>Mini-batch</b> <b>gradient</b> <b>descent</b> is the go-to method since it\u2019s a combination of the concepts of SGD and batch <b>gradient</b> <b>descent</b>. It simply splits the training dataset into small batches and performs ...", "dateLastCrawled": "2020-10-25T07:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Extensions to Gradient Descent: from momentum to</b> AdaBound \u2013 MachineCurve", "url": "https://www.machinecurve.com/index.php/2019/11/03/extensions-to-gradient-descent-from-momentum-to-adabound/", "isFamilyFriendly": true, "displayUrl": "https://www.machinecurve.com/index.php/2019/11/03/<b>extensions-to-gradient-descent-from</b>...", "snippet": "<b>Stochastic</b> <b>gradient</b> <b>descent</b>, which optimizes your model each time a sample is fed forward, based on the loss generated for this sample. Although it\u2019s blazing fast, especially compared to batch <b>gradient</b> <b>descent</b>, it is much less accurate. Imagine what happens when a statistical outlier is fed forward \u2013 your model will swing away from its path to the minimum. <b>Minibatch</b> <b>gradient</b> <b>descent</b>, which lies somewhere in between: your model is optimized based on a weights change determined by mini ...", "dateLastCrawled": "2022-01-21T09:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Gradient Descent</b> from Scratch in Python - Morioh", "url": "https://morioh.com/p/e5b706f79119", "isFamilyFriendly": true, "displayUrl": "https://morioh.com/p/e5b706f79119", "snippet": "It is related to <b>stochastic</b> <b>gradient descent</b> and to <b>mini-batch</b> <b>gradient descent</b>, but it predates them. The main takeaway between all of these flavour of <b>gradient descent</b> is that the performance of the algorithm will change (speed and convergence) depending on how many point you are taking to calculate the <b>gradient</b>. If you take n=N it\u2019s the original <b>gradient descent</b>, if you take n=1 it\u2019s <b>stochastic</b> <b>gradient descent</b> if you take n=m where m is between 1 and N it\u2019s <b>mini-batch</b> <b>gradient</b> ...", "dateLastCrawled": "2021-10-09T02:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Toward an Integration of Deep Learning and Neuroscience", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5021692/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC5021692", "snippet": "Interestingly, however, <b>stochastic</b> <b>gradient</b> <b>descent</b> <b>can</b> be used to generate a system that samples adaptively (Alain et al., 2015; Bouchard et al., 2015). In other words, a system <b>can</b> learn, by <b>gradient</b> <b>descent</b>, how to choose its own input data samples in order to learn most quickly from them by <b>gradient</b> <b>descent</b>.", "dateLastCrawled": "2022-01-10T16:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 5, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Gradient descent</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Gradient_descent", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Gradient_descent</b>", "snippet": "<b>Gradient descent</b> is based on the observation that if the multi-variable function is defined and differentiable in a neighborhood of a point , then () decreases fastest if one goes from in the direction of the negative <b>gradient</b> of at , ().It follows that, if + = for a small enough step size or learning rate +, then (+).In other words, the term () is subtracted from because we want to move against the <b>gradient</b>, toward the local minimum. With this observation in mind, one starts with a guess ...", "dateLastCrawled": "2022-02-03T03:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) Mastering the game of Go <b>with deep neural networks and tree search</b>", "url": "https://www.researchgate.net/publication/292074166_Mastering_the_game_of_Go_with_deep_neural_networks_and_tree_search", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/292074166", "snippet": "selected <b>mini-batch</b> of m samples from the augmented KGS data-set, {s k, a k} m k =1 and applied an asynchronous <b>stochastic</b> <b>gradient</b> <b>descent</b> update to maximize the log likelihood of the action,", "dateLastCrawled": "2022-01-30T17:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Basic Interview Q&#39;s On ML</b> PDF | PDF | Ordinary Least Squares ...", "url": "https://www.scribd.com/document/440086307/Basic-Interview-Q-s-on-ML-pdf", "isFamilyFriendly": true, "displayUrl": "https://www.scribd.com/document/440086307/<b>Basic-Interview-Q-s-on-ML</b>-pdf", "snippet": "Building a linear model using <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> is also helpful. 7. We <b>can</b> also apply our business understanding to estimate which all predictors <b>can</b> impact the response variable. But, this is an intuitive approach, failing to identify useful predictors might result in significant loss of information. Note: For point 4 &amp; 5, make sure you read about online learning algorithms &amp; <b>Stochastic</b> <b>Gradient</b> <b>Descent</b>. These are advanced methods. Q2. Is rotation necessary in PCA? If yes, Why ...", "dateLastCrawled": "2022-02-02T02:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Descent</b>, <b>descent</b> definition is - derivation from an ancestor : birth ...", "url": "https://milionewereld.com/wiki/Descent,_Part_II_(episode)p98n9d3313zebc4b", "isFamilyFriendly": true, "displayUrl": "https://milionewereld.com/wiki/<b>Descent</b>,_Part_II_(episode)p98n9d3313zebc4b", "snippet": "Each example zis a pai <b>Mini-batch</b> <b>stochastic</b> <b>gradient</b> <b>descent</b> ( <b>mini-batch</b> SGD) is a compromise between full-batch iteration and SGD. A <b>mini-batch</b> is typically between 10 and 1,000 examples, chosen at random. <b>Mini-batch</b> SGD reduces the amount of noise in SGD but is still more efficient than full-batch. To simplify the explanation, we focused on <b>gradient</b> <b>descent</b> for a. <b>Gradient</b> <b>Descent</b> is an optimization algorithm for finding a local minimum of a differentiable function. <b>Gradient</b> <b>descent</b> is ...", "dateLastCrawled": "2021-12-28T16:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "arxiv-cs-analysis/cluster_phrase_semicolon_50.txt at master \u00b7 tf-dbis ...", "url": "https://github.com/tf-dbis-uni-freiburg/arxiv-cs-analysis/blob/master/Noun%20Phrase%20Frequencies%20Visualization/NPFreqSolrDash/cluster_phrase_semicolon_50.txt", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/tf-dbis-uni-freiburg/arxiv-cs-analysis/blob/master/Noun Phrase...", "snippet": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters.", "dateLastCrawled": "2022-01-31T10:39:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Mini-batch</b> <b>Gradient</b> <b>Descent</b>", "url": "https://topic.alibabacloud.com/a/mini-batch-gradient-descent_8_8_10274857.html", "isFamilyFriendly": true, "displayUrl": "https://topic.alibabacloud.com/a/<b>mini-batch</b>-<b>gradient</b>-<b>descent</b>_8_8_10274857.html", "snippet": "The <b>stochastic</b> <b>gradient</b> <b>descent</b> algorithm (<b>stochastic</b> <b>gradient</b> <b>descent</b>) <b>can</b> be seen as a special case of <b>mini-batch</b> <b>gradient</b> <b>descent</b>, i.e., the parameters in the model are adjusted only one sample at a time in the random <b>gradient</b> <b>descent</b> method, <b>Mini-batch</b> <b>gradient</b> <b>descent</b>, which is equivalent to the B=1 case described above, has only one training sample per <b>Mini-batch</b>. The optimization process of the <b>stochastic</b> <b>gradient</b> <b>descent</b> method is: The random <b>gradient</b> <b>descent</b> is updated once per ...", "dateLastCrawled": "2021-04-27T07:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Machine Learning FAQ _ Several <b>gradient</b> <b>descent</b> method __ Machine Learning", "url": "https://topic.alibabacloud.com/a/machine-learning-faq-_-several-gradient-descent-method-__-machine-learning_8_8_20286622.html", "isFamilyFriendly": true, "displayUrl": "https://topic.alibabacloud.com/a/machine-learning-faq-_-several-<b>gradient</b>-<b>descent</b>...", "snippet": "The <b>stochastic</b> <b>gradient</b> <b>descent</b> algorithm (<b>stochastic</b> <b>gradient</b> <b>descent</b>) <b>can</b> be considered as a special case of <b>mini-batch</b> <b>gradient</b> <b>descent</b>, in which the parameters in the model are adjusted at one time in the random <b>gradient</b> <b>descent</b> method, Equivalent to the <b>Mini-batch</b> <b>gradient</b> <b>descent</b> in the above B=1 case, that is, there is only one training sample per <b>Mini-batch</b>. The optimization process of <b>stochastic</b> <b>gradient</b> <b>descent</b> method is: Random <b>gradient</b> <b>descent</b> is an iterative update through each ...", "dateLastCrawled": "2021-07-07T20:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Gradient</b> <b>Descent</b> Algorithm: In-Depth Explanation-DeepVidhya", "url": "https://deepvidhya.com/blog/gradient-descent-algorithm:-in-depth-explanation-1208", "isFamilyFriendly": true, "displayUrl": "https://deepvidhya.com/blog/<b>gradient</b>-<b>descent</b>-algorithm:-in-depth-explanation-1208", "snippet": "Batch <b>gradient</b> <b>descent</b>/ Vanilla <b>gradient</b> <b>descent</b>; SGD(<b>Stochastic</b> <b>gradient</b> <b>descent</b>) <b>Mini-batch</b> <b>gradient</b> <b>descent</b>; Let\u2019s see how they differ from each other\u2019s. 1. Batch <b>gradient</b> <b>descent</b>/ Vanilla <b>gradient</b> <b>descent</b>. <b>Gradient</b> update rule: BGD uses the info of the whole training set to calculate the <b>gradient</b> of the cost function to the parameters. Disadvantages: Because this method calculates the <b>gradient</b> for the whole data set in one update, the calculation is extremely slow, it&#39;ll be very ...", "dateLastCrawled": "2021-12-16T22:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Gradient</b> <b>descent</b> Algorithm: In-Depth explanation-InsideAIML", "url": "https://www.insideaiml.com/blog/Gradient-descent-Algorithm%3A-In-Depth-explanation-1047", "isFamilyFriendly": true, "displayUrl": "https://www.insideaiml.com/blog/<b>Gradient</b>-<b>descent</b>-Algorithm:-In-Depth-explanation-1047", "snippet": "SGD (<b>Stochastic</b> <b>gradient</b> <b>descent</b>) 3. <b>Mini-batch</b> <b>gradient</b> <b>descent</b>. Let\u2019s see how they differ from each other\u2019s. Batch <b>gradient</b> <b>descent</b>/ Vanilla <b>gradient</b> <b>descent</b>. <b>Gradient</b> update rule: BGD uses the data of the entire training set to calculate the <b>gradient</b> of the cost function to the parameters: Disadvantages: Because this method calculates the <b>gradient</b> for the entire data set in one update, the calculation is very slow, it will be very tricky to encounter a large number of data sets, and ...", "dateLastCrawled": "2022-02-03T15:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>All about Gradient Descent and its variants</b> | by Anjana Yadav ...", "url": "https://medium.com/analytics-vidhya/all-about-gradient-descent-and-its-variants-d095be1a833b", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/<b>all-about-gradient-descent-and-its-variants</b>-d095be...", "snippet": "<b>Mini-Batch</b> <b>Gradient</b> <b>Descent</b> with N = 5 and K = 3 Thus we <b>can</b> clearly see a decrease in the oscillations and faster optimization. We have thus come so far learning about the improvements done in ...", "dateLastCrawled": "2022-01-27T13:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Optimization Algorithms in Neural Networks - <b>KDnuggets</b>", "url": "https://www.kdnuggets.com/2020/12/optimization-algorithms-neural-networks.html", "isFamilyFriendly": true, "displayUrl": "https://www.<b>kdnuggets</b>.com/2020/12/optimization-algorithms-neural-networks.html", "snippet": "<b>Mini-batch</b> <b>Gradient</b> <b>Descent</b> is relatively more stable than <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> (SGD) but does have oscillations as <b>gradient</b> steps are being taken in the direction of a sample of the training set and not the entire set as in BGD. It is observed that in SGD the updates take more number iterations <b>compared</b> to <b>gradient</b> <b>descent</b> to reach ...", "dateLastCrawled": "2022-01-31T07:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine Learning: Linear Regression</b> | by Shubham singh | Medium", "url": "https://shubhamsindal0098.medium.com/machine-learning-linear-regression-7b2e04454368", "isFamilyFriendly": true, "displayUrl": "https://shubhamsindal0098.medium.com/<b>machine-learning-linear-regression</b>-7b2e04454368", "snippet": "<b>Mini batch</b> <b>gradient</b> <b>descent</b> finds a balance between Batch <b>gradient</b> <b>descent</b> and <b>stochastic</b> <b>gradient</b>. It splits the dataset into several small batches and then updates the model parameters by calculating the <b>gradient</b> of small batches. The advantage of using <b>mini-batch</b> <b>gradient</b> <b>descent</b> is that it uses matrix operations to speed up the calculation. Since we are using mini-batches in the training process, the fluctuation of the cost function is a bit like <b>stochastic</b> <b>gradient</b> <b>descent</b>. However ...", "dateLastCrawled": "2022-01-31T21:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Hottest &#39;<b>gradient</b>-<b>descent</b>&#39; Answers - Cross Validated", "url": "https://stats.stackexchange.com/tags/gradient-descent/hot?filter=all", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/tags/<b>gradient</b>-<b>descent</b>/hot?filter=all", "snippet": "The batch size parameter is just one of the hyper-parameters you&#39;ll be tuning when you train a neural network with <b>mini-batch</b> <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> (SGD) and is data dependent. The most basic method of hyper-parameter search is to do a grid search over the learning rate and batch ... machine-learning neural-networks <b>gradient</b>-<b>descent</b> backpropagation. answered Mar 11 &#39;15 at 6:55. sabalaba. 1,660 13 13 silver badges 12 12 bronze badges. 75 Why is Newton&#39;s method not widely used in machine ...", "dateLastCrawled": "2022-01-09T13:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) <b>Spatial interpolation using conditional generative adversarial</b> ...", "url": "https://www.researchgate.net/publication/332450640_Spatial_interpolation_using_conditional_generative_adversarial_neural_networks", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/332450640_Spatial_interpolation_using...", "snippet": "The network is trained using <b>mini-batch</b> <b>stochastic</b> <b>gradient</b> <b>descent</b> (SGD) with a batch size of 64. The training dataset with 48,000 DEM images is randomly divided", "dateLastCrawled": "2022-01-27T17:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Tree-CNN: A hierarchical <b>Deep Convolutional Neural</b> Network for ...", "url": "https://www.sciencedirect.com/science/article/pii/S0893608019302710", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0893608019302710", "snippet": "The networks are trained using <b>mini-batch</b> <b>stochastic</b> <b>gradient</b> <b>descent</b> with fixed momentum of 0.9. Dropout (Srivastava, Hinton, Krizhevsky, Sutskever, &amp; Salakhutdinov, 2014) is used between the final fully connected layers, and between pooling layers to regularize the network.", "dateLastCrawled": "2022-01-19T17:16:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Batch vs <b>Mini-batch</b> vs <b>Stochastic Gradient Descent</b> with Code Examples ...", "url": "https://medium.datadriveninvestor.com/batch-vs-mini-batch-vs-stochastic-gradient-descent-with-code-examples-cd8232174e14", "isFamilyFriendly": true, "displayUrl": "https://medium.datadriveninvestor.com/batch-vs-<b>mini-batch</b>-vs-<b>stochastic</b>-<b>gradient</b>...", "snippet": "It is possible to use only the <b>Mini-batch</b> <b>Gradient Descent</b> code to implement all versions of <b>Gradient Descent</b>, you just need to set the <b>mini_batch</b>_size equals one to <b>Stochastic</b> GD or the number of training examples to Batch GD. Thus, the main difference between Batch, <b>Mini-batch</b>, and <b>Stochastic Gradient Descent</b> is the number of examples used for each epoch and the time and effort necessary to reach the global minimum value of the Cost Function.", "dateLastCrawled": "2022-01-27T13:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Batch, <b>Mini Batch</b> &amp; <b>Stochastic</b> <b>Gradient Descent</b> | by Sushant Patrikar ...", "url": "https://towardsdatascience.com/batch-mini-batch-stochastic-gradient-descent-7a62ecba642a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/batch-<b>mini-batch</b>-<b>stochastic</b>-<b>gradient-descent</b>-7a62ecba642a", "snippet": "<b>Mini Batch</b> <b>Gradient Descent</b>. We have seen the Batch <b>Gradient Descent</b>. We have also seen the <b>Stochastic</b> <b>Gradient Descent</b>. Batch <b>Gradient Descent</b> can be used for smoother curves. SGD can be used when the dataset is large. Batch <b>Gradient Descent</b> converges directly to minima. SGD converges faster for larger datasets. But, since in SGD we use only ...", "dateLastCrawled": "2022-02-02T11:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Batch vs <b>Mini-batch vs Stochastic Gradient Descent</b> with Code Examples", "url": "https://edgeaiguru.com/Batch-vs-Mini-batch-vs-Stochastic-Gradient-Descent", "isFamilyFriendly": true, "displayUrl": "https://edgeaiguru.com/Batch-vs-<b>Mini-batch-vs-Stochastic-Gradient-Descent</b>", "snippet": "It is possible to use only the <b>Mini-batch</b> <b>Gradient</b> <b>Descent</b> code to implement all versions of <b>Gradient</b> <b>Descent</b>, you just need to set the <b>mini_batch</b>_size equals one to <b>Stochastic</b> GD or to the number of training examples to Batch GD. Thus, the main difference between Batch, <b>Mini-batch</b> and <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> is the number of examples used for each epoch and the time and effort necessary to reach the global minimum value of the Cost Function.", "dateLastCrawled": "2022-01-27T23:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "13.6 <b>Stochastic and mini-batch gradient descent</b>", "url": "https://kenndanielso.github.io/mlrefined/blog_posts/13_Multilayer_perceptrons/13_6_Stochastic_and_minibatch_gradient_descent.html", "isFamilyFriendly": true, "displayUrl": "https://kenndanielso.github.io/.../13_6_<b>Stochastic_and_minibatch_gradient_descent</b>.html", "snippet": "It is noteworthy that because moderately accurate solutions (provided by a moderate amount of minimization of a cost function) tend to perform reasonably well in <b>machine</b> <b>learning</b> applications, and because with large datasets a random initialization will tend to lie far from a convergent point, in many cases even a single iteration of <b>stochastic</b>/<b>mini-batch</b> <b>gradient</b> <b>descent</b> can provide a good solution.", "dateLastCrawled": "2022-02-02T00:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Gradient</b> <b>Descent</b>: A Quick, Simple Introduction | Built In", "url": "https://builtin.com/data-science/gradient-descent", "isFamilyFriendly": true, "displayUrl": "https://builtin.com/data-science/<b>gradient</b>-<b>descent</b>", "snippet": "Types of <b>gradient</b> <b>descent</b>: batch, <b>stochastic</b>, <b>mini-batch</b>; Introduction to <b>Gradient</b> <b>Descent</b>. <b>Gradient</b> <b>descent</b> is an optimization algorithm that&#39;s used when training a <b>machine</b> <b>learning</b> model. It&#39;s based on a convex function and tweaks its parameters iteratively to minimize a given function to its local minimum. What is <b>Gradient</b> <b>Descent</b>? <b>Gradient</b> <b>Descent</b> is an optimization algorithm for finding a local minimum of a differentiable function. <b>Gradient</b> <b>descent</b> is simply used in <b>machine</b> <b>learning</b> to ...", "dateLastCrawled": "2022-02-02T07:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Understanding the 3 Primary <b>Types of Gradient Descent</b> | by ODSC - Open ...", "url": "https://medium.com/odscjournal/understanding-the-3-primary-types-of-gradient-descent-987590b2c36", "isFamilyFriendly": true, "displayUrl": "https://medium.com/odscjournal/understanding-the-3-primary-<b>types-of-gradient-descent</b>...", "snippet": "<b>Mini Batch</b> <b>Gradient Descent</b> is commonly used for deep <b>learning</b> problems. Conclusion This article should give you the basic motivation for the <b>gradient descent</b> process in <b>machine</b> <b>learning</b>.", "dateLastCrawled": "2022-02-03T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Empirical Risk Minimization and <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> for ...", "url": "http://proceedings.mlr.press/v89/veitch19a/veitch19a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v89/veitch19a/veitch19a.pdf", "snippet": "models, <b>mini-batch</b> <b>stochastic</b> <b>gradient</b> <b>descent</b> (SGD) can e\ufb03ciently solve the minimization problem (albeit, approximately). The ease of SGD comes from the de\ufb01- nition of the empirical risk as the expectation over a randomly subsampled example: the <b>gradient</b> of the loss on a randomly subsampled example is an unbiased es-timate of the <b>gradient</b> of the empirical risk. Combined with automatic di\ufb00erentiation, this provides a turnkey approach to \ufb01tting <b>machine</b>-<b>learning</b> models. Returning to ...", "dateLastCrawled": "2021-09-18T06:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Stochastic Gradient Descent</b> \u2014 Clearly Explained !! | by Aishwarya V ...", "url": "https://towardsdatascience.com/stochastic-gradient-descent-clearly-explained-53d239905d31", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>stochastic-gradient-descent</b>-clearly-explained-53d239905d31", "snippet": "<b>Stochastic gradient descent</b> is a very popular and common algorithm used in various <b>Machine</b> <b>Learning</b> algorithms, most importantly forms the basis of Neural Networks. In this article, I have tried my\u2026 Get started. Open in app. Sign in. Get started. Follow. 618K Followers \u00b7 Editors&#39; Picks Features Deep Dives Grow Contribute. About. Get started. Open in app. <b>Stochastic Gradient Descent</b> \u2014 Clearly Explained !! Aishwarya V Srinivasan. Sep 7, 2019 \u00b7 4 min read. <b>Stochastic gradient descent</b> is a ...", "dateLastCrawled": "2022-02-02T12:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Basics and Beyond: <b>Gradient Descent</b> | by Kumud Lakara | The Startup ...", "url": "https://medium.com/swlh/basics-and-beyond-gradient-descent-87fa964c31dd", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/basics-and-beyond-<b>gradient-descent</b>-87fa964c31dd", "snippet": "3. <b>Mini-batch Gradient Descent</b>. This is actually the best of both worlds. It accounts for the computational expenses in case of <b>batch gradient descent</b> and the high variance in case of SGD. Mini ...", "dateLastCrawled": "2021-05-02T18:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "11.5. <b>Minibatch</b> <b>Stochastic</b> <b>Gradient Descent</b> \u2014 Dive into Deep <b>Learning</b> 0 ...", "url": "http://d2l.ai/chapter_optimization/minibatch-sgd.html", "isFamilyFriendly": true, "displayUrl": "d2l.ai/chapter_optimization/<b>minibatch</b>-sgd.html", "snippet": "So far we encountered two extremes in the approach to <b>gradient</b> based <b>learning</b>: Section 11.3 uses the full dataset to compute gradients and to update parameters, one pass at a time. Conversely Section 11.4 processes one observation at a time to make progress. Each of them has its own drawbacks. <b>Gradient Descent</b> is not particularly data efficient whenever data is very similar. <b>Stochastic</b> <b>Gradient Descent</b> is not particularly computationally efficient since CPUs and GPUs cannot exploit the full ...", "dateLastCrawled": "2022-02-01T18:13:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(mini-batch stochastic gradient descent)  is like +(descending a mountain)", "+(mini-batch stochastic gradient descent) is similar to +(descending a mountain)", "+(mini-batch stochastic gradient descent) can be thought of as +(descending a mountain)", "+(mini-batch stochastic gradient descent) can be compared to +(descending a mountain)", "machine learning +(mini-batch stochastic gradient descent AND analogy)", "machine learning +(\"mini-batch stochastic gradient descent is like\")", "machine learning +(\"mini-batch stochastic gradient descent is similar\")", "machine learning +(\"just as mini-batch stochastic gradient descent\")", "machine learning +(\"mini-batch stochastic gradient descent can be thought of as\")", "machine learning +(\"mini-batch stochastic gradient descent can be compared to\")"]}