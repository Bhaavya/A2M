{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Mask RCNN <b>Instance Segmentation with PyTorch</b> - Learn OpenCV | OpenCV ...", "url": "https://learnopencv.com/mask-r-cnn-instance-segmentation-with-pytorch/", "isFamilyFriendly": true, "displayUrl": "https://learnopencv.com/mask-r-cnn-<b>instance-segmentation-with-pytorch</b>", "snippet": "If you are a beginner, think of the convolutional layers as <b>a black</b> <b>box</b> <b>that takes</b> in a 3-channel <b>input</b> image, <b>and outputs</b> an \u201cimage\u201d with a much smaller spatial dimension (7\u00d77), but a large number of channels (512). Region Proposal Network (RPN). The output of the convolutional layers is used to train a network that proposes regions that enclose objects. Classifier: The same feature map is also used to train a classifier that assigns a label to the object inside the <b>box</b>. Also, recall ...", "dateLastCrawled": "2022-01-30T03:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Recurrent neural networks: building a custom</b> LSTM cell | AI Summer", "url": "https://theaisummer.com/understanding-lstm/", "isFamilyFriendly": true, "displayUrl": "https://theaisummer.com/understanding-lstm", "snippet": "And, for a lot of people in the computer vision community, recurrent neural networks (RNNs) are <b>like</b> this. More or less, another <b>black</b> <b>box</b> in the pile. However, in this tutorial, we will attempt to open the <b>RNN</b> magic <b>black</b> <b>box</b> and unravel its mysteries! Even though I have come across hundreds of tutorials on LSTM\u2019s out there, I felt there was something missing. Therefore, I honestly hope that this tutorial serves as a modern guide to RNNs. We try to deal with multiple details of practical ...", "dateLastCrawled": "2022-01-30T16:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Attention</b>, CNN and what not for Text Classification | by Rahul Agarwal ...", "url": "https://towardsdatascience.com/nlp-learning-series-part-3-attention-cnn-and-what-not-for-text-classification-4313930ed566", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/nlp-learning-series-part-3-<b>attention</b>-cnn-and-what-not...", "snippet": "For a most simplistic explanation of Bidirectional <b>RNN</b>, think of <b>RNN</b> cell as <b>a black</b> <b>box</b> taking as <b>input</b> a hidden state(a vector) and a word vector and giving out an output vector and the next hidden state. This <b>box</b> has some weights which are to be tuned using Backpropagation of the losses. Also, the same cell is applied to all the words so that the weights are shared across the words in the sentence. This phenomenon is called weight-sharing. Hidden state, Word vector -&gt;(<b>RNN</b> Cell) -&gt; Output ...", "dateLastCrawled": "2022-01-31T11:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Black</b>-<b>Box</b> Attacks against <b>RNN</b> based Malware Detection Algorithms ...", "url": "https://www.researchgate.net/publication/317088315_Black-Box_Attacks_against_RNN_based_Malware_Detection_Algorithms", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/317088315_<b>Black</b>-<b>Box</b>_Attacks_against_<b>RNN</b>_based...", "snippet": "Generally in <b>black</b> <b>box</b> attack, surrogate model is created by making guess on internal structure of target model using <b>input</b> and output [81], [240]. In addition, in a gray <b>box</b> attack [241], a type ...", "dateLastCrawled": "2022-01-30T13:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "python - TFLearn <b>RNN</b> output is always constant - New to TFLearn - Stack ...", "url": "https://stackoverflow.com/questions/48873833/tflearn-rnn-output-is-always-constant-new-to-tflearn", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/48873833", "snippet": "The GA needs <b>a &quot;black</b> <b>box</b>&quot; model it can use to generate <b>a prediction</b> of the drone behavior given a set of inputs. NN Design: My idea for creating such a model is to utilize a <b>RNN</b> <b>that takes</b> a time history of ESC motor controller command values as inputs (4 inputs, 1 for each motor), and spits out the corresponding Euler angles (3 <b>outputs</b>, roll, pitch, yaw). I have a custom flight controller I designed/wrote that enables me to log any necessary data to an SD card, ie the motor <b>outputs</b> and ...", "dateLastCrawled": "2022-01-07T08:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Neural Networks Explained: Difference between CNN</b> &amp; <b>RNN</b> | Coding Ninjas ...", "url": "https://www.codingninjas.com/blog/2020/11/17/neural-networks-explained-difference-between-cnn-and-rnn/", "isFamilyFriendly": true, "displayUrl": "https://www.codingninjas.com/.../<b>neural-networks-explained-difference-between-cnn</b>-and-<b>rnn</b>", "snippet": "It <b>takes</b> a fixed <b>input</b> and gives a fixed output, which reduces the flexibility of the CNN but helps with computing results faster. They require fewer hyperparameters and less supervision, but are very resource-intensive and needs huge training data to give the most accurate results. The common applications where CNNs are used are object detection, image classification, biometrics, medical analysis and image segmentation. Recurrent Neural Network (<b>RNN</b>): These are multi-layer neural networks ...", "dateLastCrawled": "2022-01-05T10:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "4. Recurrent Neural Networks - <b>Neural networks and deep learning</b> [Book]", "url": "https://www.oreilly.com/library/view/neural-networks-and/9781492037354/ch04.html", "isFamilyFriendly": true, "displayUrl": "https://www.oreilly.com/library/view/neural-networks-and/9781492037354/ch04.html", "snippet": "stacked_<b>rnn</b>_<b>outputs</b> = tf.reshape(<b>rnn</b>_<b>outputs</b>, [-1, n_neurons]) stacked_<b>outputs</b> = tf.layers.dense(stacked_<b>rnn</b>_<b>outputs</b>, n_<b>outputs</b>) <b>outputs</b> = tf.reshape(stacked_<b>outputs</b>, [-1, n_steps, n_<b>outputs</b>]) The rest of the code is the same as earlier. This can provide a significant speed boost since there is just one fully connected layer instead of one per time step. Creative <b>RNN</b>. Now that we have a model that can predict the future, we can use it to generate some creative sequences, as explained at the ...", "dateLastCrawled": "2022-01-28T19:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "What is <b>Recurrent Neural Network</b> | <b>Recurrent Neural Network</b> Explained", "url": "https://www.mygreatlearning.com/blog/recurrent-neural-network/", "isFamilyFriendly": true, "displayUrl": "https://www.mygreatlearning.com/blog/<b>recurrent-neural-network</b>", "snippet": "It is a subset of machine learning which <b>takes</b> the <b>input</b> data and performs a function. This function with time progressively gets better at the <b>prediction</b>. The whole idea of neural network algorithms is inspired by the structure and function of the brain called artificial neural networks. Deep learning techniques have the capability to extract features from given complex data and solve the dimension reduction problem. These algorithms by themselves can figure out edges and the patterns and ...", "dateLastCrawled": "2022-02-02T17:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A beginner\u2019s guide to neural networks and deep learning", "url": "https://blog.superannotate.com/guide-to-neural-networks-and-deep-learning/", "isFamilyFriendly": true, "displayUrl": "https://blog.superannotate.com/guide-to-neural-networks-and-deep-learning", "snippet": "The first layer is the <b>input</b> layer which directly <b>takes</b> in the feature inputs, whereas the last or the output layer creates the resulting <b>outputs</b>. Any layers in between are known as hidden layers because they don&#39;t directly &quot;see&quot; the feature inputs or <b>outputs</b>. Neurons of one layer are connected with neurons of the next layer by channels. The result of the activation function determines if the particular neuron will get activated or not. An activated neuron transmits data to the next layer ...", "dateLastCrawled": "2022-01-19T21:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Visualising <b>LSTM</b> Activations in Keras | by Praneet Bomma | Towards Data ...", "url": "https://towardsdatascience.com/visualising-lstm-activations-in-keras-b50206da96ff", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/visualising-<b>lstm</b>-activations-in-keras-b50206da96ff", "snippet": "Activation Colour Levels from 0 to 1 Step 8: Get Predictions. get_predictions function randomly chooses an <b>input</b> seed sequence and gets the predicted sequence for that seed sequence.visualize function <b>takes</b> as <b>input</b> the predicted sequence, the sigmoid values for each character in the sequence and the cell number to visualise. Based on the value of the output, character is printed with an appropriate background colour.", "dateLastCrawled": "2022-02-02T15:06:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>RNN</b>- and LSTM-Based Soft Sensors Transferability for an Industrial Process", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7865368/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7865368", "snippet": "<b>Black</b>-<b>box</b> machine learning (ML) methods are often used as an efficient tool to implement SSs. Many efforts are, however, required to properly select <b>input</b> variables, model class, model order and the needed hyperparameters. The aim of this work was to investigate the possibility to transfer the knowledge acquired in the design of a SS for a given process to a <b>similar</b> one. This has been approached as a transfer learning problem from a source to a target domain. The implementation of a transfer ...", "dateLastCrawled": "2022-01-12T03:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Neural Language Models</b> | by Arun Jagota | Towards Data Science", "url": "https://towardsdatascience.com/neural-language-models-32bec14d01dc", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>neural-language-models</b>-32bec14d01dc", "snippet": "Basic <b>RNN</b>. As a <b>black</b> <b>box</b>, i.e. from the <b>input</b>-output interface, an <b>RNN</b> looks like a feedforward neural network at first glance. It inputs a vector x <b>and outputs</b> a vector y. There is a twist though. The <b>RNN</b> expects to receive, as <b>input</b>, a sequence of vectors x(1), \u2026, x(T), one vector at a time, <b>and outputs</b> a sequence of vectors y(1), \u2026, y(T). The output y(t) at time t is influenced not only by x(t), rather by the entire sequence x(1), \u2026, x(t) of inputs received till then. This is what ...", "dateLastCrawled": "2022-01-31T14:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Black</b>-<b>Box</b> Attacks against <b>RNN</b> based Malware Detection Algorithms ...", "url": "https://www.researchgate.net/publication/317088315_Black-Box_Attacks_against_RNN_based_Malware_Detection_Algorithms", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/317088315_<b>Black</b>-<b>Box</b>_Attacks_against_<b>RNN</b>_based...", "snippet": "Generally in <b>black</b> <b>box</b> attack, surrogate model is created by making guess on internal structure of target model using <b>input</b> and output [81], [240]. In addition, in a gray <b>box</b> attack [241], a type ...", "dateLastCrawled": "2022-01-30T13:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "NLP <b>Learning Series: Part 3 - Attention</b>, CNN and what not for Text ...", "url": "https://mlwhiz.com/blog/2019/03/09/deeplearning_architectures_text_classification/", "isFamilyFriendly": true, "displayUrl": "https://mlwhiz.com/blog/2019/03/09/deeplearning_architectures_text_classification", "snippet": "For a most simplistic explanation of Bidirectional <b>RNN</b>, think of <b>RNN</b> cell as a <b>black</b> <b>box</b> taking as <b>input</b> a hidden state(a vector) and a word vector and giving out an output vector and the next hidden state. This <b>box</b> has some weights which are to be tuned using Backpropagation of the losses. Also, the same cell is applied to all the words so that the weights are shared across the words in the sentence. This phenomenon is called weight-sharing.", "dateLastCrawled": "2022-01-29T06:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Neural Networks Explained: Difference between CNN</b> &amp; <b>RNN</b> | Coding Ninjas ...", "url": "https://www.codingninjas.com/blog/2020/11/17/neural-networks-explained-difference-between-cnn-and-rnn/", "isFamilyFriendly": true, "displayUrl": "https://www.codingninjas.com/.../<b>neural-networks-explained-difference-between-cnn</b>-and-<b>rnn</b>", "snippet": "The <b>input</b> is first fed to CNN layers and the output from CNN is fed to <b>RNN</b> layers, which helps solve both the temporal and spatial problems. Some common examples of such complex problems are video labelling, gesture recognition, DNA sequence <b>prediction</b>, etc. To encapsulate, both CNN and <b>RNN</b> are very popular variants of Neural Networks, each ...", "dateLastCrawled": "2022-01-05T10:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Recurrent neural networks: building a custom</b> LSTM cell | AI Summer", "url": "https://theaisummer.com/understanding-lstm/", "isFamilyFriendly": true, "displayUrl": "https://theaisummer.com/understanding-lstm", "snippet": "Basically, a single cell receives as <b>input</b> the cell and hidden state from the previous timestep, as well as the <b>input</b> vector from the current timestep. Each LSTM cell <b>outputs</b> the new cell state and a hidden state, which will be used for processing the next timestep. The output of the cell, if needed for example in the next layer, is its hidden ...", "dateLastCrawled": "2022-01-30T16:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "4. Recurrent Neural Networks - <b>Neural networks and deep learning</b> [Book]", "url": "https://www.oreilly.com/library/view/neural-networks-and/9781492037354/ch04.html", "isFamilyFriendly": true, "displayUrl": "https://www.oreilly.com/library/view/neural-networks-and/9781492037354/ch04.html", "snippet": "The following code builds the same <b>RNN</b> again, but this time it <b>takes</b> a single <b>input</b> placeholder of shape [None, n_steps, n_inputs] where the first dimension is the mini-batch size. Then it extracts the list of <b>input</b> sequences for each time step. X_seqs is a Python list of n_steps tensors of shape [None, n_inputs], where once again the first dimension is the mini-batch size. To do this, we first swap the first two dimensions using the transpose() function, so that the time steps are now the ...", "dateLastCrawled": "2022-01-28T19:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "RStudio AI Blog: <b>Time Series Forecasting with Recurrent Neural Networks</b>", "url": "https://blogs.rstudio.com/ai/posts/2017-12-20-time-series-forecasting-with-recurrent-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://blogs.rstudio.com/ai/posts/2017-12-20-<b>time-series-forecasting-with-recurrent</b>...", "snippet": "Before you start using <b>black</b>-<b>box</b> deep-learning models to solve the temperature-<b>prediction</b> problem, let\u2019s try a simple, common-sense approach. It will serve as a sanity check, and it will establish a baseline that you\u2019ll have to beat in order to demonstrate the usefulness of more-advanced machine-learning models. Such common-sense baselines can be useful when you\u2019re approaching a new problem for which there is no known solution (yet). A classic example is that of unbalanced ...", "dateLastCrawled": "2022-01-29T06:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Deep Learning (5/5): Sequence Models</b> - Dani&#39;s Braindump", "url": "https://tiefenauer.github.io/ml/deep-learning/5", "isFamilyFriendly": true, "displayUrl": "https://tiefenauer.github.io/ml/deep-learning/5", "snippet": "The opposite is also possible: A <b>RNN</b> can take only a single value as <b>input</b> and produce a sequence as an output by re-using the previous <b>outputs</b> to make the next <b>prediction</b>. Such an architecture is called one-to-many. It could be used for example in a <b>RNN</b> that generates music by taking a genre as an <b>input</b> and generates a sequence of notes as an ...", "dateLastCrawled": "2022-02-03T17:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "A beginner\u2019s guide to neural networks and deep learning", "url": "https://blog.superannotate.com/guide-to-neural-networks-and-deep-learning/", "isFamilyFriendly": true, "displayUrl": "https://blog.superannotate.com/guide-to-neural-networks-and-deep-learning", "snippet": "The first layer is the <b>input</b> layer which directly <b>takes</b> in the feature inputs, whereas the last or the output layer creates the resulting <b>outputs</b>. Any layers in between are known as hidden layers because they don&#39;t directly &quot;see&quot; the feature inputs or <b>outputs</b>. Neurons of one layer are connected with neurons of the next layer by channels. The result of the activation function determines if the particular neuron will get activated or not. An activated neuron transmits data to the next layer ...", "dateLastCrawled": "2022-01-19T21:33:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "NLP <b>Learning Series: Part 3 - Attention</b>, CNN and what not for Text ...", "url": "https://mlwhiz.com/blog/2019/03/09/deeplearning_architectures_text_classification/", "isFamilyFriendly": true, "displayUrl": "https://mlwhiz.com/blog/2019/03/09/deeplearning_architectures_text_classification", "snippet": "For a most simplistic explanation of Bidirectional <b>RNN</b>, think of <b>RNN</b> cell as a <b>black</b> <b>box</b> taking as <b>input</b> a hidden state(a vector) and a word vector and giving out an output vector and the next hidden state. This <b>box</b> has some weights which are to be tuned using Backpropagation of the losses. Also, the same cell is applied to all the words so that the weights are shared across the words in the sentence. This phenomenon is called weight-sharing.", "dateLastCrawled": "2022-01-29T06:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) Real-Time <b>Black</b>-<b>Box Modelling with Recurrent Neural Networks</b>", "url": "https://www.researchgate.net/publication/335714458_Real-Time_Black-Box_Modelling_with_Recurrent_Neural_Networks", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/335714458_Real-Time_<b>Black</b>-<b>Box</b>_Modelling_with...", "snippet": "Measurements of the <b>input</b>- and output-signals from the analog reference devices are used to adjust a digital model treating the reference device as a \u2018<b>black</b>-<b>box</b>\u2019. With this technique the ...", "dateLastCrawled": "2021-12-22T19:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 2, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Artificial neural network</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Artificial_neural_network", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Artificial_neural_network</b>", "snippet": "This <b>can</b> <b>be thought</b> of as learning with a &quot;teacher&quot;, in the form of a function that provides continuous feedback on the quality of solutions obtained thus far. Unsupervised learning. In unsupervised learning, <b>input</b> data is given along with the cost function, some function of the data and the network&#39;s output. The cost function is dependent on the task (the model domain) and any a priori assumptions (the implicit properties of the model, its parameters and the observed variables). As a ...", "dateLastCrawled": "2022-02-07T09:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) Learning to Learn for Global Optimization of <b>Black</b> <b>Box</b> Functions", "url": "https://www.researchgate.net/publication/310122477_Learning_to_Learn_for_Global_Optimization_of_Black_Box_Functions", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/310122477_Learning_to_Learn_for_Global...", "snippet": "After learning, the <b>RNN</b> <b>can</b> be applied to learn policies in reinforcement learning, as well as other <b>black</b>-<b>box</b> learning tasks, including continuous correlated bandits and experimental design. We ...", "dateLastCrawled": "2022-01-28T21:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>LESSON 8: RECURRENT NEURAL NETWORKS</b>", "url": "https://caisplusplus.usc.edu/blog/curriculum/lesson8", "isFamilyFriendly": true, "displayUrl": "https://caisplusplus.usc.edu/blog/curriculum/lesson8", "snippet": "<b>Input</b> vectors are in red, output vectors are in blue, and green vectors hold the <b>RNN</b>&#39;s state. From left to right: (1) Vanilla mode of processing without <b>RNN</b>, from fixed-sized <b>input</b> to fixed-sized output (e.g. image classification). (2) Sequence output (e.g. image captioning <b>takes</b> an image <b>and outputs</b> a sentence of words).", "dateLastCrawled": "2022-01-19T02:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "python - TFLearn <b>RNN</b> output is always constant - New to TFLearn - Stack ...", "url": "https://stackoverflow.com/questions/48873833/tflearn-rnn-output-is-always-constant-new-to-tflearn", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/48873833", "snippet": "The GA needs a &quot;<b>black</b> <b>box</b>&quot; model it <b>can</b> use to generate <b>a prediction</b> of the drone behavior given a set of inputs. NN Design: My idea for creating such a model is to utilize a <b>RNN</b> <b>that takes</b> a time history of ESC motor controller command values as inputs (4 inputs, 1 for each motor), and spits out the corresponding Euler angles (3 <b>outputs</b>, roll, pitch, yaw). I have a custom flight controller I designed/wrote that enables me to log any necessary data to an SD card, ie the motor <b>outputs</b> and ...", "dateLastCrawled": "2022-01-07T08:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "What is the advantage of using an <b>RNN</b> and LSTM over traditional time ...", "url": "https://www.quora.com/What-is-the-advantage-of-using-an-RNN-and-LSTM-over-traditional-time-series-methods-like-ARMA-ARIMA-etc-for-streaming-data", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-the-advantage-of-using-an-<b>RNN</b>-and-LSTM-over-traditional...", "snippet": "Answer (1 of 2): The main advantage is time. You <b>can</b> use an LSTM in almost no time and start making predictions. Just transform the data from a one-dimensional array to a two-dimensional by lagging the time series for a number of steps and then start your train-test-validation. A nice method fo...", "dateLastCrawled": "2022-01-07T20:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Frontiers | <b>Illuminating the Black Box: Interpreting</b> Deep Neural ...", "url": "https://www.frontiersin.org/articles/10.3389/fpsyt.2020.551299/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/fpsyt.2020.551299", "snippet": "Instead of exploring attention weights, Aken et al. <b>takes</b> advantage of the position-preserving nature of a BERT model in which the number of positions is constant across layers, and thus, the output of each layer <b>can</b> be perceived as a transformation of the <b>input</b> at the same position. They analyze the tokens produced by each attention layer from the BERT model, probe their properties with specific tasks, perform principle component analysis, and visualize clusters of token <b>outputs</b> at each layer.", "dateLastCrawled": "2022-02-03T17:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "An overview of model <b>explainability</b> in modern machine learning | by Rui ...", "url": "https://towardsdatascience.com/an-overview-of-model-explainability-in-modern-machine-learning-fc0f22c8c29a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/an-overview-of-model-<b>explainability</b>-in-modern-machine...", "snippet": "1. Select a dataset X that you <b>can</b> run your <b>black</b> <b>box</b> model on. 2. For the selected dataset X, get the predictions of your <b>black</b> <b>box</b> model. 3. Select an interpretable model type (e.g. linear model or decision tree) 4. Train the interpretable model on the dataset X and the <b>black</b> <b>box</b>&#39;s predictions. 5. Measure how well the surrogate model ...", "dateLastCrawled": "2022-01-31T22:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Can</b> I use convolutional neural networks to approximate an unknown ...", "url": "https://www.quora.com/Can-I-use-convolutional-neural-networks-to-approximate-an-unknown-function-mapping-A-simple-feedforward-network-would-work-but-I-want-to-know-about-other-networks-CNN-RNN-etc", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>Can</b>-I-use-convolutional-neural-networks-to-approximate-an...", "snippet": "Answer: Most other types of neural networks have what are called inductive biases, baked-in assumptions about the structure of the data that constrain what functions the network <b>can</b> express. This is done intentionally to reduce the number of parameters that the model needs. Let\u2019s take an image cl...", "dateLastCrawled": "2022-01-16T23:41:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>RNN</b>- and LSTM-Based Soft Sensors Transferability for an Industrial Process", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7865368/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7865368", "snippet": "<b>Black</b>-<b>box</b> machine learning (ML) methods are often used as an efficient tool to implement SSs. Many efforts are, however, required to properly select <b>input</b> variables, model class, model order and the needed hyperparameters. The aim of this work was to investigate the possibility to transfer the knowledge acquired in the design of a SS for a given process to a similar one. This has been approached as a transfer learning problem from a source to a target domain. The implementation of a transfer ...", "dateLastCrawled": "2022-01-12T03:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Process structure-based recurrent neural network modeling for model ...", "url": "https://www.sciencedirect.com/science/article/pii/S095915241930825X", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S095915241930825X", "snippet": "Specifically, a <b>black</b> <b>box</b> NN model <b>that takes</b> all available inputs to predict the <b>outputs</b> of interest is preferred in developing a dynamic process model for the integrated upstream and downstream processes as it is easy to implement using open-source machine learning software and is able to account for all possible <b>input</b>-output relationships.", "dateLastCrawled": "2022-01-08T13:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Black</b>-<b>Box</b> Attacks against <b>RNN</b> based Malware Detection Algorithms ...", "url": "https://www.researchgate.net/publication/317088315_Black-Box_Attacks_against_RNN_based_Malware_Detection_Algorithms", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/317088315_<b>Black</b>-<b>Box</b>_Attacks_against_<b>RNN</b>_based...", "snippet": "Generally in <b>black</b> <b>box</b> attack, surrogate model is created by making guess on internal structure of target model using <b>input</b> and output [81], [240]. In addition, in a gray <b>box</b> attack [241], a type ...", "dateLastCrawled": "2022-01-30T13:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "RNNRepair: Automatic <b>RNN</b> Repair via Model-based Analysis", "url": "http://proceedings.mlr.press/v139/xie21b/xie21b.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v139/xie21b/xie21b.pdf", "snippet": "Due to their <b>black</b>-<b>box</b> nature, it is rather challenging to interpret and properly re-pair these incorrect behaviors. This paper focuses on interpreting and repairing the incorrect behav-iors of Recurrent Neural Networks (RNNs). We propose a lightweight model-based approach (RN-NRepair) to help understand and repair incorrect behaviors of an <b>RNN</b>. Speci\ufb01cally, we build an in\ufb02uence model to characterize the stateful and statistical behaviors of an <b>RNN</b> over all the train-ing data and to ...", "dateLastCrawled": "2022-01-27T14:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "What is the advantage of using an <b>RNN</b> and LSTM over traditional time ...", "url": "https://www.quora.com/What-is-the-advantage-of-using-an-RNN-and-LSTM-over-traditional-time-series-methods-like-ARMA-ARIMA-etc-for-streaming-data", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-the-advantage-of-using-an-<b>RNN</b>-and-LSTM-over-traditional...", "snippet": "Answer (1 of 2): The main advantage is time. You <b>can</b> use an LSTM in almost no time and start making predictions. Just transform the data from a one-dimensional array to a two-dimensional by lagging the time series for a number of steps and then start your train-test-validation. A nice method fo...", "dateLastCrawled": "2022-01-07T20:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "RStudio AI Blog: <b>Time Series Forecasting with Recurrent Neural Networks</b>", "url": "https://blogs.rstudio.com/ai/posts/2017-12-20-time-series-forecasting-with-recurrent-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://blogs.rstudio.com/ai/posts/2017-12-20-<b>time-series-forecasting-with-recurrent</b>...", "snippet": "Before you start using <b>black</b>-<b>box</b> deep-learning models to solve the temperature-<b>prediction</b> problem, let\u2019s try a simple, common-sense approach. It will serve as a sanity check, and it will establish a baseline that you\u2019ll have to beat in order to demonstrate the usefulness of more-advanced machine-learning models. Such common-sense baselines <b>can</b> be useful when you\u2019re approaching a new problem for which there is no known solution (yet). A classic example is that of unbalanced ...", "dateLastCrawled": "2022-01-29T06:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Text Backdoor Detection Using an Interpretable <b>RNN</b> Abstract Model ...", "url": "https://www.researchgate.net/publication/353742958_Text_Backdoor_Detection_Using_an_Interpretable_RNN_Abstract_Model", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/353742958_Text_Backdoor_Detection_Using_an...", "snippet": "In this paper we pursue an alternative approach where an accurate high-level model <b>can</b> be automatically constructed from observations of a given <b>black</b>-<b>box</b> embedded system. We adapt algorithms for ...", "dateLastCrawled": "2022-01-25T17:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Recurrent neural networks: building a custom</b> LSTM cell | AI Summer", "url": "https://theaisummer.com/understanding-lstm/", "isFamilyFriendly": true, "displayUrl": "https://theaisummer.com/understanding-lstm", "snippet": "The LSTM cell equations were written based on Pytorch documentation because you will probably use the existing layer in your project. In the original paper, c t \u2212 1 \\textbf{c}_{t-1} c t \u2212 1 is included in the Equation (1) and (2), but you <b>can</b> omit it. For consistency reasons with the Pytorch docs, I will not include these computations in the code.", "dateLastCrawled": "2022-01-30T16:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "How <b>can</b> I <b>choose from logistic regression, SVM, ANN, CNN</b>, <b>RNN</b>? - Quora", "url": "https://www.quora.com/How-can-I-choose-from-logistic-regression-SVM-ANN-CNN-RNN", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/How-<b>can</b>-I-<b>choose-from-logistic-regression-SVM-ANN-CNN</b>-<b>RNN</b>", "snippet": "Answer (1 of 2): Am not sure if this is a well thought question or just a random troll, however my 2 cents on this. 1. As a data scientist before approaching an algorithm the first thing I ask myself is what kind of dataset do you have. If the problem is Classification ( predicting the likelihoo...", "dateLastCrawled": "2022-01-14T00:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Extracting Automata from <b>Recurrent Neural</b> Networks Using Queries and ...", "url": "https://www.arxiv-vanity.com/papers/1711.09576/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1711.09576", "snippet": "For each <b>input</b> symbol the <b>RNN</b> <b>outputs</b> a state vector representing the sequence up to that point. A state vector and an <b>input</b> symbol are combined for producing the next state vector. The <b>RNN</b> is essentially a parameterized mathematical function <b>that takes</b> as <b>input</b> a state vector and an <b>input</b> vector, and produces a new state vector. The state vectors <b>can</b> be passed to a classification component that is used to produce a binary or multi-class classification decision. The <b>RNN</b> is trainable, and ...", "dateLastCrawled": "2021-12-27T06:06:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A Tour of <b>Recurrent Neural Network Algorithms for Deep Learning</b>", "url": "https://machinelearningmastery.com/recurrent-neural-network-algorithms-for-deep-learning/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>recurrent-neural-network-algorithms-for-deep-learning</b>", "snippet": "RNNs stand out from other <b>machine</b> <b>learning</b> methods for their ability to learn and carry out complicated transformations of data over extended periods of time. Moreover, it is known that RNNs are Turing-Complete and therefore have the capacity to simulate arbitrary procedures, if properly wired. The capabilities of standard RNNs are extended to simplify the solution of algorithmic tasks. This enrichment is primarily via a large, addressable memory, so, by <b>analogy</b> to Turing\u2019s enrichment of ...", "dateLastCrawled": "2022-02-02T22:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Mathematical understanding of <b>RNN</b> and its variants - <b>GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/mathematical-understanding-of-rnn-and-its-variants/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/mathematical-understanding-of-<b>rnn</b>-and-its-variants", "snippet": "<b>RNN</b> is suitable for such work thanks to their capability of <b>learning</b> the context. Other applications include speech to text conversion, building virtual assistance, time-series stocks forecasting, sentimental analysis, language modelling and <b>machine</b> translation. On the other hand, a feed-forward neural network produces an output which only depends on the current input. Examples for such are image classification task, image segmentation or object detection task. One such type of such network ...", "dateLastCrawled": "2022-01-29T04:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Deep Learning: Models for Sequence Data</b> (<b>RNN</b> and LSTM)", "url": "https://cse.iitk.ac.in/users/piyush/courses/ml_autumn16/771A_lec24_slides.pdf", "isFamilyFriendly": true, "displayUrl": "https://cse.iitk.ac.in/users/piyush/courses/ml_autumn16/771A_lec24_slides.pdf", "snippet": "Filters are like basis/dictionary (PCA <b>analogy</b>) Each lter is convolved over entire input to produce a feature map Nonlinearity and pooling and applied after each convolution layer Last layer (one that connects to outputs) is fully connected <b>Machine</b> <b>Learning</b> (CS771A) <b>Deep Learning: Models for Sequence Data</b> (<b>RNN</b> and LSTM) 3. Recap: Convolutional Neural Network Special type of feedforward neural nets (local connectivity + weight sharing) Each layer uses a set of \\ lters&quot; (basically, weights to ...", "dateLastCrawled": "2022-01-17T20:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Machine</b> <b>Learning</b> (ML) and Neural Networks (NN)\u2026 An Intuitive ...", "url": "https://medium.com/visionary-hub/machine-learning-ml-and-neural-networks-nn-an-intuitive-walkthrough-76bdaba8b0e3", "isFamilyFriendly": true, "displayUrl": "https://medium.com/visionary-hub/<b>machine</b>-<b>learning</b>-ml-and-neural-networks-nn-an...", "snippet": "A better <b>analogy</b> for unsupervised <b>learning</b>, and one that\u2019s more commonly used, is separating a group of blocks by colour. Suppose we have 10 blocks, each with different coloured faces. In the ...", "dateLastCrawled": "2022-01-30T17:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Chapter 8 Recurrent Neural Networks</b> | Deep <b>Learning</b> and its Applications", "url": "https://frcs.github.io/4C16-LectureNotes/recurrent-neural-networks.html", "isFamilyFriendly": true, "displayUrl": "https://frcs.github.io/4C16-LectureNotes/recurrent-neural-networks.html", "snippet": "In its simplest form, the inner structure of the hidden layer block is simply a dense layer of neurons with \\(\\mathrm{tanh}\\) activation. This is called a simple <b>RNN</b> architecture or Elman network.. We usually take a \\(\\mathrm{tanh}\\) activation as it can produce positive or negative values, allowing for increases and decreases of the state values. Also \\(\\mathrm{tanh}\\) bounds the state values between -1 and 1, and thus avoids a potential explosion of the state values.. The equations for ...", "dateLastCrawled": "2022-02-02T05:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Python <b>RNN</b>: Recurrent Neural Networks for Time Series Forecasting | by ...", "url": "https://towardsdatascience.com/temporal-loops-intro-to-recurrent-neural-networks-for-time-series-forecasting-in-python-b0398963dc1f", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/temporal-loops-intro-to-recurrent-neural-networks-for...", "snippet": "We have put a relatively fine-toothed comb to the <b>learning</b> rate, 0.001, and the epochs, 300, in our setup of the <b>RNN</b> model. We could also play with the dropout parameter (to make the <b>RNN</b> try out various subsets of nodes during training); and with the size of the hidden state (a higher hidden dimension value increases the <b>RNN</b>\u2019s capability to deal with more intricate patterns over longer time frames). A tuning algorithm could tweak them while rerunning the fitting process to try to achieve ...", "dateLastCrawled": "2022-02-02T15:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Sentiment Analysis</b> from Tweets using Recurrent Neural Networks | by ...", "url": "https://medium.com/@gabriel.mayers/sentiment-analysis-from-tweets-using-recurrent-neural-networks-ebf6c202b9d5", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@gabriel.mayers/<b>sentiment-analysis</b>-from-tweets-using-recurrent...", "snippet": "LSTM Architeture. This is a variation from <b>RNN</b> and very powerful alternative when you need that your network is able to memorize information for a longer period of time. LSTM is based in gates ...", "dateLastCrawled": "2022-01-23T01:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A Gentle Introduction to <b>Long Short-Term Memory</b> Networks by the Experts", "url": "https://machinelearningmastery.com/gentle-introduction-long-short-term-memory-networks-experts/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/gentle-introduction-<b>long-short-term-memory</b>-networks...", "snippet": "<b>Long Short-Term Memory</b> (LSTM) networks are a type of recurrent neural network capable of <b>learning</b> order dependence in sequence prediction problems. This is a behavior required in complex problem domains like <b>machine</b> translation, speech recognition, and more. LSTMs are a complex area of deep <b>learning</b>. It can be hard to get your hands around what LSTMs are, and how terms like bidirectional", "dateLastCrawled": "2022-01-31T03:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Coursera: Neural Networks and Deep Learning</b> (Week 1) Quiz [MCQ Answers ...", "url": "https://www.apdaga.com/2019/03/coursera-neural-networks-and-deep-learning-week-1-quiz.html", "isFamilyFriendly": true, "displayUrl": "https://www.apdaga.com/2019/03/<b>coursera-neural-networks-and-deep-learning</b>-week-1-quiz.html", "snippet": "Recommended <b>Machine</b> <b>Learning</b> Courses: ... edX: <b>Machine</b> <b>Learning</b>; Fast.ai: Introduction to <b>Machine</b> <b>Learning</b> for Coders; What does the <b>analogy</b> \u201cAI is the new electricity\u201d refer to? AI runs on computers and is thus powered by electricity, but it is letting computers do things not possible before. Similar to electricity starting about 100 years ago, AI is transforming multiple industries. Correct. Yes. AI is transforming many fields from the car industry to agriculture to supply-chain ...", "dateLastCrawled": "2022-01-30T01:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "deep-<b>learning</b>-coursera/Week 1 Quiz - Introduction to deep <b>learning</b>.md ...", "url": "https://github.com/Kulbear/deep-learning-coursera/blob/master/Neural%20Networks%20and%20Deep%20Learning/Week%201%20Quiz%20-%20Introduction%20to%20deep%20learning.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/Kulbear/deep-<b>learning</b>-coursera/blob/master/Neural Networks and Deep...", "snippet": "Why is an <b>RNN</b> (Recurrent Neural Network) used for <b>machine</b> translation, say translating English to French? (Check all that apply.) It can be trained as a supervised <b>learning</b> problem. It is strictly more powerful than a Convolutional Neural Network (CNN). It is applicable when the input/output is a sequence (e.g., a sequence of words).", "dateLastCrawled": "2022-02-03T05:16:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Introduction to Recurrent Neural Networks | <b>Machine</b> <b>Learning</b> lab", "url": "https://en.mlab.ai/blog/introduction-recurrent-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://en.mlab.ai/blog/introduction-recurrent-neural-networks", "snippet": "The <b>Machine</b> <b>Learning</b> Blog. 09/27/2018. Introduction to Recurrent Neural Networks In this article, I will explain what are Recurrent Neural Networks (RNN), how they work and what you can do with them. I will also show a very cool example of music generation using artificial intelligence. However, before discussing RNN, we need to explain the concept of sequence data. Sequence Data As the name indicates, sequence data is a collection of data in different states through time so it can form ...", "dateLastCrawled": "2022-02-01T17:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Machine Learning for NLP</b> - Aurelie Herbelot", "url": "http://aurelieherbelot.net/resources/slides/teaching/RNNs.pdf", "isFamilyFriendly": true, "displayUrl": "aurelieherbelot.net/resources/slides/teaching/RNNs.pdf", "snippet": "An RNN, step by step Now we backpropagate through time. We need to compute gradients for three matrices: Why, Whh and Wxh. The gradient of matrix Why is straightforward \u2013 it is simply the sum", "dateLastCrawled": "2021-09-18T14:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Notes on Recurrent Neural Networks</b> \u2013 humblesoftwaredev", "url": "https://humblesoftwaredev.wordpress.com/2016/12/04/notes-on-recurrent-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://humblesoftwaredev.wordpress.com/2016/12/04/<b>notes-on-recurrent-neural-networks</b>", "snippet": "Recurrent neural nets have states, unlike feed-forward networks. An analogy for RNN is the C strtok function, where calling it with the same parameter typically yields a different value (but of course, unlike strtok, RNN does not modify the input). An analogy for feed-forward networks is a function in the mathematical sense, where y=f(x) regardless of how many times\u2026", "dateLastCrawled": "2022-01-14T08:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "State-of-the-art in artificial <b>neural network applications</b>: A survey ...", "url": "https://www.sciencedirect.com/science/article/pii/S2405844018332067", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2405844018332067", "snippet": "Unlike a recurrent neural network, an <b>RNN is like</b> a hierarchical network where the input need processing hierarchically in the form of a tree because there is no time to the input sequence. 2.4. Deep <b>learning</b>. Artificial intelligence (AI) has existed over many decades, and the field is wide. AI can be view as a set that contains <b>machine</b> <b>learning</b> (ML), and deep <b>learning</b> (DL). The ML is a subset of AI, meanwhile, DL, in turn, a subset of ML. That is DL is an aspect of AI; the term deep ...", "dateLastCrawled": "2022-01-27T17:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>NLP - Transformers</b> | Blog Posts | Lumenci", "url": "https://www.lumenci.com/post/nlp-transformers", "isFamilyFriendly": true, "displayUrl": "https://www.lumenci.com/post/<b>nlp-transformers</b>", "snippet": "Thus, because weights are shared across time, <b>RNN is like</b> a state <b>machine</b> that takes actions temporally based on its historical sequential information. For example, RNN can be trained on a sequence of characters to generate the next character correctly. RNN - The activation at each time step is feedback to the next time step. For many years, RNN and its gated variants were the most popular architectures used for NLP. However, one of the main problems with RNN is the vanishing gradient ...", "dateLastCrawled": "2022-01-26T07:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Very simple example of RNN</b>? : learnmachinelearning", "url": "https://www.reddit.com/r/learnmachinelearning/comments/84bk5r/very_simple_example_of_rnn/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/learn<b>machinelearning</b>/comments/84bk5r/<b>very_simple_example_of_rnn</b>", "snippet": "basically, an <b>RNN is like</b> a regular layer (the dense layer where all neurons are connected to the next layer&#39;s neurons), except that it takes as an additional paramenter its own output from the previous training iteration.", "dateLastCrawled": "2021-01-08T07:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Deep <b>Learning Approaches for Phantom Movement Recognition</b>", "url": "https://www.researchgate.net/publication/336367291_Deep_Learning_Approaches_for_Phantom_Movement_Recognition", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/336367291_Deep_<b>Learning</b>_Approaches_for...", "snippet": "<b>RNN is, like</b> MLP, only. have good results for T A WD while other region successes are. far behind other algorithms. For <b>machine</b> <b>learning</b> algorithms, cross validation (k=10) is used to split the ...", "dateLastCrawled": "2022-01-04T05:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Artificial intelligence in drug design: algorithms, applications ...", "url": "https://www.future-science.com/doi/full/10.4155/fdd-2020-0028", "isFamilyFriendly": true, "displayUrl": "https://www.future-science.com/doi/full/10.4155/fdd-2020-0028", "snippet": "The discovery paradigm of drugs is rapidly growing due to advances in <b>machine</b> <b>learning</b> (ML) and artificial intelligence (AI). This review covers myriad faces of AI and ML in drug design. There is a plethora of AI algorithms, the most common of which are summarized in this review. In addition, AI is fraught with challenges that are highlighted along with plausible solutions to them. Examples are provided to illustrate the use of AI and ML in drug discovery and in predicting drug properties ...", "dateLastCrawled": "2022-01-29T02:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "State-of-the-art <b>in artificial neural network applications: A</b> survey", "url": "https://www.researchgate.net/publication/329149409_State-of-the-art_in_artificial_neural_network_applications_A_survey", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/329149409_State-of-the-art_in_artificial...", "snippet": "ANNs are one type of model for <b>machine</b> <b>learning</b> (ML) and has become . relatively competitive to conventional regression and stat istical models regarding. usefulness [1]. Currently, arti \ufb01 cial ...", "dateLastCrawled": "2022-01-29T22:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>The future of AI music is Magenta</b> | DataDrivenInvestor", "url": "https://www.datadriveninvestor.com/2020/04/25/the-future-of-ai-music-is-magenta/", "isFamilyFriendly": true, "displayUrl": "https://www.datadriveninvestor.com/2020/04/25/<b>the-future-of-ai-music-is-magenta</b>", "snippet": "<b>The future of AI music is Magenta</b>. Music seems to be one of the fields that, at a surface level at least, AI just can\u2019t seem to penetrate. AI is rapidly taking over so many fields, and there\u2019s huge progress in music too! There are so many awesome developments (check out the app Transformer) and progress is moving at a breakneck pace.", "dateLastCrawled": "2022-01-28T07:09:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "End to end <b>machine</b> <b>learning</b> for fault detection and classification in ...", "url": "https://www.sciencedirect.com/science/article/pii/S0378779621004119", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0378779621004119", "snippet": "The training process for <b>RNN is similar</b> to traditional ANNs. However, since the parameters are shared among time instances in RNNs, the back-propagation algorithm for RNNs is termed as Backpropagation through time (BPTT) . As the number of time steps increase in RNN, it faces a problem termed as \u201cvanishing gradients\u201d due to which it cannot retain long term dependencies. Description can be seen in 39,40]. This phenomenon makes RNNs difficult to train and render them impractical in most of ...", "dateLastCrawled": "2021-12-14T22:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "2_tensorflow_lstm", "url": "http://ethen8181.github.io/machine-learning/deep_learning/rnn/2_tensorflow_lstm.html", "isFamilyFriendly": true, "displayUrl": "ethen8181.github.io/<b>machine</b>-<b>learning</b>/deep_<b>learning</b>/rnn/2_tensorflow_lstm.html", "snippet": "Training a <b>RNN is similar</b> to training a traditional Neural Network, we also use the backpropagation algorithm, but with a little twist. Because the parameters are shared by all time steps in the network, the gradient at each output depends not only on the calculations of the current time step, but also the previous time steps. For example, in order to calculate the gradient at t=4 we would need to backpropagate 3 steps and sum up the gradients. This is called Backpropagation Through Time ...", "dateLastCrawled": "2022-02-03T13:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Deep <b>Recurrent Neural Networks</b> with Keras | <b>Paperspace Blog</b>", "url": "https://blog.paperspace.com/advanced-recurrent-neural-networks-deep-rnns/", "isFamilyFriendly": true, "displayUrl": "https://blog.paperspace.com/advanced-<b>recurrent-neural-networks</b>-deep-rnns", "snippet": "The training of a deep <b>RNN is similar</b> to the Backpropagation Through Time (BPTT) algorithm, as in an RNN but with additional hidden units. Now that you\u2019ve got an idea of what a deep RNN is, in the next section we&#39;ll build a music generator using a deep RNN and Keras. Generating Music Using a Deep RNN. Music is the ultimate language. We have been creating and rendering beautiful melodies since time unknown. In this context, do you think a computer can generate musical notes comparable to ...", "dateLastCrawled": "2022-02-03T18:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Machine</b> <b>Learning</b> - <b>Kbeznak Parmatonic</b>", "url": "https://sites.google.com/view/kbeznak-parmatonic-guru-of-ml/home", "isFamilyFriendly": true, "displayUrl": "https://<b>sites.google.com</b>/view/<b>kbeznak-parmatonic</b>-guru-of-ml/home", "snippet": "Backpropagation in <b>RNN is similar</b> to Neural Network, but we have to take care of the weight with respect to all the time steps. So, the gradient has to be calculated for all those steps going backwards, this is called Backpropagation Through Time(BPTT). Software and Tools: <b>Kbeznak Parmatonic</b> prefers Tensorflow and Caffe2 for deeplearning, and keras would help you lot in the initial stages. Author <b>Kbeznak Parmatonic</b>: Dr. <b>Kbeznak Parmatonic</b>, was a chief scientist at NASA and was well deserved ...", "dateLastCrawled": "2021-12-23T18:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A Motor-Imagery BCI System Based on Deep <b>Learning</b> Networks and Its ...", "url": "https://www.intechopen.com/chapters/60241", "isFamilyFriendly": true, "displayUrl": "https://www.intechopen.com/chapters/60241", "snippet": "Training an <b>RNN is similar</b> to training a traditional neural network (TNN). Because RNNs trained by TNN\u2019s style have difficulties in <b>learning</b> long-term dependencies due to the vanishing and exploding gradient problem. LSTMs do not have a fundamentally different architecture from RNNs, but they use a different function to calculate the states in hidden layer. The memory in LSTMs is called cells and can be thought as black boxes that take as input the previous state and current input ...", "dateLastCrawled": "2022-02-02T11:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Review of Vibration-Based Structural Health Monitoring Using Deep <b>Learning</b>", "url": "https://www.mdpi.com/2076-3417/10/5/1680/htm", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/2076-3417/10/5/1680/htm", "snippet": "An <b>RNN is similar</b> to recurrent neural networks in that it is good at dealing with sequential data. Recurrent neural networks are also called RNNs in the literature; to distinguish between the architectures, only the recursive neural network is abbreviated as RNN in this paper. An RNN models hierarchical structures in a tree fashion, which is overly time-consuming and costly. This has led to a lack of attention being given to RNNs. Because an RNN processes all information of the input ...", "dateLastCrawled": "2022-01-12T00:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Deep Neural Network</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/computer-science/deep-neural-network", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/computer-science/<b>deep-neural-network</b>", "snippet": "This dataset is designed for <b>machine</b> <b>learning</b> classification tasks and includes 60,000 training and 10,000 test gray scale images composed of 28-by-28 pixels. Every training and test case is related to one of ten labels (0\u20139). Zalando\u2019s new dataset is mainly the same as the original handwritten digits data. But instead of having images of the digits 0\u20139, Zalando\u2019s data involves images with 10 different fashion products. Hence the dataset is named fashion-MNIST dataset and can be ...", "dateLastCrawled": "2022-01-30T06:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Deep Learning</b> - SlideShare", "url": "https://www.slideshare.net/JunWang5/deep-learning-61493694", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/JunWang5/<b>deep-learning</b>-61493694", "snippet": "\u2022 ClockWork-<b>RNN is similar</b> to a simple RNN with an input, output and hidden layer \u2022 Difference lies in \u2013 The hidden layer is partitioned into g modules each with its own clock rate \u2013 Neurons in faster module are connected to neurons in a slower module RNN applications: time series Koutnik, Jan, et al. &quot;A clockwork rnn.&quot; arXiv preprint arXiv:1402.3511 (2014). A Clockwork RNN Figure 1. CW-RNN architecture is similar to a simple RNN with an input, output and hidden layer. The hidden ...", "dateLastCrawled": "2022-01-31T18:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Deep Learning</b> for Geophysics: Current and Future Trends - Yu - 2021 ...", "url": "https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2021RG000742", "isFamilyFriendly": true, "displayUrl": "https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2021RG000742", "snippet": "Different from traditional model-driven methods, <b>machine</b> <b>learning</b> (ML) is a type of data-driven approach that trains a regression or classification model through a complex nonlinear mapping with adjustable parameters based on a training data set. The comparison of model-driven and data-driven approaches is summarized in Figure 1. For decades, ML methods have been widely adopted in various geophysical applications, such as exploration geophysics (Huang et al., 2006; Helmy et al., 2010; Jia ...", "dateLastCrawled": "2022-01-31T08:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Different Architecture of Deep <b>Learning</b> Algorithms Extensive number of ...", "url": "https://www.researchgate.net/figure/Different-Architecture-of-Deep-Learning-Algorithms-Extensive-number-of-deep-learning_fig1_324149367", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/figure/Different-Architecture-of-Deep-<b>Learning</b>-Algorithms...", "snippet": "Unlike classical <b>machine</b> <b>learning</b> (support vector <b>machine</b>, k-nearest neighbour, k-mean, etc.) that require a human engineered feature to perform optimally (LeCun, et al., 2015). Over the years ...", "dateLastCrawled": "2022-01-29T15:47:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Towards deep entity resolution via soft schema matching - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S0925231221016635", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0925231221016635", "snippet": "Technically, TLM is a new fundamental architecture for deep ER, <b>just as RNN</b>. Our work and TLM based approaches falls into different lines of deep ER research, which are orthogonal and complementary to each other. Our major contribution is proposing soft schema mapping and incorporating it into (RNN based) deep ER models, which does not require huge amounts of NLP corpora for pre-training, while TLM based approaches exploit the deeper language understanding capability from tremendously pre ...", "dateLastCrawled": "2022-01-21T02:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Positional encoding, residual connections, padding masks</b>: covering the ...", "url": "https://data-science-blog.com/blog/2021/04/22/positional-encoding-residual-connections-padding-masks-all-the-details-of-transformer-model/", "isFamilyFriendly": true, "displayUrl": "https://data-science-blog.com/blog/2021/04/22/positional-encoding-residual-connections...", "snippet": "Transformer decoder also predicts the output sequences autoregressively one token at a time step, <b>just as RNN</b> decoders. I think it easy to understand this process because RNN decoder generates tokens just as you connect RNN cells one after another, like connecting rings to a chain. In this way it is easy to make sure that generating of one token in only affected by the former tokens. On the other hand, during training Transformer decoders, you input the whole sentence at once. That means ...", "dateLastCrawled": "2022-01-30T16:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Machine</b> <b>Learning</b> Archives - Data Science Blog", "url": "https://data-science-blog.com/blog/category/main-category/machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://data-science-blog.com/blog/category/main-category/<b>machine</b>-<b>learning</b>", "snippet": "Most <b>machine</b> <b>learning</b> algorithms covered by major introductory textbooks tend to be too deterministic and dependent on the size of data. Many of those algorithms have another \u201cparallel world,\u201d where you can handle inaccuracy in better ways. I hope I can also write about them, and I might prepare another trilogy for such PCA. But I will not disappoint you, like \u201cThe Phantom Menace.\u201d Appendix: making a model of a bunch of grape with ellipsoid berries. If you can control quadratic ...", "dateLastCrawled": "2022-01-05T04:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "1561982779 | PDF | Equity Crowdfunding | Investor", "url": "https://www.scribd.com/document/550868164/1878586842-1561982779", "isFamilyFriendly": true, "displayUrl": "https://www.scribd.com/document/550868164/1878586842-1561982779", "snippet": "Scribd is the world&#39;s largest social reading and publishing site.", "dateLastCrawled": "2022-01-25T03:48:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Recurrent Neural Networks and LSTM explained", "url": "https://dhrubajitdas44.blogspot.com/2018/10/recurrent-neural-networks-and-lstm.html", "isFamilyFriendly": true, "displayUrl": "https://dhrubajitdas44.blogspot.com/2018/10/recurrent-neural-networks-and-lstm.html", "snippet": "A <b>RNN can be thought of as</b> multiple copies of the same network , each passing message to . the next. Because of their internal memory, RNN\u2019s are able to remember important things about the input they received, which enables them to be very precise in predicting what\u2019s coming next. This is the reason why they are the preferred algorithm for sequential data like time series, speech, text, financial data, audio, video, weather and much more because they can form a much deeper understanding ...", "dateLastCrawled": "2022-01-10T10:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "PHC6937-<b>Machine</b> <b>Learning</b>-Guest Lecture", "url": "https://slides.com/benh-hu/phc6937machinelearning", "isFamilyFriendly": true, "displayUrl": "https://slides.com/benh-hu/phc6937<b>machinelearning</b>", "snippet": "<b>Machine</b> <b>learning</b> is predicated on this idea of <b>learning</b> from example ... A <b>RNN can be thought of as</b> the addition of loops to the archetecture of a standard feedforward NN - the output of the network may feedback as an input to the network with the next input vector, and so on The recurrent connections add state or memory to the network and allow it to learn broader abstractions from the input sequences; Reading. PHC6937-<b>Machine</b> <b>Learning</b>-Guest Lecture. By Hui Hu. PHC6937-<b>Machine</b> <b>Learning</b> ...", "dateLastCrawled": "2022-01-25T13:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Decoding Your Genes</b>. Can Neural Networks Unravel The Secrets\u2026 | by ...", "url": "https://towardsdatascience.com/decoding-your-genes-4a23e89aba98", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>decoding-your-genes</b>-4a23e89aba98", "snippet": "Conceptually, an <b>RNN can be thought of as</b> a connected sequence of feed-forward networks with information passed between them. The information being passed is the hidden-state which represents all the previous inputs to the network. At each step of the RNN, the hidden state generated from the previous step is passed in, as well as the next sequence input. This then returns an output as well as the new hidden state to be passed on again. This allows the RNN to retain a \u2018memory\u2019 of the ...", "dateLastCrawled": "2022-01-26T03:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Using RNNs for <b>Machine Translation</b> | by Aryan Misra | Towards Data Science", "url": "https://towardsdatascience.com/using-rnns-for-machine-translation-11ddded78ddf", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/using-rnns-for-<b>machine-translation</b>-11ddded78ddf", "snippet": "3. Sequence to Sequence. The RNN takes in an input sequence and outputs a sequence. <b>Machine Translation</b>: an RNN reads a sentence in one language and then outputs it in another. This should help you get a high-level understanding of RNNs, if you want to learn more about the math behind the operations an RNN performs, I recommend you check out ...", "dateLastCrawled": "2022-02-01T23:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Time series prediction of COVID-19 transmission in America using LSTM ...", "url": "https://www.sciencedirect.com/science/article/pii/S2211379721005775", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2211379721005775", "snippet": "The <b>machine</b> <b>learning</b> algorithm XGBoost was employed to build the models to predict the criticality , mortality , and ... RNNs can use their internal state (memory) to process variable length sequences of inputs. A <b>RNN can be thought of as</b> multiple copies of the same network, each passing a message to a successor (see Fig. 4). They might be able to connect previous information to the present task. However, as that gap grows, RNNs become unable to learn to connect the information. The short ...", "dateLastCrawled": "2022-01-24T06:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "[DL] 11. RNN <b>2(Bidirectional, Deep RNN, Long term connection</b>) | by Jun ...", "url": "https://medium.com/jun-devpblog/dl-11-rnn-2-bidirectional-deep-rnn-long-term-connection-8a836a7f2260", "isFamilyFriendly": true, "displayUrl": "https://medium.com/jun-devpblog/dl-11-rnn-<b>2-bidirectional-deep-rnn-long-term</b>...", "snippet": "Basically, Bidirectional <b>RNN can be thought of as</b> two RNNs in a network, one is moving forwards in time and the other one is moving backward and both are contributing to producing output ...", "dateLastCrawled": "2021-08-12T14:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Convolutional Neural Network and RNN</b> for OCR problem.", "url": "https://www.slideshare.net/vishalmishra982/convolutional-neural-network-and-rnn-for-ocr-problem-86087045", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/vishalmishra982/<b>convolutional-neural-network-and-rnn</b>-for...", "snippet": "Sequence-to-Sequence <b>Learning</b> using Deep <b>Learning</b> for Optical Character Recognition. ... <b>RNN can be thought of as</b> multiple copies of the same network, each passing a message to a successor. An unrolled RNN is shown below. \u2022 In fast last few years, there have been incredible success applying RNNs to a variety of problems: speech recognition, language modeling, translation, image captioning\u2026. The list goes on. An Unrolled RNN 44. DRAWBACK OF AN RNN \u2022 RNN has a problem of long term ...", "dateLastCrawled": "2022-01-17T23:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) Remaining useful life prediction of PEMFC based on long short ...", "url": "https://www.researchgate.net/publication/328587416_Remaining_useful_life_prediction_of_PEMFC_based_on_long_short-term_memory_recurrent_neural_networks", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/328587416_Remaining_useful_life_prediction_of...", "snippet": "LSTM <b>RNN can be thought of as</b> a series of BPNN with equal. Fig. 10 e Prognostic results of LSTM RNN at T. p. \u00bc 550 h. Fig. 11 e System training loss and test loss. Table 3 e Prediction results of ...", "dateLastCrawled": "2022-01-29T16:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A diagram of (a) the RNN and its (b) unrolled version. | Download ...", "url": "https://researchgate.net/figure/A-diagram-of-a-the-RNN-and-its-b-unrolled-version_fig1_342349801", "isFamilyFriendly": true, "displayUrl": "https://researchgate.net/figure/A-diagram-of-a-the-RNN-and-its-b-unrolled-version_fig1...", "snippet": "Download scientific diagram | A diagram of (a) the RNN and its (b) unrolled version. from publication: ML-descent: an optimization algorithm for FWI using <b>machine</b> <b>learning</b> | Full-waveform ...", "dateLastCrawled": "2021-06-06T20:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>How I Used Deep Learning To Train A Chatbot</b> To Talk Like Me (Sorta ...", "url": "https://adeshpande3.github.io/How-I-Used-Deep-Learning-to-Train-a-Chatbot-to-Talk-Like-Me", "isFamilyFriendly": true, "displayUrl": "https://adeshpande3.github.io/<b>How-I-Used-Deep-Learning-to-Train-a-Chatbot</b>-to-Talk-Like-Me", "snippet": "This paper showed great results in <b>machine</b> translation specifically, but Seq2Seq models have grown to encompass a variety of NLP tasks. ... By this logic, the final hidden state vector of the encoder <b>RNN can be thought of as</b> a pretty accurate representation of the whole input text. The decoder is another RNN, which takes in the final hidden state vector of the encoder and uses it to predict the words of the output reply. Let&#39;s look at the first cell. The cell&#39;s job is to take in the vector ...", "dateLastCrawled": "2022-01-30T02:41:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(rnn)  is like +(a black box that takes in an input and outputs a prediction)", "+(rnn) is similar to +(a black box that takes in an input and outputs a prediction)", "+(rnn) can be thought of as +(a black box that takes in an input and outputs a prediction)", "+(rnn) can be compared to +(a black box that takes in an input and outputs a prediction)", "machine learning +(rnn AND analogy)", "machine learning +(\"rnn is like\")", "machine learning +(\"rnn is similar\")", "machine learning +(\"just as rnn\")", "machine learning +(\"rnn can be thought of as\")", "machine learning +(\"rnn can be compared to\")"]}