{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A Gentle Introduction to <b>Cross-Entropy for Machine Learning</b>", "url": "https://machinelearningmastery.com/cross-entropy-for-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>cross-entropy-for-machine-learning</b>", "snippet": "<b>Cross-entropy</b> builds upon the idea of entropy from information theory and calculates the <b>number</b> <b>of bits</b> required to represent or transmit an average event from one <b>distribution</b> compared to another <b>distribution</b>. \u2026 the <b>cross entropy</b> is the average <b>number</b> <b>of bits</b> <b>needed</b> <b>to encode</b> data coming from a source with <b>distribution</b> p when we use model q \u2026", "dateLastCrawled": "2022-02-03T03:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Cross entropy</b> calculator | Taskvio", "url": "https://taskvio.com/maths/probability-distributions/cross-entropy/", "isFamilyFriendly": true, "displayUrl": "https://taskvio.com/maths/probability-<b>distributions</b>/<b>cross-entropy</b>", "snippet": "The KL Divergence is that the average <b>number</b> of additional <b>bits</b> <b>needed</b> <b>to encode</b> the info, thanks to the very fact that we&#39;d <b>like</b> <b>distribution</b> q <b>to encode</b> the info rather than truth <b>distribution</b> p. <b>Cross-Entropy</b> as Loss Function <b>Cross entropy</b> is broadly used as a Loss Function when you optimizing classification models.", "dateLastCrawled": "2022-02-02T15:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "What is <b>Cross-Entropy</b> in Machine learning? | by Neelam Tyagi ...", "url": "https://medium.com/analytics-steps/what-is-cross-entropy-in-machine-learning-10479639ec28", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-steps/what-is-<b>cross-entropy</b>-in-machine-learning-10479639ec28", "snippet": "The KL Divergence is the average <b>number</b> of extra <b>bits</b> <b>needed</b> <b>to encode</b> the data, due to the fact that we need <b>distribution</b> q <b>to encode</b> the data instead of the true <b>distribution</b> p. <b>Cross-Entropy</b> as ...", "dateLastCrawled": "2022-01-29T16:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Understanding <b>Entropy</b>: the Golden Measurement of Machine Learning | by ...", "url": "https://towardsdatascience.com/understanding-entropy-the-golden-measurement-of-machine-learning-4ea97c663dc3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-<b>entropy</b>-the-golden-measurement-of-machine...", "snippet": "<b>Cross-entropy</b>, <b>like</b> Kullback-Lieber Divergence (KLD), ... The measure is defined as the average <b>number</b> <b>of bits</b> <b>needed</b> <b>to encode</b> data coming from a source with <b>distribution</b> p using model <b>distribution</b> q. If we consider a target <b>distribution</b> p and the approximation q, we would want to reduce the <b>number</b> <b>of bits</b> required to represent an event using q instead of p. On the other hand, relative <b>entropy</b> (KLD) measures the additional <b>number</b> <b>of bits</b> required to represent an event from p in a ...", "dateLastCrawled": "2022-02-02T21:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Cross Entropy</b> and Kullback-Leibler | aimless agents", "url": "https://aimless-agents.github.io/articles/2021-03/cekl", "isFamilyFriendly": true, "displayUrl": "https://aimless-agents.github.io/articles/2021-03/cekl", "snippet": "Intuitively, it can be thought of as the expected <b>number</b> of extra <b>bits</b> <b>needed</b> <b>to encode</b> outcomes drawn from the true <b>distribution</b> \\(p\\) if we\u2019re using <b>distribution</b> \\(q\\), or \u201cthe measure of inefficiency in using the probability <b>distribution</b> q to approximate the true probability <b>distribution</b> p\u201d (thanks Wikipedia!). If we have a perfect prediction, i.e., our predicted <b>distribution</b> equals the true, then <b>cross entropy</b> equals the true <b>distribution</b>\u2019s entropy, making KL divergence 0 (its ...", "dateLastCrawled": "2022-02-01T00:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Understanding Cross-entropy</b> for Machine Learning", "url": "https://rubikscode.net/2021/08/10/understanding-cross-entropy/", "isFamilyFriendly": true, "displayUrl": "https://rubikscode.net/2021/08/10/<b>understanding-cross-entropy</b>", "snippet": "This means that <b>Cross-entropy</b> can be defined as the <b>number</b> <b>of bits</b> we need <b>to encode</b> information from y using the wrong encoding tool y\u2019. Mathematically, this can be written <b>like</b> this: The other way to write this expression is using expectation: H (y, y\u2019) represents expectation using y and the encoding size using y\u2019.", "dateLastCrawled": "2021-12-19T11:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A <b>Gentle Introduction to Cross-Entropy for Machine Learning</b> \u2013 AiProBlog.Com", "url": "https://www.aiproblog.com/index.php/2019/10/20/a-gentle-introduction-to-cross-entropy-for-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.aiproblog.com/index.php/2019/10/20/a-<b>gentle-introduction-to-cross-entropy</b>...", "snippet": "\u2026 the <b>cross entropy</b> is the average <b>number</b> <b>of bits</b> <b>needed</b> <b>to encode</b> data coming from a source with <b>distribution</b> p when we use model q \u2026 \u2014 Page 57, Machine Learning: A Probabilistic Perspective , 2012.", "dateLastCrawled": "2021-12-28T17:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "information theory - <b>Qualitively what is Cross Entropy</b> - Cross Validated", "url": "https://stats.stackexchange.com/questions/80967/qualitively-what-is-cross-entropy", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/80967/<b>qualitively-what-is-cross-entropy</b>", "snippet": "This question gives a quantitative definition of <b>cross entropy</b>, in terms of it&#39;s formula.. I&#39;m looking for a more notional definition, wikipedia says: In information theory, the <b>cross entropy</b> between two probability distributions measures the average <b>number</b> <b>of bits</b> <b>needed</b> to identify an event from a set of possibilities, if a coding scheme is used based on a given probability <b>distribution</b> q, rather than the &quot;true&quot; <b>distribution</b> p. I have Emphasised the part that is giving me trouble in ...", "dateLastCrawled": "2022-01-19T01:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Information Theory for Machine Learning</b>: Entropy, <b>Cross Entropy</b>, and KL ...", "url": "https://scriptreference.com/information-theory-for-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://scriptreference.com/<b>information-theory-for-machine-learning</b>", "snippet": "If we know the probability of an event from a uniform <b>distribution</b>, we know how many <b>bits</b> we\u2019ll need <b>to encode</b> this <b>distribution</b> by calculating \\( -\\log_2(p) \\). We can also move in the opposite direction. If a uniform <b>distribution</b> is encoded using \\(n\\) <b>bits</b>, then each event must have a probability of \\( \\frac{1}{2^n} \\).", "dateLastCrawled": "2021-12-31T15:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Loss Function</b> - Pipline", "url": "https://nuoxu2016.gitbook.io/pipline/preprocessing-1/loss-function", "isFamilyFriendly": true, "displayUrl": "https://nuoxu2016.gitbook.io/pipline/preprocessing-1/<b>loss-function</b>", "snippet": "\u2026 the <b>cross entropy</b> is the average <b>number</b> <b>of bits</b> <b>needed</b> <b>to encode</b> data coming from a source with <b>distribution</b> p when we use model q \u2026 \u2014 Page 57, Machine Learning: A Probabilistic Perspective , 2012.", "dateLastCrawled": "2022-01-24T22:26:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A Gentle Introduction to <b>Cross-Entropy for Machine Learning</b>", "url": "https://machinelearningmastery.com/cross-entropy-for-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>cross-entropy-for-machine-learning</b>", "snippet": "<b>Cross-entropy</b> builds upon the idea of entropy from information theory and calculates the <b>number</b> <b>of bits</b> required to represent or transmit an average event from one <b>distribution</b> compared to another <b>distribution</b>. \u2026 the <b>cross entropy</b> is the average <b>number</b> <b>of bits</b> <b>needed</b> <b>to encode</b> data coming from a source with <b>distribution</b> p when we use model q \u2026", "dateLastCrawled": "2022-02-03T03:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Cross Entropy</b> and Kullback-Leibler | aimless agents", "url": "https://aimless-agents.github.io/articles/2021-03/cekl", "isFamilyFriendly": true, "displayUrl": "https://aimless-agents.github.io/articles/2021-03/cekl", "snippet": "Intuitively, it can be thought of as the expected <b>number</b> of extra <b>bits</b> <b>needed</b> <b>to encode</b> outcomes drawn from the true <b>distribution</b> \\(p\\) if we\u2019re using <b>distribution</b> \\(q\\), or \u201cthe measure of inefficiency in using the probability <b>distribution</b> q to approximate the true probability <b>distribution</b> p\u201d (thanks Wikipedia!). If we have a perfect prediction, i.e., our predicted <b>distribution</b> equals the true, then <b>cross entropy</b> equals the true <b>distribution</b>\u2019s entropy, making KL divergence 0 (its ...", "dateLastCrawled": "2022-02-01T00:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "What is <b>Cross-Entropy</b> in Machine learning? | by Neelam Tyagi ...", "url": "https://medium.com/analytics-steps/what-is-cross-entropy-in-machine-learning-10479639ec28", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-steps/what-is-<b>cross-entropy</b>-in-machine-learning-10479639ec28", "snippet": "The KL Divergence is the average <b>number</b> of extra <b>bits</b> <b>needed</b> <b>to encode</b> the data, due to the fact that we need <b>distribution</b> q <b>to encode</b> the data instead of the true <b>distribution</b> p. <b>Cross-Entropy</b> as ...", "dateLastCrawled": "2022-01-29T16:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Understanding Cross-entropy</b> for Machine Learning", "url": "https://rubikscode.net/2021/08/10/understanding-cross-entropy/", "isFamilyFriendly": true, "displayUrl": "https://rubikscode.net/2021/08/10/<b>understanding-cross-entropy</b>", "snippet": "Basically, we use probability <b>distribution</b> as a means <b>to encode</b> input. Our optimal tool would be entropy, ... This means that <b>Cross-entropy</b> can be defined as the <b>number</b> <b>of bits</b> we need <b>to encode</b> information from y using the wrong encoding tool y \u2019. Mathematically, this can be written like this: The other way to write this expression is using expectation: H(y, y\u2019) represents expectation using y and the encoding size using y\u2019. From this, we can conclude that H(y, y\u2019) and H(y\u2019, y) are ...", "dateLastCrawled": "2021-12-19T11:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Loss Function</b> - Pipline", "url": "https://nuoxu2016.gitbook.io/pipline/preprocessing-1/loss-function", "isFamilyFriendly": true, "displayUrl": "https://nuoxu2016.gitbook.io/pipline/preprocessing-1/<b>loss-function</b>", "snippet": "\u2026 the <b>cross entropy</b> is the average <b>number</b> <b>of bits</b> <b>needed</b> <b>to encode</b> data coming from a source with <b>distribution</b> p when we use model q \u2026 \u2014 Page 57, Machine Learning: A Probabilistic Perspective , 2012.", "dateLastCrawled": "2022-01-24T22:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "What is the connection between entropy and <b>cross-entropy</b> loss function ...", "url": "https://www.quora.com/What-is-the-connection-between-entropy-and-cross-entropy-loss-function", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-the-connection-between-entropy-and-<b>cross-entropy</b>-loss...", "snippet": "Answer: Entropy, as defined by Claude Shannon measure the disorder of a probability <b>distribution</b>. Specifically, if p=(p1, p2, \u2026, ) is a vector which represent the probability of being at any specific state, p1 is the probability of state 1, p2 the probability of being at state 2, and so on, then ...", "dateLastCrawled": "2022-01-08T17:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Quick Review on Information Theory \u2013 THE ARTIFICIAIL INTELLIGENCE ADVANTAGE", "url": "https://theaiadvantage.com/2021/09/27/quick-review-on-information-theory/", "isFamilyFriendly": true, "displayUrl": "https://theaiadvantage.com/2021/09/27/quick-review-on-information-theory", "snippet": "<b>Cross-Entropy</b>. <b>Cross entropy</b> builds ... then what is the <b>number</b> <b>of bits</b> <b>needed</b> <b>to encode</b>? The result will be a positive <b>number</b> measured in <b>bits</b> and will be equal to the entropy of the <b>distribution</b> if the two probability distributions are identical. Kullback-Leibler divergence (KL divergence) Relative to <b>cross-entropy</b>, KL divergence measures the differences between distributions and is the expectation under the probability of P of the log ratio of P over Q. Discrete and continuous probability ...", "dateLastCrawled": "2022-01-14T20:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Lecture 4: Logistic Regression", "url": "https://shuaili8.github.io/Teaching/VE445/L4_logistic%20regression.pdf", "isFamilyFriendly": true, "displayUrl": "https://shuaili8.github.io/Teaching/VE445/L4_logistic regression.pdf", "snippet": "Which one is more <b>similar</b> to norm <b>distribution</b>? 20 \u2022Information inequality \u2022Entropy \u2022 \u2022Is a measure of the uncertainty \u2022Discrete <b>distribution</b> with the maximum entropy is the uniform <b>distribution</b> \u2022<b>Cross entropy</b> \u2022 \u2022Is the average <b>number</b> <b>of bits</b> <b>needed</b> <b>to encode</b> data coming from a source with <b>distribution</b> p when we use model q to de\ufb01ne our codebook KL divergence (cont.) 21. <b>Cross entropy</b> loss \u2022<b>Cross entropy</b> \u2022Discrete case: \ud835\udc3b , =\u2212", "dateLastCrawled": "2022-02-03T01:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Essential Math for Data Science</b>: Information Theory | by Hadrien Jean ...", "url": "https://towardsdatascience.com/essential-math-for-data-science-information-theory-5d0380232ca1", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>essential-math-for-data-science</b>-information-theory-5d...", "snippet": "The result is a lower bound on the <b>number</b> <b>of bits</b>, that is, the minimum amount <b>of bits</b> <b>needed</b> <b>to encode</b> a sequence with an optimal encoding. The logarithm of a product is equal to the sum of the elements: log2(ab)=log2(a)+log2(b).This property is useful <b>to encode</b> the additive property of the Shannon information.", "dateLastCrawled": "2022-01-19T02:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Mystery of Object Detection</b> \u2013 Blog", "url": "https://dudeperf3ct.github.io/object/detection/2019/01/07/Mystery-of-Object-Detection/", "isFamilyFriendly": true, "displayUrl": "https://dudeperf3ct.github.io/object/detection/2019/01/07/<b>Mystery-of-Object-Detection</b>", "snippet": "The entropy rate of a data source means the average <b>number</b> <b>of bits</b> per symbol <b>needed</b> <b>to encode</b> it without any loss of information. Entropy of probability <b>distribution</b> p is given by \\(H(p) = -\\sum_{i}^{}p(i)\\log_{2}{p(i)}\\). Let p be the true distrubtion and q be the predicted <b>distribution</b> over our labels, then <b>cross entropy</b> of both <b>distribution</b> is defined as. \\(H(p, q) = -\\sum_{i}^{}p(i)\\log_{2}{q(i)}\\). It looks like pretty <b>similar</b> to equation of entropy above but instead of computing log ...", "dateLastCrawled": "2022-01-31T10:06:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Cross Entropy</b> and Kullback-Leibler | aimless agents", "url": "https://aimless-agents.github.io/articles/2021-03/cekl", "isFamilyFriendly": true, "displayUrl": "https://aimless-agents.github.io/articles/2021-03/cekl", "snippet": "Intuitively, it <b>can</b> <b>be thought</b> of as the expected <b>number</b> of extra <b>bits</b> <b>needed</b> <b>to encode</b> outcomes drawn from the true <b>distribution</b> \\(p\\) if we\u2019re using <b>distribution</b> \\(q\\), or \u201cthe measure of inefficiency in using the probability <b>distribution</b> q to approximate the true probability <b>distribution</b> p\u201d (thanks Wikipedia!). If we have a perfect prediction, i.e., our predicted <b>distribution</b> equals the true, then <b>cross entropy</b> equals the true <b>distribution</b>\u2019s entropy, making KL divergence 0 (its ...", "dateLastCrawled": "2022-02-01T00:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A Gentle Introduction to <b>Cross-Entropy for Machine Learning</b>", "url": "https://machinelearningmastery.com/cross-entropy-for-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>cross-entropy-for-machine-learning</b>", "snippet": "<b>Cross-entropy</b> builds upon the idea of entropy from information theory and calculates the <b>number</b> <b>of bits</b> required to represent or transmit an average event from one <b>distribution</b> compared to another <b>distribution</b>. \u2026 the <b>cross entropy</b> is the average <b>number</b> <b>of bits</b> <b>needed</b> <b>to encode</b> data coming from a source with <b>distribution</b> p when we use model q \u2026", "dateLastCrawled": "2022-02-03T03:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Understanding Cross-entropy</b> for Machine Learning", "url": "https://rubikscode.net/2021/08/10/understanding-cross-entropy/", "isFamilyFriendly": true, "displayUrl": "https://rubikscode.net/2021/08/10/<b>understanding-cross-entropy</b>", "snippet": "Basically, we use probability <b>distribution</b> as a means <b>to encode</b> input. Our optimal tool would be entropy, ... This means that <b>Cross-entropy</b> <b>can</b> be defined as the <b>number</b> <b>of bits</b> we need <b>to encode</b> information from y using the wrong encoding tool y \u2019. Mathematically, this <b>can</b> be written like this: The other way to write this expression is using expectation: H(y, y\u2019) represents expectation using y and the encoding size using y\u2019. From this, we <b>can</b> conclude that H(y, y\u2019) and H(y\u2019, y) are ...", "dateLastCrawled": "2021-12-19T11:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A <b>Gentle Introduction to Cross-Entropy for Machine Learning</b> \u2013 AiProBlog.Com", "url": "https://www.aiproblog.com/index.php/2019/10/20/a-gentle-introduction-to-cross-entropy-for-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.aiproblog.com/index.php/2019/10/20/a-<b>gentle-introduction-to-cross-entropy</b>...", "snippet": "\u2026 the <b>cross entropy</b> is the average <b>number</b> <b>of bits</b> <b>needed</b> <b>to encode</b> data coming from a source with <b>distribution</b> p when we use model q \u2026 \u2014 Page 57, Machine Learning: A Probabilistic Perspective , 2012.", "dateLastCrawled": "2021-12-28T17:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Introducing <b>Cross-Entropy for Machine Learning</b> - BLOCKGENI", "url": "https://blockgeni.com/introducing-cross-entropy-for-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://blockgeni.com/introducing-<b>cross-entropy-for-machine-learning</b>", "snippet": "<b>Cross-entropy</b> builds upon the idea of entropy from information theory and calculates the <b>number</b> <b>of bits</b> required to represent or transmit an average event from one <b>distribution</b> compared to another <b>distribution</b>. \u2026 the <b>cross entropy</b> is the average <b>number</b> <b>of bits</b> <b>needed</b> <b>to encode</b> data coming from a source with <b>distribution</b> p when we use model q \u2026", "dateLastCrawled": "2022-01-27T13:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Cross entropy</b> calculator | Taskvio", "url": "https://taskvio.com/maths/probability-distributions/cross-entropy/", "isFamilyFriendly": true, "displayUrl": "https://taskvio.com/maths/probability-<b>distributions</b>/<b>cross-entropy</b>", "snippet": "events measure the typical <b>number</b> <b>of bits</b> <b>needed</b> to spot an occasion drawn from the set if a coding the scheme used for the set is optimized for an estimated probability <b>distribution</b> q, instead of truth <b>distribution</b> p. It is most commonly used as machine learning as a function. <b>Cross-entropy</b> may be a measure from the sector of data theory, building upon entropy and usually calculating the difference between two probability distributions. It closely associated with but is different from KL ...", "dateLastCrawled": "2022-02-02T15:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Why smoothing in language modeling?", "url": "https://www.cs.cornell.edu/courses/cs674/2005sp/Handouts/DShultz-chen-goodman-smoothing.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.cornell.edu/courses/cs674/2005sp/Handouts/DShultz-chen-goodman...", "snippet": "Entropy(X): The expected <b>number</b> <b>of bits</b> <b>needed</b> <b>to encode</b> a randomly drawn example from X. Informally, entropy <b>can</b> <b>be thought</b> of as a mathematical measure of information or uncertainty. In general assign short encodings for more probable events and longer encodings for less probable events. If we <b>can</b> <b>encode</b> with fewer <b>bits</b> this implies less uncertainty and more information But what if we don\u2019t know the probability <b>distribution</b> p that generated some data? Use <b>Cross Entropy</b> <b>Cross entropy</b> is ...", "dateLastCrawled": "2021-12-29T11:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Information Theory for Machine Learning</b>: Entropy, <b>Cross Entropy</b>, and KL ...", "url": "https://scriptreference.com/information-theory-for-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://scriptreference.com/<b>information-theory-for-machine-learning</b>", "snippet": "Now let\u2019s perform a <b>thought</b> experiment. The event \u201cRain\u201d has a probability of 0.03125. If this event was pulled from a ... For event x, it is the <b>number</b> <b>of bits</b> that would be <b>needed</b> <b>to encode</b> a uniform <b>distribution</b> wherein each event\u2019s probability is the same as that of x. We might also phrase it as the question, \u201cin a world where everything is as likely as x, how many <b>bits</b> is x worth?\u201d Thus we have: \\[\\begin{align} \\text{events in theoretical uniform <b>distribution</b>} &amp;= \\frac{1}{p ...", "dateLastCrawled": "2021-12-31T15:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>1 Introduction</b> 2 Entropy", "url": "https://www.cs.cmu.edu/~odonnell/toolkit13/lecture20.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.cmu.edu/~odonnell/toolkit13/lecture20.pdf", "snippet": "The entropy <b>can</b> <b>be thought</b> of as any of the following intuitive de nitions: 1. The amount of randomness in X (in <b>bits</b>) 2. Minimum <b>number</b> of random <b>bits</b> you need to generate a draw from X (on average) 3. Average <b>number</b> <b>of bits</b> you need to store a draw from X with compression 4. Minimum <b>number</b> <b>of bits</b> <b>needed</b> for Alice to communicate one draw from ...", "dateLastCrawled": "2022-02-02T14:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Understanding <b>Entropy</b>: the Golden Measurement of Machine Learning | by ...", "url": "https://towardsdatascience.com/understanding-entropy-the-golden-measurement-of-machine-learning-4ea97c663dc3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-<b>entropy</b>-the-golden-measurement-of-machine...", "snippet": "However, KLD measures the relative <b>entropy</b> between two distributions, whereas <b>cross-entropy</b> measures the \u2018total <b>entropy</b>\u2019 between the distributions. The measure is defined as the average <b>number</b> <b>of bits</b> <b>needed</b> <b>to encode</b> data coming from a source with <b>distribution</b> p using model <b>distribution</b> q.", "dateLastCrawled": "2022-02-02T21:46:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A Gentle Introduction to <b>Cross-Entropy for Machine Learning</b>", "url": "https://machinelearningmastery.com/cross-entropy-for-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>cross-entropy-for-machine-learning</b>", "snippet": "<b>Cross-entropy</b> builds upon the idea of entropy from information theory and calculates the <b>number</b> <b>of bits</b> required to represent or transmit an average event from one <b>distribution</b> <b>compared</b> to another <b>distribution</b>. \u2026 the <b>cross entropy</b> is the average <b>number</b> <b>of bits</b> <b>needed</b> <b>to encode</b> data coming from a source with <b>distribution</b> p when we use model q \u2026", "dateLastCrawled": "2022-02-03T03:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Cross-Entropy Loss in ML</b>. What is Entropy in ML? | by Inara Koppert ...", "url": "https://medium.com/unpackai/cross-entropy-loss-in-ml-d9f22fc11fe0", "isFamilyFriendly": true, "displayUrl": "https://medium.com/unpackai/<b>cross-entropy-loss-in-ml</b>-d9f22fc11fe0", "snippet": "Determination of multi-class and multi-label data \u201c\u2026 the <b>cross entropy</b> is the average <b>number</b> <b>of bits</b> <b>needed</b> <b>to encode</b> data coming from a source with <b>distribution</b> p when we use model q ...", "dateLastCrawled": "2022-01-31T20:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Cross Entropy</b> and Kullback-Leibler | aimless agents", "url": "https://aimless-agents.github.io/articles/2021-03/cekl", "isFamilyFriendly": true, "displayUrl": "https://aimless-agents.github.io/articles/2021-03/cekl", "snippet": "Intuitively, it <b>can</b> be thought of as the expected <b>number</b> of extra <b>bits</b> <b>needed</b> <b>to encode</b> outcomes drawn from the true <b>distribution</b> \\(p\\) if we\u2019re using <b>distribution</b> \\(q\\), or \u201cthe measure of inefficiency in using the probability <b>distribution</b> q to approximate the true probability <b>distribution</b> p\u201d (thanks Wikipedia!). If we have a perfect prediction, i.e., our predicted <b>distribution</b> equals the true, then <b>cross entropy</b> equals the true <b>distribution</b>\u2019s entropy, making KL divergence 0 (its ...", "dateLastCrawled": "2022-02-01T00:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "ML&amp;PR_7: <b>Cross Entropy</b>, Kullback-Leibler divergence and Jensen-Shannon ...", "url": "https://khoatranrb.github.io/2020/08/07/ml&pr-7", "isFamilyFriendly": true, "displayUrl": "https://khoatranrb.github.io/2020/08/07/ml&amp;pr-7", "snippet": "mlpr7 1. <b>Cross entropy</b> <b>Cross-entropy</b> is a measure the difference between two probability of distributions and , denoted as: <b>Cross-entropy</b> calculates the <b>number</b> <b>of bits</b> required to represent or transmit an average event from one <b>distribution</b> <b>compared</b> to another <b>distribution</b>. In formula , is the target <b>distribution</b>, is the approximation of the target <b>distribution</b>. By Lagrange multiplier method, we <b>can</b> prove reaches the minimum value when , it means:", "dateLastCrawled": "2021-11-08T12:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A <b>Gentle Introduction to Cross-Entropy for Machine Learning</b> \u2013 AiProBlog.Com", "url": "https://www.aiproblog.com/index.php/2019/10/20/a-gentle-introduction-to-cross-entropy-for-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.aiproblog.com/index.php/2019/10/20/a-<b>gentle-introduction-to-cross-entropy</b>...", "snippet": "Specifically, it builds upon the idea of entropy from information theory and calculates the average <b>number</b> <b>of bits</b> required to represent or transmit an event from one <b>distribution</b> <b>compared</b> to the other <b>distribution</b>. \u2026 the <b>cross entropy</b> is the average <b>number</b> <b>of bits</b> <b>needed</b> <b>to encode</b> data coming from a source with <b>distribution</b> p when we use ...", "dateLastCrawled": "2021-12-28T17:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Probability - <b>Cross Entropy</b>", "url": "http://cross-entropy.net/ML310/Probability_and_Spectral_Clustering.pdf", "isFamilyFriendly": true, "displayUrl": "<b>cross-entropy</b>.net/ML310/Probability_and_Spectral_Clustering.pdf", "snippet": "\u2022Defined as the average <b>number</b> <b>of bits</b> <b>needed</b> <b>to encode</b> data, caused by using <b>distribution</b> ^q <b>to encode</b> the data rather than <b>distribution</b> p \u2022The second term is known as <b>cross entropy</b>: measuring the dissimilarity of the two distributions Information Theory. Mutual Information \u2022Measures the strength of the relationship between variables \u2022Expected value of the ratio of the joint probability to the product of priors \u2022Mutual information <b>can</b> capture relationships missed by Pearsons ...", "dateLastCrawled": "2021-08-30T07:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Lecture 4: Logistic Regression", "url": "https://shuaili8.github.io/Teaching/VE445/L4_logistic%20regression.pdf", "isFamilyFriendly": true, "displayUrl": "https://shuaili8.github.io/Teaching/VE445/L4_logistic regression.pdf", "snippet": "\u2022Discrete <b>distribution</b> with the maximum entropy is the uniform <b>distribution</b> \u2022<b>Cross entropy</b> \u2022 \u2022Is the average <b>number</b> <b>of bits</b> <b>needed</b> <b>to encode</b> data coming from a source with <b>distribution</b> p when we use model q to de\ufb01ne our codebook KL divergence (cont.) 21", "dateLastCrawled": "2022-02-03T01:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) <b>Cross entropy</b>, dissimilarity measures, and characterizations of ...", "url": "https://www.researchgate.net/publication/3084227_Cross_entropy_dissimilarity_measures_and_characterizations_of_Quadratic_Entropy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/3084227_<b>Cross_entropy</b>_dissimilarity_measures...", "snippet": "The <b>cross-entropy</b> H(p, q) indicates the <b>number</b> <b>of bits</b> <b>needed</b> (on average) <b>to encode</b> events drawn from the true <b>distribution</b> p, if using an optimal code for estimated <b>distribution</b> q [172] - [174]. ...", "dateLastCrawled": "2021-12-13T14:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Why do we use Kullback-Leibler <b>divergence</b> rather than <b>cross entropy</b> in ...", "url": "https://stats.stackexchange.com/questions/265966/why-do-we-use-kullback-leibler-divergence-rather-than-cross-entropy-in-the-t-sne", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/265966/why-do-we-use-kullback-leibler...", "snippet": "For example, in classification problems, the commonly used <b>cross entropy</b> loss (aka log loss), measures the <b>cross entropy</b> between the empirical <b>distribution</b> of the labels (given the inputs) and the <b>distribution</b> predicted by the classifier. The empirical <b>distribution</b> for each data point simply assigns probability 1 to the class of that data point, and 0 to all other classes. Side note: The <b>cross entropy</b> in this case turns out to be proportional to the negative log likelihood, so minimizing it ...", "dateLastCrawled": "2022-01-26T18:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Loss Function</b> - Pipline", "url": "https://nuoxu2016.gitbook.io/pipline/preprocessing-1/loss-function", "isFamilyFriendly": true, "displayUrl": "https://nuoxu2016.gitbook.io/pipline/preprocessing-1/<b>loss-function</b>", "snippet": "\u2026 the <b>cross entropy</b> is the average <b>number</b> <b>of bits</b> <b>needed</b> <b>to encode</b> data coming from a source with <b>distribution</b> p when we use model q \u2026 \u2014 Page 57, Machine Learning: A Probabilistic Perspective , 2012.", "dateLastCrawled": "2022-01-24T22:26:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Why and How to use <b>Cross Entropy</b>. The fundamental reasons for ...", "url": "https://towardsdatascience.com/why-and-how-to-use-cross-entropy-4e983cbdd873", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/why-and-how-to-use-<b>cross-entropy</b>-4e983cbdd873", "snippet": "The fundamental reasons for minimizing binary <b>cross entropy</b> (log loss) with probabilistic classification models . Will Arliss. Sep 26, 2020 \u00b7 7 min read. Introduction. This post discusses why logistic regression necessarily uses a different loss function than linear regression. First, the simple yet inefficient way to solve logistic regression will be presented, then the slightly less simple but much more efficient way will be explained and compared. The simple way. Linear regression is the ...", "dateLastCrawled": "2022-01-31T14:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Cross-Entropy</b> Demystified. What is it? Is there any relation to\u2026 | by ...", "url": "https://naokishibuya.medium.com/demystifying-cross-entropy-e80e3ad54a8", "isFamilyFriendly": true, "displayUrl": "https://naokishibuya.medium.com/demystifying-<b>cross-entropy</b>-e80e3ad54a8", "snippet": "However, the <b>machine</b> <b>learning</b> application uses the base e logarithm for implementation convenience. Binary <b>Cross-Entropy</b>. We can use the binary <b>cross-entropy</b> for binary classification where we have yes/no answer. For example, there are only dogs or cats in images. For the binary classifications, the <b>cross-entropy</b> formula contains only two ...", "dateLastCrawled": "2022-01-26T13:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A Gentle Introduction to Information Entropy - <b>Machine</b> <b>Learning</b> Mastery", "url": "https://machinelearningmastery.com/what-is-information-entropy/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/what-is-information-entropy", "snippet": "Calculating information and entropy is a useful tool in <b>machine</b> <b>learning</b> and is used as the basis for techniques such as feature selection, building decision trees, and, more generally, fitting classification models. As such, a <b>machine</b> <b>learning</b> practitioner requires a strong understanding and intuition for information and entropy. In this post, you will discover a gentle introduction to information entropy. After reading this post, you will know: Information theory is concerned with data ...", "dateLastCrawled": "2022-02-02T13:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>machine</b> <b>learning</b> - <b>Cross-entropy loss</b> explanation - Data Science Stack ...", "url": "https://datascience.stackexchange.com/questions/20296/cross-entropy-loss-explanation", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/20296", "snippet": "The answer from Neil is correct. However I think its important to point out that while the loss does not depend on the distribution between the incorrect classes (only the distribution between the correct class and the rest), the gradient of this loss function does effect the incorrect classes differently depending on how wrong they are. So when you use cross-ent in <b>machine</b> <b>learning</b> you will change weights differently for [0.1 0.5 0.1 0.1 0.2] and [0.1 0.6 0.1 0.1 0.1].", "dateLastCrawled": "2022-01-27T03:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine</b> <b>Learning</b> Concepts for Revision | by Raunak Sarada | Medium", "url": "https://raunaksarada-cse21.medium.com/machine-learning-concepts-for-revision-491384952d27", "isFamilyFriendly": true, "displayUrl": "https://raunaksarada-cse21.medium.com/<b>machine</b>-<b>learning</b>-concepts-for-revision-491384952d27", "snippet": "ML Concepts. A.I \u2014 Intelligence showed by machines which is common for humans <b>Machine</b> <b>Learning</b>- Recognize the pattern in data and automatically learn and improve through experience without explicitly being programmed Deep <b>Learning</b>- branch of <b>machine</b> <b>learning</b>.We have to deal with lots of data so in that case problems can\u2019t be solved with simple ML algorithms.", "dateLastCrawled": "2022-01-25T20:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Shannon <b>entropy</b> in the context of <b>machine</b> <b>learning</b> and AI | by Frank ...", "url": "https://medium.com/swlh/shannon-entropy-in-the-context-of-machine-learning-and-ai-24aee2709e32", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/shannon-<b>entropy</b>-in-the-context-of-<b>machine</b>-<b>learning</b>-and-ai-24...", "snippet": "Closely related to <b>cross entropy</b>, the KL divergence from q to p, written DKL(p||q), is another similarity measure often used in <b>machine</b> <b>learning</b>. In the language of Bayesian Inference, DKL(p||q ...", "dateLastCrawled": "2022-01-30T12:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Main concepts behind Machine Learning</b> | by Bruno Eidi Nishimoto ...", "url": "https://medium.com/neuronio/main-concepts-behind-machine-learning-22cd81d68a11", "isFamilyFriendly": true, "displayUrl": "https://medium.com/neuronio/<b>main-concepts-behind-machine-learning</b>-22cd81d68a11", "snippet": "<b>Machine</b> <b>Learning</b> is a concept that is currently trending. It is a subarea from Artificial Intelligence and it consists on the fact that the <b>machine</b> can learn by itself without being explicitly ...", "dateLastCrawled": "2022-01-19T01:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Lecture 4 Fundamentals of deep <b>learning</b> and neural networks", "url": "https://web.stanford.edu/class/biods388/downloads/BIODS388_Lecture_4.pdf", "isFamilyFriendly": true, "displayUrl": "https://web.stanford.edu/class/biods388/downloads/BIODS388_Lecture_4.pdf", "snippet": "Deep <b>learning</b>: <b>Machine</b> <b>learning</b> models based on \u201cdeep\u201d neural networks comprising millions (sometimes billions) of parameters organized into hierarchical layers. Features are multiplied and added together repeatedly, with the outputs from one layer of parameters being fed into the next layer -- before a prediction is made. Contrast with linear regression: Agenda for today - More on the structure of neural network models - <b>Machine</b> <b>learning</b> training loop and concept of loss, in the context ...", "dateLastCrawled": "2022-02-02T09:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Deep Learning and Information Theory</b> \u2013 Deep &amp; Shallow", "url": "https://deep-and-shallow.com/2020/01/09/deep-learning-and-information-theory/", "isFamilyFriendly": true, "displayUrl": "https://deep-and-shallow.com/2020/01/09/<b>deep-learning-and-information-theory</b>", "snippet": "If you have tried to understand the maths behind <b>machine</b> <b>learning</b>, including deep <b>learning</b>, you would have come across topics from Information Theory \u2013 Entropy, <b>Cross Entropy</b>, KL Divergence, etc. The concepts from information theory is ever prevalent in the realm of <b>machine</b> <b>learning</b>, right from the splitting criteria of a Decision Tree to loss functions in Generative Adversarial Networks.", "dateLastCrawled": "2022-02-01T00:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "[D] A Short Introduction to Entropy, <b>Cross-Entropy</b> and KL-Divergence ...", "url": "https://www.reddit.com/r/MachineLearning/comments/7vhmp7/d_a_short_introduction_to_entropy_crossentropy/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/<b>MachineLearning</b>/comments/7vhmp7/d_a_short_introduction_to...", "snippet": "I am having trouble reconciling the concept with the <b>analogy</b>. At 2:35 even if a rainy day was 25% likely, there&#39;s still only two states, rainy and sunny, and therefor only 1 bit of information is needed to convey that, so only one bit of data needs to be sent, even though the 1 bit of data reduces the uncertainty of a rainy day by a factor of 4. I quite don&#39;t get what he means by this being 2 bits of information. I guess where I am stuck is how the uncertainty reduction factor translates to ...", "dateLastCrawled": "2021-08-20T08:03:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Beat the Bookmakers With Tree-Based <b>Machine</b> <b>Learning</b> Algorithms | by ...", "url": "https://medium.com/analytics-vidhya/beat-the-bookmakers-with-tree-based-machine-learning-algorithms-1d349335b54", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/beat-the-bookmakers-with-tree-based-<b>machine</b>...", "snippet": "<b>Cross-entropy is similar</b> to Gini Impurity, but it involves using the concept of entropy from information theory. This article won\u2019t go in depth about it, but essentially, as the cross-entropy ...", "dateLastCrawled": "2022-01-26T04:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>A Traveler\u2019s Diary on the Road to Machine</b> <b>Learning</b> - Chapter 1 | by ...", "url": "https://medium.com/swlh/a-travelers-diary-on-the-road-to-machine-learning-chapter-1-8850ec5b4243", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/<b>a-travelers-diary-on-the-road-to-machine</b>-<b>learning</b>-chapter-1...", "snippet": "Types of <b>Machine</b> <b>Learning</b> algorithms: ... Sparse categorical <b>cross entropy is similar</b> to categorical cross entropy, only difference is it uses only one value as target. It saves memory as well as ...", "dateLastCrawled": "2021-05-21T04:27:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Deep Learning for Computer Architects</b> | Chen Jeff - Academia.edu", "url": "https://www.academia.edu/40860009/Deep_Learning_for_Computer_Architects", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/40860009/<b>Deep_Learning_for_Computer_Architects</b>", "snippet": "This text serves as a primer for computer architects in a new and rapidly evolving \ufb01eld. We review how <b>machine</b> <b>learning</b> has evolved since its inception in the 1960s and track the key developments leading up to the emergence of the powerful deep <b>learning</b> techniques that emerged in the last decade.", "dateLastCrawled": "2022-01-28T02:18:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(cross-entropy)  is like +(number of bits needed to encode a distribution)", "+(cross-entropy) is similar to +(number of bits needed to encode a distribution)", "+(cross-entropy) can be thought of as +(number of bits needed to encode a distribution)", "+(cross-entropy) can be compared to +(number of bits needed to encode a distribution)", "machine learning +(cross-entropy AND analogy)", "machine learning +(\"cross-entropy is like\")", "machine learning +(\"cross-entropy is similar\")", "machine learning +(\"just as cross-entropy\")", "machine learning +(\"cross-entropy can be thought of as\")", "machine learning +(\"cross-entropy can be compared to\")"]}