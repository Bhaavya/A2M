{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A Gentle Introduction to Calculating the <b>BLEU</b> Score for <b>Text</b> in Python", "url": "https://machinelearningmastery.com/calculate-bleu-score-for-text-python/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/calculate-<b>bleu</b>-score-for-<b>text</b>-pyth", "snippet": "<b>BLEU</b>, or the <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>, is a score for comparing a candidate translation of <b>text</b> to one or more reference <b>translations</b>. Although developed for translation, it can be <b>used</b> to evaluate <b>text</b> generated for a suite of natural language processing tasks. In this tutorial, you will discover the <b>BLEU</b> score for evaluating and scoring candidate <b>text</b> using the NLTK library in", "dateLastCrawled": "2022-01-30T22:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>BLEU</b>: a Method for <b>Automatic Evaluation of Machine Translation</b>", "url": "https://www.researchgate.net/publication/2588204_BLEU_a_Method_for_Automatic_Evaluation_of_Machine_Translation", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/2588204_", "snippet": "We describe <b>two</b> metrics for <b>automatic evaluation of machine trans-lation</b> quality. These metrics, <b>BLEU</b> and NEE, are compared to hu-man judgment of quality of translation of Arabic, Chinese, French ...", "dateLastCrawled": "2022-02-01T20:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A detailed review of prevailing image captioning methods using deep ...", "url": "https://link.springer.com/article/10.1007%2Fs11042-021-11293-1", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11042-021-11293-1", "snippet": "<b>BLEU</b> (<b>Bilingual</b> <b>evaluation</b> <b>understudy</b>) Papineni et al. anticipated the idea of the <b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) metric. <b>BLEU</b>&#39;s concept is to design a <b>machine</b> having a high resemblance to the translation with the human behavioral pattern . The original <b>BLEU</b> metric was developed <b>to compare</b> and count the repetitive, overlapping n-grams ...", "dateLastCrawled": "2022-02-01T17:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Evaluation</b> of automatic video captioning using direct assessment ...", "url": "https://europepmc.org/article/MED/30180174", "isFamilyFriendly": true, "displayUrl": "https://europepmc.org/article/MED/30180174", "snippet": "<b>BLEU</b> (<b>bilingual</b> <b>evaluation</b> <b>understudy</b>) is a metric <b>used</b> in MT and was one of the first metrics to achieve a high correlation with human judgments of quality. It is known to perform poorly if it is <b>used</b> to evaluate the quality of short segments of <b>text</b>, <b>like</b> single sentences for example rather than to evaluate variants of <b>text</b> at a corpus level. In the VTT task in TRECVid 2016, the videos are independent thus there is no corpus to work from, so our expectations are lowered when it comes to ...", "dateLastCrawled": "2021-10-13T23:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Keyphrase based <b>Evaluation</b> of Automatic <b>Text</b> Summarization", "url": "https://research.ijcaonline.org/volume117/number7/pxc3902953.pdf", "isFamilyFriendly": true, "displayUrl": "https://research.ijcaonline.org/volume117/number7/pxc3902953.pdf", "snippet": "<b>Bleu</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) [9] is an n-gram precision based evaluator metric initially designed for the <b>evaluation</b> of <b>machine</b> translation. The main idea of <b>BLEU</b> is to measure the translation closeness between a candidate translation and a set of reference <b>translations</b> with a numerical metric. They use a weighted average of variable length n-gram matches between system <b>translations</b> and a set of human reference <b>translations</b>. Lin et al. [8] have applied the same idea of <b>Bleu</b> to the ...", "dateLastCrawled": "2021-11-19T12:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Attention for Neural Machine Translation (NMT</b>)", "url": "https://www.linkedin.com/pulse/attention-neural-machine-translation-nmt-ajay-taneja", "isFamilyFriendly": true, "displayUrl": "https://www.linkedin.com/pulse/attention-neural-<b>machine</b>-translation-nmt-ajay-taneja", "snippet": "The <b>BLEU</b> score, which stands for a <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>. It&#39;s an algorithm that was developed to solve some of the most difficult problems in NLP, including <b>Machine</b> Translation. It&#39;s ...", "dateLastCrawled": "2022-01-23T13:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A F MEASUREMENT OF DIVERSITY IN DATASETS WITH RANDOM NETWORK DISTILLATION", "url": "https://openreview.net/pdf?id=1RqyBxJU_Wy", "isFamilyFriendly": true, "displayUrl": "https://openreview.net/pdf?id=1RqyBxJU_Wy", "snippet": "<b>used</b> <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b> (<b>BLEU</b>) score as a metric for evaluating the quality of <b>machine</b> translated <b>text</b>. <b>BLEU</b> uses a modi\ufb01ed form of precision <b>to compare</b> a candidate translation against multiple reference <b>translations</b>, where diversity ideally can be evaluated by including all plausible <b>translations</b> as references when computing the score. However, this requires massive annotation cost, and it remains dif\ufb01cult to capture all viable <b>translations</b> for a given sentence. Li et al ...", "dateLastCrawled": "2022-01-23T13:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Comparison of Different Orthographies for <b>Machine</b> Translation of Under ...", "url": "https://www.readkong.com/page/comparison-of-different-orthographies-for-machine-3624122", "isFamilyFriendly": true, "displayUrl": "https://www.readkong.com/page/comparison-of-different-orthographies-for-<b>machine</b>-3624122", "snippet": "The authors have shown that there was a significant improvement in <b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) score and shown that the problem of data sparsity is reduced. In the work by [8], the authors translated lexicon induction for a heavily code-switched <b>text</b> of historically unwritten colloquial words via loanwords using expert knowledge with just language information. Their method is to take word pronunciation (IPA) from a donor language and convert them in the borrowing language. This ...", "dateLastCrawled": "2021-12-18T00:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "SUGAMAN: describing floor plans for visually impaired by annotation ...", "url": "https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/iet-ipr.2018.5627", "isFamilyFriendly": true, "displayUrl": "https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/iet-ipr.2018.5627", "snippet": "For that purpose several metrics for example <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b> (<b>BLEU</b>) , Recall-Oriented <b>Understudy</b> for ... we have compared <b>machine</b>-generated description with these descriptions and an analysis regarding closeness of <b>machine</b> <b>translations</b> with human-written descriptions is done. Next, we describe the steps in our framework in detail. 5 Semantic segmentation and room classification. In all the previous approaches available in the literature, rooms have been classified by ...", "dateLastCrawled": "2022-01-24T16:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Sivaji Bandyopadhyay</b> - ACL Anthology", "url": "https://aclanthology.org/people/s/sivaji-bandyopadhyay/", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/people/s/<b>sivaji-bandyopadhyay</b>", "snippet": "The evaluated results are declared at the LoResMT 2020 shared task, which reports that our system achieves the <b>bilingual</b> <b>evaluation</b> <b>understudy</b> (<b>BLEU</b>) score of 0.59, precision score of 3.43, recall score of 5.48, F-measure score of 4.22, and rank-based intuitive <b>bilingual</b> <b>evaluation</b> score (RIBES) of 0.180147 in Russian to Hindi translation. And for Hindi to Russian translation, we have achieved <b>BLEU</b>, precision, recall, F-measure, and RIBES score of 1.11, 4.72, 4.41, 4.56, and 0.026842 ...", "dateLastCrawled": "2022-01-23T10:57:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A Gentle Introduction to Calculating the <b>BLEU</b> Score for <b>Text</b> in Python", "url": "https://machinelearningmastery.com/calculate-bleu-score-for-text-python/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/calculate-<b>bleu</b>-score-for-<b>text</b>-pyth", "snippet": "<b>BLEU</b>, or the <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>, is a score for comparing a candidate translation of <b>text</b> to one or more reference <b>translations</b>. Although developed for translation, it can be <b>used</b> to evaluate <b>text</b> generated for a suite of natural language processing tasks. In this tutorial, you will discover the <b>BLEU</b> score for evaluating and scoring candidate <b>text</b> using the NLTK library in", "dateLastCrawled": "2022-01-30T22:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A detailed review of prevailing image captioning methods using deep ...", "url": "https://link.springer.com/article/10.1007%2Fs11042-021-11293-1", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11042-021-11293-1", "snippet": "<b>BLEU</b> (<b>Bilingual</b> <b>evaluation</b> <b>understudy</b>) Papineni et al. anticipated the idea of the <b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) metric. <b>BLEU</b>&#39;s concept is to design a <b>machine</b> having a high resemblance to the translation with the human behavioral pattern . The original <b>BLEU</b> metric was developed <b>to compare</b> and count the repetitive, overlapping n-grams ...", "dateLastCrawled": "2022-02-01T17:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>BLEU</b>: a Method for <b>Automatic Evaluation of Machine Translation</b>", "url": "https://www.researchgate.net/publication/2588204_BLEU_a_Method_for_Automatic_Evaluation_of_Machine_Translation", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/2588204_", "snippet": "We describe <b>two</b> metrics for <b>automatic evaluation of machine trans-lation</b> quality. These metrics, <b>BLEU</b> and NEE, are compared to hu-man judgment of quality of translation of Arabic, Chinese, French ...", "dateLastCrawled": "2022-02-01T20:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Extending BLEU Evaluation Method with Linguistic Weight</b> | Request PDF", "url": "https://www.researchgate.net/publication/221301858_Extending_BLEU_Evaluation_Method_with_Linguistic_Weight", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221301858_Extending_<b>BLEU</b>_<b>Evaluation</b>_Method...", "snippet": "There are many automatic methods <b>used</b> to evaluate different <b>machine</b> translators, one of these methods; <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b> (<b>BLEU</b>) method. <b>BLEU</b> is <b>used</b> to evaluate translation quality ...", "dateLastCrawled": "2021-11-08T17:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Towards Neural <b>Similarity</b> Evaluators - OpenReview", "url": "https://openreview.net/pdf?id=S1xkQac9LB", "isFamilyFriendly": true, "displayUrl": "https://openreview.net/pdf?id=S1xkQac9LB", "snippet": "<b>Evaluation</b> metrics play a central role in the <b>machine</b> <b>learning</b> community. They direct the efforts of the research community and are <b>used</b> to de\ufb01ne the state of the art models. In <b>machine</b> translation and summarization, the <b>two</b> most common metrics <b>used</b> for evaluating <b>similarity</b> between candidate and reference texts are <b>BLEU</b> [Papineni et al., 2002] and ROUGE [Lin, 2004]. Both approaches rely on counting the matching n-grams in the candidates summary to n-grams in the reference <b>text</b>. <b>BLEU</b> is ...", "dateLastCrawled": "2022-01-15T22:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Information | Free Full-<b>Text</b> | A Literature Survey of Recent Advances ...", "url": "https://www.mdpi.com/2078-2489/13/1/41/htm", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/2078-2489/13/1/41/htm", "snippet": "<b>bilingual</b> <b>evaluation</b> <b>understudy</b> (<b>BLEU</b>) is widely <b>used</b> to assess various NLP tasks, even though it was first implemented to measure <b>machine</b> translation outputs. The <b>BLEU</b> metric assigns a value to a translation on a scale of 0 to 1, however it is typically expressed as a percentage. The closer the translation is to 1, the more closely it resembles a human translation. Simply said, the <b>BLEU</b> metric counts the number of words that overlap in a translation when compared to a reference translation ...", "dateLastCrawled": "2022-02-01T05:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Evaluation</b> of automatic video captioning using direct assessment ...", "url": "https://europepmc.org/article/MED/30180174", "isFamilyFriendly": true, "displayUrl": "https://europepmc.org/article/MED/30180174", "snippet": "<b>BLEU</b> (<b>bilingual</b> <b>evaluation</b> <b>understudy</b>) is a metric <b>used</b> in MT and was one of the first metrics to achieve a high correlation with human judgments of quality. It is known to perform poorly if it is <b>used</b> to evaluate the quality of short segments of <b>text</b>, like single sentences for example rather than to evaluate variants of <b>text</b> at a corpus level. In the VTT task in TRECVid 2016, the videos are independent thus there is no corpus to work from, so our expectations are lowered when it comes to ...", "dateLastCrawled": "2021-10-13T23:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A F MEASUREMENT OF DIVERSITY IN DATASETS WITH RANDOM NETWORK DISTILLATION", "url": "https://openreview.net/pdf?id=1RqyBxJU_Wy", "isFamilyFriendly": true, "displayUrl": "https://openreview.net/pdf?id=1RqyBxJU_Wy", "snippet": "<b>used</b> <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b> (<b>BLEU</b>) score as a metric for evaluating the quality of <b>machine</b> translated <b>text</b>. <b>BLEU</b> uses a modi\ufb01ed form of precision <b>to compare</b> a candidate translation against multiple reference <b>translations</b>, where diversity ideally can be evaluated by including all plausible <b>translations</b> as references when computing the score. However, this requires massive annotation cost, and it remains dif\ufb01cult to capture all viable <b>translations</b> for a given sentence. Li et al ...", "dateLastCrawled": "2022-01-23T13:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Comparison of Different Orthographies for <b>Machine</b> Translation of Under ...", "url": "https://www.readkong.com/page/comparison-of-different-orthographies-for-machine-3624122", "isFamilyFriendly": true, "displayUrl": "https://www.readkong.com/page/comparison-of-different-orthographies-for-<b>machine</b>-3624122", "snippet": "The authors have shown that there was a significant improvement in <b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) score and shown that the problem of data sparsity is reduced. In the work by [8], the authors translated lexicon induction for a heavily code-switched <b>text</b> of historically unwritten colloquial words via loanwords using expert knowledge with just language information. Their method is to take word pronunciation (IPA) from a donor language and convert them in the borrowing language. This ...", "dateLastCrawled": "2021-12-18T00:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Sivaji Bandyopadhyay</b> - ACL Anthology", "url": "https://aclanthology.org/people/s/sivaji-bandyopadhyay/", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/people/s/<b>sivaji-bandyopadhyay</b>", "snippet": "The evaluated results are declared at the LoResMT 2020 shared task, which reports that our system achieves the <b>bilingual</b> <b>evaluation</b> <b>understudy</b> (<b>BLEU</b>) score of 0.59, precision score of 3.43, recall score of 5.48, F-measure score of 4.22, and rank-based intuitive <b>bilingual</b> <b>evaluation</b> score (RIBES) of 0.180147 in Russian to Hindi translation. And for Hindi to Russian translation, we have achieved <b>BLEU</b>, precision, recall, F-measure, and RIBES score of 1.11, 4.72, 4.41, 4.56, and 0.026842 ...", "dateLastCrawled": "2022-01-23T10:57:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Human <b>Evaluation</b> Of <b>Machine</b> Translation", "url": "https://groups.google.com/g/ug5zfjhnv/c/JfGeRYENjvg", "isFamilyFriendly": true, "displayUrl": "https://groups.google.com/g/ug5zfjhnv/c/JfGeRYENjvg", "snippet": "The <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b> Score, <b>can</b>, this ever increase our number of faces and vertices of the models thus its complexity therefore better not desired in VR applications. At out end as each report date also summarize key points with him own Insights. Stay informed on the latest trending ML papers with code, ignoring the position. <b>Machine</b> translation has contribute a long ways over the years. Also, electrochemical and industrial processes, September. Contrary to <b>BLEU</b> which aims to ...", "dateLastCrawled": "2022-01-22T08:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A thorough review of models, <b>evaluation</b> metrics, and datasets on image ...", "url": "https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/ipr2.12367", "isFamilyFriendly": true, "displayUrl": "https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/ipr2.12367", "snippet": "<b>BLEU</b> The <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b> (<b>BLEU</b>) method is adopted to evaluate the quality of translated sentences in <b>machine</b> translation. It compares each translation segment with a set of reference <b>translations</b> with good translation quality and calculates each segment score then estimates the overall quality of the translation. In the field of image description, as a <b>similarity</b> measurement method, <b>BLEU</b> adopts an n \u2212 g r a m matching rule. The <b>BLEU</b> <b>evaluation</b> metric <b>can</b> be evaluated by ...", "dateLastCrawled": "2022-01-10T19:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Information | Free Full-<b>Text</b> | A Literature Survey of Recent Advances ...", "url": "https://www.mdpi.com/2078-2489/13/1/41/htm", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/2078-2489/13/1/41/htm", "snippet": "<b>bilingual</b> <b>evaluation</b> <b>understudy</b> (<b>BLEU</b>) is widely <b>used</b> to assess various NLP tasks, even though it was first implemented to measure <b>machine</b> translation outputs. The <b>BLEU</b> metric assigns a value to a translation on a scale of 0 to 1, however it is typically expressed as a percentage. The closer the translation is to 1, the more closely it resembles a human translation. Simply said, the <b>BLEU</b> metric counts the number of words that overlap in a translation when compared to a reference translation ...", "dateLastCrawled": "2022-02-01T05:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Attention for Neural Machine Translation (NMT</b>)", "url": "https://www.linkedin.com/pulse/attention-neural-machine-translation-nmt-ajay-taneja", "isFamilyFriendly": true, "displayUrl": "https://www.linkedin.com/pulse/attention-neural-<b>machine</b>-translation-nmt-ajay-taneja", "snippet": "The <b>BLEU</b> score, which stands for a <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>. It&#39;s an algorithm that was developed to solve some of the most difficult problems in NLP, including <b>Machine</b> Translation. It&#39;s ...", "dateLastCrawled": "2022-01-23T13:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Evaluation</b> of <b>Text Generation</b>: A Survey | DeepAI", "url": "https://deepai.org/publication/evaluation-of-text-generation-a-survey", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>evaluation</b>-of-<b>text-generation</b>-a-survey", "snippet": "The <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b> (<b>bleu</b>) is one of the first metrics <b>used</b> to measure <b>the similarity</b> between <b>two</b> sentences (Papineni et al., 2002). Originally proposed for <b>machine</b> translation, it compares a candidate translation of <b>text</b> to one or more reference <b>translations</b>. <b>bleu</b>. is a weighted geometric mean of . n-gram precision scores ...", "dateLastCrawled": "2022-02-03T15:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Automatic Evaluation of Summaries Using</b> n-gram Co-occurrence Statistics ...", "url": "https://www.researchgate.net/publication/224890498_Automatic_Evaluation_of_Summaries_Using_n-gram_Co-occurrence_Statistics", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/224890498_Automatic_<b>Evaluation</b>_of_Summaries...", "snippet": "<b>BLEU</b> (<b>BiLingual</b> <b>Evaluation</b> <b>Understudy</b>) (Papineni et al., 2002) is a metric <b>used</b> to evaluate the quality of <b>machine</b> <b>translations</b> by measuring the n-gram overlap of prediction and ground truth.", "dateLastCrawled": "2022-01-19T02:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) Assamese-English <b>Bilingual</b> <b>Machine</b> Translation", "url": "https://www.researchgate.net/publication/263774231_Assamese-English_Bilingual_Machine_Translation", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/263774231_Assamese-English_<b>Bilingual</b>_<b>Machine</b>...", "snippet": "The NMT-2 system has achieved a higher <b>bilingual</b> <b>evaluation</b> <b>understudy</b> (<b>BLEU</b>) score of 10.03 for English-to-Assamese and <b>BLEU</b> score 13.10 for Assamese-to-English translation, respectively. View ...", "dateLastCrawled": "2021-12-16T04:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "NUBIA: NeUral Based Interchangeability Assessor for <b>Text</b> Generation ...", "url": "https://deepai.org/publication/nubia-neural-based-interchangeability-assessor-for-text-generation", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/nubia-neural-based-interchangeability-assessor-for-<b>text</b>...", "snippet": "In many <b>text</b> generation tasks, especially in <b>machine</b> translation and summarization, the <b>two</b> most common metrics <b>used</b> for evaluating <b>similarity</b> between candidate and reference texts are <b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>)(Papineni et al., 2002) and ROUGE (Recall-Oriented <b>Understudy</b> for Gisting <b>Evaluation</b>)(Lin, 2004).", "dateLastCrawled": "2021-12-06T00:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "DataHunter: A System for Finding Datasets", "url": "https://data-hunter.io/possible_queries", "isFamilyFriendly": true, "displayUrl": "https://data-hunter.io/possible_queries", "snippet": "The comparative analysis evaluates the systems by three performance <b>evaluation</b> measures: <b>BLEU</b> (<b>BiLingual</b> <b>Evaluation</b> <b>Understudy</b>), METEOR (Metric for <b>Evaluation</b> of Translation with Explicit ORdering) and NIST (National Institute of Standard and Technology) with the help of a standard corpus. The results show that Google translator is far better than Bing and Babylon translators. It outperforms, on the average, Babylon by 28.55% and Bing by 15.74%.", "dateLastCrawled": "2022-01-26T22:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Bert Perplexity [KFQIDB]", "url": "https://request.to.it/Bert_Perplexity.html", "isFamilyFriendly": true, "displayUrl": "https://request.to.it/Bert_Perplexity.html", "snippet": "<b>BLEU</b>, or the <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>, is a score for comparing a candidate translation of <b>text</b> to one or more reference <b>translations</b>. Label Perplexity Music. Catalog PMWC001. Training operations use Volta Tensor Core and runs for 45,000 steps to reach perplexity equal to 34. They had been staying with people thirty kilometers from here. Methods for Function perplexity. *FREE* shipping on qualifying offers. Unfortunately, large-scale training is very. The optimal number of topics is ...", "dateLastCrawled": "2022-01-29T08:23:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>BLEU</b>: a Method for <b>Automatic Evaluation of Machine Translation</b>", "url": "https://www.researchgate.net/publication/2588204_BLEU_a_Method_for_Automatic_Evaluation_of_Machine_Translation", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/2588204_", "snippet": "We describe <b>two</b> metrics for <b>automatic evaluation of machine trans-lation</b> quality. These metrics, <b>BLEU</b> and NEE, are <b>compared</b> to hu-man judgment of quality of translation of Arabic, Chinese, French ...", "dateLastCrawled": "2022-02-01T20:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A thorough review of models, <b>evaluation</b> metrics, and datasets on image ...", "url": "https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/ipr2.12367", "isFamilyFriendly": true, "displayUrl": "https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/ipr2.12367", "snippet": "<b>BLEU</b> The <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b> (<b>BLEU</b>) method is adopted to evaluate the quality of translated sentences in <b>machine</b> translation. It compares each translation segment with a set of reference <b>translations</b> with good translation quality and calculates each segment score then estimates the overall quality of the translation. In the field of image description, as a <b>similarity</b> measurement method, <b>BLEU</b> adopts an n \u2212 g r a m matching rule. The <b>BLEU</b> <b>evaluation</b> metric <b>can</b> be evaluated by ...", "dateLastCrawled": "2022-02-03T14:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Top PDF <b>Bleu</b>: a Method for Automatic <b>Evaluation</b> of <b>Machine</b> Translation ...", "url": "https://1library.net/title/bleu-method-automatic-evaluation-machine-translation", "isFamilyFriendly": true, "displayUrl": "https://1library.net/title/<b>bleu</b>-method-automatic-<b>evaluation</b>-<b>machine</b>-translation", "snippet": "We present a <b>machine</b> <b>learning</b> approach to evaluating the well- formedness of output of a <b>machine</b> translation system, using classifiers that learn to distinguish human reference <b>translations</b> from <b>machine</b> <b>translations</b>. This approach <b>can</b> be <b>used</b> to evaluate an MT system, tracking improvements over time; to aid in the kind of failure analysis that <b>can</b> help guide system development; and to select among alternative output strings. The method presented is fully automated and independent of source ...", "dateLastCrawled": "2022-01-28T19:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Extending BLEU Evaluation Method with Linguistic Weight</b> | Request PDF", "url": "https://www.researchgate.net/publication/221301858_Extending_BLEU_Evaluation_Method_with_Linguistic_Weight", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221301858_Extending_<b>BLEU</b>_<b>Evaluation</b>_Method...", "snippet": "There are many automatic methods <b>used</b> to evaluate different <b>machine</b> translators, one of these methods; <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b> (<b>BLEU</b>) method. <b>BLEU</b> is <b>used</b> to evaluate translation quality ...", "dateLastCrawled": "2021-11-08T17:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A detailed review of prevailing image captioning methods using deep ...", "url": "https://link.springer.com/article/10.1007%2Fs11042-021-11293-1", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11042-021-11293-1", "snippet": "<b>BLEU</b> (<b>Bilingual</b> <b>evaluation</b> <b>understudy</b>) Papineni et al. anticipated the idea of the <b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) metric. <b>BLEU</b>&#39;s concept is to design a <b>machine</b> having a high resemblance to the translation with the human behavioral pattern . The original <b>BLEU</b> metric was developed <b>to compare</b> and count the repetitive, overlapping n-grams ...", "dateLastCrawled": "2022-02-01T17:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Evaluation</b> of automatic video captioning using direct assessment ...", "url": "https://europepmc.org/article/MED/30180174", "isFamilyFriendly": true, "displayUrl": "https://europepmc.org/article/MED/30180174", "snippet": "<b>BLEU</b> (<b>bilingual</b> <b>evaluation</b> <b>understudy</b>) is a metric <b>used</b> in MT and was one of the first metrics to achieve a high correlation with human judgments of quality. It is known to perform poorly if it is <b>used</b> to evaluate the quality of short segments of <b>text</b>, like single sentences for example rather than to evaluate variants of <b>text</b> at a corpus level. In the VTT task in TRECVid 2016, the videos are independent thus there is no corpus to work from, so our expectations are lowered when it comes to ...", "dateLastCrawled": "2021-10-13T23:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "SUGAMAN: describing floor plans for visually impaired by annotation ...", "url": "https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/iet-ipr.2018.5627", "isFamilyFriendly": true, "displayUrl": "https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/iet-ipr.2018.5627", "snippet": "For that purpose several metrics for example <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b> (<b>BLEU</b>) , Recall-Oriented <b>Understudy</b> for Gisting <b>Evaluation</b> ... The whole system is divided into <b>two</b> stages (i) room annotation <b>learning</b> and (ii) description synthesis. At first, room semantic information is extracted by room segmentation process, which gives all the required information about an input floor plan image. For example, individual room area, door information which gives room neighbourhood information ...", "dateLastCrawled": "2022-01-24T16:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "BERTScore_and_COMET_blog/MT Blog.md at main - <b>github.com</b>", "url": "https://github.com/huakehe/BERTScore_and_COMET_blog/blob/main/MT%20Blog.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/huakehe/BERTScore_and_COMET_blog/blob/main/MT Blog.md", "snippet": "In 2002, the most commonly <b>used</b> <b>evaluation</b> metric, <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b> (<b>BLEU</b>), was developed by Kishore et al. [2]. <b>BLEU</b> measures the difference between references and <b>machine</b> translation candidates through n-grams and brevity penalty. Based on the preliminary that the \u201chighest correlation with monolingual human judgements\u201d is four, n-grams measure the exact word segment correspondence of length one to four in the sentence pair. The brevity penalty is included to avoid short ...", "dateLastCrawled": "2021-12-03T07:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Comparison of Different Orthographies for <b>Machine</b> Translation of Under ...", "url": "https://www.readkong.com/page/comparison-of-different-orthographies-for-machine-3624122", "isFamilyFriendly": true, "displayUrl": "https://www.readkong.com/page/comparison-of-different-orthographies-for-<b>machine</b>-3624122", "snippet": "The authors have shown that there was a significant improvement in <b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) score and shown that the problem of data sparsity is reduced. In the work by [8], the authors translated lexicon induction for a heavily code-switched <b>text</b> of historically unwritten colloquial words via loanwords using expert knowledge with just language information. Their method is to take word pronunciation (IPA) from a donor language and convert them in the borrowing language. This ...", "dateLastCrawled": "2021-12-18T00:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Proceedings of the Sixth Conference on <b>Machine</b> Translation - ACL Anthology", "url": "https://aclanthology.org/volumes/2021.wmt-1/", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/volumes/2021.wmt-1", "snippet": "Our model has achieved a <b>bilingual</b> <b>evaluation</b> <b>understudy</b> (<b>BLEU</b>) score of 4.05, rank-based intuitive <b>bilingual</b> <b>evaluation</b> score (RIBES) score of 24.80 and translation edit rate (TER) score of 97.24 for both Tamil-to-Telugu and Telugu-to-Tamil <b>translations</b> respectively. pdf bib abs Low Resource Similar Language Neural <b>Machine</b> Translation for T amil-T elugu Vandan Mujadia | Dipti Sharma. This paper describes the participation of team oneNLP (LTRC, IIIT-Hyderabad) for the WMT 2021 task, similar ...", "dateLastCrawled": "2022-02-02T20:35:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine</b> <b>Learning</b> Glossary: Language <b>Evaluation</b> | Google Developers", "url": "https://developers.google.com/machine-learning/glossary/language", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine</b>-<b>learning</b>/glossary/language", "snippet": "<b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) #language. A score between 0.0 and 1.0, inclusive, indicating the quality of a translation between two human languages (for example, between English and Russian). A <b>BLEU</b> score of 1.0 indicates a perfect translation; a <b>BLEU</b> score of 0.0 indicates a terrible translation. C. causal language model. #language. Synonym for unidirectional language model. See bidirectional language model to contrast different directional approaches in language modeling. crash ...", "dateLastCrawled": "2022-01-29T02:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Evaluation</b> of an <b>NLP</b> model \u2014 latest benchmarks | by Ria Kulshrestha ...", "url": "https://towardsdatascience.com/evaluation-of-an-nlp-model-latest-benchmarks-90fd8ce6fae5", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>evaluation</b>-of-an-<b>nlp</b>-model-latest-benchmarks-90fd8ce6fae5", "snippet": "<b>BLEU</b> Score \u2014 <b>BiLingual</b> <b>Evaluation</b> <b>Understudy</b>. As the name suggests, it was originally used to evaluate translations from one language to another. How to calculate <b>BLEU</b> score? Calculating unigram precision: Step 1: Look at each word in the output sentence and assign it a score of 1 if it shows up in any of the reference sentences and 0 if it doesn\u2019t. Step 2: Normalize that count, so that it\u2019s always between 0 and 1, by dividing the number of words that showed up in one of the reference ...", "dateLastCrawled": "2022-01-28T07:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Natrual language processing basic concepts - language model - word ...", "url": "https://shuffleai.blog/blog/nlp_concepts_part_1.html", "isFamilyFriendly": true, "displayUrl": "https://shuffleai.blog/blog/nlp_concepts_part_1.html", "snippet": "<b>BLEU</b> stands for <b>bilingual</b> <b>evaluation</b> <b>understudy</b>. It&#39;s an automatic metric to evaluate how close a sequence of text generated by a language model is to a reference. At first, it&#39;s used to evaluate the quality of <b>machine</b> translation text. Now other natural language processing tasks such as task-oriented dialogue generation adopt it as well. For a reference &quot;The man returned to the store&quot;, a generated text &quot;the the man the&quot; would get a BLUE score as below. For each word in the generated text ...", "dateLastCrawled": "2021-12-24T07:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) pathak2019.pdf | Aditya Kumar Pathak and Priyankit Acharya ...", "url": "https://www.academia.edu/38228943/pathak2019_pdf", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/38228943/pathak2019_pdf", "snippet": "The standard metric people are using for <b>evaluation</b> of MT systems is <b>BLEU</b> score.<b>Bilingual</b> <b>evaluation</b> <b>understudy</b> (<b>BLEU</b>) is the algorithm to determine the quality of text translated by a <b>machine</b> translation. Quality is the comparison between <b>machine</b>-translated output to that of human-generated output; the closer <b>machine</b> translation is to human-generated translation, the better is the <b>BLEU</b> score. <b>BLEU</b> score is a n-gram overlap of <b>machine</b> translation to that of reference translation.<b>BLEU</b> \u00bc min ...", "dateLastCrawled": "2021-02-16T17:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "9.7. <b>Sequence</b> to <b>Sequence</b> <b>Learning</b> \u2014 Dive into Deep <b>Learning</b> 0.17.2 ...", "url": "https://d2l.ai/chapter_recurrent-modern/seq2seq.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_recurrent-modern/seq2seq.html", "snippet": "<b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>), though originally proposed for evaluating <b>machine</b> translation results [Papineni et al., 2002], has been extensively used in measuring the quality of output sequences for different applications.", "dateLastCrawled": "2022-01-26T00:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "The <b>Computational</b> Limits of Deep <b>Learning</b> | DeepAI", "url": "https://deepai.org/publication/the-computational-limits-of-deep-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/the-<b>computational</b>-limits-of-deep-<b>learning</b>", "snippet": "<b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) [papineni2002bleu] score is a metric for translation and computes the similarity between human translation and <b>machine</b> translation based on n-gram. An n-gram is a continuous sequence of n items from a given text. The score is based on precision, brevity penalty, and clipping. The modified n-gram precision ...", "dateLastCrawled": "2022-01-28T07:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Automatic Image Captioning Based on ResNet50 and</b> LSTM with ... - <b>Hindawi</b>", "url": "https://www.hindawi.com/journals/wcmc/2020/8909458/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.hindawi.com</b>/journals/wcmc/2020/8909458", "snippet": "<b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) is an algorithm that measures the precision of an -gram between the generated and reference captions. <b>BLEU</b>-() scores can be calculated based on the length of the reference sentence, the generated sentence, the uniform weights, and the modified -gram precisions.", "dateLastCrawled": "2022-01-30T14:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Arti cial Intelligence Master Thesis", "url": "https://upcommons.upc.edu/bitstream/handle/2117/105513/122533.pdf?sequence=1", "isFamilyFriendly": true, "displayUrl": "https://upcommons.upc.edu/bitstream/handle/2117/105513/122533.pdf?sequence=1", "snippet": "2:87 <b>BLEU</b> (<b>BiLingual</b> <b>Evaluation</b> <b>Understudy</b>) score over the baseline; attention model, for German-English translation, and 0:34 <b>BLEU</b> score improvement for Catalan-Spanish trans-lation. Keywords <b>Machine</b> <b>Learning</b>, Deep <b>Learning</b>, Natural Language Processing, Neural <b>Machine</b> Transla-tion", "dateLastCrawled": "2021-12-27T01:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Machine</b> translation from text to sign language: a systematic review ...", "url": "https://link.springer.com/article/10.1007/s10209-021-00823-1", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10209-021-00823-1", "snippet": "The SMT component of the system uses MOSES for generating a language model and decodes the input sentence. The approach uses the <b>BLEU</b> metric for <b>evaluation</b> and reports the scores as <b>BLEU</b>-4 12.64% <b>BLEU</b>-3 19.28% <b>BLEU</b>-2 31.48% <b>BLEU</b>-1 53.17%. The results reported are satisfactory; however, the system needs a virtual avatar tool for completeness.", "dateLastCrawled": "2022-01-30T15:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(PDF) <b>Evaluation of machine translation systems and related procedures</b>", "url": "https://www.researchgate.net/publication/326320090_Evaluation_of_machine_translation_systems_and_related_procedures", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/326320090_<b>Evaluation</b>_of_<b>machine</b>_translation...", "snippet": "<b>Evaluation of machine translation systems and related procedures</b>. June 2018 ; Journal of Engineering and Applied Sciences 13(12):3961-3972; Project: <b>Machine</b> <b>learning</b>; Authors: Musatafa Albadr ...", "dateLastCrawled": "2022-01-15T04:04:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(bleu (bilingual evaluation understudy))  is like +(machine learning technique used to compare the similarity of two text translations)", "+(bleu (bilingual evaluation understudy)) is similar to +(machine learning technique used to compare the similarity of two text translations)", "+(bleu (bilingual evaluation understudy)) can be thought of as +(machine learning technique used to compare the similarity of two text translations)", "+(bleu (bilingual evaluation understudy)) can be compared to +(machine learning technique used to compare the similarity of two text translations)", "machine learning +(bleu (bilingual evaluation understudy) AND analogy)", "machine learning +(\"bleu (bilingual evaluation understudy) is like\")", "machine learning +(\"bleu (bilingual evaluation understudy) is similar\")", "machine learning +(\"just as bleu (bilingual evaluation understudy)\")", "machine learning +(\"bleu (bilingual evaluation understudy) can be thought of as\")", "machine learning +(\"bleu (bilingual evaluation understudy) can be compared to\")"]}