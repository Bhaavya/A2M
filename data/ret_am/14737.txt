{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Markov decision</b> processes - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/0377221789903482", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/0377221789903482", "snippet": "The intent of this paper is to review a mathe- matically-based optimization <b>model</b> of discrete- stage, sequential <b>decision</b> making in a stochastic <b>environment</b>, called the <b>Markov decision process</b> (<b>MDP</b>). The dynamics of the <b>MDP</b> are described by a controlled <b>Markov</b> <b>process</b>. An (uncon- trolled) <b>Markov</b> <b>process</b>--the concept of which is due to A.A. <b>Markov</b> (1856-1922)--is a well- studied stochastic <b>process</b> that has been used to <b>model</b> a variety of stochastic dynamic systems, e.g., clinical medicine ...", "dateLastCrawled": "2021-12-06T07:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "CE 273 <b>Markov</b> <b>Decision</b> Processes", "url": "http://civil.iisc.ac.in/~tarunr/CE_273/Lecture_22.pdf", "isFamilyFriendly": true, "displayUrl": "civil.iisc.ac.in/~tarunr/CE_273/Lecture_22.pdf", "snippet": "Previously on <b>Markov</b> <b>Decision</b> Processes In such cases, we can write the state as (x k;y k) where x k is a ected by u k and y k is not. Let p i represent the pmf of y k. In such cases, the DP algorithm can be simpli ed as J^ k(x k) = Xm i=1 p iJ k(x k;i) J^ k(x k) = Xm i=1 p i min uk2Uk(xk) E wk \u02c6 g k(x k;u k;w k) + J^ k+1(f k(x k;u k;w k))jy k = i \u02d9 In the case of Tetris, x k is the board con guration and y k is the shape of the block. There is no exogenous disturbance and the action ...", "dateLastCrawled": "2022-01-20T04:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A <b>Markov Decision Process</b> Approach for Cost-Benefit Analysis of ...", "url": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3657479", "isFamilyFriendly": true, "displayUrl": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3657479", "snippet": "In this paper, we develop a novel <b>Markov decision process</b> (<b>MDP</b>) <b>model</b> for CBA of infrastructure resilience upgrades that offer prevention (reduce the probability of a disaster) and/or protection (mitigate the cost of a disaster) benefits. Stochastic features of the <b>model</b> include disaster occurrences and whether or not a disaster terminates the effective life of an earlier resilience upgrade. From our <b>MDP</b> <b>model</b>, we derive analytical expressions for the <b>decision</b>-<b>maker&#39;s</b> willingness to pay (WTP ...", "dateLastCrawled": "2022-01-01T02:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Markov Decision Process</b> Measurement <b>Model</b> | SpringerLink", "url": "https://link.springer.com/article/10.1007/s11336-017-9570-0", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11336-017-9570-0", "snippet": "A <b>Markov decision process</b> (<b>MDP</b>) is a <b>model</b> for <b>decision</b> making in the presence of uncertainty based on a longitudinal cost\u2013benefit analysis (Puterman, 1994).MDPs have been used extensively in artificial intelligence and robotics to choose optimal actions in stochastic, dynamic situations (Mnih et al. 2015; Russell &amp; Norvig, 2009) and in economics to <b>model</b> individual choice strategies (Rust, 1994). Formally, an <b>MDP</b> is defined by \\(\\{S,A,T,R,\\gamma \\}\\) where S is the set of possible states ...", "dateLastCrawled": "2021-12-30T09:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "It\u2019s <b>Markov</b> All The Way Down (A <b>Mental Model For Investing &amp; Everything</b> ...", "url": "https://macro-ops.com/its-markov-all-the-way-down-a-mental-model-for-investing-everything-else/", "isFamilyFriendly": true, "displayUrl": "https://macro-ops.com/its-<b>markov</b>-all-the-way-down-a-mental-<b>model</b>-for-<b>investing</b>...", "snippet": "<b>Markov Decision Process</b>: The Holy Grail of Investor Probability Models . An <b>MDP</b> provides a mathematical framework for modeling <b>decision</b>-making in situations where outcomes are partly random and partly under the control of the <b>decision</b>-maker. The <b>model</b> goes <b>like</b> this: At each step, the <b>process</b> is in some state (s) <b>Decision</b>-maker may choose any action (a) that is available in that state (s) <b>Process</b> responds by randomly moving to a new state (s1) The new state gives the <b>decision</b>-maker a reward ...", "dateLastCrawled": "2022-02-03T07:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "It&#39;s <b>Markov</b> All The Way Down - Macro Ops Musings", "url": "https://macroops.substack.com/p/its-markov-all-the-way-down", "isFamilyFriendly": true, "displayUrl": "https://macroops.substack.com/p/its-<b>markov</b>-all-the-way-down", "snippet": "<b>Markov Decision Process</b>: The Holy Grail of Investor Probability Models An <b>MDP</b> provides a mathematical framework for modeling <b>decision</b>-making in situations where outcomes are partly random and partly under the control of the <b>decision</b>-maker. The <b>model</b> goes <b>like</b> this: At each step, the <b>process</b> is in some state (s)<b>Decision</b>-maker may choose any action (a) that is available in that state (s)<b>Process</b> responds by randomly moving to a new state (s1)The new state gives the <b>decision</b>-maker a reward (Ra ...", "dateLastCrawled": "2021-12-27T08:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "CE 273 <b>Markov</b> <b>Decision</b> Processes", "url": "http://civil.iisc.ac.in/~tarunr/CE_273/Lecture_23.pdf", "isFamilyFriendly": true, "displayUrl": "civil.iisc.ac.in/~tarunr/CE_273/Lecture_23.pdf", "snippet": "I <b>Markov Decision Process</b> ! Dynamic choice models Lecture 23 Inverse Reinforcement Learning. 4/37 Previously on <b>Markov</b> <b>Decision</b> Processes Just <b>like</b> static choice models, the expression for the probability can be written as follows P[a t jx t] = P[v t(x t;a t) + t (a t) &gt;v t(x t;a0) + t(a0)8a02A tnfa tg] where v t(x t;a t) is the conditional value function, which is a measure of the utility from choosing a t at time t and behaving \\optimally&quot; thereafter. Recall that the conditional value ...", "dateLastCrawled": "2022-01-18T14:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "An introduction to Reinforcement Learning | by Sara Ghibaudo | Betacom ...", "url": "https://medium.com/betacom/an-introduction-to-reinforcement-learning-b749abd1f281", "isFamilyFriendly": true, "displayUrl": "https://medium.com/betacom/an-introduction-to-reinforcement-learning-b749abd1f281", "snippet": "At this point, it is important to introduce the <b>Markov Decision Process</b> (<b>MDP</b>) that is defined by a set of states, actions, rewards, and a state transition probability. It models <b>decision</b>-making ...", "dateLastCrawled": "2021-12-16T18:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Reinforcement-Learning \u2013 NoSimpler", "url": "https://www.nosimpler.me/reinforcement-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.nosimpler.me/reinforcement-learning", "snippet": "Formally this is called <b>Markov Decision Process</b> (<b>MDP</b>) where the agent can see all the numbers in the <b>environment</b> and use that information to come up with the <b>Markov</b> State. <b>Markov</b> <b>decision</b> processes formally describe an <b>environment</b> for reinforcement learning. Where the <b>environment</b> is fully observable, that is the current state completely ...", "dateLastCrawled": "2022-01-06T06:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Fanalytics U Class 5: In-Game Decisions", "url": "https://www.fandomanalytics.com/post/fanalytics-class-5-in-game-decisions", "isFamilyFriendly": true, "displayUrl": "https://www.fandomanalytics.com/post/fanalytics-class-5-in-game-<b>decisions</b>", "snippet": "The best tool for providing in-game <b>decision</b> support analytics is the <b>Markov Decision Process</b> (<b>MDP</b>). Informally, a <b>Markov</b> <b>Process</b> is a stochastic <b>model</b> of how the <b>environment</b> evolves based on the current state. Really informally, the idea of a <b>Markov</b> <b>process</b> is that a situation (a game) evolves somewhat randomly from play to play. The coach can affect how the game evolves. The <b>MDP</b> framework involves several elements. First, there are a set of variables that define the state of the ...", "dateLastCrawled": "2022-01-11T07:53:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 0, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Markov decision process</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Markov_decision_process", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Markov_decision_process</b>", "snippet": "In mathematics, a <b>Markov decision process</b> (<b>MDP</b>) is a discrete-time stochastic control <b>process</b>. It provides a mathematical framework for modeling <b>decision</b> making in situations where outcomes are partly random and partly under the control <b>of a decision</b> maker. MDPs are useful for studying optimization problems solved via dynamic programming.MDPs were known at least as early as the 1950s; a core body of research on <b>Markov</b> <b>decision</b> processes resulted from Ronald Howard&#39;s 1960 book, Dynamic ...", "dateLastCrawled": "2022-02-02T15:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Markov Decision Process</b> Measurement <b>Model</b> | SpringerLink", "url": "https://link.springer.com/article/10.1007/s11336-017-9570-0", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11336-017-9570-0", "snippet": "A <b>Markov decision process</b> (<b>MDP</b>) is a <b>model</b> for <b>decision</b> making in the presence of uncertainty based on a longitudinal cost\u2013benefit analysis (Puterman, 1994).MDPs have been used extensively in artificial intelligence and robotics to choose optimal actions in stochastic, dynamic situations (Mnih et al. 2015; Russell &amp; Norvig, 2009) and in economics <b>to model</b> individual choice strategies (Rust, 1994). Formally, an <b>MDP</b> is defined by \\(\\{S,A,T,R,\\gamma \\}\\) where S is the set of possible states ...", "dateLastCrawled": "2021-12-30T09:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Markov decision</b> processes - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/0377221789903482", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/0377221789903482", "snippet": "The intent of this paper is to review a mathe- matically-based optimization <b>model</b> of discrete- stage, sequential <b>decision</b> making in a stochastic <b>environment</b>, called the <b>Markov decision process</b> (<b>MDP</b>). The dynamics of the <b>MDP</b> are described by a controlled <b>Markov</b> <b>process</b>. An (uncon- trolled) <b>Markov</b> <b>process</b>--the concept of which is due to A.A. <b>Markov</b> (1856-1922)--is a well- studied stochastic <b>process</b> that has been used <b>to model</b> a variety of stochastic dynamic systems, e.g., clinical medicine ...", "dateLastCrawled": "2021-12-06T07:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A stochastic <b>model</b> of acute-care decisions based on patient and ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5415592/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC5415592", "snippet": "A <b>Markov Decision Process</b> (<b>MDP</b>) <b>model</b> provides a framework for representing multi-stage <b>decision</b> problems in the presence of uncertainty. Markovian models are commonly used in health care to support screening, diagnosis, and treatment decisions for chronic conditions, patient flow, and hospital operations optimization [16\u201319].", "dateLastCrawled": "2020-02-12T08:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "CE 273 <b>Markov</b> <b>Decision</b> Processes", "url": "http://civil.iisc.ac.in/~tarunr/CE_273/Lecture_23.pdf", "isFamilyFriendly": true, "displayUrl": "civil.iisc.ac.in/~tarunr/CE_273/Lecture_23.pdf", "snippet": "<b>Markov</b> <b>Decision</b> Processes Lecture 23 Inverse Reinforcement Learning Lecture 23 Inverse Reinforcement Learning. 2/37 Previously on <b>Markov</b> <b>Decision</b> Processes Discrete choice theory is built on the assumption that <b>decision</b> <b>makers</b> calculate the utility from di erent alternatives and choose the one with maximum utility. But an analyst may not have access to many attributes that individuals consider when choosing an alternative. For instance, consider the problem of selecting a mode. <b>Decision</b> ...", "dateLastCrawled": "2022-01-18T14:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Fanalytics U Class 5: In-Game Decisions", "url": "https://www.fandomanalytics.com/post/fanalytics-class-5-in-game-decisions", "isFamilyFriendly": true, "displayUrl": "https://www.fandomanalytics.com/post/fanalytics-class-5-in-game-<b>decisions</b>", "snippet": "The best tool for providing in-game <b>decision</b> support analytics is the <b>Markov Decision Process</b> (<b>MDP</b>). Informally, a <b>Markov</b> <b>Process</b> is a stochastic <b>model</b> of how the <b>environment</b> evolves based on the current state. Really informally, the idea of a <b>Markov</b> <b>process</b> is that a situation (a game) evolves somewhat randomly from play to play. The coach can affect how the game evolves. The <b>MDP</b> framework involves several elements. First, there are a set of variables that define the state of the ...", "dateLastCrawled": "2022-01-11T07:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Experts in a Markov Decision Process</b>. - ResearchGate", "url": "https://www.researchgate.net/publication/221620535_Experts_in_a_Markov_Decision_Process", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221620535_<b>Experts_in_a_Markov_Decision_Process</b>", "snippet": "The online <b>Markov decision process</b> (<b>MDP</b>) is a generalization of the classical <b>Markov decision process</b> that incorporates changing reward functions. In this paper, we propose practical online <b>MDP</b> ...", "dateLastCrawled": "2022-01-25T00:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Proximal Policy Optimization for Multi-rotor UAV Autonomous Guidance ...", "url": "https://link.springer.com/article/10.1007/s42405-021-00427-2", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s42405-021-00427-2", "snippet": "A <b>Markov decision process</b> <b>model</b> with two stages of long-distance autonomous guidance and short-distance autonomous tracking of obstacle avoidance was developed in this study, aiming to address the performance problem of multi-rotor unmanned aerial vehicles (UAV) to ground dynamic target. On this basis, an improved proximal policy optimization (PPO) algorithm is proposed. The proposed algorithm uses long short-term memory (LSTM) network to calculate reward values, update network parameters ...", "dateLastCrawled": "2022-02-06T18:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "2D Racing game using reinforcement learning and supervised learning", "url": "https://neuro.cs.ut.ee/wp-content/uploads/2018/02/2d_racing.pdf", "isFamilyFriendly": true, "displayUrl": "https://neuro.cs.ut.ee/wp-content/uploads/2018/02/2d_racing.pdf", "snippet": "2.2 <b>Markov decision process</b> All the different approaches of reinforcement learning use a common mathematical formulation of how the agent interacts with the <b>environment</b>. The <b>environment</b> is modelled as a <b>Markov Decision Process</b> (<b>MDP</b>), which works as follows: There is a set of actions A and a set of states S. By performing some action a\u2208A, the agent can move from state to state. At each time step, the <b>process</b> is in some state \ud835\udc60, and the <b>decision</b> maker may choose any action \ud835\udc4e\u2208\ud835\udc34that ...", "dateLastCrawled": "2022-02-03T01:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Decision</b> prioritization and causal reasoning in <b>decision</b> hierarchies", "url": "https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1009688", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1009688", "snippet": "For small problems the usual approach is to transform the POMDP over latent states into a fully-observable <b>Markov decision process</b> (<b>MDP</b>) over belief states\u2013a probability distribution over the latent states\u2013and solve it using Bellman\u2019s equation [15, 16]. However, this solution works only for problems with few states and actions and a short planning horizon; for more complex problems, efficient planning must rely on heuristic strategies and relaxations that are less well characterized ...", "dateLastCrawled": "2022-01-05T03:48:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "International Journal of Engineering Applied Sciences and Technology ...", "url": "https://www.ijeast.com/papers/1-6,Tesma201,IJEAST.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.ijeast.com/papers/1-6,Tesma201,IJEAST.pdf", "snippet": "<b>MDP</b> <b>can</b> <b>be thought</b> of as appropriate method to deal with a discrete time stochastic control <b>process</b>. Reinforcement learning (RL) <b>can</b> solve <b>Markov</b> <b>decision</b> processes without distinct specification of the transition probabilities [11]. RL is learning <b>process</b> which interacts with an <b>environment</b>.", "dateLastCrawled": "2022-01-16T11:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "It\u2019s <b>Markov</b> All The Way Down (A <b>Mental Model For Investing &amp; Everything</b> ...", "url": "https://macro-ops.com/its-markov-all-the-way-down-a-mental-model-for-investing-everything-else/", "isFamilyFriendly": true, "displayUrl": "https://macro-ops.com/its-<b>markov</b>-all-the-way-down-a-mental-<b>model</b>-for-<b>investing</b>...", "snippet": "This essay dives deep into The Partially Observable <b>Markov Decision Process</b> or POMDP. The POMDP is one of four <b>Markov</b> Models developed by Russian Mathematician Andrey <b>Markov</b>. <b>Markov</b> Models helps us think about avoiding collisions not only in planes but in life and <b>investing</b>. Distilling the noise. Creating an <b>environment</b> where we <b>can</b> make informed decisions with varying degrees of confidence. These models allow us to have peace in our inability to forecast anything with precision. It frees us ...", "dateLastCrawled": "2022-02-03T07:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Markov decision</b> processes - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/0377221789903482", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/0377221789903482", "snippet": "The intent of this paper is to review a mathe- matically-based optimization <b>model</b> of discrete- stage, sequential <b>decision</b> making in a stochastic <b>environment</b>, called the <b>Markov decision process</b> (<b>MDP</b>). The dynamics of the <b>MDP</b> are described by a controlled <b>Markov</b> <b>process</b>. An (uncon- trolled) <b>Markov</b> <b>process</b>--the concept of which is due to A.A. <b>Markov</b> (1856-1922)--is a well- studied stochastic <b>process</b> that has been used to <b>model</b> a variety of stochastic dynamic systems, e.g., clinical medicine ...", "dateLastCrawled": "2021-12-06T07:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Temporal concatenation for <b>Markov</b> <b>decision</b> processes | Probability in ...", "url": "https://www.cambridge.org/core/journals/probability-in-the-engineering-and-informational-sciences/article/abs/temporal-concatenation-for-markov-decision-processes/9F799719DCA333A132DCA3D957B2FD80", "isFamilyFriendly": true, "displayUrl": "https://www.cambridge.org/core/journals/probability-in-the-engineering-and...", "snippet": "1. Introduction. We are interested in devising computationally efficient architectures for solving finite-horizon <b>Markov</b> <b>decision</b> processes (<b>MDP</b>), a popular framework for modeling multi-stage <b>decision</b>-making problems [Reference Bellman 1, Reference Puterman 22] with a wide range of applications from scheduling in data and call centers [Reference Harrison and Zeevi 12] to energy management with intermittent renewable resources [Reference Harsha and Dahleh 13].In an <b>MDP</b>, at each stage, an ...", "dateLastCrawled": "2022-01-24T20:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Reinforcement-Learning \u2013 NoSimpler", "url": "https://www.nosimpler.me/reinforcement-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.nosimpler.me/reinforcement-learning", "snippet": "Formally this is called <b>Markov Decision Process</b> (<b>MDP</b>) where the agent <b>can</b> see all the numbers in the <b>environment</b> and use that information to come up with the <b>Markov</b> State. <b>Markov</b> <b>decision</b> processes formally describe an <b>environment</b> for reinforcement learning. Where the <b>environment</b> is fully observable, that is the current state completely characterizes the <b>process</b>. Almost all RL problems <b>can</b> be formalized as MDPs. Partially observable problems <b>can</b> be converted into MDPs. Bandits are MDPs with ...", "dateLastCrawled": "2022-01-06T06:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "It\u2019s <b>Markov</b> All The Way Down (A Mental <b>Model</b> For Investing &amp; Everything ...", "url": "https://valuewalkpremium.com/its-markov-all-the-way-down-a-mental-model-for-investing-everything-else/", "isFamilyFriendly": true, "displayUrl": "https://valuewalkpremium.com/its-<b>markov</b>-all-the-way-down-a-mental-<b>model</b>-for-investing...", "snippet": "This essay dives deep into The Partially Observable <b>Markov Decision Process</b> or POMDP. The POMDP is one of four <b>Markov</b> Models developed by Russian Mathematician Andrey <b>Markov</b>. <b>Markov</b> Models helps us think about avoiding collisions not only in planes but in life and investing. Distilling the noise. Creating an <b>environment</b> where we <b>can</b> make informed decisions with varying degrees of confidence. These models allow us to have peace in our inability to forecast anything with precision. It frees us ...", "dateLastCrawled": "2022-01-10T23:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "It&#39;s <b>Markov</b> All The Way Down - Macro Ops Musings", "url": "https://macroops.substack.com/p/its-markov-all-the-way-down", "isFamilyFriendly": true, "displayUrl": "https://macroops.substack.com/p/its-<b>markov</b>-all-the-way-down", "snippet": "Price movements are numerical expressions of the crowd\u2019s Hidden <b>Markov</b> <b>Model</b> at work! This is why divorcing short-term price movements from long-term business performance is important. The stock price tells us nothing about the long-term probabilities of a given set of observations. HMMs get us closer in understanding how to interact with the partially observable investment landscape. But we <b>can</b> do better. Enter the <b>Markov Decision Process</b> (<b>MDP</b>) and Partially Observed <b>Markov</b> <b>Decision</b> ...", "dateLastCrawled": "2021-12-27T08:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) <b>An MDP-based recommender system</b> - ResearchGate", "url": "https://www.researchgate.net/publication/234054512_An_MDP-based_recommender_system", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/234054512_<b>An_MDP-based_recommender_system</b>", "snippet": "<b>MDP-based recommender system</b> using this predictive <b>model</b>. 3.1 The Basic <b>Model</b>. A <b>Markov</b> chain is a <b>model</b> of system dynamics \u2013 in our case, user \u201cdynamics. \u201d To use it, we need. to formulate ...", "dateLastCrawled": "2022-01-22T08:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Robust Markov Decision Processes</b> - researchgate.net", "url": "https://www.researchgate.net/publication/46449898_Robust_Markov_Decision_Processes", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/46449898_<b>Robust_Markov_Decision_Processes</b>", "snippet": "<b>Markov</b> <b>Decision</b> Processes (<b>MDP</b>) are a widely used <b>model</b> for dynamic <b>decision</b>-making problems. However, MDPs require precise specification of <b>model</b> parameters, and often the cost of a policy <b>can</b> be ...", "dateLastCrawled": "2022-01-13T13:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Decision</b> prioritization and causal reasoning in <b>decision</b> hierarchies", "url": "https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1009688", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1009688", "snippet": "For small problems the usual approach is to transform the POMDP over latent states into a fully-observable <b>Markov decision process</b> (<b>MDP</b>) over belief states\u2013a probability distribution over the latent states\u2013and solve it using Bellman\u2019s equation [15, 16]. However, this solution works only for problems with few states and actions and a short planning horizon; for more complex problems, efficient planning must rely on heuristic strategies and relaxations that are less well characterized ...", "dateLastCrawled": "2022-01-05T03:48:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A stochastic <b>model</b> of acute-care decisions based on patient and ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5415592/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC5415592", "snippet": "A <b>Markov Decision Process</b> (<b>MDP</b>) <b>model</b> provides a framework for representing multi-stage <b>decision</b> problems in the presence of uncertainty. Markovian models are commonly used in health care to support screening, diagnosis, and treatment decisions for chronic conditions, patient flow, and hospital operations optimization [16\u201319].", "dateLastCrawled": "2020-02-12T08:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Markov Decision Process</b> Measurement <b>Model</b> | SpringerLink", "url": "https://link.springer.com/article/10.1007/s11336-017-9570-0", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11336-017-9570-0", "snippet": "A <b>Markov decision process</b> (<b>MDP</b>) is a <b>model</b> for <b>decision</b> making in the presence of uncertainty based on a longitudinal cost\u2013benefit analysis (Puterman, 1994).MDPs have been used extensively in artificial intelligence and robotics to choose optimal actions in stochastic, dynamic situations (Mnih et al. 2015; Russell &amp; Norvig, 2009) and in economics <b>to model</b> individual choice strategies (Rust, 1994). Formally, an <b>MDP</b> is defined by \\(\\{S,A,T,R,\\gamma \\}\\) where S is the set of possible states ...", "dateLastCrawled": "2021-12-30T09:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Humanoid robot path planning with fuzzy <b>Markov</b> <b>decision</b> processes ...", "url": "https://www.sciencedirect.com/science/article/pii/S1665642316300700", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1665642316300700", "snippet": "<b>Markov</b> <b>decision</b> processes. <b>Markov</b> <b>decision</b> processes (<b>MDP</b>) <b>can</b> be considered as an extension of <b>Markov</b> chains with some differences such as allowing choice and giving motivation. <b>MDP</b>, is a mathematical <b>decision</b> making tool which may be applicable in situations where series are partly random and partly under the control <b>of a decision</b> maker.", "dateLastCrawled": "2022-01-26T11:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Large Scale <b>Markov</b> <b>Decision</b> Processes with Changing Rewards", "url": "https://proceedings.neurips.cc/paper/8505-large-scale-markov-decision-processes-with-changing-rewards.pdf", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/8505-large-scale-<b>markov</b>-<b>decision</b>-<b>process</b>es-with...", "snippet": "a new state according to some known transition probability. In particular, the standard <b>MDP</b> <b>model</b> assumes that the <b>decision</b> maker has complete knowledge of the reward function r(s;a), which does not change over time. Over the past two decades, there has been much interest in sequential learning and <b>decision</b> making in an unknown and possibly adversarial <b>environment</b>. A wide range of sequential learning problems <b>can</b> be modeled using the framework of Online Convex Optimization (OCO) [45, 20]. In ...", "dateLastCrawled": "2022-01-26T17:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Experts in a Markov Decision Process</b>. - ResearchGate", "url": "https://www.researchgate.net/publication/221620535_Experts_in_a_Markov_Decision_Process", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221620535_<b>Experts_in_a_Markov_Decision_Process</b>", "snippet": "One formalism that <b>can</b> be used <b>to model</b> the kind of situations described above is a non-stationary <b>Markov Decision Process</b> (<b>MDP</b>), where the set of states represented by observations of the ...", "dateLastCrawled": "2022-01-25T00:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "No-Regret Learning with High-Probability in Adversarial <b>Markov</b> <b>Decision</b> ...", "url": "https://proceedings.mlr.press/v161/ghasemi21a/ghasemi21a.pdf", "isFamilyFriendly": true, "displayUrl": "https://proceedings.mlr.press/v161/ghasemi21a/ghasemi21a.pdf", "snippet": "<b>decision</b>-making systems is that of <b>Markov</b> <b>decision</b> pro-cesses (MDPs). MDPs enable modeling <b>decision</b>-<b>makers</b> (learners) that need to make a sequence of decisions in the presence of uncertainty in the <b>decision</b>-<b>maker\u2019s</b> <b>environment</b>. In this scenario, a loss (or reward) function captures the task expected from the learner. Therefore, the <b>decision</b>-<b>maker\u2019s</b> goal is to design a learning algorithm that, despite operating under uncertainty, learns a policy with the lowest cumulative loss (or the ...", "dateLastCrawled": "2022-02-03T07:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "It\u2019s <b>Markov</b> All The Way Down (A <b>Mental Model For Investing &amp; Everything</b> ...", "url": "https://macro-ops.com/its-markov-all-the-way-down-a-mental-model-for-investing-everything-else/", "isFamilyFriendly": true, "displayUrl": "https://macro-ops.com/its-<b>markov</b>-all-the-way-down-a-mental-<b>model</b>-for-<b>investing</b>...", "snippet": "Price movements are numerical expressions of the crowd\u2019s Hidden <b>Markov</b> <b>Model</b> at work! This is why divorcing short-term price movements from long-term business performance is important. The stock price tells us nothing about the long-term probabilities of a given set of observations. HMMs get us closer in understanding how to interact with the partially observable investment landscape. But we <b>can</b> do better. Enter the <b>Markov Decision Process</b> (<b>MDP</b>) and Partially Observed <b>Markov</b> <b>Decision</b> ...", "dateLastCrawled": "2022-02-03T07:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "No-Regret Learning with High-Probability in Adversarial <b>Markov</b> <b>Decision</b> ...", "url": "https://auai.org/uai2021/pdf/uai2021.370.pdf", "isFamilyFriendly": true, "displayUrl": "https://auai.org/uai2021/pdf/uai2021.370.pdf", "snippet": "<b>decision</b>-making systems is that of <b>Markov</b> <b>decision</b> pro-cesses (MDPs). MDPs enable modeling <b>decision</b>-<b>makers</b> (learners) that need to make a sequence of decisions in the presence of uncertainty in the <b>decision</b>-<b>maker\u2019s</b> <b>environment</b>. In this scenario, a loss (or reward) function captures the task *The first two authors contributed equally. \u2020 expected from the learner. Therefore, the <b>decision</b>-<b>maker\u2019s</b> goal is to design a learning algorithm that, despite operating under uncertainty, learns a ...", "dateLastCrawled": "2021-11-30T02:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Poisoning finite-horizon Markov decision processes at</b> design time ...", "url": "https://www.sciencedirect.com/science/article/pii/S0305054820303026", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0305054820303026", "snippet": "<b>Markov</b> <b>decision</b> processes (MDPs) are utilized in a variety of system controllers, and attacks against them are of particular interest, even though this problem structure is relatively understudied in the adversarial learning literature. Therefore, in this research, we consider the finite-horizon <b>MDP</b> poisoning problem wherein an adversary perturbs a <b>decision</b> <b>maker\u2019s</b> baseline <b>MDP</b> formulation to induce desired behavior while balancing the risk of attack detection. We formally define the ...", "dateLastCrawled": "2021-11-27T23:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Proximal Policy Optimization for Multi-rotor UAV Autonomous Guidance ...", "url": "https://link.springer.com/article/10.1007/s42405-021-00427-2", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s42405-021-00427-2", "snippet": "A <b>Markov decision process</b> <b>model</b> with two stages of long-distance autonomous guidance and short-distance autonomous tracking of obstacle avoidance was developed in this study, aiming to address the performance problem of multi-rotor unmanned aerial vehicles (UAV) to ground dynamic target. On this basis, an improved proximal policy optimization (PPO) algorithm is proposed. The proposed algorithm uses long short-term memory (LSTM) network to calculate reward values, update network parameters ...", "dateLastCrawled": "2022-02-06T18:32:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Why does <b>Markov Decision Process</b> matter in Reinforcement <b>Learning</b>? | by ...", "url": "https://towardsdatascience.com/why-does-malkov-decision-process-matter-in-reinforcement-learning-b111b46b41bd", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/why-does-malkov-<b>decision</b>-<b>process</b>-matter-in...", "snippet": "It is named by <b>analogy</b> to \u201cone-armed bandit\u201d(= a slot <b>machine</b>) although the framework has k levers instead of one. ... we introduce <b>Markov Decision Process</b>(<b>MDP</b>) to solve such a problem. An <b>MDP</b> consists of two elements; the agent and the environment. The agent is a learner or <b>decision</b>-maker. In the above example, the agent is the rabbit. The environment is everything surrounding the agent. In the example, the environment includes everything in the field where the rabbit is with food and ...", "dateLastCrawled": "2022-01-31T01:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Introduction to Machine Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/24/introduction-to-machine-learning-2/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/24/<b>introduction-to-machine-learning</b>-2", "snippet": "<b>Markov decision process</b> (<b>MDP</b>): In reinforcement <b>learning</b>, <b>MDP</b> is a mathematical framework for modeling <b>decision</b>-making of an agent in situations or environments where outcomes are partly random and partly under control. In this model, environment is modeled as a set of states and actions that can be performed by an agent to control the system\u2019s state. The objective is to control the system in such a way that the agent\u2019s total payoff is maximized.", "dateLastCrawled": "2022-01-17T05:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Markov Decision Process</b>: How Does Value Iteration Work? | Baeldung on ...", "url": "https://www.baeldung.com/cs/mdp-value-iteration", "isFamilyFriendly": true, "displayUrl": "https://www.baeldung.com/cs/<b>mdp</b>-value-iteration", "snippet": "From this point, we can make an <b>analogy</b> with the <b>Markov</b> model since the solution for this problem is a sequence of actions. A <b>Markov Decision Process</b> is used to model the agent, considering that the agent itself generates a series of actions. In the real world, we can have observable, hidden, or partially observed states, depending on the ...", "dateLastCrawled": "2022-01-30T20:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "18.1. <b>Markov Decision Process</b> (<b>MDP</b>) \u2014 Dive into Deep <b>Learning</b> 0.17.0 ...", "url": "http://preview.d2l.ai.s3-website-us-west-2.amazonaws.com/d2l-en/master/chapter_reinforcement_learning/mdp.html", "isFamilyFriendly": true, "displayUrl": "preview.d2l.ai.s3-website-us-west-2.amazonaws.com/...reinforcement_<b>learning</b>/<b>mdp</b>.html", "snippet": "In this section, we will discuss how to formulate reinforcement <b>learning</b> problems using <b>Markov</b> <b>decision</b> processes (MDPs) and describe in detail various components of MDPs. Definition of an <b>MDP</b> \u00b6 A <b>Markov decision process</b> (<b>MDP</b>) is a model for how the state of a system evolves as different actions are applied to it.", "dateLastCrawled": "2022-01-27T05:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Reinforcement <b>Learning</b> | Ioannis Anifantakis | Analytics Vidhya", "url": "https://medium.com/analytics-vidhya/reinforcement-learning-basic-understanding-4fcb91ba4e4", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/reinforcement-<b>learning</b>-basic-understanding-4fcb91ba4e4", "snippet": "<b>Markov Decision Process</b> (<b>MDP</b>) The States, Actions, Rewards, their mechanics (known as One-Step Dynamics ), together with the discount rate (\u03b3) define a <b>Markov Decision Process</b> (<b>MDP</b>) .", "dateLastCrawled": "2021-08-02T19:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Markov decision process</b>: policy iteration with code implementation | by ...", "url": "https://medium.com/@ngao7/markov-decision-process-policy-iteration-42d35ee87c82?source=post_internal_links---------0-------------------------------", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@ngao7/<b>markov-decision-process</b>-policy-iteration-42d35ee87c82?source=...", "snippet": "<b>Markov decision process</b>: policy iteration with code implementation . Nan. Dec 19, 2021 \u00b7 16 min read. In today\u2019s story we focus on policy iteration of <b>MDP</b>. We are still using the grid world ...", "dateLastCrawled": "2022-01-22T13:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "An <b>introduction to Q-Learning: Reinforcement Learning</b>", "url": "https://blog.floydhub.com/an-introduction-to-q-learning-reinforcement-learning/", "isFamilyFriendly": true, "displayUrl": "https://blog.floydhub.com/an-<b>introduction-to-q-learning-reinforcement-learning</b>", "snippet": "Here is the definition of <b>Markov</b> <b>Decision</b> Processes (collected from Wikipedia): A <b>Markov decision process</b> (<b>MDP</b>) is a discrete time stochastic control <b>process</b>. It provides a mathematical framework for modeling <b>decision</b> making in situations where outcomes are partly random and partly under the control of a <b>decision</b> maker.", "dateLastCrawled": "2022-01-31T09:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Meta-<b>Learning</b> as a <b>Markov Decision Process</b>", "url": "https://hal.archives-ouvertes.fr/tel-02422144v2/document", "isFamilyFriendly": true, "displayUrl": "https://hal.archives-ouvertes.fr/tel-02422144v2/document", "snippet": "Meta-<b>Learning</b> as a <b>Markov Decision Process</b>. <b>Machine</b> <b>Learning</b> [cs.LG]. Uni-versit\u00e9 Paris Saclay (COmUE), 2019. English. \uffffNNT: 2019SACLS588\uffff. \ufffftel-02422144v2\uffff I would like to dedicate this thesis to my loving parents doctorat CLS588 Meta-<b>Learning</b> as a <b>Markov Decision Process</b> Th\u00e8se de doctorat de l\u2019Universit\u00e9 Paris-Saclay pr\u00e9par\u00e9e \u00e0 l\u2019Universit\u00e9 Paris-Sud Ecole doctorale n 580 Sciences et Technologies de l\u2019Information et de la Communication (STIC) Sp\u00e9cialit\u00e9 de doctorat ...", "dateLastCrawled": "2022-01-13T07:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>machine</b> <b>learning</b> - Why is the <b>optimal policy</b> in <b>Markov Decision Process</b> ...", "url": "https://stats.stackexchange.com/questions/132890/why-is-the-optimal-policy-in-markov-decision-process-mdp-independent-of-the-i", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/132890", "snippet": "The intuition behind the argument saying that the <b>optimal policy</b> is independent of initial state is the following: The <b>optimal policy</b> is defined by a function that selects an action for every possible state and actions in different states are independent.. Formally speaking, for an unknown initial distribution, the value function to maximize would be the following (not conditioned on initial state)", "dateLastCrawled": "2022-01-25T23:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Real-life <b>examples</b> of <b>Markov</b> <b>Decision</b> Processes - Cross Validated", "url": "https://stats.stackexchange.com/questions/145122/real-life-examples-of-markov-decision-processes", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/145122", "snippet": "A Markovian <b>Decision</b> <b>Process</b> indeed has to do with going from one state to another and is mainly used for planning and <b>decision</b> making. The theory. Just repeating the theory quickly, an <b>MDP</b> is: $$\\text{<b>MDP</b>} = \\langle S,A,T,R,\\gamma \\rangle$$", "dateLastCrawled": "2022-01-24T10:32:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Overview: Representation Techniques", "url": "https://webcms3.cse.unsw.edu.au/static/uploads/course/COMP4418/17s2/9c0e19e0e02df66fffb5d0bd4c20697922f5ffbf9a602b66bec3f74ac83fb77c/DecisionMaking.pdf", "isFamilyFriendly": true, "displayUrl": "https://webcms3.cse.unsw.edu.au/static/uploads/course/COMP4418/17s2/9c0e19e0e02df66...", "snippet": "<b>Markov Decision Process MDP is like</b> a Markov process, except every round we make a decision Transition probabilities depend on actions taken P(St+1 = S&#39; | St = s, At = a) = P(S, a, S&#39;) Rewards for every state, action pair u(St = s, At = a) Discount factor \u03b4 Example. A <b>machine</b> can be in one of three states: good, deteriorating, broken Can take ...", "dateLastCrawled": "2022-01-21T05:20:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], []], "all_bing_queries": ["+(markov decision process (mdp))  is like +(model of a decision maker's environment)", "+(markov decision process (mdp)) is similar to +(model of a decision maker's environment)", "+(markov decision process (mdp)) can be thought of as +(model of a decision maker's environment)", "+(markov decision process (mdp)) can be compared to +(model of a decision maker's environment)", "machine learning +(markov decision process (mdp) AND analogy)", "machine learning +(\"markov decision process (mdp) is like\")", "machine learning +(\"markov decision process (mdp) is similar\")", "machine learning +(\"just as markov decision process (mdp)\")", "machine learning +(\"markov decision process (mdp) can be thought of as\")", "machine learning +(\"markov decision process (mdp) can be compared to\")"]}