{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Bias</b> in A.I. &amp; <b>Machine</b> <b>Learning</b> Examples \u2013 Tanner Abraham", "url": "https://tannerabraham.com/bias-in-machine-learning-and-ai-examples/", "isFamilyFriendly": true, "displayUrl": "https://tannerabraham.com/<b>bias</b>-in-<b>machine</b>-<b>learning</b>-and-ai-examples", "snippet": "These biases include sample <b>bias</b>, reporting <b>bias</b>, prejudice <b>bias</b>, confirmation <b>bias</b>, <b>group</b> <b>attribution</b> <b>bias</b>, <b>algorithm</b> <b>bias</b>, measurement <b>bias</b>, recall <b>bias</b>, exclusion <b>bias</b>, and automation <b>bias</b>. <b>Machine</b> <b>learning</b> is highly susceptible to many forms of <b>bias</b> that can undermine model performance. After all, AI is assembled by humans, and humans are ...", "dateLastCrawled": "2022-01-28T07:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "AI <b>Bias</b> Example, Meaning And How To Avoid Them - Kotai Electronics Pvt ...", "url": "https://kotaielectronics.com/ai-bias-example-meaning-and-how-to-avoid-them/", "isFamilyFriendly": true, "displayUrl": "https://kotaielectronics.com/ai-<b>bias</b>-example-meaning-and-how-to-avoid-them", "snippet": "<b>Group</b> <b>attribution</b> AI <b>bias</b> meaning, The <b>Bias</b> that takes place when the <b>algorithm</b> puts more weight onto an individual it happens because the <b>algorithm</b> classifies the data and extrapolates a certain set of data from the rest of the data set. Here the AI <b>bias</b> example can be a tool that is used for admission and recruiting people, Here the system can put more importance on students who graduate from certain universities over others.", "dateLastCrawled": "2022-01-18T15:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "AI <b>Bias</b>: Definition, Types, Examples, and Debiasing Strategies \u2014 ITRex", "url": "https://itrexgroup.com/blog/ai-bias-definition-types-examples-debiasing-strategies/", "isFamilyFriendly": true, "displayUrl": "https://itrex<b>group</b>.com/blog/ai-<b>bias</b>-definition-types-examples-de<b>bias</b>ing-strategies", "snippet": "A simple definition of AI <b>bias</b> could sound <b>like</b> that: a phenomenon that occurs when an AI <b>algorithm</b> produces results that are systemically prejudiced due to erroneous assumptions in the <b>machine</b> <b>learning</b> process. <b>Bias</b> in artificial intelligence can take many forms \u2014 from racial <b>bias</b> and gender prejudice to recruiting inequity and age discrimination. &quot;The underlying reason for AI <b>bias</b> lies in human prejudice - conscious or unconscious - lurking in AI algorithms throughout their development ...", "dateLastCrawled": "2022-01-31T20:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Welcome To The <b>Machine</b> <b>Learning</b> Biases That Still Exist In 2019", "url": "https://analyticsindiamag.com/welcome-to-the-machine-learning-biases-that-still-exist-in-2019/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/welcome-to-the-<b>machine</b>-<b>learning</b>-<b>bias</b>es-that-still-exist...", "snippet": "5.<b>Group</b> <b>attribution</b> <b>Bias</b>: This <b>bias</b> assumes a particular attribute to the entire <b>group</b>. For example, if the majority of women are in the designing industry and a majority of men in the hardware, it tends to assume the respective professions for both. 4.<b>Algorithm</b> <b>Bias</b>: In <b>machine</b> <b>learning</b>, <b>bias</b> is a mathematical property of an <b>algorithm</b>. The ...", "dateLastCrawled": "2022-01-08T19:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "HUMAN <b>BIAS</b> THAT CAN RESULT INTO ML BIASES | by CoffeeBeans Consulting ...", "url": "https://coffeebeansconsulting.medium.com/human-bias-that-can-result-into-ml-biases-b1ea9f6d2767?source=post_internal_links---------6----------------------------", "isFamilyFriendly": true, "displayUrl": "https://coffeebeansconsulting.medium.com/human-<b>bias</b>-that-can-result-into-ml-<b>bias</b>es-b1...", "snippet": "<b>Group</b> <b>attribution</b> <b>Bias</b>: This <b>bias</b> assumes a particular attribute to the entire <b>group</b>. For example, if the majority of women are in the designing industry and a majority of men in the hardware, it tends to assume the respective professions for both. 6. <b>Algorithm</b> <b>Bias</b>: In <b>machine</b> <b>learning</b>, <b>bias</b> is a mathematical property of an <b>algorithm</b>. The counterpart to <b>bias</b> in this context is variance. ML algorithms with a high value of variance can easily fit into training data and welcome complexity but ...", "dateLastCrawled": "2022-01-25T05:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Biases in <b>machine</b> <b>learning</b> models and big data analytics: The ...", "url": "https://www.cambridge.org/core/journals/international-review-of-the-red-cross/article/biases-in-machine-learning-models-and-big-data-analytics-the-international-criminal-and-humanitarian-law-implications/86BEAC9ADD165C90B2931AB2B665FFDF", "isFamilyFriendly": true, "displayUrl": "https://www.cambridge.org/core/journals/international-review-of-the-red-cross/article/...", "snippet": "<b>Group</b> <b>attribution</b> <b>bias</b> is a tendency to impute what is true of a few individuals to an entire <b>group</b> to which they belong. For instance, imagine that an ML model is created to identify the most suitable candidates for a position with the OTP. In creating this model, the designers assume that the \u201cbest\u201d candidates are individuals with a doctorate degree from a Western European university and internship experience with the ICC, purely because some successful employees possess those traits ...", "dateLastCrawled": "2021-12-21T11:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "how to reduce <b>bias</b> in <b>machine</b> <b>learning</b> - Publicaffairsworld.com", "url": "https://publicaffairsworld.com/how-to-reduce-bias-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://publicaffairsworld.com/how-to-reduce-<b>bias</b>-in-<b>machine</b>-<b>learning</b>", "snippet": "If you have ever developed or worked on any type of <b>machine</b> <b>learning</b> <b>algorithm</b>, ... Confirmation <b>Bias</b>. \u2026 <b>Group</b> <b>attribution</b> <b>Bias</b>. What are the necessary steps to avoid biases in gathering and interpreting data? There are ways, however, to try to maintain objectivity and avoid <b>bias</b> with qualitative data analysis: Use multiple people to code the data. \u2026 Have participants review your results. \u2026 Verify with more data sources. \u2026 Check for alternative explanations. \u2026 Review findings with ...", "dateLastCrawled": "2022-01-22T04:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Machine</b> <b>Learning</b> Fairness: Types of <b>Bias</b> | by Svs Nagesh | Medium", "url": "https://nageshsomayajula.medium.com/machine-learning-fairness-types-of-bias-82bcf3df2d47", "isFamilyFriendly": true, "displayUrl": "https://nageshsomayajula.medium.com/<b>machine</b>-<b>learning</b>-fairness-types-of-<b>bias</b>-82bcf3df2d47", "snippet": "AI a p plications are <b>like</b> a small kid, we must train with the right data otherwise they can be misguided, and correcting machines or AI applications will be big challenging, for kids also (pun intended). The AI systems themselves will construct models that will explain how it works and follow anti-<b>bias</b> rules. In the <b>machine</b>, <b>learning</b> <b>bias</b> is one of the most common problems and every <b>algorithm</b> falls trap on this various kind of <b>bias</b>, let\u2019s discuss in detail various types of <b>bias</b> and how to ...", "dateLastCrawled": "2022-01-30T12:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Bias</b> in AI: What it is, Types, Examples &amp; 6 Ways to Fix it in 2022", "url": "https://research.aimultiple.com/ai-bias/", "isFamilyFriendly": true, "displayUrl": "https://research.aimultiple.com/ai-<b>bias</b>", "snippet": "AI <b>bias</b> is an anomaly in the output of <b>machine</b> <b>learning</b> algorithms, due to the prejudiced assumptions made during the <b>algorithm</b> development process or prejudices in the training data. What are the types of AI <b>bias</b>? AI systems contain biases due to two reasons: Cognitive biases: These are unconscious errors in thinking that affects individuals\u2019 judgements and decisions. These biases arise from the brain\u2019s attempt to simplify processing information about the world. More than 180 human ...", "dateLastCrawled": "2022-02-02T07:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Review into bias in algorithmic decision-making</b> - <b>GOV.UK</b>", "url": "https://www.gov.uk/government/publications/cdei-publishes-review-into-bias-in-algorithmic-decision-making/main-report-cdei-review-into-bias-in-algorithmic-decision-making", "isFamilyFriendly": true, "displayUrl": "https://<b>www.gov.uk</b>/government/publications/cdei-publishes-review-into-<b>bias</b>-in...", "snippet": "Second, A <b>machine</b> <b>learning</b> <b>algorithm</b> is chosen, and uses historical data (e.g. a set of past input data (e.g. a set of past input data, the decisions reached) to build a model, optimising against ...", "dateLastCrawled": "2022-02-03T02:05:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Bias</b> in A.I. &amp; <b>Machine</b> <b>Learning</b> Examples \u2013 Tanner Abraham", "url": "https://tannerabraham.com/bias-in-machine-learning-and-ai-examples/", "isFamilyFriendly": true, "displayUrl": "https://tannerabraham.com/<b>bias</b>-in-<b>machine</b>-<b>learning</b>-and-ai-examples", "snippet": "These biases include sample <b>bias</b>, reporting <b>bias</b>, prejudice <b>bias</b>, confirmation <b>bias</b>, <b>group</b> <b>attribution</b> <b>bias</b>, <b>algorithm</b> <b>bias</b>, measurement <b>bias</b>, recall <b>bias</b>, exclusion <b>bias</b>, and automation <b>bias</b>. <b>Machine</b> <b>learning</b> is highly susceptible to many forms of <b>bias</b> that can undermine model performance. After all, AI is assembled by humans, and humans are ...", "dateLastCrawled": "2022-01-28T07:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Biases in <b>machine</b> <b>learning</b> models and big data analytics: The ...", "url": "https://www.cambridge.org/core/journals/international-review-of-the-red-cross/article/biases-in-machine-learning-models-and-big-data-analytics-the-international-criminal-and-humanitarian-law-implications/86BEAC9ADD165C90B2931AB2B665FFDF", "isFamilyFriendly": true, "displayUrl": "https://www.cambridge.org/core/journals/international-review-of-the-red-cross/article/...", "snippet": "<b>Group</b> <b>attribution</b> <b>bias</b> is a tendency to impute what is true of a few individuals to an entire <b>group</b> to which they belong. For instance, imagine that an ML model is created to identify the most suitable candidates for a position with the OTP. In creating this model, the designers assume that the \u201cbest\u201d candidates are individuals with a doctorate degree from a Western European university and internship experience with the ICC, purely because some successful employees possess those traits ...", "dateLastCrawled": "2021-12-21T11:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Bias</b>, <b>Fairness</b> and Explainability \u2014 steps towards building Responsible ...", "url": "https://medium.com/walmartglobaltech/bias-fairness-and-explainability-steps-towards-building-responsible-ai-dc735b06279", "isFamilyFriendly": true, "displayUrl": "https://medium.com/walmartglobaltech/<b>bias</b>-<b>fairness</b>-and-explainability-steps-towards...", "snippet": "<b>Group</b> <b>Attribution</b> <b>bias</b>: ... As mentioned in Wikipedia: \u201cIn <b>machine</b> <b>learning</b>, a given <b>algorithm</b> is said to be fair, or to have <b>fairness</b>, if its results are independent of given variables ...", "dateLastCrawled": "2022-01-23T22:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Comparing different supervised <b>machine</b> <b>learning</b> algorithms for disease ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6925840/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC6925840", "snippet": "To avoid the risk of selection <b>bias</b>, from the literature we extracted those articles that used more than one supervised <b>machine</b> <b>learning</b> <b>algorithm</b>. The same supervised <b>learning</b> <b>algorithm</b> can generate different results across various study settings. There is a chance that a performance comparison between two supervised <b>learning</b> algorithms can generate imprecise results if they were employed in different studies separately. On the other side, the results of this study could suffer a variable ...", "dateLastCrawled": "2022-01-28T14:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Evolution and impact of <b>bias</b> in human and <b>machine learning</b> <b>algorithm</b> ...", "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0235502", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0235502", "snippet": "Traditionally, <b>machine learning</b> algorithms relied on reliable labels from experts to build predictions. More recently however, algorithms have been receiving data from the general population in the form of labeling, annotations, etc. The result is that algorithms are subject to <b>bias</b> that is born from ingesting unchecked information, such as biased samples and biased labels. Furthermore, people and algorithms are increasingly engaged in interactive processes wherein neither the human nor the ...", "dateLastCrawled": "2021-11-15T00:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Introduction <b>to Machine</b> <b>Learning</b>, Neural Networks, and Deep <b>Learning</b>", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7347027/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7347027", "snippet": "The performance of the <b>algorithm</b> is evaluated on the test dataset, data that the <b>algorithm</b> has never seen before.8, 9 The basic steps of supervised <b>machine</b> <b>learning</b> are (1) acquire a dataset and split it into separate training, validation, and test datasets; (2) use the training and validation datasets to inform a model of the relationship between features and target; and (3) evaluate the model via the test dataset to determine how well it predicts housing prices for unseen instances. In ...", "dateLastCrawled": "2022-02-02T05:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Frontiers | Addressing Fairness, <b>Bias</b>, and Appropriate Use of ...", "url": "https://www.frontiersin.org/articles/10.3389/frai.2020.561802/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/frai.2020.561802", "snippet": "A well-known example of <b>machine</b> <b>learning</b> <b>bias</b>, publicized by Joy Boulamwini in 2017 ... to enable the proper tuning of a <b>machine</b> <b>learning</b> <b>algorithm</b>. Individual vs. <b>Group</b> Fairness. Given the dictionary definition of fairness (impartial and just treatment), we can consider fairness at the level of an individual or a <b>group</b>. We can ask whether a computer <b>algorithm</b> disproportionately helps or harms specific individuals or specific groups of people. Ideally, an <b>algorithm</b> would be customized to an ...", "dateLastCrawled": "2022-01-29T07:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A Pilot Study on Detecting Unfairness in Human Decisions With <b>Machine</b> ...", "url": "https://deepai.org/publication/a-pilot-study-on-detecting-unfairness-in-human-decisions-with-machine-learning-algorithmic-bias-detection", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/a-pilot-study-on-detecting-unfairness-in-human...", "snippet": "An <b>algorithm</b> is fair if it gives <b>similar</b> predictions to <b>similar</b> individuals ... a <b>machine</b> <b>learning</b> <b>bias</b> reduction <b>algorithm</b> is required to eliminate all sources of the <b>bias</b> except for the <b>bias</b> inherited from the training data labels (human decision <b>bias</b>). We choose to use pre-processing <b>bias</b> mitigation techniques due to their compatibility to any classification algorithms and their simplicity (which leads to very low computational overhead) [kamiran2012data, yu2021fair]. Among the pre ...", "dateLastCrawled": "2022-01-24T15:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Underdiagnosis <b>bias</b> of artificial intelligence algorithms applied to ...", "url": "https://www.nature.com/articles/s41591-021-01595-0", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41591-021-01595-0", "snippet": "We summarize a <b>similar</b> analysis of ... S. Dissecting racial <b>bias</b> in an <b>algorithm</b> used to manage the health of populations. Science 366, 447\u2013453 (2019). CAS Article Google Scholar 6. Larrazabal ...", "dateLastCrawled": "2022-02-02T21:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Propensity score adjustment using <b>machine</b> <b>learning</b> classification ...", "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0231500", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0231500", "snippet": "Propensity estimation with the Random Forest <b>algorithm</b> is only advantageous in terms of <b>bias</b> removal in the estimations for Party 3, in which case the Random Forests <b>algorithm</b> achieves the highest <b>bias</b> reduction of all the classifiers reviewed. This is an important finding, as this missing data mechanism is particularly troublesome and, moreover, is commonly encountered in real data. The results for the MSE estimators under PSA with Random Forests show that this value may be only half that ...", "dateLastCrawled": "2021-05-29T12:08:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Frontiers | Addressing Fairness, <b>Bias</b>, and Appropriate Use of ...", "url": "https://www.frontiersin.org/articles/10.3389/frai.2020.561802/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/frai.2020.561802", "snippet": "A well-known example of <b>machine</b> <b>learning</b> <b>bias</b>, publicized by Joy Boulamwini in 2017 ... to enable the proper tuning of a <b>machine</b> <b>learning</b> <b>algorithm</b>. Individual vs. <b>Group</b> Fairness. Given the dictionary definition of fairness (impartial and just treatment), we <b>can</b> consider fairness at the level of an individual or a <b>group</b>. We <b>can</b> ask whether a computer <b>algorithm</b> disproportionately helps or harms specific individuals or specific groups of people. Ideally, an <b>algorithm</b> would be customized to an ...", "dateLastCrawled": "2022-01-29T07:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Chapter 11 Bias and Fairness</b> | Big Data and Social Science", "url": "https://textbook.coleridgeinitiative.org/chap-bias.html", "isFamilyFriendly": true, "displayUrl": "https://textbook.coleridgeinitiative.org/chap-<b>bias</b>.html", "snippet": "Unfortunately, just as there is no single <b>machine</b> <b>learning</b> <b>algorithm</b> that is best suited to every application, no one fairness metric will fit every situation. However, we hope this chapter will provide you with a grounding in the available ways of measuring algorithmic fairness that will help you navigate the trade-offs involved putting these into practice in your own applications. 11.2 Sources of <b>Bias</b>. <b>Bias</b> may be introduced into a <b>machine</b> <b>learning</b> project at any step along the way and it ...", "dateLastCrawled": "2022-01-30T01:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Evolution and impact of <b>bias</b> in human and <b>machine learning</b> <b>algorithm</b> ...", "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0235502", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0235502", "snippet": "Traditionally, <b>machine learning</b> algorithms relied on reliable labels from experts to build predictions. More recently however, algorithms have been receiving data from the general population in the form of labeling, annotations, etc. The result is that algorithms are subject to <b>bias</b> that is born from ingesting unchecked information, such as biased samples and biased labels. Furthermore, people and algorithms are increasingly engaged in interactive processes wherein neither the human nor the ...", "dateLastCrawled": "2021-11-15T00:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A <b>machine</b> <b>learning</b> toolkit for genetic engineering <b>attribution</b> to ...", "url": "https://www.nature.com/articles/s41467-020-19612-0", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41467-020-19612-0", "snippet": "With a framework for calibration of <b>attribution</b> models that <b>can</b> in principle be applied to any deep-<b>learning</b> classification <b>algorithm</b>, we next sought to expand the toolkit of genetic engineering ...", "dateLastCrawled": "2022-01-13T13:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Bias</b> does not equal <b>bias</b>. A socio-technical typology of <b>bias</b> in data ...", "url": "https://policyreview.info/articles/analysis/bias-does-not-equal-bias-socio-technical-typology-bias-data-based-algorithmic", "isFamilyFriendly": true, "displayUrl": "https://policyreview.info/articles/analysis/<b>bias</b>-does-not-equal-<b>bias</b>-socio-technical...", "snippet": "Zooming into the technical details of a <b>machine</b> <b>learning</b> system\u2019s life cycle, Suresh and Guttag (2020) described various issues that <b>can</b> introduce <b>bias</b> into a system: historical <b>bias</b>, representation <b>bias</b>, measurement <b>bias</b>, aggregation <b>bias</b>, <b>learning</b> <b>bias</b>, evaluation <b>bias</b> and deployment <b>bias</b>. Some of these types of data <b>bias</b> <b>can</b> only be identified through extensive knowledge and close examination of the development process of a particular system including the underlying data used to build ...", "dateLastCrawled": "2022-01-22T12:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Bias</b>, awareness, and ignorance in deep-<b>learning</b>-based face recognition ...", "url": "https://link.springer.com/article/10.1007%2Fs43681-021-00108-6", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s43681-021-00108-6", "snippet": "We investigate how well <b>machine</b> <b>learning</b> models <b>can</b> predict the sensitive features, such as ethnicity and gender, based on the face embedding. The intuition is that an FR model is \u201caware\u201d of a sensitive feature if it <b>can</b> be predicted from the embedding vectors produced by the FR model. This inference is a classification task and the performance depends on the classification model at hand. If simple models, more precisely models with a low number of parameters, <b>can</b> properly infer the ...", "dateLastCrawled": "2022-02-03T10:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Comparing different supervised <b>machine</b> <b>learning</b> algorithms for disease ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6925840/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC6925840", "snippet": "To avoid the risk of selection <b>bias</b>, from the literature we extracted those articles that used more than one supervised <b>machine</b> <b>learning</b> <b>algorithm</b>. The same supervised <b>learning</b> <b>algorithm</b> <b>can</b> generate different results across various study settings. There is a chance that a performance comparison between two supervised <b>learning</b> algorithms <b>can</b> generate imprecise results if they were employed in different studies separately. On the other side, the results of this study could suffer a variable ...", "dateLastCrawled": "2022-01-28T14:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Review into bias in algorithmic decision-making</b> - <b>GOV.UK</b>", "url": "https://www.gov.uk/government/publications/cdei-publishes-review-into-bias-in-algorithmic-decision-making/main-report-cdei-review-into-bias-in-algorithmic-decision-making", "isFamilyFriendly": true, "displayUrl": "https://<b>www.gov.uk</b>/government/publications/cdei-publishes-review-into-<b>bias</b>-in...", "snippet": "A <b>machine</b> <b>learning</b> <b>algorithm</b> takes data as ... not available to the <b>algorithm</b>) but <b>can</b> also introduce human <b>bias</b> into the system. Humans \u2018over the loop\u2019 monitoring the fairness of the whole ...", "dateLastCrawled": "2022-02-03T02:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Towards a pragmatist dealing with algorithmic <b>bias</b> in medical <b>machine</b> ...", "url": "https://link.springer.com/article/10.1007/s11019-021-10008-5", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11019-021-10008-5", "snippet": "The underlying assumption is that greater transparency will render algorithmic <b>bias</b> easier to detect and help understand a program\u2019s erroneous decisions, so that one <b>can</b> correct the <b>algorithm</b>\u2019s mistakes and avoid <b>bias</b> by curating the input variables accordingly. For many instances this solution <b>can</b> be sufficient, e.g. to identify so-called Clever Hans predictors that base a ML program\u2019s classification strategy on irrelevant correlations. A good example for such a misleading predictor ...", "dateLastCrawled": "2022-01-13T00:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "300+ TOP <b>Neural Networks Multiple Choice Questions and Answers</b>", "url": "https://engineeringinterviewquestions.com/neural-networks-multiple-choice-questions-and-answers/", "isFamilyFriendly": true, "displayUrl": "https://engineeringinterviewquestions.com/<b>neural-networks-multiple-choice-questions</b>...", "snippet": "34. Decision Trees <b>can</b> be used for Classification Tasks. a) True b) False. Answer: a. 35. How many types of <b>learning</b> are available in <b>machine</b> <b>learning</b>? a) 1 b) 2 c) 3 d) 4. Answer: c Explanation: The three types of <b>machine</b> <b>learning</b> are supervised, unsupervised and reinforcement. 36. Choose from the following that are Decision Tree nodes. a ...", "dateLastCrawled": "2022-02-02T15:24:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "What Is Model <b>Bias</b> In <b>Machine</b> <b>Learning</b>? \u2013 charmestrength.com", "url": "https://charmestrength.com/what-is-model-bias-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://charmestrength.com/what-is-model-<b>bias</b>-in-<b>machine</b>-<b>learning</b>", "snippet": "<b>Machine</b> <b>learning</b> <b>bias</b>, also sometimes called <b>algorithm</b> <b>bias</b> or AI <b>bias</b>, ... <b>Group</b> <b>attribution</b> <b>Bias</b>. What to do if model is Overfitting? Reduce the network&#39;s capacity by removing layers or reducing the number of elements in the hidden layers. Apply regularization , which comes down to adding a cost to the loss function for large weights. Use Dropout layers, which will randomly remove certain features by setting them to zero. How <b>can</b> <b>machine</b> <b>learning</b> models reduce <b>bias</b>? Choose the correct ...", "dateLastCrawled": "2022-01-15T06:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "AI <b>Bias</b> Example, Meaning And How To Avoid Them - Kotai Electronics Pvt ...", "url": "https://kotaielectronics.com/ai-bias-example-meaning-and-how-to-avoid-them/", "isFamilyFriendly": true, "displayUrl": "https://kotaielectronics.com/ai-<b>bias</b>-example-meaning-and-how-to-avoid-them", "snippet": "<b>Group</b> <b>attribution</b> AI <b>bias</b> meaning, The <b>Bias</b> that takes place when the <b>algorithm</b> puts more weight onto an individual it happens because the <b>algorithm</b> classifies the data and extrapolates a certain set of data from the rest of the data set. Here the AI <b>bias</b> example <b>can</b> be a tool that is used for admission and recruiting people, Here the system <b>can</b> put more importance on students who graduate from certain universities over others.", "dateLastCrawled": "2022-01-18T15:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Can</b> <b>bias</b> be eliminated from algorithms?", "url": "https://www.weforum.org/agenda/2021/12/bias-eliminated-algorithms-software-code-systems/", "isFamilyFriendly": true, "displayUrl": "https://www.weforum.org/agenda/2021/12/<b>bias</b>-eliminated-<b>algorithms</b>-software-code-systems", "snippet": "<b>Algorithm</b> <b>bias</b>, also called <b>machine</b> <b>learning</b> <b>bias</b>, is a phenomenon in which algorithms <b>can</b> act in a discriminatory or prejudiced manner due to misplaced assumptions during the <b>learning</b> phase of their development. Unconscious biases regarding gender, race and social class <b>can</b> make their way into the training data fed by programmers into \u201c<b>machine</b>-<b>learning</b> algorithms\u201d, systems which constantly improve their own performance by including new data into an existing model. These biases <b>can</b> be ...", "dateLastCrawled": "2022-02-03T05:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Bias</b>, <b>Fairness</b> and Explainability \u2014 steps towards building Responsible ...", "url": "https://medium.com/walmartglobaltech/bias-fairness-and-explainability-steps-towards-building-responsible-ai-dc735b06279", "isFamilyFriendly": true, "displayUrl": "https://medium.com/walmartglobaltech/<b>bias</b>-<b>fairness</b>-and-explainability-steps-towards...", "snippet": "<b>Group</b> <b>Attribution</b> <b>bias</b>: ... in Wikipedia: \u201cIn <b>machine</b> <b>learning</b>, a given <b>algorithm</b> is said to be fair, or to have <b>fairness</b>, if its results are independent of given variables, especially those ...", "dateLastCrawled": "2022-01-23T22:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Comparing different supervised <b>machine</b> <b>learning</b> algorithms for disease ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6925840/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC6925840", "snippet": "To avoid the risk of selection <b>bias</b>, from the literature we extracted those articles that used more than one supervised <b>machine</b> <b>learning</b> <b>algorithm</b>. The same supervised <b>learning</b> <b>algorithm</b> <b>can</b> generate different results across various study settings. There is a chance that a performance comparison between two supervised <b>learning</b> algorithms <b>can</b> generate imprecise results if they were employed in different studies separately. On the other side, the results of this study could suffer a variable ...", "dateLastCrawled": "2022-01-28T14:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Frontiers | Addressing Fairness, <b>Bias</b>, and Appropriate Use of ...", "url": "https://www.frontiersin.org/articles/10.3389/frai.2020.561802/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/frai.2020.561802", "snippet": "A well-known example of <b>machine</b> <b>learning</b> <b>bias</b>, publicized by Joy Boulamwini in 2017 ... to enable the proper tuning of a <b>machine</b> <b>learning</b> <b>algorithm</b>. Individual vs. <b>Group</b> Fairness. Given the dictionary definition of fairness (impartial and just treatment), we <b>can</b> consider fairness at the level of an individual or a <b>group</b>. We <b>can</b> ask whether a computer <b>algorithm</b> disproportionately helps or harms specific individuals or specific groups of people. Ideally, an <b>algorithm</b> would be customized to an ...", "dateLastCrawled": "2022-01-29T07:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "how to reduce <b>bias</b> in <b>machine</b> <b>learning</b> - Publicaffairsworld.com", "url": "https://publicaffairsworld.com/how-to-reduce-bias-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://publicaffairsworld.com/how-to-reduce-<b>bias</b>-in-<b>machine</b>-<b>learning</b>", "snippet": "If you have ever developed or worked on any type of <b>machine</b> <b>learning</b> <b>algorithm</b>, ... <b>Group</b> <b>attribution</b> <b>Bias</b>. What are the necessary steps to avoid biases in gathering and interpreting data? There are ways, however, to try to maintain objectivity and avoid <b>bias</b> with qualitative data analysis: Use multiple people to code the data. \u2026 Have participants review your results. \u2026 Verify with more data sources. \u2026 Check for alternative explanations. \u2026 Review findings with peers. What <b>can</b> a data ...", "dateLastCrawled": "2022-01-22T04:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A <b>machine</b> <b>learning</b> toolkit for genetic engineering <b>attribution</b> to ...", "url": "https://www.nature.com/articles/s41467-020-19612-0", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41467-020-19612-0", "snippet": "With a framework for calibration of <b>attribution</b> models that <b>can</b> in principle be applied to any deep-<b>learning</b> classification <b>algorithm</b>, we next sought to expand the toolkit of genetic engineering ...", "dateLastCrawled": "2022-01-13T13:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Addressing Fairness, Bias, and Appropriate</b> Use of Artificial ...", "url": "https://europepmc.org/article/PMC/PMC8107824", "isFamilyFriendly": true, "displayUrl": "https://europepmc.org/article/PMC/PMC8107824", "snippet": "<b>Machine</b> <b>learning</b> <b>can</b> also be used to help optimize processes or to predict ... If algorithmic <b>bias</b> leads to unfavorable treatment of one patient <b>group</b> vs. another, this <b>bias</b> <b>can</b> be judged to be unfair, from a legal or ethical point of view. While <b>bias</b> is related to fairness, it should be noted that algorithmic <b>bias</b> is independent of ethics, and is simply a mathematical and statistical consequence of an <b>algorithm</b> and its data. If an <b>algorithm</b> is discovered to have <b>bias</b>, this <b>bias</b> <b>can</b> then be ...", "dateLastCrawled": "2021-06-01T04:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Review into bias in algorithmic decision-making</b> - <b>GOV.UK</b>", "url": "https://www.gov.uk/government/publications/cdei-publishes-review-into-bias-in-algorithmic-decision-making/main-report-cdei-review-into-bias-in-algorithmic-decision-making", "isFamilyFriendly": true, "displayUrl": "https://<b>www.gov.uk</b>/government/publications/cdei-publishes-review-into-<b>bias</b>-in...", "snippet": "A <b>machine</b> <b>learning</b> <b>algorithm</b> takes data as ... not available to the <b>algorithm</b>) but <b>can</b> also introduce human <b>bias</b> into the system. Humans \u2018over the loop\u2019 monitoring the fairness of the whole ...", "dateLastCrawled": "2022-02-03T02:05:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Attribution</b> Part 3: Applying <b>Machine</b> <b>Learning</b> to <b>Attribution</b> - Trust ...", "url": "https://www.trustinsights.ai/blog/2019/11/attribution-part-3-applying-machine-learning-to-attribution/", "isFamilyFriendly": true, "displayUrl": "https://www.trustinsights.ai/blog/2019/11/<b>attribution</b>-part-3-applying-<b>machine</b>-<b>learning</b>...", "snippet": "The <b>machine</b> <b>learning</b> approach to <b>attribution</b> analysis starts with no <b>bias</b>. With the inexpensive computational power now available to us you can run all of the data from every transaction, and all the data from transactions that did not complete. <b>Machine</b> <b>learning</b> will examine every single transition from one page to the next and from one goal (milestones along the customer journey) to the next and determine the probability of a customer moving from one point to the next.", "dateLastCrawled": "2022-02-02T04:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Bias</b> in <b>AI Increasingly Recognized; Progress Being Made</b> \u2013 Jay van Zyl", "url": "https://jayvanzyl.me/bias-in-ai-increasingly-recognized-progress-being-made/", "isFamilyFriendly": true, "displayUrl": "https://jayvanzyl.me/<b>bias</b>-in-<b>ai-increasingly-recognized-progress-being-made</b>", "snippet": "<b>Bias</b> in AI decision-making and in the algorithms of <b>machine</b> <b>learning</b> has been outed as a real issue in the march of AI progress. Here is an update on where we are and efforts being made to recognize <b>bias</b> and counteract it, including a discussion of selected AI startups. AI reflects the <b>bias</b> of its creators, notes Will Bryne, CEO of Groundswell in a recent article in Fast Company. Societal <b>bias</b> \u2013 the <b>attribution</b> of individuals or groups with distinct traits without any data to back it up ...", "dateLastCrawled": "2021-03-25T19:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "6 \u2013 Interpretability \u2013 <b>Machine</b> <b>Learning</b> Blog | ML@CMU | Carnegie Mellon ...", "url": "https://blog.ml.cmu.edu/2020/08/31/6-interpretability/", "isFamilyFriendly": true, "displayUrl": "https://blog.ml.cmu.edu/2020/08/31/6-interpretability", "snippet": "For a person inexperienced in <b>machine</b> <b>learning</b>, it would be difficult to apply the Olah method across many images, especially if you wanted to explain why the model chose a labrador retriever instead of a beagle. In such a case, abstract dog visualizations would be uninformative. Moreover, there is a potential observer <b>bias</b>: if a human expects visualizations of dogs and cats, they might miss more abstract but important visualizations, such as the snow in the husky and wolf classification ...", "dateLastCrawled": "2022-02-03T05:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "RStudio AI Blog: Starting to think about AI Fairness", "url": "https://blogs.rstudio.com/ai/posts/2021-07-15-ai-fairness/", "isFamilyFriendly": true, "displayUrl": "https://blogs.rstudio.com/ai/posts/2021-07-15-ai-fairness", "snippet": "From a <b>machine</b> <b>learning</b> perspective, the interesting point is the classification of metrics into <b>bias</b>-preserving and <b>bias</b>-transforming. The terms speak for themselves: Metrics in the first <b>group</b> reflect biases in the dataset used for training; ones in the second do not. In that way, the distinction parallels", "dateLastCrawled": "2022-01-31T09:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Letter to the editor: \u201cNot all biases are bad: equitable and ...", "url": "https://insightsimaging.springeropen.com/track/pdf/10.1186/s13244-021-01022-5.pdf", "isFamilyFriendly": true, "displayUrl": "https://insightsimaging.springeropen.com/track/pdf/10.1186/s13244-021-01022-5", "snippet": "equitable and inequitable biases in <b>machine</b> <b>learning</b> and radiology\u201d ... Open Access This article is licensed under a Creative Commons <b>Attribution</b> 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are ...", "dateLastCrawled": "2021-09-16T06:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Beneficial and harmful explanatory <b>machine learning</b> | SpringerLink", "url": "https://link.springer.com/article/10.1007/s10994-020-05941-0", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10994-020-05941-0", "snippet": "(<b>Machine</b>-explained human comprehension of examples, \\(C_{ex}(D, H, M(E))\\)): Given a logic program D representing the definition of a target predicate, a <b>group</b> of humans H, a theory M(E) learned using <b>machine learning</b> algorithm M and examples E, the <b>machine</b>-explained human comprehension of examples E is the mean accuracy with which a human \\(h \\in H\\) after brief study of an explanation based on M(E) can classify new material selected from the domain of D.", "dateLastCrawled": "2022-01-21T19:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Assessing biases, relaxing moralism: On ground-truthing practices in ...", "url": "https://journals.sagepub.com/doi/full/10.1177/20539517211013569", "isFamilyFriendly": true, "displayUrl": "https://journals.sagepub.com/doi/full/10.1177/20539517211013569", "snippet": "<b>Machine</b> <b>learning</b> (ML) algorithms\u2014computerized methods of calculation that infer rules of computation from sets of data to make predictions and support decision-making tasks\u2014are now powering many commonly used devices such as Web search engines (Richardson et al., 2006), social media applications (Hazelwood et al., 2018), online purchasing platforms (Portugal et al., 2018), and surveillance systems (Chokshi, 2019).In reaction to the growing ubiquity of these statistical methods of ...", "dateLastCrawled": "2021-09-29T10:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "FIVE Podcast | AI E2: AI <b>Bias - A Tale of Sheep &amp; Field</b> | Emodo", "url": "https://www.emodoinc.com/the-five-podcast/season2-ai/s2-e2-ai-bias-a-tale-of-sheep-field/", "isFamilyFriendly": true, "displayUrl": "https://www.emodoinc.com/the-five-podcast/season2-ai/s2-e2-ai-<b>bias-a-tale-of-sheep-field</b>", "snippet": "There was a very big push a while ago to start using <b>machine</b> <b>learning</b> and AI models to identify certain kinds of melanoma skin cancers. And there was an awful lot of hoopla and a lot of writing about how good pattern recognition was at actually detecting melanomas earlier than a human pathologist could because he could see different things and the pixels. Well, the way that it got this information is they took pictures of people\u2019s arms, and they took them with professional cameras. Well ...", "dateLastCrawled": "2022-01-21T19:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "300+ TOP <b>Neural Networks Multiple Choice Questions and Answers</b>", "url": "https://engineeringinterviewquestions.com/neural-networks-multiple-choice-questions-and-answers/", "isFamilyFriendly": true, "displayUrl": "https://engineeringinterviewquestions.com/<b>neural-networks-multiple-choice-questions</b>...", "snippet": "How many types of <b>learning</b> are available in <b>machine</b> <b>learning</b>? a) 1 b) 2 c) 3 d) 4. Answer: c Explanation: The three types of <b>machine</b> <b>learning</b> are supervised, unsupervised and reinforcement. 36. Choose from the following that are Decision Tree nodes. a) Decision Nodes b) Weighted Nodes c) Chance Nodes d) End Nodes. Answer: a, c, d. 37. Decision Nodes are represented by, a) Disks b) Squares c) Circles d) Triangles. Answer: b. 38. Chance Nodes are represented by, a) Disks b) Squares c) Circles ...", "dateLastCrawled": "2022-02-02T15:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "About Us \u2013 <b>Toronto Machine Learning</b>", "url": "https://www.torontomachinelearning.com/about-us/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>torontomachinelearning</b>.com/about-us", "snippet": "The <b>Toronto Machine Learning</b> Society ... at Portland State University. Her current research focuses on conceptual abstraction, <b>analogy</b>-making, and visual recognition in artificial intelligence systems. Melanie is the author or editor of six books and numerous scholarly papers in the fields of artificial intelligence, cognitive science, and complex systems. Her book Complexity: A Guided Tour (Oxford University Press) won the 2010 Phi Beta Kappa Science Book Award and was named by Amazon.com ...", "dateLastCrawled": "2022-02-03T03:50:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Bias in A.I. &amp; <b>Machine</b> <b>Learning</b> Examples \u2013 Tanner Abraham", "url": "https://tannerabraham.com/bias-in-machine-learning-and-ai-examples/", "isFamilyFriendly": true, "displayUrl": "https://tannerabraham.com/bias-in-<b>machine</b>-<b>learning</b>-and-ai-examples", "snippet": "<b>Machine</b> <b>learning</b> algorithms are often mistaken as objective analytics and decision-making solutions to human inefficiencies. Paradoxically, humans often make <b>machine</b> <b>learning</b> algorithms inefficient by way of biases. These biases include sample bias, reporting bias, prejudice bias, confirmation bias, group attribution bias, algorithm bias, measurement bias, recall bias, exclusion bias, and automation bias. <b>Machine</b> <b>learning</b> is highly susceptible to many forms of bias that can undermine model ...", "dateLastCrawled": "2022-01-28T07:55:00.0000000Z", "language": "en", "isNavigational": false}], [], [], []], "all_bing_queries": ["+(group attribution bias)  is like +(machine learning algorithm)", "+(group attribution bias) is similar to +(machine learning algorithm)", "+(group attribution bias) can be thought of as +(machine learning algorithm)", "+(group attribution bias) can be compared to +(machine learning algorithm)", "machine learning +(group attribution bias AND analogy)", "machine learning +(\"group attribution bias is like\")", "machine learning +(\"group attribution bias is similar\")", "machine learning +(\"just as group attribution bias\")", "machine learning +(\"group attribution bias can be thought of as\")", "machine learning +(\"group attribution bias can be compared to\")"]}