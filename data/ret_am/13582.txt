{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "/docs/ai/<b>gpt</b>/ Directory Listing \u00b7 Gwern.net", "url": "https://www.gwern.net/docs/ai/gpt/index", "isFamilyFriendly": true, "displayUrl": "https://www.gwern.net/docs/ai/<b>gpt</b>/index", "snippet": "We study sparse variants for all layers in the <b>Transformer</b> and propose Scaling Transformers, a family of next generation <b>Transformer</b> models that use sparse layers to scale efficiently and perform unbatched decoding much faster than the standard <b>Transformer</b> as we scale up the model size. Surprisingly, the sparse layers are enough to obtain the same perplexity as the standard <b>Transformer</b> with the same number of parameters. We also integrate with prior sparsity approaches to attention and ...", "dateLastCrawled": "2022-01-16T01:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Computational linguistics</b> - WikiMili, The Best Wikipedia Reader", "url": "https://wikimili.com/en/Computational_linguistics", "isFamilyFriendly": true, "displayUrl": "https://wikimili.com/en/<b>Computational_linguistics</b>", "snippet": "<b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> 2 (<b>GPT</b>-2) is an open-source artificial intelligence created by OpenAI in February 2019. <b>GPT</b>-2 translates text, answers questions, summarizes passages, and generates text output on a level that, while sometimes indistinguishable from that of humans, can become repetitive or nonsensical when generating long passages. It is a general-purpose learner; it was not specifically trained to do any of these tasks, and its ability to perform them is an extension of ...", "dateLastCrawled": "2021-08-30T21:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "artificial intelligence news | AITopics", "url": "https://aitopics.org/tag/artificial%20intelligence%20news", "isFamilyFriendly": true, "displayUrl": "https://aitopics.org/tag/artificial intelligence news", "snippet": "Information about AI from the News, Publications, and ConferencesAutomatic Classification \u2013 Tagging and Summarization \u2013 Customizable Filtering and AnalysisIf you are looking for an answer to the question What is Artificial Intelligence? and you only have a minute, then here&#39;s the definition the Association for the Advancement of Artificial Intelligence offers on its home page: &quot;the scientific understanding of the mechanisms underlying thought and intelligent behavior and their embodiment ...", "dateLastCrawled": "2022-01-30T23:35:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "/docs/ai/<b>gpt</b>/ Directory Listing \u00b7 Gwern.net", "url": "https://www.gwern.net/docs/ai/gpt/index", "isFamilyFriendly": true, "displayUrl": "https://www.gwern.net/docs/ai/<b>gpt</b>/index", "snippet": "Relative to sighted people, blind individuals are less likely to generate \u201cyellow\u201d for banana and \u201cred\u201d for stop sign but make <b>similar</b> <b>generative</b> inferences about real and novel objects\u2019 colors, and provide <b>similar</b> causal explanations. For example, people infer that 2 natural kinds (eg. bananas) and 2 artifacts with functional colors (eg. stop signs) are more likely to have the same color than 2 artifacts with nonfunctional colors (eg. cars).", "dateLastCrawled": "2022-01-16T01:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Computational linguistics</b> - WikiMili, The Best Wikipedia Reader", "url": "https://wikimili.com/en/Computational_linguistics", "isFamilyFriendly": true, "displayUrl": "https://wikimili.com/en/<b>Computational_linguistics</b>", "snippet": "<b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> 2 (<b>GPT</b>-2) is an open-source artificial intelligence created by OpenAI in February 2019. <b>GPT</b>-2 translates text, answers questions, summarizes passages, and generates text output on a level that, while sometimes indistinguishable from that of humans, can become repetitive or nonsensical when generating long passages. It is a general-purpose learner; it was not specifically trained to do any of these tasks, and its ability to perform them is an extension of ...", "dateLastCrawled": "2021-08-30T21:35:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "/docs/ai/<b>gpt</b>/ Directory Listing \u00b7 Gwern.net", "url": "https://www.gwern.net/docs/ai/gpt/index", "isFamilyFriendly": true, "displayUrl": "https://www.gwern.net/docs/ai/<b>gpt</b>/index", "snippet": "Large, <b>pre-trained</b> <b>transformer</b>-based language models such as BERT have drastically changed the Natural Language Processing (NLP) field. We present a survey of recent work that uses these large language models to solve NLP tasks via pre-training then fine-tuning, prompting, or text generation approaches. We also present approaches that use <b>pre-trained</b> language models to generate data for training augmentation or other purposes. We conclude with discussions on limitations and suggested ...", "dateLastCrawled": "2022-01-16T01:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Computational linguistics</b> - WikiMili, The Best Wikipedia Reader", "url": "https://wikimili.com/en/Computational_linguistics", "isFamilyFriendly": true, "displayUrl": "https://wikimili.com/en/<b>Computational_linguistics</b>", "snippet": "<b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> 2 (<b>GPT</b>-2) is an open-source artificial intelligence created by OpenAI in February 2019. <b>GPT</b>-2 translates text, answers questions, summarizes passages, and generates text output on a level that, while sometimes indistinguishable from that of humans, <b>can</b> become repetitive or nonsensical when generating long passages. It is a general-purpose learner; it was not specifically trained to do any of these tasks, and its ability to perform them is an extension of ...", "dateLastCrawled": "2021-08-30T21:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "artificial intelligence news | AITopics", "url": "https://aitopics.org/tag/artificial%20intelligence%20news", "isFamilyFriendly": true, "displayUrl": "https://aitopics.org/tag/artificial intelligence news", "snippet": "Information about AI from the News, Publications, and ConferencesAutomatic Classification \u2013 Tagging and Summarization \u2013 Customizable Filtering and AnalysisIf you are looking for an answer to the question What is Artificial Intelligence? and you only have a minute, then here&#39;s the definition the Association for the Advancement of Artificial Intelligence offers on its home page: &quot;the scientific understanding of the mechanisms underlying <b>thought</b> and intelligent behavior and their embodiment ...", "dateLastCrawled": "2022-01-30T23:35:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "/docs/ai/<b>gpt</b>/ Directory Listing \u00b7 Gwern.net", "url": "https://www.gwern.net/docs/ai/gpt/index", "isFamilyFriendly": true, "displayUrl": "https://www.gwern.net/docs/ai/<b>gpt</b>/index", "snippet": "Large, <b>pre-trained</b> <b>transformer</b>-based language models such as BERT have drastically changed the Natural Language Processing (NLP) field. We present a survey of recent work that uses these large language models to solve NLP tasks via pre-training then fine-tuning, prompting, or text generation approaches. We also present approaches that use pre ...", "dateLastCrawled": "2022-01-16T01:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Computational linguistics</b> - WikiMili, The Best Wikipedia Reader", "url": "https://wikimili.com/en/Computational_linguistics", "isFamilyFriendly": true, "displayUrl": "https://wikimili.com/en/<b>Computational_linguistics</b>", "snippet": "<b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> 2 (<b>GPT</b>-2) is an open-source artificial intelligence created by OpenAI in February 2019. <b>GPT</b>-2 translates text, answers questions, summarizes passages, and generates text output on a level that, while sometimes indistinguishable from that of humans, <b>can</b> become repetitive or nonsensical when generating long passages. It is a general-purpose learner; it was not specifically trained to do any of these tasks, and its ability to perform them is an extension of ...", "dateLastCrawled": "2021-08-30T21:35:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>What is GPT-3</b>? - Dr Peper MD", "url": "https://drpepermd.com/2021/02/22/what-is-gpt-3/", "isFamilyFriendly": true, "displayUrl": "https://drpepermd.com/2021/02/22/<b>what-is-gpt-3</b>", "snippet": "<b>GPT</b>-3 stands for <b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> 3 (the third version). Some have called it the most important and useful advance in AI in years. The abilities of <b>GPT</b>-3 have both shocked and excited many within the AI community. As one developer said: \u201cPlaying with <b>GPT</b>-3 feels like seeing the future.\u201d But, how was <b>GPT</b>-3 developed? Find out in this episode of Short and Sweet AI. You can listen to this episode below or keep reading. Another Mind-Blowing Tool from OpenAI. How does <b>GPT</b>-3 ...", "dateLastCrawled": "2022-01-11T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Using machine learning to generate recipes that</b> actually work | by ...", "url": "https://towardsdatascience.com/using-machine-learning-to-generate-recipes-that-actually-works-b2331c85ab72", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>using-machine-learning-to-generate-recipes-that</b>...", "snippet": "<b>GPT</b>-2. <b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> 2 (<b>GPT</b>-2) is a so-called <b>transformer</b>. They learn from the training data how likely a word is to occur depending on the other words in the full text, but different words are given different weights, a process called attention. In this way, it can keep the context theoretically indefinitely. The way to use <b>GPT</b>-2 is to write a few words as a starter and let the <b>transformer</b> fill in what word is most likely to follow, then look at the new string and ...", "dateLastCrawled": "2022-02-02T18:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>GPT</b>-3 and <b>the Artificial Intelligence That Surrounds Us</b> | by R/GA | Medium", "url": "https://rga.medium.com/gpt-3-and-the-artificial-intelligence-that-surrounds-us-98572617fd05", "isFamilyFriendly": true, "displayUrl": "https://rga.medium.com/<b>gpt</b>-3-and-<b>the-artificial-intelligence-that-surrounds-us</b>...", "snippet": "By Nicol\u00e1s Rodr\u00edguez. OpenAI, the San Francisco-based AI lab, just released the third iteration of its <b>GPT</b> (<b>Generative</b> <b>Pre-trained</b> <b>Transformer</b>) model, or <b>GPT</b>-3 for short. After investing around $4.6 million, the program has shaken up every corner of the Internet, generating a mix of excitement and trepidation. But what is <b>GPT</b>-3, exactly?", "dateLastCrawled": "2022-01-23T01:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>The AI few days after GPT-3</b> - Ivan Moreira", "url": "https://ivanmoreira.org/blog/the-ai-few-days-after-gpt-3/", "isFamilyFriendly": true, "displayUrl": "https://ivanmoreira.org/blog/<b>the-ai-few-days-after-gpt-3</b>", "snippet": "On past July OpenAI released a beta test of one of the most AI model called <b>GPT</b>-3 (<b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> 3), that uses Deep <b>Learning</b> (part of a broader a <b>machine</b> <b>learning</b> method, based on neural networks. This transformational system is more sophisticated, and the full version has a capacity of 175 billion ML parameters when the older version only has 17 billion, less than 10% of this new one. <b>GPT</b>-3 is a turning point in AI field and will bring to us a new era of AI computing ...", "dateLastCrawled": "2022-01-26T18:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Is <b>GPT</b>-3 the first Artificial General Intelligence? | by Bruce H ...", "url": "https://chatbotslife.com/is-gpt-3-the-adam-of-natural-language-cf59656456f2", "isFamilyFriendly": true, "displayUrl": "https://chatbotslife.com/is-<b>gpt</b>-3-the-adam-of-natural-language-cf59656456f2", "snippet": "The <b>GPT</b>-3 (<b>Generative</b> <b>Pre-Trained</b> <b>Transformer</b>-3) ... The API runs the <b>pre-trained</b> <b>GPT</b>-3 model family for a wide range of NLP tasks [3]. Unlike the usual AI community practice, the <b>GPT</b>-3 model weights are not released to the public. Conclusion . OpenAI has long asserted that immense computational horsepower in conjunction with reinforcement <b>learning</b> is a necessary step on the road to AGI, or AI that can learn any task a human can [14]. The fathers of AI 2.0, such as Yoshua Bengio and Yann ...", "dateLastCrawled": "2022-01-08T13:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "How close is <b>GPT</b>-3 to Artificial General Intelligence? | by Bruce H ...", "url": "https://towardsdatascience.com/how-close-is-gpt-3-to-artificial-general-intelligence-cb057a8c503d", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/how-close-is-<b>gpt</b>-3-to-artificial-general-intelligence...", "snippet": "The <b>GPT</b>-3 (<b>Generative</b> <b>Pre-Trained</b> <b>Transformer</b>-3) is OpenAI\u2019s most massive natural language prediction (NLP) model to date (available to the public June 2020). <b>GPT</b>-3 has approximately 185 billion parameters. In contrast, the human brain has approximately 86 billion neurons with on the average 7,000 synapses per neuron [2,3]; Comparing apples to oranges, the human brain has about 60 trillion parameters or about 300x more parameters than <b>GPT</b>-3. Note: If 10% of the human brain capacity is ...", "dateLastCrawled": "2022-01-27T08:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Transformer</b> Neural Network In Deep <b>Learning</b> - Overview - GeeksforGeeks", "url": "https://www.geeksforgeeks.org/transformer-neural-network-in-deep-learning-overview/", "isFamilyFriendly": true, "displayUrl": "https://www.geeksforgeeks.org/<b>transformer</b>-neural-network-in-deep-<b>learning</b>-overview", "snippet": "The successor to <b>GPT</b> and GPT2 is the GPT3, and is one of the most controversial <b>pre-trained</b> models, by OpenAI the large-scale <b>transformer</b>-based language model has been trained on 175 billion parameters, which is 10 times more than any previous non-sparsed language model. The model has been trained to achieve strong performance on much NLP dataset, including task translation, answering questions, as well as several other tasks.", "dateLastCrawled": "2022-02-02T11:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "The Illustrated <b>GPT</b>-2 (Visualizing <b>Transformer</b> Language Models) \u2013 Jay ...", "url": "https://jalammar.github.io/illustrated-gpt2/", "isFamilyFriendly": true, "displayUrl": "https://jalammar.github.io/illustrated-<b>gpt</b>2", "snippet": "Discussions: Hacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments) Translations: Korean, Russian This year, we saw a dazzling application of <b>machine</b> <b>learning</b>. The OpenAI <b>GPT</b>-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The <b>GPT</b>-2 wasn\u2019t a particularly novel architecture \u2013 it\u2019s architecture is very similar to the decoder-only <b>transformer</b>.", "dateLastCrawled": "2022-01-30T06:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) GALAXY: A <b>Generative</b> <b>Pre-trained</b> Model for Task-Oriented Dialog ...", "url": "https://www.researchgate.net/publication/356631427_GALAXY_A_Generative_Pre-trained_Model_for_Task-Oriented_Dialog_with_Semi-Supervised_Learning_and_Explicit_Policy_Injection", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/356631427_GALAXY_A_<b>Generative</b>_<b>Pre-trained</b>...", "snippet": "GALAXY: A <b>Generative</b> <b>Pre-trained</b> Model f or T ask-Oriented Dialog with Semi-Supervised <b>Learning</b> and Explicit Policy Injection W anwei He 1 * \u2020 , Yinpei Dai 2 * , Yinhe Zheng 2 , Y uchuan Wu 2 ...", "dateLastCrawled": "2022-01-29T16:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How to perform Text Summarization with Python, HuggingFace Transformers ...", "url": "https://www.machinecurve.com/index.php/2020/12/21/easy-text-summarization-with-huggingface-transformers-and-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/index.php/2020/12/21/easy-text-summarization-with-hugging...", "snippet": "A <b>Transformer</b> is a <b>machine</b> <b>learning</b> architecture that combines an encoder with a decoder and jointly learns them, allowing us to convert input sequences (e.g. phrases) into some intermediate format before we convert it back into human-understandable format. A human <b>analogy</b> would be two translators which both speak some imaginary language and a human-interpretable one, such as German and French. The first translator can translate French into the imaginary language; the second then has learned ...", "dateLastCrawled": "2022-02-02T23:43:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(gpt (generative pre-trained transformer))  is like +(\"universal translator\" for machines)", "+(gpt (generative pre-trained transformer)) is similar to +(\"universal translator\" for machines)", "+(gpt (generative pre-trained transformer)) can be thought of as +(\"universal translator\" for machines)", "+(gpt (generative pre-trained transformer)) can be compared to +(\"universal translator\" for machines)", "machine learning +(gpt (generative pre-trained transformer) AND analogy)", "machine learning +(\"gpt (generative pre-trained transformer) is like\")", "machine learning +(\"gpt (generative pre-trained transformer) is similar\")", "machine learning +(\"just as gpt (generative pre-trained transformer)\")", "machine learning +(\"gpt (generative pre-trained transformer) can be thought of as\")", "machine learning +(\"gpt (generative pre-trained transformer) can be compared to\")"]}