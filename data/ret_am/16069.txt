{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "New submissions for Tue, 19 Oct 21 \u00b7 Issue #443 \u00b7 kobiso/daily-arxiv ...", "url": "https://github.com/kobiso/daily-arxiv-noti/issues/443", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/kobiso/daily-arxiv-noti/issues/443", "snippet": "<b>Multi-head</b>, key-value attention is the backbone of the widely successful Transformer model and its variants. This attention mechanism uses multiple parallel key-value attention blocks (called heads), each performing two fundamental computations: (1) search - selection of a relevant entity from a set via query-key interactions, and (2) retrieval - extraction of relevant features from the selected entity via a value matrix. Importantly, standard attention heads learn a rigid mapping between ...", "dateLastCrawled": "2022-01-02T05:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Research</b> - Jian @ WatVis", "url": "https://www.jeffjianzhao.com/research/", "isFamilyFriendly": true, "displayUrl": "https://www.jeffjianzhao.com/<b>research</b>", "snippet": "Recently, they have been further evolved into an advanced approach called <b>multi-head</b> <b>self-attention</b> networks, which can encode a set of input vectors, e.g., word vectors in a sentence, into another set of vectors. Such encoding aims at simultaneously capturing diverse syntactic and semantic features within a set, each of which corresponds to a particular attention head, forming altogether <b>multi-head</b> attention. Meanwhile, the increased model complexity prevents users from easily understanding ...", "dateLastCrawled": "2022-02-03T00:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "[D] <b>Why are transformers so much more useful</b> in NLP than vision ...", "url": "https://www.reddit.com/r/MachineLearning/comments/fcycvp/d_why_are_transformers_so_much_more_useful_in_nlp/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/MachineLearning/comments/fcycvp/d_why_are_transformers_so...", "snippet": "Why are transformers and <b>self-attention</b> so much more widely used for NLP? There&#39;s a few examples of using transformers in vision, <b>like</b> <b>self-attention</b> GAN or <b>self-attention</b> for adversarial defense, but they haven&#39;t taken over in vision <b>like</b> they have in NLP.Also, the use of convolution seems so limited in NLP, even though there have been some promising results with CNN variants.. I imagine this difference isn&#39;t just fashion, so what is different about NLP and vision?", "dateLastCrawled": "2022-02-01T10:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Computer Vision \u2013 ECCV 2020</b> | springerprofessional.de", "url": "https://www.springerprofessional.de/computer-vision-eccv-2020/18555458", "isFamilyFriendly": true, "displayUrl": "https://www.springerprofessional.de/<b>computer-vision-eccv-2020</b>/18555458", "snippet": "Further, each head in our <b>multi-head</b> <b>self-attention</b> layer focuses on a different subset of relations. Our approach has two advantages: (1) each head considers local context instead of dispersing the attention amongst all visual entities; (2) we avoid learning redundant features. We show that our model improves the absolute accuracy of current state-of-the-art methods on TextVQA by 2.2% overall over an improved baseline, and 4.62% on questions that involve spatial reasoning and can be ...", "dateLastCrawled": "2021-12-04T05:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "2021.03.30 | AI Research Trends", "url": "https://mavenlin.github.io/ai_research_trends/2021/03/30/2021.03.30.html", "isFamilyFriendly": true, "displayUrl": "https://mavenlin.github.io/ai_research_trends/2021/03/30/2021.03.30.html", "snippet": "Our system uses a large pre-trained language model as the encoder and an additional dual <b>multi-head</b> co-attention layer to strengthen the relationship between passages and question-answer pairs, following the current state-of-the-art model DUMA. The main difference is that we stack the passage-question and question-passage attention modules instead of calculating parallelly to simulate re-considering process. We also add a layer normalization module to improve the performance of our model ...", "dateLastCrawled": "2022-01-22T05:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "L-Verse: Bidirectional Generation Between Image and Text", "url": "https://www.researchgate.net/publication/356455630_L-Verse_Bidirectional_Generation_Between_Image_and_Text", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/356455630_L-Verse_Bidirectional_Generation...", "snippet": "With the masked dot-pr oduct <b>multi-head</b> attention, the. conventional auto-regressi ve transformer [3] can only un-derstand a given sequence from left to right. Bidirectional. generation between ...", "dateLastCrawled": "2021-12-14T21:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Advancing Acoustic-to-Word CTC Model with Attention and ... - deepai.org", "url": "https://deepai.org/publication/advancing-acoustic-to-word-ctc-model-with-attention-and-mixed-units", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/advancing-acoustic-to-word-ctc-model-with-attention-and...", "snippet": "12/31/18 - The acoustic-to-word model based on the Connectionist Temporal Classification (CTC) criterion is a natural end-to-end (E2E) system...", "dateLastCrawled": "2022-01-13T05:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Google AI Blog", "url": "https://ai.googleblog.com/", "isFamilyFriendly": true, "displayUrl": "https://ai.googleblog.com", "snippet": "Posted by Sungyong Seo, Software Engineer and Sercan O. Arik, Research Scientist, Google Research, Cloud AI Team Deep neural networks (DNNs) provide more accurate results as the size and coverage of their training data increases. While investing in high-quality and large-scale labeled datasets is one path to model improvement, another is leveraging prior knowledge, concisely referred to as \u201crules\u201d \u2014 reasoning heuristics, equations, associative logic, or constraints.", "dateLastCrawled": "2022-02-02T16:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "\u7ffb\u8a33\u7d50\u679c: L-Verse: Bidirectional Generation Between Image and Text", "url": "https://fugumt.com/fugumt/paper/translated/2111.11133.pdf.html", "isFamilyFriendly": true, "displayUrl": "https://fugumt.com/fugumt/paper/translated/2111.11133.pdf.html", "snippet": "With the masked dot-product <b>multi-head</b> attention, the conventional auto-regressive transformer [3] can only understand a given sequence from left to right. \u30de\u30b9\u30af\u4ed8\u304d\u30c9\u30c3\u30c8\u88fd\u54c1\u30de\u30eb\u30c1\u30d8\u30c3\u30c9\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u306b\u3088\u308a\u3001\u5f93\u6765\u306e\u81ea\u5df1\u56de\u5e30\u5909\u63db\u5668[3]\u306f\u3001\u5de6\u304b\u3089\u53f3\u3078\u306e\u7279\u5b9a\u306e\u30b7\u30fc\u30b1\u30f3\u30b9\u3057\u304b\u7406\u89e3\u3067\u304d\u306a\u3044\u3002 0.58: Bidirectional generation between text and image doesn\u2019t require a transformer to be fully-bidirectional: learning how to distinguish an image ...", "dateLastCrawled": "2021-11-24T09:14:00.0000000Z", "language": "ja", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "ICML <b>2019 Videos</b>", "url": "https://icml.cc/Conferences/2019/Videos", "isFamilyFriendly": true, "displayUrl": "https://icml.cc/Conferences/<b>2019/Videos</b>", "snippet": "The result of the listening test shows that our proposed model generated more human-<b>like</b> performances compared to a baseline model and a hierarchical attention network model that handles music score as a word-<b>like</b> sequence. Oral {daterange} @ Seaside Ballroom . Sparse Extreme Multi-label Learning with Oracle Property. In Supervised Learning. Weiwei Liu \u00b7 Xiaobo Shen The pioneering work of sparse local embeddings on multilabel learning has shown great promise in multilabel classification ...", "dateLastCrawled": "2022-01-29T16:20:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "New submissions for Tue, 19 Oct 21 \u00b7 Issue #443 \u00b7 kobiso/daily-arxiv ...", "url": "https://github.com/kobiso/daily-arxiv-noti/issues/443", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/kobiso/daily-arxiv-noti/issues/443", "snippet": "<b>Multi-head</b>, key-value attention is the backbone of the widely successful Transformer model and its variants. This attention mechanism uses multiple parallel key-value attention blocks (called heads), each performing two fundamental computations: (1) search - selection of a relevant entity from a set via query-key interactions, and (2) retrieval - extraction of relevant features from the selected entity via a value matrix. Importantly, standard attention heads learn a rigid mapping between ...", "dateLastCrawled": "2022-01-02T05:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Research</b> - WatVis", "url": "https://www.jeffjianzhao.com/research/", "isFamilyFriendly": true, "displayUrl": "https://www.jeffjianzhao.com/<b>research</b>", "snippet": "Recently, they have been further evolved into an advanced approach called <b>multi-head</b> <b>self-attention</b> networks, which can encode a set of input vectors, e.g., word vectors in a sentence, into another set of vectors. Such encoding aims at simultaneously capturing diverse syntactic and semantic features within a set, each of which corresponds to a particular attention head, forming altogether <b>multi-head</b> attention. Meanwhile, the increased model complexity prevents users from easily understanding ...", "dateLastCrawled": "2022-02-03T00:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "L-Verse: Bidirectional Generation Between Image and Text", "url": "https://www.researchgate.net/publication/356455630_L-Verse_Bidirectional_Generation_Between_Image_and_Text", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/356455630_L-Verse_Bidirectional_Generation...", "snippet": "As the dot-product <b>multi-head</b> attention [42] ... and <b>taking</b> a picture of buildings. GT: T wo men playing a game of frisbee. on a lush green field. L-V erse: A man is throwing a. frisbee in a field ...", "dateLastCrawled": "2021-12-14T21:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>The Unreasonable Effectiveness of Deep Features</b> as a Perceptual Metric ...", "url": "https://www.researchgate.net/publication/322418794_The_Unreasonable_Effectiveness_of_Deep_Features_as_a_Perceptual_Metric", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/322418794_The_Unreasonable_Effectiveness_of...", "snippet": "First, unlike the conventional <b>multi-head</b> <b>self-attention</b> in previous Vision Transformers (ViTs), RestoreFormer incorporates a <b>multi-head</b> cross-attention layer to learn fully-spatial interactions ...", "dateLastCrawled": "2022-01-27T20:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "2021.03.30 | AI Research Trends", "url": "https://mavenlin.github.io/ai_research_trends/2021/03/30/2021.03.30.html", "isFamilyFriendly": true, "displayUrl": "https://mavenlin.github.io/ai_research_trends/2021/03/30/2021.03.30.html", "snippet": "Our system uses a large pre-trained language model as the encoder and an additional dual <b>multi-head</b> co-attention layer to strengthen the relationship between passages and question-answer pairs, following the current state-of-the-art model DUMA. The main difference is that we stack the passage-question and question-passage attention modules instead of calculating parallelly to simulate re-considering process. We also add a layer normalization module to improve the performance of our model ...", "dateLastCrawled": "2022-01-22T05:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Diffusion Science radio", "url": "https://www.diffusionradio.com/diffusion.html", "isFamilyFriendly": true, "displayUrl": "https://www.diffusionradio.com/diffusion.html", "snippet": "Interpretable <b>Multi-Head</b> <b>Self-Attention</b> Architecture for Sarcasm Detection in Social Media UCF Team Develops Artificial Intelligence that Can Detect Sarcasm in Social Media. A volumetric display for visual, tactile and audio presentation using acoustic trapping. 2021 Trivia special (November 29, 2021) Gut bugs! (November 22, 2021) Transhuman and Transhumanitarian (November 15, 2021 ...", "dateLastCrawled": "2022-01-22T17:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Advancing Acoustic-to-Word CTC Model with Attention and ... - deepai.org", "url": "https://deepai.org/publication/advancing-acoustic-to-word-ctc-model-with-attention-and-mixed-units", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/advancing-acoustic-to-word-ctc-model-with-attention-and...", "snippet": "12/31/18 - The acoustic-to-word model based on the Connectionist Temporal Classification (CTC) criterion is a natural end-to-end (E2E) system...", "dateLastCrawled": "2022-01-13T05:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "[D] <b>Why are transformers so much more useful</b> in NLP than vision ...", "url": "https://www.reddit.com/r/MachineLearning/comments/fcycvp/d_why_are_transformers_so_much_more_useful_in_nlp/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/MachineLearning/comments/fcycvp/d_why_are_transformers_so...", "snippet": "Why are transformers and <b>self-attention</b> so much more widely used for NLP? There&#39;s a few examples of using transformers in vision, like <b>self-attention</b> GAN or <b>self-attention</b> for adversarial defense, but they haven&#39;t taken over in vision like they have in NLP.Also, the use of convolution seems so limited in NLP, even though there have been some promising results with CNN variants.. I imagine this difference isn&#39;t just fashion, so what is different about NLP and vision?", "dateLastCrawled": "2022-02-01T10:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "ICML <b>2019 Videos</b>", "url": "https://icml.cc/Conferences/2019/Videos", "isFamilyFriendly": true, "displayUrl": "https://icml.cc/Conferences/<b>2019/Videos</b>", "snippet": "<b>Self-attention</b> using graph convolution allows our pooling method to consider both node features and graph topology. To ensure a fair comparison, the same training procedures and model architectures were used for the existing pooling methods and our method. The experimental results demonstrate that our method achieves superior graph classification performance on the benchmark datasets using a reasonable number of parameters.", "dateLastCrawled": "2022-01-29T16:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "\u7ffb\u8a33\u7d50\u679c: L-Verse: Bidirectional Generation Between Image and Text", "url": "https://fugumt.com/fugumt/paper/translated/2111.11133.pdf.html", "isFamilyFriendly": true, "displayUrl": "https://fugumt.com/fugumt/paper/translated/2111.11133.pdf.html", "snippet": "With the masked dot-product <b>multi-head</b> attention, the conventional auto-regressive transformer [3] can only understand a given sequence from left to right. \u30de\u30b9\u30af\u4ed8\u304d\u30c9\u30c3\u30c8\u88fd\u54c1\u30de\u30eb\u30c1\u30d8\u30c3\u30c9\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u306b\u3088\u308a\u3001\u5f93\u6765\u306e\u81ea\u5df1\u56de\u5e30\u5909\u63db\u5668[3]\u306f\u3001\u5de6\u304b\u3089\u53f3\u3078\u306e\u7279\u5b9a\u306e\u30b7\u30fc\u30b1\u30f3\u30b9\u3057\u304b\u7406\u89e3\u3067\u304d\u306a\u3044\u3002 0.58: Bidirectional generation between text and image doesn\u2019t require a transformer to be fully-bidirectional: learning how to distinguish an image ...", "dateLastCrawled": "2021-11-24T09:14:00.0000000Z", "language": "ja", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "New submissions for Tue, 19 Oct 21 \u00b7 Issue #443 \u00b7 kobiso/daily-arxiv ...", "url": "https://github.com/kobiso/daily-arxiv-noti/issues/443", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/kobiso/daily-arxiv-noti/issues/443", "snippet": "<b>Multi-head</b> attention is a driving force behind state-of-the-art transformers which achieve remarkable performance across a variety of natural language processing (NLP) and computer vision tasks. It has been observed that for many applications, those attention heads learn redundant embedding, and most of them <b>can</b> be removed without degrading the performance of the model. Inspired by this observation, we propose Transformer with a Mixture of Gaussian Keys (Transformer-MGK), a novel transformer ...", "dateLastCrawled": "2022-01-02T05:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>The Unreasonable Effectiveness of Deep Features</b> as a Perceptual Metric ...", "url": "https://www.researchgate.net/publication/322418794_The_Unreasonable_Effectiveness_of_Deep_Features_as_a_Perceptual_Metric", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/322418794_The_Unreasonable_Effectiveness_of...", "snippet": "First, unlike the conventional <b>multi-head</b> <b>self-attention</b> in previous Vision Transformers (ViTs), RestoreFormer incorporates a <b>multi-head</b> cross-attention layer to learn fully-spatial interactions ...", "dateLastCrawled": "2022-01-27T20:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Research</b> - Jian @ WatVis", "url": "https://www.jeffjianzhao.com/research/", "isFamilyFriendly": true, "displayUrl": "https://www.jeffjianzhao.com/<b>research</b>", "snippet": "Recently, they have been further evolved into an advanced approach called <b>multi-head</b> <b>self-attention</b> networks, which <b>can</b> encode a set of input vectors, e.g., word vectors in a sentence, into another set of vectors. Such encoding aims at simultaneously capturing diverse syntactic and semantic features within a set, each of which corresponds to a particular attention head, forming altogether <b>multi-head</b> attention. Meanwhile, the increased model complexity prevents users from easily understanding ...", "dateLastCrawled": "2022-02-03T00:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Google AI Blog", "url": "https://ai.googleblog.com/", "isFamilyFriendly": true, "displayUrl": "https://ai.googleblog.com", "snippet": "Detecting previously unseen conditions <b>can</b> <b>be thought</b> of as an out-of-distribution (OOD) detection task. By successfully identifying OOD samples, preventive measures <b>can</b> be taken, like abstaining from prediction or deferring to a human expert. Traditional computer vision OOD detection benchmarks work to detect dataset distribution shifts. For example, a model may be trained on CIFAR images but be presented with street view house numbers (SVHN) as OOD samples, two datasets with very different ...", "dateLastCrawled": "2022-02-02T16:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "2021.03.30 | AI Research Trends", "url": "https://mavenlin.github.io/ai_research_trends/2021/03/30/2021.03.30.html", "isFamilyFriendly": true, "displayUrl": "https://mavenlin.github.io/ai_research_trends/2021/03/30/2021.03.30.html", "snippet": "The model <b>can</b> be trained in just 30 to 45 minutes (based on ocean basin) and <b>can</b> predict the landfall&#39;s location and time in a few seconds, which makes it suitable for real time prediction. XRJL-HKUST at SemEval-2021 Task 4: WordNet-Enhanced Dual <b>Multi-head</b> Co-Attention for Reading Comprehension of Abstract Meaning arxiv:2103.16102 3", "dateLastCrawled": "2022-01-22T05:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Diffusion Science radio", "url": "https://www.diffusionradio.com/diffusion.html", "isFamilyFriendly": true, "displayUrl": "https://www.diffusionradio.com/diffusion.html", "snippet": "Interpretable <b>Multi-Head</b> <b>Self-Attention</b> Architecture for Sarcasm Detection in Social Media UCF Team Develops Artificial Intelligence that <b>Can</b> Detect Sarcasm in Social Media. A volumetric display for visual, tactile and audio presentation using acoustic trapping. 2021 Trivia special (November 29, 2021) Gut bugs! (November 22, 2021) Transhuman and Transhumanitarian (November 15, 2021 ...", "dateLastCrawled": "2022-01-22T17:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "ICML 2019 Videos", "url": "https://icml.cc/Conferences/2019/Videos?source=post_page---------------------------", "isFamilyFriendly": true, "displayUrl": "https://icml.cc/Conferences/2019/Videos?source=post_page---------------------------", "snippet": "Over the past few years, the PAC-Bayesian approach has been applied to numerous settings, including classification, high-dimensional sparse regression, image denoising and reconst", "dateLastCrawled": "2022-01-10T23:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Implications for Malaysia - POLITICAL ISLAM Hijrah or Pop Islam? - IIUM ...", "url": "https://www.readkong.com/page/implications-for-malaysia-political-islam-hijrah-or-pop-3100489", "isFamilyFriendly": true, "displayUrl": "https://www.readkong.com/page/implications-for-malaysia-political-islam-hijrah-or-pop...", "snippet": "The government or the state <b>can</b> no longer make exclusive decisions when it comes to the role of religion within society. There is also the rise of new voices for Islam that include celebrities, entrepreneurs, and the new intellectuals. Muslims in Malaysia are now spoiled for choices when it comes to portraying piety in what is termed as \u201cpop Islam\u201d, or popular Islam. Pop Islam Could Lead to Artificial Piety Pop Islam is the phenomenon that <b>can</b> be observed across the Muslim world in which ...", "dateLastCrawled": "2022-01-29T07:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "proceedings.mlr.press", "url": "http://proceedings.mlr.press/v97/assets/bib/bibliography.bib", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v97/assets/bib/bibliography.bib", "snippet": "This process <b>can</b> either be voluntary, as in the case of a person <b>taking</b> a facial image to unlock his/her phone, or incidental, such as traffic cameras collecting videos on pedestrians. An undesirable side effect of these processes is that shared data <b>can</b> carry information about attributes that users might consider as sensitive, even when such information is of limited use for the task. It is therefore desirable for both data collectors and users to design procedures that minimize sensitive ...", "dateLastCrawled": "2021-07-22T03:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Proceedings of Machine Learning Research", "url": "https://proceedings.mlr.press/v97/assets/rss/feed.xml", "isFamilyFriendly": true, "displayUrl": "https://proceedings.mlr.press/v97/assets/rss/feed.xml", "snippet": "<b>Self-Attention</b> Generative Adversarial Networks. In standard classification problems, the assumption is that the entity making the decision (the principal) has access to all the samples. However, in many contexts, she either does not have direct access to the samples, or <b>can</b> inspect only a limited set of samples and does not know which are the most relevant ones. In such cases, she must rely on another party (the agent) to either provide the samples or point out the most relevant ones. If the ...", "dateLastCrawled": "2021-12-28T04:04:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "New submissions for Tue, 19 Oct 21 \u00b7 Issue #443 \u00b7 kobiso/daily-arxiv ...", "url": "https://github.com/kobiso/daily-arxiv-noti/issues/443", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/kobiso/daily-arxiv-noti/issues/443", "snippet": "<b>Multi-head</b> attention is a driving force behind state-of-the-art transformers which achieve remarkable performance across a variety of natural language processing (NLP) and computer vision tasks. It has been observed that for many applications, those attention heads learn redundant embedding, and most of them <b>can</b> be removed without degrading the performance of the model. Inspired by this observation, we propose Transformer with a Mixture of Gaussian Keys (Transformer-MGK), a novel transformer ...", "dateLastCrawled": "2022-01-02T05:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Research</b> - Jian @ WatVis", "url": "https://www.jeffjianzhao.com/research/", "isFamilyFriendly": true, "displayUrl": "https://www.jeffjianzhao.com/<b>research</b>", "snippet": "Recently, they have been further evolved into an advanced approach called <b>multi-head</b> <b>self-attention</b> networks, which <b>can</b> encode a set of input vectors, e.g., word vectors in a sentence, into another set of vectors. Such encoding aims at simultaneously capturing diverse syntactic and semantic features within a set, each of which corresponds to a particular attention head, forming altogether <b>multi-head</b> attention. Meanwhile, the increased model complexity prevents users from easily understanding ...", "dateLastCrawled": "2022-02-03T00:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "L-Verse: Bidirectional Generation Between Image and Text", "url": "https://www.researchgate.net/publication/356455630_L-Verse_Bidirectional_Generation_Between_Image_and_Text", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/356455630_L-Verse_Bidirectional_Generation...", "snippet": "With the masked dot-pr oduct <b>multi-head</b> attention, the. conventional auto-regressi ve transformer [3] <b>can</b> only un-derstand a given sequence from left to right. Bidirectional. generation between ...", "dateLastCrawled": "2021-12-14T21:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Computer Vision \u2013 ECCV 2020</b> | springerprofessional.de", "url": "https://www.springerprofessional.de/computer-vision-eccv-2020/18555458", "isFamilyFriendly": true, "displayUrl": "https://www.springerprofessional.de/<b>computer-vision-eccv-2020</b>/18555458", "snippet": "Further, each head in our <b>multi-head</b> <b>self-attention</b> layer focuses on a different subset of relations. Our approach has two advantages: (1) each head considers local context instead of dispersing the attention amongst all visual entities; (2) we avoid learning redundant features. We show that our model improves the absolute accuracy of current state-of-the-art methods on TextVQA by 2.2% overall over an improved baseline, and 4.62% on questions that involve spatial reasoning and <b>can</b> be ...", "dateLastCrawled": "2021-12-04T05:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Google AI Blog", "url": "https://ai.googleblog.com/", "isFamilyFriendly": true, "displayUrl": "https://ai.googleblog.com", "snippet": "Posted by Sungyong Seo, Software Engineer and Sercan O. Arik, Research Scientist, Google Research, Cloud AI Team Deep neural networks (DNNs) provide more accurate results as the size and coverage of their training data increases. While investing in high-quality and large-scale labeled datasets is one path to model improvement, another is leveraging prior knowledge, concisely referred to as \u201crules\u201d \u2014 reasoning heuristics, equations, associative logic, or constraints.", "dateLastCrawled": "2022-02-02T16:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Advancing Acoustic-to-Word CTC Model with Attention and ... - deepai.org", "url": "https://deepai.org/publication/advancing-acoustic-to-word-ctc-model-with-attention-and-mixed-units", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/advancing-acoustic-to-word-ctc-model-with-attention-and...", "snippet": "12/31/18 - The acoustic-to-word model based on the Connectionist Temporal Classification (CTC) criterion is a natural end-to-end (E2E) system...", "dateLastCrawled": "2022-01-13T05:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Diffusion Science radio", "url": "https://www.diffusionradio.com/diffusion.html", "isFamilyFriendly": true, "displayUrl": "https://www.diffusionradio.com/diffusion.html", "snippet": "Interpretable <b>Multi-Head</b> <b>Self-Attention</b> Architecture for Sarcasm Detection in Social Media UCF Team Develops Artificial Intelligence that <b>Can</b> Detect Sarcasm in Social Media. A volumetric display for visual, tactile and audio presentation using acoustic trapping. 2021 Trivia special (November 29, 2021) Gut bugs! (November 22, 2021) Transhuman and Transhumanitarian (November 15, 2021 ...", "dateLastCrawled": "2022-01-22T17:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "2021.03.30 | AI Research Trends", "url": "https://mavenlin.github.io/ai_research_trends/2021/03/30/2021.03.30.html", "isFamilyFriendly": true, "displayUrl": "https://mavenlin.github.io/ai_research_trends/2021/03/30/2021.03.30.html", "snippet": "<b>Compared</b> with traditional bounding box (BBox) based tracking, this setting guides object tracking with high-level semantic information, addresses the ambiguity of BBox, and links local and global search organically together. Those benefits may bring more flexible, robust and accurate tracking performance in practical scenarios. However, existing natural language initialized trackers are developed and <b>compared</b> on benchmark datasets proposed for tracking-by-BBox, which <b>can</b>&#39;t reflect the true ...", "dateLastCrawled": "2022-01-22T05:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "ICML <b>2019 Videos</b>", "url": "https://icml.cc/Conferences/2019/Videos", "isFamilyFriendly": true, "displayUrl": "https://icml.cc/Conferences/<b>2019/Videos</b>", "snippet": "In this paper, we propose the <b>Self-Attention</b> Generative Adversarial Network (SAGAN) which allows attention-driven, long-range dependency modeling for image generation tasks. Traditional convolutional GANs generate high-resolution details as a function of only spatially local points in lower-resolution feature maps. In SAGAN, details <b>can</b> be generated using cues from all feature locations. Moreover, the discriminator <b>can</b> check that highly detailed features in distant portions of the image are ...", "dateLastCrawled": "2022-01-29T16:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Age <b>Estimation by Multi-scale Convolutional Network</b> | Request PDF", "url": "https://www.researchgate.net/publication/285628284_Age_Estimation_by_Multi-scale_Convolutional_Network", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/285628284_Age_Estimation_by_Multi-scale...", "snippet": "<b>Compared</b> to traditional age estimation methods, both the feature representation and prediction tasks <b>can</b> be integrated into a deep learning framework by end-to-end network training, thus the ...", "dateLastCrawled": "2022-01-24T01:01:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "10.5. <b>Multi-Head Attention</b> \u2014 Dive into Deep <b>Learning</b> 0.17.2 documentation", "url": "http://d2l.ai/chapter_attention-mechanisms/multihead-attention.html", "isFamilyFriendly": true, "displayUrl": "d2l.ai/chapter_attention-mechanisms/<b>multihead-attention</b>.html", "snippet": "This design is called <b>multi-head attention</b>, where each of the \\(h\\) attention pooling outputs is a head [Vaswani et al., 2017]. Using fully-connected layers to perform learnable linear transformations, Fig. 10.5.1 describes <b>multi-head attention</b>.", "dateLastCrawled": "2022-02-02T01:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Introduction to Transformers in Machine Learning</b>", "url": "https://www.machinecurve.com/index.php/2020/12/28/introduction-to-transformers-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/.../12/28/<b>introduction-to-transformers-in-machine-learning</b>", "snippet": "The masked <b>multi-head</b> attention segment, which performs <b>multi-head</b> <b>self-attention</b> on the outputs, but does so in a masked way, so that positions depend on the past only. The <b>multi-head</b> attention segment , which performs <b>multi-head</b> <b>self-attention</b> on a combination of the ( encoded ) inputs and the outputs, so that the model learns to correlate encoded inputs with desired outputs.", "dateLastCrawled": "2022-02-03T03:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Attending to Attention. A summary of a revolutionary paper\u2026 | by Akash ...", "url": "https://towardsdatascience.com/attending-to-attention-eba798f0e940", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/attending-to-attention-eba798f0e940", "snippet": "The first is a <b>multi-head</b> <b>self-attention</b> mechanism, and the second is a simple, position-wise fully connected feed-forward network. We employ a residual connection around each of the two sub-layers, followed by layer normalization. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of ...", "dateLastCrawled": "2022-01-25T10:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "10.7. <b>Transformer</b> \u2014 Dive into Deep <b>Learning</b> 0.17.2 documentation", "url": "https://d2l.ai/chapter_attention-mechanisms/transformer.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_attention-mechanisms/<b>transformer</b>.html", "snippet": "The first is a <b>multi-head</b> <b>self-attention</b> pooling and the second is a positionwise feed-forward network. Specifically, in the encoder <b>self-attention</b>, queries, keys, and values are all from the the outputs of the previous encoder layer. Inspired by the ResNet design in Section 7.6, a residual connection is employed around both sublayers.", "dateLastCrawled": "2022-01-29T19:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Intuitive Introduction to BERT \u2013 MachineCurve</b>", "url": "https://www.machinecurve.com/index.php/2021/01/04/intuitive-introduction-to-bert/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/index.php/2021/01/04/<b>intuitive-introduction-to-bert</b>", "snippet": "From our article about GPT: \u201cThe input is then served to a masked <b>multi-head</b> attention segment, which computes <b>self-attention</b> in a unidirectional way.Here, the residual is added and the result is layer normalized.\u201d Indeed, GPT (which uses the Transformer decoder segment autoregressively during pretraining) and the original Transformer (which performs Seq2Seq), apply a mask in one of the attention modules \u2013 the masked <b>multi-head</b> <b>self-attention</b> subsegment in the decoder segment.. For any ...", "dateLastCrawled": "2022-01-30T22:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Capturing Multi-Resolution Context by Dilated <b>Self-Attention</b>", "url": "https://www.merl.com/publications/docs/TR2021-036.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.merl.com/publications/docs/TR2021-036.pdf", "snippet": "to <b>machine</b> translation or language modeling, where close-by words are more likely to have a dependent relationship, while only a few distant words or word groups are relevant to trace the semantic con-text and syntax of a sentence [15]. This hypothesis is investigated in this work by combining re-stricted (or time-restricted) <b>self-attention</b> with a dilation mechanism, whereby a high <b>self-attention</b> resolution for neighboring frames and a lower <b>self-attention</b> resolution for distant information ...", "dateLastCrawled": "2021-12-02T00:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Dilated Residual Network with Multi-head</b> <b>Self-attention</b> for Speech ...", "url": "https://www.researchgate.net/publication/332791636_Dilated_Residual_Network_with_Multi-head_Self-attention_for_Speech_Emotion_Recognition", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/332791636_<b>Dilated_Residual_Network_with_Multi</b>...", "snippet": "While Li et al. [23] proposed the combining use of Dilated Residual Network and <b>Multi-head</b> <b>Self-attention</b> for feature <b>learning</b> in speech emotion recognition. <b>Multi-head</b> <b>Self-attention</b> models ...", "dateLastCrawled": "2021-12-21T23:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Machine</b> <b>Learning</b> Glossary: Language Evaluation | Google Developers", "url": "https://developers.google.com/machine-learning/glossary/language?hl=ko", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine</b>-<b>learning</b>/glossary/language?hl=ko", "snippet": "Refer also to <b>self-attention</b> and <b>multi-head</b> <b>self-attention</b>, which are the building blocks of Transformers. B. bag of words. #language. A representation of the words in a phrase or passage, irrespective of order. For example, bag of words represents the following three phrases identically: the dog jumps; jumps the dog; dog jumps the; Each word is mapped to an index in a sparse vector, where the vector has an index for every word in the vocabulary. For example, the phrase the dog jumps is ...", "dateLastCrawled": "2021-12-23T08:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "CoAtNet: how to perfectly combine CNNs and Transformers | by Leonardo ...", "url": "https://medium.com/codex/coatnet-how-to-perfectly-combine-cnns-and-transformer-9632e187ecbf", "isFamilyFriendly": true, "displayUrl": "https://medium.com/codex/coatnet-how-to-perfectly-combine-cnns-and-transformer-9632e...", "snippet": "The <b>multi-head</b> attention block computes <b>self-attention</b> several times with different weight matrices and then concatenates the results together, which are resized to the embedding dimension using ...", "dateLastCrawled": "2022-01-26T06:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Transformers in NLP: A beginner friendly explanation | Towards Data Science", "url": "https://towardsdatascience.com/transformers-89034557de14", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>transformer</b>s-89034557de14", "snippet": "If you are looking for an <b>analogy</b> between <b>self attention</b> and attention, think of z serving the purpose of context vectors and not global alignment weights. The <b>Transformer</b> . \u26a0\ufe0f A word of caution: the contents of this image may appear exponentially more complicated than they are. We will break this scary beast down into small baby beasts and it will all make sense. (I promise #2) (left) The <b>Transformer</b> architecture. Source: paper. (right) An abstracted version of the same for better ...", "dateLastCrawled": "2022-02-02T13:19:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(multi-head self-attention)  is like +(taking a selfie)", "+(multi-head self-attention) is similar to +(taking a selfie)", "+(multi-head self-attention) can be thought of as +(taking a selfie)", "+(multi-head self-attention) can be compared to +(taking a selfie)", "machine learning +(multi-head self-attention AND analogy)", "machine learning +(\"multi-head self-attention is like\")", "machine learning +(\"multi-head self-attention is similar\")", "machine learning +(\"just as multi-head self-attention\")", "machine learning +(\"multi-head self-attention can be thought of as\")", "machine learning +(\"multi-head self-attention can be compared to\")"]}