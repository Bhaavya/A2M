{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "How To <b>Fine</b> Tune Your <b>Machine</b> <b>Learning</b> Models To Improve Forecasting ...", "url": "https://medium.com/fintechexplained/how-to-fine-tune-your-machine-learning-models-to-improve-forecasting-accuracy-e18e67e58898", "isFamilyFriendly": true, "displayUrl": "https://medium.com/fintechexplained/how-to-<b>fine</b>-tune-your-<b>machine</b>-<b>learning</b>-<b>models</b>-to...", "snippet": "<b>Fine</b> <b>tuning</b> <b>machine</b> <b>learning</b> predictive <b>model</b> is a crucial step to improve accuracy of the forecasted results. In the recent past, I have written a number of articles that explain how <b>machine</b> ...", "dateLastCrawled": "2022-02-02T18:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Fine</b>-<b>tuning a Neural Network explained</b> - deeplizard", "url": "https://deeplizard.com/learn/video/5T-iXNNiwIs", "isFamilyFriendly": true, "displayUrl": "https://deeplizard.com/learn/video/5T-iXNNiwIs", "snippet": "<b>Fine</b>-<b>tuning</b> is a way of applying or utilizing transfer <b>learning</b>. Specifically, <b>fine</b>-<b>tuning</b> is a process that takes a <b>model</b> that has already been trained for one given task and then tunes or tweaks the <b>model</b> to make it perform a second similar task.", "dateLastCrawled": "2022-02-03T15:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Fine</b>-<b>Tuning</b> <b>Machine</b> <b>Learning</b> Models with Scikit-Learn | by Oluwatobi ...", "url": "https://medium.com/kernel-x/fine-tuning-machine-learning-models-using-scikit-learn-9e274782ef05?source=post_internal_links---------7----------------------------", "isFamilyFriendly": true, "displayUrl": "https://medium.com/kernel-x/<b>fine</b>-<b>tuning</b>-<b>machine</b>-<b>learning</b>-<b>models</b>-using-scikit-learn-9e...", "snippet": "<b>Model</b> <b>fine</b>-<b>tuning</b> is the art of tweaking your <b>model</b>(s) to get optimal results from them. It can be a computationally expensive task and exhaustive, but the results are quite worth the computational\u2026", "dateLastCrawled": "2022-01-28T23:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "How To <b>Fine</b> Tune Your <b>Machine</b> <b>Learning Models To Improve Forecasting</b> ...", "url": "https://www.kdnuggets.com/2019/01/fine-tune-machine-learning-models-forecasting.html", "isFamilyFriendly": true, "displayUrl": "https://www.<b>kdnuggets</b>.com/2019/01/<b>fine</b>-tune-<b>machine</b>-<b>learning</b>-<b>models</b>-forecasting.html", "snippet": "<b>Fine</b> <b>tuning</b> <b>machine</b> <b>learning</b> <b>model</b> is a black art. It can turn out to be an exhaustive task. I will be covering a number of methodologies in this article that we can follow to get accurate results in a shorter time. I am often asked a question on the techniques that can be utilised to tune the forecasting models when the features are stable and the feature set is decomposed. Once everything is tried, we should look to tune our <b>machine</b> <b>learning</b> models. <b>Tuning</b> <b>Machine</b> <b>Learning</b> <b>Model</b> <b>Is Like</b> ...", "dateLastCrawled": "2022-01-29T21:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "How to Efficiently <b>Fine</b>-Tune your <b>Machine Learning</b> Models | by Khuyen ...", "url": "https://towardsdatascience.com/how-to-fine-tune-your-machine-learning-models-with-ease-8ca62d1217b1", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/how-to-<b>fine</b>-tune-your-<b>machine-learning</b>-<b>models</b>-with-ease...", "snippet": "To set the parameters of a particular class, we use class_name__parameter = [para_1, para_2, para_3]. Make sure to have two underscores between class\u2019s name and parameter. grid_search.fit(X_train, y_train) creates several runs using different parameters with specified transformations, and estimator.The combination of parameters yielding the best result will be chosen for the transformation step.", "dateLastCrawled": "2022-01-31T06:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Transfer Learning/Fine-Tuning a</b> Deep <b>Learning</b> <b>Model</b> - <b>Machine</b> <b>Learning</b>", "url": "https://datastronomy.com/eli5-transfer-learning-fine-tuning-a-deep-learning-model/", "isFamilyFriendly": true, "displayUrl": "https://datastronomy.com/<b>eli5-transfer-learning-fine-tuning-a</b>-deep-<b>learning</b>-<b>model</b>", "snippet": "Transfer <b>learning</b>, or <b>fine</b>-<b>tuning</b>, is a process whereby you take a deep <b>learning</b> <b>model</b> that has been trained on lots of data (1M+ examples) and continue training it on a smaller dataset to \u201coverfit\u201d it to that particular class of problem. The <b>model</b> becomes inferior at its original task and better at the new specific task, but it also performs much better than a <b>model</b> that was only trained on the small problem-specific dataset. Transfer <b>learning</b> is most commonly used in computer vision ...", "dateLastCrawled": "2022-01-19T02:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine</b> <b>Learning</b> <b>Model</b> Optimization with Hyper Parameter <b>Tuning</b> Approach", "url": "https://globaljournals.org/GJCST_Volume21/2-Machine-Learning-Model-Optimization.pdf", "isFamilyFriendly": true, "displayUrl": "https://globaljournals.org/GJCST_Volume21/2-<b>Machine</b>-<b>Learning</b>-<b>Model</b>-Optimization.pdf", "snippet": "<b>Machine</b> <b>Learning</b> <b>Model</b> Optimization with Hyper Parameter <b>Tuning</b> Approach By Md Riyad Hossain &amp; Dr. Douglas Timmer . UTRGV Abstract-Hyper-parameters <b>tuning</b> is a key step to find the optimal <b>machine</b> <b>learning</b> parameters. Determining the best hyper-parameters takes a good deal of time, especially when the objective functions are costly to determine, or a large number of parameters are required to be tuned. In contrast to the conventional <b>machine</b> <b>learning</b> algorithms, Neural Network requires ...", "dateLastCrawled": "2022-01-19T18:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "What <b>does a fine-tuning model look like</b>? \u2013 MachineCurve", "url": "https://www.machinecurve.com/index.php/question/what-does-a-fine-tuning-model-look-like/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/index.php/question/what-<b>does-a-fine-tuning-model-look-like</b>", "snippet": "In other words, <b>fine</b>-<b>tuning</b> a <b>model</b> involves taking a pretrained <b>model</b>, then training it further, but then with a different type of loss function that takes into account a specific task \u2013 such as summarization, question answering, or natural language entailment.", "dateLastCrawled": "2022-01-22T18:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "performance - What is <b>tuning</b> in <b>machine</b> <b>learning</b>? - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/22903267/what-is-tuning-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/22903267", "snippet": "Translating this into common sense, <b>tuning</b> is essentially selecting the best parameters for an algorithm to optimize its performance given a working environment such as hardware, specific workloads, etc. And <b>tuning</b> in <b>machine</b> <b>learning</b> is an automated process for doing this. For example, there is no such thing as a &quot;perfect set&quot; of optimizations ...", "dateLastCrawled": "2022-01-16T04:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>machine</b> <b>learning</b> - <b>Fine tuning vs Retraining</b> - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/45134834/fine-tuning-vs-retraining", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/45134834", "snippet": "So I am <b>learning</b> how to use Tensorflow to <b>fine</b> tune the Inception-v3 <b>model</b> for a custom dataset. I found two tutorials related to this. One was about &quot;How to Retrain Inception&#39;s Final Layer for New Categories&quot; and the other was &quot; Train your own image classifier with Inception in TensorFlow with <b>Fine</b> <b>tuning</b> &quot;.I did the first retraining tutorial on a virtual <b>machine</b> and it took only 2-3 hours to complete.", "dateLastCrawled": "2022-01-28T11:36:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Fine</b>-<b>tuning a Neural Network explained</b> - deeplizard", "url": "https://deeplizard.com/learn/video/5T-iXNNiwIs", "isFamilyFriendly": true, "displayUrl": "https://deeplizard.com/learn/video/5T-iXNNiwIs", "snippet": "<b>Fine</b>-<b>tuning</b> is a way of applying or utilizing transfer <b>learning</b>. Specifically, <b>fine</b>-<b>tuning</b> is a process that takes a <b>model</b> that has already been trained for one given task and then tunes or tweaks the <b>model</b> to make it perform a second <b>similar</b> task.", "dateLastCrawled": "2022-02-03T15:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "How To <b>Fine</b> Tune Your <b>Machine</b> <b>Learning</b> Models To Improve Forecasting ...", "url": "https://medium.com/fintechexplained/how-to-fine-tune-your-machine-learning-models-to-improve-forecasting-accuracy-e18e67e58898", "isFamilyFriendly": true, "displayUrl": "https://medium.com/fintechexplained/how-to-<b>fine</b>-tune-your-<b>machine</b>-<b>learning</b>-<b>models</b>-to...", "snippet": "<b>Fine</b> <b>tuning</b> <b>machine</b> <b>learning</b> predictive <b>model</b> is a crucial step to improve accuracy of the forecasted results. In the recent past, I have written a number of articles that explain how <b>machine</b> ...", "dateLastCrawled": "2022-02-02T18:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "13.2. <b>Fine</b>-<b>Tuning</b> \u2014 Dive into Deep <b>Learning</b> 0.17.2 documentation", "url": "https://d2l.ai/chapter_computer-vision/fine-tuning.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_computer-vision/<b>fine</b>-<b>tuning</b>.html", "snippet": "13.2.1. Steps\u00b6. In this section, we will introduce a common technique in transfer <b>learning</b>: <b>fine</b>-<b>tuning</b>.As shown in Fig. 13.2.1, <b>fine</b>-<b>tuning</b> consists of the following four steps:. Pretrain a neural network <b>model</b>, i.e., the source <b>model</b>, on a source dataset (e.g., the ImageNet dataset).. Create a new neural network <b>model</b>, i.e., the target <b>model</b>.This copies all <b>model</b> designs and their parameters on the source <b>model</b> except the output layer.", "dateLastCrawled": "2022-02-02T04:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>A comparative study of</b> <b>fine</b>-<b>tuning</b> deep <b>learning</b> models for plant ...", "url": "https://www.sciencedirect.com/science/article/pii/S0168169917313303", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0168169917313303", "snippet": "<b>Fine</b>-<b>tuning</b> the models. <b>Fine</b>-<b>tuning</b> is a concept of transfer <b>learning</b>. Transfer <b>learning</b> is <b>a machine</b> <b>learning</b> technique, where knowledge gain during training in one type of problem is used to train in other related task or domain (Pan and Fellow, 2009). In deep <b>learning</b>, the first few layers are trained to identify features of the task. During ...", "dateLastCrawled": "2022-01-26T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Hyperparameter <b>Tuning</b> of <b>Machine</b> <b>Learning</b> <b>Model</b> in Python | Engineering ...", "url": "https://www.section.io/engineering-education/hyperparameter-tuning-of-machine-learning-model-in-python/", "isFamilyFriendly": true, "displayUrl": "https://www.section.io/.../hyperparameter-<b>tuning</b>-of-<b>machine</b>-<b>learning</b>-<b>model</b>-in-python", "snippet": "Hyperparameter <b>Tuning</b> of <b>Machine</b> <b>Learning</b> <b>Model</b> in Python December 10, 2021. Topics: <b>Machine</b> <b>Learning</b>; Hyperparameters are parameters that can be <b>fine</b>-tuned and adjusted. This increases the accuracy score of <b>a machine</b> <b>learning</b> <b>model</b>. <b>Machine</b> algorithms such as Random forest, K-Nearest Neighbor and Decison trees have parameters that can be <b>fine</b>-tuned to achieve an optimized <b>model</b>. This tutorial will increase the <b>model</b>\u2019s accuracy score. This ensures that the <b>model</b> makes accurate predictions ...", "dateLastCrawled": "2022-02-01T18:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Fine</b>-<b>Tuning</b> NLP Models With Hugging Face | by Kedion | Medium", "url": "https://kedion.medium.com/fine-tuning-nlp-models-with-hugging-face-f92d55949b66", "isFamilyFriendly": true, "displayUrl": "https://kedion.medium.com/<b>fine</b>-<b>tuning</b>-nlp-<b>models</b>-with-hugging-face-f92d55949b66", "snippet": "Generally, <b>fine</b>-<b>tuning</b> involves the following steps: Collecting data for training. Selecting a <b>model</b> that has been pre-trained on data that\u2019s <b>similar</b> to your dataset. Preprocessing data in accordance with the <b>model</b>\u2019s expected input format. <b>Fine</b>-<b>tuning</b> and training our <b>model</b> until we achieve sufficient performance.", "dateLastCrawled": "2022-02-02T11:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Transfer learning and fine-tuning</b> | TensorFlow Core", "url": "https://www.tensorflow.org/tutorials/images/transfer_learning", "isFamilyFriendly": true, "displayUrl": "https://www.tensorflow.org/tutorials/images/transfer_<b>learning</b>", "snippet": "<b>Fine</b>-<b>tuning</b> a pre-trained <b>model</b>: To further improve performance, one might want to repurpose the top-level layers of the pre-trained models to the new dataset via <b>fine</b>-<b>tuning</b>. In this case, you tuned your weights such that your <b>model</b> learned high-level features specific to the dataset. This technique is usually recommended when the training dataset is large and very <b>similar</b> to the original dataset that the pre-trained <b>model</b> was trained on.", "dateLastCrawled": "2022-02-02T18:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>machine</b> <b>learning</b> - Is finetuning from a pretrained <b>model</b> always better ...", "url": "https://datascience.stackexchange.com/questions/15371/is-finetuning-from-a-pretrained-model-always-better-than-training-from-scratch", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/15371", "snippet": "$\\begingroup$ Yes I get that if the task <b>is similar</b> then <b>fine</b> <b>tuning</b> is good. But how does one define <b>similar</b>? And how does one weigh the benefits of the pretrained architecture having the general ability to look across different image types? I&#39;m afraid I may be venturing into uncharted areas here, since the understanding of what the layers represent is still unclear at the moment.", "dateLastCrawled": "2022-01-26T08:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>machine</b> <b>learning</b> - <b>Fine</b>-<b>tuning</b> with a subset of the same data - Cross ...", "url": "https://stats.stackexchange.com/questions/289036/fine-tuning-with-a-subset-of-the-same-data", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/289036/<b>fine</b>-<b>tuning</b>-with-a-subset-of-the-same...", "snippet": "Reference for Transfer <b>Learning</b>. Boosting, bagging and randomization are methods to improve <b>model</b> performance but on samples of same data. Boosting and bagging are more specifically ensemble methods that create a number of classifiers and then combine them using various methods to get an improved <b>model</b> - or <b>fine</b> <b>tuning</b> as you say.", "dateLastCrawled": "2022-01-21T23:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Fine</b>-<b>Tuning</b> Pre-trained <b>Model</b> <b>VGG-16</b> | by Muriel Kosaka | Towards Data ...", "url": "https://towardsdatascience.com/fine-tuning-pre-trained-model-vgg-16-1277268c537f", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>fine</b>-<b>tuning</b>-pre-trained-<b>model</b>-<b>vgg-16</b>-1277268c537f", "snippet": "After reading his article, I\u2019ve come to realize that rather than using pre-trained models as feature extractors, I should be <b>fine</b>-<b>tuning</b> the <b>model</b> by training some layers and leaving other layers frozen as my dataset is small (1,440 files) and not <b>similar</b> to the <b>VGG-16</b> <b>model</b> dataset. Here I will explore this type of <b>fine</b>-<b>tuning</b> of the <b>VGG-16</b> pre-trained <b>model</b> on the RAVDESS Audio Dataset and determine its effect on <b>model</b> accuracy.", "dateLastCrawled": "2022-02-01T20:15:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Fine</b>-<b>Tuning</b> <b>Machine</b> <b>Learning</b> Models with Scikit-Learn | by Oluwatobi ...", "url": "https://medium.com/kernel-x/fine-tuning-machine-learning-models-using-scikit-learn-9e274782ef05?source=post_internal_links---------7----------------------------", "isFamilyFriendly": true, "displayUrl": "https://medium.com/kernel-x/<b>fine</b>-<b>tuning</b>-<b>machine</b>-<b>learning</b>-<b>models</b>-using-scikit-learn-9e...", "snippet": "<b>Model</b> <b>fine</b>-<b>tuning</b> is the art of tweaking your <b>model</b>(s) to get optimal results from them. It <b>can</b> be a computationally expensive task and exhaustive, but the results are quite worth the computational\u2026", "dateLastCrawled": "2022-01-28T23:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "performance - What is <b>tuning</b> in <b>machine</b> <b>learning</b>? - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/22903267/what-is-tuning-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/22903267", "snippet": "Thus, <b>tuning</b> an algorithm or <b>machine</b> <b>learning</b> technique, <b>can</b> be simply <b>thought</b> of as process which one goes through in which they optimize the parameters that impact the <b>model</b> in order to enable the algorithm to perform the best (once, of course you have defined what &quot;best&quot; actual is). To make it more concrete, here are a few examples. If you ...", "dateLastCrawled": "2022-01-16T04:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Transfer Learning/Fine-Tuning a</b> Deep <b>Learning</b> <b>Model</b> - <b>Machine</b> <b>Learning</b>", "url": "https://datastronomy.com/eli5-transfer-learning-fine-tuning-a-deep-learning-model/", "isFamilyFriendly": true, "displayUrl": "https://datastronomy.com/<b>eli5-transfer-learning-fine-tuning-a</b>-deep-<b>learning</b>-<b>model</b>", "snippet": "Transfer <b>learning</b>, or <b>fine</b>-<b>tuning</b>, is a process whereby you take a deep <b>learning</b> <b>model</b> that has been trained on lots of data (1M+ examples) and continue training it on a smaller dataset to \u201coverfit\u201d it to that particular class of problem. The <b>model</b> becomes inferior at its original task and better at the new specific task, but it also performs much better than a <b>model</b> that was only trained on the small problem-specific dataset.", "dateLastCrawled": "2022-01-19T02:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Fine-tuning with Keras and Deep Learning</b> - PyImageSearch", "url": "https://www.pyimagesearch.com/2019/06/03/fine-tuning-with-keras-and-deep-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.pyimagesearch.com/2019/06/03/<b>fine-tuning-with-keras-and-deep-learning</b>", "snippet": "Figure 1: <b>Fine-tuning with Keras and deep learning</b> using Python involves retraining the head of a network to recognize classes it was not originally intended for. Note: The following section has been adapted from my book, Deep <b>Learning</b> for Computer Vision with Python.For the full set of chapters on transfer <b>learning</b> and <b>fine</b>-<b>tuning</b>, please refer to the text. Earlier in this series of posts on transfer <b>learning</b>, we learned how to treat a pre-trained Convolutional Neural Network as a feature ...", "dateLastCrawled": "2022-02-02T20:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "computer vision - <b>Fine Tuning</b> vs. Transferlearning vs. <b>Learning</b> from ...", "url": "https://stats.stackexchange.com/questions/343763/fine-tuning-vs-transferlearning-vs-learning-from-scratch", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/343763", "snippet": "In <b>Fine-tuning</b>, an approach of Transfer <b>Learning</b>, we have a dataset, and we use let&#39;s say 90% of it in training. Then, we train the same <b>model</b> with the remaining 10%. Usually, we change the <b>learning</b> rate to a smaller one, so it does not have a significant impact on the already adjusted weights. You <b>can</b> also have a base <b>model</b> working for a similar task and then freezing some of the layers to keep the old knowledge when performing the new training session with the new data. The output layer ...", "dateLastCrawled": "2022-01-23T22:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>machine</b> <b>learning</b> - Tensorflow <b>Fine</b>-<b>tuning</b> a <b>model</b> from my own ...", "url": "https://datascience.stackexchange.com/questions/40856/tensorflow-fine-tuning-a-model-from-my-own-checkpoint", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/40856", "snippet": "Assume that I am going to do more training with a similar data set in the future, is there any benefit to me using a <b>fine</b> tune checkpoint from a <b>model</b> that I created from my own training as opposed to the original SSD_Mobilenet_V1 version (for example 5000 images and 50000 steps). Does it improve any future training or am I just better off using the original one every time I train? I&#39;m probably searching for the wrong thing, but I cannot find anything that suggests improvement. In my reading ...", "dateLastCrawled": "2022-01-23T22:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Model</b> <b>tuning</b> - ML exam study guide", "url": "https://www.mlexam.com/model-tuning/", "isFamilyFriendly": true, "displayUrl": "https://www.mlexam.com/<b>model</b>-<b>tuning</b>", "snippet": "Hyperparameters <b>can</b> <b>be thought</b> of as the external controls that influence how the <b>model</b> operates, just as flight instruments control how an aeroplane flies. These values are external to the <b>model</b> and are controlled by the user. They <b>can</b> influence how an algorithm is trained and the structure of the final <b>model</b>. The optimized settings are difficult to determine empirically although prior experience with the <b>model</b> and data may help. Exhaustive manual searches for the best hyperparameters would ...", "dateLastCrawled": "2022-01-31T23:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Hyperparameter Tuning of Keras Deep</b> <b>Learning</b> <b>Model</b> in Python - One Zero ...", "url": "https://onezero.blog/hyperparameter-tuning-of-keras-deep-learning-model-in-python/", "isFamilyFriendly": true, "displayUrl": "https://onezero.blog/<b>hyperparameter-tuning-of-keras-deep</b>-<b>learning</b>-<b>model</b>-in-python", "snippet": "Thus, I <b>thought</b> it would be interesting to <b>model</b> the concrete\u2019s compressive strength using a deep <b>learning</b> <b>model</b>. Hence, in this article, we are going to use the concrete dataset [1] obtained from the UCI <b>Machine</b> <b>Learning</b> library. The dataset includes the following variables, which are the ingredients for making durable high strength concrete. I1: Cement (C1): kg in a m3 mixture I2: Blast Furnace Slag (C2): kg in a m3 mixture I3: Fly Ash (C3): kg in a m3 mixture I4: Water (C4): kg in a m3 ...", "dateLastCrawled": "2022-02-03T06:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Is <b>fine</b> <b>tuning</b> twice a viable thing to do?? [D] : MLQuestions", "url": "https://www.reddit.com/r/MLQuestions/comments/mwgik3/is_fine_tuning_twice_a_viable_thing_to_do_d/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/MLQuestions/comments/mwgik3/is_<b>fine</b>_<b>tuning</b>_twice_a_viable...", "snippet": "The datasets aren&#39;t similar enough to be merged fully Hence <b>thought</b> of double <b>fine</b> <b>tuning</b> Plus double <b>fine</b> <b>tuning</b> will give me freedom to a certain degree on how much of the original weights i want to change. 3. Reply . Share. Report Save Follow. Continue this thread level 1 \u00b7 9 mo. ago. If your dataset is a smaller subset of a more general dataset, it prob makes sense to train on the more general dataset and then finetune on your smaller dataset. You&#39;ll prob have to play around with ...", "dateLastCrawled": "2022-01-17T05:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What is <b>parameter tuning in machine learning? - Quora</b>", "url": "https://www.quora.com/What-is-parameter-tuning-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>parameter-tuning-in-machine-learning</b>", "snippet": "Answer (1 of 3): Hyperparameters contain the data that govern the training process itself. Your training application handles three categories of data as it trains your <b>model</b>: * Your input data (also called training data) is a collection of individual records (instances) containing the features...", "dateLastCrawled": "2022-01-17T00:02:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Fine</b>-<b>Tuning</b> <b>Machine</b> <b>Learning</b> Models with Scikit-Learn | by Oluwatobi ...", "url": "https://medium.com/kernel-x/fine-tuning-machine-learning-models-using-scikit-learn-9e274782ef05?source=post_internal_links---------7----------------------------", "isFamilyFriendly": true, "displayUrl": "https://medium.com/kernel-x/<b>fine</b>-<b>tuning</b>-<b>machine</b>-<b>learning</b>-<b>models</b>-using-scikit-learn-9e...", "snippet": "<b>Model</b> <b>fine</b>-<b>tuning</b> is the art of tweaking your <b>model</b>(s) to get optimal results from them. It <b>can</b> be a computationally expensive task and exhaustive, but the results are quite worth the computational\u2026", "dateLastCrawled": "2022-01-28T23:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>A comparative study of</b> <b>fine</b>-<b>tuning</b> deep <b>learning</b> models for plant ...", "url": "https://www.sciencedirect.com/science/article/pii/S0168169917313303", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0168169917313303", "snippet": "<b>Fine</b>-<b>tuning</b> the models. <b>Fine</b>-<b>tuning</b> is a concept of transfer <b>learning</b>. Transfer <b>learning</b> is <b>a machine</b> <b>learning</b> technique, where knowledge gain during training in one type of problem is used to train in other related task or domain (Pan and Fellow, 2009). In deep <b>learning</b>, the first few layers are trained to identify features of the task. During ...", "dateLastCrawled": "2022-01-26T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>machine</b> <b>learning</b> - Why do we <b>fine</b>-tune language models and not just ...", "url": "https://datascience.stackexchange.com/questions/99447/why-do-we-fine-tune-language-models-and-not-just-include-the-data-in-the-pre-tra", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/99447/why-do-we-<b>fine</b>-tune-language...", "snippet": "While we have shown that ULMFiT <b>can</b> achieve state-of-the-art performance on widely used text classification tasks, we believe that language <b>model</b> <b>fine</b>-<b>tuning</b> will be particularly useful in the following settings <b>compared</b> to existing transfer <b>learning</b> approaches (Conneau et al., 2017; McCann et al., 2017; Peters et al., 2018): a) NLP for non-English languages, where training data for supervised pretraining tasks is scarce; b) new NLP tasks where no state-of-the-art architecture exists; and c ...", "dateLastCrawled": "2022-01-15T06:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Fine</b> <b>Tuning</b> results are low in Matching networks for One-shot <b>learning</b>", "url": "https://stackoverflow.com/questions/70896234/fine-tuning-results-are-low-in-matching-networks-for-one-shot-learning", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/70896234/<b>fine</b>-<b>tuning</b>-results-are-low-in-matching...", "snippet": "What I didn&#39;t understand is why are the results from not <b>fine</b>-<b>tuning</b> are higher as <b>compared</b> to <b>fine</b>-<b>tuning</b>. Does this <b>fine</b>-tune mean <b>tuning</b> the hyperparameters? or is it entirely different? Thanks . python <b>machine</b>-<b>learning</b> deep-<b>learning</b> transfer-<b>learning</b> meta-<b>learning</b>. Share. Follow asked 1 min ago. Pathi_rao Pathi_rao. 38 7 7 bronze badges. Add a comment | Active Oldest Votes. Know someone who <b>can</b> answer? Share a link to this question via email, Twitter, or Facebook. Your Answer Thanks for ...", "dateLastCrawled": "2022-01-28T15:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Hyperparameters Optimization. An introduction on how to <b>fine</b>-tune\u2026 | by ...", "url": "https://towardsdatascience.com/hyperparameters-optimization-526348bb8e2d", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/hyper<b>parameters</b>-optimization-526348bb8e2d", "snippet": "An introduction on how to <b>fine</b>-tune <b>Machine</b> and Deep <b>Learning</b> models using techniques such as: Random Search, Automated Hyperparameter <b>Tuning</b> and Artificial Neural Networks <b>Tuning</b>. Pier Paolo Ippolito. Sep 26, 2019 \u00b7 10 min read. Introduction. <b>Machine</b> <b>Learning</b> models are composed of two different types of <b>parameters</b>: Hyperparameters = are all the <b>parameters</b> which <b>can</b> be arbitrarily set by the user before starting training (eg. number of estimators in Random Forest). <b>Model</b> <b>parameters</b> = are ...", "dateLastCrawled": "2022-01-30T11:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Transfer learning and fine-tuning</b> | TensorFlow Core", "url": "https://www.tensorflow.org/tutorials/images/transfer_learning", "isFamilyFriendly": true, "displayUrl": "https://www.tensorflow.org/tutorials/images/transfer_<b>learning</b>", "snippet": "<b>Fine</b>-<b>tuning</b> a pre-trained <b>model</b>: To further improve performance, one might want to repurpose the top-level layers of the pre-trained models to the new dataset via <b>fine</b>-<b>tuning</b>. In this case, you tuned your weights such that your <b>model</b> learned high-level features specific to the dataset. This technique is usually recommended when the training dataset is large and very similar to the original dataset that the pre-trained <b>model</b> was trained on.", "dateLastCrawled": "2022-02-02T18:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Fine-Tuning your Linear Regression Model</b> | Jigsaw Academy", "url": "https://www.jigsawacademy.com/fine-tuning-your-linear-regression-model/", "isFamilyFriendly": true, "displayUrl": "https://www.jigsawacademy.com/<b>fine-tuning-your-linear-regression-model</b>", "snippet": "With zero prior coding knowledge but a complete focus on wanting to excel in the domain of Data Science and <b>Machine</b> <b>Learning</b>, Abhishek Prasad Nonia enrolled in our exhaustive program. Delighted with the <b>learning</b> experience and thrilled to have acquired the knowledge, he says, \u201c I have zero coding experience. I have never done coding before. But because of the experienced faculty of the jigsaw, I get to learn new things every day. Jigsaw&#39;s experienced faculty helps the student a lot in ...", "dateLastCrawled": "2022-01-30T12:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>machine</b> <b>learning</b> - <b>Fine Tuning of GoogLeNet Model</b> - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/36841158/fine-tuning-of-googlenet-model", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/36841158", "snippet": "<b>Fine</b>-<b>tuning</b> is a very useful trick to achieve a promising accuracy <b>compared</b> to past manual feature. @Shai already posted a good tutorial for <b>fine</b>-<b>tuning</b> the Googlenet using Caffe, so I just want to give some recommends and tricks for <b>fine</b>-<b>tuning</b> for general cases.. In most of time, we face a task classification problem that new dataset (e.g. Oxford 102 flower dataset or Cat&amp;Dog) has following four common situations CS231n: New dataset is small and similar to original dataset.", "dateLastCrawled": "2022-01-21T11:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "computer vision - <b>Fine Tuning</b> vs. Transferlearning vs. <b>Learning</b> from ...", "url": "https://stats.stackexchange.com/questions/343763/fine-tuning-vs-transferlearning-vs-learning-from-scratch", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/343763", "snippet": "Transfer <b>learning</b> is when a <b>model</b> developed for one task is reused to work on a second task. <b>Fine-tuning</b> is one approach to transfer <b>learning</b> where you change the <b>model</b> output to fit the new task and train only the output <b>model</b>. In Transfer <b>Learning</b> or Domain Adaptation, we train the <b>model</b> with a dataset. Then, we train the same <b>model</b> with another dataset that has a different distribution of classes, or even with other classes than in the first training dataset). In <b>Fine-tuning</b>, an approach ...", "dateLastCrawled": "2022-01-23T22:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Is <b>fine</b> <b>tuning</b> <b>a pre-trained model equivalent to</b> transfer <b>learning</b>? - Quora", "url": "https://www.quora.com/Is-fine-tuning-a-pre-trained-model-equivalent-to-transfer-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Is-<b>fine</b>-<b>tuning</b>-<b>a-pre-trained-model-equivalent-to</b>-transfer-<b>learning</b>", "snippet": "Answer (1 of 4): Yes, if the data on which the <b>model</b> is <b>fine</b>-tuned is of a different nature from the original data used to pre-train the <b>model</b>. It is a form of transfer <b>learning</b>, and it has worked extremely well in many object classification tasks.", "dateLastCrawled": "2022-01-15T07:22:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "What Is <b>Machine</b> <b>Learning</b> (Explained in 5 Minutes) | 365 Data Science", "url": "https://365datascience.com/trending/machine-learning-explained-in-5-minutes/", "isFamilyFriendly": true, "displayUrl": "https://365datascience.com/trending/<b>machine</b>-<b>learning</b>-explained-in-5-minutes", "snippet": "So, very often the art of the data scientist and <b>Machine</b> <b>Learning</b> engineer professions is in the <b>fine</b>-<b>tuning</b> of an already well-performing model. In some cases, a 0.1% improvement in accuracy could be of important significance \u2013 especially when the ML model is applied in areas like healthcare , fraud prevention , and self-driving vehicles.", "dateLastCrawled": "2022-01-26T20:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Transfer Learning/Fine-Tuning a</b> Deep <b>Learning</b> Model - <b>Machine</b> <b>Learning</b>", "url": "https://datastronomy.com/eli5-transfer-learning-fine-tuning-a-deep-learning-model/", "isFamilyFriendly": true, "displayUrl": "https://datastronomy.com/<b>eli5-transfer-learning-fine-tuning-a</b>-deep-<b>learning</b>-model", "snippet": "What you just did was the human equivalent of transfer <b>learning</b>. You took a trained brain\u2014or stepping back from our <b>analogy</b>, a neural net\u2014and you adapted it to a specialized problem. Transfer <b>learning</b>, or <b>fine</b>-<b>tuning</b>, is a process whereby you take a deep <b>learning</b> model that has been trained on lots of data (1M+ examples) and continue training it on a smaller dataset to \u201coverfit\u201d it to that particular class of problem. The model becomes inferior at its original task and better at the ...", "dateLastCrawled": "2022-01-19T02:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "13.2. <b>Fine</b>-<b>Tuning</b> \u2014 Dive into Deep <b>Learning</b> 0.17.2 documentation", "url": "https://d2l.ai/chapter_computer-vision/fine-tuning.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_computer-vision/<b>fine</b>-<b>tuning</b>.html", "snippet": "13.2.1. Steps\u00b6. In this section, we will introduce a common technique in transfer <b>learning</b>: <b>fine</b>-<b>tuning</b>.As shown in Fig. 13.2.1, <b>fine</b>-<b>tuning</b> consists of the following four steps:. Pretrain a neural network model, i.e., the source model, on a source dataset (e.g., the ImageNet dataset).. Create a new neural network model, i.e., the target model.This copies all model designs and their parameters on the source model except the output layer.", "dateLastCrawled": "2022-02-02T04:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "TRANSFER <b>LEARNING</b> AND <b>FINE</b> <b>TUNING</b> OF NEURAL NETWORKS", "url": "https://www.indusmic.com/post/transfer-learning-and-fine-tuning-of-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://www.indusmic.com/post/transfer-<b>learning</b>-and-<b>fine</b>-<b>tuning</b>-of-neural-networks", "snippet": "Author: Parakh Jain Human beings are able to learn, detect objects and classify them with their eyes. If we want to learn the difference between objects like cars and trucks. First we need to see the images of cars and trucks and will learn from the images. Only then we are able to distinguish between cars and trucks. Similarly, a <b>machine</b> needs data from which it can learn and make able to distinguish and classify images. Training a Model from Scratch <b>Learning</b> from scratch means building a netwo", "dateLastCrawled": "2022-01-31T02:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Fine-tuning with Keras and Deep Learning</b> - PyImageSearch", "url": "https://www.pyimagesearch.com/2019/06/03/fine-tuning-with-keras-and-deep-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.pyimagesearch.com/2019/06/03/<b>fine-tuning-with-keras-and-deep-learning</b>", "snippet": "A standard <b>machine</b> <b>learning</b> classifier (in our case, Logistic Regression), was trained on top of the CNN features, exactly as we would do with hand-engineered features such as SIFT, HOG, LBPs, etc. This approach to transfer <b>learning</b> is called feature extraction. But there is another type of transfer <b>learning</b>, one that can actually outperform the feature extraction method. This method is called <b>fine</b>-<b>tuning</b> and requires us to perform \u201cnetwork surgery\u201d. First, we take a scalpel and cut off ...", "dateLastCrawled": "2022-02-02T20:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "14.2. <b>Fine-Tuning</b> \u2014 Dive into Deep <b>Learning</b> 0.17.0 documentation", "url": "http://preview.d2l.ai/d2l-en/master/chapter_computer-vision/fine-tuning.html", "isFamilyFriendly": true, "displayUrl": "preview.d2l.ai/d2l-en/master/chapter_computer-vision/<b>fine-tuning</b>.html", "snippet": "14.2.1. Steps\u00b6. In this section, we will introduce a common technique in transfer <b>learning</b>: <b>fine-tuning</b>.As shown in Fig. 14.2.1, <b>fine-tuning</b> consists of the following four steps:. Pretrain a neural network model, i.e., the source model, on a source dataset (e.g., the ImageNet dataset).. Create a new neural network model, i.e., the target model.This copies all model designs and their parameters on the source model except the output layer.", "dateLastCrawled": "2021-12-20T07:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Handling Data <b>Scarcity while building Machine Learning applications</b> ...", "url": "https://towardsdatascience.com/handling-data-scarcity-while-building-machine-learning-applications-e6c243b284b0", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/handling-data-<b>scarcity-while-building-machine-learning</b>...", "snippet": "When training a <b>machine</b> <b>learning</b> model, ... The second step is to \u201c<b>fine</b> tune\u201d the first model on the target dataset (using your few 100 tweet examples). The <b>fine</b> <b>tuning</b> step involves the same setup (language modeling) as the first step with the difference being the model is now being trained to capture idiosyncrasies in your target dataset (tweets). The intuition here is the language in tweets is different from the language in wikipedia articles, hence there is a need to capture this ...", "dateLastCrawled": "2022-01-30T09:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "How <b>to use ConvNets in different ways - a brief analogy</b> | by Himanshu ...", "url": "https://medium.com/voice-tech-podcast/how-to-use-convnets-in-different-ways-a-brief-analogy-1b69c3e88f3b", "isFamilyFriendly": true, "displayUrl": "https://medium.com/.../how-<b>to-use-convnets-in-different-ways-a-brief-analogy</b>-1b69c3e88f3b", "snippet": "<b>Fine</b>-<b>tuning</b> the Pre-trained ConvNets \u2014 Transfer <b>Learning</b>. <b>Fine</b>-<b>tuning</b> means taking weights of a trained neural network and use it as initialization for a new model being trained on data from the ...", "dateLastCrawled": "2021-05-19T10:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Should I Learn Machine Learning</b>? | GenUI", "url": "https://www.genui.com/resources/ml-for-developers", "isFamilyFriendly": true, "displayUrl": "https://www.genui.com/resources/ml-for-developers", "snippet": "It\u2019s no longer necessary to have an advanced degree in data science to make use of <b>machine</b> <b>learning</b>. The <b>analogy</b> we like to give is with databases. Every seasoned developer knows about databases, both SQL and NoSQL, and knows enough about them to use them effectively in typical projects. Yes, there\u2019s a subset of projects, of such complexity or scale, where average database knowledge is not enough. In those cases, expert knowledge of things like performance <b>tuning</b> and database ...", "dateLastCrawled": "2022-01-30T18:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>machine</b> <b>learning</b> - <b>Fine</b>-<b>tuning</b> Glove Embeddings - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/50909726/fine-tuning-glove-embeddings", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/50909726", "snippet": "<b>Fine</b>-<b>tuning</b> word2vec embeddings has proven very efficient for me in a various NLP tasks, ... Also, Glove is very good at <b>analogy</b>, and performs well on the word2vec dataset. Share. Follow edited Sep 1 &#39;21 at 9:34. Timbus Calin . 10.7k 3 3 gold badges 29 29 silver badges 46 46 bronze badges. answered Sep 1 &#39;21 at 9:23. kiriloff kiriloff. 23.7k 33 33 gold badges 137 137 silver badges 210 210 bronze badges. 4. Very suggestive explanation and summary in short time. \u2013 Timbus Calin. Sep 1 &#39;21 at ...", "dateLastCrawled": "2022-01-15T15:17:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "What Is Fine Tuning In <b>Machine</b> <b>Learning</b>? \u2013 chetumenu.com", "url": "https://chetumenu.com/what-is-fine-tuning-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://chetumenu.com/what-is-fine-tuning-in-<b>machine</b>-<b>learning</b>", "snippet": "Related Question for What Is Fine Tuning In <b>Machine</b> <b>Learning</b>? What is Pretraining and fine tuning? 1. The answer is a mere difference in the terminology used. When the model is trained on a large generic corpus, it is called &#39;pre-training&#39;. When it is adapted to a particular task or dataset it is called as &#39;fine-tuning&#39;. Is fine tuning necessary? A Simple Fine-tuning Is All You Need: Towards Robust Deep <b>Learning</b> Via Adversarial Fine-tuning. Adversarial Training (AT) with Projected Gradient ...", "dateLastCrawled": "2021-12-16T18:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "What is <b>the difference between Transfer Learning vs</b> Fine Tuning vs ...", "url": "https://www.researchgate.net/post/What-is-the-difference-between-Transfer-Learning-vs-Fine-Tuning-vs-Learning-from-scratch", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/post/What-is-<b>the-difference-between-Transfer-Learning-vs</b>...", "snippet": "<b>Fine tuning is like</b> optimization. We optimize the network to achieve the optimal results. May be we can change the number of layers used, no of filters, <b>learning</b> rate and we have many parameters ...", "dateLastCrawled": "2022-01-30T04:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A Gentle Introduction to <b>Transfer Learning</b> in NLP | by Neil Sinclair ...", "url": "https://towardsdatascience.com/a-gentle-introduction-to-transfer-learning-in-nlp-b71e87241d66", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-gentle-introduction-to-<b>transfer-learning</b>-in-nlp-b71e...", "snippet": "To give you a sense of the difference between fine-tuning and training a model, <b>fine-tuning is like</b> taking your car to the mechanic and getting new spark plugs whereas training/pre-training is like getting a whole new engine. As a concrete example, let\u2019s say you run a large food delivery company and you want to get the sentiment (positive or negative intention) of tweets people make about your company so you can quickly attend to the negative ones. Considering you have enough tweets to ...", "dateLastCrawled": "2022-02-02T23:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Philosophers On GPT-3 (updated with replies by GPT-3) | <b>Daily Nous</b>", "url": "https://dailynous.com/2020/07/30/philosophers-gpt-3/", "isFamilyFriendly": true, "displayUrl": "https://<b>dailynous</b>.com/2020/07/30/philosophers-gpt-3", "snippet": "<b>Fine-tuning is like</b> cramming for an exam. The benefit of this is that you do much better in that one exam, but you can end up performing worse on others as a result. In-context <b>learning</b> is like taking the exam after looking at the instructions and some sample questions. GPT-3 might not reach the performance of a student that crams for one particular exam if it doesn\u2019t cram too, but it can wander into a series of exam rooms and perform pretty well from just looking at the paper. It performs ...", "dateLastCrawled": "2022-02-02T10:11:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Deep <b>learning</b> \u2014 Deep Boltzmann <b>Machine</b> (DBM) | by Renu Khandelwal ...", "url": "https://medium.datadriveninvestor.com/deep-learning-deep-boltzmann-machine-dbm-e3253bb95d0f", "isFamilyFriendly": true, "displayUrl": "https://medium.datadriveninvestor.com/deep-<b>learning</b>-deep-boltzmann-<b>machine</b>-dbm-e3253bb...", "snippet": "Boltzmann <b>machine</b> uses randomly initialized Markov chains to approximate the gradient of the likelihood function which is too slow to be practical. DBM uses greedy layer by layer pre training to speed up <b>learning</b> the weights. It relies on <b>learning</b> stacks of Restricted Boltzmann <b>Machine</b> with a small modification using contrastive divergence.", "dateLastCrawled": "2022-01-31T21:17:00.0000000Z", "language": "en", "isNavigational": false}], [], []], "all_bing_queries": ["+(fine tuning)  is like +(tuning a machine learning model)", "+(fine tuning) is similar to +(tuning a machine learning model)", "+(fine tuning) can be thought of as +(tuning a machine learning model)", "+(fine tuning) can be compared to +(tuning a machine learning model)", "machine learning +(fine tuning AND analogy)", "machine learning +(\"fine tuning is like\")", "machine learning +(\"fine tuning is similar\")", "machine learning +(\"just as fine tuning\")", "machine learning +(\"fine tuning can be thought of as\")", "machine learning +(\"fine tuning can be compared to\")"]}