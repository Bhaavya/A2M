{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Assessing</b> <b>the quality</b> of economic evaluations of clinical nurse ...", "url": "https://www.sciencedirect.com/science/article/pii/S2352900815000047", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2352900815000047", "snippet": "<b>Inter-rater</b> <b>agreement</b> (prior to consensus process) was high (83% <b>agreement</b>). Four criteria for <b>the quality</b> of economic evaluations were consistently addressed: specification of clear, measurable objectives; pre-specification of subgroups for subgroup analyses; justified conclusions based on study results; and disclosure of study funding source. A clear statement of the primary outcome measures, incremental analysis, and assessment of uncertainty were often unclear or missing. Due to poor ...", "dateLastCrawled": "2021-12-09T14:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Validity and <b>Inter-rater</b> Reliability Testing of <b>Quality</b> Assessment ...", "url": "https://effectivehealthcare.ahrq.gov/products/quality-tools-testing/research", "isFamilyFriendly": true, "displayUrl": "https://effectivehealthcare.ahrq.gov/products/<b>quality</b>-tools-testing/research", "snippet": "<b>Inter-rater</b> <b>agreement</b> was assessed using kappa statistics. We assessed the association between ES and risk of bias using meta-regression. We examined the impact of study-level factors on the association between risk of bias and ES using subgroup analyses. Two reviewers independently applied the NOS to 131 cohort studies from 8 meta-analyses. <b>Inter-rater</b> <b>agreement</b> was calculated using kappa statistics. Within each meta-analysis, we generated a ratio of pooled estimates for each <b>quality</b> domain ...", "dateLastCrawled": "2022-01-27T23:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A Guide to <b>Inter-rater</b> <b>Agreement</b>", "url": "https://www.cde.state.co.us/educatoreffectiveness/iraguide", "isFamilyFriendly": true, "displayUrl": "https://<b>www.cde.state.co.us</b>/educatoreffectiveness/iraguide", "snippet": "What is <b>Inter-rater</b> <b>Agreement</b>? Because classroom observations can provide teachers with formative feedback to improve their practice, it is important to improve classroom observation techniques to ensure evaluators are consistently identifying high- <b>quality</b> teaching practice and identifying area for improvements. This critical work is where the topic of <b>inter- rater</b> <b>agreement</b>, or IRA, comes in. One way to understand IRA is to break down the jargon, beginning with the two terms you most often ...", "dateLastCrawled": "2022-01-31T22:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "INVESTIGATING <b>INTER-RATER</b> RELIABILITY OF QUALITATIVE TEXT ANNOTATIONS ...", "url": "https://erinmacd.stanford.edu/wp-content/uploads/2020/06/2020-Investigating-Inter-Rater-Reliability-of-Qualitative-Text-Annotations-in-Machine-Learning-Datasets_Dehaibi_Design-2020.pdf", "isFamilyFriendly": true, "displayUrl": "https://erinmacd.stanford.edu/wp-content/uploads/2020/06/2020-Investigating-Inter...", "snippet": "reliability of the annotations since <b>the quality</b> of the <b>model</b> depends on it. In machine learning research, a common way to evaluate the dataset is by looking at the evaluation metrics of the <b>model</b> such as precision, recall, F1 (Jurafsky and Martin, 2017). While these metrics can provide external validity for a <b>model</b>, they are not commonly used in the design research space. A more common approach for <b>assessing</b> reliability of annotator data in design research is <b>inter-rater</b> reliability (IRR ...", "dateLastCrawled": "2022-01-30T04:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "The Pitfalls of <b>Inter-Rater</b> Reliability in Data Labeling and Machine ...", "url": "https://www.surgehq.ai/blog/the-pitfalls-of-inter-rater-reliability-in-data-labeling-and-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.surgehq.ai/blog/the-pitfalls-of-<b>inter-rater</b>-reliability-in-data-labeling...", "snippet": "<b>Inter-rater</b> reliability values range from 0 to 1, where 1 is perfect <b>agreement</b> and 0 is indistinguishable from a random baseline. When evaluating <b>the quality</b> of labeled data, <b>inter-rater</b> reliability (IRR) is often viewed as a holy grail metric. Common wisdom holds that the more raters agree on a given rating, the higher the chance that the ...", "dateLastCrawled": "2022-01-30T04:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Assessing the Reliability of Rating Data</b>", "url": "https://www.pbarrett.net/presentations/rater.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.pbarrett.net/presentations/rater.pdf", "snippet": "<b>Assessing the Reliability of Rating Data</b> Ratings are any kind of coding (qualitative or quantitative) made concerning attitudes, behaviours, or cognitions. Here, I am concerned with those kinds of ratings made by third-parties of a particular individual\u2019s attitudes, behaviour, or cognitions. These might be from rating scales, observational check-lists, or symptom check-lists etc. The principle aim of reliability analysis is to determine the degree of <b>agreement</b> between raters when using a ...", "dateLastCrawled": "2022-02-02T07:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Assessment of the <b>Human Factors Analysis and Classification System</b> ...", "url": "https://tigerprints.clemson.edu/cgi/viewcontent.cgi?article=2231&context=all_dissertations", "isFamilyFriendly": true, "displayUrl": "https://tigerprints.clemson.edu/cgi/viewcontent.cgi?article=2231&amp;context=all_dissertations", "snippet": "evaluated using percent <b>agreement</b>, Krippendorff\u201fs Alpha, and Cohen\u201fs Kappa, while the <b>inter-rater</b> was analyzed using percent <b>agreement</b>, Krippendorff\u201fs Alpha, and Fleiss\u201f Kappa. Because of analytical limitations, only percent <b>agreement</b> and Krippendorff\u201fs Alpha were used for the intra-rater evaluation at the individual tier and category level and Fleiss\u201f Kappa and Krippendorff\u201fs Alpha, for the corresponding <b>inter-rater</b> evaluation. The overall intra-rater and <b>inter-rater</b> results ...", "dateLastCrawled": "2022-01-28T00:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Development and validation of an A3 problem ... - BMJ <b>Quality</b> &amp; Safety", "url": "https://qualitysafety.bmj.com/content/early/2021/03/25/bmjqs-2020-012105", "isFamilyFriendly": true, "displayUrl": "https://<b>quality</b>safety.bmj.com/content/early/2021/03/25/bmjqs-2020-012105", "snippet": "Tests of <b>inter-rater</b> <b>agreement</b> were conducted in cycles 3, 4 and 5. The final assessment tool was tested in a study involving 12 raters <b>assessing</b> 23 items on six A3s that were modified to enable testing a range of scores. Results The intraclass correlation coefficient (ICC) for overall assessment of an A3 (rater\u2019s mean on 23 items per A3 compared across 12 raters and 6 A3s) was 0.89 (95% CI 0.75 to 0.98), indicating excellent reliability. For the 20 items with appreciable variation in ...", "dateLastCrawled": "2022-01-31T00:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A systematic review of questionnaires <b>assessing</b> the psychological ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8383475/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8383475", "snippet": "A <b>quality</b> appraisal of the identified scales was made by three reviewers using the COSMIN checklist for methodological issues and the Terwee criteria for measurement properties. Our search strategy yielded a total of 855 results. Of these, 832 articles were excluded according to exclusion criteria, 23 were assessed for eligibility and 10 were finally included. These are presented in the text with additional useful information found separately. The identified scales tended to be quite short ...", "dateLastCrawled": "2022-01-28T14:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Evaluating the <b>agreement</b> and reliability of a web-based facial analysis ...", "url": "https://link.springer.com/article/10.1007/s11548-021-02423-z", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11548-021-02423-z", "snippet": "The <b>inter-rater</b> reliability was calculated using a two-rater, absolute <b>agreement</b>, two-way mixed-effects <b>model</b> . Facial measurements We selected facial parameters from the pertaining literature [ 6 , 11 ] with an emphasis on selecting the feature points (landmarks) and measurements that a rhinoplasty surgeon would prefer and that were possible to measure using calipers.", "dateLastCrawled": "2022-02-03T18:06:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "How to assess and compare <b>inter-rater</b> reliability, <b>agreement</b> and ...", "url": "https://www.frontiersin.org/articles/10.3389/fpsyg.2014.00509/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/fpsyg.2014.00509", "snippet": "In order to capture the degree of <b>agreement</b> between raters, as well as the relation between ratings, it is important to consider three different aspects: (1) <b>inter-rater</b> reliability <b>assessing</b> to what extent the used measure is able to differentiate between participants with different ability levels, when evaluations are provided by different raters. Measures of <b>inter-rater</b>-reliability can also serve to determine the least amount of divergence between two scores necessary to establish a ...", "dateLastCrawled": "2022-01-29T02:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Inter-rater</b> <b>agreement</b> in the assessment of exposure to carcinogens in ...", "url": "https://oem.bmj.com/content/64/9/582", "isFamilyFriendly": true, "displayUrl": "https://oem.bmj.com/content/64/9/582", "snippet": "Objectives: To evaluate the reliability of an expert team <b>assessing</b> exposure to carcinogens in the offshore petroleum industry and to study how the information provided influenced the <b>agreement</b> among raters. Methods: Eight experts individually assessed the likelihood of exposure for combinations of 17 carcinogens, 27 job categories and four time periods (1970\u20131979, 1980\u20131989, 1990\u20131999 and 2000\u20132005). Each rater assessed 1836 combinations based on summary documents on carcinogenic ...", "dateLastCrawled": "2022-01-28T22:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Types of Reliability in Research</b> | How to Measure it", "url": "https://www.irelandassignmenthelp.com/blogs/types-of-reliability-in-research/", "isFamilyFriendly": true, "displayUrl": "https://www.irelandassignmenthelp.com/blogs/<b>types-of-reliability-in-research</b>", "snippet": "2. <b>Inter-rater</b> Reliability. It is also recognized to be as inter-observer reliability. <b>Inter-rater</b> reliability helps in measuring the level of <b>agreement</b> among the number of people <b>assessing</b> a <b>similar</b> thing. It is considered being an alternative form of reliability. You can utilize <b>inter-rater</b> reliability when investigators collect facts by ...", "dateLastCrawled": "2022-02-03T04:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Guide to <b>Inter-rater</b> <b>Agreement</b>", "url": "https://www.cde.state.co.us/educatoreffectiveness/iraguide", "isFamilyFriendly": true, "displayUrl": "https://<b>www.cde.state.co.us</b>/educatoreffectiveness/iraguide", "snippet": "What is <b>Inter-rater</b> <b>Agreement</b>? Because classroom observations can provide teachers with formative feedback to improve their practice, it is important to improve classroom observation techniques to ensure evaluators are consistently identifying high- <b>quality</b> teaching practice and identifying area for improvements. This critical work is where the topic of <b>inter- rater</b> <b>agreement</b>, or IRA, comes in. One way to understand IRA is to break down the jargon, beginning with the two terms you most often ...", "dateLastCrawled": "2022-01-31T22:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Inter-rater</b> reliability - WikiMili, The Best Wikipedia Reader", "url": "https://wikimili.com/en/Inter-rater_reliability", "isFamilyFriendly": true, "displayUrl": "https://wikimili.com/en/<b>Inter-rater</b>_reliability", "snippet": "In statistics, <b>inter-rater</b> reliability (also called by various <b>similar</b> names, such as <b>inter-rater</b> <b>agreement</b>, <b>inter-rater</b> concordance, inter-observer reliability, and so on) is the degree of <b>agreement</b> among independent observers who rate, code, or assess the same phenomenon. Contents. Concept; Statistics; Joint probability of <b>agreement</b>; Kappa statistics; Correlation coefficients; Intra-class correlation coefficient", "dateLastCrawled": "2021-09-11T01:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 5, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Inter-rater reliability</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Inter-rater_reliability", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Inter-rater_reliability</b>", "snippet": "In statistics, <b>inter-rater reliability</b> (also called by various <b>similar</b> names, such as <b>inter-rater</b> <b>agreement</b>, <b>inter-rater</b> concordance, inter-observer reliability, and so on) is the degree of <b>agreement</b> among independent observers who rate, code, or assess the same phenomenon. In contrast, intra-rater reliability is a score of the consistency in ratings given by the same person across multiple instances. For example, the grader should not let elements like fatigue influence their grading ...", "dateLastCrawled": "2022-02-02T05:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A prospective evaluation of <b>inter-rater</b> <b>agreement</b> of routine medical ...", "url": "https://bmchealthservres.biomedcentral.com/articles/10.1186/s12913-020-05495-w", "isFamilyFriendly": true, "displayUrl": "https://bmchealthservres.biomedcentral.com/articles/10.1186/s12913-020-05495-w", "snippet": "<b>The quality</b> of patient medical records is intrinsically related to patient safety, clinical decision-making, communication between health providers, and continuity of care. Additionally, its data are widely used in observational studies. However, the reliability of the information extracted from the records is a matter of concern in audit processes to ensure <b>inter-rater</b> <b>agreement</b> (IRA). Thus, the objective of this study is to evaluate the IRA among members of the Patient Health Record Review ...", "dateLastCrawled": "2022-02-02T04:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Comparing inter-rater agreement between classes</b> of raters - <b>Cross Validated</b>", "url": "https://stats.stackexchange.com/questions/132609/comparing-inter-rater-agreement-between-classes-of-raters", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/132609/comparing-<b>inter-rater</b>-<b>agreement</b>...", "snippet": "Check the <b>inter-rater</b> <b>agreement</b> within each group and say if they are distinguishable from each other. I&#39;ve searched the literature. Doing (a) seems straightforward with Krippendorf&#39;s alpha. My dataset (which is from real data, not a designed experiment) includes multiple ratings (0-3) per object from each raters&#39; group (experts, semi-experts). I thought of averaging ratings per object, per group, thus creating a dataset with 2 rows that emulate two &quot;raters&quot; (the prototypical expert and the ...", "dateLastCrawled": "2022-01-22T06:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Three-dimensional printing models increase <b>inter-rater</b> <b>agreement</b> for ...", "url": "https://pssjournal.biomedcentral.com/articles/10.1186/s13037-021-00312-7", "isFamilyFriendly": true, "displayUrl": "https://pssjournal.biomedcentral.com/articles/10.1186/s13037-021-00312-7", "snippet": "Proximal humerus fractures (PHF) are frequent, however, several studies show low <b>inter-rater</b> <b>agreement</b> in the diagnosis and treatment of these injuries. Differences are usually related to the experience of the evaluators and/or the diagnostic methods used. This study was designed to investigate the hypothesis that shoulder surgeons and diagnostic imaging specialists using 3D printing models and shoulder CT scans in <b>assessing</b> proximal humerus fractures. We obtained 75 tomographic exams of PHF ...", "dateLastCrawled": "2022-01-22T01:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What is Kappa and How Does <b>It Measure Inter-rater Reliability</b>? - The ...", "url": "https://www.theanalysisfactor.com/kappa-measures-inter-rater-reliability/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>theanalysisfactor</b>.com/kappa-measures-<b>inter-rater</b>-reliability", "snippet": "The Kappa Statistic or Cohen\u2019s* Kappa is a statistical measure of <b>inter-rater</b> reliability for categorical variables. In fact, it&#39;s almost synonymous with <b>inter-rater</b> reliability.Kappa is used when two raters both apply a criterion based on a tool to assess whether or not some condition occurs. Examples include:", "dateLastCrawled": "2022-02-02T19:14:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "How to assess and compare <b>inter-rater</b> reliability, <b>agreement</b> and ...", "url": "https://www.frontiersin.org/articles/10.3389/fpsyg.2014.00509/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/fpsyg.2014.00509", "snippet": "On the example of <b>assessing</b> whether ELAN ratings <b>can</b> be reliably obtained from daycare teachers as well as from parents we show that rater <b>agreement</b>, linear correlation, and <b>inter-rater</b> reliability all have to be considered. Otherwise, an exhaustive conclusion about a rating scale&#39;s employability with different rater groups cannot be made. We also considered the factors gender and bilingualism of the evaluated child as potentially influencing the likelihood of rating <b>agreement</b>.", "dateLastCrawled": "2022-01-29T02:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A Guide to <b>Inter-rater</b> <b>Agreement</b>", "url": "https://www.cde.state.co.us/educatoreffectiveness/iraguide", "isFamilyFriendly": true, "displayUrl": "https://<b>www.cde.state.co.us</b>/educatoreffectiveness/iraguide", "snippet": "What is <b>Inter-rater</b> <b>Agreement</b>? Because classroom observations <b>can</b> provide teachers with formative feedback to improve their practice, it is important to improve classroom observation techniques to ensure evaluators are consistently identifying high- <b>quality</b> teaching practice and identifying area for improvements. This critical work is where the topic of <b>inter- rater</b> <b>agreement</b>, or IRA, comes in. One way to understand IRA is to break down the jargon, beginning with the two terms you most often ...", "dateLastCrawled": "2022-01-31T22:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Assessing the inter-rater agreement for ordinal data through</b> weighted ...", "url": "https://www.researchgate.net/publication/261752528_Assessing_the_inter-rater_agreement_for_ordinal_data_through_weighted_indexes", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/261752528_<b>Assessing_the_inter-rater_agreement</b>...", "snippet": "Abstract. <b>Assessing the inter-rater agreement</b> between observers, in the case of ordinal variables, is an important issue in both the statistical theory and biomedical applications. Typically, this ...", "dateLastCrawled": "2022-01-16T16:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Intra and <b>Inter-Rater</b> Reliability of Screening for Movement Impairments ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4424474/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC4424474", "snippet": "<b>Quality</b> of movement, ... <b>Inter-rater</b> <b>agreement</b>. The percentage <b>agreement</b> for criteria ranged from 67.5 to 100% (mean overall <b>agreement</b> 86.5%), with only two of the 40 criteria in Table 5 with less than 70% <b>agreement</b>. Each test as a whole had <b>agreement</b> above 70% as shown in Table 3. <b>Agreement</b> varied between tests, with some having ratings for all criteria agreeing &gt;80% (Test 3, 4, 7) and for one test &gt;90% (Test 8). Tests 5 and 9 were less reliable, with criteria below 75% (Test 5 D 67.5% ...", "dateLastCrawled": "2021-10-15T13:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Taking Aim at Good Targets: <b>Inter-rater</b> <b>Agreement</b> of Listening ...", "url": "https://www.researchgate.net/publication/254305252_Taking_Aim_at_Good_Targets_Inter-rater_Agreement_of_Listening_Competency", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/254305252_Taking_Aim_at_Good_Targets_Inter...", "snippet": "An examination of listening competency using the Organizational Listening Survey (OLS) demonstrated significant <b>inter-rater</b> <b>agreement</b> in assessments of their co-workers&#39; listening abilities ...", "dateLastCrawled": "2022-01-30T08:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Inter-rater</b> reliability of descriptors for the classification of ...", "url": "https://www.sciencedirect.com/science/article/pii/S1036731421001788", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1036731421001788", "snippet": "Fleiss&#39; kappa (\u03ba) is a measure of <b>inter-rater</b> <b>agreement</b> used in situations where there are two or more raters and the response variable is a categorical variable. 21 Cohen&#39;s kappa coefficient was used to assess how strong the level of <b>agreement</b> was between raters, where a value of &lt;0.20 is poor, 0.21 to 0.40 is fair, 0.41 to 0.60 is moderate, 0.61 to 0.80 is good, and 0.81 to 1.00 is very good. 22 There are six assumptions required to be met when using Fleiss&#39; kappa: (i) a categorical ...", "dateLastCrawled": "2022-01-29T11:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "How to Determine the Validity and Reliability of an Instrument ...", "url": "https://sites.miamioh.edu/discovery-center/2016/11/how-to-determine-the-validity-and-reliability-of-an-instrument/", "isFamilyFriendly": true, "displayUrl": "https://sites.miamioh.edu/discovery-center/2016/11/how-to-determine-the-validity-and...", "snippet": "<b>Inter-rater</b> reliability checks the degree of <b>agreement</b> among raters (i.e., those completing items on an instrument). Common situations where more than one rater is involved may occur when more than one person conducts classroom observations, uses an observation protocol or scores an open-ended test, using a rubric or other standard protocol. Kappa statistics, correlation coefficients, and intra-class correlation (ICC) coefficient are some of the commonly reported measures of <b>inter-rater</b> ...", "dateLastCrawled": "2022-02-02T23:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Debugging a Crowdsourced Task with Low <b>Inter-Rater</b> <b>Agreement</b>", "url": "https://marc.najork.org/pdfs/jcdl2015.pdf", "isFamilyFriendly": true, "displayUrl": "https://marc.najork.org/pdfs/jcdl2015.pdf", "snippet": "high-<b>quality</b> social media content\u2014interesting tweets\u2014with the ultimate aim of building a classifier that would automatically curate Twitter content. We describe the effects of varying the genre and recency of the dataset, of testing the reliability of the workers, and of recruiting workers from different crowdsourcing platforms. We also examined the effect of redesigning the work itself, both to make it easier and to potentially improve <b>inter-rater</b> <b>agreement</b>. As a result of the debugging ...", "dateLastCrawled": "2021-11-01T16:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Inter-rater reliability of the Bereavement Risk Assessment Tool</b> ...", "url": "https://www.cambridge.org/core/journals/palliative-and-supportive-care/article/abs/interrater-reliability-of-the-bereavement-risk-assessment-tool/9C91D92389F182D07376E047B0C25267", "isFamilyFriendly": true, "displayUrl": "https://www.cambridge.org/core/journals/palliative-and-supportive-care/article/abs/...", "snippet": "<b>Inter-rater</b> <b>Agreement</b>. For the 31 BRAT items used in this study, values of Fleiss\u2019 kappa ranged from 0.05 to 0.97. Six items had kappa values less than 0.4 (slight to fair <b>agreement</b>) and the remaining 25 items (81%) had kappa values above 0.4 (moderate to almost perfect <b>agreement</b>) (Landis &amp; Koch, Reference Landis and Koch 1977).", "dateLastCrawled": "2022-01-25T00:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Comparing inter-rater agreement between classes</b> of raters - <b>Cross Validated</b>", "url": "https://stats.stackexchange.com/questions/132609/comparing-inter-rater-agreement-between-classes-of-raters", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/132609/comparing-<b>inter-rater</b>-<b>agreement</b>...", "snippet": "3) Sufficient <b>inter-rater</b> reliability for semi-experts is not adequate, of course, if they are not in <b>agreement</b> with the experts. Here you could do a statistical comparison of the two groups distributions and central tendency - if you are comfortable comparing means on ordinal data, you have sufficient observations, and the data look normal, use standard tests like a t-test or ANOVA. Otherwise a crosstab and $\\chi^2$ test may be more appropriate (just keep in mind sample size sensitivities ...", "dateLastCrawled": "2022-01-22T06:57:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "How to assess and compare <b>inter-rater</b> reliability, <b>agreement</b> and ...", "url": "https://www.frontiersin.org/articles/10.3389/fpsyg.2014.00509/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/fpsyg.2014.00509", "snippet": "On the example of <b>assessing</b> whether ELAN ratings <b>can</b> be reliably obtained from daycare teachers as well as from parents we show that rater <b>agreement</b>, linear correlation, and <b>inter-rater</b> reliability all have to be considered. Otherwise, an exhaustive conclusion about a rating scale&#39;s employability with different rater groups cannot be made. We also considered the factors gender and bilingualism of the evaluated child as potentially influencing the likelihood of rating <b>agreement</b>.", "dateLastCrawled": "2022-01-29T02:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "The Use of Bayesian Networks to Assess <b>the Quality</b> of Evidence from ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4696848/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC4696848", "snippet": "The main advantage to our approach is the potential to improve <b>inter-rater</b> <b>agreement</b> of GRADE assessments particularly when used by less experienced researchers, because such judgements <b>can</b> be complex and challenging to apply without training. This is the first study examining the <b>inter-rater</b> <b>agreement</b> of the SAQAT. Methods. We conducted two studies to compare: a) the <b>inter-rater</b> <b>agreement</b> of two researchers using the SAQAT independently on 28 meta-analyses and b) the <b>inter-rater</b> <b>agreement</b> ...", "dateLastCrawled": "2021-12-20T23:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Comparing inter-rater agreement between classes</b> of raters - <b>Cross Validated</b>", "url": "https://stats.stackexchange.com/questions/132609/comparing-inter-rater-agreement-between-classes-of-raters", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/132609/comparing-<b>inter-rater</b>-<b>agreement</b>...", "snippet": "3) Sufficient <b>inter-rater</b> reliability for semi-experts is not adequate, of course, if they are not in <b>agreement</b> with the experts. Here you could do a statistical comparison of the two groups distributions and central tendency - if you are comfortable comparing means on ordinal data, you have sufficient observations, and the data look normal, use standard tests like a t-test or ANOVA. Otherwise a crosstab and $\\chi^2$ test may be more appropriate (just keep in mind sample size sensitivities ...", "dateLastCrawled": "2022-01-22T06:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Reliability (<b>Inter-rater</b> <b>Agreement</b>) of the Barthel Index for Assessment ...", "url": "https://www.ahajournals.org/doi/10.1161/STROKEAHA.112.678615", "isFamilyFriendly": true, "displayUrl": "https://www.ahajournals.org/doi/10.1161/<b>STROKE</b>AHA.112.678615", "snippet": "We assessed study <b>quality</b> and risk of bias using a prespecified bespoke tool based on the Guidelines for Reporting Reliability and <b>Agreement</b> studies (GRRAS). 15 Important elements were: description of the study population (recruitment, demographics, <b>stroke</b> severity); description of the assessors (number, occupation, experience with functional assessment, or BI); description of time between first and second assessment; description of methodology of BI administration; description of training ...", "dateLastCrawled": "2022-01-10T03:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Assessing</b> <b>the quality</b> of economic evaluations of clinical nurse ...", "url": "https://www.sciencedirect.com/science/article/pii/S2352900815000047", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2352900815000047", "snippet": "<b>Inter-rater</b> <b>agreement</b> (prior to consensus process) was high (83% <b>agreement</b>). Four criteria for <b>the quality</b> of economic evaluations were consistently addressed: specification of clear, measurable objectives; pre-specification of subgroups for subgroup analyses; justified conclusions based on study results; and disclosure of study funding source. A clear statement of the primary outcome measures, incremental analysis, and assessment of uncertainty were often unclear or missing. Due to poor ...", "dateLastCrawled": "2021-12-09T14:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "INVESTIGATING <b>INTER-RATER</b> RELIABILITY OF QUALITATIVE TEXT ANNOTATIONS ...", "url": "https://erinmacd.stanford.edu/wp-content/uploads/2020/06/2020-Investigating-Inter-Rater-Reliability-of-Qualitative-Text-Annotations-in-Machine-Learning-Datasets_Dehaibi_Design-2020.pdf", "isFamilyFriendly": true, "displayUrl": "https://erinmacd.stanford.edu/wp-content/uploads/2020/06/2020-Investigating-Inter...", "snippet": "reliability of the annotations since <b>the quality</b> of the <b>model</b> depends on it. In machine learning research, a common way to evaluate the dataset is by looking at the evaluation metrics of the <b>model</b> such as precision, recall, F1 (Jurafsky and Martin, 2017). While these metrics <b>can</b> provide external validity for a <b>model</b>, they are not commonly used in the design research space. A more common approach for <b>assessing</b> reliability of annotator data in design research is <b>inter-rater</b> reliability (IRR ...", "dateLastCrawled": "2022-01-30T04:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A variance-based measure of <b>inter-rater</b> <b>agreement</b> in medical databases ...", "url": "https://www.sciencedirect.com/science/article/pii/S1532046403000364", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1532046403000364", "snippet": "The distinction between what one wants to measure and what one <b>can</b> reliably measure may be a trade-off between relevance and <b>quality</b> of the information, which reflects the usefulness of information. A coding scheme organized according to some aspect of disease that is difficult to measure or agree upon in the diagnostic process will produce unreliable data. Given two aggregation schemes that provide the same amount of information but with different levels of inter-coder <b>agreement</b>, choosing ...", "dateLastCrawled": "2021-10-28T17:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Can</b> we agree on <b>the quality</b> of clinical supervision? <b>Inter-rater</b> ...", "url": "https://www.cambridge.org/core/journals/the-cognitive-behaviour-therapist/article/can-we-agree-on-the-quality-of-clinical-supervision-interrater-reliability-of-the-shortsage-supervision-adherence-and-guidance-evaluation-scale/14F8CA92D870998BBEBB0F81C9D53F17", "isFamilyFriendly": true, "displayUrl": "https://www.cambridge.org/core/journals/the-cognitive-behaviour-therapist/article/<b>can</b>...", "snippet": "However, instruments for <b>assessing</b> CBT competence, such as CTS-R, have been able to prove adequate <b>inter-rater</b> reliability (Blackburn et al., Reference Blackburn, James, Milne, Baker, Standart, Garland and Reichelt 2001). In their study, Blackburn and colleagues had a total of 102 sessions coded by two out of four coders. The present study had a similar approach using fewer sessions (i.e., at least three coders coded each recorded session in a cross-over design). However, the restricted ...", "dateLastCrawled": "2022-02-01T12:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "The Use of Bayesian Networks to Assess <b>the Quality</b> of Evidence from ...", "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0123511", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0123511", "snippet": "The main advantage to our approach is the potential to improve <b>inter-rater</b> <b>agreement</b> of GRADE assessments particularly when used by less experienced researchers, because such judgements <b>can</b> be complex and challenging to apply without training. This is the first study examining the <b>inter-rater</b> <b>agreement</b> of the SAQAT. Methods We conducted two studies to compare: a) the <b>inter-rater</b> <b>agreement</b> of two researchers using the SAQAT independently on 28 meta-analyses and b) the <b>inter-rater</b> <b>agreement</b> ...", "dateLastCrawled": "2021-12-21T00:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "A Comparison of Andragogy and Pedagogy: <b>Assessing</b> the Relationship ...", "url": "https://trace.tennessee.edu/cgi/viewcontent.cgi?article=8141&context=utk_graddiss", "isFamilyFriendly": true, "displayUrl": "https://trace.tennessee.edu/cgi/viewcontent.cgi?article=8141&amp;context=utk_graddiss", "snippet": "<b>quality</b> of my life to a level I&#39;ve never before experienced. I am forever in her debt, and I hope that I <b>can</b> be as helpful, as loving, and as supportive to her, in everything that she does. Thanks also to my friends and associates at TV A. Cathy Hammond for making this project happen, the trainers who", "dateLastCrawled": "2022-01-29T19:10:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Understanding Interobserver <b>Agreement</b>: The Kappa Statistic", "url": "http://web2.cs.columbia.edu/~julia/courses/CS6998/Interrater_agreement.Kappa_statistic.pdf", "isFamilyFriendly": true, "displayUrl": "web2.cs.columbia.edu/~julia/courses/CS6998/<b>Interrater</b>_<b>agreement</b>.Kappa_statistic.pdf", "snippet": "call the <b>analogy</b> of a target and how close we get to the bull\u2019s-eye (Figure 1). If we actually hit the bull\u2019s-eye (representing <b>agreement</b> with the gold standard), we are accurate. If all our shots land together, we have good precision (good reliability). If all our shots land together and we hit the bull\u2019s-eye, we are accurate as well as precise. It is possible, however, to hit the bull\u2019s-eye purely by chance. Referring to Figure 1, only the center black dot in target A is accurate ...", "dateLastCrawled": "2022-01-28T23:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Leveraging Inter-rater Agreement for Audio-Visual Emotion Recognition</b>", "url": "https://www.researchgate.net/publication/283487589_Leveraging_Inter-rater_Agreement_for_Audio-Visual_Emotion_Recognition", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/283487589_Leveraging_<b>Inter-rater</b>_<b>Agreement</b>...", "snippet": "In <b>machine</b> <b>learning</b> tasks an actual \u2018ground truth\u2019 may not be available. Then, machines often have to rely on human labelling of data. This becomes challenging the more subjective the <b>learning</b> ...", "dateLastCrawled": "2021-08-28T14:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Multilingual <b>Twitter Sentiment Classification</b>: The Role of Human ... - PLOS", "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0155036", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0155036", "snippet": "The researchers in the fields of <b>inter-rater</b> <b>agreement</b> and <b>machine</b> <b>learning</b> typically employ different evaluation measures. We report all the results in terms of four selected measures which we deem appropriate for the three-valued sentiment classification task (the details are in the Evaluation measures subsection in Methods). In this section, however, the results are summarized only in terms of Krippendorff\u2019s Alpha-reliability Alpha) , to highlight the main conclusions. Alpha is a ...", "dateLastCrawled": "2021-03-30T23:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Machine</b> <b>learning</b> glossary - DataTime", "url": "https://www.dtalg.com/article-1/", "isFamilyFriendly": true, "displayUrl": "https://www.dtalg.com/article-1", "snippet": "See also Cohen\u2019s kappa, which is one of the most popular <b>inter-rater</b> <b>agreement</b> measurements. intersection over union (IoU) #image. The intersection of two sets divided by their union. In <b>machine</b>-<b>learning</b> image-detection tasks, IoU is used to measure the accuracy of the model\u2019s predicted bounding box with respect to the ground-truth bounding ...", "dateLastCrawled": "2022-01-25T17:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Analyzing and Interpreting Data From Rating Scales</b> | by Kevin C Lee ...", "url": "https://towardsdatascience.com/analyzing-and-interpreting-data-from-rating-scales-d169d66211db", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>analyzing-and-interpreting-data-from-rating-scales</b>-d169...", "snippet": "<b>Inter-Rater</b> Reliability. In B), we plot the pairwise correlations between the students with a heatmap. Most of the correlations are &gt; 0.6 with a few exceptions. A small number of respondents showing low correlations with others is acceptable as long as most students are able to respond similarly. P.S. The use of Pearson Correlation is only ...", "dateLastCrawled": "2022-01-29T00:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "The Art <b>and Science of Analyzing Software Data</b>", "url": "https://www.slideshare.net/timmenzies/the-art-and-science-of-analyzing-software-data", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/timmenzies/the-art-<b>and-science-of-analyzing-software-data</b>", "snippet": "Analyzing survey data \u2022 <b>Inter-rater</b> <b>agreement</b> \u2013 Coding is a subjective activity \u2013 Increase reliability by using multiple raters for entire data or a subset of the data \u2013 Cohen\u2019s Kappa or Fleiss\u2019 Kappa can be used to measure the <b>agreement</b> between multiple raters. \u2013 \u201cWe measured <b>inter-rater</b> <b>agreement</b> for the first author\u2019s categorization on a simple random sample of 100 cards with a closed card sort and two additional raters (third and fourth author); the Fleiss\u2019 Kappa ...", "dateLastCrawled": "2022-01-19T09:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Use of analogies, metaphors, and similes by students and reviewers at ...", "url": "https://www.cambridge.org/core/journals/ai-edam/article/use-of-analogies-metaphors-and-similes-by-students-and-reviewers-at-an-undergraduate-architectural-design-review/FB80EB57099A898FE15564497D5B06C7", "isFamilyFriendly": true, "displayUrl": "https://www.cambridge.org/core/journals/ai-edam/article/use-of-analogies-metaphors-and...", "snippet": "We used the Delphi Method to determine the <b>inter-rater</b> <b>agreement</b>. In the first step after the second round of discussion, there was 66.67% <b>agreement</b> between the authors\u2019 coding and that of the independent coder. In the second step, <b>agreement</b> on the type of similarities was determined using the Delphi Method. At the end of second round of discussions, there was 90.1% <b>agreement</b>. Table 1. Categories and sub-categories used for coding the reviews. Any statement which explicitly or implicitly ...", "dateLastCrawled": "2022-02-02T16:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Target <b>analogy</b> of accuracy and precision | Download Scientific Diagram", "url": "https://researchgate.net/figure/Target-analogy-of-accuracy-and-precision_fig1_24399044", "isFamilyFriendly": true, "displayUrl": "https://researchgate.net/figure/Target-<b>analogy</b>-of-accuracy-and-precision_fig1_24399044", "snippet": "The intraclass correlation coefficient (ICC) was calculated to assess intra-rater and <b>inter-rater</b> <b>agreement</b> of I 3M . 31 A sample of OPTs was randomly divided into training dataset (819) and test ...", "dateLastCrawled": "2021-06-28T05:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "ENH/FAQ: Fleiss Kappa giving nan results, randolph&#39;s kappa \u00b7 Issue ...", "url": "https://github.com/statsmodels/statsmodels/issues/4387", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/statsmodels/statsmodels/issues/4387", "snippet": "I&#39;m trying to use Fleiss Kappa to calculate the <b>agreement</b> of several raters. First I have an array of two subjects and 7 raters per subject. There are two categories, however, the second category is never chosen. ratings = [[0,0,0,0,0,0,...", "dateLastCrawled": "2022-01-12T20:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Quadratic weighted kappa</b> strength of <b>agreement</b> - Cross Validated", "url": "https://stats.stackexchange.com/questions/46296/quadratic-weighted-kappa-strength-of-agreement", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/46296", "snippet": "In the case of the kappa-value there are some attempts to qualify how good or bad the agreements are. For example Landis &amp; Koch in the article The Measurement of Observer <b>Agreement</b> for Categorical Data talks about &quot;strength of <b>agreement</b>&quot; based on kappa values:. Kappa Strength of <b>agreement</b> ===== ===== 0.0-0.20 Slight 0.21-0.40 Fair 0.41-0.60 Moderate 0.61-0.80 Substantial 0.81-0.90 Almost perfect", "dateLastCrawled": "2022-01-20T17:56:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Reliability and Learnability of Human Bandit Feedback for Sequence-to ...", "url": "https://aclanthology.org/P18-1165.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/P18-1165.pdf", "snippet": "intra- and <b>inter-rater agreement is similar</b> for both tasks, with highest inter-rater reliability for stan-dardized 5-point ratings. In a next step, we address the issue of <b>machine</b> learnability of human rewards. We use deep learn- ing models to train reward estimators by regres-sion against cardinal feedback, and by \ufb01tting a Bradley-Terry model (Bradley and Terry,1952) to ordinal feedback. Learnability is understood by a slight misuse of the <b>machine</b> <b>learning</b> notion of learnability (Shalev ...", "dateLastCrawled": "2021-12-22T00:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "arXiv:1805.10627v3 [cs.CL] 13 Dec 2018", "url": "https://www.researchgate.net/profile/Joshua-Uyheng/publication/325413588_Reliability_and_Learnability_of_Human_Bandit_Feedback_for_Sequence-to-Sequence_Reinforcement_Learning/links/5ea04de5a6fdccd7cee0eebe/Reliability-and-Learnability-of-Human-Bandit-Feedback-for-Sequence-to-Sequence-Reinforcement-Learning.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/profile/Joshua-Uyheng/publication/325413588_Reliability...", "snippet": "\ufb01ed by bandit <b>learning</b> for neural <b>machine</b> trans-lation (NMT). Our aim is to show that successful <b>learning</b> from simulated bandit feedback (Sokolov et al.,2016b;Kreutzer et al.,2017;Nguyen et al ...", "dateLastCrawled": "2021-08-22T12:54:00.0000000Z", "language": "en", "isNavigational": false}], [], [], []], "all_bing_queries": ["+(inter-rater agreement)  is like +(assessing the quality of a model)", "+(inter-rater agreement) is similar to +(assessing the quality of a model)", "+(inter-rater agreement) can be thought of as +(assessing the quality of a model)", "+(inter-rater agreement) can be compared to +(assessing the quality of a model)", "machine learning +(inter-rater agreement AND analogy)", "machine learning +(\"inter-rater agreement is like\")", "machine learning +(\"inter-rater agreement is similar\")", "machine learning +(\"just as inter-rater agreement\")", "machine learning +(\"inter-rater agreement can be thought of as\")", "machine learning +(\"inter-rater agreement can be compared to\")"]}