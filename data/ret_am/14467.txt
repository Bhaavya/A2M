{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "BioBERT: a pre-trained biomedical <b>language</b> representation <b>model</b> for ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7703786/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7703786", "snippet": "BERT (Devlin et al., 2019) is a contextualized word representation <b>model</b> that is based on a masked <b>language</b> <b>model</b> and pre-trained using bidirectional transformers (Vaswani et al., 2017). Due to the nature of <b>language</b> modeling where future words cannot be seen, previous <b>language</b> models were limited to a combination of two <b>unidirectional</b> <b>language</b> models (i.e. left-to-right and right-to-left). BERT uses a masked <b>language</b> <b>model</b> that predicts randomly masked words in a sequence, and hence can be ...", "dateLastCrawled": "2022-02-02T22:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Facts2Story: Controlling Text Generation by Key Facts", "url": "https://aclanthology.org/2020.coling-main.211.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/2020.coling-main.211.pdf", "snippet": "a <b>unidirectional</b> auto regressive <b>language</b> <b>model</b> on a concatenated sequence of the metadata and the body, the <b>model</b> is capable of generating an appropriate article to a headline not seen before. Keskar et al. (2019) <b>train</b> a very large scale (1.63B Parameters) <b>unidirectional</b> auto regressive <b>language</b> <b>model</b>", "dateLastCrawled": "2022-02-02T15:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Universal Language Model Fine-tuning for Text Classification</b>", "url": "https://www.researchgate.net/publication/334116365_Universal_Language_Model_Fine-tuning_for_Text_Classification", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/334116365_Universal_<b>Language</b>_<b>Model</b>_Fine...", "snippet": "<b>Language</b> adaptive fine-tuning (LAFT) is an effective method of adapting PLMs to a new <b>language</b> by finetuning PLMs MLM on unlabeled texts in the new <b>language</b> (Pfeiffer et al., 2020). The approach ...", "dateLastCrawled": "2022-02-02T23:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Multimodal sentiment analysis with <b>unidirectional</b> modality translation ...", "url": "https://www.sciencedirect.com/science/article/pii/S0925231221013990", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0925231221013990", "snippet": "Given only <b>language</b> modality, it is difficult to determine the affective orientation of the utterance. However, our proposed MTSA <b>model</b> predicts the sentiment as 1.109, which is close to the label. The same trend appears in the third example. These facts strongly suggest that: 1) Visual and audio modalities play an important role in multimodal ...", "dateLastCrawled": "2022-01-26T12:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "How to Implement A Singly Linked List in Data Structures", "url": "https://www.simplilearn.com/tutorials/data-structure-tutorial/singly-linked-list", "isFamilyFriendly": true, "displayUrl": "https://www.simplilearn.com/tutorials/data-structure-tutorial/singly-linked-list", "snippet": "A singly linked list <b>is like</b> a <b>train</b> system, where it connects each bogie to the next bogie. A singly linked list is a <b>unidirectional</b> linked list; i.e., you can only traverse it from head node to tail node. It is used to do a slideshow or some basic operations on a notepad <b>like</b> undo and redo. How to Implement a Singly Linked List? You can create nodes of singly linked lists using classes or structures. And you link them using the next pointer. Code: // implementation of singly linked list. # ...", "dateLastCrawled": "2022-02-02T22:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Fine-Tuning Transformer-Based <b>Language</b> Models", "url": "https://ymeadows.com/en-articles/fine-tuning-transformer-based-language-models", "isFamilyFriendly": true, "displayUrl": "https://ymeadows.com/en-articles/fine-tuning-transformer-based-<b>language</b>-<b>models</b>", "snippet": "In addition to fine-tuning a pre-trained <b>language</b> <b>model</b>, one can pre-<b>train</b> a domain specific <b>language</b> <b>model</b> with the BERT architecture using just the domain data. This approach requires a large amount of data and an extensive training procedure. For a given domain, intuitively, pre-training should perform better than a fine-tuned BERT <b>model</b> since it will learn the style of a specific domain, rather than just general English. To exemplify this phenomena, consider a customer service <b>language</b> ...", "dateLastCrawled": "2022-01-30T03:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Offensive <b>Language</b> and Hate Speech Detection with Deep Learning and ...", "url": "https://deepai.org/publication/offensive-language-and-hate-speech-detection-with-deep-learning-and-transfer-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/offensive-<b>language</b>-and-hate-speech-detection-with-deep...", "snippet": "Next, we introduce a transfer learning approach for hate speech detection using an existing pre-trained <b>language</b> <b>model</b> BERT (Bidirectional Encoder Representations from Transformers), DistilBert (Distilled version of BERT) and GPT-2 (Generative Pre-Training). We perform hyper parameters tuning analysis of our best <b>model</b> (BI-LSTM) considering different neural network architectures, learn-ratings and normalization methods etc. After tuning the <b>model</b> and with the best combination of parameters ...", "dateLastCrawled": "2022-01-27T02:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Domain-specific language model pretraining</b> for biomedical natural ...", "url": "https://www.microsoft.com/en-us/research/blog/domain-specific-language-model-pretraining-for-biomedical-natural-language-processing/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.microsoft.com</b>/en-us/research/blog/domain-specific-<b>language</b>-<b>model</b>-p...", "snippet": "It is impossible to keep <b>track</b> of such rapid progress by manual efforts alone. In the era of big data and precision medicine, the urgency has never been higher to advance natural <b>language</b> processing (NLP) methods that can help scientists stay versed in the deluge of information. NLP can help researchers quickly identify and cross-reference important findings in papers that are both directly and tangentially related to their own research at a large scale\u2014instead of researchers having to ...", "dateLastCrawled": "2022-01-28T03:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Difference between react vs angular \u00bb Coresumo", "url": "https://coresumo.com/react-vs-angular/", "isFamilyFriendly": true, "displayUrl": "https://coresumo.com/react-vs-angular", "snippet": "Testing in stoutly compartmented <b>language</b> <b>like</b> Javascript makes it delicate to achieve 100% testing pretensions. That\u2019s the reason why law written in Javascript has to come up with a strong set of tests. Angular have multiple features similar as insulation of the unit of law and is written with all the enterprises regarding testability in mind react vs angular.", "dateLastCrawled": "2022-01-30T15:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Why are <b>neural machine translation systems unidirectional while humans</b> ...", "url": "https://www.quora.com/Why-are-neural-machine-translation-systems-unidirectional-while-humans-can-translate-languages-both-ways", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Why-are-<b>neural-machine-translation-systems-unidirectional-while</b>...", "snippet": "Answer (1 of 3): I disagree with your premise, actually. I know a bit of French. If you give me a dictionary, I\u2019d probably be 95% successful in translating French into English. If you asked me to translate English into French, anything beyond trivial would be bad French, or at least very awkward ...", "dateLastCrawled": "2021-12-24T00:40:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Universal Language Model Fine-tuning for Text Classification</b>", "url": "https://www.researchgate.net/publication/334116365_Universal_Language_Model_Fine-tuning_for_Text_Classification", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/334116365_Universal_<b>Language</b>_<b>Model</b>_Fine...", "snippet": "Bidirectional <b>language</b> <b>model</b> <b>Similar</b> to ... limited to \ufb01ne-tuning a <b>unidirectional</b> <b>language</b>. <b>model</b>. F or all our experiments, we pretrain both a. forward and a backward LM. W e \ufb01ne-tune a clas ...", "dateLastCrawled": "2022-02-02T23:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "BioBERT: a pre-trained biomedical <b>language</b> representation <b>model</b> for ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7703786/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7703786", "snippet": "BERT (Devlin et al., 2019) is a contextualized word representation <b>model</b> that is based on a masked <b>language</b> <b>model</b> and pre-trained using bidirectional transformers (Vaswani et al., 2017). Due to the nature of <b>language</b> modeling where future words cannot be seen, previous <b>language</b> models were limited to a combination of two <b>unidirectional</b> <b>language</b> models (i.e. left-to-right and right-to-left). BERT uses a masked <b>language</b> <b>model</b> that predicts randomly masked words in a sequence, and hence can be ...", "dateLastCrawled": "2022-02-02T22:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "BPTT for Text Classification BPT3C <b>Lan guage</b> models are trained with ...", "url": "https://www.coursehero.com/file/p2d3ctmo/BPTT-for-Text-Classification-BPT3C-Lan-guage-models-are-trained-with/", "isFamilyFriendly": true, "displayUrl": "https://www.coursehero.com/file/p2d3ctmo/BPTT-for-Text-Classification-BPT3C-<b>Lan-guage</b>...", "snippet": "Bidirectional <b>language</b> <b>model</b> <b>Similar</b> to exist-ing work (Peters et al., 2017, 2018), we are not limited to fine-tuning a <b>unidirectional</b> <b>language</b> <b>model</b>. For all our experiments, we pretrain both a forward and a backward LM. We fine-tune a clas-sifier for each LM independently using BPT3C and average the classifier predictions. 4 Experiments While our approach is equally applicable to se-quence labeling tasks, we focus on text classifica-tion tasks in this work due to their important real-world ...", "dateLastCrawled": "2022-01-20T03:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) Fine-tuned <b>Language</b> Models for Text Classification", "url": "https://www.researchgate.net/publication/322592284_Fine-tuned_Language_Models_for_Text_Classification", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/322592284_Fine-tuned_<b>Language</b>_<b>Models</b>_for_Text...", "snippet": "Bidirectional <b>language</b> <b>model</b> <b>Similar</b> to exist - ... transfer learning is being able <b>to train</b> a <b>model</b> for. Pretraining IMDb TREC-6 AG. Without pretraining 5.63 10.67 5.52. With pretraining 5.00 5 ...", "dateLastCrawled": "2021-12-16T06:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Multimodal sentiment analysis with <b>unidirectional</b> modality translation ...", "url": "https://www.sciencedirect.com/science/article/pii/S0925231221013990", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0925231221013990", "snippet": "Given only <b>language</b> modality, it is difficult to determine the affective orientation of the utterance. However, our proposed MTSA <b>model</b> predicts the sentiment as 1.109, which is close to the label. The same trend appears in the third example. These facts strongly suggest that: 1) Visual and audio modalities play an important role in multimodal ...", "dateLastCrawled": "2022-01-26T12:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Fine-Tuning Transformer-Based <b>Language</b> Models", "url": "https://ymeadows.com/en-articles/fine-tuning-transformer-based-language-models", "isFamilyFriendly": true, "displayUrl": "https://ymeadows.com/en-articles/fine-tuning-transformer-based-<b>language</b>-<b>models</b>", "snippet": "In addition to fine-tuning a pre-trained <b>language</b> <b>model</b>, one can pre-<b>train</b> a domain specific <b>language</b> <b>model</b> with the BERT architecture using just the domain data. This approach requires a large amount of data and an extensive training procedure. For a given domain, intuitively, pre-training should perform better than a fine-tuned BERT <b>model</b> since it will learn the style of a specific domain, rather than just general English. To exemplify this phenomena, consider a customer service <b>language</b> ...", "dateLastCrawled": "2022-01-30T03:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>BioBERT</b>: a pre-trained biomedical <b>language</b> representation <b>model</b> for ...", "url": "https://academic.oup.com/bioinformatics/article/36/4/1234/5566506", "isFamilyFriendly": true, "displayUrl": "https://academic.oup.com/bioinformatics/article/36/4/1234/5566506", "snippet": "BERT (Devlin et al., 2019) is a contextualized word representation <b>model</b> that is based on a masked <b>language</b> <b>model</b> and pre-trained using bidirectional transformers (Vaswani et al., 2017). Due to the nature of <b>language</b> modeling where future words cannot be seen, previous <b>language</b> models were limited to a combination of two <b>unidirectional</b> <b>language</b> models (i.e. left-to-right and right-to-left). BERT uses a masked <b>language</b> <b>model</b> that predicts randomly masked words in a sequence, and hence can be ...", "dateLastCrawled": "2022-01-19T12:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Difference between react vs angular \u00bb Coresumo", "url": "https://coresumo.com/react-vs-angular/", "isFamilyFriendly": true, "displayUrl": "https://coresumo.com/react-vs-angular", "snippet": "Further, with React, the data- list process is <b>unidirectional</b>, meaning that tapes aren\u2019t assigned watchers, and accordingly, the workload is reduced. The same doesn\u2019t be with Angular. Since it has a bidirectional data list process, each list requires a watcher to <b>track</b> variations, and each circle remains until all the watchers are verified ...", "dateLastCrawled": "2022-01-30T15:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Offensive <b>Language</b> and Hate Speech Detection with Deep Learning and ...", "url": "https://deepai.org/publication/offensive-language-and-hate-speech-detection-with-deep-learning-and-transfer-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/offensive-<b>language</b>-and-hate-speech-detection-with-deep...", "snippet": "More than 2/3 of the tweets were classified as class 1. This implies that the data is imbalanced. Furthermore, we noticed that the words used frequently in class 0 and class 1 are <b>similar</b>, as shown below, which makes it even more challenging <b>to train</b> the <b>model</b> to accurately classify tweets as class 0 or class 1. The most commonly used words in ...", "dateLastCrawled": "2022-01-27T02:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>skip-thoughts</b> \u00b7 PyPI", "url": "https://pypi.org/project/skip-thoughts/", "isFamilyFriendly": true, "displayUrl": "https://pypi.org/project/<b>skip-thoughts</b>", "snippet": "<b>Model</b> overview. The <b>Skip-Thoughts</b> <b>model</b> is a sentence encoder. It learns to encode input sentences into a fixed-dimensional vector representation that is useful for many tasks, for example to detect paraphrases or to classify whether a product review is positive or negative. See the Skip-Thought Vectors paper for details of the <b>model</b> ...", "dateLastCrawled": "2022-02-01T15:12:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Universal Language Model Fine-tuning for Text Classification</b>", "url": "https://www.researchgate.net/publication/334116365_Universal_Language_Model_Fine-tuning_for_Text_Classification", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/334116365_Universal_<b>Language</b>_<b>Model</b>_Fine...", "snippet": "<b>Language</b> adaptive fine-tuning (LAFT) is an effective method of adapting PLMs to a new <b>language</b> by finetuning PLMs MLM on unlabeled texts in the new <b>language</b> (Pfeiffer et al., 2020). The approach ...", "dateLastCrawled": "2022-02-02T23:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "BPTT for Text Classification BPT3C <b>Lan guage</b> models are trained with ...", "url": "https://www.coursehero.com/file/p2d3ctmo/BPTT-for-Text-Classification-BPT3C-Lan-guage-models-are-trained-with/", "isFamilyFriendly": true, "displayUrl": "https://www.coursehero.com/file/p2d3ctmo/BPTT-for-Text-Classification-BPT3C-<b>Lan-guage</b>...", "snippet": "Bidirectional <b>language</b> <b>model</b> Similar to exist-ing work (Peters et al., 2017, 2018), we are not limited to fine-tuning a <b>unidirectional</b> <b>language</b> <b>model</b>. For all our experiments, we pretrain both a forward and a backward LM. We fine-tune a clas-sifier for each LM independently using BPT3C and average the classifier predictions. 4 Experiments While our approach is equally applicable to se-quence labeling tasks, we focus on text classifica-tion tasks in this work due to their important real-world ...", "dateLastCrawled": "2022-01-20T03:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) Universal <b>Language</b> <b>Model</b> Fine-tuning for Text Classification ...", "url": "https://www.academia.edu/40034541/Universal_Language_Model_Fine_tuning_for_Text_Classification", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/40034541/Universal_<b>Language</b>_<b>Model</b>_Fine_tuning_for_Text...", "snippet": "Universal <b>Language</b> <b>Model</b> Fine-tuning for Text Classification. Download. Universal <b>Language</b> <b>Model</b> Fine-tuning for Text Classification. Erkan Altan. Sebastian Ruder. Erkan Altan. Sebastian Ruder. Related Papers. A data-driven neural network architecture for sentiment analysis. By Erion \u00c7ano. M A N N I N G. By Mihaela Iftene. Deep learning with python. By Mohamed Elhwary. Learning Representations of NLT.pdf. By Aggeliki Vlachostergiou, Ph.D. Shakespearizing Modern <b>Language</b> Using Copy-Enriched ...", "dateLastCrawled": "2021-05-06T05:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) Fine-tuned <b>Language</b> Models for Text Classification", "url": "https://www.researchgate.net/publication/322592284_Fine-tuned_Language_Models_for_Text_Classification", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/322592284_Fine-tuned_<b>Language</b>_<b>Models</b>_for_Text...", "snippet": "limited to \ufb01ne-tuning a <b>unidirectional</b> <b>language</b>. <b>model</b>. F or all our experiments, we pretrain both a. forward and a backward LM. W e \ufb01ne-tune a clas-si\ufb01er for each LM independently using ...", "dateLastCrawled": "2021-12-16T06:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Offensive <b>Language</b> and Hate Speech Detection with Deep Learning and ...", "url": "https://deepai.org/publication/offensive-language-and-hate-speech-detection-with-deep-learning-and-transfer-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/offensive-<b>language</b>-and-hate-speech-detection-with-deep...", "snippet": "Next, we introduce a transfer learning approach for hate speech detection using an existing pre-trained <b>language</b> <b>model</b> BERT (Bidirectional Encoder Representations from Transformers), DistilBert (Distilled version of BERT) and GPT-2 (Generative Pre-Training). We perform hyper parameters tuning analysis of our best <b>model</b> (BI-LSTM) considering different neural network architectures, learn-ratings and normalization methods etc. After tuning the <b>model</b> and with the best combination of parameters ...", "dateLastCrawled": "2022-01-27T02:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>skip-thoughts</b> \u00b7 PyPI", "url": "https://pypi.org/project/skip-thoughts/", "isFamilyFriendly": true, "displayUrl": "https://pypi.org/project/<b>skip-thoughts</b>", "snippet": "<b>Model</b> overview. The <b>Skip-Thoughts</b> <b>model</b> is a sentence encoder. It learns to encode input sentences into a fixed-dimensional vector representation that is useful for many tasks, for example to detect paraphrases or to classify whether a product review is positive or negative. See the Skip-<b>Thought</b> Vectors paper for details of the <b>model</b> ...", "dateLastCrawled": "2022-02-01T15:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Understanding Transformers, the Data Science</b> Way - KDnuggets", "url": "https://www.kdnuggets.com/2020/10/understanding-transformers-data-science-way.html", "isFamilyFriendly": true, "displayUrl": "https://www.kdnuggets.com/2020/10/understanding-transformers-data-science-way.html", "snippet": "It <b>can</b> be used for <b>language</b> modeling, Translation, or Classification as required, and it does it fast by removing the sequential nature of the problem. So, the transformer in a machine translation application would convert one <b>language</b> to another, or for a classification problem will provide the class probability using an appropriate output layer.", "dateLastCrawled": "2022-01-29T20:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>GitHub</b> - <b>RetroRabbit/skip_thoughts</b>: Installable package for skip ...", "url": "https://github.com/RetroRabbit/skip_thoughts", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/RetroRabbit/skip_<b>thoughts</b>", "snippet": "<b>Track</b> Training Progress. Optionally, you <b>can</b> run the <b>track</b>_perplexity script in a separate process. This will log per-word perplexity on the validation set which allows training progress to be monitored on TensorBoard. Note that you may run out of memory if you run the this script on the same GPU as the training script. You <b>can</b> set the environment variable CUDA_VISIBLE_DEVICES=&quot;&quot; to force the script to run on CPU. If it runs too slowly on CPU, you <b>can</b> decrease the value of --num_eval ...", "dateLastCrawled": "2021-11-02T03:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "NS/ The brain has a \u201ccompass\u201d for navigating thoughts | by Paradigm ...", "url": "https://medium.com/paradigm-fund/ns-the-brain-has-a-compass-for-navigating-thoughts-18c9247688ff", "isFamilyFriendly": true, "displayUrl": "https://medium.com/paradigm-fund/ns-the-brain-has-a-compass-for-navigating-<b>thoughts</b>-18...", "snippet": "The global neuroscience market size was valued at USD 28.4 billion in 2016 and it is expected to reach USD 38.9 billion by 2027. Some time ago, the research team provided experimental evidence that\u2026", "dateLastCrawled": "2022-01-19T02:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Is BERT a revolutionary approach for the NLP</b>? - Quora", "url": "https://www.quora.com/Is-BERT-a-revolutionary-approach-for-the-NLP", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>Is-BERT-a-revolutionary-approach-for-the-NLP</b>", "snippet": "Answer (1 of 3): BERT brought transfer learning to NLP world. You <b>can</b> pre-<b>train</b> a general purpose <b>model</b> on a large body of <b>language</b> corpus and then fine tune that to build task specific models (like Named entity, sequence classification, etc.). You <b>can</b> achieve stunning results with very little tr...", "dateLastCrawled": "2022-01-14T16:29:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "BioBERT: a pre-trained biomedical <b>language</b> representation <b>model</b> for ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7703786/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7703786", "snippet": "BERT (Devlin et al., 2019) is a contextualized word representation <b>model</b> that is based on a masked <b>language</b> <b>model</b> and pre-trained using bidirectional transformers (Vaswani et al., 2017). Due to the nature of <b>language</b> modeling where future words cannot be seen, previous <b>language</b> models were limited to a combination of two <b>unidirectional</b> <b>language</b> models (i.e. left-to-right and right-to-left). BERT uses a masked <b>language</b> <b>model</b> that predicts randomly masked words in a sequence, and hence <b>can</b> be ...", "dateLastCrawled": "2022-02-02T22:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Sentence Representation</b> | SpringerLink", "url": "https://link.springer.com/chapter/10.1007%2F978-981-15-5573-2_4", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/chapter/10.1007/978-981-15-5573-2_4", "snippet": "<b>Compared</b> with the standard <b>unidirectional</b> conditional <b>language</b> <b>model</b>, which <b>can</b> only be trained in one direction, MLM aims <b>to train</b> a deep bidirectional representation <b>model</b>. This task is inspired by Cloze . (2) The objective of NSP is to capture relationships between sentences for some sentence-based downstream tasks such as natural <b>language</b> inference (NLI) and question answering (QA). In this task, a binary classifier is trained to predict whether the sentence is the next sentence for the ...", "dateLastCrawled": "2021-12-21T11:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Offensive <b>Language</b> and Hate Speech Detection with Deep Learning and ...", "url": "https://deepai.org/publication/offensive-language-and-hate-speech-detection-with-deep-learning-and-transfer-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/offensive-<b>language</b>-and-hate-speech-detection-with-deep...", "snippet": "Next, we introduce a transfer learning approach for hate speech detection using an existing pre-trained <b>language</b> <b>model</b> BERT (Bidirectional Encoder Representations from Transformers), DistilBert (Distilled version of BERT) and GPT-2 (Generative Pre-Training). We perform hyper parameters tuning analysis of our best <b>model</b> (BI-LSTM) considering different neural network architectures, learn-ratings and normalization methods etc. After tuning the <b>model</b> and with the best combination of parameters ...", "dateLastCrawled": "2022-01-27T02:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Domain-specific language model pretraining</b> for biomedical natural ...", "url": "https://www.microsoft.com/en-us/research/blog/domain-specific-language-model-pretraining-for-biomedical-natural-language-processing/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.microsoft.com</b>/en-us/research/blog/domain-specific-<b>language</b>-<b>model</b>-p...", "snippet": "Pretrained neural <b>language</b> models are the underpinning of state-of-the-art NLP methods. Pretraining works by masking some words from text and training a <b>language</b> <b>model</b> to predict them from the rest. Then, the pre-trained <b>model</b> <b>can</b> be fine-tuned for various downstream tasks using task-specific training data. As in mainstream NLP, prior work on ...", "dateLastCrawled": "2022-01-28T03:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Pre-<b>training, Transformers, and Bi-directionality</b> - KDnuggets", "url": "https://www.kdnuggets.com/2019/07/pre-training-transformers-bi-directionality.html", "isFamilyFriendly": true, "displayUrl": "https://www.kdnuggets.com/2019/07/pre-<b>train</b>ing-transformers-bi-directionality.html", "snippet": "A pre-trained <b>language</b> <b>model</b> <b>can</b> be applied to downstream tasks using two approaches: a feature-based approach or a fine-tuning approach. The feature-based approach uses the learned embeddings of the pre-trained <b>model</b> as a feature in the training of the downstream task. In contrast, the fine-tuning approach (which BERT focuses on) re-trains the pre-trained <b>model</b> on that downstream task, using a minimal number of task-specific parameters. In any case, the more the <b>model</b> <b>can</b> generalize to ...", "dateLastCrawled": "2022-01-30T22:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Pre-Trained Models: Past, Present and Future - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S2666651021000231", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2666651021000231", "snippet": "(Wang et al. 2021) pre-<b>train</b> models based on the descriptions of Wikidata entities, by incorporating a <b>language</b> <b>model</b> loss and a knowledge embedding loss together to get knowledge-enhanced representations. Some works regard the paths and even sub-graphs in knowledge graphs as a whole, and directly <b>model</b> them and the aligned text to retain more structural information. Since aligning entities and relations to raw text is often troublesome and <b>can</b> introduce noise in data pre-processing, another ...", "dateLastCrawled": "2022-01-27T11:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Fine-Tuning Transformer-Based <b>Language</b> Models", "url": "https://ymeadows.com/en-articles/fine-tuning-transformer-based-language-models", "isFamilyFriendly": true, "displayUrl": "https://ymeadows.com/en-articles/fine-tuning-transformer-based-<b>language</b>-<b>models</b>", "snippet": "We have observed that the results -<b>compared</b> to the baseline <b>model</b>- <b>can</b> be improved between 5 to 10% depending on the layer freezing scheme selected. In addition to freezing and unfreezing certain layers for fine-tuning, one <b>can</b> apply different learning rates to each layer, as opposed to applying one learning rate to the entire <b>model</b>. The idea behind layer-wise learning rate is to treat different layers separately because each layer captures a different aspect of domain <b>language</b> and supports ...", "dateLastCrawled": "2022-01-30T03:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Seq2SQL: Generating Structured <b>Queries from Natural Language using</b> ...", "url": "https://deepai.org/publication/seq2sql-generating-structured-queries-from-natural-language-using-reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/seq2sql-generating-structured-queries-from-natural...", "snippet": "Our <b>model</b> leverages the structure of SQL queries to reduce the output space of the <b>model</b>. <b>To train</b> Seq2SQL, we applied in-the-loop query execution to learn a policy for generating the conditions of the SQL query, which is unordered and unsuitable for optimization via cross entropy loss. We also introduced WikiSQL, a dataset of questions and SQL queries that is an order of magnitude larger than comparable datasets. Finally, we showed that Seq2SQL outperforms a state-of-the-art semantic parser ...", "dateLastCrawled": "2022-01-28T07:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Hybrid multi-document summarization using pre-trained <b>language</b> models ...", "url": "https://www.sciencedirect.com/science/article/pii/S0957417421015979", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0957417421015979", "snippet": "The key idea behind developing them is <b>to train</b> a large <b>model</b> with a large-scale unlabeled dataset and then use it in various NLP tasks, such as text classification and question answering. BERT and XLNet use encoders only, while BART and T5 have both encoder and decoder components. The former is applicable for classification tasks, while the latter is appropriate for generation tasks. It is important to note that these pre-trained <b>language</b> models have achieved state-of-the-art results on ...", "dateLastCrawled": "2022-01-09T09:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Three-dimensional <b>train</b>\u2013<b>track</b> <b>model</b> for study of rail corrugation ...", "url": "https://www.researchgate.net/publication/223740075_Three-dimensional_train-track_model_for_study_of_rail_corrugation", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/223740075_Three-dimensional_<b>train</b>-<b>track</b>_<b>model</b>...", "snippet": "The proposed <b>model</b> is finally applied to investigate the difference between dynamic performances obtained using the entire-<b>train</b>/<b>track</b> <b>model</b> (TTM) and the single-vehicle/<b>track</b> <b>model</b> (VTM). Several ...", "dateLastCrawled": "2022-01-08T19:13:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine</b> <b>Learning</b> Glossary: <b>Language</b> Evaluation | Google Developers", "url": "https://developers.google.com/machine-learning/glossary/language", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine</b>-<b>learning</b>/glossary/<b>language</b>", "snippet": "A term used to describe a system that evaluates the text that both precedes and follows a target section of text. In contrast, a <b>unidirectional</b> system only evaluates the text that precedes a target section of text. For example, consider a masked <b>language</b> <b>model</b> that must determine probabilities for the word(s) representing the underline in the following question:. What is the _____ with you? A <b>unidirectional</b> <b>language</b> <b>model</b> would have to base its probabilities only on the context provided by ...", "dateLastCrawled": "2022-01-29T02:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Word Acquisition in Neural <b>Language</b> Models | Transactions of the ...", "url": "https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00444/109271/Word-Acquisition-in-Neural-Language-Models", "isFamilyFriendly": true, "displayUrl": "https://direct.mit.edu/.../tacl_a_00444/109271/Word-Acquisition-in-Neural-<b>Language</b>-<b>Models</b>", "snippet": "A quadratic <b>model</b> of log-frequency also provided a slightly better fit for <b>unidirectional</b> <b>language</b> models (R 2 = 0.93 to 0.94), particularly for high-frequency words; in <b>language</b> models, this could be due either to a floor effect on age of acquisition for high-frequency words or to slower <b>learning</b> of function words. Regardless, significant effects of other predictors remained the same when using a quadratic <b>model</b> for log-frequency.", "dateLastCrawled": "2022-02-02T10:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Fine-tuned <b>Language Models for Text Classification</b> | DeepAI", "url": "https://deepai.org/publication/fine-tuned-language-models-for-text-classification", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/fine-tuned-<b>language-models-for-text-classification</b>", "snippet": "In <b>analogy</b>, a hypercolumn for a word or sentence in NLP is the concatenation of embeddings at different layers in a pretrained <b>model</b>. and is used by peters2017semi, deepcontext2017, Wieting2017, Conneau2017, and Mccann2017 who use <b>language</b> modeling, paraphrasing, entailment, and <b>Machine</b> Translation (MT) respectively for pretraining. Specifically, deepcontext2017 require engineered custom architectures, while we show state-of-the-art performance with the same basic architecture across a range ...", "dateLastCrawled": "2021-12-23T06:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) <b>Machine learning, artificial neural networks and social</b> research", "url": "https://www.researchgate.net/publication/344171463_Machine_learning_artificial_neural_networks_and_social_research", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/344171463_<b>Machine</b>_<b>learning</b>_artificial_neural...", "snippet": "<b>Machine</b> <b>Learning</b> (ML) is an automatic <b>learning</b> process in which data sets are processed (Di Franco and Santurro, 2020). An ML system learns directly from the data and learns to connect one or more ...", "dateLastCrawled": "2022-02-02T23:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "[<b>MCQ&#39;s] Machine Learning - Last Moment Tuitions</b>", "url": "https://lastmomenttuitions.com/mcqs-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://lastmomenttuitions.com/<b>mcqs-machine-learning</b>", "snippet": "13. Newton\u2019s method is seldom used in <b>machine</b> <b>learning</b> because a. common loss functions are not self-concordant b. Newton\u2019s method does not work well on noisy data c. <b>machine</b> <b>learning</b> researchers don\u2019t really understand linear algebra d. it is generally not practical to form or store the Hessian in such problems, due to large problem size ...", "dateLastCrawled": "2022-02-03T02:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Deep Learning</b> for NLP - GitHub Pages", "url": "https://strikingloo.github.io/wiki-articles/machine-learning/deep-learning-NLP", "isFamilyFriendly": true, "displayUrl": "https://strikingloo.github.io/wiki-articles/<b>machine</b>-<b>learning</b>/<b>deep-learning</b>-NLP", "snippet": "Then feed to your main <b>model</b> both a char-RNN rep\u2019n, a word embedding and, after going through a bi-directional LSTM, concatenate hidden states with the concatenated hidden states of the (now pre-trained and frozen) <b>language</b> <b>model</b>. This beat SOTA by a narrow margin (0.3) but it was a much simpler <b>model</b> than the competition. ELMo", "dateLastCrawled": "2021-09-30T03:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "On the character of Indian Stock Markets: A <b>Machine</b> <b>Learning</b> Approach ...", "url": "https://www.ijert.org/on-the-character-of-indian-stock-markets-a-machine-learning-approach", "isFamilyFriendly": true, "displayUrl": "https://www.ijert.org/on-the-character-of-indian-stock-markets-a-<b>machine</b>-<b>learning</b>-approach", "snippet": "On the character of Indian Stock Markets: A <b>Machine</b> <b>Learning</b> Approach. Shubham popli Northcap University Gurgaon, Haryana. Abstract- The enterprise of forecasting the stock market is as old as the market itself, ranging from the many traditional approaches like regression analysis and linear methods like AR, MA, ARIMA and ARMA, and of course fuzzier methods like experts intuitions and sentiment analysis of news cycles.", "dateLastCrawled": "2021-12-29T01:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Andrew-NG-Notes/andrewng-p-5-sequence-models.md at master ... - <b>GitHub</b>", "url": "https://github.com/ashishpatel26/Andrew-NG-Notes/blob/master/andrewng-p-5-sequence-models.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/.../Andrew-NG-Notes/blob/master/andrewng-p-5-sequence-<b>models</b>.md", "snippet": "What is a <b>language</b> <b>model</b>. Let&#39;s say we are solving a speech recognition problem and someone says a sentence that can be interpreted into to two sentences: The apple and pair salad; The apple and pear salad; Pair and pear sounds exactly the same, so how would a speech recognition application choose from the two. That&#39;s where the <b>language</b> <b>model</b> comes in. It gives a probability for the two sentences and the application decides the best based on this probability. The job of a <b>language</b> <b>model</b> is ...", "dateLastCrawled": "2022-02-03T05:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) \u201cA Passage to India\u201d: Pre-trained Word Embeddings for Indian ...", "url": "https://www.academia.edu/66588950/_A_Passage_to_India_Pre_trained_Word_Embeddings_for_Indian_Languages", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/66588950/_A_Passage_to_India_Pre_trained_Word_Embeddings_for...", "snippet": "Dense word vectors or \u2018word embeddings\u2019 which encode semantic properties of words, have now become integral to NLP tasks like <b>Machine</b> Translation (MT), Question Answering (QA), Word Sense Disambiguation (WSD), and Information Retrieval (IR). In this", "dateLastCrawled": "2022-01-17T22:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Sequence Classification with LSTM Recurrent Neural Networks in Python ...", "url": "https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/sequence-classification-", "snippet": "Sequence Classification with LSTM Recurrent Neural Networks in Python with Keras. Sequence classification is a predictive modeling problem where you have some sequence of inputs over space or time and the task is to predict a category for the sequence. What makes this problem difficult is that the sequences can vary in length, be comprised of a ...", "dateLastCrawled": "2022-02-02T22:19:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(unidirectional language model)  is like +(train track)", "+(unidirectional language model) is similar to +(train track)", "+(unidirectional language model) can be thought of as +(train track)", "+(unidirectional language model) can be compared to +(train track)", "machine learning +(unidirectional language model AND analogy)", "machine learning +(\"unidirectional language model is like\")", "machine learning +(\"unidirectional language model is similar\")", "machine learning +(\"just as unidirectional language model\")", "machine learning +(\"unidirectional language model can be thought of as\")", "machine learning +(\"unidirectional language model can be compared to\")"]}