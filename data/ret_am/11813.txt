{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Equilibrium and effectiveness of two-parameter scoring rules ...", "url": "https://www.sciencedirect.com/science/article/pii/S016548961300108X", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S016548961300108X", "snippet": "The types of the n + 1 voters are stochastically independent <b>and identically</b> <b>distributed</b> (<b>i.i.d</b>.). We assume furthermore that for each voter i, the three Bernoulli utilities u A i, u B i and u C i are drawn <b>independently</b> from the unit interval [0, 1] according to a <b>distribution</b> G with continuous and strictly positive density g on (0, 1). As a consequence, the probability that a voter has a particular ordinal ranking is the same for all six possible ordinal rankings. The above features of the ...", "dateLastCrawled": "2021-10-30T23:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Evaluating Machine Learning Models</b> \u2013 O\u2019Reilly", "url": "https://www.oreilly.com/content/evaluating-machine-learning-models/", "isFamilyFriendly": true, "displayUrl": "https://www.oreilly.com/content/<b>evaluating-machine-learning-models</b>", "snippet": "Assuming that all data points are <b>i.i.d</b>. (<b>independently</b> <b>and identically</b> <b>distributed</b>), we simply randomly hold out part of the data for validation. We train the model on the larger portion of the data and evaluate validation metrics on the smaller hold-out set. Computationally speaking, hold-out validation is simple to program and fast to run. The downside is that it is less powerful statistically. The validation results are derived from a small subset of the data, hence its estimate of the ...", "dateLastCrawled": "2022-01-26T02:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>understanding-machine-learning</b>-theory-algorithms[1] Pages 1 - 50 - Flip ...", "url": "https://fliphtml5.com/flqg/grxi/basic", "isFamilyFriendly": true, "displayUrl": "https://fliphtml5.com/flqg/grxi/basic", "snippet": "Formally, \u2022 The <b>i.i.d</b>. assumption: The examples in the training set are <b>independently</b> <b>and identically</b> <b>distributed</b> (<b>i.i.d</b>.) according to the <b>distribution</b> D. That is, every xi in S is freshly sampled according to D and then labeled according to the labeling function, f . We denote this assumption by S \u223c Dm where m is the size of S, and Dm denotes the probability over m-tuples induced by applying D to pick each element of the tuple <b>independently</b> of the other members of the tuple ...", "dateLastCrawled": "2021-12-18T06:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Understanding Machine Learning: From Theory</b> to Algorithms [pdf] [PDF ...", "url": "https://authorzilla.com/JjpMG/understanding-machine-learning-from-theory-to-algorithms-pdf.html", "isFamilyFriendly": true, "displayUrl": "https://authorzilla.com/JjpMG/<b>understanding-machine-learning-from-theory</b>-to-algorithms...", "snippet": "Formally, The <b>i.i.d</b>. assumption: The examples in the training set are <b>independently</b> <b>and identically</b> <b>distributed</b> (<b>i.i.d</b>.) according to the <b>distribution</b> D. That is, every xi in S is freshly sampled according to D and then labeled according to the labeling function, f . We denote this assumption by S Dm where m is the size of S, and Dm denotes the probability over m-tuples induced by applying D to pick each element of the tuple <b>independently</b> of the other members of the tuple. Intuitively, the ...", "dateLastCrawled": "2022-02-03T04:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Real-time model learning <b>using Incremental Sparse Spectrum Gaussian</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0893608012002249", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0893608012002249", "snippet": "Aside from being unsuitable in a real-time context, an additional difficulty with this method is that convergence requires independent <b>and identically</b> <b>distributed</b> (<b>i.i.d</b>.) sampling. The problem of a linearly growing kernel expansion can be avoided by projecting \u03bd -approximate linear dependent samples onto each other, where increasing the value \u03bd induces more sparsity at the cost of accuracy.", "dateLastCrawled": "2021-12-02T00:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Highest scored &#39;<b>iid</b>&#39; questions - <b>Cross Validated</b>", "url": "https://stats.stackexchange.com/questions/tagged/iid?tab=Votes", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/tagged/<b>iid</b>?tab=Votes", "snippet": "This is an interview question for a quantitative analyst position, reported here. Suppose we are <b>drawing</b> from a uniform $[0,1]$ <b>distribution</b> and the draws are <b>iid</b>, what is the expected length of a ... probability <b>random</b>-variable expected-value uniform-<b>distribution</b> <b>iid</b>. asked Jun 12 &#39;18 at 0:42. Amazonian. 1,394 1 1 gold badge 10 10 silver badges 19 19 bronze badges. 25. votes. 1answer 3k views. Properties of PCA for dependent observations. We usually use PCA as a dimensionality reduction ...", "dateLastCrawled": "2022-02-03T11:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Statistics for Applications Exam 3 Solution", "url": "https://ocw.mit.edu/courses/mathematics/18-443-statistics-for-applications-spring-2015/exams/MIT18_443S15_Exam3_Sol.pdf", "isFamilyFriendly": true, "displayUrl": "https://<b>ocw.mit.edu</b>/courses/mathematics/18-443-statistics-for-applications-spring-2015/...", "snippet": "are 2independent <b>and identically</b> <b>distribution</b> N(0,\u03c3) <b>random</b> variables with \ufb01xed, but unknown variance \u03c3. 2 &gt; 0. When x i = 0, then E[Y i | x i,\u03b2] = 0. (a). Solve for the least-squares line \u2013 Y\u02c6 = \u03b2x.\u02c6 (b). Find the <b>distribution</b> of. \u03b2,\u02c6. the slope of the least squares line. (c). What is the <b>distribution</b> of the sum of squared ...", "dateLastCrawled": "2022-02-02T02:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Recently Active &#39;random-variable&#39; Questions - Page</b> 3 - <b>Cross Validated</b>", "url": "https://stats.stackexchange.com/questions/tagged/random-variable?page=3&sort=active", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/tagged/<b>random</b>-variable?page=3&amp;sort=active", "snippet": "Confusion in definition of independent <b>and identically</b> <b>distributed</b> <b>random</b> variables [duplicate] From what I learnt, a <b>random</b> variable is a function which assigns real values to outcome space, and the probability <b>distribution</b> is a function that assigns probability to different values produced by ... probability mathematical-statistics <b>random</b>-variable <b>iid</b>. answered Sep 14 &#39;21 at 13:50. Xi&#39;an. 89.7k 9 9 gold badges 157 157 silver badges 574 574 bronze badges. 1. vote. 2answers 176 views ...", "dateLastCrawled": "2022-01-26T02:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Lect 14 Probability - Vision Labs", "url": "http://vision.psych.umn.edu/users/kersten/kersten-lab/courses/Psy5038WF2014/Lectures/Lect_14_Probability2/Lect_14_Probability.nb.pdf", "isFamilyFriendly": true, "displayUrl": "vision.psych.umn.edu/users/kersten/kersten-lab/courses/Psy5038WF2014/Lectures/Lect_14...", "snippet": "but we\u2019d <b>like</b> to better understand some principles behind generating <b>random</b> <b>numbers</b> for a specified <b>distribution</b>. Method 1: Just for Gaussian. Use Central Limit Theorem If all we want to do is make a Gaussian <b>random</b> number generator from a uniformly <b>distributed</b> genera- tor, we can use the Central Limit Theorem. The Central Limit Theorem says that the sum of a suffi-ciently large number of independent <b>random</b> variables drawn from the same underlying <b>distribution</b> (with finite mean and ...", "dateLastCrawled": "2021-09-01T02:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Mathematical statistics and data analysis</b> - SILO.PUB", "url": "https://silo.pub/mathematical-statistics-and-data-analysis.html", "isFamilyFriendly": true, "displayUrl": "https://silo.pub/<b>mathematical-statistics-and-data-analysis</b>.html", "snippet": "Generally, the times between events of a Poisson process are independent, <b>identically</b> <b>distributed</b>, exponential <b>random</b> variables. Proteins and other biologically important molecules are regulated in various ways. Some undergo aging and are thus more likely to degrade when they are old than when they are young. If a molecule was not subject to aging, but its chance of degradation was the same at any age, its lifetime would follow an exponential <b>distribution</b>.", "dateLastCrawled": "2022-01-27T17:16:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) Statistical mechanical assessment of a reconstruction limit of ...", "url": "https://www.researchgate.net/publication/230954173_Statistical_mechanical_assessment_of_a_reconstruction_limit_of_compressed_sensing_Toward_theoretical_analysis_of_correlated_signals", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/230954173_Statistical_mechanical_assessment...", "snippet": "F, F \u00b5i entry is <b>independently</b> <b>and identically</b> <b>distributed</b> (<b>i.i.d</b>.) from a Gaussian <b>distribution</b> of the zero mean. and variance N \u2212 1. The compression rate is de\ufb01ned by. \u03b1 \u2261 P /N &lt; 1. In ...", "dateLastCrawled": "2021-12-19T15:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) Face <b>Recognition with Image Sets Using Manifold Density</b> ...", "url": "https://www.academia.edu/8477008/Face_Recognition_with_Image_Sets_Using_Manifold_Density_Divergence", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/8477008/Face_<b>Recognition_with_Image_Sets_Using</b>_Manifold...", "snippet": "As a result, it can be expected that face images of independent <b>and identically</b> <b>distributed</b> (<b>i.i.d</b>.) observa- are con\ufb01ned to a face space, a manifold of lower dimension tions drawn from p(i) (1). Similarly, the input set S0 is d D embedded in the image space [6]. Below, we for- assumed to be <b>i.i.d</b>. drawn from the test subject\u2019s face im- malize this notion and propose an algorithm for comparing age density p(0) . The recognition task can then be formu- the estimated densities on the ...", "dateLastCrawled": "2021-11-19T14:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Evaluating Machine Learning Models</b> \u2013 O\u2019Reilly", "url": "https://www.oreilly.com/content/evaluating-machine-learning-models/", "isFamilyFriendly": true, "displayUrl": "https://www.oreilly.com/content/<b>evaluating-machine-learning-models</b>", "snippet": "Assuming that all data points are <b>i.i.d</b>. (<b>independently</b> <b>and identically</b> <b>distributed</b>), we simply randomly hold out part of the data for validation. We train the model on the larger portion of the data and evaluate validation metrics on the smaller hold-out set. Computationally speaking, hold-out validation is simple to program and fast to run. The downside is that it is less powerful statistically. The validation results are derived from a small subset of the data, hence its estimate of the ...", "dateLastCrawled": "2022-01-26T02:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Equilibrium and effectiveness of two-parameter scoring rules ...", "url": "https://www.sciencedirect.com/science/article/pii/S016548961300108X", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S016548961300108X", "snippet": "The types of the n + 1 voters are stochastically independent <b>and identically</b> <b>distributed</b> (<b>i.i.d</b>.). We assume furthermore that for each voter i, the three Bernoulli utilities u A i, u B i and u C i are drawn <b>independently</b> from the unit interval [0, 1] according to a <b>distribution</b> G with continuous and strictly positive density g on (0, 1). As a consequence, the probability that a voter has a particular ordinal ranking is the same for all six possible ordinal rankings. The above features of the ...", "dateLastCrawled": "2021-10-30T23:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Understanding Machine Learning: From Theory</b> to Algorithms [pdf] [PDF ...", "url": "https://authorzilla.com/JjpMG/understanding-machine-learning-from-theory-to-algorithms-pdf.html", "isFamilyFriendly": true, "displayUrl": "https://authorzilla.com/JjpMG/<b>understanding-machine-learning-from-theory</b>-to-algorithms...", "snippet": "Formally, The <b>i.i.d</b>. assumption: The examples in the training set are <b>independently</b> <b>and identically</b> <b>distributed</b> (<b>i.i.d</b>.) according to the <b>distribution</b> D. That is, every xi in S is freshly sampled according to D and then labeled according to the labeling function, f . We denote this assumption by S Dm where m is the size of S, and Dm denotes the probability over m-tuples induced by applying D to pick each element of the tuple <b>independently</b> of the other members of the tuple. Intuitively, the ...", "dateLastCrawled": "2022-02-03T04:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Random Projection Estimation of Discrete-Choice Models</b> with Large ...", "url": "https://deepai.org/publication/random-projection-estimation-of-discrete-choice-models-with-large-choice-sets", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>random-projection-estimation-of-discrete-choice-models</b>...", "snippet": "A sparse <b>random</b> projection matrix is a k-by-d matrix R such that each i, j-th entry is <b>independently</b> <b>and identically</b> <b>distributed</b> according to the following discrete <b>distribution</b>: R i, j = \u221a s \u23a7 \u23aa \u23aa \u23a8 \u23aa \u23aa \u23a9 + 1 with probability 1 2 s 0 with probability 1 \u2212 1 s \u2212 1 with probability 1 2 s (s &gt; 1). By choosing a higher s, we produce sparser <b>random</b> projection matrices. Li et al. (2006) show that: V a r (\u2225 R u \u2212 R v \u2225 2) = 1 k (2 \u2225 u \u2212 v \u2225 4 + (s \u2212 3) d \u2211 j = 1 ...", "dateLastCrawled": "2021-11-29T21:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Real-time model learning <b>using Incremental Sparse Spectrum Gaussian</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0893608012002249", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0893608012002249", "snippet": "Aside from being unsuitable in a real-time context, an additional difficulty with this method is that convergence requires independent <b>and identically</b> <b>distributed</b> (<b>i.i.d</b>.) sampling. The problem of a linearly growing kernel expansion can be avoided by projecting \u03bd -approximate linear dependent samples onto each other, where increasing the value \u03bd induces more sparsity at the cost of accuracy.", "dateLastCrawled": "2021-12-02T00:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Statistics for Applications Exam 3 Solution", "url": "https://ocw.mit.edu/courses/mathematics/18-443-statistics-for-applications-spring-2015/exams/MIT18_443S15_Exam3_Sol.pdf", "isFamilyFriendly": true, "displayUrl": "https://<b>ocw.mit.edu</b>/courses/mathematics/18-443-statistics-for-applications-spring-2015/...", "snippet": "are 2independent <b>and identically</b> <b>distribution</b> N(0,\u03c3) <b>random</b> variables with \ufb01xed, but unknown variance \u03c3. 2 &gt; 0. When x i = 0, then E[Y i | x i,\u03b2] = 0. (a). Solve for the least-squares line \u2013 Y\u02c6 = \u03b2x.\u02c6 (b). Find the <b>distribution</b> of. \u03b2,\u02c6. the slope of the least squares line. (c). What is the <b>distribution</b> of the sum of squared ...", "dateLastCrawled": "2022-02-02T02:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) Introduction to Statistics and Data Analysis - Christian Heumann ...", "url": "https://www.academia.edu/48943360/Introduction_to_Statistics_and_Data_Analysis_Christian_Heumann_and_Michael_Schomaker_Shalabh", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/48943360/Introduction_to_Statistics_and_Data_Analysis...", "snippet": "With Exercises, Solutions and Applications in R Springer. Introduction to Statistics and <b>Data Analysis - Christian Heumann &amp; Michael Schomaker Shalabh</b>", "dateLastCrawled": "2022-01-26T08:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(PDF) <b>Introduction to Probability and Statistics</b> Using R | jimoh diya ...", "url": "https://www.academia.edu/31814896/Introduction_to_Probability_and_Statistics_Using_R", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/31814896", "snippet": "Academia.edu is a platform for academics to share research papers.", "dateLastCrawled": "2022-02-02T14:26:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Evaluating Machine Learning Models</b> \u2013 O\u2019Reilly", "url": "https://www.oreilly.com/content/evaluating-machine-learning-models/", "isFamilyFriendly": true, "displayUrl": "https://www.oreilly.com/content/<b>evaluating-machine-learning-models</b>", "snippet": "Assuming that all data points are <b>i.i.d</b>. (<b>independently</b> <b>and identically</b> <b>distributed</b>), we simply randomly hold out part of the data for validation. We train the model on the larger portion of the data and evaluate validation metrics on the smaller hold-out set. Computationally speaking, hold-out validation is simple to program and fast to run. The downside is that it is less powerful statistically. The validation results are derived from a small subset of the data, hence its estimate of the ...", "dateLastCrawled": "2022-01-26T02:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>understanding-machine-learning</b>-theory-algorithms[1] Pages 1 - 50 - Flip ...", "url": "https://fliphtml5.com/flqg/grxi/basic", "isFamilyFriendly": true, "displayUrl": "https://fliphtml5.com/flqg/grxi/basic", "snippet": "Formally, \u2022 The <b>i.i.d</b>. assumption: The examples in the training set are <b>independently</b> <b>and identically</b> <b>distributed</b> (<b>i.i.d</b>.) according to the <b>distribution</b> D. That is, every xi in S is freshly sampled according to D and then labeled according to the labeling function, f . We denote this assumption by S \u223c Dm where m is the size of S, and Dm denotes the probability over m-tuples induced by applying D to pick each element of the tuple <b>independently</b> of the other members of the tuple ...", "dateLastCrawled": "2021-12-18T06:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Understanding Machine Learning: From Theory</b> to Algorithms [pdf] [PDF ...", "url": "https://authorzilla.com/JjpMG/understanding-machine-learning-from-theory-to-algorithms-pdf.html", "isFamilyFriendly": true, "displayUrl": "https://authorzilla.com/JjpMG/<b>understanding-machine-learning-from-theory</b>-to-algorithms...", "snippet": "Formally, The <b>i.i.d</b>. assumption: The examples in the training set are <b>independently</b> <b>and identically</b> <b>distributed</b> (<b>i.i.d</b>.) according to the <b>distribution</b> D. That is, every xi in S is freshly sampled according to D and then labeled according to the labeling function, f . We denote this assumption by S Dm where m is the size of S, and Dm denotes the probability over m-tuples induced by applying D to pick each element of the tuple <b>independently</b> of the other members of the tuple. Intuitively, the ...", "dateLastCrawled": "2022-02-03T04:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Highest scored &#39;<b>iid</b>&#39; questions - <b>Cross Validated</b>", "url": "https://stats.stackexchange.com/questions/tagged/iid?tab=Votes", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/tagged/<b>iid</b>?tab=Votes", "snippet": "This is an interview question for a quantitative analyst position, reported here. Suppose we are <b>drawing</b> from a uniform $[0,1]$ <b>distribution</b> and the draws are <b>iid</b>, what is the expected length of a ... probability <b>random</b>-variable expected-value uniform-<b>distribution</b> <b>iid</b>. asked Jun 12 &#39;18 at 0:42. Amazonian. 1,394 1 1 gold badge 10 10 silver badges 19 19 bronze badges. 25. votes. 1answer 3k views. Properties of PCA for dependent observations. We usually use PCA as a dimensionality reduction ...", "dateLastCrawled": "2022-02-03T11:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Performance Modeling and Design of Computer systems: Queueing Theory in ...", "url": "https://docshare.tips/performance-modeling-and-design-of-computer-systems-queueing-theory-in-action_5871b5a4b6d87f2f098b4945.html", "isFamilyFriendly": true, "displayUrl": "https://docshare.tips/performance-modeling-and-design-of-computer-systems-queueing...", "snippet": "<b>independently</b> <b>and identically</b> <b>distributed</b> according to <b>random</b> variable S where 1 with probability 3/4 S=. 2 otherwise You have measured the mean response time: E [T ] = 29. 12 Based on this information, compute the mean slowdown, E [Slowdown], (j ), where where the slowdown of job j is defined as Slowdown(j) = TS (j) T (j) is the response time of job j and S(j) is the size of job j . (b) If the service order in part (a) had been Shortest-Job-First (SJF), would the same technique have worked ...", "dateLastCrawled": "2022-01-31T03:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Several reviews of Deborah Mayo\u2019s new book, Statistical Inference as ...", "url": "https://statmodeling.stat.columbia.edu/2019/04/12/several-reviews-of-deborah-mayos-new-book-statistical-inference-as-severe-testing-how-to-get-beyond-the-statistics-wars/", "isFamilyFriendly": true, "displayUrl": "https://statmodeling.stat.columbia.edu/2019/04/12/several-reviews-of-deborah-mayos-new...", "snippet": "Worse, only regular but not general patterns of dependence <b>can</b> be distinguished from independence by data; any non-<b>i.i.d</b>. pattern <b>can</b> be explained by either dependence or non-identity of distributions, and telling these apart requires constraints on dependence and non-identity structures that <b>can</b> itself not be tested on the data (in the example <b>given</b> in 4.11 of Mayo, 2018, all tests discover specific regular alternatives to the model assumption). <b>Given</b> that this is so, the question arises on ...", "dateLastCrawled": "2022-01-16T04:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Statistics</b> | PDF | Statistical Hypothesis Testing | Integral", "url": "https://www.scribd.com/document/393137596/statistics", "isFamilyFriendly": true, "displayUrl": "https://www.scribd.com/document/393137596/<b>statistics</b>", "snippet": "If S has a compound Poisson <b>distribution</b> <b>given</b> by \u03bb=3,p(1) ... Weak Law of Large <b>Numbers</b> for <b>iid</b> <b>random</b> variables, Bernoulli Law of Large <b>Numbers</b>, Central Limit Theorem for independent. <b>and identically</b> <b>distributed</b> <b>random</b> variables (Lindberg-Levy form). 12 hours. Book for reference. 1. V.K. Rohatgi: An Introduction to Probability theory and Mathematical. Statististics, Wiley Eastern. 2. S.C.Gupta and V.K.K.Kapoor: fundamentals of Mathematical <b>Statistics</b>, Sultan Chand and sons. 3. Mood A.M ...", "dateLastCrawled": "2022-02-02T16:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) Introduction to Statistics and Data Analysis - Christian Heumann ...", "url": "https://www.academia.edu/48943360/Introduction_to_Statistics_and_Data_Analysis_Christian_Heumann_and_Michael_Schomaker_Shalabh", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/48943360/Introduction_to_Statistics_and_Data_Analysis...", "snippet": "With Exercises, Solutions and Applications in R Springer. Introduction to Statistics and <b>Data Analysis - Christian Heumann &amp; Michael Schomaker Shalabh</b>", "dateLastCrawled": "2022-01-26T08:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Probability: For the Enthusiastic Beginner</b> [1&amp;nbsp;ed.] 1523318678 ...", "url": "https://dokumen.pub/probability-for-the-enthusiastic-beginner-1nbsped-1523318678-9781523318674.html", "isFamilyFriendly": true, "displayUrl": "https://dokumen.pub/<b>probability-for-the-enthusiastic-beginner</b>-1nbsped-1523318678...", "snippet": "\u2022 For P3 , Table 6.1 shows that P3 = 6 <b>can</b> <b>be thought</b> of as three groups (characterized by which number appears first) of the P2 = 2 permutations of the second and third <b>numbers</b>. So we have P3 = 3P2 = 3 \u00b7 2 \u00b7 1. \u2022 Similarly, for P4 , Table 1.2 shows that P4 = 24 <b>can</b> <b>be thought</b> of as four groups (characterized by which number appears first) of the P3 = 6 permutations of the second, third, and fourth <b>numbers</b>. So we have P4 = 4P3 = 4 \u00b7 3 \u00b7 2 \u00b7 1. \u2022 Likewise, the above reasoning for N ...", "dateLastCrawled": "2022-01-24T05:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Calam\u00e9o - Schay - Introduction to Pbrobability with Statistical ...", "url": "https://www.calameo.com/books/000217381714e59d9ecd2", "isFamilyFriendly": true, "displayUrl": "https://www.calameo.com/books/000217381714e59d9ecd2", "snippet": "We are <b>given</b> the probabilities P(U1) = P(U2) = 1/2, since this is what it means that an urn is picked at <b>random</b>; and, <b>given</b> that urn 1 is chosen, the <b>random</b> choice of a ball gives us the conditional probability P(W|U1) = 2/8, and similarly P(W|U2) = 2/4. Then, by Formula 3. 23, P(W \u2229 U1) = P(W|U1)P(U1) = 2 8 \u00b7 1 2 = 1 8 , (3. 31) and P(W \u2229 U2) = P(W|U2)P(U2) = 2 4 \u00b7 1 2 = 1 4 . (3. 32) Now, obviously, W is the union of the disjoint events W \u2229 U1 and W \u2229 U2, and so by the additivity ...", "dateLastCrawled": "2021-11-29T21:25:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Evaluating Machine Learning Models</b> \u2013 O\u2019Reilly", "url": "https://www.oreilly.com/content/evaluating-machine-learning-models/", "isFamilyFriendly": true, "displayUrl": "https://www.oreilly.com/content/<b>evaluating-machine-learning-models</b>", "snippet": "Assuming that all data points are <b>i.i.d</b>. (<b>independently</b> <b>and identically</b> <b>distributed</b>), we simply randomly hold out part of the data for validation. We train the model on the larger portion of the data and evaluate validation metrics on the smaller hold-out set. Computationally speaking, hold-out validation is simple to program and fast to run. The downside is that it is less powerful statistically. The validation results are derived from a small subset of the data, hence its estimate of the ...", "dateLastCrawled": "2022-01-26T02:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) Face <b>Recognition with Image Sets Using Manifold Density</b> ...", "url": "https://www.academia.edu/8477008/Face_Recognition_with_Image_Sets_Using_Manifold_Density_Divergence", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/8477008/Face_<b>Recognition_with_Image_Sets_Using</b>_Manifold...", "snippet": "As a result, it <b>can</b> be expected that face images of independent <b>and identically</b> <b>distributed</b> (<b>i.i.d</b>.) observa- are con\ufb01ned to a face space, a manifold of lower dimension tions drawn from p(i) (1). Similarly, the input set S0 is d D embedded in the image space [6]. Below, we for- assumed to be <b>i.i.d</b>. drawn from the test subject\u2019s face im- malize this notion and propose an algorithm for comparing age density p(0) . The recognition task <b>can</b> then be formu- the estimated densities on the ...", "dateLastCrawled": "2021-11-19T14:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Understanding Machine Learning: From Theory</b> to Algorithms [pdf] [PDF ...", "url": "https://authorzilla.com/JjpMG/understanding-machine-learning-from-theory-to-algorithms-pdf.html", "isFamilyFriendly": true, "displayUrl": "https://authorzilla.com/JjpMG/<b>understanding-machine-learning-from-theory</b>-to-algorithms...", "snippet": "Formally, The <b>i.i.d</b>. assumption: The examples in the training set are <b>independently</b> <b>and identically</b> <b>distributed</b> (<b>i.i.d</b>.) according to the <b>distribution</b> D. That is, every xi in S is freshly sampled according to D and then labeled according to the labeling function, f . We denote this assumption by S Dm where m is the size of S, and Dm denotes the probability over m-tuples induced by applying D to pick each element of the tuple <b>independently</b> of the other members of the tuple. Intuitively, the ...", "dateLastCrawled": "2022-02-03T04:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) Statistical mechanical assessment of a reconstruction limit of ...", "url": "https://www.researchgate.net/publication/230954173_Statistical_mechanical_assessment_of_a_reconstruction_limit_of_compressed_sensing_Toward_theoretical_analysis_of_correlated_signals", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/230954173_Statistical_mechanical_assessment...", "snippet": "F, F \u00b5i entry is <b>independently</b> <b>and identically</b> <b>distributed</b> (<b>i.i.d</b>.) from a Gaussian <b>distribution</b> of the zero mean. and variance N \u2212 1. The compression rate is de\ufb01ned by. \u03b1 \u2261 P /N &lt; 1. In ...", "dateLastCrawled": "2021-12-19T15:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Equilibrium and effectiveness of two-parameter scoring rules ...", "url": "https://www.sciencedirect.com/science/article/pii/S016548961300108X", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S016548961300108X", "snippet": "The types of the n + 1 voters are stochastically independent <b>and identically</b> <b>distributed</b> (<b>i.i.d</b>.). We assume furthermore that for each voter i, the three Bernoulli utilities u A i, u B i and u C i are drawn <b>independently</b> from the unit interval [0, 1] according to a <b>distribution</b> G with continuous and strictly positive density g on (0, 1). As a consequence, the probability that a voter has a particular ordinal ranking is the same for all six possible ordinal rankings. The above features of the ...", "dateLastCrawled": "2021-10-30T23:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Statistics for Applications Exam 3 Solution", "url": "https://ocw.mit.edu/courses/mathematics/18-443-statistics-for-applications-spring-2015/exams/MIT18_443S15_Exam3_Sol.pdf", "isFamilyFriendly": true, "displayUrl": "https://<b>ocw.mit.edu</b>/courses/mathematics/18-443-statistics-for-applications-spring-2015/...", "snippet": "are 2independent <b>and identically</b> <b>distribution</b> N(0,\u03c3) <b>random</b> variables with \ufb01xed, but unknown variance \u03c3. 2 &gt; 0. When x i = 0, then E[Y i | x i,\u03b2] = 0. (a). Solve for the least-squares line \u2013 Y\u02c6 = \u03b2x.\u02c6 (b). Find the <b>distribution</b> of. \u03b2,\u02c6. the slope of the least squares line. (c). What is the <b>distribution</b> of the sum of squared ...", "dateLastCrawled": "2022-02-02T02:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Real-time model learning <b>using Incremental Sparse Spectrum Gaussian</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0893608012002249", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0893608012002249", "snippet": "In most real-life applications, however, the typical <b>i.i.d</b>. assumption is violated and discrepancies between the train and test <b>distribution</b> may result in suboptimal performance of batch models. Incremental learning algorithms, on the other hand, <b>can</b> address these differences in <b>distribution</b> by continuously adapting their model to recent observations. Here we investigate this advantage <b>compared</b> to the batch learning approach at the hand of the real-life problem of modeling inverse dynamics ...", "dateLastCrawled": "2021-12-02T00:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) Introduction to Statistics and Data Analysis - Christian Heumann ...", "url": "https://www.academia.edu/48943360/Introduction_to_Statistics_and_Data_Analysis_Christian_Heumann_and_Michael_Schomaker_Shalabh", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/48943360/Introduction_to_Statistics_and_Data_Analysis...", "snippet": "With Exercises, Solutions and Applications in R Springer. Introduction to Statistics and <b>Data Analysis - Christian Heumann &amp; Michael Schomaker Shalabh</b>", "dateLastCrawled": "2022-01-26T08:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "On <b>the convolution of exponential distribution</b> | Request PDF", "url": "https://www.researchgate.net/publication/228806988_On_the_convolution_of_exponential_distribution", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/.../228806988_On_<b>the_convolution_of_exponential_distribution</b>", "snippet": "<b>Given</b> that we are in the aggregated state 1, which happens with probability 1 \u2212 \u03bd(0), the <b>distribution</b> of W is <b>given</b> by an exponentially <b>distributed</b> <b>random</b> variable with parameter \u00b5 N . ...", "dateLastCrawled": "2021-12-17T11:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Probability: For the Enthusiastic Beginner</b> [1&amp;nbsp;ed.] 1523318678 ...", "url": "https://dokumen.pub/probability-for-the-enthusiastic-beginner-1nbsped-1523318678-9781523318674.html", "isFamilyFriendly": true, "displayUrl": "https://dokumen.pub/<b>probability-for-the-enthusiastic-beginner</b>-1nbsped-1523318678...", "snippet": "Examples include picking committees of people (because a <b>given</b> person <b>can</b>\u2019t be repeated, of course), assigning people to seats, and <b>drawing</b> balls from a box without replacement. As far as the \u201corder matters\u201d and \u201corder doesn\u2019t matter\u201d descriptors in Table 1.11 go, you <b>can</b> simply imagine the ordered objects appearing in a line, and the unordered objects appearing in an amorphous group, or blob. We <b>can</b> therefore describe the four possibilities in Table 1.11 with the four phrases ...", "dateLastCrawled": "2022-01-24T05:29:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "What is <b>Learning</b>? <b>Machine Learning: Introduction and Unsupervised Learning</b>", "url": "http://pages.cs.wisc.edu/~bgibson/cs540/handouts/learning_intro.pdf", "isFamilyFriendly": true, "displayUrl": "pages.cs.wisc.edu/~bgibson/cs540/handouts/<b>learning</b>_intro.pdf", "snippet": "<b>learning</b> process \u2022x i = (x i1, . . . , x iD) \u2022Assume these instances are sampled <b>independently</b> from an unknown (population) distribution, P(x) \u2022We denote this by x i \u223c P(x), where <b>i.i.d</b>. stands for independent <b>and identically</b> <b>distributed</b> <b>i.i.d</b>. Training Sample \u2022A training sample is the \u201cexperience\u201d given to a <b>learning</b> algorithm", "dateLastCrawled": "2021-08-25T13:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "2 - The Process of <b>Learning</b>.pdf - CMPSC 448 <b>Machine</b> <b>Learning</b> Lecture 2 ...", "url": "https://www.coursehero.com/file/113918059/2-The-Process-of-Learningpdf/", "isFamilyFriendly": true, "displayUrl": "https://www.coursehero.com/file/113918059/2-The-Process-of-<b>Learning</b>pdf", "snippet": "<b>I.I.D</b> assumption Training/test data is independent <b>and identically</b> <b>distributed</b> (<b>i.i.d</b>) if: All objects come from the same distribution (<b>identically</b> <b>distributed</b>). The object are sampled <b>independently</b> (order doesn\u2019t matter). We do NOT need to know the underlying distribution as long as the samples are sampled <b>i.i.d</b>. Examples in terms of cards: Pick a card, put it back in the deck, re-shuffle, repeat. Pick a card, put it back in the deck, repeat.", "dateLastCrawled": "2021-12-30T01:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "What is <b>Learning</b>? <b>Machine</b> <b>Learning</b>: Introduction and Unsupervised <b>Learning</b>", "url": "https://pages.cs.wisc.edu/~dyer/cs540/notes/08_learning-intro.pdf", "isFamilyFriendly": true, "displayUrl": "https://pages.cs.wisc.edu/~dyer/cs540/notes/08_<b>learning</b>-intro.pdf", "snippet": "the inputto the <b>learning</b> process \u2022x i=(x i1, . . . , x iD) \u2022Assume these instances are all sampled independentlyfrom the same, unknown (population) distribution, P(x) \u2022We denote this by x i\u223cP(x), where <b>i.i.d</b>. stands for independent <b>and identically</b> <b>distributed</b> \u2022Example: Repeated throws of dice <b>i.i.d</b>. 13", "dateLastCrawled": "2022-02-03T16:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "11._Intro_to_<b>Machine</b>_<b>Learning</b>.pdf - CMPSC 442 Artificial Intelligence ...", "url": "https://www.coursehero.com/file/121916721/11-Intro-to-Machine-Learningpdf/", "isFamilyFriendly": true, "displayUrl": "https://www.coursehero.com/file/121916721/11-Intro-to-<b>Machine</b>-<b>Learning</b>pdf", "snippet": "<b>I.I.D</b> assumption Training/test data is independent <b>and identically</b> <b>distributed</b> (<b>i.i.d</b>) if: All objects come from the same distribution (<b>identically</b> <b>distributed</b>). The object are sampled <b>independently</b> (order doesn\u2019t matter). We do NOT need to know the underlying distribution as long as the samples are sampled <b>i.i.d</b>. Examples in terms of cards: Pick a card, put it back in the deck, re-shuffle, repeat. Pick a card, put it back in the deck, repeat. Pick a card, don\u2019t put it back, re-shuffle ...", "dateLastCrawled": "2022-01-15T13:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Background on <b>machine</b> <b>learning</b> and <b>learning</b> theory", "url": "https://matthewhirn.files.wordpress.com/2020/02/cmse890_spring2020_chapter1.pdf", "isFamilyFriendly": true, "displayUrl": "https://matthewhirn.files.wordpress.com/2020/02/cmse890_spring2020_chapter1.pdf", "snippet": "Often we will assume that the #i are <b>independently</b> <b>and identically</b> <b>distributed</b> (<b>i.i.d</b>.) according to the normal distribution with mean zero and variance s2, i.e. #i \u21e0N(0,s2). In this case, if X is the random variable that takes values in Rd according to the probability distribution PX, and Y is the random variable that takes values in R ...", "dateLastCrawled": "2021-08-12T13:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Learning</b> from Examples as an <b>Inverse Problem</b> - Journal of <b>Machine</b> ...", "url": "https://jmlr.csail.mit.edu/papers/volume6/devito05a/devito05a.pdf", "isFamilyFriendly": true, "displayUrl": "https://jmlr.csail.mit.edu/papers/volume6/devito05a/devito05a.pdf", "snippet": "<b>and identically</b> <b>distributed</b> (<b>i.i.d</b>.) according to \u03c1. Given the sample z, the aim of <b>learning</b> theory is to \ufb01nd a function fz: X \u2192R such that fz(x) is a good estimate of the output y when a new input x is given. The function fz is called estimator and the map providing fz, for any training set z, is called <b>learning</b> algorithm.", "dateLastCrawled": "2021-09-19T04:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "8. Recurrent Neural Networks \u2014 Dive into <b>Deep Learning</b> 0.17.2 documentation", "url": "https://d2l.ai/chapter_recurrent-neural-networks/index.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_recurrent-neural-networks/index.html", "snippet": "Most importantly, so far we tacitly assumed that our data are all drawn from some distribution, and all the examples are <b>independently</b> <b>and identically</b> <b>distributed</b> (<b>i.i.d</b>.). Unfortunately, this is not true for most data. For instance, the words in this paragraph are written in sequence, and it would be quite difficult to decipher its meaning if they were permuted randomly. Likewise, image frames in a video, the audio signal in a conversation, and the browsing behavior on a website, all follow ...", "dateLastCrawled": "2022-02-03T01:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Assignment 1</b> - Department of Computer Science and Electrical Engineering", "url": "https://www.csee.umbc.edu/courses/undergraduate/473/f19/content/materials/a1.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.csee.umbc.edu/courses/undergraduate/473/f19/content/materials/a1.pdf", "snippet": "i is an <b>i.i.d</b>. sample, where <b>i.i.d</b>. means \u201c<b>independently</b> <b>and identically</b> <b>distributed</b>. ... Using a programming <b>analogy</b>, we can say that word types are like classes while word tokens are like instances of that class. For example, in the following sentence there are six types and eight tokens: the gray cat chased the tabby cat . Notice that this computation includes punctuation. (b)In the training \ufb01le, how many different word types and tokens are there? Do not perform any processing that ...", "dateLastCrawled": "2022-02-02T03:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>The Essence of RNNs</b>. The intuition behind the building\u2026 | by Taha ...", "url": "https://towardsdatascience.com/the-essence-of-rnns-44dfb4107a47", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>the-essence-of-rnns</b>-44dfb4107a47", "snippet": "When considering CNNs and MLPs, we always assumed that the data was sampled from and <b>independently</b> <b>and identically</b> <b>distributed</b> data(<b>i.i.d</b>), but with sequential data, that is not the case. Contrary to (<b>i.i.d</b>) data, the previous input points affect the outcome of the next output. Since RNNs are most widely used in natural language processing(NLP), an <b>analogy</b> from that field would suffice to make the point clear. Imagine textual data, all the words in a sequence affect the outcome of the ...", "dateLastCrawled": "2022-01-23T05:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Learning from positive</b> and unlabeled data: a survey - <b>Machine</b> <b>Learning</b>", "url": "https://link.springer.com/article/10.1007/s10994-020-05877-5", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10994-020-05877-5", "snippet": "<b>Learning from positive</b> and unlabeled data or PU <b>learning</b> is the setting where a learner only has access to positive examples and unlabeled data. The assumption is that the unlabeled data can contain both positive and negative examples. This setting has attracted increasing interest within the <b>machine</b> <b>learning</b> literature as this type of data naturally arises in applications such as medical diagnosis and knowledge base completion. This article provides a survey of the current state of the art ...", "dateLastCrawled": "2022-02-02T03:04:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(independently and identically distributed (i.i.d))  is like +(drawing a bunch of random numbers from a given distribution)", "+(independently and identically distributed (i.i.d)) is similar to +(drawing a bunch of random numbers from a given distribution)", "+(independently and identically distributed (i.i.d)) can be thought of as +(drawing a bunch of random numbers from a given distribution)", "+(independently and identically distributed (i.i.d)) can be compared to +(drawing a bunch of random numbers from a given distribution)", "machine learning +(independently and identically distributed (i.i.d) AND analogy)", "machine learning +(\"independently and identically distributed (i.i.d) is like\")", "machine learning +(\"independently and identically distributed (i.i.d) is similar\")", "machine learning +(\"just as independently and identically distributed (i.i.d)\")", "machine learning +(\"independently and identically distributed (i.i.d) can be thought of as\")", "machine learning +(\"independently and identically distributed (i.i.d) can be compared to\")"]}