{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Embedding Space</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/computer-science/embedding-space", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/computer-science/<b>embedding-space</b>", "snippet": "If the CNN has learned correctly to <b>map</b> images to the text <b>embedding space</b>, the distances between the embeddings of the images and the texts of a pair should be similar, and points in the plot should fall around the identity line y = x. Also, if the learned <b>space</b> has a semantic structure, both the distance between images embeddings and the distance between texts embeddings should be smaller for those pairs sharing more tags: The plot points&#39; color reflects the number of common tags of the ...", "dateLastCrawled": "2022-01-20T05:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Embeddings | Machine Learning Crash Course | <b>Google Developers</b>", "url": "https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/machine-learning/crash-course/<b>embedding</b>", "snippet": "An <b>embedding</b> is a relatively low-dimensional <b>space</b> into which you can translate high-dimensional vectors. Embeddings make it easier to do machine learning on large inputs <b>like</b> sparse vectors representing words. Ideally, an <b>embedding</b> captures some of the semantics of the input by placing semantically similar inputs close together in the <b>embedding</b> <b>space</b>.", "dateLastCrawled": "2022-01-30T18:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>EMBEDDING</b> AND PARAMETERIZATION", "url": "https://groups.csail.mit.edu/gdpgroup/assets/6838_spring_2021/chapter7.pdf", "isFamilyFriendly": true, "displayUrl": "https://groups.csail.mit.edu/gdpgroup/assets/6838_spring_2021/chapter7.pdf", "snippet": "We can think of an <b>embedding</b> as a <b>map</b> f: M !M0. In this case, the strongest condition we can put onto f is that it completely preserves metric <b>space</b> structure: De\ufb01nition 7.2 (Isometry). A <b>map</b> f: M !M0between metric spaces (M,d) and (M0,d0) is an isometry if it preserves pairwise distances. That is, for all x,y 2M we have d(x,y) = d0(f(x),f(y)).", "dateLastCrawled": "2021-06-23T06:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "All you need to know about Graph Embeddings", "url": "https://analyticsindiamag.com/all-you-need-to-know-about-graph-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/all-you-need-to-know-about-graph-<b>embeddings</b>", "snippet": "We just need to <b>map</b> these structures in <b>space</b> and calculate the information. We can also understand the graph <b>embedding</b> using the following points: Graph embeddings are a type of data structure that is mainly used to compare the data structures (similar or not). We use it for compressing the complex and large graph data using the information in the vertices and edges and vertices around the main vertex. We use machine learning methods for calculating the graph embeddings. We can think of ...", "dateLastCrawled": "2022-02-02T02:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "algebraic geometry - <b>Segre map is an embedding</b> - <b>Mathematics Stack Exchange</b>", "url": "https://math.stackexchange.com/questions/3683364/segre-map-is-an-embedding", "isFamilyFriendly": true, "displayUrl": "https://math.stackexchange.com/questions/3683364/<b>segre-map-is-an-embedding</b>", "snippet": "1 Answer1. Show activity on this post. Being bijective is not enough to be an <b>embedding</b>, but what you&#39;ve done is: You have constructed an inverse morphism from \u03a3 m, n to P m \u00d7 P n, showing that the Segre <b>map</b> is an isomorphism onto its image, which is what it means to be an <b>embedding</b>. No, it is definitely not the smallest number.", "dateLastCrawled": "2022-01-25T21:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Neural Network Embeddings Explained | by Will Koehrsen | Towards Data ...", "url": "https://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/neural-network-<b>embeddings</b>-explained-4d028e6f0526", "snippet": "Embeddings. An <b>embedding</b> is a mapping of a discrete \u2014 categorical \u2014 variable to a vector of continuous numbers. In the context of neural networks, embeddings are low-dimensional, learned continuous vector representations of discrete variables. Neural network embeddings are useful because they can reduce the dimensionality of categorical variables and meaningfully represent categories in the transformed <b>space</b>.. Neural network embeddings have 3 primary purposes:", "dateLastCrawled": "2022-02-03T06:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 6, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Embedding</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Embedding", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Embedding</b>", "snippet": "The fact that a <b>map</b> f : X \u2192 Y is an <b>embedding</b> is often indicated by the use of a &quot;hooked arrow&quot; (U+21AA \u21aa RIGHTWARDS ARROW WITH HOOK); thus: :. (On the other hand, this notation is sometimes reserved for inclusion maps.) Given X and Y, several different embeddings of X in Y may be possible. In many cases of interest there is a standard (or &quot;canonical&quot;) <b>embedding</b>, <b>like</b> those of the natural numbers in the integers, the integers in the rational numbers, the rational numbers in the real ...", "dateLastCrawled": "2022-02-02T15:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Embedding</b> to non-Euclidean spaces \u2014 <b>umap</b> 0.5 documentation", "url": "https://umap-learn.readthedocs.io/en/latest/embedding_space.html", "isFamilyFriendly": true, "displayUrl": "https://<b>umap</b>-learn.readthedocs.io/en/latest/<b>embedding</b>_<b>space</b>.html", "snippet": "<b>Embedding</b> on a Custom Metric <b>Space</b>\u00b6. What if you have some other custom notion of a metric <b>space</b> that you would <b>like</b> to embed data into? In the same way that <b>UMAP</b> can support custom written distance metrics for the input data (as long as they can be compiled with numba), the output_metric parameter can accept custom distance functions. One catch is that, to support gradient descent optimization, the distance function needs to return both the distance, and a vector for the gradient of the ...", "dateLastCrawled": "2022-01-30T00:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "RStudio AI Blog: <b>Word Embeddings with Keras</b>", "url": "https://blogs.rstudio.com/ai/posts/2017-12-22-word-embeddings-with-keras/", "isFamilyFriendly": true, "displayUrl": "https://blogs.rstudio.com/ai/posts/2017-12-22-<b>word-embeddings-with-keras</b>", "snippet": "Word <b>embedding</b> is a method used to <b>map</b> words of a vocabulary to dense vectors of real numbers where semantically similar words are mapped to nearby points. Representing words in this vector <b>space</b> help algorithms achieve better performance in natural language processing tasks <b>like</b> syntactic parsing and sentiment analysis by grouping similar words. For example, we expect that in the <b>embedding</b> <b>space</b> \u201ccats\u201d and \u201cdogs\u201d are mapped to nearby points since they are both animals, mammals, pets ...", "dateLastCrawled": "2022-02-02T20:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What do <b>we intuitively mean by embedding</b> a manifold into a higher ...", "url": "https://www.quora.com/What-do-we-intuitively-mean-by-embedding-a-manifold-into-a-higher-dimensional-space-Can-you-give-some-examples", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-do-<b>we-intuitively-mean-by-embedding</b>-a-manifold-into-a...", "snippet": "Answer (1 of 4): Sure. Here&#39;s an example: The circle is a 1-dimensional manifold, and here it is embedded in the plane, which is the 2-dimensional Euclidean <b>space</b>. Here&#39;s a circle embedded in 3-<b>space</b>: of course it is pictured here on a flat screen, but you can use your imagination to imagine a...", "dateLastCrawled": "2022-01-20T17:42:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Embedding Space</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/computer-science/embedding-space", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/computer-science/<b>embedding-space</b>", "snippet": "If the CNN has learned correctly <b>to map</b> images to the text <b>embedding space</b>, the distances between the embeddings of the images and the texts of a pair should be <b>similar</b>, and points in the plot should fall around the identity line y = x. Also, if the learned <b>space</b> has a semantic structure, both the distance between images embeddings and the distance between texts embeddings should be smaller for those pairs sharing more tags: The plot points&#39; color reflects the number of common tags of the ...", "dateLastCrawled": "2022-01-20T05:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Embeddings | Machine Learning Crash Course | <b>Google Developers</b>", "url": "https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/machine-learning/crash-course/<b>embedding</b>", "snippet": "An <b>embedding</b> is a relatively low-dimensional <b>space</b> into which you can translate high-dimensional vectors. Embeddings make it easier to do machine learning on large inputs like sparse vectors representing words. Ideally, an <b>embedding</b> captures some of the semantics of the input by placing semantically <b>similar</b> inputs close together in the <b>embedding</b> <b>space</b>.", "dateLastCrawled": "2022-01-30T18:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Neural Network Embeddings Explained | by Will Koehrsen | Towards Data ...", "url": "https://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/neural-network-<b>embeddings</b>-explained-4d028e6f0526", "snippet": "Embeddings. An <b>embedding</b> is a mapping of a discrete \u2014 categorical \u2014 variable to a vector of continuous numbers. In the context of neural networks, embeddings are low-dimensional, learned continuous vector representations of discrete variables. Neural network embeddings are useful because they can reduce the dimensionality of categorical variables and meaningfully represent categories in the transformed <b>space</b>.. Neural network embeddings have 3 primary purposes:", "dateLastCrawled": "2022-02-03T06:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "All you need to know about Graph Embeddings", "url": "https://analyticsindiamag.com/all-you-need-to-know-about-graph-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/all-you-need-to-know-about-graph-<b>embeddings</b>", "snippet": "We just need <b>to map</b> these structures in <b>space</b> and calculate the information. We can also understand the graph <b>embedding</b> using the following points: Graph embeddings are a type of data structure that is mainly used to compare the data structures (<b>similar</b> or not). We use it for compressing the complex and large graph data using the information in the vertices and edges and vertices around the main vertex. We use machine learning methods for calculating the graph embeddings. We can think of ...", "dateLastCrawled": "2022-02-02T02:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Embedding</b> in Machine Learning | Cathy\u2019s Notes", "url": "https://cathyqian.github.io/2020/08/27/Embedding_In_Machine_Learning.html", "isFamilyFriendly": true, "displayUrl": "https://cathyqian.github.io/2020/08/27/<b>Embedding</b>_In_Machine_Learning.html", "snippet": "<b>Embedding</b> in the context of deep learning is <b>to map</b> high-dimensional vectors or categorical variables to relatively low-dimensional <b>space</b> so that <b>similar</b> items are close to each other. It can be applied to any high dimensional, sparse or categorical features, i.e., IP addresses from ad impressions, raw pixels in a video, audio data from speech ...", "dateLastCrawled": "2022-01-22T06:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Embedding</b> to non-Euclidean spaces \u2014 <b>umap</b> 0.5 documentation", "url": "https://umap-learn.readthedocs.io/en/latest/embedding_space.html", "isFamilyFriendly": true, "displayUrl": "https://<b>umap</b>-learn.readthedocs.io/en/latest/<b>embedding</b>_<b>space</b>.html", "snippet": "We see that we have gotten a result <b>similar</b> to a standard <b>embedding</b> into euclidean <b>space</b>, but with less clear clustering, and more points between clusters. To get a clearer idea of what is going on it will be necessary to devise a means to display some of the extra information contained in the extra 3 dimensions providing covariance data. To do this it will be helpful to be able to draw ellipses corresponding to super-level sets of the PDF of the 2d Gaussian. We can start on this by writing ...", "dateLastCrawled": "2022-01-30T00:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Graph <b>Embedding</b>: Understanding Graph <b>Embedding</b> Algorithms", "url": "https://www.tigergraph.com/blog/understanding-graph-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://www.tigergraph.com/blog/<b>understanding-graph-embeddings</b>", "snippet": "Items that are near each other in this <b>embedding</b> <b>space</b> are considered <b>similar</b> to each other in the real world. Embeddings focus on performance, not explainability. Embeddings are ideal for \u201cfuzzy\u201d match problems. If you have hundreds or thousands of lines of complex if-then statements to build cohorts, <b>graph embeddings</b> provide a way to make this code much smaller and easier to maintain. <b>Graph embeddings</b> work with other graph algorithms. If you are doing clustering or classification ...", "dateLastCrawled": "2022-02-03T02:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "RStudio AI Blog: <b>Word Embeddings with Keras</b>", "url": "https://blogs.rstudio.com/ai/posts/2017-12-22-word-embeddings-with-keras/", "isFamilyFriendly": true, "displayUrl": "https://blogs.rstudio.com/ai/posts/2017-12-22-<b>word-embeddings-with-keras</b>", "snippet": "Word <b>embedding</b> is a method used <b>to map</b> words of a vocabulary to dense vectors of real numbers where semantically <b>similar</b> words are mapped to nearby points. Representing words in this vector <b>space</b> help algorithms achieve better performance in natural language processing tasks like syntactic parsing and sentiment analysis by grouping <b>similar</b> words. For example, we expect that in the <b>embedding</b> <b>space</b> \u201ccats\u201d and \u201cdogs\u201d are mapped to nearby points since they are both animals, mammals, pets ...", "dateLastCrawled": "2022-02-02T20:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Embeddings: Translating to a Lower-Dimensional <b>Space</b> | Machine Learning ...", "url": "https://developers.google.com/machine-learning/crash-course/embeddings/translating-to-a-lower-dimensional-space", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/.../<b>embeddings</b>/translating-to-a-lower-dimensional-<b>space</b>", "snippet": "As you can see from the paper exercises, even a small multi-dimensional <b>space</b> provides the freedom to group semantically <b>similar</b> items together and keep dissimilar items far apart. Position (distance and direction) in the <b>vector</b> <b>space</b> can encode semantics in a good <b>embedding</b>. For example, the following visualizations of real embeddings show geometrical relationships that capture semantic relations like the relation between a country and its capital:", "dateLastCrawled": "2022-02-02T04:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "tensorflow - How to train a reverse <b>embedding</b>, like vec2word? - Stack ...", "url": "https://stackoverflow.com/questions/43515400/how-to-train-a-reverse-embedding-like-vec2word", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/43515400", "snippet": "The example I&#39;m interested in is where the vector representation is the output of a word2vec <b>embedding</b>, and I&#39;d like <b>to map</b> onto the the individual words which were in the language used to train the <b>embedding</b>, so I guess this is vec2word? In a bit more detail; if I understand correctly, a cluster of points in embedded <b>space</b> represents <b>similar</b> words. Thus if you sample from points in that cluster, and use it as the input to vec2word, the output should be a mapping to <b>similar</b> individual words ...", "dateLastCrawled": "2022-01-17T21:47:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Embedding Space</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/computer-science/embedding-space", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/computer-science/<b>embedding-space</b>", "snippet": "If the CNN has learned correctly to <b>map</b> images to the text <b>embedding space</b>, the distances between the embeddings of the images and the texts of a pair should be similar, and points in the plot should fall around the identity line y = x. Also, if the learned <b>space</b> has a semantic structure, both the distance between images embeddings and the distance between texts embeddings should be smaller for those pairs sharing more tags: The plot points&#39; color reflects the number of common tags of the ...", "dateLastCrawled": "2022-01-20T05:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Graph <b>Embedding</b>: Understanding Graph <b>Embedding</b> Algorithms", "url": "https://www.tigergraph.com/blog/understanding-graph-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://www.tigergraph.com/blog/<b>understanding-graph-embeddings</b>", "snippet": "There may not be \u201csemantics\u201d or meaning associated with each number in an <b>embedding</b>. Embeddings <b>can</b> <b>be thought</b> of as a low-dimensional representation of an item in a vector <b>space</b>. Items that are near each other in this <b>embedding</b> <b>space</b> are considered similar to each other in the real world. Embeddings focus on performance, not explainability. Embeddings are ideal for \u201cfuzzy\u201d match problems. If you have hundreds or thousands of lines of complex if-then statements to build cohorts ...", "dateLastCrawled": "2022-02-03T02:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Document <b>Embedding</b> Techniques. A review of notable literature on the ...", "url": "https://towardsdatascience.com/document-embedding-techniques-fed3e7a6a25d", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/document-<b>embedding</b>-techniques-fed3e7a6a25d", "snippet": "Document <b>embedding</b> approaches. A possible way to <b>map</b> the field is into the following four prominent approaches: ... This, of course, <b>can</b> be taken as an <b>embedding</b> <b>space</b> for these documents, and \u2014 depending on the choice of K \u2014 it <b>can</b> be of a significantly smaller dimension than vocabulary-based ones. Indeed, while a main use case for LDA is unsupervised topic/community discovery, other cases include the use of the resulting latent topic <b>space</b> as an <b>embedding</b> <b>space</b> for the document corpus ...", "dateLastCrawled": "2022-02-03T01:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Where are we in <b>embedding</b> spaces? A Comprehensive Analysis on Network ...", "url": "https://deepai.org/publication/where-are-we-in-embedding-spaces-a-comprehensive-analysis-on-network-embedding-approaches-for-recommender-systems", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/where-are-we-in-<b>embedding</b>-<b>spaces</b>-a-comprehensive...", "snippet": "Meanwhile, hyperbolic <b>space</b> <b>can</b> <b>be thought</b> as a continuous version of trees. Therefore, such networks are consistent with hyperbolic <b>space</b> due to their analogous structure, and <b>can</b> be naturally modeled by hyperbolic <b>space</b> with a much lower distortion compared to Euclidean <b>space</b>. Although hyperbolic embeddings are gaining great attention for recommender systems nowadays (liu2019hyperbolic; wang2019hyperbolic; chami2019hyperbolic; chamberlain2019scalable), it is not clear under what ...", "dateLastCrawled": "2022-01-17T00:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Spectral <b>Embedding</b> and Locally Linear <b>Embedding</b>", "url": "http://www.mustafahajij.com/wp-content/uploads/2018/07/Locally-Linear-Embedding-and-Spectral-Embedding.pdf", "isFamilyFriendly": true, "displayUrl": "www.mustafahajij.com/wp-content/uploads/2018/07/Locally-Linear-<b>Embedding</b>-and-Spectral...", "snippet": "Consider the digit dataset. This dataset <b>can</b> <b>be thought</b> of as a high-dimensional data with =64. So every image <b>can</b> <b>be thought</b> of as a vector =[ 1,\u2026, 64] Spectral <b>embedding</b> assigns to the point x new coordinates =[ 1,\u2026, \ud835\udc58]where \ud835\udc58\u226464. Usually we choose &lt;&lt;\ud835\udc58. In the example above we choose \ud835\udc58=2.", "dateLastCrawled": "2021-09-17T11:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>What is an Embedding Layer</b>? - GDCoder", "url": "https://gdcoder.com/what-is-an-embedding-layer/", "isFamilyFriendly": true, "displayUrl": "https://gdcoder.com/<b>what-is-an-embedding-layer</b>", "snippet": "Generally speaking, we use an <b>embedding</b> layer to compress the input feature <b>space</b> into a smaller one. Imagine that we have 80,000 unique words in a text classification problem and we select to preprocess the text and create a term document matrix. This matrix will be sparse and a sequence of the sequence [&#39;i&#39;, &#39;love&#39;, &#39;you&#39;] is a 80,000-dimensional vector that is all zeros except from 3 elements that correspond to those words. In the case, we pass this matrix as input to the model it will ...", "dateLastCrawled": "2022-02-02T14:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Embedding</b> Story Maps in websites and blogs", "url": "https://www.esri.com/arcgis-blog/products/arcgis-online/mapping/embedding-story-maps-in-websites-and-blogs/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.esri.com</b>/<b>arcgis-blog</b>/products/arcgis-online/<b>map</b>ping/<b>embedding</b>-story-<b>map</b>s...", "snippet": "Many story maps are intended for standalone use in a browser, but they <b>can</b> also be embedded in a website or blog. <b>Embedding</b> any story <b>map</b> is easy, follow these steps to learn how. * Embed your Story <b>Map</b>. Story Maps <b>can</b> be embedded using an iFrame tag, but the needed HTML is automatically generated for you, so you don\u2019t even need to know what an iFrame is. Step 1: Sign in to Story Maps. Go to the story maps website at storymaps.arcgis.com and sign in to your account. Look for Sign In at the ...", "dateLastCrawled": "2022-01-31T16:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "general topology - <b>Embedding Euclidean Space Into Real Projective Space</b> ...", "url": "https://math.stackexchange.com/questions/3829022/embedding-euclidean-space-into-real-projective-space", "isFamilyFriendly": true, "displayUrl": "https://math.stackexchange.com/questions/3829022/<b>embedding-euclidean-space-into-real</b>...", "snippet": "<b>Embedding Euclidean Space Into Real Projective Space</b>. Ask Question Asked 1 year, 2 months ago. Active 1 year, 2 months ago. Viewed 58 times 2 $\\begingroup$ I&#39;m struggling with what I think should be a pretty straightforward proof. ...", "dateLastCrawled": "2021-12-06T00:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "differential geometry - Image of <b>Embedding</b> on a Manifold is a ...", "url": "https://math.stackexchange.com/questions/3332499/image-of-embedding-on-a-manifold-is-a-submanifold", "isFamilyFriendly": true, "displayUrl": "https://math.stackexchange.com/questions/3332499/image-of-<b>embedding</b>-on-a-manifold-is-a...", "snippet": "$\\begingroup$ actually wait I&#39;m confused by your question, did you phrase it correctly? because an <b>embedding</b> is by definition a homeomorphism and hence trivially an open <b>map</b> onto its mage. So, could you clarify whether what you actually wrote is what you intended? I feel like my answer doesnt answer your question as stated, but perhaps might answer what you actually intended to ask $\\endgroup$ \u2013 peek-a-boo", "dateLastCrawled": "2022-01-18T04:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Visualizing feature <b>vectors</b>/embeddings using t-SNE and PCA | by ...", "url": "https://towardsdatascience.com/visualizing-feature-vectors-embeddings-using-pca-and-t-sne-ef157cea3a42", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/visualizing-feature-<b>vectors</b>-<b>embeddings</b>-using-pca-and-t...", "snippet": "Visualization is a very powerful tool and <b>can</b> provide invaluable information. In this post, I\u2019ll be discussing two very powerful techniques that <b>can</b> help you visualise higher dimensional data in a lower-dimensional <b>space</b> to find trends and patterns, namely PCA and t-SNE. We will be taking a CNN based example and inject noise in the test dataset to do our visualization investigation. Introduction. I\u2019ll briefly talk about the two techniques before diving into how to use them. Principal ...", "dateLastCrawled": "2022-02-03T13:36:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Embedding Space</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/computer-science/embedding-space", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/computer-science/<b>embedding-space</b>", "snippet": "If the CNN has learned correctly <b>to map</b> images to the text <b>embedding space</b>, the distances between the embeddings of the images and the texts of a pair should be similar, and points in the plot should fall around the identity line y = x. Also, if the learned <b>space</b> has a semantic structure, both the distance between images embeddings and the distance between texts embeddings should be smaller for those pairs sharing more tags: The plot points&#39; color reflects the number of common tags of the ...", "dateLastCrawled": "2022-01-20T05:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Cross-Lingual BERT Contextual <b>Embedding</b> <b>Space</b> Mapping with Isotropic ...", "url": "https://deepai.org/publication/cross-lingual-bert-contextual-embedding-space-mapping-with-isotropic-and-isometric-conditions", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/cross-lingual-bert-contextual-<b>embedding</b>-<b>space</b>-<b>map</b>ping...", "snippet": "An <b>embedding</b> <b>space</b> is isotropic if the directions of <b>embedding</b> vectors are uniformly distributed. Unfortunately, contextual word representation is usually anisotropic. Geometric speaking, normalized word <b>embedding</b> vectors are more likely to gather on a narrow conical surface of a hypersphere rather than uniformly distributed in all directions. Commonly, various language vector spaces have various degrees of anisotropy. Figure 2(a) illustrates the different size of areas that contextual ...", "dateLastCrawled": "2021-12-26T07:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Embedding</b> in Machine Learning | Cathy\u2019s Notes", "url": "https://cathyqian.github.io/2020/08/27/Embedding_In_Machine_Learning.html", "isFamilyFriendly": true, "displayUrl": "https://cathyqian.github.io/2020/08/27/<b>Embedding</b>_In_Machine_Learning.html", "snippet": "What is <b>Embedding</b>? <b>Embedding</b> in the context of deep learning is <b>to map</b> high-dimensional vectors or categorical variables to relatively low-dimensional <b>space</b> so that similar items are close to each other. It <b>can</b> be applied to any high dimensional, sparse or categorical features, i.e., IP addresses from ad impressions, raw pixels in a video, audio data from speech, texts from a job description.", "dateLastCrawled": "2022-01-22T06:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Many-to-<b>Many Graph Matching via Metric Embedding</b>", "url": "http://facweb.cs.depaul.edu/research/vc/publications/ykasfdsd2003cvpr.pdf", "isFamilyFriendly": true, "displayUrl": "facweb.cs.depaul.edu/research/vc/publications/ykasfdsd2003cvpr.pdf", "snippet": "a single <b>embedding</b> that <b>can</b> <b>map</b> each graph to the same vector <b>space</b>, in which the two embeddings <b>can</b> be directly <b>compared</b>. However, in general, this is not possible without introducing unacceptable distortion. We will therefore tackle the problem in two steps. First, we will seek low-distortion embeddings J that <b>map</b> sets to normedspaces J 3 , LM ON: &gt; QPSR. Next, we will align the normed spaces, so that the embeddings <b>can</b> be directly <b>compared</b>. Using these mappings, the problem of many-to ...", "dateLastCrawled": "2021-09-28T01:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Cross-modal semantic autoencoder with <b>embedding</b> consensus | Scientific ...", "url": "https://www.nature.com/articles/s41598-021-92750-7", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41598-021-92750-7", "snippet": "The methods are <b>compared</b> on the WIKI dataset. It <b>can</b> be ... We <b>map</b> the datasets to an <b>embedding</b> <b>space</b>, learn projections by multi-modal semantic autoencoder and reconstruct original features. \\((V ...", "dateLastCrawled": "2022-01-26T15:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Document <b>Embedding</b> Techniques. A review of notable literature on the ...", "url": "https://towardsdatascience.com/document-embedding-techniques-fed3e7a6a25d", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/document-<b>embedding</b>-techniques-fed3e7a6a25d", "snippet": "Document <b>embedding</b> approaches. A possible way <b>to map</b> the field is into the following four prominent approaches: Summarizing word vectors This is the classic approach. Bag-of-words does exactly this for one-hot word vectors, and the various weighing schemes you <b>can</b> apply to it are variations on this way to summarizing word vectors. However, this approach is also valid when used with the most state-of-the-art word representations (usually by averaging instead of summing), especially when word ...", "dateLastCrawled": "2022-02-03T01:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "pca - What is <b>embedding</b>? (in <b>the context of dimensionality reduction</b> ...", "url": "https://stats.stackexchange.com/questions/487545/what-is-embedding-in-the-context-of-dimensionality-reduction", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/487545/what-is-<b>embedding</b>-in-the-context-of...", "snippet": "An <b>embedding</b> is a relatively low-dimensional <b>space</b> [subspace] into which you <b>can</b> translate high-dimensional vectors. Embeddings make it easier to do machine learning on large inputs like sparse vectors representing words. Ideally, an <b>embedding</b> captures some of the semantics of the input by placing semantically similar inputs close together in the <b>embedding</b> <b>space</b>", "dateLastCrawled": "2022-01-23T04:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Entity embedding using t-SNE</b>. The well known dimensionality reduction ...", "url": "https://towardsdatascience.com/entity-embedding-using-t-sne-973cb5c730d7", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>entity-embedding-using-t-sne</b>-973cb5c730d7", "snippet": "The goal of such <b>embedding</b> is <b>to map</b> categorical features to vectors in a low dimensional <b>space</b>. The advantage is this mapping dramatically reduce overfitting, <b>compared</b> to 1-hot encoding. However we <b>can</b> lose information and make the learning more difficult if the <b>embedding</b> is chosen incorrectly. In order to increase the quality of <b>embedding</b>, we use the category similarity information (that we <b>can</b> set a priori or by computing the similarity of conditional probability distribution). Kernel PCA ...", "dateLastCrawled": "2022-02-03T03:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Diffusion maps <b>embedding</b> and transition matrix analysis of the large ...", "url": "https://iopscience.iop.org/article/10.1088/1361-6544/ab6a76/pdf", "isFamilyFriendly": true, "displayUrl": "https://iopscience.iop.org/article/10.1088/1361-6544/ab6a76/pdf", "snippet": "geometric features in the <b>embedding</b> <b>space</b> spanned by the two dominant diffusion-maps eigenvectors. From this two-dimensional <b>embedding</b> we <b>can</b> measure azimuthal drift and diffusion rates, as well as coherence times of the LSC. In addition, we <b>can</b> distinguish from the data clearly the single roll state (SRS), when a single roll extends through the whole cell, from the double roll state (DRS), when two counter-rotating rolls are on top of each other. Based on this <b>embedding</b> we also build a ...", "dateLastCrawled": "2021-07-19T09:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Funding <b>map</b> using paragraph <b>embedding</b> based on semantic diversity ...", "url": "https://link.springer.com/article/10.1007/s11192-018-2783-x", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11192-018-2783-x", "snippet": "A time bar on the left side represents the start year of the projects, and the <b>map</b> <b>can</b> move along the time bar. Since each node is coupled with a publication month/year, we <b>can</b> zoom in the specified time window to detect emerging trends. Comparison with the baseline on the unclustered problem. Figures 1 and 4 illustrate the FP7 maps obtained by the baseline and the proposed methods with the same hyperparameters, respectively. Footnote 15 In Fig. 1, project nodes were scattered, and thus when ...", "dateLastCrawled": "2021-12-11T17:57:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Finding the Word <b>Analogy</b> from given words using Word2Vec embeddings ...", "url": "https://www.geeksforgeeks.org/finding-the-word-analogy-from-given-words-using-word2vec-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/finding-the-word-<b>analogy</b>-from-given-words-using-word2vec...", "snippet": "In the word <b>analogy</b> task, ... What if we can use a <b>Machine</b> <b>Learning</b> algorithm to automate this task of finding the word <b>analogy</b>. In this tutorial, we will be using Word2Vec model and a pre-trained model named \u2018GoogleNews-vectors-negative300.bin\u2018 which is trained on over 50 Billion words by Google. Each word inside the pre-trained dataset is embedded in a 300-dimensional <b>space</b> and the words which are similar in context/meaning are placed closer to each other in the <b>space</b>. Methodology to ...", "dateLastCrawled": "2022-01-26T14:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Analogical <b>Embedding</b> for <b>Analogy</b>-Based <b>Learning</b> to Rank", "url": "https://link.springer.com/chapter/10.1007/978-3-030-74251-5_7", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/chapter/10.1007/978-3-030-74251-5_7", "snippet": "A suitable representation is therefore essential for the success of <b>analogy</b>-based <b>learning</b> to rank. Therefore, we propose a method for analogical <b>embedding</b>, i.e., for <b>embedding</b> the data in a target <b>space</b> such that, in this <b>space</b>, the aforementioned <b>analogy</b> assumption is as valid and strongly pronounced as possible. This is accomplished by means of a neural network with a quadruple Siamese structure, which is trained on a suitably designed set of examples in the form of quadruples of objects ...", "dateLastCrawled": "2022-01-17T09:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Analogical <b>Embedding</b> for <b>Analogy</b>-Based <b>Learning</b> to Rank", "url": "https://homepages.uni-paderborn.de/ahmadim/IDA%202021.pdf", "isFamilyFriendly": true, "displayUrl": "https://homepages.uni-paderborn.de/ahmadim/IDA 2021.pdf", "snippet": "7 Intelligent Systems and <b>Machine</b> <b>Learning</b> <b>Embedding</b> By ignoring irrelevant or noisy features, the performance can often be improved Common feature selection techniques tailored for the case of <b>analogy</b>-based <b>learning</b> to rank. <b>Analogy</b>-based <b>learning</b> to rank (able2rank) 8 Intelligent Systems and <b>Machine</b> <b>Learning</b> Extension to feature vectors Degree of <b>analogy</b>. Analogical <b>Embedding</b> 9 Intelligent Systems and <b>Machine</b> <b>Learning</b> Positive example: preferences on both sides are coherent Negative ...", "dateLastCrawled": "2022-01-06T10:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Towards Analogy-Based Explanations in Machine Learning</b> | DeepAI", "url": "https://deepai.org/publication/towards-analogy-based-explanations-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>towards-analogy-based-explanations-in-machine-learning</b>", "snippet": "More specifically, we take the view that an <b>analogy</b>-based approach is a viable alternative to existing approaches in the realm of explainable AI and interpretable <b>machine</b> <b>learning</b>, and that <b>analogy</b>-based explanations of the predictions produced by a <b>machine</b> <b>learning</b> algorithm can complement similarity-based explanations in a meaningful way. To corroborate these claims, we outline the basic idea of an <b>analogy</b>-based explanation and illustrate its potential usefulness by means of some examples.", "dateLastCrawled": "2022-01-10T12:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Classifying and completing word analogies by <b>machine</b> <b>learning</b> ...", "url": "https://www.sciencedirect.com/science/article/abs/pii/S0888613X21000141", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/abs/pii/S0888613X21000141", "snippet": "With the emergence of word <b>embedding</b> models, a lot of progress has been made in NLP, essentially assuming that a word <b>analogy</b> like m a n: k i n g:: w o m a n: q u e e n is an instance of a parallelogram within the underlying vector <b>space</b>. In this paper, we depart from this assumption to adopt a <b>machine</b> <b>learning</b> approach, i.e., <b>learning</b> a substitute of the parallelogram model. To achieve our goal, we first review the formal modeling of analogical proportions, highlighting the properties which ...", "dateLastCrawled": "2021-11-13T01:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "How to Solve <b>Analogies</b> with Word2Vec | by Khuyen Tran | Towards Data ...", "url": "https://towardsdatascience.com/how-to-solve-analogies-with-word2vec-6ebaf2354009", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/how-to-solve-<b>analogies</b>-with-word2vec-6ebaf2354009", "snippet": "To make words understood by <b>machine</b> <b>learning</b> algorithms, word <b>embedding</b> is used to map words into vectors of real numbers. There are various word <b>embedding</b> models and word2vec is one of them. In simple words, word2vec is a group of related models that are used to produce word embeddings. These models are trained to construct the linguistic contexts of words. Word2vec takes a large corpus of text and produces a vector <b>space</b>, with each unique word in the corpus being assigned to a ...", "dateLastCrawled": "2022-01-30T16:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Graph <b>Embedding</b> for Deep <b>Learning</b> | by Flawnson Tong | Towards Data Science", "url": "https://towardsdatascience.com/overview-of-deep-learning-on-graph-embeddings-4305c10ad4a4", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/overview-of-deep-<b>learning</b>-on-graph-<b>embeddings</b>-4305c10ad4a4", "snippet": "Using an <b>analogy</b> with word2vec, if a document is made of sentences (which is then made of words), then a graph is made of sub-graphs ... Graph <b>embedding</b> techniques take graphs and embed them in a lower dimensional continuous latent <b>space</b> before passing that representation through a <b>machine</b> <b>learning</b> model. Walk <b>embedding</b> methods perform graph traversals with the goal of preserving structure and features and aggregates these traversals which can then be passed through a recurrent neural ...", "dateLastCrawled": "2022-02-01T16:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "In the word <b>embedding</b> <b>space</b>, the <b>analogy</b> pairs exhibit interesting ...", "url": "https://www.researchgate.net/figure/In-the-word-embedding-space-the-analogy-pairs-exhibit-interesting-algebraic_fig1_319370400", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/figure/In-the-word-<b>embedding</b>-<b>space</b>-the-<b>analogy</b>-pairs...", "snippet": "Download scientific diagram | In the word <b>embedding</b> <b>space</b>, the <b>analogy</b> pairs exhibit interesting algebraic relationships. from publication: Visual Exploration of Semantic Relationships in Neural ...", "dateLastCrawled": "2021-12-21T11:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>What Are Word Embeddings</b> for Text? - <b>Machine</b> <b>Learning</b> Mastery", "url": "https://machinelearningmastery.com/what-are-word-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>what-are-word-embeddings</b>", "snippet": "This approach of <b>learning</b> an <b>embedding</b> layer requires a lot of training data and can be slow, but will learn an <b>embedding</b> both targeted to the specific text data and the NLP task. 2. Word2Vec. Word2Vec is a statistical method for efficiently <b>learning</b> a standalone word <b>embedding</b> from a text corpus. It was developed by Tomas Mikolov, et al. at Google in 2013 as a response to make the neural-network-based training of the <b>embedding</b> more efficient and since then has become the de facto standard ...", "dateLastCrawled": "2022-01-30T18:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Word Embeddings in NLP - GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/word-embeddings-in-nlp/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/word-<b>embeddings</b>-in-nlp", "snippet": "Word <b>Embedding</b> or Word Vector is a numeric vector input that represents a word in a lower-dimensional <b>space</b>. It allows words with similar meaning to have a similar representation. They can also approximate meaning. A word vector with 50 values can represent 50 unique features. Features: Anything that relates words to one another. Eg: Age, Sports, Fitness, Employed etc. Each word vector has values corresponding to these features. Goal of Word Embeddings. To reduce dimensionality; To use a ...", "dateLastCrawled": "2022-01-30T08:38:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Zero-shot <b>learning</b> via discriminative representation extraction ...", "url": "https://www.sciencedirect.com/science/article/pii/S0167865517303501", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0167865517303501", "snippet": "The pioneer work in ZSL can be traced to Larochelle et al. , where it verified that when test images belong to some classes that are not available at training stage, a <b>machine</b> <b>learning</b> system can still figure out what a test image is. Due to the importance of zero-shot <b>learning</b>, the number of proposed approaches has increased steadily recently.The number of new zero-shot <b>learning</b> approaches proposed every year was increasing.", "dateLastCrawled": "2021-10-30T07:08:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A self-supervised domain-general <b>learning</b> framework for human ventral ...", "url": "https://www.nature.com/articles/s41467-022-28091-4", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41467-022-28091-4", "snippet": "On this view, the <b>embedding space can be thought of as</b> a high-fidelity perceptual interface, with useful visual primitives over which separate conceptual representational systems can operate.", "dateLastCrawled": "2022-01-25T10:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Spectral Af\ufb01ne-Kernel Embeddings</b> - NSF", "url": "https://par.nsf.gov/servlets/purl/10039348", "isFamilyFriendly": true, "displayUrl": "https://par.nsf.gov/servlets/purl/10039348", "snippet": "Since <b>machine</b> <b>learn-ing</b> algorithms struggle with high dimensions (an issue known as the curse of dimensionality in this context), one typically needs to map these data points from their high-dimensional space into a lower dimensional space without signi\ufb01cant distortion. Mapping data (living in RD with D\u02db1 but sampling a manifold of low in-trinsic dimensionality d \u02ddD) into a low-dimensional <b>embedding space can be thought of as</b> a preliminary feature extraction step in <b>machine</b> <b>learning</b> ...", "dateLastCrawled": "2022-01-29T21:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Predicting affinity ties in a surname network", "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0256603", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0256603", "snippet": "<b>Machine</b> <b>learning</b>-based approaches for knowledge graph completion To cover the broadest possible range of methods and architectures in the evaluation, we identified representative methods of different model families, taking care that these methods achieve state-of-the-art performances in knowledge graph completion and have open-source implementations that favor the reproducibility of the reported results.", "dateLastCrawled": "2021-09-03T03:18:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(embedding space)  is like +(map)", "+(embedding space) is similar to +(map)", "+(embedding space) can be thought of as +(map)", "+(embedding space) can be compared to +(map)", "machine learning +(embedding space AND analogy)", "machine learning +(\"embedding space is like\")", "machine learning +(\"embedding space is similar\")", "machine learning +(\"just as embedding space\")", "machine learning +(\"embedding space can be thought of as\")", "machine learning +(\"embedding space can be compared to\")"]}