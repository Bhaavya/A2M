{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>machine</b> <b>learning</b> - Why do we want an objective <b>function</b> to be a <b>convex</b> ...", "url": "https://stats.stackexchange.com/questions/219899/why-do-we-want-an-objective-function-to-be-a-convex-function", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/219899", "snippet": "this figure shows a non-<b>convex</b> <b>function</b> that carries the above property. It seems to me that, as long as the local <b>minimum</b> is the <b>global</b> <b>minimum</b>, we should be <b>able</b> to use gradient-decent (or similar methods) <b>to find</b> the <b>global</b> <b>minimum</b>. So, my question is, why do <b>convex</b> functions get more attention, compared to other functions that also carry ...", "dateLastCrawled": "2022-01-14T20:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Univariate Function Optimization in Python</b> - <b>Machine</b> <b>Learning</b> Mastery", "url": "https://machinelearningmastery.com/univariate-function-optimization-in-python/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>univariate-function-optimization-in-python</b>", "snippet": "Univariate <b>function</b> optimization involves finding the input to a <b>function</b> that results in the optimal output from an objective <b>function</b>. This is a common procedure in <b>machine</b> <b>learning</b> when fitting a model with one parameter or tuning a model that has a single hyperparameter. An efficient <b>algorithm</b> is required to solve optimization problems of ...", "dateLastCrawled": "2022-02-01T10:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Artificial intelligence to deep <b>learning</b>: <b>machine</b> intelligence approach ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8040371/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8040371", "snippet": "AI is an umbrella term where computer programs are <b>able</b> to think and behave as humans do, whereas ML is beyond that where data are inputted in the <b>machine</b> along with an <b>algorithm</b> <b>like</b> Na\u00efve Bayes, decision tree (DT), hidden Markov models (HMM) and others, which helps the <b>machine</b> to learn without being explicitly programmed. Later, with the development of neural networks, machines could classify and organize inputted data that mimics <b>like</b> a human brain, which further shows advancement in AI ...", "dateLastCrawled": "2022-02-02T03:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Coursera <b>Machine</b> <b>Learning</b>: Introduction and Linear Regression ...", "url": "https://david-salazar.github.io/2019/12/26/coursera-machine-learning-week-1/", "isFamilyFriendly": true, "displayUrl": "https://david-salazar.github.io/2019/12/26/courser<b>a-machine</b>-<b>learning</b>-week-1", "snippet": "The <b>machine</b> <b>learning</b> <b>algorithm</b>, then, will be one that given some training examples, we derive a <b>function</b> that performs the classification from the <b>learning</b> space into the output space: \\[ML:\\{(X_n, Y_n)\\} \\subset X \\times Y \\to f\\] Thus, we want our <b>algorithm</b> to be <b>able</b> <b>to find</b> a <b>function</b> given some training examples, but it is remarked from the beginning that the training examples belong to a wider space of all the possible <b>learning</b> space. However, we just don\u2019t want a <b>function</b> that ...", "dateLastCrawled": "2021-11-29T19:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "[<b>MCQ&#39;s] Machine Learning - Last Moment Tuitions</b>", "url": "https://lastmomenttuitions.com/mcqs-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://lastmomenttuitions.com/<b>mcqs-machine-learning</b>", "snippet": "Which of the following is a widely used and effective <b>machine</b> <b>learning</b> <b>algorithm</b> based on the idea of bagging? A. Decision Tree B. Regression C. Classification D. Random Forest Answer : D Explanation: The Radom Forest <b>algorithm</b> builds an ensemble of Decision Trees, mostly trained with the bagging method. 12. <b>To find</b> the <b>minimum</b> or the <b>maximum</b> of a <b>function</b>, we set the gradient to zero because: A. The value of the gradient at extrema of a <b>function</b> is always zero B. Depends on the type of ...", "dateLastCrawled": "2022-02-03T02:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Local Optimization Versus Global Optimization</b> - <b>Machine</b> <b>Learning</b> Mastery", "url": "https://machinelearningmastery.com/local-optimization-versus-global-optimization/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>local-optimization-versus-global-optimization</b>", "snippet": "<b>A global</b> optimum is the extrema (<b>minimum</b> <b>or maximum</b>) of the objective <b>function</b> for the entire input search space. <b>Global</b> optimization, where the <b>algorithm</b> searches for the <b>global</b> optimum by employing mechanisms to search larger parts of the search space. \u2014 Page 37, Computational Intelligence: An Introduction, 2007.", "dateLastCrawled": "2022-02-03T01:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>machine</b> <b>learning</b> - Does <b>gradient descent</b> always converge to an optimum ...", "url": "https://datascience.stackexchange.com/questions/24534/does-gradient-descent-always-converge-to-an-optimum", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/24534", "snippet": "During <b>machine</b> <b>learning</b> era it was considered that <b>gradient descent</b> will <b>find</b> the local/<b>global</b> optimum but in deep <b>learning</b> era where the dimension of input features are too much it is shown in practice that the probability that all of the features be located in there optimal value at a single point is not too much and rather seeing to have optimal locations in cost functions, most of the time saddle points are observed. This is one of the reasons that training with lots of data and training ...", "dateLastCrawled": "2022-01-27T08:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>machine</b> <b>learning</b> - How can stochastic gradient descent avoid the ...", "url": "https://stats.stackexchange.com/questions/90874/how-can-stochastic-gradient-descent-avoid-the-problem-of-a-local-minimum", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/90874", "snippet": "However, the <b>learning</b> rates tend to zero because in this way, when the <b>algorithm</b> is close to the <b>minimum</b> of a <b>convex</b> <b>function</b>, it stops oscillating and converges. The key of the proof of convergence of stochastic gradient are the conditions imposed on the the series of <b>learning</b> rates. See equations (6) and (27) of the original paper of Robbins and Monro.", "dateLastCrawled": "2022-02-01T07:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Machine Learning</b> (Stanford) Coursera (Week 1, Quiz 2) for the github ...", "url": "https://gist.github.com/mGalarnyk/cc964bea99b09e3c733b339ad3b7b019", "isFamilyFriendly": true, "displayUrl": "https://gist.github.com/mGalarnyk/cc964bea99b09e3c733b339ad3b7b019", "snippet": "Gradient descent is likely to get stuck at a local <b>minimum</b> and fail <b>to find</b> the <b>global</b> <b>minimum</b>. For this to be true, we must have \u03b8 0 =0 and \u03b8 1 =0 so that h \u03b8 (x)=0. Our training set can be fit perfectly by a straight line, i.e., all of our training examples lie perfectly on some straight line. Answers:", "dateLastCrawled": "2022-02-02T18:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Why do convex optimization algorithms seem to</b> work well for non-<b>convex</b> ...", "url": "https://www.quora.com/Why-do-convex-optimization-algorithms-seem-to-work-well-for-non-convex-problems-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>Why-do-convex-optimization-algorithms-seem-to</b>-work-well-for-non...", "snippet": "Answer: Simply because there are many local minima points with equivalent low error values. For a <b>convex</b> loss surface you can always <b>find</b> the <b>global</b> <b>minimum</b> by ...", "dateLastCrawled": "2022-01-21T02:00:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>machine</b> <b>learning</b> - Why do we want an objective <b>function</b> to be a <b>convex</b> ...", "url": "https://stats.stackexchange.com/questions/219899/why-do-we-want-an-objective-function-to-be-a-convex-function", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/219899", "snippet": "this figure shows a non-<b>convex</b> <b>function</b> that carries the above property. It seems to me that, as long as the local <b>minimum</b> is the <b>global</b> <b>minimum</b>, we should be <b>able</b> to use gradient-decent (or <b>similar</b> methods) <b>to find</b> the <b>global</b> <b>minimum</b>. So, my question is, why do <b>convex</b> functions get more attention, compared to other functions that also carry ...", "dateLastCrawled": "2022-01-14T20:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Coursera <b>Machine</b> <b>Learning</b>: Introduction and Linear Regression ...", "url": "https://david-salazar.github.io/2019/12/26/coursera-machine-learning-week-1/", "isFamilyFriendly": true, "displayUrl": "https://david-salazar.github.io/2019/12/26/coursera-<b>machine</b>-<b>learning</b>-week-1", "snippet": "The <b>machine</b> <b>learning</b> <b>algorithm</b>, then, will be one that given some training examples, we derive a <b>function</b> that performs the classification from the <b>learning</b> space into the output space: \\[ML:\\{(X_n, Y_n)\\} \\subset X \\times Y \\to f\\] Thus, we want our <b>algorithm</b> to be <b>able</b> <b>to find</b> a <b>function</b> given some training examples, but it is remarked from the beginning that the training examples belong to a wider space of all the possible <b>learning</b> space. However, we just don\u2019t want a <b>function</b> that ...", "dateLastCrawled": "2021-11-29T19:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Artificial intelligence to deep <b>learning</b>: <b>machine</b> intelligence approach ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8040371/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8040371", "snippet": "AI is an umbrella term where computer programs are <b>able</b> to think and behave as humans do, whereas ML is beyond that where data are inputted in the <b>machine</b> along with an <b>algorithm</b> like Na\u00efve Bayes, decision tree (DT), hidden Markov models (HMM) and others, which helps the <b>machine</b> to learn without being explicitly programmed. Later, with the development of neural networks, machines could classify and organize inputted data that mimics like a human brain, which further shows advancement in AI ...", "dateLastCrawled": "2022-02-02T03:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Feature selection in <b>machine</b> <b>learning</b>: an exact penalty approach using ...", "url": "https://link.springer.com/article/10.1007/s10994-014-5455-y", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10994-014-5455-y", "snippet": "where the <b>function</b> \\(f\\) corresponding to a given <b>convex</b> criterion is assumed to be <b>convex</b> and the regularization parameter \\(\\lambda \\) makes the trade-off between the criterion \\(f\\) and the sparsity of \\(x\\).Here \\(\\mu \\) is the variable that does not deal with the sparsity. This problem is a common model that can be used in several <b>learning</b> contexts including feature selection in classification, feature selection in linear regression, sparse Fisher linear discriminant analysis, feature ...", "dateLastCrawled": "2022-01-19T22:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine Learning</b> Glossary | <b>Google Developers</b>", "url": "https://developers.google.com/machine-learning/glossary/", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine-learning</b>/glossary", "snippet": "A strictly <b>convex</b> <b>function</b> has exactly one local <b>minimum</b> point, which is also the <b>global</b> <b>minimum</b> point. The classic U-shaped functions are strictly <b>convex</b> functions. However, some <b>convex</b> functions (for example, straight lines) are not U-shaped. A lot of the common loss functions, including the following, are <b>convex</b> functions: L 2 loss; Log Loss; L 1 regularization; L 2 regularization; Many variations of gradient descent are guaranteed <b>to find</b> a point close to the <b>minimum</b> of a strictly <b>convex</b> ...", "dateLastCrawled": "2022-02-03T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "An overview of <b>gradient descent</b> optimisation algorithms | by Vidit Jain ...", "url": "https://medium.com/analytics-vidhya/an-overview-of-gradient-descent-optimisation-algorithms-eb75302b5d84", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/an-overview-of-<b>gradient-descent</b>-optimisation...", "snippet": "<b>Gradient Descent</b> is an optimisation <b>algorithm</b> used <b>to find</b> the optimum parameters (weights and biases) for a <b>machine</b> <b>learning</b> model such that it minimises a loss/cost <b>function</b> used to evaluate the\u2026", "dateLastCrawled": "2021-11-02T02:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Gradient Descent</b> Explained. A comprehensive guide to Gradient\u2026 | by ...", "url": "https://towardsdatascience.com/gradient-descent-explained-9b953fc0d2c", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>gradient-descent</b>-explained-9b953fc0d2c", "snippet": "In <b>machine</b>/deep <b>learning</b> terminology, it\u2019s the task of minimizing the cost/loss <b>function</b> J(w) parameterized by the model\u2019s parameters w \u2208 R^d. Optimization algorithms (in the case of minimization) have one of the following goals: <b>Find</b> the <b>global</b> <b>minimum</b> of the objective <b>function</b>. This is feasible if the objective <b>function</b> is <b>convex</b>, i.e ...", "dateLastCrawled": "2022-02-02T16:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "[<b>MCQ&#39;s] Machine Learning - Last Moment Tuitions</b>", "url": "https://lastmomenttuitions.com/mcqs-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://lastmomenttuitions.com/<b>mcqs-machine-learning</b>", "snippet": "Which of the following is a widely used and effective <b>machine</b> <b>learning</b> <b>algorithm</b> based on the idea of bagging? A. Decision Tree B. Regression C. Classification D. Random Forest Answer : D Explanation: The Radom Forest <b>algorithm</b> builds an ensemble of Decision Trees, mostly trained with the bagging method. 12. <b>To find</b> the <b>minimum</b> or the <b>maximum</b> of a <b>function</b>, we set the gradient to zero because: A. The value of the gradient at extrema of a <b>function</b> is always zero B. Depends on the type of ...", "dateLastCrawled": "2022-02-03T02:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Understanding <b>Gradient Descent with Python</b>", "url": "https://rubikscode.net/2021/06/28/ml-optimization-pt-1-gradient-descent-with-python/", "isFamilyFriendly": true, "displayUrl": "https://rubikscode.net/2021/06/28/ml-optimization-pt-1-<b>gradient-descent-with-python</b>", "snippet": "In general, every <b>machine</b> <b>learning</b> <b>algorithm</b> is composed of three integral parts: A loss <b>function</b>. Optimization criteria based on the loss <b>function</b>, like a cost <b>function</b>. Optimization technique \u2013 this process leverages training data <b>to find</b> a solution for optimization criteria (cost <b>function</b>). As you were <b>able</b> to see in previous articles, some algorithms were created intuitively and didn\u2019t have optimization criteria in mind. In fact, mathematical explanations of why and how these ...", "dateLastCrawled": "2022-02-02T07:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What is <b>the difference between convex and non-convex optimization problems</b>?", "url": "https://www.researchgate.net/post/What_is_the_difference_between_convex_and_non-convex_optimization_problems", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/post/What_is_<b>the_difference_between_convex_and</b>_non-<b>convex</b>...", "snippet": "In the non-<b>convex</b> <b>function</b> the finding the <b>global</b> solution is very difficult. because it is a particular one like the saddle (seat of on a horse) point which is neither a <b>maximum</b> neither a <b>minimum</b> ...", "dateLastCrawled": "2022-02-02T08:35:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>UML V: Convex Learning Problems</b> - LessWrong 2.0 viewer", "url": "https://www.greaterwrong.com/posts/FnFsLRj4Qybii22ae/uml-v-convex-learning-problems", "isFamilyFriendly": true, "displayUrl": "https://www.greaterwrong.com/posts/FnFsLRj4Qybii22ae/<b>uml-v-convex-learning-problems</b>", "snippet": "(This is part five in a sequence on <b>Machine</b> <b>Learning</b> based on this book. Click here for part 1.) ... The main reason why convexity is a desirable property is that, for a <b>convex</b> <b>function</b>, every local <b>minimum</b> is <b>a global</b> <b>minimum</b>. Here is a proof: Suppose that x \u2208 R d is a local <b>minimum</b>. Then we <b>find</b> some ball B d (x, \u03f5): = {p \u2208 R d | | | p \u2212 x | | \u2264 \u03f5} around x such that f (y) \u2265 x for all y in the ball (this is what it means to be a local <b>minimum</b> in R d). Now let z be an arbitrary ...", "dateLastCrawled": "2021-12-21T23:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "How <b>to Choose an Optimization Algorithm</b> - <b>Machine</b> <b>Learning</b> Mastery", "url": "https://machinelearningmastery.com/tour-of-optimization-algorithms/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/tour-of-optimization-<b>algorithms</b>", "snippet": "Optimization is the problem of finding a set of inputs to an objective <b>function</b> that results in a <b>maximum</b> or <b>minimum</b> <b>function</b> evaluation. It is the challenging problem that underlies many <b>machine</b> <b>learning</b> algorithms, from fitting logistic regression models to training artificial neural networks. There are perhaps hundreds of popular optimization algorithms, and perhaps tens of algorithms to choose from in popular", "dateLastCrawled": "2022-02-03T03:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "[<b>MCQ&#39;s] Machine Learning - Last Moment Tuitions</b>", "url": "https://lastmomenttuitions.com/mcqs-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://lastmomenttuitions.com/<b>mcqs-machine-learning</b>", "snippet": "Which of the following is a widely used and effective <b>machine</b> <b>learning</b> <b>algorithm</b> based on the idea of bagging? A. Decision Tree B. Regression C. Classification D. Random Forest Answer : D Explanation: The Radom Forest <b>algorithm</b> builds an ensemble of Decision Trees, mostly trained with the bagging method. 12. <b>To find</b> the <b>minimum</b> or the <b>maximum</b> of a <b>function</b>, we set the gradient to zero because: A. The value of the gradient at extrema of a <b>function</b> is always zero B. Depends on the type of ...", "dateLastCrawled": "2022-02-03T02:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Introduction to Stochastic Gradient Descent</b> - Great <b>Learning</b>", "url": "https://www.mygreatlearning.com/blog/introduction-to-stochastic-gradient-descent/", "isFamilyFriendly": true, "displayUrl": "https://www.mygreat<b>learning</b>.com/blog/<b>introduction-to-stochastic-gradient-descent</b>", "snippet": "Now with Stochastic Gradient Descent, <b>machine</b> <b>learning</b> algorithms work very well when trained, though it reaches the local <b>minimum</b> in the reasonable amount of time. A crucial parameter for SGD is the <b>learning</b> rate, it is necessary to decrease the <b>learning</b> rate over time, so we now denote the <b>learning</b> rate at iteration k as Ek.", "dateLastCrawled": "2022-02-03T00:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "The 5 Levels of <b>Machine</b> <b>Learning</b> <b>Iteration</b>", "url": "https://elitedatascience.com/machine-learning-iteration", "isFamilyFriendly": true, "displayUrl": "https://elitedatascience.com/<b>machine</b>-<b>learning</b>-<b>iteration</b>", "snippet": "In <b>machine</b> <b>learning</b>, that <b>function</b> is typically the ... Ideally, you want <b>to find</b> the lowest possible valley (<b>global</b> <b>minimum</b>). There are clever ways to prevent the ball from being stuck in local minima (e.g. initializing multiple balls, giving it more momentum so it <b>can</b> traverse small hills, etc.) Oh yeah, and if the mountain terrain is shaped like a bowl (<b>convex</b> <b>function</b>), then the ball is guaranteed to reach the lowest point. Here&#39;s a great short video from Andrew Ng further explaining the ...", "dateLastCrawled": "2022-01-27T23:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Machine Learning</b> (Stanford) Coursera (Week 1, Quiz 2) for the github ...", "url": "https://gist.github.com/mGalarnyk/cc964bea99b09e3c733b339ad3b7b019", "isFamilyFriendly": true, "displayUrl": "https://gist.github.com/mGalarnyk/cc964bea99b09e3c733b339ad3b7b019", "snippet": "Gradient descent is likely to get stuck at a local <b>minimum</b> and fail <b>to find</b> the <b>global</b> <b>minimum</b>. For this to be true, we must have \u03b8 0 =0 and \u03b8 1 =0 so that h \u03b8 (x)=0. Our training set <b>can</b> be fit perfectly by a straight line, i.e., all of our training examples lie perfectly on some straight line. Answers:", "dateLastCrawled": "2022-02-02T18:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Neural Network <b>Learning</b> Internals(<b>Error</b> <b>Function</b> Surface, Non-Convexity ...", "url": "https://becominghuman.ai/neural-network-learning-internals-error-function-surface-non-convexity-sgd-optimization-c0209d8fdf88", "isFamilyFriendly": true, "displayUrl": "https://becominghuman.ai/neural-network-<b>learning</b>-internals-<b>error</b>-<b>function</b>-surface-non...", "snippet": "Of course here we have the graph and we <b>can</b> see that the <b>function</b> is <b>minimum</b> when x = 1.5 but in reality, when do not have the graph available we won\u2019t be <b>able</b> to know whether that point is min or max, because in order <b>to find</b> out the points where a <b>function</b> is max we will do exactly the same trick. At this point we only know that x=1.5 is a critical point a.i. it could be either a min or a max point. Or a point where the derivative changes the sign. So to know whether the point is min or ...", "dateLastCrawled": "2022-01-31T20:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "[MCQ,<b>s] Artificial Intelligence &amp; Soft Computing</b> - Last Moment Tuitions", "url": "https://lastmomenttuitions.com/mcqs/computer-engineering/artificial-intelligence-soft-computing/", "isFamilyFriendly": true, "displayUrl": "https://lastmomenttuitions.com/mcqs/computer-engineering/artificial-intelligence-soft...", "snippet": "23. A complete, local search <b>algorithm</b> always finds goal if one exists, an optimal <b>algorithm</b> always finds <b>a global</b> <b>minimum</b>/<b>maximum</b>. a) True b) False Answer: a Explanation: An <b>algorithm</b> is complete if it finds a solution if exists and optimal if finds optimal goal (<b>minimum</b> <b>or maximum</b>). 24. _____ Is an <b>algorithm</b>, a loop that continually moves in ...", "dateLastCrawled": "2022-02-03T00:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "What is <b>the difference between convex and non-convex optimization problems</b>?", "url": "https://www.researchgate.net/post/What_is_the_difference_between_convex_and_non-convex_optimization_problems", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/post/What_is_<b>the_difference_between_convex_and</b>_non-<b>convex</b>...", "snippet": "For nonconvex functions you <b>can</b>&#39;t <b>find</b> <b>a global</b> <b>minimum</b> with an <b>algorithm</b>. To check whether a <b>function</b> is <b>convex</b> or not check the second derivative of the Hessian, if exists. In case you do not ...", "dateLastCrawled": "2022-02-02T08:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "30+ <b>Data Science Interview Questions from FAANG Tech</b> Giants | by ...", "url": "https://towardsdatascience.com/30-data-science-interview-questions-from-faang-tech-giants-1eea134db7c7", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/30-<b>data-science-interview-questions-from-faang-tech</b>...", "snippet": "When a cost <b>function</b> is non-<b>convex</b>, it means that there\u2019s a likelihood that the <b>function</b> may <b>find</b> local minima instead of the <b>global</b> <b>minimum</b>, which is typically undesired in <b>machine</b> <b>learning</b> models from an optimization perspective.", "dateLastCrawled": "2022-02-02T17:08:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>machine</b> <b>learning</b> - Why do we want an objective <b>function</b> to be a <b>convex</b> ...", "url": "https://stats.stackexchange.com/questions/219899/why-do-we-want-an-objective-function-to-be-a-convex-function", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/219899", "snippet": "this figure shows a non-<b>convex</b> <b>function</b> that carries the above property. It seems to me that, as long as the local <b>minimum</b> is the <b>global</b> <b>minimum</b>, we should be <b>able</b> to use gradient-decent (or similar methods) <b>to find</b> the <b>global</b> <b>minimum</b>. So, my question is, why do <b>convex</b> functions get more attention, <b>compared</b> to other functions that also carry ...", "dateLastCrawled": "2022-01-14T20:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Machine</b> <b>Learning</b>-Regression Algorithms(Linear regression) | by Mohamed ...", "url": "https://medium.com/nerd-for-tech/machine-learning-regression-algorithms-linear-regression-ea97c92f2d9b", "isFamilyFriendly": true, "displayUrl": "https://medium.com/nerd-for-tech/<b>machine</b>-<b>learning</b>-regression-<b>algorithms</b>-linear...", "snippet": "<b>Machine</b> <b>Learning</b> is about Creating an <b>algorithm</b> for which the computer finds a model to fit the data as best as possible and accurately predict. The <b>machine</b> <b>learning</b> <b>algorithm</b> learns the <b>function</b>\u2026", "dateLastCrawled": "2022-01-26T18:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Local Optimization Versus Global Optimization</b> - <b>Machine</b> <b>Learning</b> Mastery", "url": "https://machinelearningmastery.com/local-optimization-versus-global-optimization/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>local-optimization-versus-global-optimization</b>", "snippet": "A local optima is the extrema (<b>minimum</b> <b>or maximum</b>) of the objective <b>function</b> for a given region of the input space, e.g. a basin in a minimization problem. \u2026 we seek a point that is only locally optimal, which means that it minimizes the objective <b>function</b> among feasible points that are near it \u2026 \u2014 Page 9, <b>Convex</b> Optimization, 2004. An objective <b>function</b> may have many local optima, or it may have a single local optima, in which case the local optima is also the <b>global</b> optima. Local ...", "dateLastCrawled": "2022-02-03T01:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Gentle <b>Introduction to Function Optimization</b> - <b>Machine</b> <b>Learning</b> Mastery", "url": "https://machinelearningmastery.com/introduction-to-function-optimization/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>introduction-to-function-optimization</b>", "snippet": "<b>Function</b> optimization is a foundational area of study and the techniques are used in almost every quantitative field. Importantly, <b>function</b> optimization is central to almost all <b>machine</b> <b>learning</b> algorithms, and predictive modeling projects. As such, it is critical to understand what <b>function</b> optimization is, the terminology used in the field, and the elements that constitute a <b>function</b> optimization problem. In this tutorial,", "dateLastCrawled": "2022-02-03T02:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Gradient Descent</b> Explained. A comprehensive guide to Gradient\u2026 | by ...", "url": "https://towardsdatascience.com/gradient-descent-explained-9b953fc0d2c", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>gradient-descent</b>-explained-9b953fc0d2c", "snippet": "In <b>machine</b>/deep <b>learning</b> terminology, it\u2019s the task of minimizing the cost/loss <b>function</b> J(w) parameterized by the model\u2019s parameters w \u2208 R^d. Optimization algorithms (in the case of minimization) have one of the following goals: <b>Find</b> the <b>global</b> <b>minimum</b> of the objective <b>function</b>. This is feasible if the objective <b>function</b> is <b>convex</b>, i.e ...", "dateLastCrawled": "2022-02-02T16:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Neural Network <b>Optimization</b>. Covering optimizers, momentum, adaptive ...", "url": "https://towardsdatascience.com/neural-network-optimization-7ca72d4db3e0", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/neural-network-<b>optimization</b>-7ca72d4db3e0", "snippet": "Saddle point \u2014 simultaneously a local <b>minimum</b> and a local <b>maximum</b>. An example <b>function</b> that is often used for testing the performance of <b>optimization</b> algorithms on saddle points is the Rosenbrook <b>function</b>.The <b>function</b> is described by the formula: f(x,y) = (a-x)\u00b2 + b(y-x\u00b2)\u00b2, which has <b>a global</b> <b>minimum</b> at (x,y) = (a,a\u00b2). This is a non-<b>convex</b> <b>function</b> with <b>a global</b> <b>minimum</b> located within a long and narrow valley.", "dateLastCrawled": "2022-01-30T15:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "[<b>MCQ&#39;s] Machine Learning - Last Moment Tuitions</b>", "url": "https://lastmomenttuitions.com/mcqs-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://lastmomenttuitions.com/<b>mcqs-machine-learning</b>", "snippet": "Which of the following is a widely used and effective <b>machine</b> <b>learning</b> <b>algorithm</b> based on the idea of bagging? A. Decision Tree B. Regression C. Classification D. Random Forest Answer : D Explanation: The Radom Forest <b>algorithm</b> builds an ensemble of Decision Trees, mostly trained with the bagging method. 12. <b>To find</b> the <b>minimum</b> or the <b>maximum</b> of a <b>function</b>, we set the gradient to zero because: A. The value of the gradient at extrema of a <b>function</b> is always zero B. Depends on the type of ...", "dateLastCrawled": "2022-02-03T02:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>machine</b> <b>learning</b> - How <b>can</b> stochastic gradient descent avoid the ...", "url": "https://stats.stackexchange.com/questions/90874/how-can-stochastic-gradient-descent-avoid-the-problem-of-a-local-minimum", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/90874", "snippet": "However, the <b>learning</b> rates tend to zero because in this way, when the <b>algorithm</b> is close to the <b>minimum</b> of a <b>convex</b> <b>function</b>, it stops oscillating and converges. The key of the proof of convergence of stochastic gradient are the conditions imposed on the the series of <b>learning</b> rates. See equations (6) and (27) of the original paper of Robbins and Monro.", "dateLastCrawled": "2022-02-01T07:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>On Hyperparameter Optimization of Machine Learning Algorithms</b>: Theory ...", "url": "https://deepai.org/publication/on-hyperparameter-optimization-of-machine-learning-algorithms-theory-and-practice", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>on-hyperparameter-optimization-of-machine-learning</b>...", "snippet": "The mathematical expression of the <b>function</b> f varies, depending on the objective <b>function</b> of the chosen ML <b>algorithm</b> and the performance metric <b>function</b>. Model performance <b>can</b> be evaluated by various metrics, like accuracy, RMSE, F1-score, and false alarm rate. On the other hand, in practice, time budgets are an essential constraint for optimizing HPO models and must be considered. It often requires a massive amount of time to optimize the objective <b>function</b> of a ML model with a reasonable ...", "dateLastCrawled": "2022-01-30T06:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Machine</b> <b>learning methods for landslide susceptibility</b> studies: A ...", "url": "https://www.sciencedirect.com/science/article/pii/S0012825220302713", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0012825220302713", "snippet": "In this context, \u2018lazy\u2019 or \u2018lazy <b>learning</b>\u2019 refers to a model in which the <b>learning</b> is delayed until a query is made to the system, as opposed to \u2018eager <b>learning</b>\u2019, where the <b>algorithm</b> tries to generalize the training data before receiving queries (L\u00f3pez et al., 2012). The term \u2018non-parametric&#39;, refers to the fact that the <b>algorithm</b> does not depend upon the distribution of the data for its computations, and the model adapts to the data automatically. This is extremely useful for ...", "dateLastCrawled": "2022-01-27T13:28:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Introduction to Machine Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/24/introduction-to-machine-learning-2/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/24/<b>introduction-to-machine-learning</b>-2", "snippet": "<b>Machine</b> <b>learning</b> terminology for model building and validation There seems to be an <b>analogy</b> between statistical modeling and <b>machine</b> <b>learning</b> that we will cover in subsequent chapters in depth. However, a quick view has been provided as follows: in statistical modeling, linear regression with two independent variables is trying to fit the best plane with\u2026", "dateLastCrawled": "2022-01-17T05:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "11.2. <b>Convexity</b> \u2014 Dive into Deep <b>Learning</b> 0.17.2 documentation", "url": "https://d2l.ai/chapter_optimization/convexity.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_optimization/<b>convexity</b>.html", "snippet": "A twice-differentiable <b>function</b> is <b>convex</b> if and only if its Hessian (a matrix of second derivatives) is positive semidefinite. <b>Convex</b> constraints can be added via the Lagrangian. In practice we may simply add them with a penalty to the objective <b>function</b>. Projections map to points in the <b>convex</b> set closest to the original points.", "dateLastCrawled": "2022-01-30T21:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Introduction to <b>Machine</b> <b>Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/30/introduction-to-machine-learning-3/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/30/introduction-to-<b>machine</b>-<b>learning</b>-3", "snippet": "The loss <b>function</b> or cost <b>function</b> in <b>machine</b> <b>learning</b> is a <b>function</b> that maps the values of variables onto a real number intuitively representing some cost associated with the variable values. Optimization methods are applied to minimize the loss <b>function</b> by changing the parameter values, which is the central theme of <b>machine</b> <b>learning</b>.", "dateLastCrawled": "2022-01-28T18:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Weak <b>learning</b> <b>convex</b> sets under normal distributions", "url": "http://proceedings.mlr.press/v134/de21a/de21a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v134/de21a/de21a.pdf", "snippet": "Keywords: weak <b>learning</b>, <b>convex</b> geometry, Gaussian space 1. Introduction Background and motivation. Several results in Boolean <b>function</b> analysis and computational <b>learning</b> theory suggest an <b>analogy</b> between <b>convex</b> sets in Gaussian space and monotone Boolean functions1 with respect to the uniform distribution over the hypercube. As an example ...", "dateLastCrawled": "2022-01-21T10:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Basic Concepts in Machine Learning</b>", "url": "https://machinelearningmastery.com/basic-concepts-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>basic-concepts-in-machine-learning</b>", "snippet": "Probability Estimation: when the output of the <b>function</b> is a probability. <b>Machine Learning</b> in Practice. <b>Machine learning</b> algorithms are only a very small part of using <b>machine learning</b> in practice as a data analyst or data scientist. In practice, the process often looks like: Start Loop Understand the domain, prior knowledge and goals. Talk to ...", "dateLastCrawled": "2022-02-02T15:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>machine</b> <b>learning</b> - How does Gradient Descent work? - Data Science Stack ...", "url": "https://datascience.stackexchange.com/questions/102509/how-does-gradient-descent-work", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/102509/how-does-gradient-descent-work", "snippet": "If the <b>function</b> we minimize was <b>convex</b>, it would not matter what we choose for initial values, as gradient descent would get us to the minimum no matter what. But as the dimensions of the model increase, it is extremely unlikely that we have a <b>convex</b> loss <b>function</b>. And in this case, initialization of the weight depends on the activation functions used in the model. As discussed in", "dateLastCrawled": "2022-01-16T12:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Solving Word <b>Analogies: A Machine Learning Perspective</b> | Request PDF", "url": "https://www.researchgate.net/publication/335597029_Solving_Word_Analogies_A_Machine_Learning_Perspective", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/335597029_Solving_Word_Analogies_A_<b>Machine</b>...", "snippet": "We introduce a supervised corpus-based <b>machine</b> <b>learning</b> algorithm for classifying analogous word pairs, and we show that it can solve multiple-choice SAT <b>analogy</b> questions, TOEFL synonym questions ...", "dateLastCrawled": "2021-10-16T21:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Gradient</b> Descent: A Quick, Simple Introduction | Built In", "url": "https://builtin.com/data-science/gradient-descent", "isFamilyFriendly": true, "displayUrl": "https://builtin.com/data-science/<b>gradient</b>-descent", "snippet": "<b>Gradient</b> descent is an optimization algorithm that&#39;s used when training a <b>machine</b> <b>learning</b> model. It&#39;s based on a <b>convex</b> <b>function</b> and tweaks its parameters iteratively to minimize a given <b>function</b> to its local minimum.", "dateLastCrawled": "2022-02-02T07:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Regularization : What? Why? and How? (Part -1) | by Siddhant Rai ...", "url": "https://medium.com/mlearning-ai/regularization-what-why-and-how-part-1-ef6bdb6bafea", "isFamilyFriendly": true, "displayUrl": "https://medium.com/m<b>learning</b>-ai/regularization-what-why-and-how-part-1-ef6bdb6bafea", "snippet": "In general <b>machine</b> <b>learning</b> sense, it is solving an objective <b>function</b> to perform maximum or minimum evaluation. In reality, optimization is lot more profound in usage. Then we have two terms ...", "dateLastCrawled": "2022-01-28T00:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>machine</b> <b>learning</b> - Cost <b>function</b> of neural network is non-<b>convex</b> ...", "url": "https://stats.stackexchange.com/questions/106334/cost-function-of-neural-network-is-non-convex", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/106334", "snippet": "$\\begingroup$ I mean, this is how it should be interpreted, not just an <b>analogy</b>. $\\endgroup$ \u2013 avocado. May 23 &#39;16 at 12:27 . 5 $\\begingroup$ @loganecolss You are correct that this is not the only reason why cost functions are non-<b>convex</b>, but one of the most obvious reasons. Depdending on the network and the training set, there might be other reasons why there are multiple minima. But the bottom line is: The permuation alone creates non-convexity, regardless of other effects. $\\endgroup ...", "dateLastCrawled": "2022-02-03T01:18:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(convex function)  is like +(a machine learning algorithm that is able to find a global minimum or maximum)", "+(convex function) is similar to +(a machine learning algorithm that is able to find a global minimum or maximum)", "+(convex function) can be thought of as +(a machine learning algorithm that is able to find a global minimum or maximum)", "+(convex function) can be compared to +(a machine learning algorithm that is able to find a global minimum or maximum)", "machine learning +(convex function AND analogy)", "machine learning +(\"convex function is like\")", "machine learning +(\"convex function is similar\")", "machine learning +(\"just as convex function\")", "machine learning +(\"convex function can be thought of as\")", "machine learning +(\"convex function can be compared to\")"]}