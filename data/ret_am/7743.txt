{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Recovering from Biased <b>Data</b>: Can Fairness Constraints Improve Accuracy?", "url": "https://par.nsf.gov/servlets/purl/10190440", "isFamilyFriendly": true, "displayUrl": "https://par.nsf.gov/servlets/purl/10190440", "snippet": "We also consider other recovery methods including re-<b>weighting</b> the training <b>data</b>, <b>Equalized</b> <b>Odds</b>, and Demographic Parity, and Calibration. These theoretical results provide additional motivation for considering fairness interventions even if an actor cares primarily about accuracy. 2012 ACM Subject Classi\ufb01cation Theory of computation \u2192 Machine learning theory Keywords and phrases fairness in machine learning, equal opportunity, bias, machine learning Digital Object Identi\ufb01er 10.4230 ...", "dateLastCrawled": "2021-08-31T01:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Identifying and Correcting Label <b>Bias</b> in Machine ... - Towards <b>Data</b> Science", "url": "https://towardsdatascience.com/identifying-and-correcting-label-bias-in-machine-learning-ed177d30349e", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/identifying-and-correcting-label-<b>bias</b>-in-machine...", "snippet": "<b>Equalized</b> <b>odds</b> - Classifier should ... In post-processing, researchers attempt to reduce <b>bias</b> by manipulating the training <b>data</b> after training the algorithm. <b>Like</b> in pre-processing, a key challenge in post-processing is finding techniques that recognize the <b>bias</b> accurately, allowing them to reduce <b>bias</b> and maintain algorithm accuracy. Algorithm Augmentation (Lagrangian Approach) A recent approach is to incorporate fairness into the training algorithm itself, by penalizing the impact of ...", "dateLastCrawled": "2022-01-30T18:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Fairly Evaluating and Scoring Items in</b> a <b>Data</b> Set", "url": "https://www.vldb.org/pvldb/vol13/p3445-asudeh.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.vldb.org/pvldb/vol13/p3445-asudeh.pdf", "snippet": "and <b>Equalized</b> <b>odds</b> follow the separation model. Suf\ufb01ciency model requires independence of a target variable and a sensitive attribute conditional to the scores. In other words, a score satis\ufb01es suf\ufb01- ciency if the sensitive attribute and target variable are clear from the context. Predictive parity is an example \ufb01tting into this model. Figure 1: The architecture of evaluation systems based on scoring. In addition to fairness, we want a scoring to be stable with re-spect to changes in ...", "dateLastCrawled": "2022-02-03T13:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Reprogramming Fairness: Affirmative Action in Algorithmic Criminal</b> ...", "url": "http://hrlr.law.columbia.edu/hrlr-online/reprogramming-fairness-affirmative-action-in-algorithmic-criminal-sentencing/", "isFamilyFriendly": true, "displayUrl": "hrlr.law.columbia.edu/hrlr-online/<b>reprogramming-fairness-affirmative-action-in</b>...", "snippet": "Certain definitions will be more applicable in attempting to achieve certain goals, and <b>equalized</b> <b>odds</b> may not be the best in each scenario, but the ideals of our justice system and the potential benefits of <b>equalized</b> <b>odds</b> for defendants suggest that equalizing <b>odds</b> is at least as valid as adhering to predictive parity\u2014especially when the unequal <b>odds</b> currently fall along racial lines. Because of this, it is worth examining in greater detail how the law might facilitate and implement risk ...", "dateLastCrawled": "2022-02-01T07:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Fairness through Equality of Effort", "url": "https://www.yongkaiwu.com/publication/huang-2020-fairness/huang-2020-fairness.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.yongkaiwu.com/publication/huang-2020-fairness/huang-2020-fairness.pdf", "snippet": "opportunity and <b>equalized</b> <b>odds</b> that consider both decisions in the training <b>data</b> and decisions made by predictive models. In this paper, we develop a new causal-based fairness notation, called equality of effort. Different from existing fairness notions which mainly focus on discovering the disparity of decisions between two groups of individuals, the proposed equality of effort notation helps answer questions <b>like</b> to what extend a legitimate variable should change to make a particular ...", "dateLastCrawled": "2022-01-15T07:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Can The ML Models <b>Ever Be Devoid Of Labelling Biases</b>", "url": "https://analyticsindiamag.com/can-the-ml-models-ever-be-devoid-of-labelling-biases/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/can-the-ml-models-<b>ever-be-devoid-of-labelling-biases</b>", "snippet": "<b>Equalized</b> <b>odds</b>; Disparate impact; Machine learning engineers work around bias or the offsets in a model by drawing insights from the output, gauging the losses,gouging through tonnes of <b>data</b> and repeating till a agreeable results have been obtained. This is a traditional process which takes time but works decently. An alternative to this approach to this is the Lagrangian approach, a mathematical method to find the local maxima and local minima of a function when provided with equality ...", "dateLastCrawled": "2022-01-20T03:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) Enforcing fairness in logistic regression algorithm", "url": "https://www.researchgate.net/publication/344855305_Enforcing_fairness_in_logistic_regression_algorithm", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/344855305_Enforcing_fairness_in_logistic...", "snippet": "<b>equalized</b> <b>odds</b>. However, if <b>data</b> at hand do have a low level . of unfairness, then a tr ade-off betw een predictive accuracy . and fairness will not be that big (as seen in the Com pas . dataset ...", "dateLastCrawled": "2021-09-12T18:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Spectro-Temporal <b>Weighting</b> of Loudness", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3509144/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC3509144", "snippet": "The <b>data</b> consistently showed that (i) the first 300 ms of the sounds had a greater influence on overall loudness perception than later temporal portions (primacy effect), and (ii) the lowest noise band contributed significantly more to overall loudness than the higher bands. The temporal weights did not differ between the three frequency bands. Notably, the spectral weights and temporal weights estimated from the conditions with only spectral or only temporal variability were very similar to ...", "dateLastCrawled": "2021-12-17T22:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Demographics Should Not Be the Reason of Toxicity: Mitigating ...", "url": "https://deepai.org/publication/demographics-should-not-be-the-reason-of-toxicity-mitigating-discrimination-in-text-classifications-with-instance-weighting", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/demographics-should-not-be-the-reason-of-toxicity...", "snippet": "Jiang and Zhai propose an instance <b>weighting</b> framework for domain adaptation in NLP, which requires the <b>data</b> of the target domain. In our work, we formalize the discrimination problem as a kind of \u201cNot Missing at Random\u201d (NMAR) Rubin ( 1976 ) selection bias from the non-discrimination distribution to the discrimination distribution , and propose to mitigate the unintended bias with instance <b>weighting</b>.", "dateLastCrawled": "2021-12-08T17:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Demographics Should Not Be the Reason of Toxicity: Mitigating ...", "url": "https://aclanthology.org/2020.acl-main.380.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/2020.acl-main.380.pdf", "snippet": "balanced one. However, <b>data</b> manipulation is not always practical. <b>Data</b> supplementation often re-quires careful selection of the additional sentences w.r.t. the identity-terms, the labels, and even the lengths of sentences (Dixon et al.,2018), bringing a high cost for extra <b>data</b> collection and annota-tion. <b>Data</b> augmentation may result in meaningless", "dateLastCrawled": "2022-01-17T16:58:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Recovering from Biased <b>Data</b>: Can Fairness Constraints Improve Accuracy?", "url": "https://par.nsf.gov/servlets/purl/10190440", "isFamilyFriendly": true, "displayUrl": "https://par.nsf.gov/servlets/purl/10190440", "snippet": "We also consider other recovery methods including re-<b>weighting</b> the training <b>data</b>, <b>Equalized</b> <b>Odds</b>, and Demographic Parity, and Calibration. These theoretical results provide additional motivation for considering fairness interventions even if an actor cares primarily about accuracy. 2012 ACM Subject Classi\ufb01cation Theory of computation \u2192 Machine learning theory Keywords and phrases fairness in machine learning, equal opportunity, bias, machine learning Digital Object Identi\ufb01er 10.4230 ...", "dateLastCrawled": "2021-08-31T01:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) Impact of Model Ensemble On the Fairness of Classifiers in ...", "url": "https://www.researchgate.net/publication/352846664_Impact_of_Model_Ensemble_On_the_Fairness_of_Classifiers_in_Machine_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/352846664_Impact_of_Model_Ensemble_On_the...", "snippet": "3) Equal opportunity: <b>Similar</b> to <b>equalized</b> <b>odds</b>, this notion only requires equal values of the FNR across the group, i.e., a classi\ufb01er satis\ufb01es equal opportunity if P ( \u02c6", "dateLastCrawled": "2021-12-19T13:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Identifying and Correcting Label <b>Bias</b> in Machine ... - Towards <b>Data</b> Science", "url": "https://towardsdatascience.com/identifying-and-correcting-label-bias-in-machine-learning-ed177d30349e", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/identifying-and-correcting-label-<b>bias</b>-in-machine...", "snippet": "Disparate impact \u2014 <b>Similar</b> to demographic parity but without the classifier knowing which protected population groups exist and which <b>data</b> points relate to such protected groups. Equal opportunity - Classifier should have equal true positive rates on a protected population group as those of the entire population. <b>Equalized</b> <b>odds</b> - Classifier should have both equal true positive and false positive rates on a protected population group as those of the entire population. Each high-level metric ...", "dateLastCrawled": "2022-01-30T18:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Statistically weighted reviews to enhance sentiment classification</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S2405609X1530004X", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2405609X1530004X", "snippet": "The unbalanced training <b>data</b> can be <b>equalized</b> by changing the importance of frequency and <b>odds</b> of frequency among classes. General intuitive conclusions about a good feature are words with high document frequency and words with high category ratio. But any one of the intuitions does not give best weight to the word. Each domain <b>data</b> set distribution is different from other sets. So <b>weighting</b> the corpus with <b>odds</b> ratio can improve accuracy of classifier to some extent. So, combination of ...", "dateLastCrawled": "2021-10-16T22:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Fairness through Equality of Effort", "url": "https://www.yongkaiwu.com/publication/huang-2020-fairness/huang-2020-fairness.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.yongkaiwu.com/publication/huang-2020-fairness/huang-2020-fairness.pdf", "snippet": "opportunity and <b>equalized</b> <b>odds</b> that consider both decisions in the training <b>data</b> and decisions made by predictive models. In this paper, we develop a new causal-based fairness notation, called equality of effort. Different from existing fairness notions which mainly focus on discovering the disparity of decisions between two groups of individuals, the proposed equality of effort notation helps answer questions like to what extend a legitimate variable should change to make a particular ...", "dateLastCrawled": "2022-01-15T07:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "The Cost of Fairness in AI: Evidence from E-Commerce", "url": "https://link.springer.com/article/10.1007/s12599-021-00716-w", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s12599-021-00716-w", "snippet": "Reweighing was ineffective for both training and test <b>data</b>, providing a level of statistical parity and <b>equalized</b> <b>odds</b> <b>similar</b> to that of the default classifier. Moreover, adversarial debiasing did provide a high level of statistical parity and <b>equalized</b> <b>odds</b> on the training <b>data</b>, but a level <b>similar</b> to that of the default classifier when applied to the test <b>data</b>. This observation can be explained by the nature of how the different algorithms operate. If fairness is injected at an early ...", "dateLastCrawled": "2022-01-25T17:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) Enforcing fairness in logistic regression algorithm", "url": "https://www.researchgate.net/publication/344855305_Enforcing_fairness_in_logistic_regression_algorithm", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/344855305_Enforcing_fairness_in_logistic...", "snippet": "<b>Similar</b> findings can be found in th e Compas dataset. ... <b>equalized</b> <b>odds</b>. However, if <b>data</b> at hand do have a low level . of unfairness, then a tr ade-off betw een predictive accuracy . and ...", "dateLastCrawled": "2021-09-12T18:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Can The ML Models <b>Ever Be Devoid Of Labelling Biases</b>", "url": "https://analyticsindiamag.com/can-the-ml-models-ever-be-devoid-of-labelling-biases/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/can-the-ml-models-<b>ever-be-devoid-of-labelling-biases</b>", "snippet": "<b>Equalized</b> <b>odds</b>; Disparate impact; Machine learning engineers work around bias or the offsets in a model by drawing insights from the output, gauging the losses,gouging through tonnes of <b>data</b> and repeating till a agreeable results have been obtained. This is a traditional process which takes time but works decently. An alternative to this ...", "dateLastCrawled": "2022-01-20T03:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "FairNN - <b>Conjoint Learning of Fair Representations for Fair Decisions</b>", "url": "https://www.mi.fu-berlin.de/en/inf/groups/ag-KIML/Publications/FairNN/FairNN.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.mi.fu-berlin.de/en/inf/groups/ag-KIML/Publications/FairNN/FairNN.pdf", "snippet": "For example, [14] proposes instance re-<b>weighting</b>, label swapping, and <b>data</b> augmentation to eliminate discrimination in the input <b>data</b>. <b>Similar</b> ideas, but for the online scenario, were proposed by [13]. <b>Data</b> augmentation has also been used in [11] in order to force the model so as to learn e\ufb03ciently all the population segments. In [10] a bagging schema is proposed to equalize the <b>data</b> distributions for the di\ufb00erent population segments. In [2] a probabilistic frame-work for discrimination ...", "dateLastCrawled": "2021-11-27T05:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Demographics Should Not Be the Reason of Toxicity: Mitigating ...", "url": "https://aclanthology.org/2020.acl-main.380.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/2020.acl-main.380.pdf", "snippet": "marized as <b>data</b> manipulation methods (Sun et al., 2019). For example,Dixon et al.(2018) propose to apply <b>data</b> supplementation with additional la- beled sentences to make toxic/non-toxic balanced across different demographic groups.Park et al. (2018) proposes to use <b>data</b> augmentation by ap-plying gender-swapping to sentences with identity-terms to mitigate gender bias. The core of these works is to transform the training sets to an identity-balanced one. However, <b>data</b> manipulation is not ...", "dateLastCrawled": "2022-01-17T16:58:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>REPROGRAMMING FAIRNESS: AFFIRMATIVE ACTION IN ALGORITHMIC CRIMINAL</b> ...", "url": "http://hrlr.law.columbia.edu/files/2020/04/8-Humerick_FINAL.pdf", "isFamilyFriendly": true, "displayUrl": "hrlr.law.columbia.edu/files/2020/04/8-Humerick_FINAL.pdf", "snippet": "algorithms based on the definition of <b>equalized</b> <b>odds</b>, emulating the model of affirmative action in higher education. 2020] ... <b>data</b> points through its multi-level, proprietary actuarial model, will tell the judge how likely the defendant is to recidivate: low risk, medium risk, or high risk.6 This determination, like a critical piece of the puzzle, gifts the judge a recommended sentence.7 In theory, this recommended sentence removes individual human bias while maintaining fairness and ...", "dateLastCrawled": "2021-10-18T12:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A Review on Fairness in Machine Learning | ACM Computing Surveys", "url": "https://dl.acm.org/doi/10.1145/3494672", "isFamilyFriendly": true, "displayUrl": "https://dl.acm.org/doi/10.1145/3494672", "snippet": "<b>Equalized</b> <b>odds</b>: This measure was designed by Hardt et al. to ... we <b>can</b> also note that learning fairly to predict the outcome of an unprivileged group <b>can</b> <b>be thought</b> of as learning to predict in a different domain [66, 141], and therefore notions from the field of domain adaptation <b>can</b> be adopted to enhance the study of the field of algorithmic fairness and vice versa. 5.3 Fair Word Embedding. Word embedding models construct representations of words and map them to vectors (also commonly ...", "dateLastCrawled": "2022-02-07T09:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Equalizing Financial Impact in Supervised Learning | DeepAI", "url": "https://deepai.org/publication/equalizing-financial-impact-in-supervised-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/equalizing-financial-impact-in-supervised-learning", "snippet": "Now we discuss the second notion of fairness, called <b>equalized</b> <b>odds</b>, which appears in the work of Hardt, Price, and Srebro . The motivation of <b>equalized</b> <b>odds</b> is that, unlike statistical parity, it does not prevent the ground truth from being a classifier in the case where base rates are unequal. <b>Equalized</b> <b>odds</b> enforces the conditions that the ...", "dateLastCrawled": "2022-01-13T12:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Reprogramming Fairness: Affirmative Action in Algorithmic Criminal</b> ...", "url": "http://hrlr.law.columbia.edu/hrlr-online/reprogramming-fairness-affirmative-action-in-algorithmic-criminal-sentencing/", "isFamilyFriendly": true, "displayUrl": "hrlr.law.columbia.edu/hrlr-online/<b>reprogramming-fairness-affirmative-action-in</b>...", "snippet": "B. <b>Weighting</b> Low Risk Scores Greater than High Risk Scores 69. C. Ensuring Validation of Algorithms for Race 70. D. A More Radical Solution: Affirmative Action for Sentencing Algorithms 70 . Conclusion 72. Introduction. A judge sits in her chambers, painstakingly weighing a range of factors in deciding upon the length of a defendant\u2019s criminal sentence: the applicable sentencing guidelines recommendations, the harm done to the community, the defendant\u2019s criminal history, and pieces of ...", "dateLastCrawled": "2022-02-01T07:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Inquiry into the Relationship between Equity Weights and</b> the Value of ...", "url": "https://www.sciencedirect.com/science/article/pii/S109830151201666X", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S109830151201666X", "snippet": "Quantitative evidence <b>can</b> be used to adjust\u2014explicitly and quantitatively\u2014either the cost-effectiveness threshold or the ICER. The most prominent quantitative approaches are equity <b>weighting</b> and multicriteria decision analysis (for the latter, see , ). Equity <b>weighting</b> allows a quantitative adjustment of the estimated ICER to account for ...", "dateLastCrawled": "2021-10-18T15:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) Blackbox Post-Processing for Multiclass Fairness", "url": "https://www.researchgate.net/publication/357790861_Blackbox_Post-Processing_for_Multiclass_Fairness", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/357790861_Blackbox_Post-Processing_for_Multi...", "snippet": "<b>Equalized</b> <b>Odds</b> by Resampling Sensitive ... and <b>can</b> intuitively <b>be thought</b>. of as requiring that the initial blackbox classi\ufb01er has reason- able discriminative performance. In other words, relev ...", "dateLastCrawled": "2022-01-25T23:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Sex Differences in Cardiovascular Outcomes of Older Adults After ...", "url": "https://www.ahajournals.org/doi/10.1161/JAHA.121.022883", "isFamilyFriendly": true, "displayUrl": "https://www.ahajournals.org/doi/10.1161/JAHA.121.022883", "snippet": "Sex differences in baseline features were <b>equalized</b> using inverse probability <b>weighting</b> adjustment. Women were older, with different comorbidity profiles and rarer ST\u2010segment\u2013elevation MI and revascularization, compared with men. Adenosine diphosphate inhibitors, anticoagulation, statins, and high\u2010dose statins were more frequently used by men, and renin\u2010angiotensin\u2010aldosterone inhibitors and beta blockers by women. After balancing these differences by inverse probability <b>weighting</b> ...", "dateLastCrawled": "2022-01-30T12:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Equality bias impairs collective decision-making across cultures | <b>PNAS</b>", "url": "https://www.pnas.org/content/112/12/3835", "isFamilyFriendly": true, "displayUrl": "https://<b>www.pnas.org</b>/content/112/12/3835", "snippet": "According to the latest World Values Survey,* 71.1% of Northern European respondents (<b>data</b> from Norway and Sweden) and 52.3% of Chinese respondents, but only 10.6% of Iranian respondents, favored the sentence \u201cmost people <b>can</b> be trusted\u201d over \u201cyou <b>can</b> never be too careful when dealing with people.\u201d", "dateLastCrawled": "2022-01-20T04:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "AI/ML Ethics - What Will it Take to Trust the Model - SMARTER Risk ...", "url": "https://www.smarterriskmanagement.com/ai-ml-ethics-what-will-it-take-to-trust-the-model/", "isFamilyFriendly": true, "displayUrl": "https://www.smarterriskmanagement.com/ai-ml-ethics-what-will-it-take-to-trust-the-model", "snippet": "For example, one specific type of this risk is a dangerous cyber-attack that <b>can</b> dramatically impact the results of a model: <b>data</b> poisoning. <b>Data</b> poisoning <b>can</b> be as simple as intentionally feeding a large number of bad <b>data</b> into an algorithm to make it classify results or interpret <b>data</b> incorrectly. On a small scale, this could mean something as simple as <b>weighting</b> different factors incorrectly, but on larger scales, it could mean a massively incorrect algorithm comes out of the other side ...", "dateLastCrawled": "2021-12-12T02:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Is talker variability a critical component of effective phonetic ...", "url": "https://www.sciencedirect.com/science/article/pii/S0095447021000437", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0095447021000437", "snippet": "To examine training-induced changes in perceptual <b>weighting</b> of the spectral and temporal cues, the <b>data</b> for the synthetic vowel stimuli were subject to binary logistic regression analysis, in which case a logistic regression model was fitted to the individual\u2019s proportion of /i/ responses. The model provided spectrally- and duration-tuned coefficients to explain the effect on the log <b>odds</b> (the probability of selecting /i/) of a one-unit change in the corresponding predictors (F1, F2, and ...", "dateLastCrawled": "2022-01-09T21:12:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Identifying and Correcting Label <b>Bias</b> in Machine ... - Towards <b>Data</b> Science", "url": "https://towardsdatascience.com/identifying-and-correcting-label-bias-in-machine-learning-ed177d30349e", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/identifying-and-correcting-label-<b>bias</b>-in-machine...", "snippet": "<b>Equalized</b> <b>odds</b> - Classifier should ... Technical - <b>Data</b> <b>can</b> be biased in complex ways, making it difficult for an algorithm to translate it to a new dataset which is both accurate and unbiased. Legal - In some cases it\u2019s not allowed to train the decision algorithm on non-raw <b>data</b>. <b>Data</b> Post-processing. In post-processing, researchers attempt to reduce <b>bias</b> by manipulating the training <b>data</b> after training the algorithm. Like in pre-processing, a key challenge in post-processing is finding ...", "dateLastCrawled": "2022-01-30T18:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Statistically weighted reviews to enhance sentiment classification</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S2405609X1530004X", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2405609X1530004X", "snippet": "The unbalanced training <b>data</b> <b>can</b> be <b>equalized</b> by changing the importance of frequency and <b>odds</b> of frequency among classes. General intuitive conclusions about a good feature are words with high document frequency and words with high category ratio. But any one of the intuitions does not give best weight to the word. Each domain <b>data</b> set distribution is different from other sets. So <b>weighting</b> the corpus with <b>odds</b> ratio <b>can</b> improve accuracy of classifier to some extent. So, combination of ...", "dateLastCrawled": "2021-10-16T22:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Reprogramming Fairness: Affirmative Action in Algorithmic Criminal</b> ...", "url": "http://hrlr.law.columbia.edu/hrlr-online/reprogramming-fairness-affirmative-action-in-algorithmic-criminal-sentencing/", "isFamilyFriendly": true, "displayUrl": "hrlr.law.columbia.edu/hrlr-online/<b>reprogramming-fairness-affirmative-action-in</b>...", "snippet": "Certain definitions will be more applicable in attempting to achieve certain goals, and <b>equalized</b> <b>odds</b> may not be the best in each scenario, but the ideals of our justice system and the potential benefits of <b>equalized</b> <b>odds</b> for defendants suggest that equalizing <b>odds</b> is at least as valid as adhering to predictive parity\u2014especially when the unequal <b>odds</b> currently fall along racial lines. Because of this, it is worth examining in greater detail how the law might facilitate and implement risk ...", "dateLastCrawled": "2022-02-01T07:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Identifying and correcting Label Bias in Machine Learning - Pye AI", "url": "https://www.pye.ai/2020/11/15/identifying-and-correcting-label-bias-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.pye.ai/2020/11/15/identifying-and-correcting-label-bias-in-machine-learning", "snippet": "<b>Equalized</b> <b>odds</b> \u2013 Classifier should have both equal ... Technical \u2013 <b>Data</b> <b>can</b> be biased in complex ways, making it difficult for an algorithm to translate it to a new dataset which is both accurate and unbiased. Legal \u2013 In some cases it\u2019s not allowed to train the decision algorithm on non-raw <b>data</b>. <b>Data</b> Post-processing. In post-processing, researchers attempt to reduce bias by manipulating the training <b>data</b> after training the algorithm. Like in pre-processing, a key challenge in post ...", "dateLastCrawled": "2022-01-28T14:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Statistically weighted reviews to enhance sentiment classification - CORE", "url": "https://core.ac.uk/download/pdf/82078996.pdf", "isFamilyFriendly": true, "displayUrl": "https://core.ac.uk/download/pdf/82078996.pdf", "snippet": "The unbalanced training <b>data</b> <b>can</b> be <b>equalized</b> by changing the importance of frequency and <b>odds</b> of frequency among classes. General intuitive conclusions about a good feature are words with high document frequency and words with high category ratio. But any one of the intuitions does not give best weight to the word. Each domain <b>data</b> set distribution is different from other sets. So <b>weighting</b> the corpus with <b>odds</b> ratio <b>can</b> improve accuracy of classi\ufb01er to some extent. So, combination of ...", "dateLastCrawled": "2020-11-13T10:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) Enforcing fairness in logistic regression algorithm", "url": "https://www.researchgate.net/publication/344855305_Enforcing_fairness_in_logistic_regression_algorithm", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/344855305_Enforcing_fairness_in_logistic...", "snippet": "<b>equalized</b> <b>odds</b> with out sacrificing both predi ctive accuracy . and disparate impact. Constraint logistic regression <b>can</b> be lear ned using . traditional constra ined gradient-based metho ds, or in ...", "dateLastCrawled": "2021-09-12T18:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Assessing Fairness in the Presence of Missing <b>Data</b>", "url": "https://proceedings.neurips.cc/paper/2021/file/85dca1d270f7f9aef00c9d372f114482-Paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/2021/file/85dca1d270f7f9aef00c9d372f114482-Paper.pdf", "snippet": "Similarly, [24] proposed <b>equalized</b> <b>odds</b>, which requires both false positive rate (FPR) and false negative rate (FNR) to be the same between two groups. In the regression setting, fairness is usually associated with the parity of loss between two groups [1, 35, 16]. To \ufb01x ideas, we propose to use in this paper accuracy parity gap as the fairness notion in learning tasks including classi\ufb01cation and regression. We consider the technique of re-<b>weighting</b> on assessing fairness of a given ...", "dateLastCrawled": "2022-01-03T21:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "The Cost of Fairness in AI: Evidence from E-Commerce", "url": "https://link.springer.com/article/10.1007/s12599-021-00716-w", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s12599-021-00716-w", "snippet": "Reweighing was ineffective for both training and test <b>data</b>, providing a level of statistical parity and <b>equalized</b> <b>odds</b> similar to that of the default classifier. Moreover, adversarial debiasing did provide a high level of statistical parity and <b>equalized</b> <b>odds</b> on the training <b>data</b>, but a level similar to that of the default classifier when applied to the test <b>data</b>. This observation <b>can</b> be explained by the nature of how the different algorithms operate. If fairness is injected at an early ...", "dateLastCrawled": "2022-01-25T17:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "AI Fairness -A Brief Introduction to AI Fairness 360 | by ...", "url": "https://transformernlp.medium.com/ai-fairness-a-brief-introduction-to-ai-fairness-360-b2e39c96ca49", "isFamilyFriendly": true, "displayUrl": "https://transformernlp.medium.com/ai-fairness-a-brief-introduction-to-ai-fairness-360...", "snippet": "Average <b>odds</b> difference: The average difference of false positive rate and true positive rate between unprivileged group to the privileged group. Ideal value is 0. Equation4 Algorithms. Bias mitigation algorithms attempt to improve the fairness metrics by modifying the training <b>data</b>, the learning algorithm, or the predictions. These algorithm categories are known as pre-processing, in-processing, and post-processing, respectively [6]. The choice among algorithm categories <b>can</b> partially be ...", "dateLastCrawled": "2022-01-18T14:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Trustworthy Machine Learning - Kush R. Varshney - Chapter 10: Fairness", "url": "http://www.trustworthymachinelearning.com/trustworthymachinelearning-10.htm", "isFamilyFriendly": true, "displayUrl": "www.trustworthymachinelearning.com/trustworthymachinelearning-10.htm", "snippet": "Another way to pre-process the training <b>data</b> set is through sample weights, similar to inverse probability <b>weighting</b> and importance <b>weighting</b> seen in chapter 8 and chapter 9, respectively. The reweighing method is geared toward improving statistical parity (\u201cwe\u2019re all equal\u201d worldview), which <b>can</b> be assessed before the care management model is trained and is a dataset fairness metric. [16]", "dateLastCrawled": "2022-01-07T03:04:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine Learning</b> Glossary | <b>Google Developers</b>", "url": "https://developers.google.com/machine-learning/glossary/", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine-learning</b>/glossary", "snippet": "A <b>machine learning</b> technique that iteratively combines a set of simple and not very accurate classifiers ... Contrast with <b>equalized</b> <b>odds</b> and equality of opportunity, which permit classification results in aggregate to depend on sensitive attributes, but do not permit classification results for certain specified ground-truth labels to depend on sensitive attributes. See &quot;Attacking discrimination with smarter <b>machine learning</b>&quot; for a visualization exploring the tradeoffs when optimizing for ...", "dateLastCrawled": "2022-02-03T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Residual Unfairness in Fair <b>Machine</b> <b>Learning</b> from Prejudiced Data", "url": "http://proceedings.mlr.press/v80/kallus18a/kallus18a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v80/kallus18a/kallus18a.pdf", "snippet": "chine <b>learning</b> has considered some subcomponents of the overall problem we study of <b>learning</b> fair policies from bi-ased datasets. Hardt et al. (2016) formalize the criteria of equal opportunity and <b>equalized</b> <b>odds</b>. Lum &amp; Isaac (2016) show that a predictive policing algorithm for drug enforce-ment in Oakland, trained on police records, will ...", "dateLastCrawled": "2022-01-31T07:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A Survey on Bias and Fairness in <b>Machine</b> <b>Learning</b> \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/1908.09635/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1908.09635", "snippet": "With the widespread use of AI systems and applications in our everyday lives, it is important to take fairness issues into consideration while designing and engineering these types of systems. Such systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that the decisions do not reflect discriminatory behavior toward certain groups or populations. We have recently seen work in <b>machine</b> <b>learning</b>, natural language ...", "dateLastCrawled": "2021-11-15T17:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Human-centric Approach to Fairness in AI", "url": "https://www.timlrx.com/blog/a-human-centric-approach-to-fairness-in-ai", "isFamilyFriendly": true, "displayUrl": "https://www.timlrx.com/blog/a-human-centric-approach-to-fairness-in-ai", "snippet": "We embed the evaluation of AI fairness within the best practices of <b>machine</b> <b>learning</b> development and operations such as version control, ... This includes measures such as Demographic Parity / Statistical Parity (Dwork et al., 2012), <b>Equalized</b> <b>Odds</b> Metric (Hardt et al., 2016) and Calibration within Groups (Chouldechova, 2017). They are all statistical measures derived from the predictions of a classification model and differ in terms of which element(s) of the confusion matrix they are ...", "dateLastCrawled": "2022-02-03T02:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Mitigating Unwanted Biases with Adversarial Learning</b> | DeepAI", "url": "https://deepai.org/publication/mitigating-unwanted-biases-with-adversarial-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>mitigating-unwanted-biases-with-adversarial-learning</b>", "snippet": "<b>Machine</b> <b>learning</b> leverages data to build models capable of assessing the labels and properties of novel data. Unfortunately, the available training data frequently contains biases with respect to things that we would rather not use for decision making. <b>Machine</b> <b>learning</b> builds models faithful to training data and can lead to perpetuating these undesirable biases. For example, systems designed to predict creditworthiness and systems designed to perform <b>analogy</b> completion have been demonstrated ...", "dateLastCrawled": "2021-12-10T20:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Mitigating Unwanted Biases with Adversarial Learning</b>", "url": "https://www.aies-conference.com/2018/contents/papers/main/AIES_2018_paper_162.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.aies-conference.com/2018/contents/papers/main/AIES_2018_paper_162.pdf", "snippet": "<b>Machine</b> <b>learning</b> is a tool for building models that accurately represent input training data. When undesired biases concern- ing demographic groups are in the training data, well-trained models will re\ufb02ect those biases. We present a framework for mitigating such biases by including a variable for the group of interest and simultaneously <b>learning</b> a predictor and an ad-versary. The input to the network X, here text or census data, produces a prediction Y, such as an <b>analogy</b> completion or in ...", "dateLastCrawled": "2021-12-17T22:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "RStudio AI Blog: Starting to think about AI Fairness", "url": "https://blogs.rstudio.com/ai/posts/2021-07-15-ai-fairness/", "isFamilyFriendly": true, "displayUrl": "https://blogs.rstudio.com/ai/posts/2021-07-15-ai-fairness", "snippet": "Papers on fairness in <b>machine</b> <b>learning</b>, as is common in fields like computer science, abound with formulae. Even the papers referenced here, though selected not for their theorems and proofs but for the ideas they harbor, are no exception. But to start thinking about fairness as it might apply to an ML process at hand, common language \u2013 and common sense \u2013 will do just fine. If, after analyzing your use case, you judge that the more technical results are relevant to the process in ...", "dateLastCrawled": "2022-01-31T09:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Classification - Fairness and <b>machine</b> <b>learning</b>", "url": "https://fairmlbook.org/classification.html", "isFamilyFriendly": true, "displayUrl": "https://fairmlbook.org/classification.html", "snippet": "Simply put, the goal of classification is to determine a plausible value for an unknown variable Y given an observed variable X.For example, we might try to predict whether a loan applicant will pay back her loan by looking at various characteristics such as credit history, income, and net worth. Classification also applies in situations where the variable Y does not refer to an event that lies in the future. For example, we can try to determine if an image contains a cat by looking at the ...", "dateLastCrawled": "2022-02-03T07:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "On Predicting Recidivism: Epistemic Risk, Tradeoffs, and Values in ...", "url": "https://www.cambridge.org/core/journals/canadian-journal-of-philosophy/article/on-predicting-recidivism-epistemic-risk-tradeoffs-and-values-in-machine-learning/7E541FA03E78C3141A65EA99A0CA6E9A", "isFamilyFriendly": true, "displayUrl": "https://www.cambridge.org/core/journals/canadian-journal-of-philosophy/article/on...", "snippet": "This paper examines the role of value judgments in the design of <b>machine</b>-<b>learning</b> (ML) systems generally and in recidivism-prediction algorithms specifically. Drawing on work on inductive and epistemic risk, the paper argues that ML systems are value laden in ways similar to human decision making, because the development and design of ML systems requires human decisions that involve tradeoffs that reflect values. In many cases, these decisions have significant\u2014and, in some cases, disparate ...", "dateLastCrawled": "2022-01-26T20:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Measuring discrimination in algorithmic <b>decision making</b> | SpringerLink", "url": "https://link.springer.com/article/10.1007/s10618-017-0506-1", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10618-017-0506-1", "snippet": "A <b>machine</b> <b>learning</b> algorithm is a procedure used for producing a predictive model from historical data. A model is a collection of decision rules used for <b>decision making</b> for new incoming data. The model would take personal characteristics as inputs (for example, income, credit history, employment status), and produce a prediction (for example, credit risk level). Fig. 1. A typical <b>machine</b> <b>learning</b> setting. Full size image. <b>Learning</b> algorithms as such cannot discriminate, because they are ...", "dateLastCrawled": "2022-01-29T20:54:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(equalized odds)  is like +(weighting data)", "+(equalized odds) is similar to +(weighting data)", "+(equalized odds) can be thought of as +(weighting data)", "+(equalized odds) can be compared to +(weighting data)", "machine learning +(equalized odds AND analogy)", "machine learning +(\"equalized odds is like\")", "machine learning +(\"equalized odds is similar\")", "machine learning +(\"just as equalized odds\")", "machine learning +(\"equalized odds can be thought of as\")", "machine learning +(\"equalized odds can be compared to\")"]}