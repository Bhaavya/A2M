{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "10 <b>Leading Language Models For NLP</b> In 2021 - TOPBOTS", "url": "https://www.topbots.com/leading-nlp-language-models-2020/", "isFamilyFriendly": true, "displayUrl": "https://www.topbots.com/leading-nlp-<b>language</b>-<b>models</b>-2020", "snippet": "<b>Masked</b> <b>language</b> modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK] and then train a <b>model</b> to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some ...", "dateLastCrawled": "2022-02-02T10:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Emerging Cross-lingual Structure in Pretrained <b>Language</b> Models", "url": "https://aclanthology.org/2020.acl-main.536.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/2020.acl-main.536.pdf", "snippet": "representation <b>learning</b> from a <b>language</b> <b>model</b>. The representations are used in a transfer <b>learning</b> setup to improve performance on a variety of down-stream NLP tasks. Follow-up work byHoward and Ruder(2018);Radford et al.(2018) further improves on this idea by \ufb01ne-tuning the entire <b>lan-guage</b> <b>model</b>. BERT (Devlin et al.,2019) signi\ufb01-cantly outperforms these methods by introducing a <b>masked</b>-<b>language</b> <b>model</b> and next-sentence pre-diction objectives combined with a bi-directional transformer ...", "dateLastCrawled": "2021-12-30T00:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Top 10 <b>Pre-Trained</b> NLP <b>Language</b> Models - Daffodil", "url": "https://insights.daffodilsw.com/blog/top-5-nlp-language-models", "isFamilyFriendly": true, "displayUrl": "https://insights.daffodilsw.com/blog/top-5-nlp-<b>language</b>-<b>models</b>", "snippet": "<b>Masked</b> <b>language</b> modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK] and then train a <b>model</b> to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of computing to be effective. As an alternative, experts propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, their approach corrupts it by ...", "dateLastCrawled": "2022-02-02T17:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Transformer-based <b>Language</b> Models | The Ezra Tech Blog", "url": "https://techblog.ezra.com/an-overview-of-different-transformer-based-language-models-c9d3adafead8", "isFamilyFriendly": true, "displayUrl": "https://techblog.ezra.com/an-<b>overview-of-different-transformer-based-language-models</b>-c...", "snippet": "<b>Masked</b> <b>Language</b> <b>Model</b>. This task is used to allow for the <b>model</b>\u2019s bidirectionality. The standard conditional <b>language</b> models where the target word is predicted from the previous or next word do not allow for both left-to-right and right-to-left training. This is because with bidirectional conditioning the word indirectly sees itself. In order to achieve the bidirectional nature of the BERT <b>language</b> <b>model</b>, 15% of the training tokens are <b>masked</b> using a special [MASK] token. The <b>model</b> is ...", "dateLastCrawled": "2022-01-27T18:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Top 50 NLP Interview Questions and Answers for 2022", "url": "https://www.projectpro.io/article/nlp-interview-questions-and-answers/439", "isFamilyFriendly": true, "displayUrl": "https://www.projectpro.io/article/nlp-interview-questions-and-answers/439", "snippet": "The <b>Masked</b> <b>Language</b> <b>Model</b> is a <b>model</b> that takes a sentence with a few hidden (<b>masked</b>) words as input and tries to complete the sentence by correctly guessing those hidden words. 10. What is the Bag-of-words <b>model</b> in NLP? Bag-of-words refers to an unorganized set of words. The Bag-of-words <b>model</b> is NLP is a <b>model</b> that assigns a vector to a ...", "dateLastCrawled": "2022-01-29T15:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Mixed-Lingual Pre-training for Cross-lingual Summarization", "url": "https://aclanthology.org/2020.aacl-main.53.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/2020.aacl-main.53.pdf", "snippet": "such as translation and monolingual tasks <b>like</b> <b>masked</b> <b>language</b> models. Thus, our <b>model</b> can leverage the massive monolingual data to enhance its modeling of <b>language</b>. Moreover, the architecture has no task-speci\ufb01c compo- nents, which saves memory and increases opti-mization ef\ufb01ciency. We show in experiments that this pre-training scheme can effectively boost the performance of cross-lingual summa-rization. In Neural Cross-Lingual Summariza-tion (NCLS) (Zhu et al.,2019b) dataset, our <b>model</b> ...", "dateLastCrawled": "2022-01-19T14:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "How to <b>Develop Word-Based Neural Language Models in Python</b> with Keras", "url": "https://machinelearningmastery.com/develop-word-based-neural-language-models-python-keras/", "isFamilyFriendly": true, "displayUrl": "https://machine<b>learning</b>mastery.com/<b>develop-word-based-neural-language-models</b>-python-keras", "snippet": "<b>Language</b> modeling involves predicting the next word in a sequence given the sequence of words already present. A <b>language</b> <b>model</b> is a key element in many natural <b>language</b> processing models such as machine translation and speech recognition. The choice of how the <b>language</b> <b>model</b> is framed must match how the <b>language</b> <b>model</b> is intended to be used. In this tutorial, you will", "dateLastCrawled": "2022-02-02T23:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Word knowledge, <b>learning</b> and acquisition in a second <b>language</b>: Proposed ...", "url": "https://www.cambridge.org/core/journals/language-teaching/article/word-knowledge-learning-and-acquisition-in-a-second-language-proposed-replications-of-elgort-2011-and-qiao-and-forster-2017/BC4A26C7F6CF77137FC114E1BE8C39D9", "isFamilyFriendly": true, "displayUrl": "https://www.cambridge.org/core/journals/<b>language</b>-teaching/article/word-knowledge...", "snippet": "For example, the PLE may be observed for known L2 words with Chinese-English bilinguals who learned English in an English as a second <b>language</b> (ESL) but not English as <b>a foreign</b> <b>language</b> (EFL) context. In this case, with the right <b>learning</b> treatment, it should be possible to replicate the PLE for the recently learned words even if the PLE is not observed for \u2018known\u2019 L2 words. Such an outcome with Chinese speakers would provide even stronger support to the finding of the original study ...", "dateLastCrawled": "2022-01-28T04:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>BERT sentence perplexity</b> : LanguageTechnology - <b>reddit</b>", "url": "https://www.reddit.com/r/LanguageTechnology/comments/eh4lt9/bert_sentence_perplexity/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/<b>Language</b>Technology/comments/eh4lt9/<b>bert_sentence_perplexity</b>", "snippet": "While traditional NMT is capable of translating a single <b>language</b> pair, training a separate <b>model</b> for each <b>language</b> pair is time-consuming, especially given the world\u2019s thousands of languages. As a result, multilingual NMT is designed to handle many <b>language</b> pairs in a single <b>model</b>, lowering the cost of offline training and online deployment significantly. Furthermore, parameter sharing in multilingual neural machine translation promotes positive knowledge transfer between languages and is ...", "dateLastCrawled": "2022-01-10T01:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Struggling to learn a <b>new language - these porn stars could help</b> (no ...", "url": "https://www.mirror.co.uk/news/weird-news/struggling-learn-new-language-porn-11287637", "isFamilyFriendly": true, "displayUrl": "https://<b>www.mirror.co.uk</b>/news/weird-news/struggling-learn-new-<b>language</b>-porn-11287637", "snippet": "People hoping to learn a new <b>language</b> can now sign up to a bizarre series of classes - taught by porn stars.. Two month courses, teaching people French, Japanese, Spanish and English have been ...", "dateLastCrawled": "2022-01-30T06:47:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Emerging Cross-lingual Structure in Pretrained <b>Language</b> Models", "url": "https://aclanthology.org/2020.acl-main.536.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/2020.acl-main.536.pdf", "snippet": "representation <b>learning</b> from a <b>language</b> <b>model</b>. The representations are used in a transfer <b>learning</b> setup to improve performance on a variety of down- stream NLP tasks. Follow-up work byHoward and Ruder(2018);Radford et al.(2018) further improves on this idea by \ufb01ne-tuning the entire <b>lan-guage</b> <b>model</b>. BERT (Devlin et al.,2019) signi\ufb01-cantly outperforms these methods by introducing a <b>masked</b>-<b>language</b> <b>model</b> and next-sentence pre-diction objectives combined with a bi-directional transformer ...", "dateLastCrawled": "2021-12-30T00:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Transformer-based <b>Language</b> Models | The Ezra Tech Blog", "url": "https://techblog.ezra.com/an-overview-of-different-transformer-based-language-models-c9d3adafead8", "isFamilyFriendly": true, "displayUrl": "https://techblog.ezra.com/an-<b>overview-of-different-transformer-based-language-models</b>-c...", "snippet": "<b>Masked</b> <b>Language</b> <b>Model</b>. This task is used to allow for the <b>model</b>\u2019s bidirectionality. The standard conditional <b>language</b> models where the target word is predicted from the previous or next word do not allow for both left-to-right and right-to-left training. This is because with bidirectional conditioning the word indirectly sees itself. In order to achieve the bidirectional nature of the BERT <b>language</b> <b>model</b>, 15% of the training tokens are <b>masked</b> using a special [MASK] token. The <b>model</b> is ...", "dateLastCrawled": "2022-01-27T18:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Top 50 NLP Interview Questions and Answers for 2022", "url": "https://www.projectpro.io/article/nlp-interview-questions-and-answers/439", "isFamilyFriendly": true, "displayUrl": "https://www.projectpro.io/article/nlp-interview-questions-and-answers/439", "snippet": "The <b>Masked</b> <b>Language</b> <b>Model</b> is a <b>model</b> that takes a sentence with a few hidden (<b>masked</b>) words as input and tries to complete the sentence by correctly guessing those hidden words. 10. What is the Bag-of-words <b>model</b> in NLP? Bag-of-words refers to an unorganized set of words. The Bag-of-words <b>model</b> is NLP is a <b>model</b> that assigns a vector to a ...", "dateLastCrawled": "2022-01-29T15:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Mixed-Lingual Pre-training for Cross-lingual Summarization", "url": "https://aclanthology.org/2020.aacl-main.53.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/2020.aacl-main.53.pdf", "snippet": "lingual <b>masked</b> <b>language</b> <b>model</b> (CMLM) and ma-chine translation (MT). This mixed-lingual pre-training scheme can take advantage of massive un-labeled monolingual data to improve the <b>model</b>\u2019s <b>language</b> modeling capability, and leverage cross-lingual tasks to improve the <b>model</b>\u2019s cross-lingual representation. We then \ufb01netune the <b>model</b> on the", "dateLastCrawled": "2022-01-19T14:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Top 10 <b>Pre-Trained</b> NLP <b>Language</b> Models - Daffodil", "url": "https://insights.daffodilsw.com/blog/top-5-nlp-language-models", "isFamilyFriendly": true, "displayUrl": "https://insights.daffodilsw.com/blog/top-5-nlp-<b>language</b>-<b>models</b>", "snippet": "Transfer <b>learning</b>, where a <b>model</b> is first <b>pre-trained</b> on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural <b>language</b> processing (NLP). The effectiveness of transfer <b>learning</b> has given rise to a diversity of approaches, methodology, and practice. The Google research team suggests a unified approach to transfer <b>learning</b> in NLP to set a new state of the art in the field. To this end, they propose treating each NLP problem as a \u201ctext ...", "dateLastCrawled": "2022-02-02T17:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "10 <b>Leading Language Models For NLP</b> In 2021 - TOPBOTS", "url": "https://www.topbots.com/leading-nlp-language-models-2020/", "isFamilyFriendly": true, "displayUrl": "https://www.topbots.com/leading-nlp-<b>language</b>-<b>models</b>-2020", "snippet": "Transfer <b>learning</b>, where a <b>model</b> is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural <b>language</b> processing (NLP). The effectiveness of transfer <b>learning</b> has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer <b>learning</b> techniques for NLP by introducing a unified framework that converts every <b>language</b> problem into a text-to-text format. Our ...", "dateLastCrawled": "2022-02-02T10:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Zero Shot Cross-Lingual Transfer with <b>Multilingual</b> BERT | by Ceshine ...", "url": "https://medium.com/the-artificial-impostor/zero-shot-cross-lingual-transfer-with-multilingual-bert-9fe111e02bb", "isFamilyFriendly": true, "displayUrl": "https://medium.com/the-artificial-impostor/zero-shot-cross-lingual-transfer-with...", "snippet": "BERT[1] is a <b>language</b> representation <b>model</b> that uses two new pre-training objectives \u2014 <b>masked</b> <b>language</b> <b>model</b>(MLM) and next sentence prediction, that obtained SOTA results on many downstream ...", "dateLastCrawled": "2022-01-28T20:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "How to <b>Develop Word-Based Neural Language Models in Python</b> with Keras", "url": "https://machinelearningmastery.com/develop-word-based-neural-language-models-python-keras/", "isFamilyFriendly": true, "displayUrl": "https://machine<b>learning</b>mastery.com/<b>develop-word-based-neural-language-models</b>-python-keras", "snippet": "<b>Language</b> modeling involves predicting the next word in a sequence given the sequence of words already present. A <b>language</b> <b>model</b> is a key element in many natural <b>language</b> processing models such as machine translation and speech recognition. The choice of how the <b>language</b> <b>model</b> is framed must match how the <b>language</b> <b>model</b> is intended to be used. In this tutorial, you will", "dateLastCrawled": "2022-02-02T23:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Word knowledge, <b>learning</b> and acquisition in a second <b>language</b>: Proposed ...", "url": "https://www.cambridge.org/core/journals/language-teaching/article/word-knowledge-learning-and-acquisition-in-a-second-language-proposed-replications-of-elgort-2011-and-qiao-and-forster-2017/BC4A26C7F6CF77137FC114E1BE8C39D9", "isFamilyFriendly": true, "displayUrl": "https://www.cambridge.org/core/journals/<b>language</b>-teaching/article/word-knowledge...", "snippet": "For example, the PLE may be observed for known L2 words with Chinese-English bilinguals who learned English in an English as a second <b>language</b> (ESL) but not English as <b>a foreign</b> <b>language</b> (EFL) context. In this case, with the right <b>learning</b> treatment, it should be possible to replicate the PLE for the recently learned words even if the PLE is not observed for \u2018known\u2019 L2 words. Such an outcome with Chinese speakers would provide even stronger support to the finding of the original study ...", "dateLastCrawled": "2022-01-28T04:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Why is Fine tuning a text <b>model</b> so influential on the results ...", "url": "https://www.reddit.com/r/LanguageTechnology/comments/rzuyqo/why_is_fine_tuning_a_text_model_so_influential_on/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/<b>Language</b>Technology/comments/rzuyqo/why_is_fine_tuning_a_text...", "snippet": "Newbie to this field, but nonetheless BERT was trained on 3.3 billion+ words, when I do a <b>masked</b> <b>learning</b> task it is fairly successful on my healthcare dataset without fine tuning. However, when I fine tune the dataset, maybe adding only additional 1 million words (only ~0.02% more words), suddenly the same task is significantly more accurate.", "dateLastCrawled": "2022-01-11T13:46:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Emerging Cross-lingual Structure in Pretrained <b>Language</b> Models", "url": "https://aclanthology.org/2020.acl-main.536.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/2020.acl-main.536.pdf", "snippet": "representation <b>learning</b> from a <b>language</b> <b>model</b>. The representations are used in a transfer <b>learning</b> setup to improve performance on a variety of down- stream NLP tasks. Follow-up work byHoward and Ruder(2018);Radford et al.(2018) further improves on this idea by \ufb01ne-tuning the entire <b>lan-guage</b> <b>model</b>. BERT (Devlin et al.,2019) signi\ufb01-cantly outperforms these methods by introducing a <b>masked</b>-<b>language</b> <b>model</b> and next-sentence pre-diction objectives combined with a bi-directional transformer ...", "dateLastCrawled": "2021-12-30T00:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Transformer-based <b>Language</b> Models | The Ezra Tech Blog", "url": "https://techblog.ezra.com/an-overview-of-different-transformer-based-language-models-c9d3adafead8", "isFamilyFriendly": true, "displayUrl": "https://techblog.ezra.com/an-<b>overview-of-different-transformer-based-language-models</b>-c...", "snippet": "<b>Masked</b> <b>Language</b> <b>Model</b>. This task is used to allow for the <b>model</b>\u2019s bidirectionality. The standard conditional <b>language</b> models where the target word is predicted from the previous or next word do not allow for both left-to-right and right-to-left training. This is because with bidirectional conditioning the word indirectly sees itself. In order to achieve the bidirectional nature of the BERT <b>language</b> <b>model</b>, 15% of the training tokens are <b>masked</b> using a special [MASK] token. The <b>model</b> is ...", "dateLastCrawled": "2022-01-27T18:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Language Modeling</b> - GitHub Pages", "url": "https://lena-voita.github.io/nlp_course/language_modeling.html", "isFamilyFriendly": true, "displayUrl": "https://lena-voita.github.io/nlp_course/<b>language_modeling</b>.html", "snippet": "Since left-to-right neural <b>language</b> models <b>can</b> <b>be thought</b> of as classifiers, ... Later in the course, we will see more examples of <b>language</b> models <b>learning</b> lots of cool stuff when given huge training datasets. Use Interpretable Neurons to Control Generated Texts. Interpretable neurons are not only fun, but also <b>can</b> be used to control your <b>language</b> <b>model</b>. For example, we <b>can</b> fix the sentiment neuron to generate texts with a desired sentiment. Below are the examples of samples starting from ...", "dateLastCrawled": "2022-01-29T13:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "10 <b>Leading Language Models For NLP</b> In 2021 - TOPBOTS", "url": "https://www.topbots.com/leading-nlp-language-models-2020/", "isFamilyFriendly": true, "displayUrl": "https://www.topbots.com/leading-nlp-<b>language</b>-<b>models</b>-2020", "snippet": "<b>Masked</b> <b>language</b> modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK] and then train a <b>model</b> to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some ...", "dateLastCrawled": "2022-02-02T10:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "GPT-3 &amp; <b>Beyond: 10 NLP Research Papers You Should Read</b>", "url": "https://www.topbots.com/nlp-research-papers-2020/", "isFamilyFriendly": true, "displayUrl": "https://www.topbots.com/nlp-research-papers-2020", "snippet": "<b>Masked</b> <b>language</b> modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK] and then train a <b>model</b> to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some ...", "dateLastCrawled": "2022-01-29T19:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "How to <b>Develop Word-Based Neural Language Models in Python</b> with Keras", "url": "https://machinelearningmastery.com/develop-word-based-neural-language-models-python-keras/", "isFamilyFriendly": true, "displayUrl": "https://machine<b>learning</b>mastery.com/<b>develop-word-based-neural-language-models</b>-python-keras", "snippet": "<b>Language</b> modeling involves predicting the next word in a sequence given the sequence of words already present. A <b>language</b> <b>model</b> is a key element in many natural <b>language</b> processing models such as machine translation and speech recognition. The choice of how the <b>language</b> <b>model</b> is framed must match how the <b>language</b> <b>model</b> is intended to be used. In this tutorial, you will", "dateLastCrawled": "2022-02-02T23:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Using lexical <b>language</b> models to detect borrowings in monolingual wordlists", "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0242709", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0242709", "snippet": "Although <b>masked</b> with time, <b>language</b>-internal evidence for borrowing <b>can</b> be observed in many languages from different families. In many Hmong-Mien languages, for example, some Chinese words are borrowed with a very specific tone that only occurs in Chinese words . Similarly, it is easy for German speakers to identify job as a loan from English, since only in borrowed words the grapheme j is pronounced as [dZ] in German. In the same line, but in a radically different context, speakers of ...", "dateLastCrawled": "2020-12-10T17:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A critical analysis of Vygotsky and Piagets theory of <b>language</b> <b>learning</b> ...", "url": "https://www.grin.com/document/268010", "isFamilyFriendly": true, "displayUrl": "https://www.grin.com/document/268010", "snippet": "According to Vygostky, the most important part is to be played by the <b>language</b> in the cognitive development, Vygostky believes in the centrality of <b>language</b> as a tool for <b>thought</b> or a powerful means of mediation (Mitchell, R. &amp; Myles, F. 2004, Second <b>Language</b> <b>Learning</b> Theories, 2 nd Ed. London: Hodder Arnold, p. 194, 195) Vygostky further elaborated his socio-cultural theory by explaining the importance of Cultural Tools through mediation and <b>language</b>, Co-constructed process, self-regulation ...", "dateLastCrawled": "2021-12-27T22:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Language</b> <b>Learning</b> and Control in Monolinguals and Bilinguals ...", "url": "https://onlinelibrary.wiley.com/doi/full/10.1111/j.1551-6709.2012.01243.x", "isFamilyFriendly": true, "displayUrl": "https://onlinelibrary.wiley.com/doi/full/10.1111/j.1551-6709.2012.01243.x", "snippet": "The <b>learning</b> paradigm was designed to equate novel <b>language</b> attainment across monolingual and bilingual groups, so that any observed differences in between-<b>language</b> competition could not be attributed to novel-<b>language</b> proficiency. Participants were first familiarized with the 24 pictures and their translations in Colbertian. A picture appeared on a computer screen and 500 ms later the participant heard the Colbertian word over headphones; the participant was instructed to repeat the word ...", "dateLastCrawled": "2021-12-13T20:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Being bilingual really does boost brain power</b> | Daily <b>Mail Online</b>", "url": "https://www.dailymail.co.uk/sciencetech/article-2907330/Being-bilingual-really-does-boost-brain-power-Learning-language-10-years-age-changes-mind-s-white-matter.html", "isFamilyFriendly": true, "displayUrl": "https://<b>www.dailymail.co.uk</b>/sciencetech/article-2907330", "snippet": "<b>Learning</b> a second <b>language</b> in early childhood has long been known to boost brainpower. But a new study suggests that the effects extend to those who learn a <b>language</b> from the age of 10 onwards.", "dateLastCrawled": "2022-01-31T05:35:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "10 <b>Leading Language Models For NLP</b> In 2021 - TOPBOTS", "url": "https://www.topbots.com/leading-nlp-language-models-2020/", "isFamilyFriendly": true, "displayUrl": "https://www.topbots.com/leading-nlp-<b>language</b>-<b>models</b>-2020", "snippet": "<b>Masked</b> <b>language</b> modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK] and then train a <b>model</b> to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some ...", "dateLastCrawled": "2022-02-02T10:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Cross-lingual <b>Language</b> <b>Model</b> Pretraining for Retrieval", "url": "https://maroo.cs.umass.edu/getpdf.php?id=1440", "isFamilyFriendly": true, "displayUrl": "https://maroo.cs.umass.edu/getpdf.php?id=1440", "snippet": "Cross-lingual <b>Language</b> <b>Model</b> Pretraining for Retrieval Puxuan Yu, Hongliang Fei, Ping Li Cognitive Computing Lab Baidu Research Bellevue, WA, USA {pxyuwhu,feihongliang0,pingli98}@gmail.com ABSTRACT Existing research on cross-lingual retrieval cannot take good advan-tage of large-scale pretrained <b>language</b> models such as multilingual BERT and XLM. We hypothesize that the absence of cross-lingual passage-level relevance data for finetuning and the lack of query-document style pretraining are ...", "dateLastCrawled": "2022-01-23T02:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Zero Shot Cross-Lingual Transfer with <b>Multilingual</b> BERT | by Ceshine ...", "url": "https://medium.com/the-artificial-impostor/zero-shot-cross-lingual-transfer-with-multilingual-bert-9fe111e02bb", "isFamilyFriendly": true, "displayUrl": "https://medium.com/the-artificial-impostor/zero-shot-cross-lingual-transfer-with...", "snippet": "BERT[1] is a <b>language</b> representation <b>model</b> that uses two new pre-training objectives \u2014 <b>masked</b> <b>language</b> <b>model</b>(MLM) and next sentence prediction, that obtained SOTA results on many downstream ...", "dateLastCrawled": "2022-01-28T20:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "The Bilingual Adaptation: How Minds Accommodate Experience", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5324728/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC5324728", "snippet": "The <b>model</b> described three such contexts \u2013 single <b>language</b>, dual <b>language</b>, and dense code-switching \u2013 each of which makes different demands on selection. The <b>model</b> provides an excellent framework for understanding these questions and makes detailed predictions about the cognitive and brain changes that should follow from each of the contexts. At this point, however, research supporting the claims from this <b>model</b> is extremely preliminary, a point the authors fully acknowledge, so a ...", "dateLastCrawled": "2022-01-21T02:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Differences in word <b>learning</b> in children: Bilingualism or linguistic ...", "url": "https://www.researchgate.net/publication/344947778_Differences_in_word_learning_in_children_Bilingualism_or_linguistic_experience", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/344947778_Differences_in_word_<b>learning</b>_in...", "snippet": "This paper reviews the psycholinguistic factors that affect ease of <b>learning</b> of <b>foreign</b> <b>language</b> (FL) vocabulary and investigates their role in 47 students&#39; <b>learning</b> of German under Repetition ...", "dateLastCrawled": "2022-01-16T16:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Quantitative evaluation of a pre-trained <b>BERT model</b> | by Ajit ...", "url": "https://towardsdatascience.com/quantitative-evaluation-of-a-pre-trained-bert-model-73d56719539e", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/quantitative-evaluation-of-a-pre-trained-<b>bert-model</b>-73d...", "snippet": "Quantifying the <b>learning</b> of a fine tuned <b>model</b> relative to a pre-trained <b>model</b>. ... term was present in the underlying vocabulary) is unreliable from factual correctness perspective, just as it is for any <b>language</b> <b>model</b>, regardless of it being an autoregressive or auto encoder <b>model</b>. However, <b>BERT</b>\u2019s prediction of the entity type of a <b>masked</b> position is consistently accurate if the <b>model</b> was pre-trained on the corpus with a custom vocabulary that is truly representative of the input ...", "dateLastCrawled": "2022-01-25T21:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "GPT-3 &amp; <b>Beyond: 10 NLP Research Papers You Should Read</b>", "url": "https://www.topbots.com/nlp-research-papers-2020/", "isFamilyFriendly": true, "displayUrl": "https://www.topbots.com/nlp-research-papers-2020", "snippet": "<b>Masked</b> <b>language</b> modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK] and then train a <b>model</b> to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some ...", "dateLastCrawled": "2022-01-29T19:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Variability and Consistency in First and Second <b>Language</b> Processing: A ...", "url": "https://onlinelibrary.wiley.com/doi/full/10.1111/lang.12370", "isFamilyFriendly": true, "displayUrl": "https://onlinelibrary.wiley.com/doi/full/10.1111/lang.12370", "snippet": "A number of previous studies found that despite having reached a high level of proficiency in a given <b>language</b>, L2 speakers may show reduced <b>masked</b> priming effects relative to L1 speakers, particularly for regularly inflected word forms (Jacob et al., 2018; Kirkici &amp; Clahsen, 2013; Silva &amp; Clahsen, 2008). Furthermore, L2 processing of morphologically complex words has been found to be more susceptible to surface form prime\u2013target overlap than L1 processing. Unlike L1 control groups ...", "dateLastCrawled": "2021-11-24T15:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Word knowledge, <b>learning</b> and acquisition in a second <b>language</b>: Proposed ...", "url": "https://www.cambridge.org/core/journals/language-teaching/article/word-knowledge-learning-and-acquisition-in-a-second-language-proposed-replications-of-elgort-2011-and-qiao-and-forster-2017/BC4A26C7F6CF77137FC114E1BE8C39D9", "isFamilyFriendly": true, "displayUrl": "https://www.cambridge.org/core/journals/<b>language</b>-teaching/article/word-knowledge...", "snippet": "Elgort&#39;s (Reference Elgort 2011) study investigated whether deliberate word <b>learning</b> using a paired-associate <b>learning</b> approach <b>can</b> result in the kind of knowledge that learners need in real <b>language</b> use (such as reading). Although the efficiency of paired-associate <b>learning</b> is not normally contested, the effectiveness of this treatment is less widely accepted. Theories of SLA that prioritise the role of comprehensible input, such as the Monitor theory (Krashen,", "dateLastCrawled": "2022-01-28T04:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Language Modeling</b> - GitHub Pages", "url": "https://lena-voita.github.io/nlp_course/language_modeling.html", "isFamilyFriendly": true, "displayUrl": "https://lena-voita.github.io/nlp_course/<b>language_modeling</b>.html", "snippet": "The key point of this paper is that a <b>language</b> <b>model</b> <b>can</b> be used to estimate the &quot;predictability&quot; of a word given context. Computational LM instead of a Human one - a very novel idea Previously, the predictability of a word given context was estimated in cloze-style tasks: humans were asked to guess the next word given context.", "dateLastCrawled": "2022-01-29T13:36:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine</b> <b>Learning</b> Glossary: <b>Language</b> Evaluation | Google Developers", "url": "https://developers.google.com/machine-learning/glossary/language", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine</b>-<b>learning</b>/glossary/<b>language</b>", "snippet": "For instance, a <b>masked</b> <b>language</b> <b>model</b> can calculate probabilities for candidate word(s) to replace the underline in the following sentence: The ____ in the hat came back. The literature typically uses the string &quot;MASK&quot; instead of an underline. For example: The &quot;MASK&quot; in the hat came back. Most modern <b>masked</b> <b>language</b> models are bidirectional.", "dateLastCrawled": "2022-01-29T02:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "The Illustrated GPT-2 (Visualizing Transformer <b>Language</b> Models) \u2013 Jay ...", "url": "http://jalammar.github.io/illustrated-gpt2/", "isFamilyFriendly": true, "displayUrl": "jalammar.github.io/illustrated-gpt2", "snippet": "GPT-2 <b>Masked</b> Self-Attention; Beyond <b>Language</b> modeling; You\u2019ve Made it! Part 3: Beyond <b>Language</b> Modeling. <b>Machine</b> Translation; Summarization ; Transfer <b>Learning</b>; Music Generation; Part #1: GPT2 And <b>Language</b> Modeling # So what exactly is a <b>language</b> <b>model</b>? What is a <b>Language</b> <b>Model</b>. In The Illustrated Word2vec, we\u2019ve looked at what a <b>language</b> <b>model</b> is \u2013 basically a <b>machine</b> <b>learning</b> <b>model</b> that is able to look at part of a sentence and predict the next word. The most famous <b>language</b> models ...", "dateLastCrawled": "2022-02-01T04:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "The <b>language</b> of <b>proteins: NLP, machine learning &amp; protein sequences</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S2001037021000945", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2001037021000945", "snippet": "For example, in the <b>masked</b> <b>language</b> task, some fraction of the tokens in the original text are <b>masked</b> at random, and the <b>language</b> <b>model</b> attempts to predict the original text. (B) (Pre-)trained <b>language</b> models are commonly fine-tuned on downstream tasks over labeled text, through a standard supervised-<b>learning</b> approach. Fine-tuning is typically much faster and provides superior performance than training a <b>model</b> from scratch, especially when labeled data is scarce.", "dateLastCrawled": "2022-02-03T02:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Word Embeddings, WordPiece and Language-Agnostic BERT</b> (LaBSE) | by ...", "url": "https://medium.com/mlearning-ai/word-embeddings-wordpiece-and-language-agnostic-bert-labse-98c7626878c7", "isFamilyFriendly": true, "displayUrl": "https://medium.com/m<b>learning</b>-ai/<b>word-embeddings-wordpiece-and-language-agnostic-bert</b>...", "snippet": "LaBSE <b>model</b> combines <b>masked</b> <b>language</b> <b>model</b> (MLM) and translation <b>language</b> <b>model</b> (TLM) pretraining with a translation ranking task using bi-directional dual encoders.", "dateLastCrawled": "2022-02-03T17:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Natrual <b>language</b> processing basic concepts - <b>language</b> <b>model</b> - word ...", "url": "https://shuffleai.blog/blog/nlp_concepts_part_1.html", "isFamilyFriendly": true, "displayUrl": "https://shuffleai.blog/blog/nlp_concepts_part_1.html", "snippet": "Before deep <b>learning</b>&#39;s domination in natural <b>language</b> processing, a <b>language</b> <b>model</b> is basically a large lookup table, recording frequencies of different combinations of words&#39; occurrences in a large corpus. Now it&#39;s a neural network trained on a corpus or dataset. In addition, a causal <b>language</b> <b>model</b>(e.g., GPT) predicts the next word, and a <b>masked</b> <b>language</b> <b>model</b>(e.g., BERT) fills the blank given the rest of a sentence. If you input &quot;The man ____ to the store&quot; to BERT, it will predict the ...", "dateLastCrawled": "2021-12-24T07:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "An introduction to Deep <b>Learning</b> in Natural <b>Language</b> Processing: Models ...", "url": "https://www.sciencedirect.com/science/article/pii/S0925231221010997", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0925231221010997", "snippet": "The pre-training was driven by two <b>language</b> <b>model</b> objectives, i.e. <b>Masked</b> <b>Language</b> <b>Model</b> (MLM) and Next Sentence Prediction (NSP). In MLM, showed in Fig. 8 , the network masks a small number of words of the input sequence and it tries to predict them in output, whereas in NSP the network tries to understand the relations between sentences by means of a binary loss.", "dateLastCrawled": "2022-01-04T17:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>GitHub</b> - <b>rosinality/ml-papers</b>: My collection of <b>machine</b> <b>learning</b> papers", "url": "https://github.com/rosinality/ml-papers", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/rosinality/ml-papers", "snippet": "210413 <b>Masked</b> <b>Language</b> Modeling and the Distributional Hypothesis #<b>language</b>_<b>model</b> #mlm; 210417 mT6 #<b>language</b>_<b>model</b>; 210418 Data-Efficient <b>Language</b>-Supervised Zero-Shot <b>Learning</b> with #multimodal; 210422 ImageNet-21K Pretraining for the Masses #backbone; 210510 Are Pre-trained Convolutions Better than Pre-trained Transformers #nlp #convolution # ...", "dateLastCrawled": "2022-01-31T15:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "The 5 <b>Components Towards Building Production-Ready Machine Learning Systems</b>", "url": "https://www.topbots.com/building-production-ready-machine-learning-systems/", "isFamilyFriendly": true, "displayUrl": "https://www.topbots.com/<b>building-production-ready-machine-learning-systems</b>", "snippet": "A well-known recent case study of applying knowledge distillation in practice is Hugging Face\u2019s DistilBERT, which is a smaller <b>language</b> <b>model</b> derived from the supervision of the popular BERT <b>language</b> <b>model</b>. DistilBERT removed the toke-type embeddings and the pooler (used for the next sentence classification task) from BERT while keeping the rest of the architecture identical and reducing the number of layers by a factor of two. Overall, DistilBERT has about half the total number of ...", "dateLastCrawled": "2022-01-25T15:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "SpringerLink - International Journal of <b>Machine</b> <b>Learning</b> and Cybernetics", "url": "https://link.springer.com/article/10.1007/s13042-020-01069-8", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s13042-020-01069-8", "snippet": "The Neural Network <b>Language</b> <b>Model</b> (NNLM) is a pioneering work which introduces the idea of deep <b>learning</b> into <b>language</b> modeling and successfully mitigates the curse of dimensionality (i.e. Sequences in the test set is likely to have not been observed in the training data) by <b>learning</b> a distributed representation of words. The goal of <b>language</b> modeling is to learn a <b>model</b> that predicts the next word given previous ones. Practically, we assume the", "dateLastCrawled": "2022-01-29T10:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Intuitive Introduction to BERT \u2013 MachineCurve</b>", "url": "https://www.machinecurve.com/index.php/2021/01/04/intuitive-introduction-to-bert/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/index.php/2021/01/04/<b>intuitive-introduction-to-bert</b>", "snippet": "We do however often want to create a <b>machine</b> <b>learning</b> <b>model</b> that can perform one task really well. This is where finetuning comes in: using a labeled corpus, which is often smaller, we can then train the pretrained <b>model</b> further, with an additional or replacing NLP task. The end result is a <b>model</b> that has been pretrained on the large unlabeled corpus and which is finetuned to a specific <b>language</b> task, such as summarization, text generation in a particular domain, or translation.", "dateLastCrawled": "2022-01-30T22:30:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Improving Text Generation with Dynamic Masking and Recovering", "url": "https://www.ijcai.org/proceedings/2021/0534.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.ijcai.org/proceedings/2021/0534.pdf", "snippet": "tokens, <b>just as masked language model</b> does. Therefore, our approach jointly maximizes both the likelihoods of both sen-tence generation and prediction of masked tokens. We verify the effectiveness and generality of our ap-proach on three types of text generation tasks which use var-ious forms of input data including text, graph, and image. For sequence-to-sequence (seq2seq) generation task (specif-ically, <b>machine</b> translation), our model obtains signi\ufb01cant improvement of 1.01 and 0.90 BLEU ...", "dateLastCrawled": "2022-01-29T07:50:00.0000000Z", "language": "en", "isNavigational": false}], [], []], "all_bing_queries": ["+(masked language model)  is like +(learning a foreign language)", "+(masked language model) is similar to +(learning a foreign language)", "+(masked language model) can be thought of as +(learning a foreign language)", "+(masked language model) can be compared to +(learning a foreign language)", "machine learning +(masked language model AND analogy)", "machine learning +(\"masked language model is like\")", "machine learning +(\"masked language model is similar\")", "machine learning +(\"just as masked language model\")", "machine learning +(\"masked language model can be thought of as\")", "machine learning +(\"masked language model can be compared to\")"]}