{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Convex Optimization</b> - NYU Courant", "url": "https://cims.nyu.edu/~cfgranda/pages/MTDS_spring19/notes/convex_optimization.pdf", "isFamilyFriendly": true, "displayUrl": "https://cims.nyu.edu/~cfgranda/pages/MTDS_spring19/notes/<b>convex_optimization</b>.pdf", "snippet": "<b>Convex Optimization</b> 1 Motivation 1.1 Sparse <b>regression</b> In our description of <b>linear</b> <b>regression</b> in the notes on the SVD, we observed that the performance of <b>linear</b> <b>regression</b> degrades when the number of features is close to the number of training data. This makes sense: the number of parameters of a model should be signi cantly smaller than the number of measurements used to t it. However, when the number of features is very large, it is often possible to achieve accurate prediction using ...", "dateLastCrawled": "2022-02-03T14:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Report for CS229: <b>Convex</b> <b>Optimization</b> For Machine Learning (cvx4ml)", "url": "https://cs229.stanford.edu/proj2017/final-reports/5242031.pdf", "isFamilyFriendly": true, "displayUrl": "https://cs229.stanford.edu/proj2017/final-reports/5242031.pdf", "snippet": "Unconstrained <b>Linear</b> <b>regression</b> with norm square (a.k.a. least-squares) and translated by G.W.Stewart in 1995. Which was very popular during 20 We estimate parameters of <b>linear</b> model under assumption that nois is is additive, unbiased. Old technic introduced by Gauss in 1820 th century. <b>linear</b>_<b>regression</b>_L2 <b>Linear</b> <b>regression</b> with norm", "dateLastCrawled": "2021-11-02T09:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Learning as Optimization: Linear Regression</b>", "url": "https://cse.iitk.ac.in/users/piyush/courses/ml_autumn16/771A_lec4_slides.pdf", "isFamilyFriendly": true, "displayUrl": "https://cse.iitk.ac.in/users/piyush/courses/ml_autumn16/771A_lec4_slides.pdf", "snippet": "We would <b>like</b> to nd f that minimizes thetrue loss or \\risk&quot;de ned as L(f) = E (x;y)\u02d8P[\u2018(y;f(x)] = Z \u2018(y;f(x)dP(x;y) Problem:We (usually) don\u2019t know the true distribution and only have nite set of samples from it, in form of the N training examples f(x n;y n)gN n=1 Solution:Work with the \\empirical&quot; risk de ned on the training data L emp(f) = 1 N XN n=1 \u2018(y n;f(x n)) Machine Learning (CS771A) <b>Learning as Optimization: Linear Regression</b> 2. Learning as <b>Optimization</b> Consider asupervised ...", "dateLastCrawled": "2022-02-03T16:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Convexity of <b>linear regression</b> - Cross Validated", "url": "https://stats.stackexchange.com/questions/206115/convexity-of-linear-regression", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/206115/<b>convex</b>ity-of-<b>linear-regression</b>", "snippet": "I know that <b>linear regression</b> leads to a <b>convex</b> <b>optimization</b> problem. I&#39;d <b>like</b> to visually show this with a simple example. Assume that there are two parameters (x and y) and a single data point &lt;1, 1&gt; with 2 as the y value (no intercept term. Then the cost function becomes. ( x + y \u2212 2) 2. However if you plot this function you will get the ...", "dateLastCrawled": "2022-01-26T16:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Why is <b>linear regression a convex optimisation problem</b>? - Quora", "url": "https://www.quora.com/Why-is-linear-regression-a-convex-optimisation-problem", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Why-is-<b>linear-regression-a-convex-optimisation-problem</b>", "snippet": "Answer (1 of 3): <b>Linear</b> <b>regression</b> fits a straight line to the datapoints, such as the error between the data points and the straight line is minimized. (Image ...", "dateLastCrawled": "2022-01-26T02:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Learning <b>Convex</b> <b>Optimization</b> Models - Stanford University", "url": "https://web.stanford.edu/~boyd/papers/pdf/learning_copt_models.pdf", "isFamilyFriendly": true, "displayUrl": "https://web.stanford.edu/~boyd/papers/pdf/learning_copt_models.pdf", "snippet": "special cases many well-known models <b>like</b> <b>linear</b> and logistic <b>regression</b>. We propose a heuristic for learning the parameters in a <b>convex</b> <b>optimization</b> model given a dataset of input-output pairs, using recently developed methods for differentiating the solution of a <b>convex</b> <b>optimization</b> problem with respect to its parameters. We describe three general classes of <b>convex</b> <b>optimization</b> models, maximum a posteriori (MAP) models, utility maximization models, and agent models, and present a numerical ...", "dateLastCrawled": "2022-02-02T22:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Introduction to <b>Convex Optimization</b> for Machine Learning", "url": "https://people.eecs.berkeley.edu/~jordan/courses/294-fall09/lectures/optimization/slides.pdf", "isFamilyFriendly": true, "displayUrl": "https://people.eecs.berkeley.edu/~jordan/courses/294-fall09/lectures/<b>optimization</b>/...", "snippet": "<b>Linear</b> <b>regression</b>: minimize w kXw \u2212yk2 Classi\ufb01cation (logistic regresion or SVM): minimize w Xn i=1 log 1+exp(\u2212yixT i w) or kwk2 +C Xn i=1 \u03bei s.t. \u03bei \u2265 1\u2212yixTiw,\u03bei \u2265 0. Duchi (UC Berkeley) <b>Convex Optimization</b> for Machine Learning Fall 2009 5 / 53", "dateLastCrawled": "2022-02-03T03:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A <b>convex</b> <b>optimization</b> formulation for multivariate <b>regression</b>", "url": "https://proceedings.neurips.cc/paper/2020/file/ccd2d123f4ec4d777fc6ef757d0fb642-Paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/2020/file/ccd2d123f4ec4d777fc6ef757d0fb642-Paper.pdf", "snippet": "proposed <b>convex</b> formulation is in contrast with most existing high-dimensional multivariate <b>linear</b> <b>regression</b> methodologies, most of which either focus only on estimating the <b>regression</b> coef\ufb01cients alone, or estimating the <b>regression</b> coef\ufb01cients and the precision matrix alternately by solving a nonconvex <b>optimization</b> problem.", "dateLastCrawled": "2022-02-03T06:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Visualizing <b>Optimization</b> in <b>Linear</b> <b>Regression</b> and Logistic <b>Regression</b> ...", "url": "https://medium.com/swlh/from-animation-to-intuition-linear-regression-and-logistic-regression-f641a31e1caf", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/from-animation-to-intuition-<b>linear</b>-<b>regression</b>-and-logistic...", "snippet": "This is called <b>convex</b> <b>optimization</b> and it\u2019s desired whenever possible. Nonconvex <b>optimization</b> is much harder. Surface plot of the 2D <b>linear</b> <b>regression</b> cost function. From this plot, we can ...", "dateLastCrawled": "2022-01-25T19:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Why is Convex Optimization such a big</b> deal in Machine Learning? - Quora", "url": "https://www.quora.com/Why-is-Convex-Optimization-such-a-big-deal-in-Machine-Learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>Why-is-Convex-Optimization-such-a-big</b>-deal-in-Machine-Learning", "snippet": "Answer (1 of 10): <b>Convex</b> <b>optimization</b> is the core of most machine learning methods Some key concepts here: - <b>Convex</b> functions are those for which it&#39;s possible to draw a line segment from any two points on the graph and this line will always be inside the graph (except at the endpoints). This m...", "dateLastCrawled": "2022-01-17T17:05:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Why is <b>linear regression a convex optimisation problem</b>? - Quora", "url": "https://www.quora.com/Why-is-linear-regression-a-convex-optimisation-problem", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Why-is-<b>linear-regression-a-convex-optimisation-problem</b>", "snippet": "Answer (1 of 3): <b>Linear</b> <b>regression</b> fits a straight line to the datapoints, such as the error between the data points and the straight line is minimized. (Image ...", "dateLastCrawled": "2022-01-26T02:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>convex</b> <b>optimization</b> - <b>Linear regression with the addition of</b> a ...", "url": "https://math.stackexchange.com/questions/4152698/linear-regression-with-the-addition-of-a-euclidean-norm-penalty", "isFamilyFriendly": true, "displayUrl": "https://math.stackexchange.com/questions/4152698/<b>linear-regression-with-the-addition</b>...", "snippet": "<b>convex</b>-<b>optimization</b> <b>linear</b>-<b>regression</b>. Share. Cite. Follow edited May 27 &#39;21 at 7:40. Evan Aad. asked May 27 &#39;21 at 7:34. Evan Aad Evan Aad. 10k 1 1 gold badge 28 28 silver badges 69 69 bronze badges $\\endgroup$ 3 $\\begingroup$ The solution changes. The additional terms reduces the domain for feasible $\\beta&#39;s$. It <b>is similar</b> to having an <b>optimization</b> problem with restrictions (where one uses Lagrange multipliers) $\\endgroup$ \u2013 Oliver Diaz. May 27 &#39;21 at 7:45 $\\begingroup$ @OliverDiaz ...", "dateLastCrawled": "2022-01-13T21:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Convex Optimization</b> - NYU Courant", "url": "https://cims.nyu.edu/~cfgranda/pages/MTDS_spring19/notes/convex_optimization.pdf", "isFamilyFriendly": true, "displayUrl": "https://cims.nyu.edu/~cfgranda/pages/MTDS_spring19/notes/<b>convex_optimization</b>.pdf", "snippet": "<b>Convex Optimization</b> 1 Motivation 1.1 Sparse <b>regression</b> In our description of <b>linear</b> <b>regression</b> in the notes on the SVD, we observed that the performance of <b>linear</b> <b>regression</b> degrades when the number of features is close to the number of training data. This makes sense: the number of parameters of a model should be signi cantly smaller than the number of measurements used to t it. However, when the number of features is very large, it is often possible to achieve accurate prediction using ...", "dateLastCrawled": "2022-02-03T14:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Convex Optimization</b> - NUS Computing - Home", "url": "https://www.comp.nus.edu.sg/~rahul/allfiles/cs6234-16-convex-opt.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.comp.nus.edu.sg/~rahul/allfiles/cs6234-16-<b>convex</b>-opt.pdf", "snippet": "<b>Convex</b> Domain <b>Linear</b> <b>Regression</b> method is applicable only if nonlinear function is <b>linear</b> in terms of function parameters: f(x;a) = Xm k=1 a kh k(x) Many nonlinear functions are not like that, for example: f 1(x) = x2 a 1 +(x-a 2) f 2(x,y,z) = x2 a 1 +x2 + y2 a 2 +y2 + z2 a 3 +z2 Advanced Algorithms <b>Convex Optimization</b> Jan 20th, 2016 17 / 42", "dateLastCrawled": "2022-01-27T18:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "What is <b>the relationship between linear programming and</b> <b>convex</b> ...", "url": "https://www.researchgate.net/post/What-is-the-relationship-between-linear-programming-and-convex-optimization", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/post/What-is-<b>the-relationship-between-linear-programming</b>...", "snippet": "<b>Convex</b> <b>optimization</b> involves minimizing a <b>convex</b> objective function (or maximizing a concave objective function) over a <b>convex</b> set of constraints. <b>Linear</b> programming is a special case of <b>convex</b> ...", "dateLastCrawled": "2022-01-28T04:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Price Optimisation with <b>convex</b> and non-<b>convex</b> loss functions | by ...", "url": "https://towardsdatascience.com/convex-and-non-convex-optimisation-899174802b60", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>convex</b>-and-non-<b>convex</b>-optimisation-899174802b60", "snippet": "In a <b>similar</b> manner a <b>linear</b> <b>regression</b> with cost as the dependent variable \u2018y\u2019 and volume as the independent variable \u2018X\u2019 is performed below. We can see that the bias and co-efficient for the cost model approximates the function used to generate the cost data. Hence we can now use these trained models to determine cost and volume for the <b>convex</b> optimisation. Results of <b>Linear</b> <b>regression</b> model \u2014Cost vs Volume . Optimisation using scipy. We will use the scipy optimise library for ...", "dateLastCrawled": "2022-01-30T23:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Visualizing <b>Optimization</b> in <b>Linear</b> <b>Regression</b> and Logistic <b>Regression</b> ...", "url": "https://medium.com/swlh/from-animation-to-intuition-linear-regression-and-logistic-regression-f641a31e1caf", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/from-animation-to-intuition-<b>linear</b>-<b>regression</b>-and-logistic...", "snippet": "This is called <b>convex</b> <b>optimization</b> and it\u2019s desired whenever possible. Nonconvex <b>optimization</b> is much harder. Nonconvex <b>optimization</b> is much harder. Surface plot of the 2D <b>linear</b> <b>regression</b> cost ...", "dateLastCrawled": "2022-01-25T19:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Lecture notes (<b>linear</b> <b>regression</b>)", "url": "https://web.cs.ucla.edu/~chohsieh/teaching/CS260_Winter2019/notes_linearregression.pdf", "isFamilyFriendly": true, "displayUrl": "https://web.cs.ucla.edu/~chohsieh/teaching/CS260_Winter2019/notes_<b>linearregression</b>.pdf", "snippet": "For a <b>convex</b> and continuously di erentiable <b>convex</b> function, we know w is a global minimum of f(w) if and only if rf(w) = 0: So, to solve (2), it\u2019s equivalent to nding a w such that rf(w ) = 0. This means w is minimum if and only if it satis es the following equation: normal equation: XTXw = XTy: (3) This is called \\normal equation&quot; for <b>linear</b> <b>regression</b>. To solve (3), we consider the following two cases: When XTX is invertible, eq (3) directly implies w = (XTX) 1XTy is the unique solution ...", "dateLastCrawled": "2022-01-30T02:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A <b>Convex Framework for Fair Regression</b>", "url": "https://www.cis.upenn.edu/~mkearns/papers/ConvexFATML.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cis.upenn.edu/~mkearns/papers/<b>Convex</b>FATML.pdf", "snippet": "A <b>Convex Framework for Fair Regression</b> Richard Berk, Hoda Heidari, Shahin Jabbari, Matthew Joseph Michael Kearns, Jamie Morgenstern, Seth Neel, Aaron Roth University of Pennsylvania Abstract We introduce a exible family of fairness regularizers for (<b>linear</b> and logistic) <b>regression</b> problems. These regular-izers all enjoy convexity, permitting fast <b>optimization</b>, and span the range from group fairness to strong indi-vidual fairness. We study the accuracy-fairness trade-o on any given dataset ...", "dateLastCrawled": "2022-01-28T01:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Why is Convex Optimization such a big</b> deal in Machine Learning? - Quora", "url": "https://www.quora.com/Why-is-Convex-Optimization-such-a-big-deal-in-Machine-Learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>Why-is-Convex-Optimization-such-a-big</b>-deal-in-Machine-Learning", "snippet": "Answer (1 of 10): <b>Convex</b> <b>optimization</b> is the core of most machine learning methods Some key concepts here: - <b>Convex</b> functions are those for which it&#39;s possible to draw a line segment from any two points on the graph and this line will always be inside the graph (except at the endpoints). This m...", "dateLastCrawled": "2022-01-17T17:05:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>convex</b> <b>optimization</b> - <b>linear Regression Simplification</b> - Mathematics ...", "url": "https://math.stackexchange.com/questions/1762474/linear-regression-simplification", "isFamilyFriendly": true, "displayUrl": "https://math.stackexchange.com/questions/1762474", "snippet": "<b>Mathematics Stack Exchange</b> is a question and answer site for people studying math at any level and professionals in related fields. It only takes a minute to sign up.", "dateLastCrawled": "2022-01-17T04:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Convex Optimization</b> - NUS Computing", "url": "https://www.comp.nus.edu.sg/~rahul/allfiles/cs6234-16-convex-opt.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.comp.nus.edu.sg/~rahul/allfiles/cs6234-16-<b>convex</b>-opt.pdf", "snippet": "<b>Convex Optimization</b> Hu SiXing, Hakki <b>Can</b> Karaimer, Pan An, Philipp Keck National University of Singapore Jan 20th, 2016 Advanced Algorithms <b>Convex Optimization</b> Jan 20th, 2016 1 / 42 . Motivation Unconstrained <b>Optimization</b> <b>Convex</b> Domain ApplicationsReferences <b>Linear</b> <b>Regression</b> Example Advanced Algorithms <b>Convex Optimization</b> Jan 20th, 2016 2 / 42. Motivation Unconstrained <b>Optimization</b> <b>Convex</b> Domain ApplicationsReferences Ordinary Least Squares Input: points (x i,y i) <b>Regression</b> line: y= mx+b ...", "dateLastCrawled": "2022-01-27T18:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Convex Optimization</b> - CUHK", "url": "https://www.inc.cuhk.edu.hk/sites/default/files/seminars/slides/cuhk_15.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.inc.cuhk.edu.hk/sites/default/files/seminars/slides/cuhk_15.pdf", "snippet": "I equality constraints are <b>linear</b> I f 0;:::;f m are <b>convex</b>: for 2[0;1], f i( x + (1 )y) f i(x) + (1 )f i(y) i.e., f i have nonnegative (upward) curvature <b>Convex Optimization</b> 13 . Why I beautiful, nearly complete theory I duality, optimality conditions, ... I e ective algorithms, methods (in theory and practice) I get global solution (and optimality certi cate) I polynomial complexity I conceptual uni cation of many methods I lots of applications (many more than previously <b>thought</b>) <b>Convex</b> ...", "dateLastCrawled": "2022-02-01T09:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Convex Optimization</b> Overview - Stanford University", "url": "http://cs229.stanford.edu/section/cs229-cvxopt.pdf", "isFamilyFriendly": true, "displayUrl": "cs229.stanford.edu/section/cs229-cvxopt.pdf", "snippet": "<b>Convex Optimization</b> Overview Zico Kolter (updated by Honglak Lee) October 17, 2008 1 Introduction Many situations arise in machine learning where we would like to optimize the value of some function. That is, given a function f : Rn \u2192 R, we want to \ufb01nd x \u2208 Rn that minimizes (or maximizes) f(x). We have already seen several examples of <b>optimization</b> problems in class: least-squares, logistic <b>regression</b>, and support vector machines <b>can</b> all be framed as <b>optimization</b> problems. It turns out ...", "dateLastCrawled": "2022-01-31T17:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "What is <b>the relationship between linear programming and</b> <b>convex</b> ...", "url": "https://www.researchgate.net/post/What-is-the-relationship-between-linear-programming-and-convex-optimization", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/post/What-is-<b>the-relationship-between-linear-programming</b>...", "snippet": "However, note that nonlinear programming, while technically including <b>convex</b> <b>optimization</b> (and excluding <b>linear</b> programming), <b>can</b> be used to refer to situations where the problem is not known to ...", "dateLastCrawled": "2022-01-28T04:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "8.1 Least squares <b>linear</b> <b>regression</b>", "url": "https://kenndanielso.github.io/mlrefined/blog_posts/8_Linear_regression/8_1_Least_squares_regression.html", "isFamilyFriendly": true, "displayUrl": "https://kenndanielso.github.io/mlrefined/blog_posts/8_<b>Linear</b>_<b>regression</b>/8_1_Least...", "snippet": "However the Least Squares cost function for <b>linear</b> <b>regression</b> <b>can</b> mathematically shown to be - in general - a <b>convex</b> function for any dataset (this is because one <b>can</b> show that it is always a <b>convex</b> quadratic - see Section 8.1.6 for details). Because of this we <b>can</b> easily apply either gradient descent or Newton&#39;s method in order to minimize it.", "dateLastCrawled": "2022-01-23T06:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Why is Convex Optimization such a big</b> deal in Machine Learning? - Quora", "url": "https://www.quora.com/Why-is-Convex-Optimization-such-a-big-deal-in-Machine-Learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>Why-is-Convex-Optimization-such-a-big</b>-deal-in-Machine-Learning", "snippet": "Answer (1 of 10): <b>Convex</b> <b>optimization</b> is the core of most machine learning methods Some key concepts here: - <b>Convex</b> functions are those for which it&#39;s possible to draw a line segment from any two points on the graph and this line will always be inside the graph (except at the endpoints). This m...", "dateLastCrawled": "2022-01-17T17:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "What do <b>we optimize in linear regression</b>? - Quora", "url": "https://www.quora.com/What-do-we-optimize-in-linear-regression", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-do-<b>we-optimize-in-linear-regression</b>", "snippet": "Answer (1 of 4): In the most common forms of <b>linear</b> <b>regression</b>, \u201coptimizing\u201d is actually minimizing. When our data include uncertainties and the errors are sufficiently approximated as Gaussian random variables (the vast majority of cases in the physical sciences), we minimize the value of chi-sq...", "dateLastCrawled": "2022-01-20T15:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "ML Series2: Regularization in <b>Linear</b> <b>Regression</b> | by Junlin Liu | Medium", "url": "https://duckmoll.medium.com/ml-series2-regularization-in-linear-regression-9ce6df083506", "isFamilyFriendly": true, "displayUrl": "https://duckmoll.medium.com/ml-series2-regularization-in-<b>linear</b>-<b>regression</b>-9ce6df083506", "snippet": "Multicollinearity <b>can</b> create inaccurate estimates of the <b>regression</b> coefficients, inflate the standard errors of the <b>regression</b> coefficients, deflate the partial t-tests for the <b>regression</b> coefficients, give false, nonsignificant, p-values, and degrade the predictability of the model. If there are duplicate features, a general purpose <b>optimization</b> algorithm will generate arbitrary breakdown between them.", "dateLastCrawled": "2022-01-12T18:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Can</b> We Use Stochastic Gradient Descent (SGD) on a <b>Linear</b> <b>Regression</b> ...", "url": "https://towardsdatascience.com/can-we-use-stochastic-gradient-descent-sgd-on-a-linear-regression-model-e50327b07d33", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>can</b>-we-use-stochastic-gradient-descent-sgd-on-a-<b>linear</b>...", "snippet": "Although the above proof is done on a <b>linear</b> <b>regression</b> model, you <b>can</b> recognize that the structure of the loss function of many neural network models are the same as the <b>linear</b> <b>regression</b> model. This means the above proof also applies to those neural network models, and this is the reason why stochastic gradient descent, or variations of it, such as Adam, <b>can</b> be the optimizer for neural networks.", "dateLastCrawled": "2022-01-31T07:00:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Convex Optimization</b> - NYU Courant", "url": "https://cims.nyu.edu/~cfgranda/pages/MTDS_spring19/notes/convex_optimization.pdf", "isFamilyFriendly": true, "displayUrl": "https://cims.nyu.edu/~cfgranda/pages/MTDS_spring19/notes/<b>convex_optimization</b>.pdf", "snippet": "<b>Convex Optimization</b> 1 Motivation 1.1 Sparse <b>regression</b> In our description of <b>linear</b> <b>regression</b> in the notes on the SVD, we observed that the performance of <b>linear</b> <b>regression</b> degrades when the number of features is close to the number of training data. This makes sense: the number of parameters of a model should be signi cantly smaller than the number of measurements used to t it. However, when the number of features is very large, it is often possible to achieve accurate prediction using ...", "dateLastCrawled": "2022-02-03T14:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A DIFFERENCE OF <b>CONVEX</b> <b>OPTIMIZATION</b> ALGORITHM FOR PIECEWISE <b>LINEAR</b> ...", "url": "https://www.aimsciences.org/article/exportPdf?id=843d75f3-c2d1-4c24-a97c-8057f6256f97", "isFamilyFriendly": true, "displayUrl": "https://www.aimsciences.org/article/exportPdf?id=843d75f3-c2d1-4c24-a97c-8057f6256f97", "snippet": "A DIFFERENCE OF <b>CONVEX</b> <b>OPTIMIZATION</b> ALGORITHM FOR PIECEWISE <b>LINEAR</b> <b>REGRESSION</b> Adil Bagirov and Sona Taheri Faculty of Science and Technology, Federation University Australia Victoria, Australia Soodabeh Asadi Department of Mathematics, Shahrekord University, Iran (Communicated by Gerhard-Wilhelm Weber) Abstract. The problem of nding a continuous piecewise <b>linear</b> function ap-proximating a <b>regression</b> function is considered. This problem is formulated as a nonconvex nonsmooth <b>optimization</b> ...", "dateLastCrawled": "2022-01-01T20:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A Difference of <b>Convex</b> Functions Algorithm for Switched <b>Linear</b> <b>Regression</b>", "url": "https://hal.archives-ouvertes.fr/hal-00931206/document", "isFamilyFriendly": true, "displayUrl": "https://hal.archives-ouvertes.fr/hal-00931206/document", "snippet": "Algorithm for Switched <b>Linear</b> <b>Regression</b>. IEEE Transactions on Automatic Control, Institute of Electrical and Electronics Engineers, 2014, \uffff10.1109/TAC.2014.2301575\uffff. \uffffhal-00931206\uffff IEEE TRANSACTIONS ON AUTOMATIC CONTROL 1 A Difference of <b>Convex</b> Functions Algorithm for Switched <b>Linear</b> <b>Regression</b> Tao PHAM DINH, Hoai Minh LE, Hoai An LE THI, and Fabien LAUER Abstract\u2014This paper deals with switched <b>linear</b> system iden-ti\ufb01cation and more particularly aims at solving switched <b>linear</b> ...", "dateLastCrawled": "2022-01-18T05:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Convex</b> <b>optimization</b> methods for dimension reduction and coefficient ...", "url": "https://experts.umn.edu/en/publications/convex-optimization-methods-for-dimension-reduction-and-coefficie", "isFamilyFriendly": true, "displayUrl": "https://experts.umn.edu/en/publications/<b>convex</b>-<b>optimization</b>-methods-for-dimension...", "snippet": "In this paper, we study <b>convex</b> <b>optimization</b> methods for computing the nuclear (or, trace) norm regularized least squares estimate in multivariate <b>linear</b> <b>regression</b>. The so-called factor estimation and selection method, recently proposed by Yuan et al. (J Royal Stat Soc Ser B (Statistical Methodology) 69(3):329-346, 2007) conducts parameter estimation and factor selection simultaneously and have been shown to enjoy nice properties in both large and finite samples. To compute the estimates ...", "dateLastCrawled": "2021-07-28T05:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A <b>convex</b> <b>optimization</b> formulation for multivariate <b>regression</b>", "url": "https://papers.nips.cc/paper/2020/file/ccd2d123f4ec4d777fc6ef757d0fb642-Paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://papers.nips.cc/paper/2020/file/ccd2d123f4ec4d777fc6ef757d0fb642-Paper.pdf", "snippet": "proposed <b>convex</b> formulation is in contrast with most existing high-dimensional multivariate <b>linear</b> <b>regression</b> methodologies, most of which either focus only on estimating the <b>regression</b> coef\ufb01cients alone, or estimating the <b>regression</b> coef\ufb01cients and the precision matrix alternately by solving a nonconvex <b>optimization</b> problem.", "dateLastCrawled": "2021-11-05T15:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Convex</b> <b>Optimization</b> Methods for Dimension Reduction and Coe\ufb03cient ...", "url": "http://www.optimization-online.org/DB_FILE///2008/01/1877.pdf", "isFamilyFriendly": true, "displayUrl": "www.<b>optimization</b>-online.org/DB_FILE///2008/01/1877.pdf", "snippet": "<b>Convex</b> <b>Optimization</b> Methods for Dimension Reduction and Coe\ufb03cient Estimation in Multivariate <b>Linear</b> <b>Regression</b> ... The performance of these methods is then <b>compared</b> using a set of randomly generated instances. We show that the best of Nesterov\u2019s \ufb01rst-order methods substantially outperforms the interior point method implemented in SDPT3 version 4.0 (beta) [15]. Moreover, the former method is much more memory e\ufb03cient. Key words: Cone programming, smooth saddle point problem, \ufb01rst ...", "dateLastCrawled": "2021-08-12T12:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "What are <b>the advantages of convex optimization compared to more general</b> ...", "url": "https://www.quora.com/What-are-the-advantages-of-convex-optimization-compared-to-more-general-optimization-problems", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-are-<b>the-advantages-of-convex-optimization-compared</b>-to-more...", "snippet": "Answer: Convexity confers two advantages. The first is that, in a constrained problem, a <b>convex</b> feasible region makes it easier to ensure that you do not generate infeasible solutions while searching for an optimum. If you have two feasible solutions, any solution within the line segment connecti...", "dateLastCrawled": "2022-01-14T12:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "why is the least square cost function for <b>linear regression</b> <b>convex</b>", "url": "https://math.stackexchange.com/questions/2774106/why-is-the-least-square-cost-function-for-linear-regression-convex", "isFamilyFriendly": true, "displayUrl": "https://math.stackexchange.com/questions/2774106/why-is-the-least-square-cost-function...", "snippet": "You might find it helpful to check out some <b>linear</b> algebra books, such as Gilbert Strang&#39;s book Introduction <b>to Linear</b> Algebra. $\\endgroup$ \u2013 littleO May 10 &#39;18 at 6:15", "dateLastCrawled": "2022-01-28T06:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Why <b>is nonconvex optimization so difficult compared</b> to <b>convex</b> ...", "url": "https://www.quora.com/Why-is-nonconvex-optimization-so-difficult-compared-to-convex-optimization", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Why-<b>is-nonconvex-optimization-so-difficult-compared</b>-to-<b>convex</b>...", "snippet": "Answer (1 of 4): It comes down to convexity, unfortunately. <b>Convex</b> problems are guaranteed to have any minimum (respectively maximum in a concave problem) be the global minimum (maximum) - by convexity. It&#39;s embedded in the definition of convexity. It&#39;s not the only way to think about a <b>convex</b> ...", "dateLastCrawled": "2022-01-22T06:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Optimization Algorithm vs Regression Models</b> - Stack Overflow", "url": "https://stackoverflow.com/questions/37393040/optimization-algorithm-vs-regression-models", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/37393040", "snippet": "These are also <b>convex</b> and <b>can</b> be trained efficiently (with not too much data). They are also designed to work well with kernels. An additional advantage (<b>compared</b> with more complex models: e.g. non-<b>convex</b>): they are quite good regarding generalization which seems to be important here because you don&#39;t have much data (sounds like a very small dataset).", "dateLastCrawled": "2022-01-28T21:04:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Optimization</b> for <b>Machine</b> <b>Learning</b>", "url": "http://optml.mit.edu/talks/pkuLectAlgo3.pdf", "isFamilyFriendly": true, "displayUrl": "optml.mit.edu/talks/pkuLectAlgo3.pdf", "snippet": "<b>Optimization</b> for <b>Machine</b> <b>Learning</b> \u2013 Sra, Nowozin, Wright Theory of <b>Convex</b> <b>Optimization</b> for <b>Machine</b> <b>Learning</b> \u2013 Bubeck NIPS 2016 <b>Optimization</b> Tutorial \u2013 Bach, Sra Some related courses: EE227A, Spring 2013, (Sra, UC Berkeley) 10-801, Spring 2014 (Sra, CMU) EE364a,b (Boyd, Stanford) EE236b,c (Vandenberghe, UCLA) Venues: NIPS, ICML, UAI, AISTATS, SIOPT, Math. Prog. Suvrit Sra(suvrit@mit.edu)<b>Optimization</b> for <b>Machine</b> <b>Learning</b> 2 / 29. Lecture Plan \u2013Introduction (3 lectures) \u2013Problems and ...", "dateLastCrawled": "2021-08-29T23:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Basic Concepts in Machine Learning</b>", "url": "https://machinelearningmastery.com/basic-concepts-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>basic-concepts-in-machine-learning</b>", "snippet": "For example combinatorial <b>optimization</b>, <b>convex</b> <b>optimization</b>, constrained <b>optimization</b>. All <b>machine learning</b> algorithms are combinations of these three components. A framework for understanding all algorithms. Types of <b>Learning</b> . There are four types of <b>machine learning</b>: Supervised <b>learning</b>: (also called inductive <b>learning</b>) Training data includes desired outputs. This is spam this is not, <b>learning</b> is supervised. Unsupervised <b>learning</b>: Training data does not include desired outputs. Example is ...", "dateLastCrawled": "2022-02-02T15:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Introduction to Machine Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/24/introduction-to-machine-learning-2/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/24/<b>introduction-to-machine-learning</b>-2", "snippet": "<b>Optimization</b> methods are applied to minimize the loss function by changing the parameter values, which is the central theme of <b>machine</b> <b>learning</b>.Zero-one loss is L0-1 = 1 (m &lt;= 0); in zero-one loss, value of loss is 0 for m &gt;= 0 whereas 1 for m &lt; 0. The difficult part with this loss is it is not differentiable, non-<b>convex</b>, and also NP-hard. Hence, in order to make <b>optimization</b> feasible and solvable, these losses are replaced by different surrogate losses for different problems.", "dateLastCrawled": "2022-01-17T05:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "11.2. <b>Convexity</b> \u2014 Dive into Deep <b>Learning</b> 0.17.2 documentation", "url": "https://d2l.ai/chapter_optimization/convexity.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_<b>optimization</b>/<b>convexity</b>.html", "snippet": "Furthermore, even though the <b>optimization</b> problems in deep <b>learning</b> are generally nonconvex, they often exhibit some properties of <b>convex</b> ones near local minima. This can lead to exciting new <b>optimization</b> variants such as [Izmailov et al., 2018].", "dateLastCrawled": "2022-01-30T21:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Gradient</b> Descent: A Quick, Simple Introduction | Built In", "url": "https://builtin.com/data-science/gradient-descent", "isFamilyFriendly": true, "displayUrl": "https://builtin.com/data-science/<b>gradient</b>-descent", "snippet": "<b>Gradient</b> descent is an <b>optimization</b> algorithm that&#39;s used when training a <b>machine</b> <b>learning</b> model. It&#39;s based on a <b>convex</b> function and tweaks its parameters iteratively to minimize a given function to its local minimum.", "dateLastCrawled": "2022-02-02T07:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Introduction to <b>Machine</b> <b>Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/30/introduction-to-machine-learning-3/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/30/introduction-to-<b>machine</b>-<b>learning</b>-3", "snippet": "Unsupervised <b>learning</b>: Similar to the teacher-student <b>analogy</b>, in which the instructor does not present and provide feedback to the student and who needs to prepare on his/her own. Unsupervised <b>learning</b> does not have as many are in supervised <b>learning</b>: Principal component analysis (PCA) K-means clustering; Reinforcement <b>learning</b>: This is the scenario in which multiple decisions need to be taken by an agent prior to reaching the target and it provides a reward, either +1 or -1, rather than ...", "dateLastCrawled": "2022-01-28T18:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>ICML Tutorial</b> \u2013 Parameter-free <b>Learning</b> and <b>Optimization</b> Algorithms", "url": "https://parameterfree.com/icml-tutorial/", "isFamilyFriendly": true, "displayUrl": "https://parameterfree.com/<b>icml-tutorial</b>", "snippet": "In particular, stochastic and batch <b>optimization</b> have become core skills for applied and theoretical <b>machine</b> <b>learning</b> researchers. This is clear from the amount of tutorials related to <b>optimization</b> in the latest years in ICML and NeurIPS. Yet, most of the advanced tutorials focused only on <b>optimization</b> under very strong assumptions, e.g. strong convexity, or finite-sum regimes, while the tutorials with a broad focus only briefly touched the basic concepts. In particular, the necessity of ...", "dateLastCrawled": "2022-02-02T03:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Regularization : What? Why? and How? (Part -1) | by Siddhant Rai ...", "url": "https://medium.com/mlearning-ai/regularization-what-why-and-how-part-1-ef6bdb6bafea", "isFamilyFriendly": true, "displayUrl": "https://medium.com/m<b>learning</b>-ai/regularization-what-why-and-how-part-1-ef6bdb6bafea", "snippet": "In general <b>machine</b> <b>learning</b> sense, it is solving an objective function to perform maximum or minimum evaluation. In reality, <b>optimization</b> is lot more profound in usage. Then we have two terms ...", "dateLastCrawled": "2022-01-28T00:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "7. <b>Optimization</b>, the central part of any <b>Machine</b> <b>Learning</b> algortithm ...", "url": "https://compphysics.github.io/MachineLearning/doc/LectureNotes/_build/html/chapteroptimization.html", "isFamilyFriendly": true, "displayUrl": "https://compphysics.github.io/<b>MachineLearning</b>/doc/LectureNotes/_build/html/chapter...", "snippet": "7. <b>Optimization</b>, the central part of any <b>Machine</b> <b>Learning</b> algortithm\u00b6. Almost every problem in <b>machine</b> <b>learning</b> and data science starts with a dataset \\(X\\), a model \\(g(\\beta)\\), which is a function of the parameters \\(\\beta\\) and a cost function \\(C(X, g(\\beta))\\) that allows us to judge how well the model \\(g(\\beta)\\) explains the observations \\(X\\).The model is fit by finding the values of \\(\\beta\\) that minimize the cost function. Ideally we would be able to solve for \\(\\beta ...", "dateLastCrawled": "2022-01-31T15:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "[2005.14605] CoolMomentum: A Method for Stochastic <b>Optimization</b> by ...", "url": "https://arxiv.org/abs/2005.14605", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/abs/2005.14605", "snippet": "This <b>analogy</b> provides useful insights for non-<b>convex</b> stochastic <b>optimization</b> in <b>machine</b> <b>learning</b>. Here we find that integration of the discretized Langevin equation gives a coordinate updating rule equivalent to the famous Momentum <b>optimization</b> algorithm. As a main result, we show that a gradual decrease of the momentum coefficient from the initial value close to unity until zero is equivalent to application of Simulated Annealing or slow cooling, in physical terms. Making use of this novel ...", "dateLastCrawled": "2021-10-23T08:45:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Best <b>Artificial Intelligence</b> Course (AIML) by UT Austin", "url": "https://www.mygreatlearning.com/pg-program-artificial-intelligence-course", "isFamilyFriendly": true, "displayUrl": "https://www.mygreat<b>learning</b>.com/pg-program-<b>artificial-intelligence</b>-course", "snippet": "<b>Machine</b> <b>Learning</b>: <b>Machine</b> <b>learning</b> is a sub-branch of AI that teaches machines to learn any task without the help of explicit directions. It teaches machines to learn by drawing inferences from past experience. <b>Machine</b> <b>learning</b> primarily focuses on developing computer programs that can access and analyze data to identify patterns and understand data behaviour to reach possible conclusions without any kind of human intervention.", "dateLastCrawled": "2022-02-01T17:58:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Which <b>machine</b> <b>learning</b> algorithms for classification support online ...", "url": "https://www.quora.com/Which-machine-learning-algorithms-for-classification-support-online-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Which-<b>machine</b>-<b>learning</b>-algorithms-for-classification-support...", "snippet": "Answer (1 of 5): Most algorithms can be adapted to make them online, even though the standard implementations may not support it. E.g. both decision trees and support ...", "dateLastCrawled": "2022-01-09T00:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "What is <b>the relationship between Online Machine Learning</b> and ...", "url": "https://www.quora.com/What-is-the-relationship-between-Online-Machine-Learning-and-Incremental-Machine-Learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>the-relationship-between-Online-Machine-Learning</b>-and...", "snippet": "Answer (1 of 4): Online <b>learning</b> usually refers to the case where each example is only used once (e.g. if you&#39;re updating an ad click prediction model online after each impression or click), while incremental methods usually pick one example at a time from a finite dataset and can process the sam...", "dateLastCrawled": "2022-01-14T06:00:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "SimplifiedMachineLearningWorkflows-book/Wolfram-Technology-Conference ...", "url": "https://github.com/antononcube/SimplifiedMachineLearningWorkflows-book/blob/master/Data/Wolfram-Technology-Conference-2016-to-2019-abstracts.csv", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/antononcube/Simplified<b>MachineLearning</b>Workflows-book/blob/master/...", "snippet": "Finally, I use <b>machine</b> <b>learning</b> algorithms to train a series of classifiers that can predict a text&#39;s authorship based on its MFW frequencies. Cross-validation indicates that Gallus and Monk are very likely one and the same author. The results also reveal the especially high and hitherto underexplored effectiveness of the Bray Curtis Distance measure and of logistic regression in shedding light on questions of authorship attribution. Data Analytics &amp; Information Science : 2016.Gunnar.Prei\u00df ...", "dateLastCrawled": "2021-12-28T12:42:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(convex optimization)  is like +(linear regression)", "+(convex optimization) is similar to +(linear regression)", "+(convex optimization) can be thought of as +(linear regression)", "+(convex optimization) can be compared to +(linear regression)", "machine learning +(convex optimization AND analogy)", "machine learning +(\"convex optimization is like\")", "machine learning +(\"convex optimization is similar\")", "machine learning +(\"just as convex optimization\")", "machine learning +(\"convex optimization can be thought of as\")", "machine learning +(\"convex optimization can be compared to\")"]}