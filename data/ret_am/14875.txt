{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Word</b> Embeddings - GitHub Pages", "url": "https://lena-voita.github.io/nlp_course/word_embeddings.html", "isFamilyFriendly": true, "displayUrl": "https://lena-voita.github.io/nlp_course/<b>word</b>_<b>embeddings</b>.html", "snippet": "How: Learn <b>word</b> vectors by <b>teaching</b> them to predict contexts. Word2Vec is a model whose parameters are <b>word</b> vectors. These parameters are optimized iteratively for a certain objective. The objective forces <b>word</b> vectors to &quot;know&quot; contexts a <b>word</b> can appear in: the vectors are trained to predict possible contexts of the corresponding words. As you remember from the distributional hypothesis, if vectors &quot;know&quot; about contexts, they &quot;know&quot; <b>word</b> meaning. Word2Vec is an iterative method. Its main ...", "dateLastCrawled": "2022-01-19T16:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A Beginner&#39;s Guide to <b>Word2Vec</b> and Neural <b>Word</b> Embeddings | Pathmind", "url": "https://wiki.pathmind.com/word2vec", "isFamilyFriendly": true, "displayUrl": "https://wiki.pathmind.com/<b>word2vec</b>", "snippet": "<b>Word2vec</b> \u201cvectorizes\u201d about words, and by doing so it makes natural <b>language</b> <b>computer</b>-readable \u2013 we can start to perform powerful mathematical operations on words to detect their similarities. So a neural <b>word</b> <b>embedding</b> represents a <b>word</b> with numbers. It\u2019s a simple, yet unlikely, translation. <b>Word2vec</b> is similar to an autoencoder, encoding each <b>word</b> in a vector, but rather than training against the input words through reconstruction, as a restricted Boltzmann machine does, <b>word2vec</b> ...", "dateLastCrawled": "2022-02-03T03:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Word embeddings</b> - Unsupervised representation learning | <b>Coursera</b>", "url": "https://www.coursera.org/lecture/intro-to-deep-learning/word-embeddings-dhzl5", "isFamilyFriendly": true, "displayUrl": "https://<b>www.coursera.org</b>/lecture/intro-to-deep-learning/<b>word-embeddings</b>-dhzl5", "snippet": "And you can, again, <b>use</b> one of those two matrices as your <b>word</b>-<b>embedding</b> matrix now. Because for example, in the <b>word</b>-to-context model, what you had is, this first matrix was, for every single possible sample, only one row of this matrix was kind of used at a time. So basically, you could assume that this kind of of row in the matrix is the vector corresponding to your <b>word</b> <b>and use</b> this matrix as your <b>word</b> <b>embedding</b>. If you train this model by yourself or if you <b>use</b> a pretrained model, you&#39;d ...", "dateLastCrawled": "2022-02-02T15:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Transfer Learning</b> Guide: A Practical Tutorial With Examples for Images ...", "url": "https://neptune.ai/blog/transfer-learning-guide-examples-for-images-and-text-in-keras", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/<b>transfer-learning</b>-guide", "snippet": "In the vector, words with similar meanings appear closer together. You can <b>use</b> the <b>embedding</b> layer in Keras to learn the <b>word</b> embeddings. Training <b>word</b> embeddings takes a lot of time, especially on large datasets, so let\u2019s <b>use</b> <b>word</b> embeddings that have already been trained. A couple of popular pre-trained <b>word</b> embeddings are Word2vec and GloVe.", "dateLastCrawled": "2022-02-03T06:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Natural <b>language</b> <b>word</b> embeddings as a glimpse into healthcare <b>language</b> ...", "url": "https://informatics.bmj.com/content/28/1/e100464", "isFamilyFriendly": true, "displayUrl": "https://informatics.bmj.com/content/28/1/e100464", "snippet": "Setting Secondary care, urban and suburban <b>teaching</b> hospitals. Participants All inpatients in 12-month period from 1 October 2018 to 30 September 2019. Methods Using unsupervised natural <b>language</b> processing, <b>word</b> <b>embedding</b> in latent space was used to generate phrase clusters with most similar semantic embeddings to \u2018Ceiling of Treatment\u2019 and their prognostication value. Results <b>Word</b> embeddings with most similarity to \u2018Ceiling of Treatment\u2019 clustered around phrases describing end-of ...", "dateLastCrawled": "2022-01-31T22:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>The Continuous Bag Of Words (CBOW</b>) Model in NLP - Hands-On", "url": "https://analyticsindiamag.com/the-continuous-bag-of-words-cbow-model-in-nlp-hands-on-implementation-with-codes/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/<b>the-continuous-bag-of-words-cbow</b>-model-in-nlp-hands-on...", "snippet": "The model tries to predict the target <b>word</b> by trying <b>to understand</b> the context of the surrounding words. Consider the same sentence as above, \u2018It is a pleasant day\u2019.The model converts this sentence into <b>word</b> pairs in the form (contextword, targetword). The user will have to set the window size. If the window for the context <b>word</b> is 2 then the <b>word</b> pairs would look <b>like</b> this: ([it, a], is), ([is, pleasant], a),([a, day], pleasant). With these <b>word</b> pairs, the model tries to predict the ...", "dateLastCrawled": "2022-02-03T11:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Let\u2019s <b>talk about the loss function</b> - <b>Word</b> Embeddings | <b>Coursera</b>", "url": "https://www.coursera.org/lecture/natural-language-processing-tensorflow/lets-talk-about-the-loss-function-PmhlZ", "isFamilyFriendly": true, "displayUrl": "https://<b>www.coursera.org</b>/lecture/natural-<b>language</b>-processing-tensorflow/lets-talk...", "snippet": "If you are a software developer who wants to build scalable AI-powered algorithms, you need <b>to understand</b> how to <b>use</b> the tools to build them. This Specialization will teach you best practices for using TensorFlow, a popular open-source framework for machine learning. In Course 3 of the deeplearning.ai TensorFlow Specialization, you will build natural <b>language</b> processing systems using TensorFlow. You will learn to process text, including tokenizing and representing sentences as vectors, so ...", "dateLastCrawled": "2022-01-23T19:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>TEACHING</b> VOCABULARY USING MATCHING <b>WORD</b> ON <b>COMPUTER</b> ASSISTED, <b>LANGUAGE</b> ...", "url": "https://eajournals.org/wp-content/uploads/Teaching-Vocabulary-Using-Matching-Word-on-Computer-Assisted-Language-Learning.pdf", "isFamilyFriendly": true, "displayUrl": "https://eajournals.org/wp-content/uploads/<b>Teaching</b>-Vocabulary-Using-Matching-<b>Word</b>-on...", "snippet": "<b>Language</b>. <b>Teaching</b> English in the city and in the rural a remote area <b>like</b> Siborong-borong is a chalange for teacher. Matching <b>word</b> on <b>computer</b> assited <b>language</b> learning, was applied to improve the students\u2019 vocabulary mastery. This is a way that could make the students easier learning vocabulary, for vocabulary mastery. Using Hot Potatoes is a good program because the students can correct and/or edit their own assignment based on the clued and feedback are given in the target <b>language</b> ...", "dateLastCrawled": "2022-01-17T20:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "The <b>Use</b> of <b>ICT in Teaching and Learning Process</b> | risqinoorh", "url": "https://risqinoorh.wordpress.com/2013/07/02/the-use-of-ict-in-teaching-and-learning-process/", "isFamilyFriendly": true, "displayUrl": "https://risqinoorh.<b>word</b>press.com/2013/07/02/the-<b>use</b>-of-<b>ict-in-teaching-and-learning</b>...", "snippet": "Risqi Noor Hidayati/ 2201410129/ 405-406Final Assignments of <b>Language</b> <b>Teaching</b> Technology The <b>Use</b> of <b>ICT in Teaching and Learning Process</b> 1. IntroductionSchool education has been criticized for focusing on irrelevant skills and knowledge and ignoring the demands of today\u2019s world. Today\u2019s world has been described as a knowledge society, referring to the fast development of information and communication\u2026", "dateLastCrawled": "2022-01-30T00:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(PDF) <b>Vocabulary and Language Teaching</b> - ResearchGate", "url": "https://www.researchgate.net/publication/49615330_Vocabulary_and_Language_Teaching", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/49615330_Vocabulary_and", "snippet": "Abrudan (2010) observes that vocabulary was, for many years, viewed as secondary to the overall purpose of <b>language</b> <b>teaching</b>, being the delivery of formal aspects of <b>language</b>, in particular ...", "dateLastCrawled": "2022-01-10T20:31:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Word</b> Embeddings - GitHub Pages", "url": "https://lena-voita.github.io/nlp_course/word_embeddings.html", "isFamilyFriendly": true, "displayUrl": "https://lena-voita.github.io/nlp_course/<b>word</b>_<b>embeddings</b>.html", "snippet": "The way machine learning models &quot;see&quot; data is different from how we (humans) do.For example, we can easily <b>understand</b> the text &quot;I saw a cat&quot;, but our models can not - they need vectors of features.Such vectors, or <b>word</b> embeddings, are representations of words which can be fed into your model.. How it works: Look-up Table (Vocabulary) In practice, you have a vocabulary of allowed words; you choose this vocabulary in advance.", "dateLastCrawled": "2022-01-19T16:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Word embeddings</b> - Unsupervised representation learning | <b>Coursera</b>", "url": "https://www.coursera.org/lecture/intro-to-deep-learning/word-embeddings-dhzl5", "isFamilyFriendly": true, "displayUrl": "https://<b>www.coursera.org</b>/lecture/intro-to-deep-learning/<b>word-embeddings</b>-dhzl5", "snippet": "So basically, you could assume that this kind of of row in the matrix is the vector corresponding to your <b>word</b> <b>and use</b> this matrix as your <b>word</b> <b>embedding</b>. If you train this model by yourself or if you <b>use</b> a pretrained model, you&#39;d actually notice that it has a lot of peculiar properties on top of what we actually wanted it to have. So of course, it does what we actually trained it for. It trains <b>similar</b> vectors for synonyms, and different vectors for semantically different words. But there&#39;s ...", "dateLastCrawled": "2022-02-02T15:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Natural <b>language</b> <b>word</b> embeddings as a glimpse into healthcare <b>language</b> ...", "url": "https://informatics.bmj.com/content/28/1/e100464", "isFamilyFriendly": true, "displayUrl": "https://informatics.bmj.com/content/28/1/e100464", "snippet": "Methods Using unsupervised natural <b>language</b> processing, <b>word</b> <b>embedding</b> in latent space was used to generate phrase clusters with most <b>similar</b> semantic embeddings to \u2018Ceiling of Treatment\u2019 and their prognostication value. Results <b>Word</b> embeddings with most similarity to \u2018Ceiling of Treatment\u2019 clustered around phrases describing end-of-life care, ceiling of care and LST discussions. The phrases have differing prognostic profile with the highest 7-day mortality in the phrases most ...", "dateLastCrawled": "2022-01-31T22:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Teaching</b> computers <b>to understand</b> the sentiment of tweets | by ...", "url": "https://towardsdatascience.com/making-computers-understand-the-sentiment-of-tweets-1271ab270bc7", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/making-<b>computers</b>-<b>understand</b>-the-sentiment-of-tweets...", "snippet": "An <b>embedding</b> model turns a <b>word</b> or a sentence into a vector, which is continuously adjusted during training such that words and sentences with <b>similar</b> meanings end up with <b>similar</b> vectors. Ideally then, the vector should capture the meaning, context, sentiment etc. of a sentence, but this is not an easy task at all, which is why many different <b>embedding</b> models have been developed. Generally, newer models perform better, but they may also be tuned to specific tasks.", "dateLastCrawled": "2022-02-02T13:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A <b>Deep Learning Approach in Predicting the</b> Next <b>Word</b>(s) | by Kamil ...", "url": "https://towardsdatascience.com/a-deep-learning-approach-in-predicting-the-next-word-s-7b0ee9341bfe", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-<b>deep-learning-approach-in-predicting-the</b>-next-<b>word</b>-s...", "snippet": "<b>Word</b> embeddings enable us to represent words in a n_dimensional space where words such as \u201cgood\u201d and \u201cgreat\u201d have <b>similar</b> representations in this n_dimensional space which the <b>computer</b> can <b>understand</b>. In the image below we can see <b>word</b> embeddings (7-dimensional) for words such as dog, puppy, cat, houses, man, woman, king, and queen. The dimensions are unknown to us (not interpretable) as the model learns these dimensions as it iterates over the data. That said, to aid the reader in ...", "dateLastCrawled": "2022-01-31T10:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>The Continuous Bag Of Words (CBOW</b>) Model in NLP - Hands-On", "url": "https://analyticsindiamag.com/the-continuous-bag-of-words-cbow-model-in-nlp-hands-on-implementation-with-codes/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/<b>the-continuous-bag-of-words-cbow</b>-model-in-nlp-hands-on...", "snippet": "The model tries to predict the target <b>word</b> by trying <b>to understand</b> the context of the surrounding words. Consider the same sentence as above, \u2018It is a pleasant day\u2019.The model converts this sentence into <b>word</b> pairs in the form (contextword, targetword). The user will have to set the window size. If the window for the context <b>word</b> is 2 then the <b>word</b> pairs would look like this: ([it, a], is), ([is, pleasant], a),([a, day], pleasant). With these <b>word</b> pairs, the model tries to predict the ...", "dateLastCrawled": "2022-02-03T11:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Transfer Learning</b> Guide: A Practical Tutorial With Examples for Images ...", "url": "https://neptune.ai/blog/transfer-learning-guide-examples-for-images-and-text-in-keras", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/<b>transfer-learning</b>-guide", "snippet": "In the vector, words with <b>similar</b> meanings appear closer together. You can <b>use</b> the <b>embedding</b> layer in Keras to learn the <b>word</b> embeddings. Training <b>word</b> embeddings takes a lot of time, especially on large datasets, so let\u2019s <b>use</b> <b>word</b> embeddings that have already been trained. A couple of popular pre-trained <b>word</b> embeddings are Word2vec and GloVe.", "dateLastCrawled": "2022-02-03T06:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Analyzing text semantic <b>similarity</b> using TensorFlow Hub and Dataflow ...", "url": "https://cloud.google.com/architecture/analyzing-text-semantic-similarity-using-tensorflow-and-cloud-dataflow", "isFamilyFriendly": true, "displayUrl": "https://cloud.google.com/architecture/analyzing-text-semantic-<b>similarity</b>-using-tensor...", "snippet": "In machine learning (ML), a text <b>embedding</b> is a real-valued feature vector that represents the semantics of a <b>word</b> (for example, by using Word2vec) or a sentence (for example, by using Universal Sentence Encoder). Embeddings can be either pre-trained in generic contexts or trained for specific tasks. Text embeddings are used to represent textual input features to ML models, such as classification, regression, and clustering.", "dateLastCrawled": "2022-01-30T00:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Word2Vec</b> - W&amp;B", "url": "https://wandb.ai/authors/embeddings/reports/Word2Vec---VmlldzozMzIxNjQ", "isFamilyFriendly": true, "displayUrl": "https://wandb.ai/authors/<b>embeddings</b>/reports/<b>Word2Vec</b>---VmlldzozMzIxNjQ", "snippet": "This is the first part of <b>word</b> <b>embedding</b> learning. Made by Devjyoti Chakraborty using Weights &amp; Biases. Devjyoti Chakraborty. Meaning -&gt; XKCD: I could care less &lt;-Humans are intelligent because they have a network of other human beings and a communication medium in <b>language</b>. <b>Language</b> is a relatively new concept in the history of humanity. Senses and learning through senses came way before <b>language</b>. With the advent of <b>language</b>, humans made a startling difference among the other animals ...", "dateLastCrawled": "2021-12-27T20:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Complete Tutorial On Txtai: An AI-Powered Search Engine", "url": "https://analyticsindiamag.com/complete-tutorial-on-txtai-an-ai-powered-search-engine/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/complete-tutorial-on-txtai-an-ai-powered-search-engine", "snippet": "Let us now <b>understand</b> how the txtai works by implementing a few small projects. Installing txtai. Since this was developed on python you can easily install this library with the pip command. To install this library <b>use</b>: pip install txtai. Implementation of <b>embedding</b> instances. The basic entry point and feature of the txtai are the <b>embedding</b> ...", "dateLastCrawled": "2022-02-02T02:57:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A Beginner&#39;s Guide to <b>Word2Vec</b> and Neural <b>Word</b> Embeddings | Pathmind", "url": "https://wiki.pathmind.com/word2vec", "isFamilyFriendly": true, "displayUrl": "https://wiki.pathmind.com/<b>word2vec</b>", "snippet": "So a neural <b>word</b> <b>embedding</b> represents a <b>word</b> with numbers. It\u2019s a simple, yet unlikely, translation. It\u2019s a simple, yet unlikely, translation. <b>Word2vec</b> is similar to an autoencoder, encoding each <b>word</b> in a vector, but rather than training against the input words through reconstruction, as a restricted Boltzmann machine does, <b>word2vec</b> trains words against other words that neighbor them in the input corpus.", "dateLastCrawled": "2022-02-03T03:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 1, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Deep learning</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Deep_learning", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Deep_learning</b>", "snippet": "<b>Word</b> <b>embedding</b>, such as word2vec, <b>can</b> <b>be thought</b> of as a representational layer in a <b>deep learning</b> architecture that transforms an atomic <b>word</b> into a positional representation of the <b>word</b> relative to other words in the dataset; the position is represented as a point in a vector space. Using <b>word</b> <b>embedding</b> as an RNN input layer allows the network to parse sentences and phrases using an effective compositional vector grammar. A compositional vector grammar <b>can</b> <b>be thought</b> of as", "dateLastCrawled": "2022-02-02T23:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Word</b> embeddings quantify 100 years of gender and ethnic stereotypes | <b>PNAS</b>", "url": "https://www.pnas.org/content/115/16/E3635", "isFamilyFriendly": true, "displayUrl": "https://<b>www.pnas.org</b>/content/115/16/E3635", "snippet": "<b>Word</b> embeddings are a popular machine-learning method that represents each English <b>word</b> by a vector, such that the geometry between these vectors captures semantic relations between the corresponding words. We demonstrate that <b>word</b> embeddings <b>can</b> be used as a powerful tool to quantify historical trends and social change. As specific applications, we develop metrics based on <b>word</b> embeddings to characterize how gender stereotypes and attitudes toward ethnic minorities in the United States ...", "dateLastCrawled": "2022-01-21T01:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Culturally Responsive Teaching</b>: 5 Strategies for Educators", "url": "https://www.northeastern.edu/graduate/blog/culturally-responsive-teaching-strategies/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.northeastern.edu</b>/graduate/blog/<b>culturally-responsive-teaching</b>-strategies", "snippet": "<b>Culturally responsive teaching</b>, also called culturally relevant <b>teaching</b>, is a pedagogy that recognizes the importance of including students\u2019 cultural references in all aspects of learning. Traditional <b>teaching</b> strategies emphasize the teacher-student dynamic: The teacher is the expert and adheres strictly to the curriculum that supports standardized tests while the student receives the knowledge. This <b>teaching</b> method is outdated, Childers-McKee says.", "dateLastCrawled": "2022-01-31T19:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "16 <b>Innovative Ideas to Make Your</b> <b>Teaching</b> Methods Effective", "url": "https://www.edsys.in/16-innovative-ideas-make-teaching-methods-effective/", "isFamilyFriendly": true, "displayUrl": "https://www.edsys.in/16-innovative-ideas-m", "snippet": "The role playing approach will help a student <b>understand</b> how the academic material will be relevant to his everyday tasks. Role playing is most effective for students of almost any age group. You just need to customize depending on the age group. You <b>can</b> even <b>use</b> this method for <b>teaching</b> preschoolers. Just make sure you keep it simple enough to capture their limited attention span. 7. Storyboard <b>Teaching</b>. Rudyard Kipling rightly said, \u201cIf history were taught in the form of stories, it ...", "dateLastCrawled": "2022-02-02T09:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "What is <b>Language</b> Modeling? - SearchEnterpriseAI", "url": "https://www.techtarget.com/searchenterpriseai/definition/language-modeling", "isFamilyFriendly": true, "displayUrl": "https://www.techtarget.com/searchenterpriseai/definition/<b>language</b>-modeling", "snippet": "stochastic: 1) Generally, stochastic (pronounced stow-KAS-tik , from the Greek stochastikos , or &quot;skilled at aiming,&quot; since stochos is a target) describes an approach to anything that is based on probability.", "dateLastCrawled": "2022-02-01T13:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Teaching aids and Instructional materials- tools for teachers</b> and ...", "url": "https://cognitiontoday.com/teaching-aids-and-instructional-materials-tools-for-teachers-and-students/", "isFamilyFriendly": true, "displayUrl": "https://cognitiontoday.com/<b>teaching-aids-and-instructional-materials</b>-tools-for...", "snippet": "Sometimes, graphical media <b>can</b> be used as both \u2013 infographics could be a <b>teaching</b> aid if they help but not a core <b>teaching</b> resource, or they <b>can</b> be embedded within a book or used as a way to summarize a larger concept directly. Digital media is often considered as an Instructional material because information is embedded in it and it needs planning for educational <b>use</b>. This planning is eventually integrated into the coursework as a learning activity.", "dateLastCrawled": "2022-02-02T22:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "How <b>To Use Technology</b> to Learn, Not Learn <b>To Use Technology</b> \u2013 Learning ...", "url": "https://myclassroomlearninglibrary.wordpress.com/2018/07/06/how-to-use-technology-to-learn-not-learn-to-use-technology/", "isFamilyFriendly": true, "displayUrl": "https://myclassroomlearninglibrary.<b>word</b>press.com/2018/07/06/how-<b>to-use-technology</b>-to...", "snippet": "Therefore, rather than <b>teaching</b> digital citizenship or digital literacy as a separate unit as I have in the past, I want to move towards <b>embedding</b> these skills into my inquiry projects so that my students <b>can</b> see their relevance and how they could then <b>use</b> these skills independently later. For example, rather than <b>teaching</b> keywording in order to google or search for information, I would want to imbed this lesson and other technology lessons into integrated units for social studies, science ...", "dateLastCrawled": "2022-01-31T11:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "The |<b>Importance of ICT in Education</b>| - ICTE Solutions", "url": "https://www.ictesolutions.com.au/blog/why-schools-should-invest-in-ict/", "isFamilyFriendly": true, "displayUrl": "https://www.ictesolutions.com.au/blog/<b>why-schools-should-invest-in-ict</b>", "snippet": "ICT <b>use</b> encourages collaboration: You just have to put a laptop, iPad or <b>computer</b> in the classroom <b>to understand</b> how this works. ICT naturally brings children together where they <b>can</b> talk and discuss what they are doing for their work and this in turn, opens up avenues for communication thus leading to <b>language</b> development. ICT <b>use</b> motivates learning: Society&#39;s demands for new technology has not left out children and their needs. Children are fascinated with technology and it encourages and ...", "dateLastCrawled": "2022-02-02T23:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "15 Technology Tools To Engage Students In The Classroom", "url": "https://www.teachthought.com/technology/technology-tools/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.teachthought.com</b>/technology/technology-tools", "snippet": "Like a few others on this list, you\u2019ve likely heard of Socrative, a tool to \u201cassess student understanding with prepared activities or on-the-fly questions, then adjust your <b>teaching</b> based on the results.\u201d 6. Kahoot! Kahoot! is a handy tool that students <b>can</b> <b>use</b> to create in-class questionnaires and quizzes. This is handy for obtaining ...", "dateLastCrawled": "2022-02-02T15:15:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Contextual and Non-Contextual <b>Word</b> Embeddings: an in-depth Linguistic ...", "url": "https://aclanthology.org/2020.repl4nlp-1.15.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/2020.repl4nlp-1.15.pdf", "snippet": "Department of <b>Computer</b> Science, University of Pisa ItaliaNLP Lab, Istituto di Linguistica Computazionale \u201cAntonio Zampolli\u201d, Pisa alessio.miaschi@phd.unipi.it, felice.dellorletta@ilc.cnr.it Abstract In this paper we present a comparison between the linguistic knowledge encoded in the inter-nal representations of a contextual <b>Language</b> Model (BERT) and a contextual-independent one (Word2vec). We <b>use</b> a wide set of prob-ing tasks, each of which corresponds to a distinct sentence-level ...", "dateLastCrawled": "2022-01-31T14:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Word</b> Embeddings - GitHub Pages", "url": "https://lena-voita.github.io/nlp_course/word_embeddings.html", "isFamilyFriendly": true, "displayUrl": "https://lena-voita.github.io/nlp_course/<b>word</b>_<b>embeddings</b>.html", "snippet": "The way machine learning models &quot;see&quot; data is different from how we (humans) do.For example, we <b>can</b> easily <b>understand</b> the text &quot;I saw a cat&quot;, but our models <b>can</b> not - they need vectors of features.Such vectors, or <b>word</b> embeddings, are representations of words which <b>can</b> be fed into your model.. How it works: Look-up Table (Vocabulary) In practice, you have a vocabulary of allowed words; you choose this vocabulary in advance.", "dateLastCrawled": "2022-01-19T16:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A <b>Deep Learning Approach in Predicting the</b> Next <b>Word</b>(s) | by Kamil ...", "url": "https://towardsdatascience.com/a-deep-learning-approach-in-predicting-the-next-word-s-7b0ee9341bfe", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-<b>deep-learning-approach-in-predicting-the</b>-next-<b>word</b>-s...", "snippet": "<b>Word</b> embeddings enable us to represent words in a n_dimensional space where words such as \u201cgood\u201d and \u201cgreat\u201d have similar representations in this n_dimensional space which the <b>computer</b> <b>can</b> <b>understand</b>. In the image below we <b>can</b> see <b>word</b> embeddings (7-dimensional) for words such as dog, puppy, cat, houses, man, woman, king, and queen. The dimensions are unknown to us (not interpretable) as the model learns these dimensions as it iterates over the data. That said, to aid the reader in ...", "dateLastCrawled": "2022-01-31T10:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Word</b> embeddings quantify 100 years of gender and ethnic stereotypes | <b>PNAS</b>", "url": "https://www.pnas.org/content/115/16/E3635", "isFamilyFriendly": true, "displayUrl": "https://<b>www.pnas.org</b>/content/115/16/E3635", "snippet": "Using the embeddings and <b>word</b> lists, one <b>can</b> measure the strength of association (<b>embedding</b> bias) between neutral words and a group. As an example, we overview the steps we <b>use</b> to quantify the occupational <b>embedding</b> bias for women. We first compute the average <b>embedding</b> distance between words that represent women\u2014e.g., she, female\u2014and words for occupations\u2014e.g., teacher, lawyer. For comparison, we also compute the average <b>embedding</b> distance between words that represent men and the same ...", "dateLastCrawled": "2022-01-21T01:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A Beginner&#39;s Guide to <b>Word2Vec</b> and Neural <b>Word</b> Embeddings | Pathmind", "url": "https://wiki.pathmind.com/word2vec", "isFamilyFriendly": true, "displayUrl": "https://wiki.pathmind.com/<b>word2vec</b>", "snippet": "So a neural <b>word</b> <b>embedding</b> represents a <b>word</b> with numbers. It\u2019s a simple, yet unlikely, translation. It\u2019s a simple, yet unlikely, translation. <b>Word2vec</b> is similar to an autoencoder, encoding each <b>word</b> in a vector, but rather than training against the input words through reconstruction, as a restricted Boltzmann machine does, <b>word2vec</b> trains words against other words that neighbor them in the input corpus.", "dateLastCrawled": "2022-02-03T03:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) <b>Vocabulary and Language Teaching</b> - ResearchGate", "url": "https://www.researchgate.net/publication/49615330_Vocabulary_and_Language_Teaching", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/49615330_Vocabulary_and", "snippet": "The computerized corpus analysis <b>can</b> effectively be used in <b>teaching</b> vocabulary (Read, 2004) since <b>word</b> frequency (Schmitt, 2000), <b>word</b> meanings in context, and the collocational patterns of words ...", "dateLastCrawled": "2022-01-10T20:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Culturally Responsive Teaching</b>: 5 Strategies for Educators", "url": "https://www.northeastern.edu/graduate/blog/culturally-responsive-teaching-strategies/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.northeastern.edu</b>/graduate/blog/<b>culturally-responsive-teaching</b>-strategies", "snippet": "<b>Culturally responsive teaching</b>, also called culturally relevant <b>teaching</b>, is a pedagogy that recognizes the importance of including students\u2019 cultural references in all aspects of learning. Traditional <b>teaching</b> strategies emphasize the teacher-student dynamic: The teacher is the expert and adheres strictly to the curriculum that supports standardized tests while the student receives the knowledge. This <b>teaching</b> method is outdated, Childers-McKee says.", "dateLastCrawled": "2022-01-31T19:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Teaching</b> computers <b>to understand</b> the sentiment of tweets | by ...", "url": "https://towardsdatascience.com/making-computers-understand-the-sentiment-of-tweets-1271ab270bc7", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/making-<b>computers</b>-<b>understand</b>-the-sentiment-of-tweets...", "snippet": "The aim of the project was to test how well computers <b>can</b> <b>understand</b> the sentiment of text using machine learning. To do this, we fed the <b>computer</b> with lots of tweets that had each been labelled as having either positive, neutral, or negative sentiment by humans. Each tweet also had an associated topic, which is important to make <b>use</b> of since a sentence <b>can</b> have very different sentiment depending on the topic discussed. For instance, the <b>word</b> \u201chigh\u201d is positive if we are talking about ...", "dateLastCrawled": "2022-02-02T13:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Strategies for Assessment for Learning", "url": "https://www.structural-learning.com/post/strategies-for-assessment-for-learning", "isFamilyFriendly": true, "displayUrl": "https://www.structural-learning.com/post/strategies-for-assessment-for-learning", "snippet": "These technologies include <b>computer</b> based testing, ... They provide opportunities for educators to assess how much knowledge learners possess before they start <b>teaching</b> them. <b>Embedding</b> these strategies in your approach <b>to teaching</b> will help you achieve better results. The mental modelling strategy using the building blocks enables teachers to assess the levels of understanding in an engaging activity. This approach to deep thinking helps students to develop conceptual understanding and ...", "dateLastCrawled": "2022-02-02T18:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How <b>To Predict Authors Features From His Writings</b>", "url": "https://analyticsindiamag.com/how-to-predict-authors-features-from-his-writings/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/how-<b>to-predict-authors-features-from-his-writings</b>", "snippet": "We <b>compared</b> predictions for 2 sentences and the model had correctly predicted the labels. print(&quot; Predicted :&quot;,y_pred ... It&#39;s really fascinating <b>teaching</b> a machine to see and <b>understand</b> images. Also, the interest gets doubled when the machine <b>can</b> tell you what it just saw. This is where I say I am highly interested in <b>Computer</b> Vision and Natural <b>Language</b> Processing. I love exploring different <b>use</b> cases that <b>can</b> be build with the power of AI. I am the person who first develops something and ...", "dateLastCrawled": "2022-01-29T06:31:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Word</b> Embeddings, Analogies, and <b>Machine</b> <b>Learning</b>: Beyond king - man ...", "url": "https://aclanthology.org/C16-1332.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/C16-1332.pdf", "snippet": "<b>Word</b> Embeddings, Analogies, and <b>Machine</b> <b>Learning</b>: Beyond King - Man + Woman = Queen Aleksandr Drozd y, Anna Gladkova z, Satoshi Matsuoka y yTokyo Institute of Technology, Meguro-ku, Tokyo 152-8550, Japan alex@smg.is.titech.ac.jp, matsu@is.titech.ac.jp z The University of Tokyo, Meguro-ku, Tokyo 153-8902 Japan gladkova@phiz.c.u-tokyo.ac.jp Abstract Solving <b>word</b> analogies became one of the most popular benchmarks for <b>word</b> embeddings on the assumption that linear relations between <b>word</b> pairs ...", "dateLastCrawled": "2022-01-20T00:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Word</b> Embeddings, Analogies, and <b>Machine</b> <b>Learning</b>: Beyond king - man ...", "url": "https://aclanthology.org/C16-1332/", "isFamilyFriendly": true, "displayUrl": "https://acl<b>anthology</b>.org/C16-1332", "snippet": "\ufeff%0 Conference Proceedings %T <b>Word</b> Embeddings, Analogies, and <b>Machine</b> <b>Learning</b>: Beyond king - man + woman = queen %A Drozd, Aleksandr %A Gladkova, Anna %A Matsuoka, Satoshi %S Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers %D 2016 %8 dec %I The COLING 2016 Organizing Committee %C Osaka, Japan %F drozd-etal-2016-<b>word</b> %X Solving <b>word</b> analogies became one of the most popular benchmarks for <b>word</b> embeddings on the assumption that ...", "dateLastCrawled": "2022-01-17T09:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>Word</b> Embeddings, Analogies, and <b>Machine</b> <b>Learning</b>: Beyond King ...", "url": "https://www.researchgate.net/publication/311843169_Word_Embeddings_Analogies_and_Machine_Learning_Beyond_King_-_Man_Woman_Queen", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/311843169_<b>Word</b>_<b>Embeddings</b>_Analogies_and...", "snippet": "<b>Word</b> Embeddings, Analogies, and <b>Machine</b> <b>Learning</b>: Beyond King - Man+ Woman= Queen December 2016 Conference: Proceedings of COLING 2016, the 26th International Conference on Computational ...", "dateLastCrawled": "2021-11-25T14:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Finding the <b>Word</b> <b>Analogy</b> from given words using Word2Vec embeddings ...", "url": "https://www.geeksforgeeks.org/finding-the-word-analogy-from-given-words-using-word2vec-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/finding-the-<b>word</b>-<b>analogy</b>-from-given-<b>words</b>-using-<b>word</b>2vec...", "snippet": "What if we can use a <b>Machine</b> <b>Learning</b> algorithm to automate this task of finding the <b>word</b> <b>analogy</b>. In ... Overview of <b>Word</b> <b>Embedding</b> using Embeddings from Language Models (ELMo) 16, Mar 21. <b>Word</b> Embeddings in NLP. 11, Oct 20. Implement your own word2vec(skip-gram) model in Python. 18, Jan 19. Scraping And Finding Ordered Words In A Dictionary using Python. 23, Jul 17 . Python - Replace all words except the given <b>word</b>. 25, Sep 20. Python | Finding &#39;n&#39; Character Words in a Text File. 15, Oct ...", "dateLastCrawled": "2022-01-26T14:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>What Are Word Embeddings</b> for Text? - <b>Machine</b> <b>Learning</b> Mastery", "url": "https://machinelearningmastery.com/what-are-word-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>what-are-word-embeddings</b>", "snippet": "<b>Word</b> embeddings are a type of <b>word</b> representation that allows words with similar meaning to have a similar representation. They are a distributed representation for text that is perhaps one of the key breakthroughs for the impressive performance of deep <b>learning</b> methods on challenging natural language processing problems. In this post, you will discover the <b>word</b> <b>embedding</b> approach for representing text data. After", "dateLastCrawled": "2022-01-30T18:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Word Embeddings in NLP - GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/word-embeddings-in-nlp/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/<b>word</b>-<b>embeddings</b>-in-nlp", "snippet": "<b>Word</b> Embeddings are a method of extracting features out of text so that we can input those features into a <b>machine</b> <b>learning</b> model to work with text data. They try to preserve syntactical and semantic information. The methods such as Bag of Words(BOW), CountVectorizer and TFIDF rely on the <b>word</b> count in a sentence but do not save any syntactical or semantic information. In these algorithms, the size of the vector is the number of elements in the vocabulary. We can get a sparse matrix if most ...", "dateLastCrawled": "2022-01-30T08:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "How to Solve <b>Analogies</b> with Word2Vec | by Khuyen Tran | Towards Data ...", "url": "https://towardsdatascience.com/how-to-solve-analogies-with-word2vec-6ebaf2354009", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/how-to-solve-<b>analogies</b>-with-<b>word</b>2vec-6ebaf2354009", "snippet": "To make words understood by <b>machine</b> <b>learning</b> algorithms, <b>word</b> <b>embedding</b> is used to map words into vectors of real numbers. There are various <b>word</b> <b>embedding</b> models and word2vec is one of them. In simple words, word2vec is a group of related models that are used to produce <b>word</b> embeddings. These models are trained to construct the linguistic contexts of words. Word2vec takes a large corpus of text and produces a vector space, with each unique <b>word</b> in the corpus being assigned to a ...", "dateLastCrawled": "2022-01-30T16:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "GitHub - jungsoh/<b>word</b>-embeddings-<b>word</b>-<b>analogy</b>-by-document-similarity ...", "url": "https://github.com/jungsoh/word-embeddings-word-analogy-by-document-similarity", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/jungsoh/<b>word</b>-<b>embeddings</b>-<b>word</b>-<b>analogy</b>-by-document-similarity", "snippet": "An example of a <b>word</b> <b>analogy</b> problem is to fill in the blank: Man is to Woman as King is to _____`. Because <b>word</b> embeddings are very computationally expensive to train, most <b>machine</b> <b>learning</b> practitioners will load a pre-trained set of embeddings. We will load a collection of pre-trained embeddings and measure similarity between <b>word</b> embeddings ...", "dateLastCrawled": "2022-01-25T02:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Introduction to <b>Word Embeddings</b>. What is a <b>word</b> <b>embedding</b>? | by Hunter ...", "url": "https://towardsdatascience.com/introduction-to-word-embeddings-4cf857b12edc", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/introduction-to-<b>word-embeddings</b>-4cf857b12edc", "snippet": "A very basic definition of a <b>word</b> <b>embedding</b> is a real number, vector representation of a <b>word</b>. Typically, these days, words with similar meaning will have vector representations that are close together in the <b>embedding</b> space (though this hasn\u2019t always been the case). When constructing a <b>word</b> <b>embedding</b> space, typically the goal is to capture some sort of relationship in that space, be it meaning, morphology, context, or some other kind of relationship. By encoding <b>word embeddings</b> in a ...", "dateLastCrawled": "2022-01-30T04:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "GitHub - jungsoh/wordvecs-<b>word</b>-<b>analogy</b>-by-document-similarity: Use of ...", "url": "https://github.com/jungsoh/wordvecs-word-analogy-by-document-similarity", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/jungsoh/<b>word</b>vecs-<b>word</b>-<b>analogy</b>-by-document-similarity", "snippet": "An example of a <b>word</b> <b>analogy</b> problem is to fill in the blank: Man is to Woman as King is to _____`. Because <b>word</b> embeddings (i.e. <b>word</b> vectors) are very computationally expensive to train, most <b>machine</b> <b>learning</b> practitioners will load a pre-trained set of embeddings. We will load a collection of pre-trained embeddings and measure similarity ...", "dateLastCrawled": "2022-01-28T11:28:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Word Embeddings Explained. What is <b>Word Embedding</b> ? | by Ashwin Prasad ...", "url": "https://medium.com/analytics-vidhya/word-embeddings-explained-62c046f7c79e", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/word-<b>embedding</b>s-explained-62c046f7c79e", "snippet": "<b>Word Embedding</b> is a technique in Natural Language Processing which is used to represent words in a Deep <b>Learning</b> environment. The main advantage of using <b>word embedding</b> is that it allows words of\u2026", "dateLastCrawled": "2022-01-24T11:15:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Survey and challenges of story generation models - A multimodal ...", "url": "https://www.sciencedirect.com/science/article/pii/S156625352030378X", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S156625352030378X", "snippet": "Pang et al. used the deep Bolzmann <b>machine</b> (DBM), which is a joint density model for the visual, auditory, and textual modalities, for <b>learning</b> highly non-linear relations between low-level features across different modalities for emotional prediction. It is trained using joint representation over multimodal inputs; thus, it can handle training samples, which is absent from certain modality. It can be used for emotional prediction and retrieval on any combination of modalities.", "dateLastCrawled": "2022-01-24T04:42:00.0000000Z", "language": "en", "isNavigational": false}], [], []], "all_bing_queries": ["+(word embedding)  is like +(teaching a computer to understand and use language)", "+(word embedding) is similar to +(teaching a computer to understand and use language)", "+(word embedding) can be thought of as +(teaching a computer to understand and use language)", "+(word embedding) can be compared to +(teaching a computer to understand and use language)", "machine learning +(word embedding AND analogy)", "machine learning +(\"word embedding is like\")", "machine learning +(\"word embedding is similar\")", "machine learning +(\"just as word embedding\")", "machine learning +(\"word embedding can be thought of as\")", "machine learning +(\"word embedding can be compared to\")"]}