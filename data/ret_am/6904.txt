{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Least Squares Regression</b> - How to Create <b>Line</b> of <b>Best</b> Fit?", "url": "https://www.wallstreetmojo.com/least-squares-regression/", "isFamilyFriendly": true, "displayUrl": "https://www.wallstreetmojo.com/<b>least-squares-regression</b>", "snippet": "<b>Line</b> of <b>Best</b> Fit in the <b>Least</b> Square <b>Regression</b>. The <b>line</b> of <b>best</b> fit is a straight <b>line</b> drawn through a scatter of data <b>points</b> that <b>best</b> represents the relationship between them. Let us consider the following graph wherein a set of data is plotted along the x and y-axis. These data <b>points</b> are represented using the blue dots. Three lines are drawn through these <b>points</b> \u2013 a green, a red, and a blue <b>line</b>. The green <b>line</b> passes through a single point, and the red <b>line</b> passes through three data ...", "dateLastCrawled": "2022-02-03T06:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Least Squares Regression</b> - <b>mathsisfun.com</b>", "url": "https://www.mathsisfun.com/data/least-squares-regression.html", "isFamilyFriendly": true, "displayUrl": "https://<b>www.mathsisfun.com</b>/data/<b>least-squares-regression</b>.html", "snippet": "<b>Least Squares Regression</b> <b>Line</b> of <b>Best</b> Fit. Imagine you have some <b>points</b>, and want to have a <b>line</b> that <b>best</b> <b>fits</b> them <b>like</b> this: We can place the <b>line</b> &quot;by eye&quot;: try to have the <b>line</b> as close as possible to all <b>points</b>, and a similar number <b>of points</b> above and below the <b>line</b>. But for better accuracy let&#39;s see how to calculate the <b>line</b> using <b>Least Squares Regression</b>. The <b>Line</b>. Our aim is to calculate the values m (slope) and b (y-intercept) in the equation of a <b>line</b>: y = mx + b. Where: y = how ...", "dateLastCrawled": "2022-02-03T04:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>What Is the Least Squares</b> <b>Regression</b> <b>Line</b>?", "url": "https://www.thoughtco.com/what-is-a-least-squares-line-3126250", "isFamilyFriendly": true, "displayUrl": "https://www.thoughtco.com/what-is-a-<b>least-squares-line</b>-3126250", "snippet": "Since the <b>least squares line</b> minimizes the squared distances between the <b>line</b> and our <b>points</b>, we can think of this <b>line</b> as the one that <b>best</b> <b>fits</b> our data. This is why the <b>least squares line</b> is also known as the <b>line</b> of <b>best</b> fit. Of all of the possible lines that could be drawn, the <b>least squares line</b> is closest to the set of data as a whole. This may mean that our <b>line</b> will miss hitting any of the <b>points</b> in our set of data.", "dateLastCrawled": "2022-02-03T05:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>12.1 Ordinary least squares regression</b> | An Introduction to Data Analysis", "url": "https://michael-franke.github.io/intro-data-analysis/ordinary-least-squares-regression.html", "isFamilyFriendly": true, "displayUrl": "https://michael-franke.github.io/intro-data-analysis/<b>ordinary-least-squares-regression</b>...", "snippet": "<b>12.1 Ordinary least squares regression</b>. This section introduces ordinary <b>least</b> <b>squares</b> (OLS) linear <b>regression</b>. The main idea is that we look for <b>the best</b>-fitting <b>line</b> in a (multi-dimensional) cloud <b>of points</b>, where \u201c<b>best</b>-fitting\u201d is defined in terms of a geometrical measure of distance (squared prediction error). 12.1.1 Prediction without any further information. We are interested in explaining or predicting the murder rates in a city using the murder data set. Concretely, we are ...", "dateLastCrawled": "2022-01-29T03:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Calculating a <b>Least</b> <b>Squares</b> <b>Regression</b> <b>Line</b>: Equation, Example ...", "url": "https://technologynetworks.com/informatics/articles/calculating-a-least-squares-regression-line-equation-example-explanation-310265", "isFamilyFriendly": true, "displayUrl": "https://technologynetworks.com/informatics/articles/calculating-a-<b>least</b>-<b>squares</b>...", "snippet": "<b>Least</b> <b>squares</b> <b>regression</b> <b>line</b> example Suppose we wanted to estimate a score for someone who had spent exactly 2.3 hours on an essay. I\u2019m sure most of us have experience in drawing lines of <b>best</b> fit , where we <b>line</b> up a ruler, think \u201cthis seems about right\u201d, and draw some lines from the X to the Y axis.", "dateLastCrawled": "2022-01-28T04:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "How to Find the <b>Line</b> of <b>Best</b> Fit in 3 Steps", "url": "https://tutorme.com/blog/post/how-to-find-line-of-best-fit/", "isFamilyFriendly": true, "displayUrl": "https://tutorme.com/blog/post/how-to-find-<b>line</b>-of-<b>best</b>-fit", "snippet": "The <b>least</b> <b>squares</b> <b>regression</b> is a simple linear <b>regression</b> analysis that is used to find the slope of the <b>line</b> that <b>best</b> <b>fits</b> or represents a set of data <b>points</b>. A linear equation represents the linear relationship between the x-values and y-values of the <b>points</b> on a graph or chart. 3 Steps to Find the Equation for the <b>Line</b> of <b>Best</b> Fit. Real-world data sets don\u2019t have perfect or exact lines. Your job is to find an equation of a <b>line</b> that can represent or approximate the data. This is ...", "dateLastCrawled": "2022-02-03T02:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "The Method of <b>Least</b> <b>Squares</b> - Williams College", "url": "https://web.williams.edu/Mathematics/sjmiller/public_html/BrownClasses/54/handouts/MethodLeastSquares.pdf", "isFamilyFriendly": true, "displayUrl": "https://web.williams.edu/.../public_html/BrownClasses/54/handouts/Method<b>LeastSquares</b>.pdf", "snippet": "The Method of <b>Least</b> <b>Squares</b> is a procedure to determine <b>the best</b> \ufb01t <b>line</b> to data; the proof uses simple calculus and linear algebra. The basic problem is to \ufb01nd <b>the best</b> \ufb01t straight <b>line</b> y = ax + b given that, for n 2 f1;:::;Ng, the pairs (xn;yn) are observed. The method easily generalizes to \ufb01nding <b>the best</b> \ufb01t of the form y = a1f1(x)+\u00a2\u00a2\u00a2+cKfK(x); (0.1) it is not necessary for the functions fk to be linearly in x \u2013 all that is needed is that y is to be a linear combination of ...", "dateLastCrawled": "2022-02-02T06:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "What is \u201c<b>Line</b> of <b>Best fit\u201d in linear regression</b>?", "url": "https://www.numpyninja.com/post/what-is-line-of-best-fit-in-linear-regression", "isFamilyFriendly": true, "displayUrl": "https://www.numpyninja.com/post/what-is-<b>line</b>-of-<b>best-fit-in-linear-regression</b>", "snippet": "Simple linear <b>regression</b> is a statistical method that allows us to summarize and study relationships between two variables: One variable is the predictor, explanatory, or independent variable and the other one is the dependent variable. Linear <b>Regression</b> is the process of <b>finding</b> a <b>line</b> that <b>best</b> <b>fits</b> the data <b>points</b> available on the plot, so that we can use it to predict output values for given inputs. So, what is \u201c<b>Best</b> fitting <b>line</b>\u201d? A <b>Line</b> of <b>best</b> fit is a straight <b>line</b> that ...", "dateLastCrawled": "2022-02-03T00:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Line of Best Fit</b> in Linear <b>Regression</b> | by Indhumathy Chelliah ...", "url": "https://towardsdatascience.com/line-of-best-fit-in-linear-regression-13658266fbc8", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>line-of-best-fit</b>-in-<b>line</b>ar-<b>regression</b>-13658266fbc8", "snippet": "<b>The Best Fit</b> <b>Line</b>. After <b>finding</b> the correlation between the variables[independent variable and target variable], and if the variables are linearly correlated, we can proceed with the Linear <b>Regression</b> model. The Linear <b>Regression</b> model will find out <b>the best fit</b> <b>line</b> for the data <b>points</b> in the scatter cloud. Let\u2019s learn how to find <b>the best fit</b> <b>line</b>. Equation of Straight <b>Line</b> y=mx+c. m \u2192slope c \u2192intercept. y=x [Slope=1, Intercept=0] -Image by Author Model Coefficient. Slope m and ...", "dateLastCrawled": "2022-02-03T00:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "9 Types of <b>Regression</b> Analysis (in ML &amp; Data Science) | FavTutor", "url": "https://favtutor.com/blogs/types-of-regression", "isFamilyFriendly": true, "displayUrl": "https://favtutor.com/blogs/types-of-<b>regression</b>", "snippet": "3) Polynomial <b>Regression</b>. In a polynomial <b>regression</b>, the power of the independent variable is more than 1. The equation below represents a polynomial equation: y = a + bx 2. In this <b>regression</b> technique, <b>the best</b> fit <b>line</b> is not a straight <b>line</b>. It is rather a curve <b>that fits</b> into the data <b>points</b>.", "dateLastCrawled": "2022-02-03T16:09:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Least Square Method</b> - Definition, Graph and Formula", "url": "https://byjus.com/maths/least-square-method/", "isFamilyFriendly": true, "displayUrl": "https://byjus.com/maths/<b>least-square-method</b>", "snippet": "The <b>least square method</b> is the process of <b>finding</b> <b>the best</b>-fitting curve or <b>line</b> of <b>best</b> fit for a set of data <b>points</b> by reducing the sum of the <b>squares</b> of the offsets (residual part) of the <b>points</b> from the curve. During the process of <b>finding</b> the relation between two variables, the trend of outcomes are estimated quantitatively. This process is termed as <b>regression</b> analysis.The method of curve fitting is an approach to <b>regression</b> analysis.", "dateLastCrawled": "2022-02-02T23:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Least Squares Regression</b> - <b>mathsisfun.com</b>", "url": "https://www.mathsisfun.com/data/least-squares-regression.html", "isFamilyFriendly": true, "displayUrl": "https://<b>www.mathsisfun.com</b>/data/<b>least-squares-regression</b>.html", "snippet": "<b>Least Squares Regression</b> <b>Line</b> of <b>Best</b> Fit. Imagine you have some <b>points</b>, and want to have a <b>line</b> that <b>best</b> <b>fits</b> them like this: We can place the <b>line</b> &quot;by eye&quot;: try to have the <b>line</b> as close as possible to all <b>points</b>, and a <b>similar</b> number <b>of points</b> above and below the <b>line</b>. But for better accuracy let&#39;s see how to calculate the <b>line</b> using <b>Least Squares Regression</b>. The <b>Line</b>. Our aim is to calculate the values m (slope) and b (y-intercept) in the equation of a <b>line</b>: y = mx + b. Where: y = how ...", "dateLastCrawled": "2022-02-03T04:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>12.1 Ordinary least squares regression</b> | An Introduction to Data Analysis", "url": "https://michael-franke.github.io/intro-data-analysis/ordinary-least-squares-regression.html", "isFamilyFriendly": true, "displayUrl": "https://michael-franke.github.io/intro-data-analysis/<b>ordinary-least-squares-regression</b>...", "snippet": "<b>12.1 Ordinary least squares regression</b>. This section introduces ordinary <b>least</b> <b>squares</b> (OLS) linear <b>regression</b>. The main idea is that we look for <b>the best</b>-fitting <b>line</b> in a (multi-dimensional) cloud <b>of points</b>, where \u201c<b>best</b>-fitting\u201d is defined in terms of a geometrical measure of distance (squared prediction error). 12.1.1 Prediction without any further information. We are interested in explaining or predicting the murder rates in a city using the murder data set. Concretely, we are ...", "dateLastCrawled": "2022-01-29T03:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "What is \u201c<b>Line</b> of <b>Best fit\u201d in linear regression</b>?", "url": "https://www.numpyninja.com/post/what-is-line-of-best-fit-in-linear-regression", "isFamilyFriendly": true, "displayUrl": "https://www.numpyninja.com/post/what-is-<b>line</b>-of-<b>best-fit-in-linear-regression</b>", "snippet": "Simple linear <b>regression</b> is a statistical method that allows us to summarize and study relationships between two variables: One variable is the predictor, explanatory, or independent variable and the other one is the dependent variable. Linear <b>Regression</b> is the process of <b>finding</b> a <b>line</b> that <b>best</b> <b>fits</b> the data <b>points</b> available on the plot, so that we can use it to predict output values for given inputs. So, what is \u201c<b>Best</b> fitting <b>line</b>\u201d? A <b>Line</b> of <b>best</b> fit is a straight <b>line</b> that ...", "dateLastCrawled": "2022-02-03T00:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Least</b>-Sq <b>Multiple Regression</b> | Real Statistics Using Excel", "url": "https://www.real-statistics.com/multiple-regression/least-squares-method-multiple-regression/", "isFamilyFriendly": true, "displayUrl": "https://www.real-statistics.com/<b>multiple-regression</b>/<b>least</b>-<b>squares</b>-method-multiple...", "snippet": "For this example, <b>finding</b> the solution is quite straightforward: b1 = 4.90 and b2 = 3.76. Thus the <b>regression</b> <b>line</b> takes the form. Using the means found in Figure 1, the <b>regression</b> <b>line</b> for Example 1 is. (Price \u2013 47.18) = 4.90 (Color \u2013 6.00) + 3.76 (Quality \u2013 4.27) or equivalently. Price = 4.90 \u2219 Color + 3.76 \u2219 Quality + 1.75.", "dateLastCrawled": "2022-02-03T01:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "9 Types of <b>Regression</b> Analysis (in ML &amp; Data Science) | FavTutor", "url": "https://favtutor.com/blogs/types-of-regression", "isFamilyFriendly": true, "displayUrl": "https://favtutor.com/blogs/types-of-<b>regression</b>", "snippet": "The type of <b>regression</b> <b>line</b>: a <b>best</b> fit straight <b>line</b>. 2) Multiple Linear <b>Regression</b>. Simple linear <b>regression</b> allows a data scientist or data analyst to make predictions about only one variable by training the model and predicting another variable. In a <b>similar</b> way, a multiple <b>regression</b> model extends to several more than one variable. Simple linear <b>regression</b> uses the following linear function to predict the value of a target variable y, with independent variable x?. y = b 0 + b 1 x 1. To ...", "dateLastCrawled": "2022-02-03T16:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Linear Regression</b>-Equation, Formula and Properties", "url": "https://byjus.com/maths/linear-regression/", "isFamilyFriendly": true, "displayUrl": "https://byjus.com/maths/<b>linear-regression</b>", "snippet": "The equation of <b>linear regression</b> <b>is similar</b> to the slope formula what we have learned before ... The most popular method to fit a <b>regression</b> <b>line</b> in the XY plot is the method of <b>least</b>-<b>squares</b>. This process determines <b>the best</b>-fitting <b>line</b> for the noted data by reducing the sum of the <b>squares</b> of the vertical deviations from each data point to the <b>line</b>. If a point rests on the fitted <b>line</b> accurately, then its perpendicular deviation is 0. Because the variations are first squared, then added ...", "dateLastCrawled": "2022-02-02T23:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Lecture 2 Linear <b>Regression</b>: A Model for the Mean", "url": "http://www.columbia.edu/~so33/SusDev/Lecture2.pdf", "isFamilyFriendly": true, "displayUrl": "<b>www.columbia.edu</b>/~so33/SusDev/Lecture2.pdf", "snippet": "<b>Least</b> <b>Squares</b> Procedure(cont.) Note that the <b>regression</b> <b>line</b> always goes through the mean X, Y. Relation Between Yield and Fertilizer 0 20 40 60 80 100 0 100 200 300 400 500 600 700 800 Fertilizer (lb/Acre) Yield (Bushel/Acre) That is, for any value of the Trend <b>line</b> independent variable there is a single most likely value for the dependent ...", "dateLastCrawled": "2022-02-03T07:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "1 Simple Linear <b>Regression</b> I \u2013 <b>Least Squares</b> Estimation", "url": "http://users.stat.ufl.edu/~winner/qmb3250/notespart2.pdf", "isFamilyFriendly": true, "displayUrl": "users.stat.ufl.edu/~winner/qmb3250/notespart2.pdf", "snippet": "1.3 <b>Least Squares</b> Estimation of \u03b20 and \u03b21 We now have the problem of using sample data to compute estimates of the parameters \u03b20 and \u03b21. First, we take a sample of n subjects, observing values y of the response variable and x of the predictor variable. We would like to choose as estimates for \u03b20 and \u03b21, the values b0 and b1 that", "dateLastCrawled": "2022-02-03T06:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Linear Regression in Excel</b> | How to do <b>Linear Regression in Excel</b>? - EDUCBA", "url": "https://www.educba.com/linear-regression-in-excel/", "isFamilyFriendly": true, "displayUrl": "https://www.educba.com/<b>linear-regression-in-excel</b>", "snippet": "A linear <b>regression</b> <b>line</b> has an equation of the kind: Y= a + bX; Where: X is the explanatory variable, Y is the dependent variable, b is the slope of the <b>line</b>, a is the y-intercept (i.e. the value of y when x=0). The <b>least</b>-<b>squares</b> method is generally used in linear <b>regression</b> that calculates <b>the best</b> fit <b>line</b> for observed data by minimizing the sum of <b>squares</b> of deviation of data <b>points</b> from the <b>line</b>. Methods for Using <b>Linear Regression in Excel</b>. This example teaches you the methods to ...", "dateLastCrawled": "2022-02-02T06:22:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>What Is the Least Squares</b> <b>Regression</b> <b>Line</b>?", "url": "https://www.thoughtco.com/what-is-a-least-squares-line-3126250", "isFamilyFriendly": true, "displayUrl": "https://www.<b>thought</b>co.com/what-is-a-<b>least-squares-line</b>-3126250", "snippet": "Since the <b>least squares line</b> minimizes the squared distances between the <b>line</b> and our <b>points</b>, we <b>can</b> think of this <b>line</b> as the one that <b>best</b> <b>fits</b> our data. This is why the <b>least squares line</b> is also known as the <b>line</b> of <b>best</b> fit. Of all of the possible lines that could be drawn, the <b>least squares line</b> is closest to the set of data as a whole. This may mean that our <b>line</b> will miss hitting any of the <b>points</b> in our set of data.", "dateLastCrawled": "2022-02-03T05:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "The <b>Least Squares Regression Line</b> - GitHub Pages", "url": "https://saylordotorg.github.io/text_introductory-statistics/s14-04-the-least-squares-regression-l.html", "isFamilyFriendly": true, "displayUrl": "https://saylordotorg.github.io/.../s14-04-the-<b>least-squares-regression</b>-l.html", "snippet": "The <b>Least Squares Regression Line</b>. Given any collection of pairs of numbers (except when all the x-values are the same) and the corresponding scatter diagram, there always exists exactly one straight <b>line</b> <b>that fits</b> the data better than any other, in the sense of minimizing the sum of the squared errors.It is called the <b>least squares regression line</b>.Moreover there are formulas for its slope and y-intercept.", "dateLastCrawled": "2022-02-02T02:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Statistics review 7: Correlation and <b>regression</b>", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC374386/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC374386", "snippet": "The <b>regression</b> <b>line</b> is obtained using the method of <b>least</b> <b>squares</b>. Any <b>line</b> y = a + bx that we draw through the <b>points</b> gives a predicted or fitted value of y for each value of x in the data set. For a particular value of x the vertical difference between the observed and fitted value of y is known as the deviation, or residual (Fig.", "dateLastCrawled": "2022-02-02T06:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "AHSS Fitting a <b>line</b> by <b>least</b> <b>squares</b> <b>regression</b>", "url": "https://spot.pcc.edu/~evega/AHSS2/fittingALineByLSR.html", "isFamilyFriendly": true, "displayUrl": "https://spot.pcc.edu/~evega/AHSS2/fittingA<b>Line</b>ByLSR.html", "snippet": "Subsection 8.2.2 An objective measure for <b>finding</b> <b>the best</b> <b>line</b>. Fitting linear models by eye is open to criticism since it is based on an individual preference. In this section, we use <b>least</b> <b>squares</b> <b>regression</b> as a more rigorous approach. This section considers family income and gift aid data from a random sample of fifty students in the freshman class of Elmhurst College in Illinois. 1 Gift aid is financial aid that does not need to be paid back, as opposed to a loan. A scatterplot of the ...", "dateLastCrawled": "2022-01-25T21:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "7 Classical Assumptions of Ordinary <b>Least</b> <b>Squares</b> (OLS) Linear <b>Regression</b>", "url": "https://statisticsbyjim.com/regression/ols-linear-regression-assumptions/", "isFamilyFriendly": true, "displayUrl": "https://statisticsbyjim.com/<b>regression</b>/ols-<b>line</b>ar-<b>regression</b>-assumptions", "snippet": "Ordinary <b>Least</b> <b>Squares</b> is the most common estimation method for linear models\u2014and that\u2019s true for a good reason.As long as your model satisfies the OLS assumptions for linear <b>regression</b>, you <b>can</b> rest easy knowing that you\u2019re getting <b>the best</b> possible estimates.. <b>Regression</b> is a powerful analysis that <b>can</b> analyze multiple variables simultaneously to answer complex research questions. However, if you don\u2019t satisfy the OLS assumptions, you might not be able to trust the results.", "dateLastCrawled": "2022-02-03T09:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Biostatistics Series Module 6: Correlation and Linear <b>Regression</b>", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5122272/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC5122272", "snippet": "In <b>regression</b>, it denotes how well the <b>regression</b> <b>line</b> approximates the real data <b>points</b>. An r 2 of 1 indicates that the <b>regression</b> <b>line</b> perfectly <b>fits</b> the data. Note that values of r 2 outside the range 0\u20131 <b>can</b> occur where it is used to measure the agreement between observed and modeled values, and the modeled values are not obtained by linear <b>regression</b>. Point Biserial and Biserial Correlation. The point biserial correlation is a special case of the product-moment correlation, in which ...", "dateLastCrawled": "2022-02-02T04:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Line</b> of <b>Best</b> Fit (<b>Least Square Method</b>) - Varsity Tutors", "url": "https://www.varsitytutors.com/hotmath/hotmath_help/topics/line-of-best-fit", "isFamilyFriendly": true, "displayUrl": "https://www.varsitytutors.com/hotmath/hotmath_help/topics/<b>line</b>-of-<b>best</b>-fit", "snippet": "A <b>line</b> of <b>best</b> fit <b>can</b> be roughly determined using an eyeball method by drawing a straight <b>line</b> on a scatter plot so that the number <b>of points</b> above the <b>line</b> and below the <b>line</b> is about equal (and the <b>line</b> passes through as many <b>points</b> as possible). A more accurate way of <b>finding</b> the <b>line</b> of <b>best</b> fit is the <b>least square method</b> .", "dateLastCrawled": "2022-02-03T06:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "12 <b>Regression</b>\u2019", "url": "https://www.colorado.edu/amath/sites/default/files/attached-files/ch12_0.pdf", "isFamilyFriendly": true, "displayUrl": "https://<b>www.colorado.edu</b>/amath/sites/default/files/attached-files/ch12_0.pdf", "snippet": "the)<b>line</b>)and)a)negative)number)if)it)liesbelow)the)<b>line</b> . The)residual)<b>can</b>)<b>be)thought</b>)of)asa)measure)of)deviation and we)<b>can</b>)summarize)the)notation)in)the)following)way: (x i, y\u02c6 i) Y i = 0 + 1x i + i \u21e1 \u02c6 0 + \u02c6 1x i +\u02c6 i = Y\u02c6 i +\u02c6 i) Y i Y\u02c6 i =\u02c6 i", "dateLastCrawled": "2022-01-30T05:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "What is <b>the best</b> <b>fit line in linear regression? - Quora</b>", "url": "https://www.quora.com/What-is-the-best-fit-line-in-linear-regression", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>the-best</b>-<b>fit-line-in-linear-regression</b>", "snippet": "Answer (1 of 3): In simple <b>regression</b> with on independent variable that coefficient is the slope of the <b>line</b> of <b>best</b> fit In <b>regression</b> with 2 Independent variables the slope is a mix of the two COEFFICIENTS The constant in <b>regression</b> eqation is is the y intercept of the <b>line</b> of <b>best</b> fit The eq...", "dateLastCrawled": "2022-01-20T10:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Regularization</b> in Machine Learning | by Prashant Gupta | Towards Data ...", "url": "https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>regularization</b>-in-machine-learning-76441ddcf99a", "snippet": "<b>Points</b> on the ellipse share the value of RSS. For a very large value of s, the green regions will contain the center of the ellipse, making coefficient estimates of both <b>regression</b> techniques, equal to the <b>least</b> <b>squares</b> estimates. But, this is not the case in the above image. In this case, the lasso and ridge <b>regression</b> coefficient estimates ...", "dateLastCrawled": "2022-02-02T22:44:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Least Square Method</b> - Definition, Graph and Formula", "url": "https://byjus.com/maths/least-square-method/", "isFamilyFriendly": true, "displayUrl": "https://byjus.com/maths/<b>least-square-method</b>", "snippet": "The <b>least square method</b> is the process of <b>finding</b> <b>the best</b>-fitting curve or <b>line</b> of <b>best</b> fit for a set of data <b>points</b> by reducing the sum of the <b>squares</b> of the offsets (residual part) of the <b>points</b> from the curve. During the process of <b>finding</b> the relation between two variables, the trend of outcomes are estimated quantitatively. This process is termed as <b>regression</b> analysis.The method of curve fitting is an approach to <b>regression</b> analysis.", "dateLastCrawled": "2022-02-02T23:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Statistics review 7: Correlation and <b>regression</b>", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC374386/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC374386", "snippet": "The value of r <b>can</b> <b>be compared</b> with those given in Table ... Method of <b>least</b> <b>squares</b>. The <b>regression</b> <b>line</b> is obtained using the method of <b>least</b> <b>squares</b>. Any <b>line</b> y = a + bx that we draw through the <b>points</b> gives a predicted or fitted value of y for each value of x in the data set. For a particular value of x the vertical difference between the observed and fitted value of y is known as the deviation, or residual (Fig. (Fig.8). 8). The method of <b>least</b> <b>squares</b> finds the values of a and b that ...", "dateLastCrawled": "2022-02-02T06:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Least Squares Regression</b> - How to Create <b>Line</b> of <b>Best</b> Fit?", "url": "https://www.wallstreetmojo.com/least-squares-regression/", "isFamilyFriendly": true, "displayUrl": "https://www.wallstreetmojo.com/<b>least-squares-regression</b>", "snippet": "<b>Line</b> of <b>Best</b> Fit in the <b>Least</b> Square <b>Regression</b>. The <b>line</b> of <b>best</b> fit is a straight <b>line</b> drawn through a scatter of data <b>points</b> that <b>best</b> represents the relationship between them. Let us consider the following graph wherein a set of data is plotted along the x and y-axis. These data <b>points</b> are represented using the blue dots. Three lines are ...", "dateLastCrawled": "2022-02-03T06:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>12.1 Ordinary least squares regression</b> | An Introduction to Data Analysis", "url": "https://michael-franke.github.io/intro-data-analysis/ordinary-least-squares-regression.html", "isFamilyFriendly": true, "displayUrl": "https://michael-franke.github.io/intro-data-analysis/<b>ordinary-least-squares-regression</b>...", "snippet": "<b>12.1 Ordinary least squares regression</b>. This section introduces ordinary <b>least</b> <b>squares</b> (OLS) linear <b>regression</b>. The main idea is that we look for <b>the best</b>-fitting <b>line</b> in a (multi-dimensional) cloud <b>of points</b>, where \u201c<b>best</b>-fitting\u201d is defined in terms of a geometrical measure of distance (squared prediction error). 12.1.1 Prediction without any further information. We are interested in explaining or predicting the murder rates in a city using the murder data set. Concretely, we are ...", "dateLastCrawled": "2022-01-29T03:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "AHSS Fitting a <b>line</b> by <b>least</b> <b>squares</b> <b>regression</b>", "url": "https://spot.pcc.edu/~evega/AHSS2/fittingALineByLSR.html", "isFamilyFriendly": true, "displayUrl": "https://spot.pcc.edu/~evega/AHSS2/fittingA<b>Line</b>ByLSR.html", "snippet": "Subsection 8.2.2 An objective measure for <b>finding</b> <b>the best</b> <b>line</b>. Fitting linear models by eye is open to criticism since it is based on an individual preference. In this section, we use <b>least</b> <b>squares</b> <b>regression</b> as a more rigorous approach. This section considers family income and gift aid data from a random sample of fifty students in the freshman class of Elmhurst College in Illinois. 1 Gift aid is financial aid that does not need to be paid back, as opposed to a loan. A scatterplot of the ...", "dateLastCrawled": "2022-01-25T21:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "7 Classical Assumptions of Ordinary <b>Least</b> <b>Squares</b> (OLS) Linear <b>Regression</b>", "url": "https://statisticsbyjim.com/regression/ols-linear-regression-assumptions/", "isFamilyFriendly": true, "displayUrl": "https://statisticsbyjim.com/<b>regression</b>/ols-<b>line</b>ar-<b>regression</b>-assumptions", "snippet": "Ordinary <b>Least</b> <b>Squares</b> is the most common estimation method for linear models\u2014and that\u2019s true for a good reason.As long as your model satisfies the OLS assumptions for linear <b>regression</b>, you <b>can</b> rest easy knowing that you\u2019re getting <b>the best</b> possible estimates.. <b>Regression</b> is a powerful analysis that <b>can</b> analyze multiple variables simultaneously to answer complex research questions. However, if you don\u2019t satisfy the OLS assumptions, you might not be able to trust the results.", "dateLastCrawled": "2022-02-03T09:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Chapter 7: <b>Correlation and Simple Linear Regression</b> \u2013 Natural Resources ...", "url": "https://milnepublishing.geneseo.edu/natural-resources-biometrics/chapter/chapter-7-correlation-and-simple-linear-regression/", "isFamilyFriendly": true, "displayUrl": "https://milnepublishing.geneseo.edu/.../chapter-7-<b>correlation-and-simple-linear-regression</b>", "snippet": "We <b>can</b> see an upward slope and a straight-<b>line</b> pattern in the plotted data <b>points</b>. A scatterplot <b>can</b> identify several different types of relationships between two variables. A relationship has no correlation when the <b>points</b> on a scatterplot do not show any pattern. A relationship is non-linear when the <b>points</b> on a scatterplot follow a pattern but not a straight <b>line</b>. A relationship is linear when the <b>points</b> on a scatterplot follow a somewhat straight <b>line</b> pattern. This is the relationship ...", "dateLastCrawled": "2022-02-02T23:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>The Regression Equation</b> \u2013 Introductory Business Statistics", "url": "https://opentextbc.ca/introbusinessstatopenstax/chapter/the-regression-equation/", "isFamilyFriendly": true, "displayUrl": "https://opentextbc.ca/introbusinessstatopenstax/chapter/<b>the-regression-equation</b>", "snippet": "<b>Regression</b> analysis is sometimes called \u201c<b>least</b> <b>squares</b>\u201d analysis because the method of determining which <b>line</b> <b>best</b> \u201c<b>fits</b>\u201d the data is to minimize the sum of the squared residuals of a <b>line</b> put through the data. Population Equation: C = \u03b2 0 + \u03b2 1 Income + \u03b5 Estimated Equation: C = b 0 + b 1 Income + e . This figure shows the assumed relationship between consumption and income from macroeconomic theory. Here the data are plotted as a scatter plot and an estimated straight <b>line</b> has ...", "dateLastCrawled": "2022-02-03T04:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Evaluate the following statement:&#39; For the <b>least</b> square method to be ...", "url": "https://www.quora.com/Evaluate-the-following-statement-For-the-least-square-method-to-be-fully-valid-it-is-required-that-the-distribution-of-Y-be-normal", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Evaluate-the-following-statement-For-the-<b>least</b>-square-method-to...", "snippet": "Answer (1 of 7): That statement is not true. There are NO assumptions about the distributions of Y or X in <b>least</b>-<b>squares</b> <b>regression</b>, only about the \u201cerror\u201d. The closest you <b>can</b> come to this is that the distribution of Y at any given X value is normal. That is, the distribution of Y|X, which is a...", "dateLastCrawled": "2022-01-28T22:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Unit 3 Test</b> Flashcards | <b>Quizlet</b>", "url": "https://quizlet.com/22819217/unit-3-test-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/22819217/<b>unit-3-test</b>-flash-cards", "snippet": "The <b>least</b> <b>squares</b> method is used to determine an estimated <b>regression</b> <b>line</b> that minimizes the squared deviations of the data values from the <b>line</b>. True or False. True. 2. The <b>least</b> <b>squares</b> method is applicable only in situations where the estimated <b>regression</b> <b>line</b> has a positive slope. True or False. False. 3. If the slope of the estimated <b>regression</b> <b>line</b> is positive, the correlation coefficient must be negative. True or False. False. 4. The slope of the estimated <b>regression</b> <b>line</b> (b1) is a ...", "dateLastCrawled": "2022-01-14T07:22:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "CS 189/289A: Introduction to <b>Machine</b> <b>Learning</b>", "url": "https://people.eecs.berkeley.edu/~jrs/189s21/", "isFamilyFriendly": true, "displayUrl": "https://people.eecs.berkeley.edu/~jrs/189s21", "snippet": "LDA vs. logistic <b>regression</b>: advantages and disadvantages. ROC curves. Weighted <b>least</b>-<b>squares</b> <b>regression</b>. <b>Least</b>-<b>squares</b> polynomial <b>regression</b>. Read ISL, Sections 4.4.3, 7.1, 9.3.3; ESL, Section 4.4.1. Optional: here is a fine short discussion of ROC curves\u2014but skip the incoherent question at the top and jump straight to the answer.", "dateLastCrawled": "2022-01-31T03:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>CS 189/289A</b>: Introduction to <b>Machine</b> <b>Learning</b>", "url": "https://people.eecs.berkeley.edu/~jrs/189/", "isFamilyFriendly": true, "displayUrl": "https://people.eecs.berkeley.edu/~jrs/189", "snippet": "<b>regression</b>: <b>least</b>-<b>squares</b> linear <b>regression</b>, logistic <b>regression</b>, polynomial <b>regression</b>, ridge <b>regression</b>, Lasso; density estimation: maximum likelihood estimation (MLE); dimensionality reduction: principal components analysis (PCA), random projection; and clustering: k-means clustering, hierarchical clustering, spectral graph clustering. Useful Links. Access the <b>CS 189/289A</b> Piazza discussion group. If you want an instructional account, you can get one online. Go to the same link if you ...", "dateLastCrawled": "2022-02-02T17:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "A difficult <b>regression</b> parameter estimation problem is posed when the data sample is hypothesized to have been generated by more than a single <b>regression</b> model. To find the best-fitting number and ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Machine Learning by Analogy</b> - SlideShare", "url": "https://www.slideshare.net/ColleenFarrelly/machine-learning-by-analogy-59094152", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ColleenFarrelly/<b>machine-learning-by-analogy</b>-59094152", "snippet": "<b>Machine Learning by Analogy</b> 1. Colleen M. Farrelly 2. Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics behind these ...", "dateLastCrawled": "2022-01-31T07:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "LSEbA: <b>least squares regression and estimation by analogy</b> in a semi ...", "url": "https://link.springer.com/article/10.1007/s10664-010-9128-6", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10664-010-9128-6", "snippet": "In this study, we indicatively applied the ordinary <b>least</b> <b>squares</b> <b>regression</b> and the estimation by <b>analogy</b> technique for the computation of the parametric and non-parametric part, respectively. However, there are lots of other well-known methods that can substitute the abovementioned methods and can be used for evaluation of these components. For example, practitioners may use a robust <b>regression</b> in the computation of the parametric portion of the proposed model in order to have a model less ...", "dateLastCrawled": "2021-12-03T23:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PPT) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> | Colleen Farrelly - Academia.edu", "url": "https://www.academia.edu/30404581/Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/30404581/<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "<b>Machine</b> <b>Learning</b> by <b>Analogy</b> Colleen M. Farrelly Overview of Problem Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics ...", "dateLastCrawled": "2022-01-02T06:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A Big Problem with Linear <b>Regression</b> and How to Solve It | Towards Data ...", "url": "https://towardsdatascience.com/robust-regression-23b633e5d6a5", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/robust-<b>regression</b>-23b633e5d6a5", "snippet": "Introduction to Robust <b>Regression</b> in <b>Machine</b> <b>Learning</b>. Hussein Abdulrahman . Just now \u00b7 7 min read. The idea behind classic linear <b>regression</b> is simple: draw a \u201cbest-fit\u201d line across the data points that minimizes the mean squared errors: Classic linear <b>regression</b> with ordinary <b>least</b> <b>squares</b>. (Image by author) Looks good. But we don\u2019t always get such clean, well behaved data in real life. Instead, we may get something like this: Same algorithm as above, but now performing poorly due ...", "dateLastCrawled": "2022-02-01T17:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Introduction to Machine Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/24/introduction-to-machine-learning-2/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/24/<b>introduction-to-machine-learning</b>-2", "snippet": "<b>Machine</b> <b>learning</b> terminology for model building and validation There seems to be an <b>analogy</b> between statistical modeling and <b>machine</b> <b>learning</b> that we will cover in subsequent chapters in depth. However, a quick view has been provided as follows: in statistical modeling, linear <b>regression</b> with two independent variables is trying to fit the best plane with\u2026", "dateLastCrawled": "2022-01-17T05:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "10.2 <b>Nonlinear Regression</b> - GitHub Pages", "url": "https://jermwatt.github.io/machine_learning_refined/notes/10_Nonlinear_intro/10_2_Regression.html", "isFamilyFriendly": true, "displayUrl": "https://jermwatt.github.io/<b>machine</b>_<b>learning</b>_refined/notes/10_Nonlinear_intro/10_2...", "snippet": "* The following is part of an early draft of the second edition of <b>Machine</b> <b>Learning</b> Refined. The published text (with ... \\right\\}_{p=1}^{P}$ we then minimized a proper <b>regression</b> cost function, e.g., the <b>Least</b> <b>Squares</b> (from Section 5.2) \\begin{equation} g\\left(\\mathbf{w}\\right) = \\frac{1}{P}\\sum_{p=1}^{P} \\left( \\mathring{\\mathbf{x}}_{p}^T \\mathbf{w} - \\overset{\\,}{{y}}_{p}^{\\,} \\right)^2 \\end{equation} in order to find optimal values for the parameters of our linear model (here, the vector ...", "dateLastCrawled": "2022-02-02T05:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Trends <b>in artificial intelligence, machine learning, and chemometrics</b> ...", "url": "https://chemistry-europe.onlinelibrary.wiley.com/doi/10.1002/ansa.202000162", "isFamilyFriendly": true, "displayUrl": "https://chemistry-europe.onlinelibrary.wiley.com/doi/10.1002/ansa.202000162", "snippet": "The derived spectra were analyzed for classification and quantification purposes using soft independent modeling of class <b>analogy</b> (SIMCA), artificial neural network (ANN), and partial <b>least</b> <b>squares</b> <b>regression</b> (PLSR). A good classification of tomatoes based on their carotenoid profile of 93% and 100% is shown using SIMCA and ANN, respectively. Besides this result, PLSR and ANN were able to achieve a good quantification of all-", "dateLastCrawled": "2022-02-01T19:23:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Bayesian <b>Learning</b> - Rebellion Research", "url": "https://www.rebellionresearch.com/bayesian-learning", "isFamilyFriendly": true, "displayUrl": "https://www.rebellionresearch.com/bayesian-<b>learning</b>", "snippet": "Linear Regression example of <b>machine learning Least Squares Regression can be thought of as</b> a very limited <b>learning</b> algorithm, where the training set consists of a number of x and y data pairs. The task would be trying to predict the y value, and the performance measure would be the sum of the squared differences between the predicted and actual y\u2019s.", "dateLastCrawled": "2022-01-19T02:15:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(least squares regression)  is like +(finding the best line that fits a group of points)", "+(least squares regression) is similar to +(finding the best line that fits a group of points)", "+(least squares regression) can be thought of as +(finding the best line that fits a group of points)", "+(least squares regression) can be compared to +(finding the best line that fits a group of points)", "machine learning +(least squares regression AND analogy)", "machine learning +(\"least squares regression is like\")", "machine learning +(\"least squares regression is similar\")", "machine learning +(\"just as least squares regression\")", "machine learning +(\"least squares regression can be thought of as\")", "machine learning +(\"least squares regression can be compared to\")"]}