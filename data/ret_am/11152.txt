{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Word</b> Embeddings \u2013 Towards Data Science", "url": "https://towardsdatascience.com/tagged/word-embeddings", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/tagged/<b>word</b>-<b>embedding</b>", "snippet": "Embeddings is a handy concept in Machine Learning (ML), and most of the time, terms <b>like</b> vectors and <b>word</b> representation appear in that context frequently. This article describes what a vector size means to an ML model and what <b>embedding</b> has to do with the model input. Embeddings is simply\u2026. <b>Read</b> more \u00b7 8 min <b>read</b>.", "dateLastCrawled": "2022-01-15T12:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Word</b> Embeddings - GitHub Pages", "url": "https://lena-voita.github.io/nlp_course/word_embeddings.html", "isFamilyFriendly": true, "displayUrl": "https://lena-voita.github.io/nlp_course/<b>word</b>_<b>embeddings</b>.html", "snippet": "How: Learn <b>word</b> vectors by <b>teaching</b> them to predict contexts. Word2Vec is a model whose parameters are <b>word</b> vectors. These parameters are optimized iteratively for a certain objective. The objective forces <b>word</b> vectors to &quot;know&quot; contexts a <b>word</b> can appear in: the vectors are trained to predict possible contexts of the corresponding words. As you remember from the distributional hypothesis, if vectors &quot;know&quot; about contexts, they &quot;know&quot; <b>word</b> meaning. Word2Vec is an iterative method. Its main ...", "dateLastCrawled": "2022-01-19T16:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Overview of <b>Word</b> <b>Embedding</b> using Embeddings from Language Models (ELMo ...", "url": "https://www.geeksforgeeks.org/overview-of-word-embedding-using-embeddings-from-language-models-elmo/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/overview-of-<b>word</b>-<b>embedding</b>-using-<b>embeddings</b>-from...", "snippet": "Embeddings from Language Models (ELMo) : ELMo is an NLP framework developed by AllenNLP. ELMo <b>word</b> vectors are calculated using a two-layer bidirectional language model (biLM). Each layer comprises forward and backward pass. Unlike Glove and Word2Vec, ELMo represents embeddings for a <b>word</b> using the complete sentence containing that <b>word</b>.", "dateLastCrawled": "2022-02-02T13:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "He&#39;s Brilliant, She&#39;s Lovely: <b>Teaching</b> Computers To Be Less Sexist ...", "url": "https://www.npr.org/sections/alltechconsidered/2016/08/12/489507182/hes-brilliant-shes-lovely-teaching-computers-to-be-less-sexist", "isFamilyFriendly": true, "displayUrl": "https://<b>www.npr.org</b>/.../hes-brilliant-shes-lovely-<b>teaching</b>-<b>computers</b>-to-be-less-sexist", "snippet": "A traditional search would just turn up any article that had that exact phrase, &quot;<b>computer</b> programmer.&quot; But a <b>word</b> <b>embedding</b> knows that the phrase &quot;<b>computer</b> programmer&quot; is related to terms <b>like</b> ...", "dateLastCrawled": "2022-01-28T11:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A Beginner&#39;s Guide to <b>Word2Vec</b> and Neural <b>Word</b> Embeddings | Pathmind", "url": "https://wiki.pathmind.com/word2vec", "isFamilyFriendly": true, "displayUrl": "https://wiki.pathmind.com/<b>word2vec</b>", "snippet": "As Elvis Costello said: \u201cWriting about music <b>is like</b> dancing about architecture.\u201d <b>Word2vec</b> \u201cvectorizes\u201d about words, and by doing so it makes natural language <b>computer</b>-readable \u2013 we can start to perform powerful mathematical operations on words to detect their similarities. So a neural <b>word</b> <b>embedding</b> represents a <b>word</b> with numbers. It ...", "dateLastCrawled": "2022-02-03T03:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Analysis of Sentiment on <b>Movie Reviews Using Word Embedding</b> Self ...", "url": "https://www.igi-global.com/article/analysis-of-sentiment-on-movie-reviews-using-word-embedding-self-attentive-lstm/275757", "isFamilyFriendly": true, "displayUrl": "https://www.igi-global.com/article/analysis-of-sentiment-on-movie-reviews-using-<b>word</b>...", "snippet": "<b>Word</b> <b>embedding</b> improves the text classification by solving the problem of sparse matrix and semantics of the <b>word</b>. In this paper, a novel architecture is proposed by combining long short-term memory (LSTM) with <b>word</b> <b>embedding</b> to extract the semantic relationship between the neighboring words and also a weighted self-attention is applied to extract the key terms from the reviews. Based on the experimental analysis on the IMDB dataset, the authors have shown that the proposed architecture <b>word</b> ...", "dateLastCrawled": "2021-12-17T14:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Natural language <b>word</b> embeddings as a glimpse into healthcare language ...", "url": "https://informatics.bmj.com/content/28/1/e100464", "isFamilyFriendly": true, "displayUrl": "https://informatics.bmj.com/content/28/1/e100464", "snippet": "Methods Using unsupervised natural language processing, <b>word</b> <b>embedding</b> in latent space was used to generate phrase clusters with most similar semantic embeddings to \u2018Ceiling of Treatment\u2019 and their prognostication value. Results <b>Word</b> embeddings with most similarity to \u2018Ceiling of Treatment\u2019 clustered around phrases describing end-of-life care, ceiling of care and LST discussions. The phrases have differing prognostic profile with the highest 7-day mortality in the phrases most ...", "dateLastCrawled": "2022-01-31T22:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Image Captioning with Keras \u2014 \u201c<b>Teaching</b> ... - Towards Data Science", "url": "https://towardsdatascience.com/image-captioning-with-keras-teaching-computers-to-describe-pictures-c88a46a311b8", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/image-<b>caption</b>ing-with-keras-<b>teaching</b>-<b>computers</b>-to...", "snippet": "Every <b>word</b> (or index) will be mapped (embedded) to higher dimensional space through one of the <b>word</b> <b>embedding</b> techniques. Later, during the model building stage, we will see that each <b>word</b>/index is mapped to a 200-long vector using a pre-trained GLOVE <b>word</b> <b>embedding</b> model.", "dateLastCrawled": "2022-02-02T08:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>The Continuous Bag Of Words (CBOW</b>) Model in NLP - Hands-On", "url": "https://analyticsindiamag.com/the-continuous-bag-of-words-cbow-model-in-nlp-hands-on-implementation-with-codes/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/<b>the-continuous-bag-of-words-cbow</b>-model-in-nlp-hands-on...", "snippet": "The user will have to set the window size. If the window for the context <b>word</b> is 2 then the <b>word</b> pairs would look <b>like</b> this: ([it, a], is), ([is, pleasant], a),([a, day], pleasant). With these <b>word</b> pairs, the model tries to predict the target <b>word</b> considered the context words. If we have 4 context words used for predicting one target <b>word</b> the input layer will be in the form of four 1XW input vectors. These input vectors will be passed to the hidden layer where it is multiplied by a WXN ...", "dateLastCrawled": "2022-02-03T11:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Digital Humanities Pedagogy - 10. <b>Teaching</b> <b>Computer</b>-Assisted Text ...", "url": "https://books.openedition.org/obp/1644", "isFamilyFriendly": true, "displayUrl": "https://books.openedition.org/obp/1644", "snippet": "10 John B. Smith&#39;s article, &quot;Image and Imagery in Joyce&#39;s Portrait: <b>A Computer</b>-Assisted Analysis,&quot; in Directions in Literary Criticism: Contemporary Approaches to Literature, ed. Stanley Weintraub and Philip Young (University Park: Pennsylvania State University Press, 1973), 220\u201327, is a nice early example of this type of analysis if you want an exemplar for students <b>to read</b>. Smith was a pioneer in the development of text analysis tools, with one of the first interactive text analysis ...", "dateLastCrawled": "2022-01-08T21:18:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Word</b> Embeddings \u2013 Towards Data Science", "url": "https://towardsdatascience.com/tagged/word-embeddings", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/tagged/<b>word</b>-<b>embedding</b>", "snippet": "Embeddings is a handy concept in Machine Learning (ML), and most of the time, terms like vectors and <b>word</b> representation appear in that context frequently. This article describes what a vector size means to an ML model and what <b>embedding</b> has to do with the model input. Embeddings is simply\u2026. <b>Read</b> more \u00b7 8 min <b>read</b>.", "dateLastCrawled": "2022-01-15T12:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Word</b> Embeddings - GitHub Pages", "url": "https://lena-voita.github.io/nlp_course/word_embeddings.html", "isFamilyFriendly": true, "displayUrl": "https://lena-voita.github.io/nlp_course/<b>word</b>_<b>embeddings</b>.html", "snippet": "How: Learn <b>word</b> vectors by <b>teaching</b> them to predict contexts. Word2Vec is a model whose parameters are <b>word</b> vectors. These parameters are optimized iteratively for a certain objective. The objective forces <b>word</b> vectors to &quot;know&quot; contexts a <b>word</b> can appear in: the vectors are trained to predict possible contexts of the corresponding words. As you remember from the distributional hypothesis, if vectors &quot;know&quot; about contexts, they &quot;know&quot; <b>word</b> meaning. Word2Vec is an iterative method. Its main ...", "dateLastCrawled": "2022-01-19T16:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Overview of <b>Word</b> <b>Embedding</b> using Embeddings from Language Models (ELMo ...", "url": "https://www.geeksforgeeks.org/overview-of-word-embedding-using-embeddings-from-language-models-elmo/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/overview-of-<b>word</b>-<b>embedding</b>-using-<b>embeddings</b>-from...", "snippet": "Embeddings from Language Models (ELMo) : ELMo is an NLP framework developed by AllenNLP. ELMo <b>word</b> vectors are calculated using a two-layer bidirectional language model (biLM). Each layer comprises forward and backward pass. Unlike Glove and Word2Vec, ELMo represents embeddings for a <b>word</b> using the complete sentence containing that <b>word</b>.", "dateLastCrawled": "2022-02-02T13:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "He&#39;s Brilliant, She&#39;s Lovely: <b>Teaching</b> Computers To Be Less Sexist ...", "url": "https://www.npr.org/sections/alltechconsidered/2016/08/12/489507182/hes-brilliant-shes-lovely-teaching-computers-to-be-less-sexist", "isFamilyFriendly": true, "displayUrl": "https://<b>www.npr.org</b>/.../hes-brilliant-shes-lovely-<b>teaching</b>-<b>computers</b>-to-be-less-sexist", "snippet": "<b>Word</b> embeddings do <b>similar</b> work in <b>computer</b> programs that we interact with every day \u2014 programs that target ads at us, decide what we see on social media, or work to improve Internet search results.", "dateLastCrawled": "2022-01-28T11:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A Beginner&#39;s Guide to <b>Word2Vec</b> and Neural <b>Word</b> Embeddings | Pathmind", "url": "https://wiki.pathmind.com/word2vec", "isFamilyFriendly": true, "displayUrl": "https://wiki.pathmind.com/<b>word2vec</b>", "snippet": "So a neural <b>word</b> <b>embedding</b> represents a <b>word</b> with numbers. It\u2019s a simple, yet unlikely, translation. It\u2019s a simple, yet unlikely, translation. <b>Word2vec</b> <b>is similar</b> to an autoencoder, encoding each <b>word</b> in a vector, but rather than training against the input words through reconstruction, as a restricted Boltzmann machine does, <b>word2vec</b> trains words against other words that neighbor them in the input corpus.", "dateLastCrawled": "2022-02-03T03:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>What is Word2Vec</b>? | Engineering Education (EngEd) Program | Section", "url": "https://www.section.io/engineering-education/what-is-word2vec/", "isFamilyFriendly": true, "displayUrl": "https://www.section.io/engineering-education/<b>what-is-word2vec</b>", "snippet": "It is also important to note that Word2vec isn\u2019t the only <b>word</b> <b>embedding</b> model that works well. Stanford\u2019s GloVe and Facebook\u2019s FastText are examples of other popular <b>word</b> <b>embedding</b> models you can explore that also work well. Feel free to also have a <b>read</b> on them. Happy coding. References and further reading", "dateLastCrawled": "2022-01-28T04:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A <b>Deep Learning Approach in Predicting the</b> Next <b>Word</b>(s) | by Kamil ...", "url": "https://towardsdatascience.com/a-deep-learning-approach-in-predicting-the-next-word-s-7b0ee9341bfe", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-<b>deep-learning-approach-in-predicting-the</b>-next-<b>word</b>-s...", "snippet": "<b>Word</b> embeddings enable us to represent words in a n_dimensional space where words such as \u201cgood\u201d and \u201cgreat\u201d have <b>similar</b> representations in this n_dimensional space which the <b>computer</b> can understand. In the image below we can see <b>word</b> embeddings (7-dimensional) for words such as dog, puppy, cat, houses, man, woman, king, and queen. The dimensions are unknown to us (not interpretable) as the model learns these dimensions as it iterates over the data. That said, to aid the reader in ...", "dateLastCrawled": "2022-01-31T10:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Recommendation engine using Text data</b> ,Cosine Similarity and <b>word</b> ...", "url": "https://ambarishg.github.io/posts/recommender-career-spacy/", "isFamilyFriendly": true, "displayUrl": "https://ambarishg.github.io/posts/recommender-career-spacy", "snippet": "<b>Word</b> embeddings give us a way to use an efficient, dense representation in which <b>similar</b> words have a <b>similar</b> encoding. Importantly, you do not have to specify this encoding by hand. An <b>embedding</b> is a dense vector of floating point values (the length of the vector is a parameter you specify). Instead of specifying the values for the <b>embedding</b> manually, they are trainable parameters (weights learned by the model during training, in the same way a model learns weights for a dense layer). It is ...", "dateLastCrawled": "2021-11-28T01:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Analyzing text semantic <b>similarity</b> using TensorFlow Hub and Dataflow ...", "url": "https://cloud.google.com/architecture/analyzing-text-semantic-similarity-using-tensorflow-and-cloud-dataflow", "isFamilyFriendly": true, "displayUrl": "https://cloud.google.com/architecture/analyzing-text-semantic-<b>similarity</b>-using-tensor...", "snippet": "In machine learning (ML), a text <b>embedding</b> is a real-valued feature vector that represents the semantics of a <b>word</b> (for example, by using Word2vec) or a sentence (for example, by using Universal Sentence Encoder). Embeddings can be either pre-trained in generic contexts or trained for specific tasks. Text embeddings are used to represent textual input features to ML models, such as classification, regression, and clustering.", "dateLastCrawled": "2022-01-30T00:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Automated Short-Answer Grading using Semantic Similarity based on <b>Word</b> ...", "url": "https://ijtech.eng.ui.ac.id/article/view/4651", "isFamilyFriendly": true, "displayUrl": "https://ijtech.eng.ui.ac.id/article/view/4651", "snippet": "The <b>word</b>-<b>embedding</b> model has shown successful results in representing words semantically in a vector space initially proposed by Mikolov and various colleagues (Mikolov et al., 2013a; Mikolov, et al., 2013b; see also, Bengio et al., 2003; Levy and Goldberg 2014). <b>Word</b> representation in a vector space reflects the semantics of the words. This paper proposes a semantic similarity calculation method based on this type of <b>word</b>-<b>embedding</b> for grading short-answer responses.", "dateLastCrawled": "2022-02-02T17:19:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Sequence Models by Andrew Ng \u2014 11 Lessons Learned | by Ryan Shrott ...", "url": "https://towardsdatascience.com/sequence-models-by-andrew-ng-11-lessons-learned-c62fb1d3485b", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/sequence-models-by-andrew-ng-11-lessons-learned-c62fb1d...", "snippet": "<b>Word</b> embeddings <b>can</b> <b>be thought</b> of as the vector representation of a given <b>word</b>. They <b>can</b> be trained using the Word2Vec, Negative Sampling or Glove algorithms. <b>Word</b> <b>embedding</b> models may be trained on a very large text corpus (say 100B words) and <b>can</b> then be used on a sequence prediction task with a smaller number of training example (say 10,000 words). For example, sentiment classification may use <b>word</b> embeddings to greatly reduce the number of training examples required to generate an ...", "dateLastCrawled": "2022-01-29T09:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Word Embedding Models Are the New</b> Topic Models \u2013 Women Writers Project", "url": "https://wwp.northeastern.edu/blog/supervised-models/", "isFamilyFriendly": true, "displayUrl": "https://wwp.northeastern.edu/blog/supervised-models", "snippet": "<b>Word</b> <b>embedding</b> models, on the other hand, look across an entire corpus for <b>word</b> collocations within a narrowly defined context, sometimes referred to as a \u201cwindow.\u201d Or, as Schmidt defines the difference, \u201cA topic model aims to reduce words down [to] some core meaning so you <b>can</b> see what each individual document in a library is really about. Effectively, this is about getting rid of words so we <b>can</b> understand documents more clearly. WEMs do nearly the opposite: they try to ignore ...", "dateLastCrawled": "2021-11-30T21:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A Beginner&#39;s Guide to <b>Word2Vec</b> and Neural <b>Word</b> Embeddings | Pathmind", "url": "https://wiki.pathmind.com/word2vec", "isFamilyFriendly": true, "displayUrl": "https://wiki.pathmind.com/<b>word2vec</b>", "snippet": "So a neural <b>word</b> <b>embedding</b> represents a <b>word</b> with numbers. It\u2019s a simple, yet unlikely, translation. It\u2019s a simple, yet unlikely, translation. <b>Word2vec</b> is similar to an autoencoder, encoding each <b>word</b> in a vector, but rather than training against the input words through reconstruction, as a restricted Boltzmann machine does, <b>word2vec</b> trains words against other words that neighbor them in the input corpus.", "dateLastCrawled": "2022-02-03T03:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 3, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Deep learning</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Deep_learning", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Deep_learning</b>", "snippet": "<b>Word</b> <b>embedding</b>, such as word2vec, <b>can</b> <b>be thought</b> of as a representational layer in a <b>deep learning</b> architecture that transforms an atomic <b>word</b> into a positional representation of the <b>word</b> relative to other words in the dataset; the position is represented as a point in a vector space. Using <b>word</b> <b>embedding</b> as an RNN input layer allows the network to parse sentences and phrases using an effective compositional vector grammar. A compositional vector grammar <b>can</b> <b>be thought</b> of as", "dateLastCrawled": "2022-02-02T23:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Vector Space Models for the Digital Humanities", "url": "http://bookworm.benschmidt.org/posts/2015-10-25-Word-Embeddings.html", "isFamilyFriendly": true, "displayUrl": "bookworm.benschmidt.org/posts/2015-10-25-<b>Word</b>-<b>Embeddings</b>.html", "snippet": "DHers use topic models because it seems at least possible that each individual topic <b>can</b> offer a useful operationalization of some basic and real element of humanities vocabulary: topics (Blei), themes (Jockers), or discourses (Underwood/Rhody). 1 The <b>word</b> <b>embedding</b> models offer something slightly more abstract, but equally compelling: a spatial analogy to relationships between words.", "dateLastCrawled": "2021-12-16T02:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Brown CS: Brown CS News - Brown University Department of <b>Computer</b> Science", "url": "https://cs.brown.edu/news/2021/03/23/close-look-embedding-socially-responsible-computing-content-cs-19/", "isFamilyFriendly": true, "displayUrl": "https://cs.brown.edu/news/2021/03/23/close-look-<b>embedding</b>-socially-responsible...", "snippet": "For decades, Brown CS has been examining the <b>computer</b> scientist\u2019s impact in a world of constant technological acceleration. ... (If you don\u2019t have time <b>to read</b> the entire article below, you might want to look at this reflection document, which looks back on the course\u2019s Socially Responsible Computing efforts and makes several recommendations for what to do and not to do when designing such assignments.) \u201cI knew it was easy to create cut-and-paste, <b>read</b>-and-respond assignments,\u201d say", "dateLastCrawled": "2022-01-29T18:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "How to Teach |ICT Skills for Primary Students|", "url": "https://www.ictesolutions.com.au/blog/key-ict-skills-in-the-primary-classroom/", "isFamilyFriendly": true, "displayUrl": "https://www.ictesolutions.com.au/blog/key-ict-<b>skills-in-the-primary-classroom</b>", "snippet": "Inserting a picture \u2013 <b>embedding</b> it tightly with text, understanding how to resize and re-position it; Inserting graphics, tables, smart art and symbols; Understanding how to set up a header, footer, page numbers; Inserting hyperlink, setting how the linked pages opens; Using templates. As a teacher, what then do you need to know for when <b>teaching</b> ICT in primary school? You will need to apply your knowledge, skills and understanding of <b>word</b> processing in two distinct ways. The first way is ...", "dateLastCrawled": "2022-02-03T03:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "How To Teach A Child <b>to Read</b> In 10 Easy Steps - I <b>Can</b> Teach My Child!", "url": "https://www.icanteachmychild.com/10-steps-to-teaching-your-child-to-read/", "isFamilyFriendly": true, "displayUrl": "https://i<b>can</b>teachmychild.com/10-steps-to-<b>teaching</b>-your-child-to-", "snippet": "<b>Teaching</b> your child <b>to read</b> <b>can</b> be one of the most rewarding experiences of your life! Here we share 10 tips on how to teach a child <b>to read</b>, from pre-readers all the way to school-age children! When Do Kids Learn <b>To Read</b>? As a former first grade teacher, <b>teaching</b> children <b>to read</b> is one of my greatest passions! But because most children don\u2019t start actually \u201creading\u201d until around 6 years old (which is upwards of the targeted age range for my blog), I didn\u2019t want parents to feel ...", "dateLastCrawled": "2022-02-03T16:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 8, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Computer</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Computer", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Computer</b>", "snippet": "<b>A computer</b> is a digital electronic machine that <b>can</b> be programmed to carry out sequences of arithmetic or logical operations (computation) automatically.Modern computers <b>can</b> perform generic sets of operations known as programs.These programs enable computers to perform a wide range of tasks. <b>A computer</b> system is a &quot;complete&quot; <b>computer</b> that includes the hardware, operating system (main software), and peripheral equipment needed and used for &quot;full&quot; operation. This term may also refer to a group ...", "dateLastCrawled": "2022-02-07T10:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(PDF) <b>Education and Learning to Think</b> - ResearchGate", "url": "https://www.researchgate.net/publication/239062773_Education_and_Learning_to_Think", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/239062773_<b>Education_and_Learning_to_Think</b>", "snippet": "<b>can</b> improve specific skills through some form of direct <b>teaching</b>, then people&#39;s ability to perform various kinds of learning, thinking, and problem-solving tasks in which such skills have been ...", "dateLastCrawled": "2022-01-31T06:48:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Word</b> Embeddings \u2013 Towards Data Science", "url": "https://towardsdatascience.com/tagged/word-embeddings", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/tagged/<b>word</b>-<b>embedding</b>", "snippet": "Embeddings is a handy concept in Machine Learning (ML), and most of the time, terms like vectors and <b>word</b> representation appear in that context frequently. This article describes what a vector size means to an ML model and what <b>embedding</b> has to do with the model input. Embeddings is simply\u2026. <b>Read</b> more \u00b7 8 min <b>read</b>.", "dateLastCrawled": "2022-01-15T12:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Word</b> Embeddings - GitHub Pages", "url": "https://lena-voita.github.io/nlp_course/word_embeddings.html", "isFamilyFriendly": true, "displayUrl": "https://lena-voita.github.io/nlp_course/<b>word</b>_<b>embeddings</b>.html", "snippet": "For each vocabulary <b>word</b>, a look-up table contains its <b>embedding</b>. This <b>embedding</b> <b>can</b> be found using the <b>word</b> index in the vocabulary (i.e., you to look up the <b>embedding</b> in the table using <b>word</b> index). To account for unknown words (the ones which are not in the vocabulary), usually a vocabulary contains a special token UNK.", "dateLastCrawled": "2022-01-19T16:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A <b>Deep Learning Approach in Predicting the</b> Next <b>Word</b>(s) | by Kamil ...", "url": "https://towardsdatascience.com/a-deep-learning-approach-in-predicting-the-next-word-s-7b0ee9341bfe", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-<b>deep-learning-approach-in-predicting-the</b>-next-<b>word</b>-s...", "snippet": "<b>Teaching</b> <b>a computer</b> to predict the next set of words in a sentence. Kamil Mysiak . Jun 4, 2021 \u00b7 14 min <b>read</b>. Photo by Szabo Viktor on Unsplash. In this tutorial, we will walk through the process of building a deep learning model used to predict the next <b>word</b>(s) following a seed phrase. For example, we\u2019ll ask the <b>computer</b> to predict the next 10 words after we have typed \u201cThe candidates are\u201d. Although cutting-edge models used in your smartphones to assist with sending text messages are ...", "dateLastCrawled": "2022-01-31T10:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Automated Short-Answer Grading using Semantic Similarity based on <b>Word</b> ...", "url": "https://ijtech.eng.ui.ac.id/article/view/4651", "isFamilyFriendly": true, "displayUrl": "https://ijtech.eng.ui.ac.id/article/view/4651", "snippet": "The <b>word</b>-<b>embedding</b> model has shown successful results in representing words semantically in a vector space initially proposed by Mikolov and various colleagues (Mikolov et al., 2013a; Mikolov, et al., 2013b; see also, Bengio et al., 2003; Levy and Goldberg 2014). <b>Word</b> representation in a vector space reflects the semantics of the words. This paper proposes a semantic similarity calculation method based on this type of <b>word</b>-<b>embedding</b> for grading short-answer responses.", "dateLastCrawled": "2022-02-02T17:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A Beginner&#39;s Guide to <b>Word2Vec</b> and Neural <b>Word</b> Embeddings | Pathmind", "url": "https://wiki.pathmind.com/word2vec", "isFamilyFriendly": true, "displayUrl": "https://wiki.pathmind.com/<b>word2vec</b>", "snippet": "So a neural <b>word</b> <b>embedding</b> represents a <b>word</b> with numbers. It\u2019s a simple, yet unlikely, translation. It\u2019s a simple, yet unlikely, translation. <b>Word2vec</b> is similar to an autoencoder, encoding each <b>word</b> in a vector, but rather than training against the input words through reconstruction, as a restricted Boltzmann machine does, <b>word2vec</b> trains words against other words that neighbor them in the input corpus.", "dateLastCrawled": "2022-02-03T03:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A Method Based on a New <b>Word Embedding Approach for Process Model</b> ...", "url": "https://www.igi-global.com/article/a-method-based-on-a-new-word-embedding-approach-for-process-model-matching/266492", "isFamilyFriendly": true, "displayUrl": "https://www.igi-global.com/article/a-method-based-on-a-new-<b>word</b>-<b>embedding</b>-approach-for...", "snippet": "This paper proposes a method based on a new <b>word</b> <b>embedding</b> approach for matching business process model. The proposed method aligns two process models in four steps. First activity labels are extracted and pre-processed to remove meaningless words, then each <b>word</b> composing an activity label and using a semantic similarity metric based on WordNet is represented with an n-dimensional vector in the space of the vocabulary of the two labels to <b>be compared</b>. Based on these representations, a ...", "dateLastCrawled": "2021-12-23T23:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A novel density-based <b>clustering method using word embedding</b> features ...", "url": "https://link.springer.com/article/10.1007%2Fs10586-016-0649-7", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10586-016-0649-7", "snippet": "Although <b>word</b> <b>embedding</b> <b>can</b> represent the semantics and roles of each <b>word</b> using solely the raw corpus, the direct use of <b>word</b> <b>embedding</b> results as lexical features is difficult. Establishing a similarity model is a complex task, because the <b>word</b> distribution is not uniform. For example, the similarity between \u201cnow\u201d and \u201chere\u201d in the <b>embedding</b> space is 0.7, whereas that for the words \u201cKorean alphabet\u201d and \u201cletter\u201d is 0.57. However, the latter two words are supposedly more ...", "dateLastCrawled": "2021-12-25T16:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) <b>Vocabulary and Language Teaching</b>", "url": "https://www.researchgate.net/publication/49615330_Vocabulary_and_Language_Teaching", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/49615330_Vocabulary_and", "snippet": "The computerized corpus analysis <b>can</b> effectively be used in <b>teaching</b> vocabulary (<b>Read</b>, 2004) since <b>word</b> frequency (Schmitt, 2000), <b>word</b> meanings in context, and the collocational patterns of words ...", "dateLastCrawled": "2022-01-10T20:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "The <b>Importance of Teaching Handwriting</b> | <b>Reading Rockets</b>", "url": "https://www.readingrockets.org/article/importance-teaching-handwriting", "isFamilyFriendly": true, "displayUrl": "https://<b>www.readingrockets.org</b>/article/importance-<b>teaching</b>-handwriting", "snippet": "In <b>teaching</b> cursive, explicitly teach connections between letters as well as formation of single letters. Unlike manuscript writing, cursive writing involves making connections between letters within a <b>word</b>. Once children <b>can</b> form individual letters, explicit <b>teaching</b> of letter connections is important. Connections involving four letters \u2014 cursive b, o, v, and w \u2014 followed by a subsequent letter (e.g., as in the words bed, on, have, will) are often especially confusing for children ...", "dateLastCrawled": "2022-02-02T07:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Technology and Teaching Children to Read</b> | LD Topics | <b>LD OnLine</b>", "url": "http://www.ldonline.org/article/12684/", "isFamilyFriendly": true, "displayUrl": "<b>www.ldonline.org</b>/article/12684", "snippet": "Give students books in which they <b>can</b> <b>read</b> 95% of the words; Fluency develops as a result of many opportunities to practice reading with a high degree of success. Technology. Electronic books, or e-books, present traditional picture book text and images in an alternative on-screen format. The simplest electronic books simply transfer the story from paper to the screen, and allow the child to listen as the program reads the story aloud. Some e-books may also highlight each <b>word</b> as the child ...", "dateLastCrawled": "2022-02-02T03:14:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Word</b> Embeddings, Analogies, and <b>Machine</b> <b>Learning</b>: Beyond king - man ...", "url": "https://aclanthology.org/C16-1332.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/C16-1332.pdf", "snippet": "<b>Word</b> Embeddings, Analogies, and <b>Machine</b> <b>Learning</b>: Beyond King - Man + Woman = Queen Aleksandr Drozd y, Anna Gladkova z, Satoshi Matsuoka y yTokyo Institute of Technology, Meguro-ku, Tokyo 152-8550, Japan alex@smg.is.titech.ac.jp, matsu@is.titech.ac.jp z The University of Tokyo, Meguro-ku, Tokyo 153-8902 Japan gladkova@phiz.c.u-tokyo.ac.jp Abstract Solving <b>word</b> analogies became one of the most popular benchmarks for <b>word</b> embeddings on the assumption that linear relations between <b>word</b> pairs ...", "dateLastCrawled": "2022-01-20T00:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Word</b> Embeddings, Analogies, and <b>Machine</b> <b>Learning</b>: Beyond king - man ...", "url": "https://aclanthology.org/C16-1332/", "isFamilyFriendly": true, "displayUrl": "https://acl<b>anthology</b>.org/C16-1332", "snippet": "\ufeff%0 Conference Proceedings %T <b>Word</b> Embeddings, Analogies, and <b>Machine</b> <b>Learning</b>: Beyond king - man + woman = queen %A Drozd, Aleksandr %A Gladkova, Anna %A Matsuoka, Satoshi %S Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers %D 2016 %8 dec %I The COLING 2016 Organizing Committee %C Osaka, Japan %F drozd-etal-2016-<b>word</b> %X Solving <b>word</b> analogies became one of the most popular benchmarks for <b>word</b> embeddings on the assumption that ...", "dateLastCrawled": "2022-01-17T09:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>Word</b> Embeddings, Analogies, and <b>Machine</b> <b>Learning</b>: Beyond King ...", "url": "https://www.researchgate.net/publication/311843169_Word_Embeddings_Analogies_and_Machine_Learning_Beyond_King_-_Man_Woman_Queen", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/311843169_<b>Word</b>_<b>Embeddings</b>_Analogies_and...", "snippet": "<b>Word</b> Embeddings, Analogies, and <b>Machine</b> <b>Learning</b>: Beyond King - Man+ Woman= Queen December 2016 Conference: Proceedings of COLING 2016, the 26th International Conference on Computational ...", "dateLastCrawled": "2021-11-25T14:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Finding the <b>Word</b> <b>Analogy</b> from given words using Word2Vec embeddings ...", "url": "https://www.geeksforgeeks.org/finding-the-word-analogy-from-given-words-using-word2vec-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/finding-the-<b>word</b>-<b>analogy</b>-from-given-<b>words</b>-using-<b>word</b>2vec...", "snippet": "What if we can use a <b>Machine</b> <b>Learning</b> algorithm to automate this task of finding the <b>word</b> <b>analogy</b>. In ... Overview of <b>Word</b> <b>Embedding</b> using Embeddings from Language Models (ELMo) 16, Mar 21. <b>Word</b> Embeddings in NLP. 11, Oct 20. Implement your own word2vec(skip-gram) model in Python. 18, Jan 19. Scraping And Finding Ordered Words In A Dictionary using Python. 23, Jul 17 . Python - Replace all words except the given <b>word</b>. 25, Sep 20. Python | Finding &#39;n&#39; Character Words in a Text File. 15, Oct ...", "dateLastCrawled": "2022-01-26T14:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Word Embeddings in NLP - GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/word-embeddings-in-nlp/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/<b>word</b>-<b>embeddings</b>-in-nlp", "snippet": "<b>Word</b> Embeddings are a method of extracting features out of text so that we can input those features into a <b>machine</b> <b>learning</b> model to work with text data. They try to preserve syntactical and semantic information. The methods such as Bag of Words(BOW), CountVectorizer and TFIDF rely on the <b>word</b> count in a sentence but do not save any syntactical or semantic information. In these algorithms, the size of the vector is the number of elements in the vocabulary. We can get a sparse matrix if most ...", "dateLastCrawled": "2022-01-30T08:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "GitHub - jungsoh/<b>word</b>-embeddings-<b>word</b>-<b>analogy</b>-by-document-similarity ...", "url": "https://github.com/jungsoh/word-embeddings-word-analogy-by-document-similarity", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/jungsoh/<b>word</b>-<b>embeddings</b>-<b>word</b>-<b>analogy</b>-by-document-similarity", "snippet": "An example of a <b>word</b> <b>analogy</b> problem is to fill in the blank: Man is to Woman as King is to _____`. Because <b>word</b> embeddings are very computationally expensive to train, most <b>machine</b> <b>learning</b> practitioners will load a pre-trained set of embeddings. We will load a collection of pre-trained embeddings and measure similarity between <b>word</b> embeddings ...", "dateLastCrawled": "2022-01-25T02:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "How to Solve <b>Analogies</b> with Word2Vec | by Khuyen Tran | Towards Data ...", "url": "https://towardsdatascience.com/how-to-solve-analogies-with-word2vec-6ebaf2354009", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/how-to-solve-<b>analogies</b>-with-<b>word</b>2vec-6ebaf2354009", "snippet": "To make words understood by <b>machine</b> <b>learning</b> algorithms, <b>word</b> <b>embedding</b> is used to map words into vectors of real numbers. There are various <b>word</b> <b>embedding</b> models and word2vec is one of them. In simple words, word2vec is a group of related models that are used to produce <b>word</b> embeddings. These models are trained to construct the linguistic contexts of words. Word2vec takes a large corpus of text and produces a vector space, with each unique <b>word</b> in the corpus being assigned to a ...", "dateLastCrawled": "2022-01-30T16:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Word2Vec in Gensim Explained for Creating <b>Word</b> <b>Embedding</b> Models ...", "url": "https://machinelearningknowledge.ai/word2vec-in-gensim-explained-for-creating-word-embedding-models-pretrained-and-custom/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>knowledge.ai/<b>word</b>2vec-in-gensim-explained-for-creating-<b>word</b>...", "snippet": "What is <b>Word</b> Embeddings? <b>Machine</b> <b>learning</b> and ... This is another way putting that word2vec can draw the <b>analogy</b> that if Man is to Woman then Kind is to Queen! The publicly released model of word2vec by Google consists of 300 features and the model is trained in the Google news dataset. The vocabulary size of the model is around 1.6 billion words. However, this might have taken a huge time for the model to be trained on but they have applied a method of simple subsampling approach to ...", "dateLastCrawled": "2022-02-02T15:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "DeepLearning <b>series: Natural Language Processing and Word Embeddings</b> ...", "url": "https://medium.com/machine-learning-bites/deeplearning-series-natural-language-processing-and-word-embeddings-70599080efc9", "isFamilyFriendly": true, "displayUrl": "https://medium.com/<b>machine</b>-<b>learning</b>-bites/deep<b>learning</b>-series-natural-language...", "snippet": "<b>Learning</b> <b>word</b> embeddings: When we implement an algorithm to learn <b>word</b> embeddings, what we end up <b>learning</b> is an <b>embedding</b> matrix. For a 300-feature <b>embedding</b> and a 10,000-<b>word</b> vocabulary, the ...", "dateLastCrawled": "2021-10-27T13:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Introduction to <b>Word Embeddings</b>. What is a <b>word</b> <b>embedding</b>? | by Hunter ...", "url": "https://towardsdatascience.com/introduction-to-word-embeddings-4cf857b12edc", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/introduction-to-<b>word-embeddings</b>-4cf857b12edc", "snippet": "A very basic definition of a <b>word</b> <b>embedding</b> is a real number, vector representation of a <b>word</b>. Typically, these days, words with similar meaning will have vector representations that are close together in the <b>embedding</b> space (though this hasn\u2019t always been the case). When constructing a <b>word</b> <b>embedding</b> space, typically the goal is to capture some sort of relationship in that space, be it meaning, morphology, context, or some other kind of relationship. By encoding <b>word embeddings</b> in a ...", "dateLastCrawled": "2022-01-30T04:18:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Word Embeddings Explained. What is <b>Word Embedding</b> ? | by Ashwin Prasad ...", "url": "https://medium.com/analytics-vidhya/word-embeddings-explained-62c046f7c79e", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/word-<b>embedding</b>s-explained-62c046f7c79e", "snippet": "<b>Word Embedding</b> is a technique in Natural Language Processing which is used to represent words in a Deep <b>Learning</b> environment. The main advantage of using <b>word embedding</b> is that it allows words of\u2026", "dateLastCrawled": "2022-01-24T11:15:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Survey and challenges of story generation models - A multimodal ...", "url": "https://www.sciencedirect.com/science/article/pii/S156625352030378X", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S156625352030378X", "snippet": "Pang et al. used the deep Bolzmann <b>machine</b> (DBM), which is a joint density model for the visual, auditory, and textual modalities, for <b>learning</b> highly non-linear relations between low-level features across different modalities for emotional prediction. It is trained using joint representation over multimodal inputs; thus, it can handle training samples, which is absent from certain modality. It can be used for emotional prediction and retrieval on any combination of modalities.", "dateLastCrawled": "2022-01-24T04:42:00.0000000Z", "language": "en", "isNavigational": false}], [], []], "all_bing_queries": ["+(word embedding)  is like +(teaching a computer to read)", "+(word embedding) is similar to +(teaching a computer to read)", "+(word embedding) can be thought of as +(teaching a computer to read)", "+(word embedding) can be compared to +(teaching a computer to read)", "machine learning +(word embedding AND analogy)", "machine learning +(\"word embedding is like\")", "machine learning +(\"word embedding is similar\")", "machine learning +(\"just as word embedding\")", "machine learning +(\"word embedding can be thought of as\")", "machine learning +(\"word embedding can be compared to\")"]}