{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Multi-Head Attention with Disagreement Regularization</b> | Request PDF", "url": "https://www.researchgate.net/publication/334116621_Multi-Head_Attention_with_Disagreement_Regularization", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/334116621_<b>Multi-Head</b>_Attention_with...", "snippet": "<b>Multi-head</b> attention is a set of <b>multiple</b> heads that jointly learn <b>different</b> representations at every position in the sequence [14]. The proposed attention method (ATT_BO) has three main parts ...", "dateLastCrawled": "2022-01-03T02:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A <b>Hierarchical Structured Multi-Head Attention Network</b> for Multi-Turn ...", "url": "https://www.researchgate.net/publication/339632026_A_Hierarchical_Structured_Multi-Head_Attention_Network_for_Multi-Turn_Response_Generation", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/339632026_A_Hierarchical_Structured_Multi...", "snippet": "In addition, we need to generate diversified comments, because <b>different</b> <b>people</b> usually have <b>different</b> opinions on the <b>same</b> news in the real world. In this paper, we propose a Gated Attention ...", "dateLastCrawled": "2022-01-16T07:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Multi-turn Dialogue Response Generation with Autoregressive Transformer ...", "url": "https://www.arxiv-vanity.com/papers/1908.01841/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1908.01841", "snippet": "The GPT-2 autoregressive transformer model is a slightly modified decoder section of the transformer architecture [Vaswani et al.2017, Radford et al.2019] and uses <b>multiple</b> layers of masked <b>multi-head</b> <b>self-attention</b> to map a sequence of input tokens to a sequence of output tokens (i.e., the input sequence token shifted one position to the right).", "dateLastCrawled": "2021-11-27T13:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Corpus of Controlled Opinionated and Knowledgeable Movie Discussions ...", "url": "https://deepai.org/publication/a-corpus-of-controlled-opinionated-and-knowledgeable-movie-discussions-for-training-neural-conversation-models", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/a-corpus-of-controlled-opinionated-and-knowledgeable...", "snippet": "It has the <b>same</b> architecture as the decoder part from vaswani2017attention with the <b>multi-head</b> attention over the encoder removed, as here we do not have an encoder. This architecture was introduced by radford2018improving and is commonly known as GPT. It has a stack of 12 identical layers and each layer consists of 2 sub-layers: a masked <b>multi-head</b> <b>self-attention</b> layer and a position-wise, fully connected feed-forward layer with", "dateLastCrawled": "2021-12-09T03:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Accepted Papers: Main Conference</b> | COLING\u20192020", "url": "https://coling2020.org/pages/accepted_papers_main_conference.html", "isFamilyFriendly": true, "displayUrl": "https://coling2020.org/pages/<b>accepted_papers_main_conference</b>.html", "snippet": "The triple-level <b>self-attention</b> treats head entity, relation, and tail entity as a sequence and captures the dependency within a triple. <b>At the same</b> <b>time</b> the pseudo residual connection retains primitive semantic features. Furthermore, to deal with symmetric and antisymmetric relations, two schemas of score function are designed via a position ...", "dateLastCrawled": "2022-01-30T19:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A Simple and Efficient Multi-Task Learning Approach for Conditioned ...", "url": "https://www.readkong.com/page/a-simple-and-efficient-multi-task-learning-approach-for-5731916", "isFamilyFriendly": true, "displayUrl": "https://www.readkong.com/page/a-simple-and-efficient-multi-task-learning-approach-for...", "snippet": "Additionally, there is a h A specific <b>self-attention</b> mask is required for general condition label g with a parametric vec- each task, while the objectives of three tasks are es- tor kg as its key and a zero vector vg as its value. sentially the <b>same</b> \u2013 some tokens of the target side The former corresponds to conditioned generation, (labeled response or text) are randomly masked, while the latter to the general dialogue that gen- and the final hidden vectors H L corresponding to erates words ...", "dateLastCrawled": "2022-02-01T08:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "New submissions for Tue, 12 Oct 21 \u00b7 Issue #438 \u00b7 kobiso/daily-arxiv ...", "url": "https://github.com/kobiso/daily-arxiv-noti/issues/438", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/kobiso/daily-arxiv-noti/issues/438", "snippet": "Most existing approaches require paired training images; i.e. images of the <b>same</b> <b>person</b> with the <b>same</b> clothing in <b>different</b> poses. However, obtaining sufficiently large datasets with paired data is challenging and costly. Previous methods that forego paired supervision lack realism. We propose a self-supervised framework named SPICE (Self-supervised <b>Person</b> Image CrEation) that closes the image quality gap with supervised methods. The key insight enabling self-supervision is to exploit 3D ...", "dateLastCrawled": "2022-01-27T01:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "nlp21/acl.dev at main \u00b7 dbamman/nlp21 \u00b7 GitHub", "url": "https://github.com/dbamman/nlp21/blob/main/HW3/acl.dev", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/dbamman/nlp21/blob/main/HW3/acl.dev", "snippet": "<b>At the same</b> <b>time</b>, many studies have highlighted the importance of text preprocessing, as an integral step to any natural language processing prediction model and downstream task. While preprocessing in affective systems is well-studied, preprocessing in word vector-based models applied to affective systems, is not. To address this limitation, we conduct a comprehensive analysis of the role of preprocessing techniques in affective analysis based on word vector models. Our analysis is the ...", "dateLastCrawled": "2021-12-24T13:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "ICPR2020 Paper Browser", "url": "https://ailb-web.ing.unimore.it/icpr/paper/305/nn", "isFamilyFriendly": true, "displayUrl": "https://ailb-web.ing.unimore.it/icpr/paper/305/nn", "snippet": "Videos that contain <b>multiple</b> potentially occluded <b>people</b> captured from freely moving monocular cameras are very common in real-world scenarios, while 3D HPE for such scenarios is quite challenging, partially because there is a lack of such data with accurate 3D ground truth labels in existing datasets. In this paper, we propose a temporal regression network with a gated convolution module to transform 2D joints to 3D and recover the missing occluded joints in the meantime. A simple yet ...", "dateLastCrawled": "2022-01-29T15:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "[D] Why is python the most popular language for Machine Learning ...", "url": "https://www.reddit.com/r/MachineLearning/comments/iigch6/d_why_is_python_the_most_popular_language_for/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/MachineLearning/comments/iigch6/d_why_is_python_the_most...", "snippet": "It was a way to get reasonable performance without <b>having</b> to write all your exploratory data munging in C++ and without <b>having</b> to mix and match <b>multiple</b> separate languages and tools even in small projects, so <b>people</b> started to use that. 2. Share. Report Save. level 1 \u00b7 1y. Python is widely used in scientific computing. It has numpy and scipy. I believe that python was actually created for scientific computing. MATLAB is also widely used, but python has the advantage that it is a &quot;real ...", "dateLastCrawled": "2021-09-12T22:39:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Multi-Head Attention with Disagreement Regularization</b> | Request PDF", "url": "https://www.researchgate.net/publication/334116621_Multi-Head_Attention_with_Disagreement_Regularization", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/334116621_<b>Multi-Head</b>_Attention_with...", "snippet": "<b>Multi-head</b> attention is a set of <b>multiple</b> heads that jointly learn <b>different</b> representations at every position in the sequence [14]. The proposed attention method (ATT_BO) has three main parts ...", "dateLastCrawled": "2022-01-03T02:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A <b>Hierarchical Structured Multi-Head Attention Network</b> for Multi-Turn ...", "url": "https://www.researchgate.net/publication/339632026_A_Hierarchical_Structured_Multi-Head_Attention_Network_for_Multi-Turn_Response_Generation", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/339632026_A_Hierarchical_Structured_Multi...", "snippet": "In addition, we need to generate diversified comments, because <b>different</b> <b>people</b> usually have <b>different</b> opinions on the <b>same</b> news in the real world. In this paper, we propose a Gated Attention ...", "dateLastCrawled": "2022-01-16T07:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Multi-turn Dialogue Response Generation with Autoregressive Transformer ...", "url": "https://www.arxiv-vanity.com/papers/1908.01841/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1908.01841", "snippet": "The GPT-2 autoregressive transformer model is a slightly modified decoder section of the transformer architecture [Vaswani et al.2017, Radford et al.2019] and uses <b>multiple</b> layers of masked <b>multi-head</b> <b>self-attention</b> to map a sequence of input tokens to a sequence of output tokens (i.e., the input sequence token shifted one position to the right).", "dateLastCrawled": "2021-11-27T13:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Simple and Efficient Multi-Task Learning Approach for Conditioned ...", "url": "https://www.readkong.com/page/a-simple-and-efficient-multi-task-learning-approach-for-5731916", "isFamilyFriendly": true, "displayUrl": "https://www.readkong.com/page/a-simple-and-efficient-multi-task-learning-approach-for...", "snippet": "Additionally, there is a h A specific <b>self-attention</b> mask is required for general condition label g with a parametric vec- each task, while the objectives of three tasks are es- tor kg as its key and a zero vector vg as its value. sentially the <b>same</b> \u2013 some tokens of the target side The former corresponds to conditioned generation, (labeled response or text) are randomly masked, while the latter to the general dialogue that gen- and the final hidden vectors H L corresponding to erates words ...", "dateLastCrawled": "2022-02-01T08:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Similar</b> papers - ailb-web.ing.unimore.it", "url": "https://ailb-web.ing.unimore.it/icpr/paper/305/nn", "isFamilyFriendly": true, "displayUrl": "https://ailb-web.ing.unimore.it/icpr/paper/305/nn", "snippet": "Videos that contain <b>multiple</b> potentially occluded <b>people</b> captured from freely moving monocular cameras are very common in real-world scenarios, while 3D HPE for such scenarios is quite challenging, partially because there is a lack of such data with accurate 3D ground truth labels in existing datasets. In this paper, we propose a temporal regression network with a gated convolution module to transform 2D joints to 3D and recover the missing occluded joints in the meantime. A simple yet ...", "dateLastCrawled": "2022-01-29T15:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Similar</b> papers - ailb-web.ing.unimore.it", "url": "https://ailb-web.ing.unimore.it/icpr/paper/550/nn", "isFamilyFriendly": true, "displayUrl": "https://ailb-web.ing.unimore.it/icpr/paper/550/nn", "snippet": "Forecasting human actions and motion trajectories addresses the problem of predicting what a <b>person</b> is going to do next and how they will perform it. This is crucial in a wide range of applications such as assisted living and future co-robotic settings. We propose to simultaneously learn actions and action-related human motion dynamics, while existing works perform them independently. In this paper, we present a method to jointly forecast categories of human action and the pose of skeletal ...", "dateLastCrawled": "2021-11-26T22:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "nlp21/acl.dev at main \u00b7 dbamman/nlp21 \u00b7 GitHub", "url": "https://github.com/dbamman/nlp21/blob/main/HW3/acl.dev", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/dbamman/nlp21/blob/main/HW3/acl.dev", "snippet": "<b>At the same</b> <b>time</b>, many studies have highlighted the importance of text preprocessing, as an integral step to any natural language processing prediction model and downstream task. While preprocessing in affective systems is well-studied, preprocessing in word vector-based models applied to affective systems, is not. To address this limitation, we conduct a comprehensive analysis of the role of preprocessing techniques in affective analysis based on word vector models. Our analysis is the ...", "dateLastCrawled": "2021-12-24T13:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "New submissions for Tue, 12 Oct 21 \u00b7 Issue #438 \u00b7 kobiso/daily-arxiv ...", "url": "https://github.com/kobiso/daily-arxiv-noti/issues/438", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/kobiso/daily-arxiv-noti/issues/438", "snippet": "Its main research goal is to retrieve persons with the <b>same</b> identity from <b>different</b> cameras. However, traditional <b>person</b> Re-ID methods require manual marking of <b>person</b> targets, which consumes a lot of labor costs. With the widespread application of deep neural networks in the field of computer vision, a large number of deep learning-based <b>person</b> Re-ID methods have emerged. To facilitate researchers to better understand the latest research results and future development trends in this field ...", "dateLastCrawled": "2022-01-27T01:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "[D] Why is python the most popular language for Machine Learning ...", "url": "https://www.reddit.com/r/MachineLearning/comments/iigch6/d_why_is_python_the_most_popular_language_for/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/MachineLearning/comments/iigch6/d_why_is_python_the_most...", "snippet": "It was a way to get reasonable performance without <b>having</b> to write all your exploratory data munging in C++ and without <b>having</b> to mix and match <b>multiple</b> separate languages and tools even in small projects, so <b>people</b> started to use that. 2. Share. Report Save. level 1 \u00b7 1y. Python is widely used in scientific computing. It has numpy and scipy. I believe that python was actually created for scientific computing. MATLAB is also widely used, but python has the advantage that it is a &quot;real ...", "dateLastCrawled": "2021-09-12T22:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(PDF) Deep <b>Learning for NLP and Speech Recognition</b> | William Jacome ...", "url": "https://www.academia.edu/43190210/Deep_Learning_for_NLP_and_Speech_Recognition", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/43190210/Deep_<b>Learning_for_NLP_and_Speech_Recognition</b>", "snippet": "Academia.edu is a platform for academics to share research papers.", "dateLastCrawled": "2022-02-02T18:16:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Multi-Head Attention with Disagreement Regularization</b> | Request PDF", "url": "https://www.researchgate.net/publication/334116621_Multi-Head_Attention_with_Disagreement_Regularization", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/334116621_<b>Multi-Head</b>_Attention_with...", "snippet": "<b>Multi-head</b> attention is a set of <b>multiple</b> heads that jointly learn <b>different</b> representations at every position in the sequence [14]. The proposed attention method (ATT_BO) has three main parts ...", "dateLastCrawled": "2022-01-03T02:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "TG Net | PDF | Cognition | Cognitive Science", "url": "https://www.scribd.com/document/553735159/tg-net", "isFamilyFriendly": true, "displayUrl": "https://www.scribd.com/document/553735159/tg-net", "snippet": "Each encoder is composed of <b>multi-head</b> attention, which serves the <b>same</b> purpose of a simple attention layer, to look at other words in a sentence while processing a single word, and a feedforward neural network. The output of each encoder is sent upwards to the next encoder in the stack of encoders. The last encoder sends its outputs to each ...", "dateLastCrawled": "2022-01-27T16:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Multi-turn Dialogue Response Generation with Autoregressive Transformer ...", "url": "https://www.arxiv-vanity.com/papers/1908.01841/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1908.01841", "snippet": "Neural dialogue models, despite their successes, still suffer from lack of relevance, diversity, and in many cases coherence in their generated responses. These issues have been attributed to reasons including (1) short-range model architectures that capture limited temporal dependencies, (2) limitations of the maximum likelihood training objective, (3) the concave entropy profile of dialogue datasets resulting into short and generic responses, and (4) out-of-vocabulary problem leading to ...", "dateLastCrawled": "2021-11-27T13:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Deep Learning for predicting neutralities in Offensive Language ...", "url": "https://www.sciencedirect.com/science/article/pii/S095741742100871X", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S095741742100871X", "snippet": "It is based on the idea of <b>self-attention</b> and is a function of queries, keys, and values. Vectors are used for queries, keys, and values. They are calculated from input embeddings using independent weight matrices for values (W v), queries (W q), and keys (W k). Queries and keys have the <b>same</b> dimension d k. Dimension of values is d v. Attention is a mapping of query and key\u2013value pairs to an output function: It is a weighted sum of values where each value weight computed is a compatibility ...", "dateLastCrawled": "2021-09-24T13:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>MoEL: Mixture of Empathetic Listeners</b> | DeepAI", "url": "https://deepai.org/publication/moel-mixture-of-empathetic-listeners", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>moel-mixture-of-empathetic-listeners</b>", "snippet": "3 <b>Mixture of Empathetic Listeners</b>. The dialogue context is an alternating set of utterances from speaker and listener. We denote the dialogue context as C={U 1,S1,U 2,S2,\u22ef,U t} and the speaker emotion state at each utterance as Emo={e1,e2,\u22ef,et} where \u2200ei\u2208{1,\u2026,n}. Then, our model aims to track the speaker emotional state et from the ...", "dateLastCrawled": "2021-12-23T09:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) Deep <b>Learning for NLP and Speech Recognition</b> | William Jacome ...", "url": "https://www.academia.edu/43190210/Deep_Learning_for_NLP_and_Speech_Recognition", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/43190210/Deep_<b>Learning_for_NLP_and_Speech_Recognition</b>", "snippet": "Academia.edu is a platform for academics to share research papers.", "dateLastCrawled": "2022-02-02T18:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "New submissions for Fri, 4 Jun 21 \u00b7 Issue #125 \u00b7 dajinstory/daily-arxiv ...", "url": "https://github.com/dajinstory/daily-arxiv-noti/issues/125", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/dajinstory/daily-arxiv-noti/issues/125", "snippet": "With our design, the <b>same</b> image <b>can</b> be embedded to <b>different</b> semantic clusters with semantic attention (i.e., coerce semantic masks) as an additional input channel. To achieve such attention, a novel two-stage training strategy is presented. We evaluate the proposed method on multi-organ medical image segmentation task, as our major task, with both in-house data and BTCV 2015 datasets. Comparing with the supervised and semi-supervised training state-of-the-art in the backbone of ResNet-50 ...", "dateLastCrawled": "2021-09-03T07:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>ONNX import/export</b> \u00b7 Issue #10 \u00b7 FluxML/ML-Coordination-Tracker \u00b7 <b>GitHub</b>", "url": "https://github.com/FluxML/ML-Coordination-Tracker/issues/10", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/FluxML/ML-Coordination-Tracker/issues/10", "snippet": "Yeah, the essence of this argument is certainly a wrench in Julias narrative about everything being first class so <b>people</b> <b>can</b> just design their own extensions to other packages. I think <b>multiple</b> representations <b>can</b> exist <b>at the same</b> <b>time</b>. Some <b>people</b> will use CompGraph, and some <b>people</b> will prefer to switch to Flux layers. We don&#39;t have to put ...", "dateLastCrawled": "2022-01-04T22:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Commonsense Reasoning for Natural Language Understanding: A Survey of ...", "url": "https://www.arxiv-vanity.com/papers/1904.01172/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1904.01172", "snippet": "Commonsense knowledge and commonsense reasoning are some of the main bottlenecks in machine intelligence. In the NLP community, many benchmark datasets and tasks have been created to address commonsense reasoning for language understanding. These tasks are designed to assess machines\u2019 ability to acquire and learn commonsense knowledge in order to reason and understand natural language text. As these tasks become instrumental and a driving force for commonsense research, this paper aims to ...", "dateLastCrawled": "2021-12-26T07:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Research Tracks</b> - Schedule - WWW2020", "url": "https://www2020.citi.sinica.edu.tw/schedule/research_track/", "isFamilyFriendly": true, "displayUrl": "https://www2020.citi.sinica.edu.tw/schedule/research_track", "snippet": "In addition to allowing the direct exchange of messages among pairs of users, the app also enables group <b>conversations</b>, where <b>multiple</b> <b>people</b> <b>can</b> interact with one another. A number of recent studies have shown that WhatsApp groups play an important role as an information dissemination platform, especially during important social mobilization events. In this paper, we build upon those prior efforts by taking a first look into the use of {\\it audio} messages in WhatsApp groups, a type of ...", "dateLastCrawled": "2022-01-29T00:41:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Multi-Head Attention with Disagreement Regularization</b> | Request PDF", "url": "https://www.researchgate.net/publication/334116621_Multi-Head_Attention_with_Disagreement_Regularization", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/334116621_<b>Multi-Head</b>_Attention_with...", "snippet": "<b>Multi-head</b> attention is a set of <b>multiple</b> heads that jointly learn <b>different</b> representations at every position in the sequence [14]. The proposed attention method (ATT_BO) has three main parts ...", "dateLastCrawled": "2022-01-03T02:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A <b>Hierarchical Structured Multi-Head Attention Network</b> for Multi-Turn ...", "url": "https://www.researchgate.net/publication/339632026_A_Hierarchical_Structured_Multi-Head_Attention_Network_for_Multi-Turn_Response_Generation", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/339632026_A_Hierarchical_Structured_Multi...", "snippet": "In addition, we need to generate diversified comments, because <b>different</b> <b>people</b> usually have <b>different</b> opinions on the <b>same</b> news in the real world. In this paper, we propose a Gated Attention ...", "dateLastCrawled": "2022-01-16T07:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) Multimodal Approach of <b>Speech Emotion Recognition Using Multi</b> ...", "url": "https://www.researchgate.net/publication/340286682_Multimodal_Approach_of_Speech_Emotion_Recognition_Using_Multi-Level_Multi-Head_Fusion_Attention-Based_Recurrent_Neural_Network", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/340286682_Multimodal_Approach_of_Speech...", "snippet": "These features are fed parallelly into the <b>self-attention</b> mechanism base RNNs to exploit the context for each timestamp; then we use the <b>multi-head</b> attention technique to fuse all representatives ...", "dateLastCrawled": "2022-01-18T05:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Multi-turn Dialogue Response Generation with Autoregressive Transformer ...", "url": "https://www.arxiv-vanity.com/papers/1908.01841/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1908.01841", "snippet": "The GPT-2 autoregressive transformer model is a slightly modified decoder section of the transformer architecture [Vaswani et al.2017, Radford et al.2019] and uses <b>multiple</b> layers of masked <b>multi-head</b> <b>self-attention</b> to map a sequence of input tokens to a sequence of output tokens (i.e., the input sequence token shifted one position to the right).", "dateLastCrawled": "2021-11-27T13:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) CGMVQA: A new Classification and Generative Model for Medical ...", "url": "https://www.researchgate.net/publication/339872979_CGMVQA_A_new_Classification_and_Generative_Model_for_Medical_Visual_Question_Answering", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/339872979_CGMVQA_A_new_Classification_and...", "snippet": "W e reduce the parameters of the <b>multi-head</b> <b>self-attention</b> transformer to cut the computational cost down. W e adjust the masking and output layers to change the functions of the model.", "dateLastCrawled": "2021-10-18T05:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Machine Learning in Python: Recent Trends, Technologies, and Challenges ...", "url": "https://speakerdeck.com/rasbt/machine-learning-in-python-recent-trends-technologies-and-challenges", "isFamilyFriendly": true, "displayUrl": "https://speakerdeck.com/rasbt/machine-learning-in-python-recent-trends-technologies...", "snippet": "Right: An illustration of <b>multi- head</b> attention (with K = 3 heads) by node 1 on its neighborhood. <b>Different</b> arrow styles and colors denote independent attention computations. The aggregated features from each head are concatenated or averaged to obtain ~ h0 1 . applying a nonlinearity, ): ~ h 0 i = 0 @ X j2Ni \u21b5ijW~ hj 1 A . (4) To stabilize the learning process of <b>self-attention</b>, we have found extending our mechanism to em- ploy <b>multi-head</b> attention to be bene\ufb01cial, similarly to Vaswani ...", "dateLastCrawled": "2022-01-28T06:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "One System to Rule them All: a Universal Intent Recognition System for ...", "url": "https://deepai.org/publication/one-system-to-rule-them-all-a-universal-intent-recognition-system-for-customer-service-chatbots", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/one-system-to-rule-them-all-a-universal-intent...", "snippet": "12/15/21 - Customer service chatbots are conversational systems designed to provide information to customers about products/services offered ...", "dateLastCrawled": "2022-01-17T12:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "ICPR2020 Paper Browser", "url": "https://ailb-web.ing.unimore.it/icpr/paper/640/nn", "isFamilyFriendly": true, "displayUrl": "https://ailb-web.ing.unimore.it/icpr/paper/640/nn", "snippet": "In this work, we propose <b>different</b> variants of the <b>self-attention</b> based network for emotion prediction from movies, which we call AttendAffectNet. We take both audio and video into account and incorporate the relation among <b>multiple</b> modalities by applying <b>self-attention</b> mechanism in a novel manner into the extracted features for emotion prediction. We compare it to the typically temporal integration of the <b>self-attention</b> based model, which in our case, allows to capture the relation of ...", "dateLastCrawled": "2022-01-06T16:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "ICPR2020 Paper Browser", "url": "https://ailb-web.ing.unimore.it/icpr/paper/545/nn", "isFamilyFriendly": true, "displayUrl": "https://ailb-web.ing.unimore.it/icpr/paper/545/nn", "snippet": "Unlike existing LSTM based methods, it pays attention to short- and long-term dependencies among video frames through an elaborate dual <b>self-attention</b> architecture, which <b>can</b> handle longer-term dependencies and admit parallel computing. To reconcile the outputs of dual <b>self-attention</b>, we rely on a two-stream capsule network to learn the underlying frame selection criteria. Experiments on real-world datasets show the advantages of the proposed approach <b>compared</b> with state-of-the-art methods.", "dateLastCrawled": "2022-01-04T00:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "ICPR2020 Paper Browser", "url": "https://ailb-web.ing.unimore.it/icpr/paper/305/nn", "isFamilyFriendly": true, "displayUrl": "https://ailb-web.ing.unimore.it/icpr/paper/305/nn", "snippet": "Videos that contain <b>multiple</b> potentially occluded <b>people</b> captured from freely moving monocular cameras are very common in real-world scenarios, while 3D HPE for such scenarios is quite challenging, partially because there is a lack of such data with accurate 3D ground truth labels in existing datasets. In this paper, we propose a temporal regression network with a gated convolution module to transform 2D joints to 3D and recover the missing occluded joints in the meantime. A simple yet ...", "dateLastCrawled": "2022-01-29T15:48:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "10.5. <b>Multi-Head Attention</b> \u2014 Dive into Deep <b>Learning</b> 0.17.2 documentation", "url": "http://d2l.ai/chapter_attention-mechanisms/multihead-attention.html", "isFamilyFriendly": true, "displayUrl": "d2l.ai/chapter_attention-mechanisms/<b>multihead-attention</b>.html", "snippet": "This design is called <b>multi-head attention</b>, where each of the \\(h\\) attention pooling outputs is a head [Vaswani et al., 2017]. Using fully-connected layers to perform learnable linear transformations, Fig. 10.5.1 describes <b>multi-head attention</b>.", "dateLastCrawled": "2022-02-02T01:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Introduction to Transformers in Machine Learning</b>", "url": "https://www.machinecurve.com/index.php/2020/12/28/introduction-to-transformers-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/.../12/28/<b>introduction-to-transformers-in-machine-learning</b>", "snippet": "The masked <b>multi-head</b> attention segment, which performs <b>multi-head</b> <b>self-attention</b> on the outputs, but does so in a masked way, so that positions depend on the past only. The <b>multi-head</b> attention segment , which performs <b>multi-head</b> <b>self-attention</b> on a combination of the ( encoded ) inputs and the outputs, so that the model learns to correlate encoded inputs with desired outputs.", "dateLastCrawled": "2022-02-03T03:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Attending to Attention. A summary of a revolutionary paper\u2026 | by Akash ...", "url": "https://towardsdatascience.com/attending-to-attention-eba798f0e940", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/attending-to-attention-eba798f0e940", "snippet": "The first is a <b>multi-head</b> <b>self-attention</b> mechanism, and the second is a simple, position-wise fully connected feed-forward network. We employ a residual connection around each of the two sub-layers, followed by layer normalization. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of ...", "dateLastCrawled": "2022-01-25T10:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Machine</b> <b>Learning</b> Glossary: Language Evaluation | Google Developers", "url": "https://developers.google.com/machine-learning/glossary/language", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine</b>-<b>learning</b>/glossary/language", "snippet": "Refer also to <b>self-attention</b> and <b>multi-head</b> <b>self-attention</b>, which are the building blocks of Transformers. B. bag of words. #language. A representation of the words in a phrase or passage, irrespective of order. For example, bag of words represents the following three phrases identically: the dog jumps; jumps the dog; dog jumps the; Each word is mapped to an index in a sparse vector, where the vector has an index for every word in the vocabulary. For example, the phrase the dog jumps is ...", "dateLastCrawled": "2022-01-29T02:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Intuitive Introduction to BERT \u2013 MachineCurve</b>", "url": "https://www.machinecurve.com/index.php/2021/01/04/intuitive-introduction-to-bert/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/index.php/2021/01/04/<b>intuitive-introduction-to-bert</b>", "snippet": "From our article about GPT: \u201cThe input is then served to a masked <b>multi-head</b> attention segment, which computes <b>self-attention</b> in a unidirectional way.Here, the residual is added and the result is layer normalized.\u201d Indeed, GPT (which uses the Transformer decoder segment autoregressively during pretraining) and the original Transformer (which performs Seq2Seq), apply a mask in one of the attention modules \u2013 the masked <b>multi-head</b> <b>self-attention</b> subsegment in the decoder segment.. For any ...", "dateLastCrawled": "2022-01-30T22:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Capturing Multi-Resolution Context by Dilated <b>Self-Attention</b>", "url": "https://www.merl.com/publications/docs/TR2021-036.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.merl.com/publications/docs/TR2021-036.pdf", "snippet": "to <b>machine</b> translation or language modeling, where close-by words are more likely to have a dependent relationship, while only a few distant words or word groups are relevant to trace the semantic con-text and syntax of a sentence [15]. This hypothesis is investigated in this work by combining re-stricted (or time-restricted) <b>self-attention</b> with a dilation mechanism, whereby a high <b>self-attention</b> resolution for neighboring frames and a lower <b>self-attention</b> resolution for distant information ...", "dateLastCrawled": "2021-12-02T00:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "CoAtNet: how to perfectly combine CNNs and Transformers | by Leonardo ...", "url": "https://medium.com/codex/coatnet-how-to-perfectly-combine-cnns-and-transformer-9632e187ecbf", "isFamilyFriendly": true, "displayUrl": "https://medium.com/codex/coatnet-how-to-perfectly-combine-cnns-and-transformer-9632e...", "snippet": "The <b>multi-head</b> attention block computes <b>self-attention</b> several times with different weight matrices and then concatenates the results together, which are resized to the embedding dimension using ...", "dateLastCrawled": "2022-01-26T06:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Transformers in NLP: A beginner friendly explanation | Towards Data Science", "url": "https://towardsdatascience.com/transformers-89034557de14", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>transformer</b>s-89034557de14", "snippet": "If you are looking for an <b>analogy</b> between <b>self attention</b> and attention, think of z serving the purpose of context vectors and not global alignment weights. The <b>Transformer</b> . \u26a0\ufe0f A word of caution: the contents of this image may appear exponentially more complicated than they are. We will break this scary beast down into small baby beasts and it will all make sense. (I promise #2) (left) The <b>Transformer</b> architecture. Source: paper. (right) An abstracted version of the same for better ...", "dateLastCrawled": "2022-02-02T13:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "5.3. Underfitting and Overfitting \u2014 Dive into Deep <b>Learning</b> 0.17.0 ...", "url": "http://preview.d2l.ai/d2l-en/master/chapter_machine-learning-fundamentals/underfit-overfit.html", "isFamilyFriendly": true, "displayUrl": "preview.d2l.ai/d2l-en/master/chapter_<b>machine</b>-<b>learning</b>-fundamentals/underfit-overfit.html", "snippet": "The noise term \\(\\epsilon\\) obeys a normal distribution with a mean of 0 and a standard deviation of 0.1. For optimization, we typically want to avoid very large values of gradients or losses. This is why the features are rescaled from \\(x^i\\) to \\(\\frac{x^i}{i!}\\).It allows us to avoid very large values for large exponents \\(i\\).We will synthesize 100 samples each for the training set and test set.", "dateLastCrawled": "2021-10-08T21:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "9.5. <b>Machine Translation</b> and the Dataset \u2014 Dive into Deep <b>Learning</b> 0.17 ...", "url": "https://d2l.ai/chapter_recurrent-modern/machine-translation-and-dataset.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_recurrent-modern/<b>machine-translation</b>-and-dataset.html", "snippet": "<b>Machine Translation</b> and the Dataset \u2014 Dive into Deep <b>Learning</b> 0.17.2 documentation. 9.5. <b>Machine Translation</b> and the Dataset. We have used RNNs to design language models, which are key to natural language processing. Another flagship benchmark is <b>machine translation</b>, a central problem domain for sequence transduction models that transform ...", "dateLastCrawled": "2022-01-29T21:57:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(multi-head self-attention)  is like +(a person having multiple conversations with different people at the same time)", "+(multi-head self-attention) is similar to +(a person having multiple conversations with different people at the same time)", "+(multi-head self-attention) can be thought of as +(a person having multiple conversations with different people at the same time)", "+(multi-head self-attention) can be compared to +(a person having multiple conversations with different people at the same time)", "machine learning +(multi-head self-attention AND analogy)", "machine learning +(\"multi-head self-attention is like\")", "machine learning +(\"multi-head self-attention is similar\")", "machine learning +(\"just as multi-head self-attention\")", "machine learning +(\"multi-head self-attention can be thought of as\")", "machine learning +(\"multi-head self-attention can be compared to\")"]}