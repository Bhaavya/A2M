{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "What <b>does \u201cfairness\u201d mean for machine learning systems</b>?", "url": "https://haas.berkeley.edu/wp-content/uploads/What-is-fairness_-EGAL2.pdf", "isFamilyFriendly": true, "displayUrl": "https://haas.berkeley.edu/wp-content/uploads/What-is-fairness_-EGAL2.pdf", "snippet": "<b>demographic</b> <b>parity</b> in particular may seem <b>like</b> a good solution but is a simplistic ap-proach to fairness that can still be at odds with other definitions of fairness5-- such as justice. Also, even if satisfying <b>demographic</b> <b>parity</b> based on gender, for example, when overlaying race on top of gender, this <b>parity</b> can be off. It\u2019s also important ...", "dateLastCrawled": "2022-01-28T05:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A Tutorial on <b>Fairness</b> in <b>Machine</b> <b>Learning</b> | by Ziyuan Zhong | Towards ...", "url": "https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-tutorial-on-<b>fairness</b>-in-<b>machine</b>-<b>learning</b>-3ff8ba1040cb", "snippet": "<b>Machine</b> <b>Learning</b>, the most widely used AI techniques, relies heavily on data. It is a common misconception that AI is absolutely objective. AI is objective only in the sense of <b>learning</b> what human teaches. The data provided by human can be highly-biased. It has been found in 2016 that COMPAS, the <b>algorithm</b> used for recidivism prediction produces much higher false positive rate for black people than white people(see Fig2, Larson et al. ProPublica, 2016). Fig2: The bias in COMPAS. (from Larson ...", "dateLastCrawled": "2022-02-01T18:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Trade-Offs between Fairness and Privacy in <b>Machine</b> <b>Learning</b>", "url": "https://crcs.seas.harvard.edu/files/crcs/files/ai4sg-21_paper_23.pdf", "isFamilyFriendly": true, "displayUrl": "https://crcs.seas.harvard.edu/files/crcs/files/ai4sg-21_paper_23.pdf", "snippet": "De\ufb01nition (<b>Demographic</b> <b>parity</b>). A binary classi\ufb01er hsatis-\ufb01es <b>demographic</b> <b>parity</b> if with respect to random variables A and Y Pr z\u02d8D [h(z) = 1jA= 1] = Pr z\u02d8D [h(z) = 1jA= 0]: De\ufb01nition (Equal opportunity [Hardt et al., 2016]). A binary classi\ufb01er hsatis\ufb01es equal opportunity if with respect to ran-dom variables Aand Y Pr z\u02d8D", "dateLastCrawled": "2021-09-30T12:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Machine</b> <b>Learning</b> Glossary: <b>Fairness</b> | <b>Google Developers</b>", "url": "https://developers.google.com/machine-learning/glossary/fairness", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine</b>-<b>learning</b>/glossary/<b>fairness</b>", "snippet": "For example, if both Lilliputians and Brobdingnagians apply to Glubbdubdrib University, <b>demographic</b> <b>parity</b> is achieved if the percentage of Lilliputians admitted is the same as the percentage of Brobdingnagians admitted, irrespective of whether one group is on average more qualified than the other. Contrast with equalized odds and equality of opportunity, which permit classification results in aggregate to depend on sensitive attributes, but do not permit classification results for certain ...", "dateLastCrawled": "2022-02-02T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Fairness in Machine Learning</b> \u2014 Labelia (ex Substra Foundation)", "url": "https://www.labelia.org/en/blog/fairness-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.labelia.org/en/blog/<b>fairness-in-machine-learning</b>", "snippet": "Some are exclusive of the other (<b>like</b> equalized odds and <b>demographic</b> <b>parity</b>, see the explanation below of the 3 main definitions). A company, or organization, will have to decide which definition applies to its product(s). Aequitas (A DSSG initiative) has issued an excellent view of fairness definitions : Aequitas. Let\u2019s focus on the most common fairness in use: <b>Demographic</b> <b>parity</b>, Equality of Opportunity. Equalized Odds. Let\u2019s take a binary parameter (0/1), G (ex: gender), that we want ...", "dateLastCrawled": "2022-01-30T18:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "How to define <b>fairness</b> to detect and prevent discriminatory outcomes in ...", "url": "https://towardsdatascience.com/how-to-define-fairness-to-detect-and-prevent-discriminatory-outcomes-in-machine-learning-ef23fd408ef2", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/how-to-define-<b>fairness</b>-to-detect-and-prevent...", "snippet": "In order to detect discriminatory outcomes in <b>Machine</b> <b>Learning</b> predictions, we need to compare how well our model treats different user segments. Valeria Cortez. Sep 23, 2019 \u00b7 9 min read. This can be achieved is by defining a metric that describes the notion of <b>fairness</b> in our model. For example, when looking at university admissions, we can compare admission rates of men and women. This corresponds to the use of <b>Demographic</b> <b>Parity</b> as the mathematical definition of <b>fairness</b>. It states that ...", "dateLastCrawled": "2022-01-28T08:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Fairness in <b>ML 2: Equal opportunity and odds</b>", "url": "https://www2.cs.duke.edu/courses/fall18/compsci590.1/lectures/FairML2.pdf", "isFamilyFriendly": true, "displayUrl": "https://www2.cs.duke.edu/courses/fall18/compsci590.1/lectures/FairML2.pdf", "snippet": "<b>Demographic</b> <b>parity</b> Issues \u2022 Does not seem \u201cfair\u201d to allow random performance on A = 0 \u2022 Perfect classification is impossible 6 A = 1 A = 0 . Perfect Classifier and Fairness \u2022 The perfect classifier may not ensure <b>demographic</b> <b>parity</b> \u2013 Y is correlated with A \u2022 What if we did not know how the classifier C was created? \u2013 No access to the classifier (to retrain) \u2013 No access to the training data (human created classifier) 7. True Positive <b>Parity</b> (TPP) (or equal opportunity ...", "dateLastCrawled": "2022-01-28T09:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "&#39;<b>Un&#39;Fair Machine Learning Algorithms</b> by Runshan Fu, Manmohan Aseri ...", "url": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3408275", "isFamilyFriendly": true, "displayUrl": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3408275", "snippet": "<b>Machine</b> <b>Learning</b> algorithms are becoming widely deployed in real world decision-making. Ensuring fairness in algorithmic decision-making is a crucial policy issue. Current legislation ensures fairness by barring <b>algorithm</b> designers from using <b>demographic</b> information in their decision-making. As a result, the algorithms need to ensure equal treatment to be legally compliant. However, in many cases, ensuring equal treatment leads to disparate impact particularly when there are differences ...", "dateLastCrawled": "2022-01-29T16:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A <b>Machine</b> <b>Learning</b>-Based Prediction Model for Cardiovascular Risk in ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8578855/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8578855", "snippet": "Objective: Preeclampsia affects 2\u20138% of women and doubles the risk of cardiovascular disease in women after preeclampsia. This study aimed to develop a model based on <b>machine</b> <b>learning</b> to predict postpartum cardiovascular risk in preeclamptic women. Methods: Collecting <b>demographic</b> characteristics and clinical serum markers associated with preeclampsia during pregnancy of 907 preeclamptic women retrospectively, we predicted the cardiovascular risk (ischemic heart disease, ischemic ...", "dateLastCrawled": "2022-01-28T23:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Responsible AI in action, locally</b> <b>and on Azure Machine Learning</b> | by ...", "url": "https://medium.com/microsoftazure/responsible-ai-in-action-locally-and-on-azure-machine-learning-a515e0585e69", "isFamilyFriendly": true, "displayUrl": "https://medium.com/microsoftazure/<b>responsible-ai-in-action-locally</b>-and-on-azure...", "snippet": "use the InterpretML library to deep dive into un-mitigated models in order to understand if/why they don\u2019t meet <b>demographic</b> <b>parity</b>; finally, I\u2019ll show you how to leverage state-of-the-art ...", "dateLastCrawled": "2022-01-22T05:42:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Identifying and Correcting Label <b>Bias</b> in <b>Machine</b> <b>Learning</b> | by Rani ...", "url": "https://towardsdatascience.com/identifying-and-correcting-label-bias-in-machine-learning-ed177d30349e", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/identifying-and-correcting-label-<b>bias</b>-in-<b>machine</b>...", "snippet": "<b>Demographic</b> <b>parity</b> \u2014 Classifier should make positive predictions on a protected population group at the same rate as the entire population. Disparate impact \u2014 <b>Similar</b> to <b>demographic</b> <b>parity</b> but without the classifier knowing which protected population groups exist and which data points relate to such protected groups.", "dateLastCrawled": "2022-01-30T18:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Identifying and correcting Label Bias in <b>Machine</b> <b>Learning</b> - Pye AI", "url": "https://www.pye.ai/2020/11/15/identifying-and-correcting-label-bias-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.pye.ai/2020/11/15/identifying-and-correcting-label-bias-in-<b>machine</b>-<b>learning</b>", "snippet": "<b>Demographic</b> <b>parity</b> \u2013 Classifier should make positive predictions on a protected population group at the same rate as the entire population. Disparate impact \u2013 <b>Similar</b> to <b>demographic</b> <b>parity</b> but without the classifier knowing which protected population groups exist and which data points relate to such protected groups.", "dateLastCrawled": "2022-01-28T14:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Tutorial #1: <b>bias and fairness in AI</b> - Borealis AI", "url": "https://www.borealisai.com/en/blog/tutorial1-bias-and-fairness-ai/", "isFamilyFriendly": true, "displayUrl": "https://www.borealisai.com/en/blog/tutorial1-bias-and-fairness-ai", "snippet": "Complementary to this is individual fairness which mandates that <b>similar</b> individuals should be treated similarly regardless of group membership. In this blog, we&#39;ll mainly focus on group fairness, three definitions of which include: (i) <b>demographic</b> <b>parity</b>, (ii) equality of odds, and (iii) equality of opportunity. We now discuss each in turn. <b>Demographic</b> <b>Parity</b>. <b>Demographic</b> <b>parity</b> or statistical <b>parity</b> suggests that a predictor is unbiased if the prediction $\\hat{y}$ is independent of the ...", "dateLastCrawled": "2022-02-02T02:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "What <b>does \u201cfairness\u201d mean for machine learning systems</b>?", "url": "https://haas.berkeley.edu/wp-content/uploads/What-is-fairness_-EGAL2.pdf", "isFamilyFriendly": true, "displayUrl": "https://haas.berkeley.edu/wp-content/uploads/What-is-fairness_-EGAL2.pdf", "snippet": "<b>demographic</b> <b>parity</b> in particular may seem like a good solution but is a simplistic ap-proach to fairness that can still be at odds with other definitions of fairness5-- such as justice. Also, even if satisfying <b>demographic</b> <b>parity</b> based on gender, for example, when overlaying race on top of gender, this <b>parity</b> can be off. It\u2019s also important ...", "dateLastCrawled": "2022-01-28T05:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "We Want Fair AI Algorithms \u2013 But How To Define Fairness? (Fairness ...", "url": "https://mostly.ai/blog/we-want-fair-ai-algorithms-but-how-to-define-fairness/", "isFamilyFriendly": true, "displayUrl": "https://<b>mostly.ai</b>/blog/we-want-fair-ai-<b>algorithms</b>-but-how-to-define-fairness", "snippet": "In case the tomato sorting <b>machine</b> <b>learning</b> <b>algorithm</b> satisfies <b>demographic</b> <b>parity</b>, we expect about 80% of red and 20% of yellow tomatoes within the \u201cAcceptable\u201d bin in the spaghetti factory. In other words, we expect the fractions of red and yellow tomatoes in the global population to be reflected in the \u201cfavorable\u201d group of \u201cAcceptable\u201d tomatoes in the factory. An unfair <b>algorithm</b>, that \u201cfavors\u201d red tomatoes and discriminates against yellow ones, would put more than 80% of ...", "dateLastCrawled": "2022-01-29T01:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Trade-Offs between Fairness and Privacy in <b>Machine</b> <b>Learning</b>", "url": "https://crcs.seas.harvard.edu/files/crcs/files/ai4sg-21_paper_23.pdf", "isFamilyFriendly": true, "displayUrl": "https://crcs.seas.harvard.edu/files/crcs/files/ai4sg-21_paper_23.pdf", "snippet": "De\ufb01nition (<b>Demographic</b> <b>parity</b>). A binary classi\ufb01er hsatis-\ufb01es <b>demographic</b> <b>parity</b> if with respect to random variables A and Y Pr z\u02d8D [h(z) = 1jA= 1] = Pr z\u02d8D [h(z) = 1jA= 0]: De\ufb01nition (Equal opportunity [Hardt et al., 2016]). A binary classi\ufb01er hsatis\ufb01es equal opportunity if with respect to ran-dom variables Aand Y Pr z\u02d8D", "dateLastCrawled": "2021-09-30T12:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Training a biased model</b> | <b>Machine Learning Bias Mitigation</b>", "url": "https://cdeiuk.github.io/bias-mitigation/baseline/", "isFamilyFriendly": true, "displayUrl": "https://cdeiuk.github.io/bias-mitigation/baseline", "snippet": "<b>Demographic</b> <b>Parity</b>. <b>Demographic</b> <b>parity</b> requires that we treat all <b>demographic</b> groups equally. We start by investigating the disparity between the sexes. We do this with box plots of the model scores. A higher score means the model thinks the individual is more likely to be a high earner. It&#39;s clear that there is a major disparity between men and women, with men being awarded systematically higher scores. This model therefore does not achieve <b>demographic</b> <b>parity</b> by some margin. We do the same ...", "dateLastCrawled": "2022-01-24T18:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A Tutorial on <b>Fairness</b> in <b>Machine</b> <b>Learning</b> | by Ziyuan Zhong | Towards ...", "url": "https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-tutorial-on-<b>fairness</b>-in-<b>machine</b>-<b>learning</b>-3ff8ba1040cb", "snippet": "<b>Machine</b> <b>Learning</b>, the most widely used AI techniques, relies heavily on data. It is a common misconception that AI is absolutely objective. AI is objective only in the sense of <b>learning</b> what human teaches. The data provided by human can be highly-biased. It has been found in 2016 that COMPAS, the <b>algorithm</b> used for recidivism prediction produces much higher false positive rate for black people than white people(see Fig2,", "dateLastCrawled": "2022-02-01T18:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Accuracy and dependency comparison with NBS as a ranker and IBk as a ...", "url": "https://www.researchgate.net/figure/Accuracy-and-dependency-comparison-with-NBS-as-a-ranker-and-IBk-as-a-base-learner-with_fig2_220764898", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/figure/Accuracy-and-dependency-comparison-with-NBS-as-a...", "snippet": "In this paper, we focus on the well established <b>Demographic</b> <b>Parity</b> ... <b>Similar</b> to other <b>machine</b> <b>learning</b> algorithms, VFL suffers from fairness issues, i.e., the learned model may be unfairly ...", "dateLastCrawled": "2021-11-10T11:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Interventions | <b>Machine Learning Bias Mitigation</b>", "url": "https://cdeiuk.github.io/bias-mitigation/interventions/", "isFamilyFriendly": true, "displayUrl": "https://cdeiuk.github.io/bias-mitigation/interventions", "snippet": "We attempt to enforce <b>demographic</b> <b>parity</b>, conditional <b>demographic</b> <b>parity</b> and equalised odds on the Adult dataset with respect to sex. Run our analysis yourself on Binder. <b>Demographic</b> <b>parity</b>. This <b>algorithm</b> is very effective at imposing <b>demographic</b> <b>parity</b>. With minimal tuning we saw a <b>demographic</b> <b>parity</b> difference fall from 0.193 to 0.025, while ...", "dateLastCrawled": "2022-02-02T02:49:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) Fairness of <b>Machine</b> <b>Learning</b> Algorithms in Demography", "url": "https://www.researchgate.net/publication/358290931_Fairness_of_Machine_Learning_Algorithms_in_Demography", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/358290931_Fairness_of_<b>Machine</b>_<b>Learning</b>...", "snippet": "it <b>can</b> smoothly be implanted to other <b>Machine</b> <b>Learning</b> models and data types, together with di\ufb00erent explanatory models. An innov ative approach, such as [23],", "dateLastCrawled": "2022-02-06T02:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A Review on Fairness in <b>Machine</b> <b>Learning</b> | ACM Computing Surveys", "url": "https://dl.acm.org/doi/10.1145/3494672", "isFamilyFriendly": true, "displayUrl": "https://dl.acm.org/doi/10.1145/3494672", "snippet": "<b>Demographic</b> <b>parity</b>: This measure is similar to disparate impact, but the difference is taken instead of the ratio [36, ... However, such mechanisms are tightly coupled with the <b>machine</b> <b>algorithm</b> itself. Hence, we see that the selection of the method depends on the availability of the ground truth, the availability of the sensitive attributes at test time, and on the desired definition of fairness, which <b>can</b> also vary from one application to another. Several preliminary attempts were made to ...", "dateLastCrawled": "2022-02-07T09:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "The <b>Impossibility Theorem</b> of <b>Machine</b> Fairness \u2013 A Causal Perspective ...", "url": "https://deepai.org/publication/the-impossibility-theorem-of-machine-fairness-a-causal-perspective", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/the-<b>impossibility-theorem</b>-of-<b>machine</b>-fairness-a-causal...", "snippet": "The goal of the <b>learning</b> <b>algorithm</b> <b>can</b> then <b>be thought</b> of as manipulating the past data and making it consistent with a notion of fairness in order to make fair predictions in the future. This is a way to ensure fairness as classification models usually assume that data is independent and identically drawn from the same distribution. Conceptually, this <b>can</b> <b>be thought</b> of as introducing a correction to the labels in the dataset.", "dateLastCrawled": "2022-01-27T23:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Handling Discriminatory Biases in Data for <b>Machine Learning</b> | by ...", "url": "https://towardsdatascience.com/machine-learning-and-discrimination-2ed1a8b01038", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>machine-learning</b>-and-<b>discrimination</b>-2ed1a8b01038", "snippet": "In this case, the biases of humans are not mitigated by the <b>machine learning</b> <b>algorithm</b>. In fact, they are reproduced in the classifications that are made. Why does this happen? Recidivism scores such as those made by the Northpointe software are based on prior arrests, age of first police contact, parents\u2019 incarceration record. This information is shaped by biases in the world (such as from cultural values and nationalism) and injustices more generally (such as racial prejudices). This ...", "dateLastCrawled": "2022-01-29T20:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>How to reduce machine learning bias</b> | by Raghav Vashisht | atoti | Medium", "url": "https://medium.com/atoti/how-to-reduce-machine-learning-bias-eb24923dd18e", "isFamilyFriendly": true, "displayUrl": "https://medium.com/atoti/<b>how-to-reduce-machine-learning-bias</b>-eb24923dd18e", "snippet": "T here could be other types of <b>machine</b> <b>learning</b> bias whose origins are NOT in data. Examples of such <b>machine</b> <b>learning</b> bias include: 1. <b>Algorithm</b> bias: when there\u2019s a problem within the <b>algorithm</b> ...", "dateLastCrawled": "2022-01-27T01:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A <b>Gentle Introduction to Algorithmic Fairness</b> in Lending", "url": "https://blog.fiddler.ai/2019/04/a-gentle-introduction-to-algorithmic-fairness/", "isFamilyFriendly": true, "displayUrl": "https://blog.fiddler.ai/2019/04/a-<b>gentle-introduction-to-algorithmic-fairness</b>", "snippet": "Impact <b>parity</b>: the fraction of people given a positive decision should be equal across different groups. This is also called <b>demographic</b> <b>parity</b>, statistical <b>parity</b>, or independence of the protected class and the score [Fair2018]. There is a large body of literature on algorithmic fairness. From [Corb2018], two more definitions: Classification ...", "dateLastCrawled": "2022-01-27T00:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Responsibility Cannot Be Delegated to an <b>Algorithm</b> \u2014 <b>MACHINE</b> <b>LEARNING</b> ...", "url": "https://www.machinelearningforscience.de/en/responsibility-cannot-be-delegated-to-an-algorithm/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machinelearning</b>forscience.de/en/responsibility-<b>can</b>not-be-delegated-to-an...", "snippet": "<b>Machine</b> <b>Learning</b> for Science: Reports of algorithmic discrimination have increased recently. Ostensibly objective, <b>algorithm</b>-based systems have made decisions that disadvantage individuals and reveal the systems to be unfair. This is about things like facial recognition programs that simply fail to recognize people of color, or programs that pre-sort job applications to favor men\u2019s resumes over those of women. \u201cFix it!\u201d society then simply demands of developers, and researchers as well ...", "dateLastCrawled": "2021-11-29T09:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) A survey of bias in <b>Machine Learning through the prism</b> of ...", "url": "https://www.researchgate.net/publication/340331723_A_survey_of_bias_in_Machine_Learning_through_the_prism_of_Statistical_Parity_for_the_Adult_Data_Set", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/340331723_A_survey_of_bias_in_<b>Machine</b>...", "snippet": "<b>machine</b> <b>learning</b> studying the statistical <b>parity</b> criterion through the analysis of the example given in the A dult Income dataset. This public dataset is av ailable on the UCI <b>Machine</b> <b>Learning</b>", "dateLastCrawled": "2021-10-21T01:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Machine</b> <b>learning</b> algorithms for predicting undernutrition among under ...", "url": "https://www.cambridge.org/core/journals/public-health-nutrition/article/machine-learning-algorithms-for-predicting-undernutrition-among-underfive-children-in-ethiopia/6F72A5F339564946C2E16B27270849A8", "isFamilyFriendly": true, "displayUrl": "https://www.cambridge.org/core/journals/public-health-nutrition/article/<b>machine</b>...", "snippet": "Furthermore, <b>machine</b> <b>learning</b> (ML) is a powerful approach that intersects artificial intelligence and statistical <b>learning</b> in the process of discovering unknown relationships or patterns (Reference Alghamdi, Al-Mallah and Keteyian 20). Modern ML algorithms have shown superior predictive ability in addressing classification problems when compared with classical statistical models. Various ML algorithms have been applied in medical research", "dateLastCrawled": "2022-01-29T11:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Feature Choice and Fairness: Less May</b> be More | by Valerie Carey ...", "url": "https://towardsdatascience.com/feature-choice-and-fairness-less-may-be-more-7809ec11772e", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>feature-choice-and-fairness-less-may</b>-be-more-7809ec11772e", "snippet": "<b>Machine</b> <b>learning</b> models are based on correlation, and any feature associated with an outcome <b>can</b> be used as a decision basis; there is reason for concern. However, the risks of such a scenario occurring depend on the information available to the model and on the specific <b>algorithm</b> used. Here, I will use sample data to illustrate differences in incorporation of incidental information in random forest vs. XGBoost models, and discuss the importance of considering missing information ...", "dateLastCrawled": "2022-01-25T06:50:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "\u201cUn\u201dFair <b>Machine</b> <b>Learning</b> Algorithms | Management Science", "url": "https://pubsonline.informs.org/doi/10.1287/mnsc.2021.4065", "isFamilyFriendly": true, "displayUrl": "https://pubsonline.informs.org/doi/10.1287/mnsc.2021.4065", "snippet": "<b>Compared</b> with the current law, which requires treatment <b>parity</b>, the fair ML algorithms, which require impact <b>parity</b>, limit the benefits of a more accurate <b>algorithm</b> for a firm. As a result, profit maximizing firms could underinvest in <b>learning</b>, that is, improving the accuracy of their <b>machine</b> <b>learning</b> algorithms. We show that the investment in ...", "dateLastCrawled": "2022-02-01T05:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "How to define <b>fairness</b> to detect and prevent discriminatory outcomes in ...", "url": "https://towardsdatascience.com/how-to-define-fairness-to-detect-and-prevent-discriminatory-outcomes-in-machine-learning-ef23fd408ef2", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/how-to-define-<b>fairness</b>-to-detect-and-prevent...", "snippet": "In order to detect discriminatory outcomes in <b>Machine</b> <b>Learning</b> predictions, we need to compare how well our model treats different user segments. Valeria Cortez. Sep 23, 2019 \u00b7 9 min read. This <b>can</b> be achieved is by defining a metric that describes the notion of <b>fairness</b> in our model. For example, when looking at university admissions, we <b>can</b> compare admission rates of men and women. This corresponds to the use of <b>Demographic</b> <b>Parity</b> as the mathematical definition of <b>fairness</b>. It states that ...", "dateLastCrawled": "2022-01-28T08:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A <b>Machine</b> <b>Learning</b>-Based Prediction Model for Cardiovascular Risk in ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8578855/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8578855", "snippet": "\u2021 <b>Parity</b>, n (%) 143 (76.9) 562 (77.9) 0: 0.755 Gestational diabetes mellitus, n (%) 46 (24.7) 157 (21.8) 0: 0.389 : Pre-pregnancy risk factor ... Based on readily available clinical and <b>demographic</b> variables, an ML <b>algorithm</b> was proposed to predict the CVD occurrence of post-preeclamptic women in this study. The proposed ML <b>algorithm</b> <b>can</b> be directly applied to clinical practice for the accurate identification of high-risk patients, and it <b>can</b> be taken as a convenient prediction tool for ...", "dateLastCrawled": "2022-01-28T23:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "&#39;<b>Un&#39;Fair Machine Learning Algorithms</b>", "url": "https://www.ftc.gov/system/files/documents/public_events/1567421/fuaserisinghsrinivasan_updated2.pdf", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ftc.gov</b>/system/files/documents/public_events/1567421/fuaserisinghsriniva...", "snippet": "<b>Compared</b> to the current law, which requires treatment <b>parity</b>, these \u201cfair\u201d algorithms, which require impact <b>parity</b>, limit the bene\ufb01ts of a more accurate <b>algorithm</b> for a \ufb01rm. As a result, pro\ufb01t maximizing \ufb01rms could under-invest in <b>learning</b>, i.e., improving the accuracy of their <b>machine</b> <b>learning</b> algorithms. We show that the investment in <b>learning</b> decreases when misclassi\ufb01cation is costly, which is exactly the case when greater accuracy is otherwise desired. Our paper highlights ...", "dateLastCrawled": "2022-02-03T08:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Interventions | <b>Machine Learning Bias Mitigation</b>", "url": "https://cdeiuk.github.io/bias-mitigation/interventions/", "isFamilyFriendly": true, "displayUrl": "https://cdeiuk.github.io/bias-mitigation/interventions", "snippet": "We attempt to enforce <b>demographic</b> <b>parity</b>, conditional <b>demographic</b> <b>parity</b> and equalised odds on the Adult dataset with respect to sex. Run our analysis yourself on Binder. <b>Demographic</b> <b>parity</b>. This <b>algorithm</b> is very effective at imposing <b>demographic</b> <b>parity</b>. With minimal tuning we saw a <b>demographic</b> <b>parity</b> difference fall from 0.193 to 0.025, while ...", "dateLastCrawled": "2022-02-02T02:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Identifying and Correcting Label <b>Bias</b> in <b>Machine</b> <b>Learning</b> | by Rani ...", "url": "https://towardsdatascience.com/identifying-and-correcting-label-bias-in-machine-learning-ed177d30349e", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/identifying-and-correcting-label-<b>bias</b>-in-<b>machine</b>...", "snippet": "Identifying and Correcting Label <b>Bias</b> in <b>Machine</b> <b>Learning</b>. Rani Horev. Feb 9, 2019 \u00b7 6 min read. As <b>machine</b> <b>learning</b> (ML) becomes more effective and widespread it is becoming more prevalent in systems with real-life impact, from loan recommendations to job application decisions. With the growing usage comes the risk of <b>bias</b> \u2014 biased training ...", "dateLastCrawled": "2022-01-30T18:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "&#39;<b>Un&#39;Fair Machine Learning Algorithms</b> by Runshan Fu, Manmohan Aseri ...", "url": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3408275", "isFamilyFriendly": true, "displayUrl": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3408275", "snippet": "<b>Compared</b> to the current law, which requires treatment <b>parity</b>, these ``fair&#39;&#39; algorithms, which require impact <b>parity</b>, limit the benefits of a more accurate <b>algorithm</b> for a firm. As a result, profit maximizing firms could under-invest in <b>learning</b>, i.e., improving the accuracy of their <b>machine</b> <b>learning</b> algorithms. We show that the investment in <b>learning</b> decreases when misclassification is costly, which is exactly the case when greater accuracy is otherwise desired. Our paper highlights the ...", "dateLastCrawled": "2022-01-29T16:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Intrahepatic cholestasis of pregnancy</b>: <b>machine</b>-<b>learning</b> <b>algorithm</b> to ...", "url": "https://link.springer.com/article/10.1007/s00404-021-05994-z", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00404-021-05994-z", "snippet": "The objective of this study was to apply <b>machine</b>-<b>learning</b> <b>algorithm</b> to women with pruritus, to identify women to whom bile acid will be elevated, thus, diagnosing ICP, without the availability of bile acid measurement. We also <b>compared</b> the sensitivity of our chosen <b>machine</b>-<b>learning</b> model to logistic regression. Materials and methods. Patients. This retrospective study included 336 women with low-risk pregnancies, with a chief complaint of pruritis without rash, during the second and third ...", "dateLastCrawled": "2022-01-13T08:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Algorithms for Fairness in Sequential Decision Making", "url": "http://proceedings.mlr.press/v130/wen21a/wen21a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v130/wen21a/wen21a.pdf", "snippet": "ing fairness constraints such as <b>demographic</b> <b>parity</b> or equality of opportunity <b>can</b> actually exacerbate unfairness. We propose toaddress this challenge by modeling feedback e\u21b5ects as Markov decision processes (MDPs). First, we propose analogs of fairness properties for the MDP setting. Second, we propose algorithms for <b>learning</b> fair decision-making policies for MDPs. Finally, we demonstrate the need to account for dynamical e\u21b5ects using simula-tions on a loan applicant MDP. 1 Introduction ...", "dateLastCrawled": "2022-01-19T17:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Responsibility Cannot Be Delegated to an <b>Algorithm</b> \u2014 <b>MACHINE</b> <b>LEARNING</b> ...", "url": "https://www.machinelearningforscience.de/en/responsibility-cannot-be-delegated-to-an-algorithm/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machinelearning</b>forscience.de/en/responsibility-<b>can</b>not-be-delegated-to-an...", "snippet": "<b>Machine</b> <b>Learning</b> for Science: Reports of algorithmic discrimination have increased recently. Ostensibly objective, <b>algorithm</b>-based systems have made decisions that disadvantage individuals and reveal the systems to be unfair. This is about things like facial recognition programs that simply fail to recognize people of color, or programs that pre-sort job applications to favor men\u2019s resumes over those of women. \u201cFix it!\u201d society then simply demands of developers, and researchers as well ...", "dateLastCrawled": "2021-11-29T09:09:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Pandas <b>Machine</b> <b>Learning</b> Example", "url": "https://groups.google.com/g/hslogb/c/-BvVGlSI3Ek", "isFamilyFriendly": true, "displayUrl": "https://groups.google.com/g/hslogb/c/-BvVGlSI3Ek", "snippet": "Regardless of your dataset, <b>demographic</b> <b>parity</b> is a <b>machine</b> <b>learning</b> algorithms. Data Munging It helps us to missing data of wedge form with another. Python with datetime module, i should equal to bring new example <b>machine</b>. Quite possibly the state important part clean the <b>machine</b> <b>learning</b> process is understanding the data you are working with and advantage it relates to reflect task you front to solve. Viewing the corresponding number of dropping down arrow illustrates that are not only all ...", "dateLastCrawled": "2022-01-24T06:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "An example of prediction which complies with <b>Demographic</b> <b>Parity</b> and ...", "url": "https://vertexdoc.com/doc/an-example-of-prediction-which-complies-with-demographic-parity-and-equalizes-group-wise-risks-in-the-context", "isFamilyFriendly": true, "displayUrl": "https://vertexdoc.com/doc/an-example-of-prediction-which-complies-with-<b>demographic</b>...", "snippet": "However, <b>Demographic</b> <b>Parity</b> and EGWR only define fairness on the group level and inspecting the individual level reveals a critical flow of this prediction rule. We have constrained our predictors to those that do not produce Disparate Treatment by prohibiting them from having the sensitive variable as direct input. Nevertheless, enforcing group level fairness constraints (such as DP and EGWR) forces the prediction rule to guess the sensitive attribute corresponding to a given feature vector", "dateLastCrawled": "2022-02-05T21:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) On Fair Representation in <b>Machine</b> <b>Learning</b>", "url": "https://www.researchgate.net/publication/341736051_On_Fair_Representation_in_Machine_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/.../341736051_On_Fair_Representation_in_<b>Machine</b>_<b>Learning</b>", "snippet": "<b>demographic</b> <b>parity</b> and metrics such as equalized odds are either ignored ( e.g. [21], [22]) or assumed to be automatically satis\ufb01ed by &quot;removing&quot; information about sensitive attributes", "dateLastCrawled": "2022-01-26T19:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "RStudio AI Blog: Starting to think about AI Fairness", "url": "https://blogs.rstudio.com/ai/posts/2021-07-15-ai-fairness/", "isFamilyFriendly": true, "displayUrl": "https://blogs.rstudio.com/ai/posts/2021-07-15-ai-fairness", "snippet": "Heidari et al. have written a paper comparing the three criteria \u2013 <b>demographic</b> <b>parity</b>, equality of opportunity, and predictive <b>parity</b> \u2013 to egalitarianism, equality of opportunity (EOP) in the Rawlsian sense, and EOP seen through the glass of luck egalitarianism, respectively. While the <b>analogy</b> is fascinating, it too assumes that we may take what is in the data at face value. In their likening predictive <b>parity</b> to luck egalitarianism, they have to go to especially great lengths, in ...", "dateLastCrawled": "2022-01-31T09:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Classification - Fairness and <b>machine</b> <b>learning</b>", "url": "https://fairmlbook.org/classification.html", "isFamilyFriendly": true, "displayUrl": "https://fairmlbook.org/classification.html", "snippet": "Simply put, the goal of classification is to determine a plausible value for an unknown variable Y given an observed variable X.For example, we might try to predict whether a loan applicant will pay back her loan by looking at various characteristics such as credit history, income, and net worth. Classification also applies in situations where the variable Y does not refer to an event that lies in the future. For example, we can try to determine if an image contains a cat by looking at the ...", "dateLastCrawled": "2022-02-03T07:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Fairness in <b>Machine</b> <b>Learning</b> | Request PDF", "url": "https://www.researchgate.net/publication/348079021_Fairness_in_Machine_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/348079021_Fairness_in_<b>Machine</b>_<b>Learning</b>", "snippet": "We propose and explore adversarial representation <b>learning</b> as a natural method of ensuring those entities will act fairly, and connect group fairness (<b>demographic</b> <b>parity</b>, equalized odds, and equal ...", "dateLastCrawled": "2022-01-03T04:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "1. Introduction \u2014 <b>Dive into Deep Learning</b> 0.17.2 documentation", "url": "http://d2l.ai/chapter_introduction/index.html", "isFamilyFriendly": true, "displayUrl": "d2l.ai/chapter_introduction/index.html", "snippet": "<b>Machine</b> <b>learning</b> is the study of powerful techniques that can learn from experience. As a <b>machine</b> <b>learning</b> algorithm accumulates more experience, typically in the form of observational data or interactions with an environment, its performance improves. Contrast this with our deterministic e-commerce platform, which performs according to the same business logic, no matter how much experience accrues, until the developers themselves learn and decide that it is time to update the software. In ...", "dateLastCrawled": "2022-01-26T11:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A Human-centric Approach to Fairness in AI", "url": "https://www.timlrx.com/blog/a-human-centric-approach-to-fairness-in-ai", "isFamilyFriendly": true, "displayUrl": "https://www.timlrx.com/blog/a-human-centric-approach-to-fairness-in-ai", "snippet": "This includes measures such as <b>Demographic</b> <b>Parity</b> / Statistical <b>Parity</b> (Dwork et al., 2012), Equalized Odds Metric (Hardt et al., 2016) and Calibration within Groups (Chouldechova, 2017). They are all statistical measures derived from the predictions of a classification model and differ in terms of which element(s) of the confusion matrix they are trying to test for equivalence. In another survey of fairness definitions, Verma &amp; Rubin (2018) listed 20 definitions of fairness, 13 belonging to ...", "dateLastCrawled": "2022-02-03T02:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Adversarial Approaches to Debiasing Word Embeddings", "url": "https://web.stanford.edu/class/cs224n/reports/final_reports/report101.pdf", "isFamilyFriendly": true, "displayUrl": "https://web.stanford.edu/class/cs224n/reports/final_reports/report101.pdf", "snippet": "<b>Machine</b> <b>learning</b> for natural language processing (NLP) leverages valuable data from human language for useful downstream applications such as <b>machine</b> translation and sentiment analysis. Recent studies, however, have shown that training data in these applications are prone to harboring stereotypes and unwanted biases commonly exhibited in human language. Since NLP systems are designed to understand novel associations within training data, they are similarly vulnerable to propagating these ...", "dateLastCrawled": "2022-01-25T20:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "The <b>measure and mismeasure of fairness: a critical review</b> of fair ...", "url": "https://blog.acolyer.org/2020/02/03/measure-mismeasure-fairness/", "isFamilyFriendly": true, "displayUrl": "https://blog.acolyer.org/2020/02/03/measure-mismeasure-fairness", "snippet": "In case you\u2019re wondering where on earth I\u2019m going with this\u2026 it\u2019s a very stretched <b>analogy</b> I\u2019ve been playing with in my mind. One premise of many models of fairness in <b>machine</b> <b>learning</b> is that you can measure (\u2018prove\u2019) fairness of a <b>machine</b> <b>learning</b> model from within the system \u2013 i.e. from properties of the model itself and perhaps the data it is trained on. Beyond the questions of whether any one model of fairness is better or worse than another, I\u2019m coming to the ...", "dateLastCrawled": "2022-01-30T11:52:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(demographic parity)  is like +(machine learning algorithm)", "+(demographic parity) is similar to +(machine learning algorithm)", "+(demographic parity) can be thought of as +(machine learning algorithm)", "+(demographic parity) can be compared to +(machine learning algorithm)", "machine learning +(demographic parity AND analogy)", "machine learning +(\"demographic parity is like\")", "machine learning +(\"demographic parity is similar\")", "machine learning +(\"just as demographic parity\")", "machine learning +(\"demographic parity can be thought of as\")", "machine learning +(\"demographic parity can be compared to\")"]}