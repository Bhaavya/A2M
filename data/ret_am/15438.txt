{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Fairness</b> <b>in Machine</b> <b>Learning</b>", "url": "https://www.cs.toronto.edu/~guerzhoy/310f19/lec/W11/fairness_new.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.toronto.edu/~guerzhoy/310f19/lec/W11/<b>fairness</b>_new.pdf", "snippet": "<b>Counterfactual</b> <b>Fairness</b>: Red Car 29 Gen der Aggr Red Car Acci dent s \u2022 No direct relationship between Gender and Accidents, but Gender and Aggressiveness both cause driving red cars \u2022 If we use Red Car as a variable (or any other variables that Gender causes, directly or indirectly), our estimates will in general not satisfy <b>counterfactual</b> ...", "dateLastCrawled": "2022-01-27T11:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Counterfactual Fairness</b> - ResearchGate", "url": "https://www.researchgate.net/publication/315454664_Counterfactual_Fairness", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/315454664_<b>Counterfactual_Fairness</b>", "snippet": "way of assessing an existing decision making process, it is not as natural as <b>counterfactual fairness</b> in. the context of <b>machine</b> <b>learning</b>. Approximate <b>fairness</b> and model validation. The notion of ...", "dateLastCrawled": "2022-01-26T14:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A Tutorial on <b>Fairness</b> <b>in Machine</b> <b>Learning</b> | by Ziyuan Zhong | Towards ...", "url": "https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-tutorial-on-<b>fairness</b>-<b>in-machine</b>-<b>learning</b>-3ff8ba1040cb", "snippet": "The content is based on: the tutorial on <b>fairness</b> given by Solon Bacrocas and Moritz Hardt at NIPS2017, day1 and day4 from CS 294: <b>Fairness</b> <b>in Machine</b> <b>Learning</b> taught by Moritz Hardt at UC Berkeley and my own understanding of <b>fairness</b> literatures. I highly encourage interested readers to check out the linked NIPS tutorial and the course website.", "dateLastCrawled": "2022-02-01T18:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Fairness in Machine Learning</b>", "url": "http://www.homepages.ucl.ac.uk/~ucgtrbd/talks/counterfactual_fairness_talk.pdf", "isFamilyFriendly": true, "displayUrl": "www.homepages.ucl.ac.uk/~ucgtrbd/talks/<b>counterfactual</b>_<b>fairness</b>_talk.pdf", "snippet": "<b>Fairness in Machine Learning</b> and Its Causal Aspects Ricardo Silva University College London and The Alan Turing Institute ricardo@stats.ucl.ac.uk With joint work by Matt Kusner, Joshua Loftus and Chris Russell (Turing Institute) Disclaimer I am not a lawyer or philosopher. I am a data scientist by training. I do not claim anything I say has a legal basis or a scholarly backing on ethics research. Much of the work in this area is at the frontier of multidisciplinary research. <b>Machine</b> <b>Learning</b> ...", "dateLastCrawled": "2021-11-09T17:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "On Formalizing <b>Fairness</b> in Prediction with <b>Machine</b> <b>Learning</b>", "url": "https://www.fatml.org/media/documents/formalizing_fairness_in_prediction_with_ml.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.fatml.org/media/documents/formalizing_<b>fairness</b>_in_prediction_with_ml.pdf", "snippet": "<b>Counterfactual</b> measures Group <b>fairness</b> Impact Individual <b>fairness</b> Preferred impact Equality of opportunity Next, we will see the existing formalizations of <b>fairness</b> in the <b>machine</b> <b>learning</b> literature. Table1summarizes how they answer the questions presented above. 2.1. <b>Fairness</b> through unawareness Any predictor which is not group-conditional satis\ufb01es this measure. Formally, it is de\ufb01ned as follows: De\ufb01nition 1 (<b>fairness</b> through unawareness) A predictor is said to achieve <b>fairness</b> ...", "dateLastCrawled": "2022-01-31T09:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Counterfactual</b> Explanations for <b>Machine</b> <b>Learning</b>: A Review", "url": "https://ml-retrospectives.github.io/neurips2020/camera_ready/5.pdf", "isFamilyFriendly": true, "displayUrl": "https://ml-retrospectives.github.io/neurips2020/camera_ready/5.pdf", "snippet": "This section gives the background about the social implications of <b>machine</b> <b>learning</b>, explainability research <b>in machine</b> <b>learning</b>, and some prior studies about <b>counterfactual</b> explanations. 2.1 Social Implications of <b>Machine</b> <b>Learning</b> Establishing <b>fairness</b> and making an automated tool\u2019s decision explainable are two broad ways in which we can ...", "dateLastCrawled": "2022-01-29T04:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Achieving Causal Fairness in Machine Learning</b>", "url": "https://scholarworks.uark.edu/cgi/viewcontent.cgi?article=5197&context=etd", "isFamilyFriendly": true, "displayUrl": "https://scholarworks.uark.edu/cgi/viewcontent.cgi?article=5197&amp;context=etd", "snippet": "develop <b>fairness</b>-aware <b>machine</b> <b>learning</b> algorithms such that the decisions made by them are not only accurate but also subject to <b>fairness</b> requirements. In the literature, <b>machine</b> <b>learning</b> researchers have proposed association-based <b>fairness</b> notions, e.g., statistical parity, disparate impact, equality of opportunity, etc., and developed respective discrimination mitigation approaches. However, these works did not consider that <b>fairness</b> should be treated as a causal relationship. Although it ...", "dateLastCrawled": "2021-11-14T08:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Contrastive <b>Fairness</b> <b>in Machine</b> <b>Learning</b> - IEEE Journals &amp; Magazine", "url": "https://ieeexplore.ieee.org/document/9134722", "isFamilyFriendly": true, "displayUrl": "https://ieeexplore.ieee.org/document/9134722", "snippet": "How can one ensure that the decisions were taken based on merit and not on protected attributes <b>like</b> race or sex? These are the questions that must be answered now that many decisions in real life can be made through <b>machine</b> <b>learning</b>. However, research in <b>fairness</b> of algorithms has focused on the <b>counterfactual</b> questions \u201cwhat if?\u201d or \u201cwhy?\u201d, whereas in real life most subjective questions of consequence are contrastive: \u201cwhy this but not that?\u201d. We introduce concepts and ...", "dateLastCrawled": "2021-01-29T10:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "9.3 <b>Counterfactual</b> Explanations | Interpretable <b>Machine</b> <b>Learning</b>", "url": "https://christophm.github.io/interpretable-ml-book/counterfactual.html", "isFamilyFriendly": true, "displayUrl": "https://christophm.github.io/interpretable-ml-book/<b>counterfactual</b>.html", "snippet": "In interpretable <b>machine</b> <b>learning</b>, <b>counterfactual</b> explanations can be used to explain predictions of individual instances. The \u201cevent\u201d is the predicted outcome of an instance, the \u201ccauses\u201d are the particular feature values of this instance that were input to the model and \u201ccaused\u201d a certain prediction. Displayed as a graph, the relationship between the inputs and the prediction is very simple: The feature values cause the prediction. FIGURE 9.9: The causal relationships between ...", "dateLastCrawled": "2022-02-03T02:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "[D] Is it possible to create <b>machine</b> <b>learning</b> system that is fair ...", "url": "https://www.reddit.com/r/MachineLearning/comments/gqugku/d_is_it_possible_to_create_machine_learning/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/<b>MachineLearning</b>/comments/gqugku/d_is_it_possible_to_create...", "snippet": "Started thinking about <b>fairness</b> of <b>machine</b> <b>learning</b> models recently. Wiki page for <b>Fairness</b>_(<b>machine</b>_<b>learning</b>) defines <b>fairness</b> as: <b>In machine</b> <b>learning</b>, a given algorithm is said to be fair, or to have <b>fairness</b> if its results are independent of some variables we consider to be sensitive and not related with it (f.e.: gender, ethnicity, sexual orientation, etc.). UC Berkley CS 294 in turn defines <b>fairness</b> as:. understanding and mitigating discrimination based on sensitive characteristics ...", "dateLastCrawled": "2022-01-01T13:56:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Counterfactual Fairness</b> - NIPS", "url": "https://papers.nips.cc/paper/2017/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://papers.nips.cc/paper/2017/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf", "snippet": "<b>Counterfactual Fairness</b> Matt Kusner The Alan Turing Institute and University of Warwick mkusner@turing.ac.uk Joshua Loftus New York University loftus@nyu.edu Chris Russell The Alan Turing Institute and University of Surrey crussell@turing.ac.uk Ricardo Silva The Alan Turing Institute and University College London ricardo@stats.ucl.ac.uk Abstract <b>Machine</b> <b>learning</b> can impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending ...", "dateLastCrawled": "2022-01-26T15:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Counterfactual Fairness</b> - ResearchGate", "url": "https://www.researchgate.net/publication/315454664_Counterfactual_Fairness", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/315454664_<b>Counterfactual_Fairness</b>", "snippet": "way of assessing an existing decision making process, it is not as natural as <b>counterfactual fairness</b> in. the context of <b>machine</b> <b>learning</b>. Approximate <b>fairness</b> and model validation. The notion of ...", "dateLastCrawled": "2022-01-26T14:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Privacy and <b>Fairness</b>: two very different <b>machine</b> <b>learning</b> ideals? | Revue", "url": "https://newsletter.mukulrathi.com/issues/privacy-and-fairness-two-very-different-machine-learning-ideals-717119", "isFamilyFriendly": true, "displayUrl": "https://newsletter.mukulrathi.com/issues/privacy-and-<b>fairness</b>-two-very-different...", "snippet": "<b>Counterfactual</b> <b>fairness</b> could be <b>similar</b> for <b>fairness</b>, where you can guarantee <b>fairness</b> given your assumptions about causality. There\u2019s a catch It appears that current methods for privacy and <b>fairness</b> seem to be conflicting.", "dateLastCrawled": "2022-01-30T17:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "[2008.13122v1] <b>Adversarial Learning for Counterfactual Fairness</b>", "url": "https://arxiv.org/abs/2008.13122v1", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/abs/2008.13122v1", "snippet": "In recent years, <b>fairness</b> has become an important topic in the <b>machine</b> <b>learning</b> research community. In particular, <b>counterfactual</b> <b>fairness</b> aims at building prediction models which ensure <b>fairness</b> at the most individual level. Rather than globally considering equity over the entire population, the idea is to imagine what any individual would look like with a variation of a given attribute of interest, such as a different gender or race for instance. Existing approaches rely on Variational ...", "dateLastCrawled": "2021-10-25T21:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A Tutorial on <b>Fairness</b> <b>in Machine</b> <b>Learning</b> | by Ziyuan Zhong | Towards ...", "url": "https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-tutorial-on-<b>fairness</b>-<b>in-machine</b>-<b>learning</b>-3ff8ba1040cb", "snippet": "The content is based on: the tutorial on <b>fairness</b> given by Solon Bacrocas and Moritz Hardt at NIPS2017, day1 and day4 from CS 294: <b>Fairness</b> <b>in Machine</b> <b>Learning</b> taught by Moritz Hardt at UC Berkeley and my own understanding of <b>fairness</b> literatures. I highly encourage interested readers to check out the linked NIPS tutorial and the course website.", "dateLastCrawled": "2022-02-01T18:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Tutorial on <b>Fairness</b> of <b>Machine</b> <b>Learning</b> in Recommender Systems", "url": "https://fairness-tutorial.github.io/files/Tutorial_on_Fairness_in_Recommendation.pdf", "isFamilyFriendly": true, "displayUrl": "https://<b>fairness</b>-tutorial.github.io/files/Tutorial_on_<b>Fairness</b>_in_Recommendation.pdf", "snippet": "Tutorial on <b>Fairness</b> of <b>Machine</b> <b>Learning</b> in Recommender Systems Yunqi Li Department of Computer Science Rutgers University, NJ, US yunqi.li@rutgers.edu Yingqiang Ge Department of Computer Science Rutgers University, NJ, US yingqiang.ge@rutgers.edu Yongfeng Zhang Department of Computer Science Rutgers University, NJ, US yongfeng.zhang@rutgers.edu ABSTRACT Recently, there has been growing attention on <b>fairness</b> considera-tions <b>in machine</b> <b>learning</b>. As one of the most pervasive applications of ...", "dateLastCrawled": "2022-01-29T16:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Verifying Individual <b>Fairness</b> <b>in Machine</b> <b>Learning</b> Models", "url": "http://proceedings.mlr.press/v124/george-john20a/george-john20a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v124/george-john20a/george-john20a.pdf", "snippet": "Verifying Individual <b>Fairness</b> <b>in Machine</b> <b>Learning</b> Models Philips George John, Deepak Vijaykeerthy, Diptikalyan Saha IBM Research AI Bengaluru 560 045, India Abstract We consider the problem of whether a given decision model, working with structured data, has individual <b>fairness</b>. Following the work of Dwork, a model is individually biased (or unfair) if there is a pair of valid inputs which are close to each other (according to an ap-propriate metric) but are treated differently by the model ...", "dateLastCrawled": "2022-02-03T15:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Measuring <b>Fairness</b> <b>in Machine</b> <b>Learning</b> Models", "url": "https://blog.dataiku.com/measuring-fairness-in-machine-learning-models", "isFamilyFriendly": true, "displayUrl": "https://blog.dataiku.com/measuring-<b>fairness</b>-<b>in-machine</b>-<b>learning</b>-models", "snippet": "However, before detecting (un)<b>fairness</b> <b>in machine</b> <b>learning</b>, we first need to be able to define it. But <b>fairness</b> is an equivocal notion \u2014 it can be expressed in various ways to reflect the specific circumstances of the use case or the ethical perspectives of the stakeholders. Consequently, there can\u2019t be a consensus in research about what <b>fairness</b> <b>in machine</b> <b>learning</b> actually is.", "dateLastCrawled": "2022-01-25T22:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Counterfactual</b> Explanations for <b>Machine</b> <b>Learning</b>: A Review", "url": "https://ml-retrospectives.github.io/neurips2020/camera_ready/5.pdf", "isFamilyFriendly": true, "displayUrl": "https://ml-retrospectives.github.io/neurips2020/camera_ready/5.pdf", "snippet": "<b>Counterfactual</b> Explanations for <b>Machine</b> <b>Learning</b>: A Review Sahil Verma Arthur AI, University of Washington Washington D.C., USA vsahil@cs.washington.edu John Dickerson Arthur AI Washington D.C., USA john@arthur.ai Keegan Hines Arthur AI Washington D.C., USA keegan@arthur.ai Abstract <b>Machine</b> <b>learning</b> plays a role in many deployed decision systems, often in ways that are dif\ufb01cult or impossible to understand by human stakeholders. Explaining, in a human-understandable way, the relationship ...", "dateLastCrawled": "2022-01-29T04:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "9.3 <b>Counterfactual</b> Explanations | Interpretable <b>Machine</b> <b>Learning</b>", "url": "https://christophm.github.io/interpretable-ml-book/counterfactual.html", "isFamilyFriendly": true, "displayUrl": "https://christophm.github.io/interpretable-ml-book/<b>counterfactual</b>.html", "snippet": "In interpretable <b>machine</b> <b>learning</b>, <b>counterfactual</b> explanations can be used to explain predictions of individual instances. The \u201cevent\u201d is the predicted outcome of an instance, the \u201ccauses\u201d are the particular feature values of this instance that were input to the model and \u201ccaused\u201d a certain prediction. Displayed as a graph, the relationship between the inputs and the prediction is very simple: The feature values cause the prediction. FIGURE 9.9: The causal relationships between ...", "dateLastCrawled": "2022-02-03T02:12:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Counterfactual Fairness</b> - ResearchGate", "url": "https://www.researchgate.net/publication/315454664_Counterfactual_Fairness", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/315454664_<b>Counterfactual_Fairness</b>", "snippet": "way of assessing an existing decision making process, it is not as natural as <b>counterfactual fairness</b> in. the context of <b>machine</b> <b>learning</b>. Approximate <b>fairness</b> and model validation. The notion of ...", "dateLastCrawled": "2022-01-26T14:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Counterfactual Fairness</b> | DeepAI", "url": "https://deepai.org/publication/counterfactual-fairness", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>counterfactual-fairness</b>", "snippet": "<b>Machine</b> <b>learning</b> <b>can</b> impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing. In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation.", "dateLastCrawled": "2022-01-26T15:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Counterfactual</b> <b>Fairness</b> \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/1703.06856/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1703.06856", "snippet": "In large part, the initial work on <b>fairness</b> <b>in machine</b> <b>learning</b> has focused on formalizing <b>fairness</b> into quantitative definitions and using them to solve a discrimination problem in a certain dataset. Unfortunately, for a practitioner, law-maker, judge, or anyone else who is interested in implementing algorithms that control for discrimination, it <b>can</b> be difficult to decide which definition of <b>fairness</b> to choose for the task at hand. Indeed, we demonstrate that depending on the relationship ...", "dateLastCrawled": "2021-12-15T09:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) <b>Counterfactual Fairness</b>", "url": "https://www.researchgate.net/publication/324600593_Counterfactual_Fairness", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/324600593_<b>Counterfactual_Fairness</b>", "snippet": "<b>Machine</b> <b>learning</b> <b>can</b> impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing.", "dateLastCrawled": "2021-12-11T00:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Fairness</b> <b>in Machine</b> <b>Learning</b> models using Causality", "url": "https://scripties.uba.uva.nl/download?fid=674413", "isFamilyFriendly": true, "displayUrl": "https://scripties.uba.uva.nl/download?fid=674413", "snippet": "2018). In particular, <b>Counterfactual</b> <b>Fairness</b> captures the intuition that model outcomes should not change when switching the sensitive variable of a pro le, also not when other attributes change as a causal result of the switch. As a problem without consensus in this domain, recent works attempt to create <b>Counterfactual</b> <b>Fairness</b> for situations in which e ects from a sensitive variable <b>can</b> be partially allowed, depending on which variables propagate the e ect to the outcome, referred to as ...", "dateLastCrawled": "2021-11-21T06:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Fairness</b> and <b>Machine</b> <b>Learning</b>", "url": "http://ai.ethicsworkshop.org/Library/fairmlbook.pdf", "isFamilyFriendly": true, "displayUrl": "ai.ethicsworkshop.org/Library/fairmlbook.pdf", "snippet": "<b>Counterfactual</b> discrimination analysis 101 Validity of causal models 106 Problem set 113 Bibliographic notes and further reading 113 Bibliography 117. About the book This book gives a perspective on <b>machine</b> <b>learning</b> that treats <b>fair-ness</b> as a central concern rather than an afterthought. We\u2019ll review the practice of <b>machine</b> <b>learning</b> in a way that highlights ethical chal-lenges. We\u2019ll then discuss approaches to mitigate these problems. We\u2019ve aimed to make the book as broadly accessible ...", "dateLastCrawled": "2022-02-02T02:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Fairness in Machine Learning</b>", "url": "http://www.homepages.ucl.ac.uk/~ucgtrbd/talks/counterfactual_fairness_talk.pdf", "isFamilyFriendly": true, "displayUrl": "www.homepages.ucl.ac.uk/~ucgtrbd/talks/<b>counterfactual</b>_<b>fairness</b>_talk.pdf", "snippet": "<b>Fairness in Machine Learning</b> and Its Causal Aspects Ricardo Silva University College London and The Alan Turing Institute ricardo@stats.ucl.ac.uk With joint work by Matt Kusner, Joshua Loftus and Chris Russell (Turing Institute) Disclaimer I am not a lawyer or philosopher. I am a data scientist by training. I do not claim anything I say has a legal basis or a scholarly backing on ethics research. Much of the work in this area is at the frontier of multidisciplinary research. <b>Machine</b> <b>Learning</b> ...", "dateLastCrawled": "2021-11-09T17:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>What if Algorithms Could be Fair</b>? - <b>Human Readable Magazine</b>", "url": "https://humanreadablemag.com/issues/0/articles/what-if-algorithms-could-be-fair/", "isFamilyFriendly": true, "displayUrl": "https://humanreadablemag.com/issues/0/articles/<b>what-if-algorithms-could-be-fair</b>", "snippet": "<b>Counterfactual</b> <b>fairness</b> is a new approach to <b>fairness</b> <b>in machine</b> <b>learning</b>, statistical models, and algorithms. It draws on the new field of causality to go beyond statistical relationships and correlations and to model the root causes of differences between protected groups. A mathematized notion of <b>fairness</b> removes the fluff and emotion from <b>fairness</b>, allowing us to clearly model and compare our beliefs about causal relationships in the world, and to empirically test our claims about bias ...", "dateLastCrawled": "2022-01-04T00:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "The Use and Misuse of Counterfactuals in Ethical <b>Machine</b> <b>Learning</b> ...", "url": "https://www.arxiv-vanity.com/papers/2102.05085/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2102.05085", "snippet": "The use of counterfactuals for considerations of algorithmic <b>fairness</b> and explainability is gaining prominence within the <b>machine</b> <b>learning</b> community and industry. This paper argues for more caution with the use of counterfactuals when the facts to be considered are social categories such as race or gender. We review a broad body of papers from philosophy and social sciences on social ontology and the semantics of counterfactuals, and we conclude that the <b>counterfactual</b> approach <b>in machine</b> ...", "dateLastCrawled": "2021-11-18T21:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Explainable AI: a comprehensive review of the main methods | by ...", "url": "https://medium.com/@dallanoce.fd/explainable-ai-a-complete-summary-of-the-main-methods-a28f9ab132f7", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@dallanoce.fd/explainable-ai-a-complete-summary-of-the-main-methods...", "snippet": "A new approach for finding \u201cprototypes\u201d in an existing <b>machine</b> <b>learning</b> program. A prototype <b>can</b> <b>be thought</b> of as a subset of the data that have a greater influence on the predictive power of ...", "dateLastCrawled": "2022-02-02T23:34:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Counterfactual Fairness</b> - ResearchGate", "url": "https://www.researchgate.net/publication/315454664_Counterfactual_Fairness", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/315454664_<b>Counterfactual_Fairness</b>", "snippet": "way of assessing an existing decision making process, it is not as natural as <b>counterfactual fairness</b> in. the context of <b>machine</b> <b>learning</b>. Approximate <b>fairness</b> and model validation. The notion of ...", "dateLastCrawled": "2022-01-26T14:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Reviews: <b>Counterfactual</b> <b>Fairness</b>", "url": "https://papers.nips.cc/paper/2017/file/a486cd07e4ac3d270571622f4f316ec5-Reviews.html", "isFamilyFriendly": true, "displayUrl": "https://papers.nips.cc/paper/2017/file/a486cd07e4ac3d270571622f4f316ec5-Reviews.html", "snippet": "As <b>machine</b> <b>learning</b> takes a more prominent role in decision making related to people, the question of <b>fairness</b> becomes increasingly important. The proposed definition is compelling and, in my opinion, better captures human conceptions of <b>fairness</b> <b>compared</b> to prior, non-<b>counterfactual</b> definitions. Additionally, the paper is well written, easy to follow, and technically correct.", "dateLastCrawled": "2021-11-21T05:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Counterfactual Fairness</b> | DeepAI", "url": "https://deepai.org/publication/counterfactual-fairness", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>counterfactual-fairness</b>", "snippet": "<b>Machine</b> <b>learning</b> <b>can</b> impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing. In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation.", "dateLastCrawled": "2022-01-26T15:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) <b>Counterfactual Fairness</b>", "url": "https://www.researchgate.net/publication/324600593_Counterfactual_Fairness", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/324600593_<b>Counterfactual_Fairness</b>", "snippet": "<b>Machine</b> <b>learning</b> <b>can</b> impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing.", "dateLastCrawled": "2021-12-11T00:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Counterfactual</b> <b>Fairness</b> \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/1703.06856/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1703.06856", "snippet": "In large part, the initial work on <b>fairness</b> <b>in machine</b> <b>learning</b> has focused on formalizing <b>fairness</b> into quantitative definitions and using them to solve a discrimination problem in a certain dataset. Unfortunately, for a practitioner, law-maker, judge, or anyone else who is interested in implementing algorithms that control for discrimination, it <b>can</b> be difficult to decide which definition of <b>fairness</b> to choose for the task at hand. Indeed, we demonstrate that depending on the relationship ...", "dateLastCrawled": "2021-12-15T09:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Fairness</b> and <b>Machine</b> <b>Learning</b>", "url": "http://ai.ethicsworkshop.org/Library/fairmlbook.pdf", "isFamilyFriendly": true, "displayUrl": "ai.ethicsworkshop.org/Library/fairmlbook.pdf", "snippet": "<b>Counterfactual</b> discrimination analysis 101 Validity of causal models 106 Problem set 113 Bibliographic notes and further reading 113 Bibliography 117. About the book This book gives a perspective on <b>machine</b> <b>learning</b> that treats <b>fair-ness</b> as a central concern rather than an afterthought. We\u2019ll review the practice of <b>machine</b> <b>learning</b> in a way that highlights ethical chal-lenges. We\u2019ll then discuss approaches to mitigate these problems. We\u2019ve aimed to make the book as broadly accessible ...", "dateLastCrawled": "2022-02-02T02:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Fair Classification with <b>Counterfactual</b> <b>Learning</b>", "url": "http://maryamtavakol.com/assets/publications/sigir20.pdf", "isFamilyFriendly": true, "displayUrl": "maryamtavakol.com/assets/publications/sigir20.pdf", "snippet": "Recent advances <b>in machine</b> <b>learning</b> have led to emerging new ap-proaches to deal with different kinds of biases that exist in the data. On the one hand, <b>counterfactual</b> <b>learning</b> copes with biases in the policy used for sampling (or logging) the data in order to evaluate and learn new policies. On the other hand, <b>fairness</b>-aware <b>learning</b> aims at <b>learning</b> fair models to avoid discrimination against certain individuals or groups. In this paper, we design a <b>counterfactual</b> framework to model ...", "dateLastCrawled": "2021-12-15T19:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "9.3 <b>Counterfactual</b> Explanations | Interpretable <b>Machine</b> <b>Learning</b>", "url": "https://christophm.github.io/interpretable-ml-book/counterfactual.html", "isFamilyFriendly": true, "displayUrl": "https://christophm.github.io/interpretable-ml-book/<b>counterfactual</b>.html", "snippet": "In interpretable <b>machine</b> <b>learning</b>, <b>counterfactual</b> explanations <b>can</b> be used to explain predictions of individual instances. The \u201cevent\u201d is the predicted outcome of an instance, the \u201ccauses\u201d are the particular feature values of this instance that were input to the model and \u201ccaused\u201d a certain prediction. Displayed as a graph, the relationship between the inputs and the prediction is very simple: The feature values cause the prediction. FIGURE 9.9: The causal relationships between ...", "dateLastCrawled": "2022-02-03T02:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Verifying Individual <b>Fairness</b> <b>in Machine</b> <b>Learning</b> Models", "url": "http://proceedings.mlr.press/v124/george-john20a/george-john20a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v124/george-john20a/george-john20a.pdf", "snippet": "Verifying Individual <b>Fairness</b> <b>in Machine</b> <b>Learning</b> Models Philips George John, Deepak Vijaykeerthy, Diptikalyan Saha IBM Research AI Bengaluru 560 045, India Abstract We consider the problem of whether a given decision model, working with structured data, has individual <b>fairness</b>. Following the work of Dwork, a model is individually biased (or unfair) if there is a pair of valid inputs which are close to each other (according to an ap-propriate metric) but are treated differently by the model ...", "dateLastCrawled": "2022-02-03T15:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Counterfactual</b> Explanations for <b>Machine</b> <b>Learning</b>: A Review", "url": "https://ml-retrospectives.github.io/neurips2020/camera_ready/5.pdf", "isFamilyFriendly": true, "displayUrl": "https://ml-retrospectives.github.io/neurips2020/camera_ready/5.pdf", "snippet": "Explanations <b>can</b> help the <b>machine</b> <b>learning</b> model developers identify, detect, and \ufb01x bugs and other performance issues. Explanations help in adhering to laws surrounding <b>machine</b>-produced decisions, e.g., GDPR [10]. Explainability <b>in machine</b> <b>learning</b> is broadly about using inherently interpretable and transparent models or generating post-hoc explanations for opaque models. Examples of the former include linear/logistic regression, decision trees, rule sets, etc. Examples of the latter ...", "dateLastCrawled": "2022-01-29T04:21:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Counterfactual Fairness</b> - ResearchGate", "url": "https://www.researchgate.net/publication/315454664_Counterfactual_Fairness", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/315454664_<b>Counterfactual_Fairness</b>", "snippet": "way of assessing an existing decision making process, it is not as natural as <b>counterfactual fairness</b> in. the context of <b>machine</b> <b>learning</b>. Approximate <b>fairness</b> and model validation. The notion of ...", "dateLastCrawled": "2022-01-26T14:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Responsible AI practices \u2013 Google AI", "url": "https://ai.google/responsibilities/responsible-ai-practices/?category=fairness", "isFamilyFriendly": true, "displayUrl": "https://ai.google/responsibilities/responsible-ai-practices/?category=<b>fairness</b>", "snippet": "A case-study on the application of <b>fairness</b> in <b>machine</b> <b>learning</b> research to a production classification system, and new insights in how to measure and address algorithmic <b>fairness</b> issues. Research paper <b>Counterfactual</b> <b>fairness</b> in text classification through robustness Provides and compares multiple approaches for addressing <b>counterfactual</b> <b>fairness</b> issues in text models. Research paper Model Cards for Model Reporting Proposes a framework to encourage transparent model reporting. Research ...", "dateLastCrawled": "2022-02-02T14:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Towards Analogy-Based Explanations in Machine Learning</b> | DeepAI", "url": "https://deepai.org/publication/towards-analogy-based-explanations-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>towards-analogy-based-explanations-in-machine-learning</b>", "snippet": "More specifically, we take the view that an <b>analogy</b>-based approach is a viable alternative to existing approaches in the realm of explainable AI and interpretable <b>machine</b> <b>learning</b>, and that <b>analogy</b>-based explanations of the predictions produced by a <b>machine</b> <b>learning</b> algorithm can complement similarity-based explanations in a meaningful way. To corroborate these claims, we outline the basic idea of an <b>analogy</b>-based explanation and illustrate its potential usefulness by means of some examples.", "dateLastCrawled": "2022-01-10T12:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Fairness in Machine Learning: Lessons from Political Philosophy</b> | DeepAI", "url": "https://deepai.org/publication/fairness-in-machine-learning-lessons-from-political-philosophy", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>fairness-in-machine-learning-lessons-from-political</b>...", "snippet": "This discussion suggests that \u2018<b>fairness</b>\u2019 as used in the fair <b>machine</b> <b>learning</b> community is best understood as a placeholder term for a variety of normative egalitarian considerations. Notably, while egalitarianism is a widely held principle, exactly what it requires is the subject of much debate. I provide an overview of some of this debate and finish with implications for the incorporation of \u2018<b>fairness</b>\u2019 into algorithmic decision-making systems.", "dateLastCrawled": "2021-12-26T22:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Counterfactual</b> Explanation of <b>Machine</b> <b>Learning</b> Survival Models - IOS Press", "url": "https://content.iospress.com/articles/informatica/infor468", "isFamilyFriendly": true, "displayUrl": "https://content.iospress.com/articles/informatica/infor468", "snippet": "A method for <b>counterfactual</b> explanation of <b>machine</b> <b>learning</b> survival models is proposed. One of the difficulties of solving the <b>counterfactual</b> explanation problem is that the classes of examples are implicitly defined through outcomes of a <b>machine</b> <b>learning</b> survival model in the form of survival functions. A condition that establishes the difference between survival functions of the original example and the <b>counterfactual</b> is introduced. This condition is based on using a distance between mean ...", "dateLastCrawled": "2022-01-15T22:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A Human-centric Approach to <b>Fairness</b> in AI", "url": "https://www.timlrx.com/blog/a-human-centric-approach-to-fairness-in-ai", "isFamilyFriendly": true, "displayUrl": "https://www.timlrx.com/blog/a-human-centric-approach-to-<b>fairness</b>-in-ai", "snippet": "A lot of what is discussed in the <b>machine</b> <b>learning</b> literature touches on <b>fairness</b> (or rather equivalence in certain outcomes) between groups, yet this narrowly constricts <b>fairness</b> to the notion of equality. Of course, we should think about <b>fairness</b> in the context of prejudiced groups, but we should also ask whether it is fair to an individual. Adding constraints in models might lead to worse outcomes for other individuals. If the decision making processs has serious consequences e.g. a fraud ...", "dateLastCrawled": "2022-02-03T02:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "[PDF] A Survey on Bias and <b>Fairness</b> in <b>Machine</b> <b>Learning</b> | Semantic Scholar", "url": "https://www.semanticscholar.org/paper/A-Survey-on-Bias-and-Fairness-in-Machine-Learning-Mehrabi-Morstatter/0090023afc66cd2741568599057f4e82b566137c", "isFamilyFriendly": true, "displayUrl": "https://www.semanticscholar.org/paper/A-Survey-on-Bias-and-<b>Fairness</b>-in-<b>Machine</b>...", "snippet": "This survey investigated different real-world applications that have shown biases in various ways, and created a taxonomy for <b>fairness</b> definitions that <b>machine</b> <b>learning</b> researchers have defined to avoid the existing bias in AI systems. With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for <b>fairness</b> has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive ...", "dateLastCrawled": "2022-01-29T11:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "dimensions of <b>machine</b> Causality and the normative", "url": "https://maxkasy.github.io/home/files/other/ML_inequality_conference/slides_loftus.pdf", "isFamilyFriendly": true, "displayUrl": "https://maxkasy.github.io/home/files/other/ML_inequality_conference/slides_loftus.pdf", "snippet": "dimensions of <b>machine</b> <b>learning</b> Joshua Loftus (LSE Statistics) High level intro Causality, what is it good for? Causal <b>fairness</b> In prediction and ranking tasks, and with intersectionality Designing interventions Optimal fair policies, causal interference Concluding thoughts 2 / 27. Tech solutionism, using ML/AI in every situation 3 / 27. Imagination Albert Einstein: Imagination is more important than knowledge. For knowledge is limited, whereas imagination [...] stimulat[es] progress, giving ...", "dateLastCrawled": "2022-01-11T12:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Stable <b>Learning</b> and its Causal Implication", "url": "http://pengcui.thumedialab.com/papers/Stable%20Learning-tutorial-valse2021.pdf", "isFamilyFriendly": true, "displayUrl": "pengcui.thumedialab.com/papers/Stable <b>Learning</b>-tutorial-valse2021.pdf", "snippet": "Application --- <b>counterfactual</b> visual explanations ... Goyal, Yash, et al. &quot;<b>Counterfactual</b> visual explanations.&quot; International Conference on <b>Machine</b> <b>Learning</b>. PMLR, 2019. Explainability with Causality Application --- causal recommendation 17 He et al. \u201dCollaborative Causal Filtering for Out-of-Distribution Recommendation.&quot; Under review. Caual structure among user features and item features Example . Explainability and OOD \u2022Explainability would be a side product when pursuing OOD with ...", "dateLastCrawled": "2022-01-28T03:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Mitigating Political Bias in Language Models Through Reinforced Calibration", "url": "https://www.cs.dartmouth.edu/~rbliu/aaai_copy.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.dartmouth.edu/~rbliu/aaai_copy.pdf", "snippet": "\ufb01rst proposed <b>counterfactual</b> <b>fairness</b>, which treats data samples equally in actual and <b>counterfactual</b> demographic groups. Zhao et al. mitigated gender bias by augmenting original data with gender-swapping and training a unbiased system on the union of two datasets. Other augmentation techniques have reduced gender bias in hate speech detec-tion (Park, Shin, and Fung 2018; Liu et al. 2020), knowledge graph building (Mitchell et al. 2019) and <b>machine</b> transla-tion (Stanovsky, Smith, and ...", "dateLastCrawled": "2022-01-28T20:16:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Counterfactual Fairness \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/1703.06856/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1703.06856", "snippet": "<b>Machine</b> <b>learning</b> has matured to the point to where it is now being considered to automate decisions in loan lending, employee hiring, and predictive policing. In many of these scenarios however, previous decisions have been made that are unfairly biased against certain subpopulations (e.g., those of a particular race, gender, or sexual orientation). Because this past data is often biased, <b>machine</b> <b>learning</b> predictors must account for this to avoid perpetuating discriminatory practices (or ...", "dateLastCrawled": "2021-12-15T09:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Counterfactual Fairness</b> - ResearchGate", "url": "https://www.researchgate.net/publication/315454664_Counterfactual_Fairness", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/315454664_<b>Counterfactual_Fairness</b>", "snippet": "<b>Machine</b> <b>learning</b> has matured to the point to where it is now being considered to automate decisions in loan lending, employee hiring, and predictive policing. In many of these scenarios however ...", "dateLastCrawled": "2022-01-26T14:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Abstract - arXiv", "url": "https://arxiv.org/pdf/1703.06856v3.pdf", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/pdf/1703.06856v3.pdf", "snippet": "<b>machine</b> <b>learning</b> predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our de\ufb01nition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it is the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real ...", "dateLastCrawled": "2020-08-09T05:32:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(counterfactual fairness)  is like +(fairness in machine learning)", "+(counterfactual fairness) is similar to +(fairness in machine learning)", "+(counterfactual fairness) can be thought of as +(fairness in machine learning)", "+(counterfactual fairness) can be compared to +(fairness in machine learning)", "machine learning +(counterfactual fairness AND analogy)", "machine learning +(\"counterfactual fairness is like\")", "machine learning +(\"counterfactual fairness is similar\")", "machine learning +(\"just as counterfactual fairness\")", "machine learning +(\"counterfactual fairness can be thought of as\")", "machine learning +(\"counterfactual fairness can be compared to\")"]}