{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Identifying sparse connectivity patterns in the <b>brain</b> using resting ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4262564/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC4262564", "snippet": "The <b>human</b> <b>brain</b> processes information via multiple distributed networks. An accurate model of the <b>brain</b>&#39;s functional connectome is critical for understanding both normal <b>brain</b> function as well as the dysfunction present in neuropsychiatric illnesses. Current methodologies that attempt to discover the organization of the functional connectome typically assume spatial or temporal separation of the underlying networks. This assumption deviates from an intuitive understanding of <b>brain</b> function ...", "dateLastCrawled": "2022-01-24T22:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Spatial regularization and sparsity for brain</b> mapping", "url": "https://www.humanbrainmapping.org/files/thirion(1).pdf", "isFamilyFriendly": true, "displayUrl": "https://www.<b>humanbrain</b>mapping.org/files/thirion(1).pdf", "snippet": "patterns that look <b>like</b>: But in a neuroimaging article, it will look more <b>like</b> If you want to show the truly discriminative pattern, you need it to be sparse ! June 2014 Spatial Regularization &amp; <b>sparsity</b> for <b>brain</b> 15 mapping Solution: (F)ISTA Gradient descent on the smooth terms FISTA = accelerated ISTA (much faster convergence) w(t) projection on the non-smooth constrains w(t+1) Lasso: the proximal operator is simply soft-threshodling . June 2014 Spatial Regularization &amp; <b>sparsity</b> for <b>brain</b> ...", "dateLastCrawled": "2021-08-30T20:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Sparsity Enables 100x Performance Acceleration in Deep Learning</b> Networks", "url": "https://numenta.com/assets/pdf/research-publications/papers/Sparsity-Enables-100x-Performance-Acceleration-Deep-Learning-Networks.pdf", "isFamilyFriendly": true, "displayUrl": "https://numenta.com/assets/pdf/research-publications/papers/<b>Sparsity</b>-Enables-100x...", "snippet": "differently than the <b>human</b> <b>brain</b>. Unlike deep learning networks, the <b>brain</b> is highly efficient, requiring a mere 20 Watts to operate, less power than a lightbulb. At Numenta, we believe that by studying the <b>brain</b> and understanding what makes it so efficient, we can create new algorithms that approach the efficiency of the <b>brain</b>. How is the <b>brain</b> so efficient? There are many reasons, but at its foundation is the notion of <b>sparsity</b>. The <b>brain</b> stores and processes information as sparse ...", "dateLastCrawled": "2022-02-02T18:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Neocortical sparsity, active dendrites, and</b> their relevance in robust ...", "url": "https://chaseking.me/assets/papers/cse599b_sparsity.pdf", "isFamilyFriendly": true, "displayUrl": "https://chaseking.me/assets/papers/cse599b_<b>sparsity</b>.pdf", "snippet": "billion neurons (compared to the roughly 86 billion in the entire <b>human</b> <b>brain</b>) (Herculano-Houzel,2009). Exact estimates of number of neurons vary, but the neocortex is widely regarded as the center for higher-order cognition and intelligence. 1Numentais a neuroscience research institute in Redwood City, CA, specializing in theoretical neuroscience and interested in applying neuroscience principles to machine intelligence. 1. A neuron is the basic functional unit of the <b>brain</b>. A typical ...", "dateLastCrawled": "2021-06-05T11:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "The Power of <b>Sparsity</b> \u2013 My <b>Brain</b>&#39;s Thoughts", "url": "https://mybrainsthoughts.com/?p=269", "isFamilyFriendly": true, "displayUrl": "https://my<b>brain</b>sthoughts.com/?p=269", "snippet": "A <b>human</b> creates panda and gibbon concepts through repeated observation of two particular patterns of matter, with the large gap in between reflecting the non-existence of \u201chalf panda-<b>like</b> half gibbon-<b>like</b>\u201d patterns of matter. If you start with a picture of a panda and gradually change it to look more <b>like</b> a gibbon, a <b>human</b> will call it out as not realistic long before they call it a gibbon. Deep convolutional neural networks, on the other hand, lack this \u201cnot realistic\u201d gap between ...", "dateLastCrawled": "2021-12-30T20:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "How Neural <b>Magic\u2019s Deep Sparse Technology Works</b>", "url": "https://neuralmagic.com/blog/how-neural-magics-deep-sparse-technology-works/", "isFamilyFriendly": true, "displayUrl": "https://neuralmagic.com/blog/how-neural-<b>magics-deep-sparse-technology-works</b>", "snippet": "After all, the <b>human</b> <b>brain</b> addresses the computational needs of neural networks by extensively using <b>sparsity</b> to reduce them instead of adding FLOPS to match them. If our brains processed information the same way today\u2019s hardware accelerators consume computing power, we could fry an egg on our head. (You can learn more about how unraveling <b>brain</b> structure can lead to new understandings of what our futuristic machine learning hardware should look <b>like</b> by watching this video: Tissue vs ...", "dateLastCrawled": "2022-01-23T19:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Frontiers | A Practical Guide to Sparse k-Means Clustering for Studying ...", "url": "https://www.frontiersin.org/articles/10.3389/fnins.2021.668293/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/fnins.2021.668293", "snippet": "In this guide, we explored the application of <b>sparsity</b>-based clustering and showed that one algorithm, RSKC, is a good fit for revealing the subtle and gradual changes of <b>human</b> <b>brain</b> development that occurs from birth to aging. In the next decade, the amount of data collected from each postmortem <b>brain</b> sample will only continue to grow as single-cell RNA sequencing methods are applied to studying <b>human</b> <b>brain</b> development. Furthermore, the push to integrate multimodal measurements, from ...", "dateLastCrawled": "2022-01-30T04:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A Practical Guide to Sparse k-Means Clustering for Studying Molecular ...", "url": "https://pubmed.ncbi.nlm.nih.gov/34867140/", "isFamilyFriendly": true, "displayUrl": "https://pubmed.ncbi.nlm.nih.gov/34867140", "snippet": "Studying the molecular development of the <b>human</b> <b>brain</b> presents unique challenges for selecting a data analysis approach. The rare and valuable nature of <b>human</b> postmortem <b>brain</b> tissue, especially for developmental studies, means the sample sizes are small (n), but the use of high throughput genomic and proteomic methods measure the expression levels for hundreds or thousands of variables [e.g., genes or proteins (p)] for each sample.This leads to a data structure that is high dimensional (p ...", "dateLastCrawled": "2021-12-15T11:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Sparse Arti\ufb01cial Neural Networks: Adaptive Performance-based ...", "url": "https://essay.utwente.nl/80559/1/Lapshyna_BA_EEMCS.pdf", "isFamilyFriendly": true, "displayUrl": "https://essay.utwente.nl/80559/1/Lapshyna_BA_EEMCS.pdf", "snippet": "done in this \ufb01eld is based on the prede\ufb01ned <b>sparsity</b> [18, 17]. Contrary, biological neural networks evolve <b>sparsity</b> over time; ... inspired by the animal <b>brain</b>. <b>Like</b> its biological counterpart, ANN consists of neurons and neuron connections. The signal transmission by synapses in the biological <b>brain</b> is modelled as weights of the ANN. Initially, the goal of the ANNs was to perform similarly to a <b>human</b> <b>brain</b>. However, currently, they are mostly used for classi\ufb01cation, prediction ...", "dateLastCrawled": "2022-01-16T08:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Why is sparsity (usually) considered to be</b> a good thing? - Quora", "url": "https://www.quora.com/Why-is-sparsity-usually-considered-to-be-a-good-thing", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>Why-is-sparsity-usually-considered-to-be</b>-a-good-thing", "snippet": "Answer: Apart from issues of memory and storage space, which seem not too important to me (although others may disagree), <b>sparsity</b> is a desirable property of a model because it indicates that they are good! Consider what <b>sparsity</b> means: you have, say, a class of objects (maybe vectors, maybe mat...", "dateLastCrawled": "2022-01-21T03:06:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Sparsity</b> enables estimation of both subcortical and cortical activity ...", "url": "https://www.pnas.org/content/114/48/E10465", "isFamilyFriendly": true, "displayUrl": "https://<b>www.pnas.org</b>/content/114/48/E10465", "snippet": "<b>sparsity</b>; Deep <b>brain</b> structures play important roles in <b>brain</b> function. ... Therefore, in principle, the problem of distinguishing subcortical sources from sparse cortical sources <b>is similar</b> in difficulty to that of distinguishing sparse cortical sources from one another. We also illustrate typical subcortical and cortical field patterns (Materials and Methods) for source current configurations corresponding to the various angles in this distribution (Fig. 3 C and D). Subcortical and ...", "dateLastCrawled": "2022-01-16T06:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Sparsity Enables 50x Performance Acceleration</b> in Deep Learning Networks", "url": "https://numenta.com/assets/pdf/research-publications/papers/Sparsity-Enables-50x-Performance-Acceleration-Deep-Learning-Networks.pdf", "isFamilyFriendly": true, "displayUrl": "https://numenta.com/assets/pdf/research-publications/papers/<b>Sparsity</b>-Enables-50x...", "snippet": "differently than the <b>human</b> <b>brain</b>. Unlike deep learning networks, the <b>brain</b> is highly efficient, requiring a mere 20 Watts to operate, less power than a lightbulb. At Numenta, we believe that by studying the <b>brain</b> and understanding what makes it so efficient, we can create new algorithms that approach the efficiency of the <b>brain</b>. How is the <b>brain</b> so efficient? There are many reasons, but at its foundation is the notion of <b>sparsity</b>. The <b>brain</b> stores and processes information as sparse ...", "dateLastCrawled": "2021-12-07T10:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Scaling Neural Networks Through <b>Sparsity</b> - Louis Kirsch", "url": "http://louiskirsch.com/assets/publications/scale-through-sparsity.pdf", "isFamilyFriendly": true, "displayUrl": "louiskirsch.com/assets/publications/scale-through-<b>sparsity</b>.pdf", "snippet": "<b>sparsity</b>: Unconditional <b>sparsity</b> is independent of the input, while conditional <b>sparsity</b> describes <b>sparsity</b> that occurs for a speci c input. The neuroscienti c argument. While the <b>human</b> <b>brain</b> has around 150 trillion synapses [16] our largest neural networks only achieve a few billion parameters at most [1]. Furthermore,", "dateLastCrawled": "2021-11-12T20:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Frontiers | A Practical Guide to Sparse k-Means Clustering for Studying ...", "url": "https://www.frontiersin.org/articles/10.3389/fnins.2021.668293/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/fnins.2021.668293", "snippet": "In this guide, we explored the application of <b>sparsity</b>-based clustering and showed that one algorithm, RSKC, is a good fit for revealing the subtle and gradual changes of <b>human</b> <b>brain</b> development that occurs from birth to aging. In the next decade, the amount of data collected from each postmortem <b>brain</b> sample will only continue to grow as single-cell RNA sequencing methods are applied to studying <b>human</b> <b>brain</b> development. Furthermore, the push to integrate multimodal measurements, from ...", "dateLastCrawled": "2022-01-30T04:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Group <b>Sparsity</b> Constrained Automatic <b>Brain</b> Label Propagation", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4197995/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC4197995", "snippet": "In this paper, we present a group <b>sparsity</b> constrained patch based label propagation method for multi-atlas automatic <b>brain</b> labeling. The proposed method formulates the label propagation process as a graph-based theoretical framework, where each voxel in the input image is linked to each candidate voxel in each atlas image by an edge in the graph. The weight of the edge is estimated based on a sparse representation framework to identify a limited number of candidate voxles whose local image ...", "dateLastCrawled": "2017-01-30T05:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Sparse Arti\ufb01cial Neural Networks: Adaptive Performance-based ...", "url": "https://essay.utwente.nl/80559/1/Lapshyna_BA_EEMCS.pdf", "isFamilyFriendly": true, "displayUrl": "https://essay.utwente.nl/80559/1/Lapshyna_BA_EEMCS.pdf", "snippet": "done in this \ufb01eld is based on the prede\ufb01ned <b>sparsity</b> [18, 17]. Contrary, biological neural networks evolve <b>sparsity</b> over time; for instance, up to 40% of neuron synapses in the <b>human</b> <b>brain</b> are replaced with new ones every day [12]. Therefore, to make Arti\ufb01cial Neural Networks more <b>similar</b> to the biological ones,", "dateLastCrawled": "2022-01-16T08:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A Practical Guide to Sparse k-Means Clustering for Studying Molecular ...", "url": "https://europepmc.org/article/PMC/PMC8636820", "isFamilyFriendly": true, "displayUrl": "https://europepmc.org/article/PMC/PMC8636820", "snippet": "In this guide, we explored the application of <b>sparsity</b>-based clustering and showed that one algorithm, RSKC, is a good fit for revealing the subtle and gradual changes of <b>human</b> <b>brain</b> development that occurs from birth to aging. In the next decade, the amount of data collected from each postmortem <b>brain</b> sample will only continue to grow as single-cell RNA sequencing methods are applied to studying <b>human</b> <b>brain</b> development. Furthermore, the push to integrate multimodal measurements, from ...", "dateLastCrawled": "2021-12-16T19:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "MultiLink Analysis: <b>Brain</b> Network Comparison via Sparse Connectivity ...", "url": "https://www.nature.com/articles/s41598-018-37300-4", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41598-018-37300-4", "snippet": "The analysis of the <b>brain</b> from a connectivity perspective is revealing novel insights into <b>brain</b> structure and function. Discovery is, however, hindered by the lack of prior knowledge used to make ...", "dateLastCrawled": "2022-01-10T23:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Different topological organization of human brain functional</b> networks ...", "url": "https://www.sciencedirect.com/science/article/pii/S1053811914000056", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1053811914000056", "snippet": "To ensure that the <b>brain</b> networks under EO and EC had the same number of edges, each correlation matrix was set to different thresholds over a specific range of <b>sparsity</b> (see the Results section), where prominent small-world properties in <b>brain</b> networks were observed (Watts and Strogatz, 1998b). For each given <b>sparsity</b>, we obtained an undirected binarized network, in which nodes represented <b>brain</b> regions and edges represented links between <b>brain</b> regions. Graph theory was then applied to ...", "dateLastCrawled": "2021-11-01T12:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Extreme <b>sparsity</b> gives rise to functional specialization", "url": "https://neural-reckoning.github.io/preprints/sparsity-specialization/", "isFamilyFriendly": true, "displayUrl": "https://neural-reckoning.github.io/preprints/<b>sparsity</b>-specialization", "snippet": "Ensuring <b>sparsity</b> of the weight matrix comes from ensuring that only the desired number of values of \\(\\mathbf{\\phi}\\) are positive at all times. ... Our method <b>is similar</b> to this latter approach but uses a different algorithm (Ramanujan et al. 2020) to train and discover the masks, and does not exploit the resulting masks in the same manner. The masks are trained as follows: each weight is fixed but is attributed a differentiable score, with only the top \\(q\\%\\) scoring weights being used ...", "dateLastCrawled": "2022-01-04T20:06:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Neocortical sparsity, active dendrites, and</b> their relevance in robust ...", "url": "https://chaseking.me/assets/papers/cse599b_sparsity.pdf", "isFamilyFriendly": true, "displayUrl": "https://chaseking.me/assets/papers/cse599b_<b>sparsity</b>.pdf", "snippet": "billion neurons (compared to the roughly 86 billion in the entire <b>human</b> <b>brain</b>) (Herculano-Houzel,2009). Exact estimates of number of neurons vary, but the neocortex is widely regarded as the center for higher-order cognition and intelligence. 1Numentais a neuroscience research institute in Redwood City, CA, specializing in theoretical neuroscience and interested in applying neuroscience principles to machine intelligence. 1. A neuron is the basic functional unit of the <b>brain</b>. A typical ...", "dateLastCrawled": "2021-06-05T11:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Understanding complexity in the <b>human</b> <b>brain</b>", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3170818/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC3170818", "snippet": "Perhaps most simply, emergence \u2013 of consciousness or otherwise \u2013 in the <b>human</b> <b>brain</b> <b>can</b> <b>be thought</b> of as characterizing the interaction between two broad levels: the mind and the physical <b>brain</b>. To visualize this dichotomy, imagine that you are walking with Leibniz through a mill . Consider that you <b>can</b> blow the mill up in size such that all components are magnified and you <b>can</b> walk among them. All that you find are mechanical components that push against each other but there is little ...", "dateLastCrawled": "2022-02-02T23:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Changes in <b>Brain</b> Structure, Function, and Network Properties in ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8668948/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8668948", "snippet": "As 0.05 \u2264 <b>Sparsity</b> \u2264 0.5, both the AN-FES group and healthy control group had higher \u03b3 value (all larger than 1) and approximately equal \u03bb value. The \u03c3 &gt; 1 of the two groups indicated that the two groups exhibited no difference in the small-world architecture of the functional <b>brain</b> network P &gt; 0.05). Compared with healthy controls, the C p, E glob, and E loc of AN-FES were lower, and the L p was higher, with statistically significant differences (Supplementary Table 2, P &lt; 0.05 ...", "dateLastCrawled": "2022-01-05T01:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Why is sparsity (usually) considered to be</b> a good thing? - Quora", "url": "https://www.quora.com/Why-is-sparsity-usually-considered-to-be-a-good-thing", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>Why-is-sparsity-usually-considered-to-be</b>-a-good-thing", "snippet": "Answer: Apart from issues of memory and storage space, which seem not too important to me (although others may disagree), <b>sparsity</b> is a desirable property of a model because it indicates that they are good! Consider what <b>sparsity</b> means: you have, say, a class of objects (maybe vectors, maybe mat...", "dateLastCrawled": "2022-01-21T03:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Extreme <b>sparsity</b> gives rise to functional specialization", "url": "https://neural-reckoning.github.io/preprints/sparsity-specialization/", "isFamilyFriendly": true, "displayUrl": "https://neural-reckoning.github.io/preprints/<b>sparsity</b>-specialization", "snippet": "Modularity of neural networks \u2013 both biological and artificial \u2013 <b>can</b> <b>be thought</b> of either structurally or functionally, and the relationship between these is an open question. We show that enforcing structural modularity via sparse connectivity between two dense sub-networks which need to communicate to solve the task leads to functional specialization of the sub-networks, but only at extreme levels of <b>sparsity</b>. With even a moderate number of interconnections, the sub-networks become ...", "dateLastCrawled": "2022-01-04T20:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Hierarchical Temporal Memory of the 1000 Brains Theory \u2013 Tim Wheeler", "url": "http://tim.hibal.org/blog/hierarchical-temporal-memory-of-the-1000-brains-theory/", "isFamilyFriendly": true, "displayUrl": "tim.hibal.org/blog/hierarchical-temporal-memory-of-the-1000-<b>brains</b>-theory", "snippet": "I recently became interested in the Thousand <b>Brain</b> Theory of Intelligence, a theory proposed by Jeff Hawkins and the Numenta team about how the <b>human</b> <b>brain</b> works. In short, the theory postulates that the <b>human</b> neocortex is made up of on the order of a million cortical columns, each of which is a sort of neural network with an identical architecture. These cortical columns learn to represent objects / concepts and track their respective locations and scales by taking in representations of ...", "dateLastCrawled": "2022-01-25T21:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "How <b>can</b> AI revolutionise our understanding of the <b>brain</b>?", "url": "https://vanda.substack.com/", "isFamilyFriendly": true, "displayUrl": "https://vanda.substack.com", "snippet": "Breakthroughs in RL-based systems and those in understanding <b>human</b> intelligence may be co-adapted to mutually advance both fields, for example, in revolutionising how we interrogate and advance current models of value computation and the use of predictive representations in the <b>brain</b>. Conversely, deeper neuroscientific understanding advances AI: the <b>brain</b> uses sparse representations at higher processing levels to encode information efficiently. Translating this principle of <b>sparsity</b> to deep ...", "dateLastCrawled": "2022-01-31T20:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A <b>review of sparsity-based clustering methods</b> - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S0165168418300574", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0165168418300574", "snippet": "Sparse coding <b>can</b> <b>be thought</b> of as a method of information localization. In this sense, sparse representations and the clustering problems are usually complementary, as clustering itself includes a form of information localization. However, note that the <b>sparsity</b> constraint alone does not imply clustering. For example, random <b>sparsity</b> is an expression of information being localized to a certain extent, but clustering would not be evident at all for such a case. Thus, <b>sparsity</b> property needs ...", "dateLastCrawled": "2021-12-20T10:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 8, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Sparse distributed memory</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Sparse_distributed_memory", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Sparse_distributed_memory</b>", "snippet": "<b>Sparse distributed memory</b> (SDM) is a mathematical model of <b>human</b> long-term memory introduced by Pentti Kanerva in 1988 while he was at NASA Ames Research Center.It is a generalized random-access memory (RAM) for long (e.g., 1,000 bit) binary words. These words serve as both addresses to and data for the memory. The main attribute of the memory is sensitivity to similarity, meaning that a word <b>can</b> be read back not only by giving the original write address but also by giving one close to it ...", "dateLastCrawled": "2021-12-25T10:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "A Theory of Consciousness. This article explains a theory of\u2026 | by ...", "url": "https://medium.com/@tylerneylon/a-theory-of-consciousness-d5866b2f8fce", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@tylerneylon/a-theory-of-consciousness-d5866b2f8fce", "snippet": "We <b>can</b> model the <b>brain</b> as executing a cycle that repeatedly chooses a goal, and then acts out a protocol toward that goal.\u00b2 One part of the <b>brain</b> is responsible for choosing goals, while another ...", "dateLastCrawled": "2022-01-22T02:18:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Group <b>Sparsity</b> Constrained Automatic <b>Brain</b> Label Propagation", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4197995/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC4197995", "snippet": "It <b>can</b> be observed from Figure 3 that by enforcing the group <b>sparsity</b> constraint, the segmentation accuracy (i.e, with 74.7% overall Dice ratio) <b>can</b> be improved <b>compared</b> with using only the global <b>sparsity</b> constraint only (i.e., with 73.3% overall Dice ratio), which reflects the importance of incorporating the representation power of groups of input variables as stated in . Mover, by extending the group-sparse-patch based label propagation framework to the nonlinear kernel space, the ...", "dateLastCrawled": "2017-01-30T05:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Neocortical sparsity, active dendrites, and</b> their relevance in robust ...", "url": "https://chaseking.me/assets/papers/cse599b_sparsity.pdf", "isFamilyFriendly": true, "displayUrl": "https://chaseking.me/assets/papers/cse599b_<b>sparsity</b>.pdf", "snippet": "billion neurons (<b>compared</b> to the roughly 86 billion in the entire <b>human</b> <b>brain</b>) (Herculano-Houzel,2009). Exact estimates of number of neurons vary, but the neocortex is widely regarded as the center for higher-order cognition and intelligence. 1Numentais a neuroscience research institute in Redwood City, CA, specializing in theoretical neuroscience and interested in applying neuroscience principles to machine intelligence. 1. A neuron is the basic functional unit of the <b>brain</b>. A typical ...", "dateLastCrawled": "2021-06-05T11:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A Practical Guide to Sparse k-Means Clustering for Studying Molecular ...", "url": "https://pubmed.ncbi.nlm.nih.gov/34867140/", "isFamilyFriendly": true, "displayUrl": "https://pubmed.ncbi.nlm.nih.gov/34867140", "snippet": "Studying the molecular development of the <b>human</b> <b>brain</b> presents unique challenges for selecting a data analysis approach. The rare and valuable nature of <b>human</b> postmortem <b>brain</b> tissue, especially for developmental studies, means the sample sizes are small (n), but the use of high throughput genomic and proteomic methods measure the expression levels for hundreds or thousands of variables [e.g., genes or proteins (p)] for each sample.This leads to a data structure that is high dimensional (p ...", "dateLastCrawled": "2021-12-15T11:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "MultiLink Analysis: <b>Brain</b> Network Comparison via Sparse Connectivity ...", "url": "https://www.nature.com/articles/s41598-018-37300-4", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41598-018-37300-4", "snippet": "Nevertheless, recent studies have shown that a <b>sparsity</b>-based approach <b>can</b> be particularly useful in graph/connectome analysis, as they <b>can</b> highlight significant connections when prior knowledge ...", "dateLastCrawled": "2022-01-10T23:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Research on Sparsity of Output Synapses in Echo State Networks</b> - <b>Hindawi</b>", "url": "https://www.hindawi.com/journals/mpe/2018/1984524/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.hindawi.com</b>/journals/mpe/2018/1984524", "snippet": "<b>Compared</b> with the computer, <b>human</b> <b>brain</b> has higher energy efficiency with powerful capacity [18 ... From Figure 2(b), we <b>can</b> see that with the increment of <b>sparsity</b>, when , the curve of energy efficiency shows a significant rising trend, and <b>can</b> reach its maximum. When , the curve of energy efficiency shows a gentle trend and tends to oscillate locally. When , the curve of energy consumption shows a significant rising trend. When , the curve of shows a relatively gentle trend. The energy ...", "dateLastCrawled": "2022-01-19T04:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Task vs. rest-different network configurations between the coactivation ...", "url": "https://europepmc.org/article/MED/24062654", "isFamilyFriendly": true, "displayUrl": "https://europepmc.org/article/MED/24062654", "snippet": "The <b>human</b> <b>brain</b> exhibits organized spontaneous fluctuations in the resting-state (Biswal et al., ... The <b>sparsity</b> range was set between 6 and 40% with an increment of 1%. This range was used because typical <b>sparsity</b> of <b>human</b> neuron network is between this range, and the large scale <b>brain</b> networks revealed small world properties within this range (Achard and Bullmore, 2007; He et al., 2008). After thresholding, all the networks were binary (unweighted) undirected networks. We first <b>compared</b> ...", "dateLastCrawled": "2022-02-03T05:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Subject-adaptive Integration of Multiple SICE <b>Brain</b> Networks with ...", "url": "https://www.sciencedirect.com/science/article/pii/S0031320316303004", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0031320316303004", "snippet": "A set of SICE networks with various <b>sparsity</b> <b>can</b> be obtained for a single subject ... <b>sparsity</b> levels of <b>brain</b> networks are integrated. The tick \u2018 [1, n] \u2019 on the x-axis means that the n densest levels of <b>brain</b> networks are integrated. <b>Compared</b> with SRC (with SK) using a single <b>sparsity</b> level, SASNI <b>can</b> consistently boost the classification performance for all integration settings. When the top four dense levels are integrated, SASNI achieves an accuracy more than 71%. When the top seven ...", "dateLastCrawled": "2021-12-22T01:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Parallel <b>Brain</b> Network Analysis Platform", "url": "https://on-demand.gputechconf.com/gtc/2014/poster/pdf/P4128_brain_connectome_graph_GPU.pdf", "isFamilyFriendly": true, "displayUrl": "https://on-demand.gputechconf.com/gtc/2014/poster/pdf/P4128_<b>brain</b>_connectome_graph_GPU.pdf", "snippet": "depending on the <b>sparsity</b> of the network. ... \u2022 <b>Can</b> do fast mapping of high-resolution <b>human</b> <b>brain</b> connectome \u2022 Faster than other widely used tools such as FSL and SPM \u2022 Parallel calculation of betweenness is available Scalable to a larger network size \u2022 Blocked design for calculation of the correlation matrix and APSP which need to store a whole or half matrix. \u2022 Use CSR format which require little memory to store sparse networks on GPU for other applications. Low cost \u2022 Only ...", "dateLastCrawled": "2021-09-02T03:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Reproducibility of structural brain connectivity</b> and network metrics ...", "url": "https://www.nature.com/articles/s41598-018-29943-0", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41598-018-29943-0", "snippet": "The presence of 1% difference in the <b>sparsity</b> <b>can</b> cause additional within-subject variations on network metrics. In conclusion, applying connectivity thresholds on structural network to exclude ...", "dateLastCrawled": "2022-01-22T16:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Group Sparsity Constrained Automatic Brain Label Propagation</b> | SpringerLink", "url": "https://link.springer.com/chapter/10.1007%2F978-3-642-35428-1_6", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/chapter/10.1007/978-3-642-35428-1_6", "snippet": "The proposed method was evaluated on the NA0-NIREP database for automatic <b>human</b> <b>brain</b> anatomical labeling. It was also <b>compared</b> with several state-of-the-art multi-atlas based <b>brain</b> labeling algorithms. Experimental results demonstrate that our method consistently achieves the highest segmentation accuracy among all methods used for comparison. Keywords Reproduce Kernel Hilbert Space Label Propagation High Dimensional Feature Space <b>Sparsity</b> Constraint Anatomical Label These keywords were ...", "dateLastCrawled": "2022-02-03T15:39:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "<b>Sparsity</b> is an essential feature of many contemporary data problems. Remote sensing, various forms of automated screening and other high throughput measurement devices collect a large amount of ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Regularization \u2014 Understanding L1 and L2 regularization for Deep <b>Learning</b>", "url": "https://medium.com/analytics-vidhya/regularization-understanding-l1-and-l2-regularization-for-deep-learning-a7b9e4a409bf", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/regularization-understanding-l1-and-l2...", "snippet": "The <b>sparsity</b> feature used in L1 regularization has been used extensively as a feature selection mechanism in <b>machine</b> <b>learning</b>. Feature selection is a mechanism which inherently simplifies a ...", "dateLastCrawled": "2022-02-01T00:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "An E\ufb03cient Sparse Metric <b>Learning</b> in High ... - <b>Machine</b> <b>Learning</b>", "url": "http://machinelearning.org/archive/icml2009/papers/46.pdf", "isFamilyFriendly": true, "displayUrl": "<b>machinelearning</b>.org/archive/icml2009/papers/46.pdf", "snippet": "This <b>sparsity</b> prior of <b>learning</b> distance metric serves to regularize the com-plexity of the distance model especially in the \u201cless example number p and high dimension d\u201d setting. Theoretically, by <b>analogy</b> to the covariance estimation problem, we \ufb01nd the proposed distance <b>learning</b> algorithm has a consistent result at rate O!&quot;# m2 logd $% n &amp; to the target distance matrix with at most m nonzeros per row. Moreover, from the imple-mentation perspective, this! 1-penalized log-determinant ...", "dateLastCrawled": "2021-11-19T11:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Dynamical <b>machine</b> <b>learning</b> volumetric reconstruction of objects ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8027224/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8027224", "snippet": "The sequence index in the angle of illumination plays the role of discrete time in the dynamical system <b>analogy</b>. Thus, the imaging problem turns into a problem of nonlinear system identification, which also suggests dynamical <b>learning</b> as a better fit to regularize the reconstructions. We devised a Recurrent Neural Network (RNN) architecture with a novel Separable-Convolution Gated Recurrent Unit (SC-GRU) as the fundamental building block. Through a comprehensive comparison of several ...", "dateLastCrawled": "2022-01-08T19:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Dynamical <b>machine</b> <b>learning</b> volumetric reconstruction of objects ...", "url": "https://thetalkingmachines.com/sites/default/files/2021-04/s41377-021-00512-x.pdf", "isFamilyFriendly": true, "displayUrl": "https://thetalking<b>machines</b>.com/sites/default/files/2021-04/s41377-021-00512-x.pdf", "snippet": "The <b>analogy</b> of the BPM computa-tional structure with a neural network was exploited, in conjunction with gradient descent optimization, to obtain the 3D refractive index as the\u201cweights\u201d of the analogous neural network in the <b>learning</b> tomography approach41\u201343. BPM has also been used with more tra-ditional <b>sparsity</b>-based inverse methods33,44. Later, a <b>machine</b> <b>learning</b> approach with a Convolutional Neural Network (CNN) replacing the iterative gradient descent algorithm exhibited even ...", "dateLastCrawled": "2021-10-11T18:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Discovering governing equations from data</b> by sparse identification of ...", "url": "https://www.pnas.org/content/pnas/113/15/3932.full.pdf", "isFamilyFriendly": true, "displayUrl": "https://<b>www.pnas.org</b>/content/<b>pnas</b>/113/15/3932.full.pdf", "snippet": "examples. In this work, we combin e <b>sparsity</b>-promoting techniques and <b>machine</b> <b>learning</b> with nonlinear dynamical systems to discover governing equations from noisy measurement data. The only as-sumption about the structureof the model is that there are onlya few important terms that govern the dy namics, so that the equations are sparse in the space of possible functions; this assumption holds for many physical systems in an appropriate basis. In particular, we use sparse regression to ...", "dateLastCrawled": "2022-01-20T20:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Dynamical machine learning volumetric reconstruction of</b> objects ...", "url": "https://www.nature.com/articles/s41377-021-00512-x", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41377-021-00512-x", "snippet": "Recently, thanks to a ground-breaking observation from 2010 that <b>sparsity</b> can be learnt by a deep neural network 48, the idea of using <b>machine</b> <b>learning</b> to approximate solutions to inverse problems ...", "dateLastCrawled": "2022-02-02T23:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Curse of Dimensionality in <b>Machine</b> <b>Learning</b> | by Ritesh Patil | Medium", "url": "https://medium.com/@patil.ritesh311/curse-of-dimensionality-in-machine-learning-c5a226b6f266", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@patil.ritesh311/curse-of-dimensionality-in-<b>machine</b>-<b>learning</b>-c5a226...", "snippet": "Curse of Dimensionality in <b>Machine</b> <b>Learning</b>. Ritesh Patil . Oct 8 \u00b7 5 min read. Hello all, this is my first attempt at writing a technical blog and please excuse me if you find it a little vague ...", "dateLastCrawled": "2021-12-24T17:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "regression - Why L1 norm for sparse models - Cross Validated", "url": "https://stats.stackexchange.com/questions/45643/why-l1-norm-for-sparse-models", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/45643", "snippet": "There are many norms that lead to <b>sparsity</b> (e.g., as you mentioned, any Lp norm with p &lt;= 1). In general, any norm with a sharp corner at zero induces <b>sparsity</b>. So, going back to the original question - the L1 norm induces <b>sparsity</b> by having a discontinuous gradient at zero (and any other penalty with this property will do so too). $\\endgroup$", "dateLastCrawled": "2022-01-26T23:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>What Are Word Embeddings</b> for Text? - <b>Machine</b> <b>Learning</b> Mastery", "url": "https://machinelearningmastery.com/what-are-word-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>what-are-word-embeddings</b>", "snippet": "The result is a <b>learning</b> model that may result in generally better word embeddings. GloVe, is a new global log-bilinear regression model for the unsupervised <b>learning</b> of word representations that outperforms other models on word <b>analogy</b>, word similarity, and named entity recognition tasks. \u2014 GloVe: Global Vectors for Word Representation, 2014.", "dateLastCrawled": "2022-01-30T18:50:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Learning Neural Representations for Network Anomaly Detection</b>", "url": "https://www.researchgate.net/publication/325797465_Learning_Neural_Representations_for_Network_Anomaly_Detection", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/325797465_<b>Learning</b>_Neural_Representations_for...", "snippet": "Many <b>machine</b> <b>learning</b> algorithms have been. Manuscript received December 22, 2017; revised March 13, 2018. This. work is funded by Vietnam International Education De velopment (VIED) and. by ...", "dateLastCrawled": "2021-12-06T22:55:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Self-representation based dual-graph regularized <b>feature selection</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0925231215010759", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0925231215010759", "snippet": "<b>Just as sparsity</b> leads to sparse representation, self-similarity results in self-representation ... Her current research interests include pattern recognition and <b>machine</b> <b>learning</b>. Licheng Jiao (SM\u05f389) received the B.S. degree from Shanghai Jiaotong University, Shanghai, China, in 1982, the M.S. and Ph.D. degrees from Xi\u05f3an Jiaotong University, Xi\u05f3an, China, in 1984 and 1990, respectively. From 1990 to 1991, he was a postdoctoral Fellow in the National Key Laboratory for Radar Signal ...", "dateLastCrawled": "2021-11-22T12:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Self-representation based dual-graph regularized feature selection ...", "url": "https://web.xidian.edu.cn/rhshang/files/20160516_172953.pdf", "isFamilyFriendly": true, "displayUrl": "https://web.xidian.edu.cn/rhshang/files/20160516_172953.pdf", "snippet": "<b>machine</b> <b>learning</b> and computer vision \ufb01elds [41]. <b>Just as sparsity</b> leads to sparse representation, self-similarity results in self-representation [41]. Taking into account of manifold <b>learning</b> and feature selection, and inspired by the self-representation property and the idea of dual-regularization <b>learning</b> [44,45], we propose a novel feature selection algorithm for clustering, named self-representation based dual-graph regularized feature selection clustering (DFSC). This algorithm ...", "dateLastCrawled": "2022-02-02T03:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Unsupervised feature selection</b> by <b>regularized self-representation</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0031320314002970", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0031320314002970", "snippet": "<b>Just as sparsity</b> leads to sparse representation, self-similarity results in self-representation. With the above considerations, in this paper we propose a simple yet very effective <b>unsupervised feature selection</b> method by exploiting the self-representation ability of features. The feature matrix is represented over itself to find the representative feature components. The representation residual is minimized by L 2, 1-norm loss to reduce the effect of outlier samples. Different from the ...", "dateLastCrawled": "2022-01-24T12:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Talk <b>Archive</b> - Research on Algorithms and Incentives in Networks", "url": "https://rain.stanford.edu/schedule/archive.shtml", "isFamilyFriendly": true, "displayUrl": "https://rain.stanford.edu/schedule/<b>archive</b>.shtml", "snippet": "McFowland\u2019s research interests\u2014which lie at the intersection of Information Systems, <b>Machine</b> <b>Learning</b>, and Public Policy\u2014include the development of computationally efficient algorithms for large-scale statistical <b>machine</b> <b>learning</b> and \u201cbig data\u201d analytics. More specifically, his research seeks to demonstrate that many real-world problems faced by organizations, and society more broadly, can be reduced to the tasks of anomalous pattern detection and discovery. As a data and ...", "dateLastCrawled": "2022-01-20T11:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Talks - <b>sites.google.com</b>", "url": "https://sites.google.com/view/dssseminarseries/talks", "isFamilyFriendly": true, "displayUrl": "https://<b>sites.google.com</b>/view/dssseminarseries/talks", "snippet": "Abstracts &amp; Bios for upcoming talks", "dateLastCrawled": "2022-01-27T14:42:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Sparse representations for text categorization</b>", "url": "https://www.researchgate.net/publication/221479613_Sparse_representations_for_text_categorization", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221479613_Sparse_representations_for_text...", "snippet": "<b>Machine</b> <b>learning</b> for text classification is the cornerstone of document categorization, news filtering, document routing, and personalization. In text domains, effective feature selection is ...", "dateLastCrawled": "2021-12-10T18:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Continual Learning via Neural Pruning</b> | DeepAI", "url": "https://deepai.org/publication/continual-learning-via-neural-pruning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>continual-learning-via-neural-pruning</b>", "snippet": "Continual <b>learning</b>, the ability of models to learn to solve new tasks beyond what has previously been trained, has garnered much attention from the <b>machine</b> <b>learning</b> community in recent years. This is driven in part by the practical advantages promised by continual <b>learning</b> schemes such as improved performance on subsequent tasks as well as a more efficient use of resources in machines with memory constraints.", "dateLastCrawled": "2021-12-30T15:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Non-negative data-<b>driven mapping of structural connections</b> with ...", "url": "https://www.sciencedirect.com/science/article/pii/S105381192030759X", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S105381192030759X", "snippet": "For ICA, <b>sparsity can be thought of as</b> a proxy for independence. 3.5. In-vivo data decompositions. For real data, we decomposed group-average tractography matrices, using independent component analysis (ICA) and non-negative matrix factorisation (NMF), with a range of model orders K. ICA was initialised with regular PCA, in which the first 500 components were retained (explaining 97% of the total variance). ICA was applied to the reduced dataset using the FastICA algorithm (Hyv\u00e4rinen and ...", "dateLastCrawled": "2021-10-11T05:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) <b>Sparse Representations for Text Categorization</b> | Dimitri Kanevsky ...", "url": "https://www.academia.edu/2738730/Sparse_Representations_for_Text_Categorization", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/2738730/<b>Sparse_Representations_for_Text_Categorization</b>", "snippet": "A Comparative Study of <b>Machine</b> <b>Learning</b> Methods for Verbal Autopsy Text Classification. By Eric S Atwell and Samuel Danso. CSC435 book proposal. By Russell Frith. Higher-Order Smoothing: A Novel Semantic Smoothing Method for Text Classification. By Murat C Ganiz, Mitat Poyraz, and Zeynep Kilimci. INFORMATION RETRIEVAL. By febi k. Introduction to information retrieval. By Valeria Mesi. Download pdf. \u00d7 Close Log In. Log In with Facebook Log In with Google. Sign Up with Apple. or. Email ...", "dateLastCrawled": "2021-10-13T23:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Continual <b>Learning</b> via Neural Pruning \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/1903.04476/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1903.04476", "snippet": "We introduce Continual <b>Learning</b> via Neural Pruning (CLNP), a new method aimed at lifelong <b>learning</b> in fixed capacity models based on neuronal model sparsification. In this method, subsequent tasks are trained using the inactive neurons and filters of the sparsified network and cause zero deterioration to the performance of previous tasks. In order to deal with the possible compromise between model sparsity and performance, we formalize and incorporate the concept of graceful forgetting: the ...", "dateLastCrawled": "2021-11-07T10:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Continual <b>Learning</b> via Neural Pruning", "url": "https://openreview.net/pdf?id=Hyl_XXYLIB", "isFamilyFriendly": true, "displayUrl": "https://openreview.net/pdf?id=Hyl_XXYLIB", "snippet": "Continual <b>learning</b>, the ability of models to learn to solve new tasks beyond what has previously been trained, has garnered much at-tention from the <b>machine</b> <b>learning</b> community in recent years. The main obstacle for effective continual <b>learning</b> is the problem of cata-strophic forgetting: machines trained on new problems forget about", "dateLastCrawled": "2022-01-05T02:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Abstract - arXiv.org e-Print archive", "url": "https://arxiv.org/pdf/1903.04476", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/pdf/1903.04476", "snippet": "Continual <b>learning</b>, the ability of models to learn to solve new tasks beyond what has previously been trained, has garnered much attention from the <b>machine</b> <b>learning</b> community in recent years. This is driven in part by the practical advantages promised by continual <b>learning</b> schemes such as improved performance on subsequent tasks as well as a more ef\ufb01cient use of resources in machines with memory constraints. There is also great interest in continual <b>learning</b> from a more long term ...", "dateLastCrawled": "2021-10-25T14:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) <b>Introduction to compressed sensing</b>", "url": "https://www.researchgate.net/publication/220043734_Introduction_to_compressed_sensing", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/220043734_<b>Introduction_to_compressed_sensing</b>", "snippet": "systems control, clustering, and <b>machine</b> <b>learning</b> [14, 15, 58, 61, 89, 193, 217, 240, 244]. Low-dimensional manifolds hav e also been prop osed as approximate mod-", "dateLastCrawled": "2022-01-14T18:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) <b>Introduction to compressed sensing</b> | Marco Duarte - Academia.edu", "url": "https://www.academia.edu/1443164/Introduction_to_compressed_sensing", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/1443164/<b>Introduction_to_compressed_sensing</b>", "snippet": "<b>Introduction to Compressed Sensing</b> For any x \u2208 \u03a3k , we can associate a k-face of C n with the support and sign pattern of x. One can show that the number of k-faces of AC n is precisely the number of index sets of size k for which signals supported on them can be recovered by (1.12) with B (y) = {z : Az = y}.", "dateLastCrawled": "2022-01-21T03:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Compressed Sensing : Theory and Applications</b> | Kutyniok, Gitta Eldar ...", "url": "https://b-ok.africa/book/2086657/84a688", "isFamilyFriendly": true, "displayUrl": "https://b-ok.africa/book/2086657/84a688", "snippet": "You can write a book review and share your experiences. Other readers will always be interested in your opinion of the books you&#39;ve read. Whether you&#39;ve loved the book or not, if you give your honest and detailed thoughts then people will find new books that are right for them.", "dateLastCrawled": "2021-12-26T07:22:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(sparsity)  is like +(human brain)", "+(sparsity) is similar to +(human brain)", "+(sparsity) can be thought of as +(human brain)", "+(sparsity) can be compared to +(human brain)", "machine learning +(sparsity AND analogy)", "machine learning +(\"sparsity is like\")", "machine learning +(\"sparsity is similar\")", "machine learning +(\"just as sparsity\")", "machine learning +(\"sparsity can be thought of as\")", "machine learning +(\"sparsity can be compared to\")"]}