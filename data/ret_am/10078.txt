{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Accuracy</b> of computer-aided image analysis in the diagnosis of ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8141318/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8141318", "snippet": "The <b>inter-rater</b> concordance was evaluated by the Kappa statistic, obtaining a strong <b>agreement</b> (K \u2265 0.81) and confirming the reproducibility and reliability of the evaluation. Then, titles were carefully read to exclude articles out of the scope of this research. Reviewers were not blinded for authorship or name of the journals. Studies in which the subject of interest could not be addressed were excluded. In phase 2, abstracts were independently analyzed by the two reviewers. At this ...", "dateLastCrawled": "2022-01-25T01:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Accuracy</b> of the SpineNav3DTM Technology to Measure the Depth of ...", "url": "https://www.scirp.org/journal/PaperInformation.aspx?PaperID=83867", "isFamilyFriendly": true, "displayUrl": "https://www.scirp.org/journal/PaperInformation.aspx?PaperID=83867", "snippet": "Background: The SpineNav3DTM technology was recently incorporated in the Accuro device for the automatic detection of spinal bone landmarks. The goal of our study was to validate the ability of the Accuro ultrasound scanner to detect the distance from skin to epidural space by comparing it to the golden standard (the standard ultrasound). The secondary end-point was the <b>inter-rater</b> <b>agreement</b> between an expert anesthesiologist and a novice trainee in determining the epidural space depth with ...", "dateLastCrawled": "2021-11-30T10:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "How to calculate Cohen&#39;s <b>kappa</b> coefficient that measures <b>inter-rater</b> ...", "url": "https://stackoverflow.com/questions/43676905/how-to-calculate-cohens-kappa-coefficient-that-measures-inter-rater-agreement", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/43676905", "snippet": "I have proceeded as usual in applying <b>machine</b> <b>learning</b> <b>algorithm</b> on my corpus, using a bag of words model. I read the Cohen&#39;s <b>Kappa</b> is a good way to measure the performance of a classifier. How do i adapt this concept to my prediction problem using sklearn ? Sklearn&#39;s documentation is not really explicit on how to proceed on this matter with a document term matrix ( if it&#39;s even the right way to do it ) sklearn.metrics.cohen_<b>kappa</b>_score(y1, y2, labels=None, weights=None) this is the example ...", "dateLastCrawled": "2022-01-10T17:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) Development <b>of a Machine</b> <b>Learning Algorithm for the Surveillance</b> ...", "url": "https://www.researchgate.net/publication/312090089_Development_of_a_Machine_Learning_Algorithm_for_the_Surveillance_of_Autism_Spectrum_Disorder", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/312090089_Development_<b>of_a_Machine</b>_<b>Learning</b>...", "snippet": "the <b>algorithm</b>-clinician <b>agreement</b> was similar to the <b>inter-rater</b> <b>agreement</b> reported by two . other groups doing similar ASD classification on the basis of health records (one reported a. kappa of ...", "dateLastCrawled": "2021-10-17T11:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Development <b>of a Machine</b> <b>Learning</b> <b>Algorithm</b> for the Surveillance of ...", "url": "https://europepmc.org/abstract/MED/28002438", "isFamilyFriendly": true, "displayUrl": "https://europepmc.org/abstract/MED/28002438", "snippet": "Because the <b>algorithm</b> is trained on the clinician-assigned ratings, it is unlikely that <b>agreement</b> between the <b>algorithm</b> and a clinician would ever exceed <b>inter-rater</b> clinician <b>agreement</b>. On the other hand, the <b>algorithm</b> will have perfect intra-rater reliability, as it will always make the same classification for a given set of evaluations. An essential question is: what level of performance\u2014if any\u2014would be considered \u201cacceptable\u201d in order to trust the <b>algorithm</b>\u2019s predictions? Of ...", "dateLastCrawled": "2021-06-22T07:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 5, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Cohen&#39;s kappa</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Cohen%27s_kappa", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Cohen&#39;s_kappa</b>", "snippet": "<b>Cohen&#39;s kappa</b> coefficient (\u03ba) is a statistic that is used to measure <b>inter-rater</b> reliability (and also intra-rater reliability) for qualitative (categorical) items. It is generally thought to be a more robust measure than simple percent <b>agreement</b> calculation, as \u03ba takes into account the possibility of the <b>agreement</b> occurring by chance. There is controversy surrounding <b>Cohen&#39;s kappa</b> due to the difficulty in interpreting indices of <b>agreement</b>.", "dateLastCrawled": "2022-02-03T03:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Cohen\u2019s <b>Kappa</b>. Understanding Cohen\u2019s <b>Kappa</b> coefficient | by Kurtis ...", "url": "https://towardsdatascience.com/cohens-kappa-9786ceceab58", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/cohens-<b>kappa</b>-9786ceceab58", "snippet": "Figure 5: Probability of <b>agreement</b>. Perfect! The next step is to work out the probability of random <b>agreement</b>. Using figure 4 as a guide, the expected value is the total number of times that Rater 1 said correct divided by the total number of instances, multiplied by the total number of times that Rater 2 said correct divided by the total number of instances, added to the total number of times that Rater 1 said incorrect multiplied by the total number of times that Rater 2 said incorrect ...", "dateLastCrawled": "2022-02-02T06:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) <b>Accuracy</b> Measures for the Comparison of Classifiers | Hocine ...", "url": "https://www.academia.edu/50524696/Accuracy_Measures_for_the_Comparison_of_Classifiers", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/50524696/<b>Accuracy</b>_Measures_for_the_Comparison_of_Classifiers", "snippet": "<b>Accuracy</b> Measures for the Comparison of Classifiers. 2011. Hocine Cherifi. Download Download PDF. Full PDF Package Download Full PDF Package. This Paper. A short summary of this paper. 37 Full PDFs related to this paper. Read Paper. Download Download PDF. Download Full PDF Package. Translate PDF. Related Papers. EVALUATION OF PERFORMANCE MEASURES FOR CLASSIFIERS COMPARISON. By Hocine Cherifi. A MODEL TO COMPARE THE DEGREE OF REFACTORING OPPORTUNITIES OF THREE PROJECTS USING A <b>MACHINE</b> ...", "dateLastCrawled": "2022-01-24T23:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A review of <b>machine learning algorithms for identification</b> and ...", "url": "https://www.sciencedirect.com/science/article/pii/S2590188519300010", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2590188519300010", "snippet": "Transductive <b>accuracy</b> measures the <b>accuracy</b> of predicting unlabelled instances in training, while inductive <b>accuracy</b> measures the <b>accuracy</b> of predicting unseen test data (testing data) (Deocadez et al., 2017). Both of these values were used to measure the <b>accuracy</b> of the SSL algorithms in one study S20 as the authors of this studies believed that there were two types of <b>learning</b> in SSL: one with testing data and the other with unlabelled data.", "dateLastCrawled": "2022-02-02T21:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Evaluation of the performance of <b>machine</b> <b>learning</b> algorithms applied to ...", "url": "https://wjarr.com/sites/default/files/WJARR-2021-0286.pdf", "isFamilyFriendly": true, "displayUrl": "https://wjarr.com/sites/default/files/WJARR-2021-0286.pdf", "snippet": "We then evaluated each <b>machine</b> <b>learning</b> <b>algorithm</b> comparing the perceptual and acoustic parameters in determining how well each <b>algorithm</b> predicts the presence of those categorized with having lesion or not by the laryngeal examination. Results: One hundred and twenty respondents were analyzed out of which 89(74.2%) were females. The mean age was . 46.5 . \u00b1. 9.2 years. The perceptual evaluation generally outperformed the acoustic evaluation. Also, the Na\u00efve Bayes Classifier (NBC ...", "dateLastCrawled": "2022-01-15T12:40:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Accuracy</b> of computer-aided image analysis in the diagnosis of ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8141318/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8141318", "snippet": "The <b>inter-rater</b> concordance was evaluated by the Kappa statistic, obtaining a strong <b>agreement</b> (K \u2265 0.81) and confirming the reproducibility and reliability of the evaluation. Then, titles were carefully read to exclude articles out of the scope of this research. Reviewers were not blinded for authorship or name of the journals. Studies in which the subject of interest could not be addressed were excluded. In phase 2, abstracts were independently analyzed by the two reviewers. At this ...", "dateLastCrawled": "2022-01-25T01:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Development <b>of a Machine</b> <b>Learning</b> <b>Algorithm</b> for the Surveillance of ...", "url": "https://europepmc.org/abstract/MED/28002438", "isFamilyFriendly": true, "displayUrl": "https://europepmc.org/abstract/MED/28002438", "snippet": "Development <b>of a Machine</b> <b>Learning</b> <b>Algorithm</b> for the Surveillance of Autism Spectrum Disorder. ... the <b>algorithm</b>-clinician <b>agreement</b> was <b>similar</b> to the <b>inter-rater</b> <b>agreement</b> reported by two other groups doing <b>similar</b> ASD classification on the basis of health records (one reported a kappa of 0.73 , and the other 88% <b>agreement</b> ). The <b>algorithm</b> was more likely to misclassify children with certain characteristics. In particular, it was less sensitive to classifying ASD among children with fewer ...", "dateLastCrawled": "2021-06-22T07:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Debugging a Crowdsourced Task with Low <b>Inter-Rater</b> <b>Agreement</b>", "url": "https://marc.najork.org/pdfs/jcdl2015.pdf", "isFamilyFriendly": true, "displayUrl": "https://marc.najork.org/pdfs/jcdl2015.pdf", "snippet": "<b>agreement</b> (Fleiss\u2019s \u03ba = 0.51) between crowd-labeled tweets and a classifier [3]. Our work ultimately strives to increase classification <b>accuracy</b> by improving label quality; the first step in this process is to productively use low <b>inter-rater</b> <b>agreement</b>. Aroyo and Welty have investigated a technique for productively using disagreement", "dateLastCrawled": "2021-11-01T16:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Types <b>of Classification Errors under Machine Learning</b>?", "url": "https://www.researchgate.net/post/Types_of_Classification_Errors_under_Machine_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/post/Types_<b>of_Classification_Errors_under_Machine_Learning</b>", "snippet": "Cohen&#39;s kappa: It gives information about <b>inter-rater</b> <b>agreement</b> between predictions of the model and the truth. The smaller kappa is less <b>agreement</b> between the truth and predictions.", "dateLastCrawled": "2022-01-29T04:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Reliability and <b>accuracy</b> of EEG interpretation for estimating age in ...", "url": "https://onlinelibrary.wiley.com/doi/full/10.1002/acn3.51132", "isFamilyFriendly": true, "displayUrl": "https://onlinelibrary.wiley.com/doi/full/10.1002/acn3.51132", "snippet": "The higher <b>agreement</b> but lower <b>accuracy</b> among aEEG-based estimates is likely related to the simplicity of the aEEG trend presenting with fewer degrees of freedom for visual interpretation. 30. The rapid developmental changes in the EEG were evident using a <b>machine</b> <b>learning</b>\u2013based <b>algorithm</b>. The FBA <b>algorithm</b> was, however, trained on the ...", "dateLastCrawled": "2022-02-01T13:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) <b>Accuracy</b> Measures for the Comparison of Classifiers | Hocine ...", "url": "https://www.academia.edu/50524696/Accuracy_Measures_for_the_Comparison_of_Classifiers", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/50524696/<b>Accuracy</b>_Measures_for_the_Comparison_of_Classifiers", "snippet": "<b>Accuracy</b> Measures for the Comparison of Classifiers. 2011. Hocine Cherifi. Download Download PDF. Full PDF Package Download Full PDF Package. This Paper. A short summary of this paper. 37 Full PDFs related to this paper. Read Paper. Download Download PDF. Download Full PDF Package. Translate PDF. Related Papers. EVALUATION OF PERFORMANCE MEASURES FOR CLASSIFIERS COMPARISON. By Hocine Cherifi. A MODEL TO COMPARE THE DEGREE OF REFACTORING OPPORTUNITIES OF THREE PROJECTS USING A <b>MACHINE</b> ...", "dateLastCrawled": "2022-01-24T23:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A review of <b>machine learning algorithms for identification</b> and ...", "url": "https://www.sciencedirect.com/science/article/pii/S2590188519300010", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2590188519300010", "snippet": "Two of the three primary studies used this <b>algorithm</b> to improve the classifier&#39;s <b>accuracy</b>; S4 optionally used requirements that received feedback from the analysts as labeled requirements, while S21, which used active <b>learning</b>, manually classified the least confident output in each iteration as a training set until an acceptable <b>accuracy</b> was achieved. However, S20 used this <b>algorithm</b> to classify NFRs in users reviews within the App Store.", "dateLastCrawled": "2022-02-02T21:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Comparative analysis of machine learning</b> algorithms for computer ...", "url": "https://www.nature.com/articles/s41598-021-85016-9", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41598-021-85016-9", "snippet": "For <b>accuracy</b>, RBL uses <b>machine</b> <b>learning</b> techniques such as perceptrons, support vector machines, and word embeddings. For negation detection, the NegEx <b>algorithm</b> was implemented in UIMA RUTA 77 , 80 .", "dateLastCrawled": "2022-02-02T12:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A comparison of <b>machine</b> <b>learning</b> models versus clinical evaluation for ...", "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0245157", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0245157", "snippet": "Nearly all <b>machine</b> <b>learning</b> models scale exceptionally well with data, and therefore substantial further improvement of diagnostic <b>accuracy</b> is likely when increasing the sample size. We also limited ourselves to sepsis patients presenting to the ED, and thus it is unknown to what degree these models translate to a broader, general ED population. Second, results presented in this study are based on retrospective data in a single center, limiting the external validity of the model ...", "dateLastCrawled": "2021-01-20T16:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Cohen&#39;s <b>kappa</b> in plain English - Cross Validated", "url": "https://stats.stackexchange.com/questions/82162/cohens-kappa-in-plain-english", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/82162", "snippet": "In essence, the <b>kappa</b> statistic is a measure of how closely the instances classified by the <b>machine</b> <b>learning</b> classifier matched the data labeled as ground truth, controlling for the <b>accuracy</b> of a random classifier as measured by the expected <b>accuracy</b>. Not only can this <b>kappa</b> statistic shed light into how the classifier itself performed, the <b>kappa</b> statistic for one model is directly comparable to the <b>kappa</b> statistic for any other model used for the same classification task.", "dateLastCrawled": "2022-01-27T22:45:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "IBM HR <b>Attrition</b> Case Study. Predicting if a particular employee is ...", "url": "https://towardsdatascience.com/using-ml-to-predict-if-an-employee-will-leave-829df149d4f8", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/using-ml-to-predict-if-an-employee-will-leave-829df149d4f8", "snippet": "The second set of information pertains to the Test set. We get a weighted <b>accuracy</b> of 1.00 with precision as 1.00 and recall as 1.00. We now look at the Cohen\u2019s Kappa Score. Cohen\u2019s kappa coefficient (\u03ba) is a statistic which measures <b>inter-rater</b> <b>agreement</b> for qualitative (categorical) items. It is generally <b>thought</b> to be a more robust ...", "dateLastCrawled": "2022-01-31T16:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Debugging a Crowdsourced Task with Low <b>Inter-Rater</b> <b>Agreement</b>", "url": "https://marc.najork.org/pdfs/jcdl2015.pdf", "isFamilyFriendly": true, "displayUrl": "https://marc.najork.org/pdfs/jcdl2015.pdf", "snippet": "<b>agreement</b> (Fleiss\u2019s \u03ba = 0.51) between crowd-labeled tweets and a classifier [3]. Our work ultimately strives to increase classification <b>accuracy</b> by improving label quality; the first step in this process is to productively use low <b>inter-rater</b> <b>agreement</b>. Aroyo and Welty have investigated a technique for productively using disagreement", "dateLastCrawled": "2021-11-01T16:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 2, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Cohen&#39;s kappa</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Cohen%27s_kappa", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Cohen&#39;s_kappa</b>", "snippet": "<b>Cohen&#39;s kappa</b> coefficient (\u03ba) is a statistic that is used to measure <b>inter-rater</b> reliability (and also intra-rater reliability) for qualitative (categorical) items. It is generally <b>thought</b> to be a more robust measure than simple percent <b>agreement</b> calculation, as \u03ba takes into account the possibility of the <b>agreement</b> occurring by chance. There is controversy surrounding <b>Cohen&#39;s kappa</b> due to the difficulty in interpreting indices of <b>agreement</b>.", "dateLastCrawled": "2022-02-03T03:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Entry 23: Scoring Classification Models - Theory</b> - Data Science Diaries", "url": "https://julielinx.github.io/blog/23_class_score_theory/", "isFamilyFriendly": true, "displayUrl": "https://julielinx.github.io/blog/23_class_score_theory", "snippet": "Cohen\u2019s Kappa measures the <b>inter-rater</b> <b>agreement</b> (<b>agreement</b> between two raters) for categorical items. It\u2019s generally <b>thought</b> to be a more robust measure than simple percent <b>agreement</b> calculation, because it takes into account the possibility of the <b>agreement</b> occurring by chance. However, it\u2019s use is controversial due to difficulty interpreting indices of <b>agreement</b>. Values range between -1 and 1 where 1 is perfect <b>agreement</b>, 0 is no <b>agreement</b>, and -1 is perfect negative <b>agreement</b>. To ...", "dateLastCrawled": "2022-01-29T03:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Cohen&#39;s kappa coefficient as a performance measure for feature ...", "url": "https://www.semanticscholar.org/paper/Cohen%27s-kappa-coefficient-as-a-performance-measure-Vieira-Kaymak/43683d869a5e232219698b439272a97de7ca228e", "isFamilyFriendly": true, "displayUrl": "https://www.semanticscholar.org/paper/Cohen&#39;s-kappa-coefficient-as-a-performance...", "snippet": "Cohen&#39;s kappa coefficient is a statistical measure of <b>inter-rater</b> <b>agreement</b> for qualitative items. It is generally <b>thought</b> to be a more robust measure than simple percent <b>agreement</b> calculation, since it takes into account the <b>agreement</b> occurring by chance. Considering that kappa is a more conservative measure, then its use in wrapper feature selection is suitable to test the performance of the models. This paper proposes the use of the kappa measure as an evaluation measure in a feature ...", "dateLastCrawled": "2022-01-10T18:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) Parameters for <b>Machine</b> <b>Learning</b>", "url": "https://www.researchgate.net/publication/333951627_Parameters_for_Machine_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/333951627_Parameters_for_<b>Machine</b>_<b>Learning</b>", "snippet": "In <b>machine</b> <b>learning</b> studies, the <b>inter-rater</b> reliability is deemed as a validity measure rather than a reliability measure, since the human consent scores are taken as a &#39;Golden Standard.&#39; In the ...", "dateLastCrawled": "2022-01-07T01:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A Proposal for Performance-based Assessment of the <b>Learning</b> of <b>Machine</b> ...", "url": "https://www.researchgate.net/publication/355087845_A_Proposal_for_Performance-based_Assessment_of_the_Learning_of_Machine_Learning_Concepts_and_Practices_in_K-12", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/355087845_A_Proposal_for_Performance-based...", "snippet": "substantial <b>inter-rater</b> <b>agreement</b> <b>can</b> be obtained using th e scale. It seems however easier It seems however easier to reco gnize very good or very poor performance, rather than an intermediate", "dateLastCrawled": "2022-01-21T19:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Central Reading of Ulcerative Colitis Clinical Trial Videos</b> Using ...", "url": "https://www.gastrojournal.org/article/S0016-5085(20)35283-5/fulltext", "isFamilyFriendly": true, "displayUrl": "https://www.gastrojournal.org/article/S0016-5085(20)35283-5/fulltext", "snippet": "The study found that a deep <b>learning</b> <b>algorithm</b> (RNN) <b>can</b> be trained to predict levels of UC severity from full-length endoscopy videos with high <b>inter-rater</b> <b>agreement</b> with human central readers. Our study is the first to evaluate the predictive performance of an ML <b>algorithm</b> in regard to disease activity scores read by human readers in the context of an industry sponsored clinical trial. Moreover, we used routinely obtained endoscopy videos rather than still images, selected for their ...", "dateLastCrawled": "2022-01-14T07:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Evaluation metrics for machine learning</b> \u2013 Deep <b>Learning</b> Garden", "url": "https://deeplearning.lipingyang.org/evaluation-metrics-for-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://deep<b>learning</b>.lipingyang.org/<b>evaluation-metrics-for-machine-learning</b>", "snippet": "This page provides an introduction to evolution metrics for <b>machine</b> <b>learning</b>. 1. Commonly used metrics (1) (Overall) <b>Accuracy</b>. The (overall) <b>accuracy</b> is computed by the ratio between the number of the correctly classified test samples and the total test samples. (2) Average/mean <b>accuracy</b>", "dateLastCrawled": "2022-01-30T14:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Using Deep <b>Learning</b> Algorithms to Grade Hydronephrosis Severity: Toward ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7000524/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7000524", "snippet": "Deep <b>learning</b>, a subset of <b>machine</b> <b>learning</b>, is a general term for an <b>algorithm</b> that trains a many layered network to learn hierarchical feature representations from raw data. Due to the hierarchical nature of deep <b>learning</b> models, complex functions <b>can</b> be learned to solve difficult classification problems that were previously unsolvable by classic <b>machine</b> <b>learning</b> algorithms ( 3 ).", "dateLastCrawled": "2022-01-28T00:56:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Reliability and <b>accuracy</b> of EEG interpretation for estimating age in ...", "url": "https://onlinelibrary.wiley.com/doi/full/10.1002/acn3.51132", "isFamilyFriendly": true, "displayUrl": "https://onlinelibrary.wiley.com/doi/full/10.1002/acn3.51132", "snippet": "<b>Inter-rater</b> <b>agreement</b> between EEG and aEEG estimates of PMA was <b>compared</b> using resampling methods (bootstrap), i.e. the distributions were calculated for the differences between ICC of aEEG and EEG estimates of PMA for 1000 resampled subsets. The 95%CI was used to determine a significant difference in ICC, that is, if the 95%CI did not span zero then the ICC was deemed to be significantly different. Systematic differences between individual EEG and aEEG reviewers were assessed using the ...", "dateLastCrawled": "2022-02-01T13:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Agreement</b> and Reliability Analysis of <b>Machine</b> <b>Learning</b> Scaling and ...", "url": "https://www.mdpi.com/2075-4426/12/1/20/htm", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/2075-4426/12/1/20/htm", "snippet": "We demonstrated that <b>machine</b>-<b>learning</b> scaling achieved substantial improvement in <b>inter-rater</b> reliability for assessing proximal weakness in clinical scores. The improved <b>agreement</b> in patient assessment between observers <b>can</b> reduce medical errors during decision-making, especially during communication in the streamline of treatment in which experts and non-experts with various roles of care are involved. In our analysis, non-expert assessment with objective measurement using sensors and ...", "dateLastCrawled": "2022-01-24T22:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A Multi-Stage Approach Combining Feature Selection with <b>Machine</b> ...", "url": "https://www.ijert.org/a-multi-stage-approach-combining-feature-selection-with-machine-learning-techniques-for-higher-prediction-reliability-and-accuracy-in-heart-disease-diagnosis", "isFamilyFriendly": true, "displayUrl": "https://www.ijert.org/a-multi-stage-approach-combining-feature-selection-with-<b>machine</b>...", "snippet": "Cohens Kappa statistic is an <b>inter-rater</b> cooperation indicator for categorical variables. This statistic determines the degree to which two tests agree with diagnostic categorization. It is commonly considered as a more rigorous indicator than a simple percent <b>accuracy</b> measurement because Kappa takes into consideration an arrangement that happens by chance [19]. Table 1 shows a comparison of performance of different <b>machine</b> <b>learning</b> on medical databases. The results do not include the ...", "dateLastCrawled": "2021-12-16T02:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Introduction to <b>machine</b> <b>learning</b>: k-nearest neighbors", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4916348/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC4916348", "snippet": "<b>Machine</b> <b>learning</b> techniques have been widely used in many scientific fields, but its use in medical literature is limited partly because of technical difficulties. k-nearest neighbors (kNN) is a simple method of <b>machine</b> <b>learning</b>. The article introduces some basic ideas underlying the kNN <b>algorithm</b>, and then focuses on how to perform kNN modeling with R. The dataset should be prepared before running the knn() function in R. After prediction of outcome with kNN <b>algorithm</b>, the diagnostic ...", "dateLastCrawled": "2022-01-20T02:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "How to calculate Cohen&#39;s <b>kappa</b> coefficient that measures <b>inter-rater</b> ...", "url": "https://stackoverflow.com/questions/43676905/how-to-calculate-cohens-kappa-coefficient-that-measures-inter-rater-agreement", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/43676905", "snippet": "I have proceeded as usual in applying <b>machine</b> <b>learning</b> <b>algorithm</b> on my corpus, using a bag of words model. I read the Cohen&#39;s <b>Kappa</b> is a good way to measure the performance of a classifier. How do i adapt this concept to my prediction problem using sklearn ? Sklearn&#39;s documentation is not really explicit on how to proceed on this matter with a document term matrix ( if it&#39;s even the right way to do it ) sklearn.metrics.cohen_<b>kappa</b>_score(y1, y2, labels=None, weights=None) this is the example ...", "dateLastCrawled": "2022-01-10T17:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Development and Validation <b>of a Machine</b> <b>Learning</b> Model Predicting ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8656573/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8656573", "snippet": "Background: Vascular access surveillance of dialysis patients is a challenging task for clinicians. We derived and validated an arteriovenous fistula failure model (AVF-FM) based on <b>machine</b> <b>learning</b>. Methods: The AVF-FM is an XG-Boost <b>algorithm</b> aimed at predicting AVF failure within three months among in-centre dialysis patients. The model was trained in the derivation set (70% of initial cohort) by exploiting the information routinely collected in the Nephrocare European Clinical Database ...", "dateLastCrawled": "2022-01-28T05:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) <b>Accuracy</b> Measures for the Comparison of Classifiers | Hocine ...", "url": "https://www.academia.edu/50524696/Accuracy_Measures_for_the_Comparison_of_Classifiers", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/50524696/<b>Accuracy</b>_Measures_for_the_Comparison_of_Classifiers", "snippet": "<b>Accuracy</b> Measures for the Comparison of Classifiers. 2011. Hocine Cherifi. Download Download PDF. Full PDF Package Download Full PDF Package. This Paper. A short summary of this paper. 37 Full PDFs related to this paper. Read Paper. Download Download PDF. Download Full PDF Package. Translate PDF. Related Papers. EVALUATION OF PERFORMANCE MEASURES FOR CLASSIFIERS COMPARISON. By Hocine Cherifi. A MODEL TO COMPARE THE DEGREE OF REFACTORING OPPORTUNITIES OF THREE PROJECTS USING A <b>MACHINE</b> ...", "dateLastCrawled": "2022-01-24T23:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A review of <b>machine learning algorithms for identification</b> and ...", "url": "https://www.sciencedirect.com/science/article/pii/S2590188519300010", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2590188519300010", "snippet": "Transductive <b>accuracy</b> measures the <b>accuracy</b> of predicting unlabelled instances in training, while inductive <b>accuracy</b> measures the <b>accuracy</b> of predicting unseen test data (testing data) (Deocadez et al., 2017). Both of these values were used to measure the <b>accuracy</b> of the SSL algorithms in one study S20 as the authors of this studies believed that there were two types of <b>learning</b> in SSL: one with testing data and the other with unlabelled data.", "dateLastCrawled": "2022-02-02T21:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Cohen&#39;s <b>kappa</b> in plain English - Cross Validated", "url": "https://stats.stackexchange.com/questions/82162/cohens-kappa-in-plain-english", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/82162", "snippet": "In essence, the <b>kappa</b> statistic is a measure of how closely the instances classified by the <b>machine</b> <b>learning</b> classifier matched the data labeled as ground truth, controlling for the <b>accuracy</b> of a random classifier as measured by the expected <b>accuracy</b>. Not only <b>can</b> this <b>kappa</b> statistic shed light into how the classifier itself performed, the <b>kappa</b> statistic for one model is directly comparable to the <b>kappa</b> statistic for any other model used for the same classification task.", "dateLastCrawled": "2022-01-27T22:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "A comparison of <b>machine</b> <b>learning</b> models versus clinical evaluation for ...", "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0245157", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0245157", "snippet": "<b>Machine</b> <b>learning</b> <b>can</b> extract information from complex, non-linear data and provide insights to support clinical decision making. Hence, the first studies emerged that report <b>machine</b> <b>learning</b>-based mortality prediction models using data from patients with sepsis presenting to the ED 15\u201326]. Unfortunately, these studies did not provide a comparison with physicians in terms of prognostic performance. Recently, a new group of <b>machine</b> <b>learning</b> algorithms termed gradient boosting trees emerged ...", "dateLastCrawled": "2021-01-20T16:52:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Understanding Interobserver <b>Agreement</b>: The Kappa Statistic", "url": "http://web2.cs.columbia.edu/~julia/courses/CS6998/Interrater_agreement.Kappa_statistic.pdf", "isFamilyFriendly": true, "displayUrl": "web2.cs.columbia.edu/~julia/courses/CS6998/<b>Interrater</b>_<b>agreement</b>.Kappa_statistic.pdf", "snippet": "call the <b>analogy</b> of a target and how close we get to the bull\u2019s-eye (Figure 1). If we actually hit the bull\u2019s-eye (representing <b>agreement</b> with the gold standard), we are accurate. If all our shots land together, we have good precision (good reliability). If all our shots land together and we hit the bull\u2019s-eye, we are accurate as well as precise. It is possible, however, to hit the bull\u2019s-eye purely by chance. Referring to Figure 1, only the center black dot in target A is accurate ...", "dateLastCrawled": "2022-01-28T23:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Leveraging Inter-rater Agreement for Audio-Visual Emotion Recognition</b>", "url": "https://www.researchgate.net/publication/283487589_Leveraging_Inter-rater_Agreement_for_Audio-Visual_Emotion_Recognition", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/283487589_Leveraging_<b>Inter-rater</b>_<b>Agreement</b>...", "snippet": "In <b>machine</b> <b>learning</b> tasks an actual \u2018ground truth\u2019 may not be available. Then, machines often have to rely on human labelling of data. This becomes challenging the more subjective the <b>learning</b> ...", "dateLastCrawled": "2021-08-28T14:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Machine</b> <b>learning</b> glossary - DataTime", "url": "https://www.dtalg.com/article-1/", "isFamilyFriendly": true, "displayUrl": "https://www.dtalg.com/article-1", "snippet": "See also Cohen\u2019s kappa, which is one of the most popular <b>inter-rater</b> <b>agreement</b> measurements. intersection over union (IoU) #image. The intersection of two sets divided by their union. In <b>machine</b>-<b>learning</b> image-detection tasks, IoU is used to measure the accuracy of the model\u2019s predicted bounding box with respect to the ground-truth bounding ...", "dateLastCrawled": "2022-01-25T17:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Multilingual <b>Twitter Sentiment Classification</b>: The Role of Human ... - PLOS", "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0155036", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0155036", "snippet": "The researchers in the fields of <b>inter-rater</b> <b>agreement</b> and <b>machine</b> <b>learning</b> typically employ different evaluation measures. We report all the results in terms of four selected measures which we deem appropriate for the three-valued sentiment classification task (the details are in the Evaluation measures subsection in Methods). In this section, however, the results are summarized only in terms of Krippendorff\u2019s Alpha-reliability Alpha) , to highlight the main conclusions. Alpha is a ...", "dateLastCrawled": "2021-03-30T23:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Clinician perspectives on <b>machine</b> <b>learning</b> prognostic algorithms in the ...", "url": "https://link.springer.com/article/10.1007/s00520-021-06774-w", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00520-021-06774-w", "snippet": "<b>Machine</b> <b>learning</b> algorithms may accurately predict mortality risk in cancer, but it is unclear how oncology clinicians would use such algorithms in practice. The purpose of this qualitative study was to assess oncology clinicians\u2019 perceptions on the utility and barriers of <b>machine</b> <b>learning</b> prognostic algorithms to prompt advance care planning. Participants included medical oncology physicians and advanced practice providers (APPs) practicing in tertiary and community practices within a ...", "dateLastCrawled": "2022-01-30T10:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Analyzing and Interpreting Data From Rating Scales</b> | by Kevin C Lee ...", "url": "https://towardsdatascience.com/analyzing-and-interpreting-data-from-rating-scales-d169d66211db", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>analyzing-and-interpreting-data-from-rating-scales</b>-d169...", "snippet": "<b>Inter-Rater</b> Reliability. In B), we plot the pairwise correlations between the students with a heatmap. Most of the correlations are &gt; 0.6 with a few exceptions. A small number of respondents showing low correlations with others is acceptable as long as most students are able to respond similarly. P.S. The use of Pearson Correlation is only ...", "dateLastCrawled": "2022-01-29T00:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Target <b>analogy</b> of accuracy and precision | Download Scientific Diagram", "url": "https://researchgate.net/figure/Target-analogy-of-accuracy-and-precision_fig1_24399044", "isFamilyFriendly": true, "displayUrl": "https://researchgate.net/figure/Target-<b>analogy</b>-of-accuracy-and-precision_fig1_24399044", "snippet": "The intraclass correlation coefficient (ICC) was calculated to assess intra-rater and <b>inter-rater</b> <b>agreement</b> of I 3M . 31 A sample of OPTs was randomly divided into training dataset (819) and test ...", "dateLastCrawled": "2021-06-28T05:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Frontiers | From What to Why, the Growing Need for a Focus Shift Toward ...", "url": "https://www.frontiersin.org/articles/10.3389/fphys.2021.821217/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/fphys.2021.821217", "snippet": "Explainable AI is far from a novel concept in the <b>machine</b> <b>learning</b> (ML) community (Goebel et al., 2018; Tosun et al., 2020a,b). While the presentation of new approaches for post-hoc explainers of deep convolutional neural networks (CNNs) is outside of the scope of this review, there are a few simple steps that can increase the interpretability and explainability of an AI-driven study ( Figure 1 ).", "dateLastCrawled": "2022-02-03T06:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Frontiers | Evaluation of Automated Hypnogram Analysis on Multi-Scored ...", "url": "https://www.frontiersin.org/articles/10.3389/fdgth.2021.707589/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/fdgth.2021.707589", "snippet": "Keywords: hypnogram analysis, sleep stage scoring, model uncertainty, <b>inter-rater</b> reliability, <b>machine</b> <b>learning</b>, polysomnography. Citation: Van der Plas D, Verbraecken J, Willemen M, Meert W and Davis J (2021) Evaluation of Automated Hypnogram Analysis on Multi-Scored Polysomnographies. Front. Digit. Health 3:707589. doi: 10.3389/fdgth.2021.707589", "dateLastCrawled": "2022-01-21T22:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Quadratic weighted kappa</b> strength of <b>agreement</b> - Cross Validated", "url": "https://stats.stackexchange.com/questions/46296/quadratic-weighted-kappa-strength-of-agreement", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/46296", "snippet": "In the case of the kappa-value there are some attempts to qualify how good or bad the agreements are. For example Landis &amp; Koch in the article The Measurement of Observer <b>Agreement</b> for Categorical Data talks about &quot;strength of <b>agreement</b>&quot; based on kappa values:. Kappa Strength of <b>agreement</b> ===== ===== 0.0-0.20 Slight 0.21-0.40 Fair 0.41-0.60 Moderate 0.61-0.80 Substantial 0.81-0.90 Almost perfect", "dateLastCrawled": "2022-01-20T17:56:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Reliability and Learnability of Human Bandit Feedback for Sequence-to ...", "url": "https://aclanthology.org/P18-1165.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/P18-1165.pdf", "snippet": "intra- and <b>inter-rater agreement is similar</b> for both tasks, with highest inter-rater reliability for stan-dardized 5-point ratings. In a next step, we address the issue of <b>machine</b> learnability of human rewards. We use deep learn- ing models to train reward estimators by regres-sion against cardinal feedback, and by \ufb01tting a Bradley-Terry model (Bradley and Terry,1952) to ordinal feedback. Learnability is understood by a slight misuse of the <b>machine</b> <b>learning</b> notion of learnability (Shalev ...", "dateLastCrawled": "2021-12-22T00:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "arXiv:1805.10627v3 [cs.CL] 13 Dec 2018", "url": "https://www.researchgate.net/profile/Joshua-Uyheng/publication/325413588_Reliability_and_Learnability_of_Human_Bandit_Feedback_for_Sequence-to-Sequence_Reinforcement_Learning/links/5ea04de5a6fdccd7cee0eebe/Reliability-and-Learnability-of-Human-Bandit-Feedback-for-Sequence-to-Sequence-Reinforcement-Learning.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/profile/Joshua-Uyheng/publication/325413588_Reliability...", "snippet": "\ufb01ed by bandit <b>learning</b> for neural <b>machine</b> trans-lation (NMT). Our aim is to show that successful <b>learning</b> from simulated bandit feedback (Sokolov et al.,2016b;Kreutzer et al.,2017;Nguyen et al ...", "dateLastCrawled": "2021-08-22T12:54:00.0000000Z", "language": "en", "isNavigational": false}], [], [], []], "all_bing_queries": ["+(inter-rater agreement)  is like +(accuracy of a machine learning algorithm)", "+(inter-rater agreement) is similar to +(accuracy of a machine learning algorithm)", "+(inter-rater agreement) can be thought of as +(accuracy of a machine learning algorithm)", "+(inter-rater agreement) can be compared to +(accuracy of a machine learning algorithm)", "machine learning +(inter-rater agreement AND analogy)", "machine learning +(\"inter-rater agreement is like\")", "machine learning +(\"inter-rater agreement is similar\")", "machine learning +(\"just as inter-rater agreement\")", "machine learning +(\"inter-rater agreement can be thought of as\")", "machine learning +(\"inter-rater agreement can be compared to\")"]}