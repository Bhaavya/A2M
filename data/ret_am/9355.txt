{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Universal</b> <b>Approximation</b> with Quadratic Deep Networks", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7076904/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7076904", "snippet": "Since a ReLU quadratic network can represent <b>any</b> univariate polynomial in a unique and global manner, and by the Weierstrass <b>theorem</b> and the Kolmogorov <b>theorem</b> that multivariate functions can be represented through summation and composition of univariate functions, we can approximate <b>any</b> multivariate function with a well-structured ReLU quadratic neural network, justifying the <b>universal</b> <b>approximation</b> power of the quadratic network. To our best knowledge, our quadratic network is the first-of ...", "dateLastCrawled": "2021-12-16T12:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>DeepOnet</b>: <b>Learning</b> nonlinear operators based on the <b>universal</b> ...", "url": "https://cbmm.mit.edu/video/deeponet-learning-nonlinear-operators-based-universal-approximation-theorem-operators", "isFamilyFriendly": true, "displayUrl": "https://cbmm.mit.edu/video/<b>deeponet</b>-<b>learning</b>-nonlinear-operators-based-<b>universal</b>...", "snippet": "This <b>universal</b> <b>approximation</b> <b>theorem</b> of operators is suggestive of the potential of NNs in <b>learning</b> from scattered data <b>any</b> continuous operator or complex system. To realize this <b>theorem</b>, we design a <b>new</b> NN with small generalization error, the deep operator network (<b>DeepONet</b>), consisting of a NN for encoding the discrete input function space (branch net) and another NN for encoding the domain of the output functions (trunk net). We demonstrate that <b>DeepONet</b> can learn various explicit ...", "dateLastCrawled": "2022-02-01T10:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>216: Universal Approximation Theorem (Neural Nets</b>) | Today I Learned", "url": "https://zwaltman.wordpress.com/2018/07/13/216-universal-approximation-theorem-neural-nets/", "isFamilyFriendly": true, "displayUrl": "https://zwaltman.wordpress.com/2018/07/13/<b>216-universal-approximation-theorem-neural-nets</b>", "snippet": "The <b>Universal</b> <b>Approximation</b> <b>Theorem</b> (due to Kurt Hornik and others) essentially states that you can approximate an arbitrary continuous function on a compact subset of with a neural net to arbitrary precision. (This is similar to the way single-variable polynomials are <b>universal</b> approximators of continuous functions on .).) In fact, you can do this with a net with just a single hidden layer. One possible statement of the <b>theorem</b> is below.", "dateLastCrawled": "2022-01-12T06:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) <b>Universal Approximation Theorems</b> - ResearchGate", "url": "https://www.researchgate.net/publication/336361517_Universal_Approximation_Theorems", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/336361517_<b>Universal_Approximation_Theorems</b>", "snippet": "The <b>universal</b> <b>approximation</b> <b>theorem</b> established the density of specific families of neural networks in the space of continuous functions and in certain Bochner spaces, defined between <b>any</b> two ...", "dateLastCrawled": "2022-01-25T23:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Universal</b> <b>approximation</b> <b>theorem</b> for artificial neural networks", "url": "https://www.linkedin.com/pulse/universal-approximation-theorem-artificial-neural-edoardo-mirabella", "isFamilyFriendly": true, "displayUrl": "https://www.linkedin.com/pulse/<b>universal</b>-<b>approximation</b>-<b>theorem</b>-artificial-neural...", "snippet": "Some reasons to read my <b>new</b> article on the <b>universal</b> <b>approximation</b> <b>theorem</b>: - get some insights on why neural networks work, - learn what mathematicians have in common with Frenchmen and chess ...", "dateLastCrawled": "2021-06-17T08:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "What is the <b>implication of the Universal Approximation</b> <b>Theorem</b> over ...", "url": "https://www.quora.com/What-is-the-implication-of-the-Universal-Approximation-Theorem-over-deep-learning-methodology", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-the-<b>implication-of-the-Universal-Approximation</b>-<b>Theorem</b>...", "snippet": "Answer (1 of 2): The <b>universal</b> <b>approximation</b> <b>theorem</b> (<b>Universal</b> <b>approximation</b> <b>theorem</b> - Wikipedia) is a powerful <b>theorem</b> stating that a neural network is a <b>universal</b> functional approximator. However, it does not say anything about the size of the one hidden-layer neural network. Except that it is...", "dateLastCrawled": "2022-01-11T08:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A <b>Universal Approximation Theorem</b> of Deep Neural Networks for ...", "url": "https://deepai.org/publication/a-universal-approximation-theorem-of-deep-neural-networks-for-expressing-distributions", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/a-<b>universal-approximation-theorem</b>-of-deep-neural...", "snippet": "The KSD has received great popularity in machine <b>learning</b> and statistics since the quantity KSD (p, \u03c0) is very easy to compute and does not depend on the normalization constant of \u03c0, which makes it suitable for statistical computation, such as hypothesis testing and statistical sampling [39, 10]. The recent paper adopts the GAN formulation (3.1) with KSD as the training loss to construct a <b>new</b> sampling algorithm called Stein Neural Sampler. 3.1 Main result. Throughout the paper, we ...", "dateLastCrawled": "2022-02-01T11:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Universal Approximation and Depth</b> \u00ab Neural Networks Blog", "url": "https://theneural.wordpress.com/2013/01/07/universal-approximation-and-depth/", "isFamilyFriendly": true, "displayUrl": "https://theneural.wordpress.com/2013/01/07/<b>universal-approximation-and-depth</b>", "snippet": "Indeed, the <b>universal</b> <b>approximation</b> construction works by allocating a neuron to every to every small volume of the input space, and <b>learning</b> the correct answer for each such volume. The problem is that the number of such small volumes grows exponentially in the dimensionality of the input space, so Hornik\u2019s construction is exponentially inefficient and is thus not useful. (it is worth noting that deep neural networks are not <b>universal</b> approximators unless they are also exponentially large ...", "dateLastCrawled": "2022-01-29T22:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "How to prove that <b>a single-layer neural network is</b> a <b>universal</b> ...", "url": "https://www.quora.com/How-do-you-prove-that-a-single-layer-neural-network-is-a-universal-approximator", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/How-do-you-prove-that-<b>a-single-layer-neural-network-is</b>-a...", "snippet": "Answer (1 of 4): This is a handy-wavy answer to a mathematical notion. You should check the reference for more details. I guess you are referring to this <b>theorem</b> [1]. The <b>Universal</b> <b>approximation</b> <b>theorem</b> roughly states that \u201csimple\u201d neural networks can approximate \u201cmany\u201d functions. Notice that ...", "dateLastCrawled": "2022-01-17T13:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Predict the next digit of</b> pi [D] : MachineLearning", "url": "https://www.reddit.com/r/MachineLearning/comments/f8x5if/predict_the_next_digit_of_pi_d/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/Machine<b>Learning</b>/comments/f8x5if/<b>predict_the_next_digit_of</b>_pi_d", "snippet": "The <b>universal</b> <b>approximation</b> <b>theorem</b> makes no guarantees about finding the weights, or the size of the NN required. For example, try to train one to predict AES256(input, key), for a fixed, unknown key. While some NN can do this, and knowing the key we could construct one by hand, doing so without would break cryptography and I can assure you will not work. NNs compress data by giving a distribution on outputs- for example <b>language</b> models predict the next word given all previous words. We can ...", "dateLastCrawled": "2021-08-12T08:06:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Universal Approximation Theorem</b>. The power of Neural Networks | by ...", "url": "https://medium.com/swlh/universal-approximation-theorem-d1a1a67c1b5b", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/<b>universal-approximation-theorem</b>-d1a1a67c1b5b", "snippet": "<b>Universal Approximation Theorem</b>, in its lose form, states that a feed-forward network with a single hidden layer containing a finite number of neurons can approximate <b>any</b> continuous function. Whoa ...", "dateLastCrawled": "2022-01-28T02:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Universal</b> <b>Approximation</b> with Quadratic Deep Networks", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7076904/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7076904", "snippet": "Since a ReLU quadratic network can represent <b>any</b> univariate polynomial in a unique and global manner, and by the Weierstrass <b>theorem</b> and the Kolmogorov <b>theorem</b> that multivariate functions can be represented through summation and composition of univariate functions, we can approximate <b>any</b> multivariate function with a well-structured ReLU quadratic neural network, justifying the <b>universal</b> <b>approximation</b> power of the quadratic network. To our best knowledge, our quadratic network is the first-of ...", "dateLastCrawled": "2021-12-16T12:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Relationship between &quot;Neural Networks&quot; and the &quot;<b>Universal</b> <b>Approximation</b> ...", "url": "https://stats.stackexchange.com/questions/561880/relationship-between-neural-networks-and-the-universal-approximation-theorem", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/561880/relationship-between-neural-networks...", "snippet": "I have the following question about the relationship between the Neural Networks and the <b>Universal</b> <b>Approximation</b> <b>Theorem</b>: For a long time, I was always interested in the reasons behind why neural networks &quot;work&quot;. We often see the mathematics behind how neural networks are trained (e.g. backpropagation algorithm, weight update formula, gradient ...", "dateLastCrawled": "2022-01-26T03:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>DeepOnet</b>: <b>Learning</b> nonlinear operators based on the <b>universal</b> ...", "url": "https://cbmm.mit.edu/video/deeponet-learning-nonlinear-operators-based-universal-approximation-theorem-operators", "isFamilyFriendly": true, "displayUrl": "https://cbmm.mit.edu/video/<b>deeponet</b>-<b>learning</b>-nonlinear-operators-based-<b>universal</b>...", "snippet": "This <b>universal</b> <b>approximation</b> <b>theorem</b> of operators is suggestive of the potential of NNs in <b>learning</b> from scattered data <b>any</b> continuous operator or complex system. To realize this <b>theorem</b>, we design a <b>new</b> NN with small generalization error, the deep operator network (<b>DeepONet</b>), consisting of a NN for encoding the discrete input function space (branch net) and another NN for encoding the domain of the output functions (trunk net). We demonstrate that <b>DeepONet</b> can learn various explicit ...", "dateLastCrawled": "2022-02-01T10:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "What is the <b>implication of the Universal Approximation</b> <b>Theorem</b> over ...", "url": "https://www.quora.com/What-is-the-implication-of-the-Universal-Approximation-Theorem-over-deep-learning-methodology", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-the-<b>implication-of-the-Universal-Approximation</b>-<b>Theorem</b>...", "snippet": "Answer (1 of 2): The <b>universal</b> <b>approximation</b> <b>theorem</b> (<b>Universal</b> <b>approximation</b> <b>theorem</b> - Wikipedia) is a powerful <b>theorem</b> stating that a neural network is a <b>universal</b> functional approximator. However, it does not say anything about the size of the one hidden-layer neural network. Except that it is...", "dateLastCrawled": "2022-01-11T08:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A <b>Universal Approximation Theorem</b> of Deep Neural Networks for ...", "url": "https://deepai.org/publication/a-universal-approximation-theorem-of-deep-neural-networks-for-expressing-distributions", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/a-<b>universal-approximation-theorem</b>-of-deep-neural...", "snippet": "The KSD has received great popularity in machine <b>learning</b> and statistics since the quantity KSD (p, \u03c0) is very easy to compute and does not depend on the normalization constant of \u03c0, which makes it suitable for statistical computation, such as hypothesis testing and statistical sampling [39, 10]. The recent paper adopts the GAN formulation (3.1) with KSD as the training loss to construct a <b>new</b> sampling algorithm called Stein Neural Sampler. 3.1 Main result. Throughout the paper, we ...", "dateLastCrawled": "2022-02-01T11:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>216: Universal Approximation Theorem (Neural Nets</b>) | Today I Learned", "url": "https://zwaltman.wordpress.com/2018/07/13/216-universal-approximation-theorem-neural-nets/", "isFamilyFriendly": true, "displayUrl": "https://zwaltman.wordpress.com/2018/07/13/<b>216-universal-approximation-theorem-neural-nets</b>", "snippet": "The <b>Universal</b> <b>Approximation</b> <b>Theorem</b> (due to Kurt Hornik and others) essentially states that you can approximate an arbitrary continuous function on a compact subset of with a neural net to arbitrary precision. (This <b>is similar</b> to the way single-variable polynomials are <b>universal</b> approximators of continuous functions on .).) In fact, you can do this with a net with just a single hidden layer. One possible statement of the <b>theorem</b> is below.", "dateLastCrawled": "2022-01-12T06:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Universal Approximation and Depth</b> \u00ab Neural Networks Blog", "url": "https://theneural.wordpress.com/2013/01/07/universal-approximation-and-depth/", "isFamilyFriendly": true, "displayUrl": "https://theneural.wordpress.com/2013/01/07/<b>universal-approximation-and-depth</b>", "snippet": "Indeed, the <b>universal</b> <b>approximation</b> construction works by allocating a neuron to every to every small volume of the input space, and <b>learning</b> the correct answer for each such volume. The problem is that the number of such small volumes grows exponentially in the dimensionality of the input space, so Hornik\u2019s construction is exponentially inefficient and is thus not useful. (it is worth noting that deep neural networks are not <b>universal</b> approximators unless they are also exponentially large ...", "dateLastCrawled": "2022-01-29T22:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "How to prove that <b>a single-layer neural network is</b> a <b>universal</b> ...", "url": "https://www.quora.com/How-do-you-prove-that-a-single-layer-neural-network-is-a-universal-approximator", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/How-do-you-prove-that-<b>a-single-layer-neural-network-is</b>-a...", "snippet": "Answer (1 of 4): This is a handy-wavy answer to a mathematical notion. You should check the reference for more details. I guess you are referring to this <b>theorem</b> [1]. The <b>Universal</b> <b>approximation</b> <b>theorem</b> roughly states that \u201csimple\u201d neural networks can approximate \u201cmany\u201d functions. Notice that ...", "dateLastCrawled": "2022-01-17T13:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "[<b>R] Universal Approximation - Transposed</b>! : MachineLearning", "url": "https://www.reddit.com/r/MachineLearning/comments/hqcf2y/r_universal_approximation_transposed/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/Machine<b>Learning</b>/comments/hqcf2y/r_<b>universal</b>_<b>approximation</b>...", "snippet": "The original <b>Universal</b> <b>Approximation</b> <b>Theorem</b> is a classical <b>theorem</b> (from 1999-ish) that states that shallow neural networks can approximate <b>any</b> function. This is one of the foundational results on the topic of &quot;why neural networks work&quot;! Here: We establish a <b>new</b> version of the <b>theorem</b> that applies to arbitrarily deep neural networks. In doing so, we demonstrate a qualitative difference between shallow neural networks and deep neural networks (with respect to allowable activation functions ...", "dateLastCrawled": "2021-08-30T07:38:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Universal</b> <b>Approximation</b> <b>Theorem</b> - Beginner&#39;s Guide", "url": "https://www.analyticsvidhya.com/blog/2021/06/beginners-guide-to-universal-approximation-theorem/", "isFamilyFriendly": true, "displayUrl": "https://www.analyticsvidhya.com/.../06/beginners-guide-to-<b>universal</b>-<b>approximation</b>-<b>theorem</b>", "snippet": "In simple words, the <b>universal</b> <b>approximation</b> <b>theorem</b> says that neural networks <b>can</b> approximate <b>any</b> function. Now, this is powerful. Because, what this means is that <b>any</b> task that <b>can</b> <b>be thought</b> of as a function computation, <b>can</b> be performed/computed by the neural networks. And almost <b>any</b> task that neural network does today is a function computation \u2013 be it <b>language</b> translation, caption generation, speech to text, etc. If you ask, how many layers will be required for <b>any</b> function ...", "dateLastCrawled": "2022-01-31T02:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Universal Approximation Theorems</b> - ResearchGate", "url": "https://www.researchgate.net/publication/336361517_Universal_Approximation_Theorems", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/336361517_<b>Universal_Approximation_Theorems</b>", "snippet": "The <b>universal</b> <b>approximation</b> <b>theorem</b> established the density of specific families of neural networks in the space of continuous functions and in certain Bochner spaces, defined between <b>any</b> two ...", "dateLastCrawled": "2022-01-25T23:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>DeepOnet</b>: <b>Learning</b> nonlinear operators based on the <b>universal</b> ...", "url": "https://cbmm.mit.edu/video/deeponet-learning-nonlinear-operators-based-universal-approximation-theorem-operators", "isFamilyFriendly": true, "displayUrl": "https://cbmm.mit.edu/video/<b>deeponet</b>-<b>learning</b>-nonlinear-operators-based-<b>universal</b>...", "snippet": "This <b>universal</b> <b>approximation</b> <b>theorem</b> of operators is suggestive of the potential of NNs in <b>learning</b> from scattered data <b>any</b> continuous operator or complex system. To realize this <b>theorem</b>, we design a <b>new</b> NN with small generalization error, the deep operator network (<b>DeepONet</b>), consisting of a NN for encoding the discrete input function space (branch net) and another NN for encoding the domain of the output functions (trunk net). We demonstrate that <b>DeepONet</b> <b>can</b> learn various explicit ...", "dateLastCrawled": "2022-02-01T10:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Universal Approximation and Depth</b> \u00ab Neural Networks Blog", "url": "https://theneural.wordpress.com/2013/01/07/universal-approximation-and-depth/", "isFamilyFriendly": true, "displayUrl": "https://theneural.wordpress.com/2013/01/07/<b>universal-approximation-and-depth</b>", "snippet": "The result claimed that a single hidden layer neural network <b>can</b> approximate <b>any</b> function, so a one hidden layer neural network should be good for <b>any</b> application. However it is not an efficient approximator for the functions we care about (this claim is true but hard to defend, since it\u2019s not so easy to describe the functions that we care about). Indeed, the <b>universal</b> <b>approximation</b> construction works by allocating a neuron to every to every small volume of the input space, and <b>learning</b> ...", "dateLastCrawled": "2022-01-29T22:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "[<b>R] Universal Approximation - Transposed</b>! : MachineLearning", "url": "https://www.reddit.com/r/MachineLearning/comments/hqcf2y/r_universal_approximation_transposed/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/Machine<b>Learning</b>/comments/hqcf2y/r_<b>universal</b>_<b>approximation</b>...", "snippet": "The original <b>Universal</b> <b>Approximation</b> <b>Theorem</b> is a classical <b>theorem</b> (from 1999-ish) that states that shallow neural networks <b>can</b> approximate <b>any</b> function. This is one of the foundational results on the topic of &quot;why neural networks work&quot;! Here: We establish a <b>new</b> version of the <b>theorem</b> that applies to arbitrarily deep neural networks. In doing so, we demonstrate a qualitative difference between shallow neural networks and deep neural networks (with respect to allowable activation functions ...", "dateLastCrawled": "2021-08-30T07:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Universal Approximations of Invariant Maps</b> by Neural Networks ...", "url": "https://link.springer.com/article/10.1007/s00365-021-09546-1", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00365-021-09546-1", "snippet": "Our work <b>can</b> be seen as an extension of results on the <b>universal</b> <b>approximation</b> property of neural networks [ 7, 10, 18, 19, 24, 29, 31, 32] to the setting of group invariant/equivariant maps and/or infinite-dimensional input spaces. Our general results in Sect. 2 are based on classical results of the theory of polynomial invariants [ 16, 17, 46 ].", "dateLastCrawled": "2022-01-23T15:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "How to prove that <b>a single-layer neural network is</b> a <b>universal</b> ...", "url": "https://www.quora.com/How-do-you-prove-that-a-single-layer-neural-network-is-a-universal-approximator", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/How-do-you-prove-that-<b>a-single-layer-neural-network-is</b>-a...", "snippet": "Answer (1 of 4): This is a handy-wavy answer to a mathematical notion. You should check the reference for more details. I guess you are referring to this <b>theorem</b> [1]. The <b>Universal</b> <b>approximation</b> <b>theorem</b> roughly states that \u201csimple\u201d neural networks <b>can</b> approximate \u201cmany\u201d functions. Notice that ...", "dateLastCrawled": "2022-01-17T13:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Deep Learning 2: Part 1 Lesson</b> 1. My personal notes from fast.ai course ...", "url": "https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@hiromi_suenaga/<b>deep-learning-2-part-1-lesson</b>-1-602f73869197", "snippet": "What <b>universal</b> <b>approximation</b> <b>theorem</b> says is that this kind of function <b>can</b> solve <b>any</b> given problem to arbitrarily close accuracy as long as you add enough parameters. All purpose parameter ...", "dateLastCrawled": "2022-01-26T00:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 8, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Artificial neural network</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Artificial_neural_network", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Artificial_neural_network</b>", "snippet": "This <b>can</b> <b>be thought</b> <b>of as learning</b> with a &quot;teacher&quot;, in the form of a function that provides continuous feedback on the quality of solutions obtained thus far. Unsupervised <b>learning</b> . In x {\\displaystyle \\textstyle x} and the network&#39;s output. The cost function is dependent on the task (the model domain) and <b>any</b> assumptions (the implicit properties of the model, its parameters and the observed variables). As a trivial example, consider the model (x a a is a constant and the cost = E \u2212 (x ...", "dateLastCrawled": "2022-02-03T16:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>GitHub</b> - adeshpande3/<b>Tensorflow-Programs-and-Tutorials</b>: Implementations ...", "url": "https://github.com/adeshpande3/Tensorflow-Programs-and-Tutorials", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/adeshpande3/<b>Tensorflow-Programs-and-Tutorials</b>", "snippet": "<b>Universal</b> <b>Approximation</b> <b>Theorem</b> (Work in Progress) - The <b>Universal</b> <b>Approximation</b> <b>Theorem</b> states that <b>any</b> feed forward neural network with a single hidden layer <b>can</b> model <b>any</b> function. In this notebook, I&#39;ll go through a practical example of illustrating why this <b>theorem</b> works, and talk about what the implications are for when you&#39;re training your own neural networks.", "dateLastCrawled": "2022-01-31T06:16:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Universal</b> <b>Approximation</b> with Quadratic Deep Networks", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7076904/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7076904", "snippet": "Since a ReLU quadratic network <b>can</b> represent <b>any</b> univariate polynomial in a unique and global manner, and by the Weierstrass <b>theorem</b> and the Kolmogorov <b>theorem</b> that multivariate functions <b>can</b> be represented through summation and composition of univariate functions, we <b>can</b> approximate <b>any</b> multivariate function with a well-structured ReLU quadratic neural network, justifying the <b>universal</b> <b>approximation</b> power of the quadratic network. To our best knowledge, our quadratic network is the first-of ...", "dateLastCrawled": "2021-12-16T12:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A <b>Universal Approximation Theorem</b> of Deep Neural Networks for ...", "url": "https://deepai.org/publication/a-universal-approximation-theorem-of-deep-neural-networks-for-expressing-distributions", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/a-<b>universal-approximation-theorem</b>-of-deep-neural...", "snippet": "In the mathematical <b>language</b>, GANs <b>can</b> be formulated as the following minimization problem: ... The KSD has received great popularity in machine <b>learning</b> and statistics since the quantity KSD (p, \u03c0) is very easy to compute and does not depend on the normalization constant of \u03c0, which makes it suitable for statistical computation, such as hypothesis testing and statistical sampling [39, 10]. The recent paper adopts the GAN formulation (3.1) with KSD as the training loss to construct a <b>new</b> ...", "dateLastCrawled": "2022-02-01T11:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "What is the <b>implication of the Universal Approximation</b> <b>Theorem</b> over ...", "url": "https://www.quora.com/What-is-the-implication-of-the-Universal-Approximation-Theorem-over-deep-learning-methodology", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-the-<b>implication-of-the-Universal-Approximation</b>-<b>Theorem</b>...", "snippet": "Answer (1 of 2): The <b>universal</b> <b>approximation</b> <b>theorem</b> (<b>Universal</b> <b>approximation</b> <b>theorem</b> - Wikipedia) is a powerful <b>theorem</b> stating that a neural network is a <b>universal</b> functional approximator. However, it does not say anything about the size of the one hidden-layer neural network. Except that it is...", "dateLastCrawled": "2022-01-11T08:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Generalizing universal function approximators</b> | Nature Machine Intelligence", "url": "https://www.nature.com/articles/s42256-021-00318-x", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s42256-021-00318-x", "snippet": "a, <b>Universal</b> <b>approximation</b> <b>theorem</b> for operators 10 provides theoretical guarantees on the ability of neural networks to accurately approximate <b>any</b> nonlinear continuous operator \u2014 a mapping from ...", "dateLastCrawled": "2022-01-31T07:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) <b>A Universal Approximation Theorem for Mixture</b> of Experts Models", "url": "https://www.researchgate.net/publication/301875152_A_Universal_Approximation_Theorem_for_Mixture_of_Experts_Models", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/301875152_A_<b>Universal</b>_<b>Approximation</b>_<b>Theorem</b>...", "snippet": "<b>Theorem</b> 1. Let X \u2282 R p be a c ompact set and let U b e a set of continuous. re al-valued functions on X. Assume that. (i) the constant function u ( x) = 1 is in U. 2. (ii) for <b>any</b> two points x 1 ...", "dateLastCrawled": "2022-01-09T17:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Universal Approximations of Invariant Maps</b> by Neural Networks ...", "url": "https://link.springer.com/article/10.1007/s00365-021-09546-1", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00365-021-09546-1", "snippet": "We describe generalizations of the <b>universal</b> <b>approximation</b> <b>theorem</b> for neural networks to maps invariant or equivariant with respect to linear representations of groups. Our goal is to establish network-like computational models that are both invariant/equivariant and provably complete in the sense of their ability to approximate <b>any</b> continuous invariant/equivariant map. Our contribution is three-fold. First, in the general case of compact groups we propose a construction of a complete ...", "dateLastCrawled": "2022-01-23T15:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Universal Approximation and Depth</b> \u00ab Neural Networks Blog", "url": "https://theneural.wordpress.com/2013/01/07/universal-approximation-and-depth/", "isFamilyFriendly": true, "displayUrl": "https://theneural.wordpress.com/2013/01/07/<b>universal-approximation-and-depth</b>", "snippet": "The result claimed that a single hidden layer neural network <b>can</b> approximate <b>any</b> function, so a one hidden layer neural network should be good for <b>any</b> application. However it is not an efficient approximator for the functions we care about (this claim is true but hard to defend, since it\u2019s not so easy to describe the functions that we care about). Indeed, the <b>universal</b> <b>approximation</b> construction works by allocating a neuron to every to every small volume of the input space, and <b>learning</b> ...", "dateLastCrawled": "2022-01-29T22:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "[<b>R] Universal Approximation - Transposed</b>! : MachineLearning", "url": "https://www.reddit.com/r/MachineLearning/comments/hqcf2y/r_universal_approximation_transposed/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/Machine<b>Learning</b>/comments/hqcf2y/r_<b>universal</b>_<b>approximation</b>...", "snippet": "The original <b>Universal</b> <b>Approximation</b> <b>Theorem</b> is a classical <b>theorem</b> (from 1999-ish) that states that shallow neural networks <b>can</b> approximate <b>any</b> function. This is one of the foundational results on the topic of &quot;why neural networks work&quot;! Here: We establish a <b>new</b> version of the <b>theorem</b> that applies to arbitrarily deep neural networks. In doing so, we demonstrate a qualitative difference between shallow neural networks and deep neural networks (with respect to allowable activation functions ...", "dateLastCrawled": "2021-08-30T07:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "What is the difference between a <b>neural network</b> and a deep <b>neural</b> ...", "url": "https://stats.stackexchange.com/questions/182734/what-is-the-difference-between-a-neural-network-and-a-deep-neural-network-and-w", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/182734", "snippet": "You must be referring to the so called <b>Universal</b> <b>approximation</b> <b>theorem</b>, proved by Cybenko in 1989 and generalized by various people in the 1990s. It basically says that a shallow <b>neural network</b> (with 1 hidden layer) <b>can</b> approximate <b>any</b> function, i.e. <b>can</b> in principle learn anything.", "dateLastCrawled": "2022-01-29T02:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What does it mean by the statement, &#39;neural networks are <b>universal</b> ...", "url": "https://www.quora.com/What-does-it-mean-by-the-statement-neural-networks-are-universal-approximators-in-a-mathematical-intuitive-sense", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-does-it-mean-by-the-statement-neural-networks-are-<b>universal</b>...", "snippet": "Answer (1 of 4): Consider the set of all continuous functions which are defined on the unit hypercube (i.e. the unit square in two dimensions, the unit cube in three dimensions, etc.). Call this set C. Given two functions in C, it is possible to define a metric which calculates a notion of dist...", "dateLastCrawled": "2022-01-06T20:17:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Universal Approximation Theorem</b>. The power of Neural Networks | by ...", "url": "https://medium.com/swlh/universal-approximation-theorem-d1a1a67c1b5b", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/<b>universal-approximation-theorem</b>-d1a1a67c1b5b", "snippet": "<b>Universal Approximation Theorem</b>, in its lose form, states that a feed-forward network with a single hidden layer containing a finite number of neurons can approximate any continuous function. Whoa ...", "dateLastCrawled": "2022-01-28T02:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Universal Approximation Theorem</b>, Neural Nets &amp; Lego Blocks | by ...", "url": "https://medium.com/analytics-vidhya/universal-approximation-theorem-neural-nets-lego-blocks-1f5a7d93542a", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/<b>universal-approximation-theorem</b>-neural-nets-lego...", "snippet": "In this post, we will look at the <b>Universal Approximation Theorem</b> \u2014 one of the fundamental theorems on which the entire concept of Deep <b>Learning</b> is based upon. We will make use of lego blocks ...", "dateLastCrawled": "2022-01-28T22:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Learning</b> Complex Functions using <b>Universal</b> Approximate <b>Theorem</b> - Ai Nxt", "url": "https://ainxt.co.in/learning-complex-functions-using-universal-approximate-theorem/", "isFamilyFriendly": true, "displayUrl": "https://ainxt.co.in/<b>learning</b>-complex-functions-using-<b>universal</b>-approximate-<b>theorem</b>", "snippet": "<b>Universal</b> <b>Approximation</b> <b>Theorem</b>. No matter how complex our output logic is, we can use collection of neurons and form Dense Neural Network to approximate our function. This is known as \u201c<b>UNIVERSAL</b> <b>APPROXIMATION</b> <b>THEOREM</b>\u201c. Lets take an example of Two Dimensional data where y = f(x) i.e. y is some function of x. Now, we need to find that ...", "dateLastCrawled": "2022-01-21T05:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Illustrative Proof of <b>Universal Approximation Theorem</b> | HackerNoon", "url": "https://hackernoon.com/illustrative-proof-of-universal-approximation-theorem-5845c02822f6", "isFamilyFriendly": true, "displayUrl": "https://hackernoon.com/illustrative-proof-of-<b>universal-approximation-theorem</b>-5845c02822f6", "snippet": "We will talk about the <b>Universal approximation theorem</b> and we will also prove the <b>theorem</b> graphically. The most commonly used sigmoid function is the logistic function, which has a characteristic of an \u201cS\u201d shaped curve. In real life, we deal with complex functions where the relationship between input and output might be complex. To solve this problem, let&#39;s take an <b>analogy</b> of building a house. The way we are going to create complex functions is that we will combine the sigmoids neurons ...", "dateLastCrawled": "2022-02-01T02:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Illustrative Proof of <b>Universal</b> <b>Approximation</b> <b>Theorem</b> - Start-Tech Academy", "url": "https://starttechacademy.com/illustrative-proof-of-universal-approximation-theorem/", "isFamilyFriendly": true, "displayUrl": "https://starttechacademy.com/illustrative-proof-of-<b>universal</b>-<b>approximation</b>-<b>theorem</b>", "snippet": "In this post, we will talk about the <b>Universal</b> <b>approximation</b> <b>theorem</b> and we will also prove the <b>theorem</b> graphically. This is a follow-up post of my previous post on Sigmoid Neuron. Citation Note: The content and the structure of this article is based on the deep <b>learning</b> lectures from One-Fourth Labs \u2014 Padhai. Sigmoid Neuron Before we \u2026 Illustrative Proof of <b>Universal</b> <b>Approximation</b> <b>Theorem</b> Read More \u00bb", "dateLastCrawled": "2022-01-26T04:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "ne.neural evol - <b>Universal Approximation Theorem</b> \u2014 Neural Networks ...", "url": "https://cstheory.stackexchange.com/questions/17545/universal-approximation-theorem-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://cstheory.stackexchange.com/questions/17545", "snippet": "<b>Universal approximation theorem</b> states that &quot;the standard multilayer feed-forward network with a single hidden layer, ... There is an advanced result, key to <b>machine</b> <b>learning</b>, known as Kolmogorov&#39;s <b>theorem</b> [1]; I have never seen an intuitive sketch of why it works. This may have to do with the different cultures that approach it. The applied <b>learning</b> crowd regards Kolmogorov&#39;s <b>theorem</b> as an existence <b>theorem</b> that merely indicates that NNs may exist, so at least the structure is not overly ...", "dateLastCrawled": "2022-02-03T04:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "TensorFlow is an interface for expressing <b>machine</b> <b>learning</b> algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "The <b>Universal</b> <b>Approximation</b> Property - Springer", "url": "https://link.springer.com/content/pdf/10.1007/s10472-020-09723-1.pdf", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/content/pdf/10.1007/s10472-020-09723-1.pdf", "snippet": "The <b>universal</b> <b>approximation</b> property of various <b>machine</b> <b>learning</b> models is currently only understood on a case-by-case basis, limiting the rapid development of new theoretically jus- tified neural network architectures and blurring our understanding of our current models\u2019 potential. This paper works towards overcoming these challenges by presenting a charac-terization, a representation, a construction method, and an existence result, each of which applies to any <b>universal</b> approximator on ...", "dateLastCrawled": "2022-01-19T15:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Machine Learning by Analogy</b> - SlideShare", "url": "https://www.slideshare.net/ColleenFarrelly/machine-learning-by-analogy-59094152", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ColleenFarrelly/<b>machine-learning-by-analogy</b>-59094152", "snippet": "<b>Machine Learning by Analogy</b> 1. Colleen M. Farrelly 2. Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics behind these ...", "dateLastCrawled": "2022-01-31T07:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "neural networks - <b>Universal Approximation Theorem and high dimension</b> ...", "url": "https://stats.stackexchange.com/questions/298622/universal-approximation-theorem-and-high-dimension-linear-regression", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/298622/<b>universal</b>-<b>approximation</b>-<b>theorem</b>-and...", "snippet": "Cross Validated is a question and answer site for people interested in statistics, <b>machine</b> <b>learning</b>, data analysis, data mining, and data visualization. It only takes a minute to sign up. Sign up to join this community", "dateLastCrawled": "2022-01-17T22:58:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(universal approximation theorem)  is like +(learning any new language)", "+(universal approximation theorem) is similar to +(learning any new language)", "+(universal approximation theorem) can be thought of as +(learning any new language)", "+(universal approximation theorem) can be compared to +(learning any new language)", "machine learning +(universal approximation theorem AND analogy)", "machine learning +(\"universal approximation theorem is like\")", "machine learning +(\"universal approximation theorem is similar\")", "machine learning +(\"just as universal approximation theorem\")", "machine learning +(\"universal approximation theorem can be thought of as\")", "machine learning +(\"universal approximation theorem can be compared to\")"]}