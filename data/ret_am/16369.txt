{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Surpassing Trillion Parameters and GPT-3 with Switch Transformers \u2013 a ...", "url": "https://machinelearningmastery.in/2021/10/01/surpassing-trillion-parameters-and-gpt-3-with-switch-transformers-a-path-to-agi/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.in/2021/10/01/surpassing-trillion-parameters-and-gpt-3...", "snippet": "Instead of producing a single output of hidden nodes (or features) <b>like</b> a conventional dense neural <b>layer</b>, a <b>self-attention</b> <b>layer</b> (or 3 separate layers) will produce 3 vectors <b>called</b> the key, query, and value vectors. The dot product of the key and query vectors becomes the attention weight for the value vector, and all attention weights are subjected to a softmax activation function to ensure that the total sum of attention weights is always the same and always takes a value of 1.0.", "dateLastCrawled": "2022-01-08T01:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Attending to Attention. A summary of a revolutionary paper\u2026 | by Akash ...", "url": "https://towardsdatascience.com/attending-to-attention-eba798f0e940", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/attending-to-attention-eba798f0e940", "snippet": "The encoder is composed of a stack of N = 6 identical layers. Each <b>layer</b> has two sub-layers. The first is a multi-head <b>self-attention</b> mechanism, and the second is a simple, position-wise fully connected feed-forward network. We employ a residual connection around each of the two sub-layers, followed by <b>layer</b> normalization. That is, the output ...", "dateLastCrawled": "2022-01-25T10:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "The Trillion Parameter Mark: Switch Transformers - DZone AI", "url": "https://dzone.com/articles/passing-the-trillion-parameter-mark-with-switch-tr", "isFamilyFriendly": true, "displayUrl": "https://dzone.com/articles/passing-the-trillion-parameter-mark-with-switch-tr", "snippet": "Instead of producing a single output of hidden nodes (or features) <b>like</b> a conventional dense neural <b>layer</b>, a <b>self-attention</b> <b>layer</b> (or 3 separate layers) will produce 3 vectors <b>called</b> key, query ...", "dateLastCrawled": "2021-12-27T20:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Toward <b>Brain Computer Interface</b>: Deep Generative Models for Brain ...", "url": "https://cbmm.mit.edu/video/toward-brain-computer-interface-deep-generative-models-brain-reading", "isFamilyFriendly": true, "displayUrl": "https://cbmm.mit.edu/video/toward-<b>brain-computer-interface</b>-deep-generative-models...", "snippet": "<b>Self-attention</b> module turns out to be useful in many applications. The intuitive way to interpret this <b>self-attention</b> model is the following. Imagine, you are an artist and you want to paint a dog as he&#39;s sitting on the grass. So what the <b>self-attention</b> model does is to pay more attention to the dog regions, instead of the grass. So this is exactly what the <b>self-attention</b> model does. It helps you guide the network to focus on important region to paint such that we can generate more realistic ...", "dateLastCrawled": "2021-12-22T14:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Deep Learning for AI | July 2021 | Communications of the ACM", "url": "https://cacm.acm.org/magazines/2021/7/253464-deep-learning-for-ai/fulltext", "isFamilyFriendly": true, "displayUrl": "https://cacm.acm.org/magazines/2021/7/253464", "snippet": "A very impressive recent demonstration is BERT, 22 which <b>also</b> exploits <b>self-attention</b> to dynamically connect groups of units, as described later. The main advantage of using vectors of neural activity to represent concepts and weight matrices to capture relationships between concepts is that this leads to automatic generalization. If Tuesday and Thursday are represented by very similar vectors, they will have very similar causal effects on other vectors of neural activity. This facilitates ...", "dateLastCrawled": "2022-01-31T09:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Trajectory prediction for intelligent vehicles using</b> spatial\u2010attention ...", "url": "https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/iet-its.2020.0274", "isFamilyFriendly": true, "displayUrl": "https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/iet-its.2020.0274", "snippet": "Google proposed the <b>self-attention</b> mechanism based on transformer architecture which dispenses with traditional CNN and ... and position <b>information</b>. Besides, it <b>also</b> provides some pre-extracted <b>information</b> including surrounding vehicles, metrics <b>like</b> time headway (THW), or time to collision (TTC), and driven manoeuvres (e.g. lane changes). From the highD data set, we extract total 333,140 driving sequences. Each sequence contains 8 s driving scenario <b>information</b>. Among them, there are ...", "dateLastCrawled": "2022-01-30T02:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) Hands-On Machine Learning with Scikit-Learn &amp; TensorFlow CONCEPTS ...", "url": "https://www.academia.edu/39825063/Hands_On_Machine_Learning_with_Scikit_Learn_and_TensorFlow_CONCEPTS_TOOLS_AND_TECHNIQUES_TO_BUILD_INTELLIGENT_SYSTEMS", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/39825063/Hands_On_Machine_Learning_with_Scikit_Learn_and...", "snippet": "Hands-On Machine Learning with <b>Scikit-Learn &amp; TensorFlow CONCEPTS, TOOLS, AND TECHNIQUES TO BUILD INTELLIGENT SYSTEMS</b>", "dateLastCrawled": "2022-01-30T23:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "The State of AI in 2020. Learn where <b>our</b> future is headed by\u2026 | by Ian ...", "url": "https://towardsdatascience.com/the-state-of-ai-in-2020-1f95df336eb0", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/the-state-of-ai-in-2020-1f95df336eb0", "snippet": "Natural Language <b>Processing</b>. The <b>current state</b> of the art in language <b>processing</b> has effectively merged recurrent and convolutional neural networks, while creating new methods within. All of this centers around Transformers, <b>self-attention</b> and word embeddings. These concepts all help to model relationships of words in massively parallel CNNs or ...", "dateLastCrawled": "2022-02-03T03:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Can you <b>get AGI from a Transformer</b>? - LessWrong 2.0 viewer", "url": "https://www.greaterwrong.com/posts/SkcM4hwgH3AP6iqjs/can-you-get-agi-from-a-transformer", "isFamilyFriendly": true, "displayUrl": "https://www.greaterwrong.com/posts/SkcM4hwgH3AP6iqjs/can-you-<b>get-agi-from-a-transformer</b>", "snippet": "But I don\u2019t think that\u2019s relevant to this question. <b>Like</b>, we don\u2019t have low-level access to <b>our</b> <b>own</b> <b>brains</b>. If you ask GPT-3 (through its API) to simulate a <b>self-attention</b> <b>layer</b>, it wouldn\u2019t do particularly well, right? So I don\u2019t think it\u2019s any evidence either way. \u201cSurpassed\u201d seems strange to me; I\u2019ll bet that the first AGI system will have a very GPT-<b>like</b> module, that will be critical to its performance, that will nevertheless not be \u201cthe whole story.\u201d <b>Like</b>, by ...", "dateLastCrawled": "2022-01-24T14:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Can you get AGI from a Transformer? - AI Alignment Forum", "url": "https://www.alignmentforum.org/posts/SkcM4hwgH3AP6iqjs/can-you-get-agi-from-a-transformer", "isFamilyFriendly": true, "displayUrl": "https://www.alignmentforum.org/posts/SkcM4hwgH3AP6iqjs/can-you-get-agi-from-a-transformer", "snippet": "But for the moment, I continue to consider it very possible that Transformers specifically, and DNN-type <b>processing</b> more generally (matrix multiplications, ReLUs, etc.), for all their amazing powers, will eventually be surpassed in AGI-type capabilities by a different kind of <b>information</b> <b>processing</b>, more <b>like</b> probabilistic programming and message-passing, and <b>also</b> more <b>like</b> the neocortex (but, just <b>like</b> DNNs, still based on relatively simple, general principles, and still requiring an awful ...", "dateLastCrawled": "2022-01-30T06:37:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "The Trillion Parameter Mark: Switch Transformers - DZone AI", "url": "https://dzone.com/articles/passing-the-trillion-parameter-mark-with-switch-tr", "isFamilyFriendly": true, "displayUrl": "https://dzone.com/articles/passing-the-trillion-parameter-mark-with-switch-tr", "snippet": "Each head in the attention <b>layer</b> learns to attend to different features of the sequence, and a residual connection is used to sum and normalize the input to and output from the <b>self-attention</b> ...", "dateLastCrawled": "2021-12-27T20:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Exxact | Deep Learning, HPC, AV, Distribution &amp; More", "url": "https://www.exxactcorp.com/blog/Deep-Learning/switch-transformers-trillion-parameter-mark", "isFamilyFriendly": true, "displayUrl": "https://www.exxactcorp.com/blog/Deep-Learning/switch-transformers-trillion-parameter-mark", "snippet": "Instead of producing a single output of hidden nodes (or features) like a conventional dense neural <b>layer</b>, a <b>self-attention</b> <b>layer</b> (or 3 separate layers) will produce 3 vectors <b>called</b> key, query, and value vectors. The dot product of the key and query vectors becomes the attention weight for the value vector, and all attention weights are subjected to a softmax activation function to ensure that the total sum of attention weights is always the same and always takes a value of 1.0.", "dateLastCrawled": "2021-12-27T19:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Attending to Attention. A summary of a revolutionary paper\u2026 | by Akash ...", "url": "https://towardsdatascience.com/attending-to-attention-eba798f0e940", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/attending-to-attention-eba798f0e940", "snippet": "The source mask <b>is similar</b> to the source vector, it contains 1 when the token is not &lt;pad&gt; and 0 otherwise, this is done to avoid the attention <b>layer</b> from focusing on the padding token. Initially, the embedding vector is passed through the Multi-Head Attention <b>layer</b>, the result along with residual connection are elementwise summed and passed through <b>layer</b> normalization. The Multi-Head-Attention is passed the source sentence as the key, value, and query (more on that later), this is done so ...", "dateLastCrawled": "2022-01-25T10:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Deep Learning for AI | July 2021 | Communications of the ACM", "url": "https://cacm.acm.org/magazines/2021/7/253464-deep-learning-for-ai/fulltext", "isFamilyFriendly": true, "displayUrl": "https://cacm.acm.org/magazines/2021/7/253464", "snippet": "A very impressive recent demonstration is BERT, 22 which <b>also</b> exploits <b>self-attention</b> to dynamically connect groups of units, as described later. The main advantage of using vectors of neural activity to represent concepts and weight matrices to capture relationships between concepts is that this leads to automatic generalization. If Tuesday and Thursday are represented by very <b>similar</b> vectors, they will have very <b>similar</b> causal effects on other vectors of neural activity. This facilitates ...", "dateLastCrawled": "2022-01-31T09:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Can you get AGI from a Transformer? - AI Alignment Forum", "url": "https://www.alignmentforum.org/posts/SkcM4hwgH3AP6iqjs/can-you-get-agi-from-a-transformer", "isFamilyFriendly": true, "displayUrl": "https://www.alignmentforum.org/posts/SkcM4hwgH3AP6iqjs/can-you-get-agi-from-a-transformer", "snippet": "But I don&#39;t think that&#39;s relevant to this question. Like, we don&#39;t have low-level access <b>to our</b> <b>own</b> <b>brains</b>. If you ask GPT-3 (through its API) to simulate a <b>self-attention</b> <b>layer</b>, it wouldn&#39;t do particularly well, right? So I don&#39;t think it&#39;s any evidence either way. &quot;Surpassed&quot; seems strange to me; I&#39;ll bet that the first AGI system will have a very GPT-like module, that will be critical to its performance, that will nevertheless not be &quot;the whole story.&quot; Like, by analogy to AlphaGo, the ...", "dateLastCrawled": "2022-01-30T06:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Comparing Transformers and RNNs on predicting human sentence <b>processing</b> ...", "url": "https://deepai.org/publication/comparing-transformers-and-rnns-on-predicting-human-sentence-processing-data", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/comparing-transformers-and-rnns-on-predicting-human...", "snippet": "A Transformer cell as originally proposed by Vaswani et al. Vaswani2016, consists of <b>self-attention</b> layers followed by a linear feed forward <b>layer</b>. In contrast to recurrent <b>processing</b>, <b>self-attention</b> layers are allowed to \u2018attend\u2019 to parts of previous input directly. Since its introduction, the Transformer has received substantial attention in the NLP community and achieved state-of-the art results on several NLP tasks [7, 17, 20]. Pre-trained Transformer based models such as BERT and ...", "dateLastCrawled": "2021-12-17T13:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Hands-On Machine Learning with Scikit-Learn &amp; TensorFlow ... - Academia.edu", "url": "https://www.academia.edu/39825063/Hands_On_Machine_Learning_with_Scikit_Learn_and_TensorFlow_CONCEPTS_TOOLS_AND_TECHNIQUES_TO_BUILD_INTELLIGENT_SYSTEMS", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/39825063/Hands_On_Machine_Learning_with_Scikit_Learn_and...", "snippet": "Hands-On Machine Learning with <b>Scikit-Learn &amp; TensorFlow CONCEPTS, TOOLS, AND TECHNIQUES TO BUILD INTELLIGENT SYSTEMS</b>", "dateLastCrawled": "2022-01-30T23:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "How to Summarize Text using Machine Learning Models | Edlitera", "url": "https://www.edlitera.com/blog/posts/text-summarization-nlp-how-to", "isFamilyFriendly": true, "displayUrl": "https://www.edlitera.com/blog/posts/text-summarization-nlp-how-to", "snippet": "Humans, like machines, have a finite amount of data we can process per given unit of time and while we cannot make <b>our</b> <b>brains</b> work faster (yet), we can certainly condense the <b>information</b>, essentially achieving a higher throughput of data processed per unit of time. Automatic text summarization. In this article, we will mostly focus on the most common type of automatic summarization: automatic text summarization. In recent years, this area has become a particular point of interest due to the ...", "dateLastCrawled": "2022-01-19T01:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "The State of AI in 2020. Learn where <b>our</b> future is headed by\u2026 | by Ian ...", "url": "https://towardsdatascience.com/the-state-of-ai-in-2020-1f95df336eb0", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/the-state-of-ai-in-2020-1f95df336eb0", "snippet": "Natural Language <b>Processing</b>. The <b>current state</b> of the art in language <b>processing</b> has effectively merged recurrent and convolutional neural networks, while creating new methods within. All of this centers around Transformers, <b>self-attention</b> and word embeddings. These concepts all help to model relationships of words in massively parallel CNNs or ...", "dateLastCrawled": "2022-02-03T03:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Can you <b>get AGI from a Transformer</b>? - LessWrong 2.0 viewer", "url": "https://www.greaterwrong.com/posts/SkcM4hwgH3AP6iqjs/can-you-get-agi-from-a-transformer", "isFamilyFriendly": true, "displayUrl": "https://www.greaterwrong.com/posts/SkcM4hwgH3AP6iqjs/can-you-<b>get-agi-from-a-transformer</b>", "snippet": "Like, we don\u2019t have low-level access <b>to our</b> <b>own</b> <b>brains</b>. If you ask GPT-3 (through its API) to simulate a <b>self-attention</b> <b>layer</b>, it wouldn\u2019t do particularly well, right? So I don\u2019t think it\u2019s any evidence either way. \u201cSurpassed\u201d seems strange to me; I\u2019ll bet that the first AGI system will have a very GPT-like module, that will be critical to its performance, that will nevertheless not be \u201cthe whole story.\u201d Like, by analogy to AlphaGo, the interesting thing was the structure ...", "dateLastCrawled": "2022-01-24T14:09:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Surpassing Trillion Parameters and GPT-3 with Switch Transformers \u2013 a ...", "url": "https://machinelearningmastery.in/2021/10/01/surpassing-trillion-parameters-and-gpt-3-with-switch-transformers-a-path-to-agi/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.in/2021/10/01/surpassing-trillion-parameters-and-gpt-3...", "snippet": "Instead of producing a single output of hidden nodes (or features) like a conventional dense neural <b>layer</b>, a <b>self-attention</b> <b>layer</b> (or 3 separate layers) will produce 3 vectors <b>called</b> the key, query, and value vectors. The dot product of the key and query vectors becomes the attention weight for the value vector, and all attention weights are subjected to a softmax activation function to ensure that the total sum of attention weights is always the same and always takes a value of 1.0.", "dateLastCrawled": "2022-01-08T01:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Details for Neural Network <b>Self Attention</b> and Related Queries", "url": "https://www.affiliatejoin.com/neural-network-self-attention", "isFamilyFriendly": true, "displayUrl": "https://www.affiliatejoin.com/neural-network-<b>self-attention</b>", "snippet": "<b>Self-Attention</b> <b>Self-attention</b>, <b>also</b> known as intra-attention, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the same sequence. It has been shown to be very useful in machine reading, abstractive summarization, or image description generation.", "dateLastCrawled": "2022-02-02T09:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "The Trillion Parameter Mark: Switch Transformers - DZone AI", "url": "https://dzone.com/articles/passing-the-trillion-parameter-mark-with-switch-tr", "isFamilyFriendly": true, "displayUrl": "https://dzone.com/articles/passing-the-trillion-parameter-mark-with-switch-tr", "snippet": "Each head in the attention <b>layer</b> learns to attend to different features of the sequence, and a residual connection is used to sum and normalize the input to and output from the <b>self-attention</b> ...", "dateLastCrawled": "2021-12-27T20:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Attending to Attention. A summary of a revolutionary paper\u2026 | by Akash ...", "url": "https://towardsdatascience.com/attending-to-attention-eba798f0e940", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/attending-to-attention-eba798f0e940", "snippet": "Encoder Architecture ()The encoder is composed of a stack of N = 6 identical layers. Each <b>layer</b> has two sub-layers. The first is a multi-head <b>self-attention</b> mechanism, and the second is a simple, position-wise fully connected feed-forward network.We employ a residual connection around each of the two sub-layers, followed by <b>layer</b> normalization.", "dateLastCrawled": "2022-01-25T10:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Exxact | Deep Learning, HPC, AV, Distribution &amp; More", "url": "https://www.exxactcorp.com/blog/Deep-Learning/switch-transformers-trillion-parameter-mark", "isFamilyFriendly": true, "displayUrl": "https://www.exxactcorp.com/blog/Deep-Learning/switch-transformers-trillion-parameter-mark", "snippet": "Each head in the attention <b>layer</b> learns to attend to different features of the sequence, and a residual connection is used to sum and normalize the input to and output from the <b>self-attention</b> <b>layer</b>. The output from the attention <b>layer</b> is then given to a router <b>layer</b>, which decides on which expert feed-forward network to recruit, and the weight to be given to the expert <b>layer</b>\u2019s output. Depending on the router\u2019s choice, the features are passed to one of many experts, and the probability ...", "dateLastCrawled": "2021-12-27T19:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "The attention schema theory. (A) Visual attention is captured by the ...", "url": "https://researchgate.net/figure/The-attention-schema-theory-A-Visual-attention-is-captured-by-the-image-of-an-apple_fig2_276065623", "isFamilyFriendly": true, "displayUrl": "https://researchgate.net/figure/The-attention-schema-theory-A-Visual-attention-is...", "snippet": "The attention schema theory. (A) Visual attention is captured by the image of an apple. On its <b>own</b>, this process results in the ability to accurately process the stimulus features \u2013 shape, color ...", "dateLastCrawled": "2021-08-30T04:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Comparing Transformers and RNNs on predicting human sentence <b>processing</b> ...", "url": "https://deepai.org/publication/comparing-transformers-and-rnns-on-predicting-human-sentence-processing-data", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/comparing-transformers-and-rnns-on-predicting-human...", "snippet": "A Transformer cell as originally proposed by Vaswani et al. Vaswani2016, consists of <b>self-attention</b> layers followed by a linear feed forward <b>layer</b>. In contrast to recurrent <b>processing</b>, <b>self-attention</b> layers are allowed to \u2018attend\u2019 to parts of previous input directly. Since its introduction, the Transformer has received substantial attention in the NLP community and achieved state-of-the art results on several NLP tasks [7, 17, 20]. Pre-trained Transformer based models such as BERT and ...", "dateLastCrawled": "2021-12-17T13:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Discrete and continuous representations and <b>processing</b> in deep learning ...", "url": "https://www.sciencedirect.com/science/article/pii/S2666651021000206", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2666651021000206", "snippet": "Interestingly, applying <b>self-attention</b> as in transformers (Vaswani et al., 2017) to a set of features <b>can</b> conceptually be seen as message passing between nodes in a fully connected graph, an analogy proposed by Veli\u010dkovi\u0107 et al. (2018). In this view, (<b>self-)attention</b> <b>can</b> be used to construct arbitrary directed graphs from input nodes in a soft manner. Graphs, much like sets as used in attention, keep factors disentangled compared to representing everything at once (in a single vector ...", "dateLastCrawled": "2022-02-02T19:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Hands-On Machine Learning with Scikit-Learn &amp; TensorFlow ... - Academia.edu", "url": "https://www.academia.edu/39825063/Hands_On_Machine_Learning_with_Scikit_Learn_and_TensorFlow_CONCEPTS_TOOLS_AND_TECHNIQUES_TO_BUILD_INTELLIGENT_SYSTEMS", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/39825063/Hands_On_Machine_Learning_with_Scikit_Learn_and...", "snippet": "Hands-On Machine Learning with <b>Scikit-Learn &amp; TensorFlow CONCEPTS, TOOLS, AND TECHNIQUES TO BUILD INTELLIGENT SYSTEMS</b>", "dateLastCrawled": "2022-01-30T23:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Social Psychology</b> Flashcards | Quizlet", "url": "https://quizlet.com/156414423/social-psychology-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/156414423/<b>social-psychology</b>-flash-cards", "snippet": "We play a role in <b>our</b> <b>own</b> social reality. exp. when you Smile, it makes a difference in your interactions with others The way we perceive reality is different for everyone. Self-Fulfilling Prophecy (3 step) 1. Perceiver forms expectations about target which tends to be false, based on inaccurate <b>information</b> 2. Target reacts towards that person in a way consistent with perciever&#39;s expectations 3. The target interpret the perceiver&#39;s expectation and response, interaction, if continues over ...", "dateLastCrawled": "2018-11-29T23:13:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Details for Neural Network <b>Self Attention</b> and Related Queries", "url": "https://www.affiliatejoin.com/neural-network-self-attention", "isFamilyFriendly": true, "displayUrl": "https://www.affiliatejoin.com/neural-network-<b>self-attention</b>", "snippet": "<b>Self-Attention</b> <b>Self-attention</b>, <b>also</b> known as intra-attention, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the same sequence. It has been shown to be very useful in machine reading, abstractive summarization, or image description generation.", "dateLastCrawled": "2022-02-02T09:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Asthma Exacerbation Prediction and Risk Factor Analysis Based on a Time ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7428917/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7428917", "snippet": "<b>Compared</b> with previous studies, <b>our</b> model is effective in modeling time <b>information</b> and obtains better overall AUCs. As the model is completely data driven and relies little on feature engineering, it <b>can</b> easily be generalized to other prediction tasks. To the best of <b>our</b> knowledge, this is the first study to predict asthma exacerbation risks using a deep learning model that includes elapsed time embeddings. Some of the top-ranked risk factors identified have gained supporting evidence from ...", "dateLastCrawled": "2022-01-29T00:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Toward <b>Brain Computer Interface</b>: Deep Generative Models for Brain ...", "url": "https://cbmm.mit.edu/video/toward-brain-computer-interface-deep-generative-models-brain-reading", "isFamilyFriendly": true, "displayUrl": "https://cbmm.mit.edu/video/toward-<b>brain-computer-interface</b>-deep-generative-models...", "snippet": "<b>Self-attention</b> module turns out to be useful in many applications. The intuitive way to interpret this <b>self-attention</b> model is the following. Imagine, you are an artist and you want to paint a dog as he&#39;s sitting on the grass. So what the <b>self-attention</b> model does is to pay more attention to the dog regions, instead of the grass. So this is exactly what the <b>self-attention</b> model does. It helps you guide the network to focus on important region to paint such that we <b>can</b> generate more realistic ...", "dateLastCrawled": "2021-12-22T14:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Comparing Transformers and RNNs on predicting human sentence <b>processing</b> ...", "url": "https://deepai.org/publication/comparing-transformers-and-rnns-on-predicting-human-sentence-processing-data", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/comparing-transformers-and-rnns-on-predicting-human...", "snippet": "In contrast to recurrent <b>processing</b>, <b>self-attention</b> layers are allowed to \u2018attend\u2019 to parts of previous input directly. Since its introduction, the Transformer has received substantial attention in the NLP community and achieved state-of-the art results on several NLP tasks [7, 17, 20]. Pre-trained Transformer based models such as BERT and GPT-2 make it possible to employ the power of networks trained on huge amounts of data. Many studies have fine-tuned such models and broken benchmark ...", "dateLastCrawled": "2021-12-17T13:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "An Attention-based Bi-LSTM <b>Method for Visual Object Classification via</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S174680942030313X", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S174680942030313X", "snippet": "<b>Compared</b> with existing research, the novelty of <b>our</b> contributions <b>can</b> be summarized as: (1) We introduce the neural attention mechanism into deep learning framework. An attention gate and a weighting method are set to Bi-LSTM based on the attention mechanism, which supplied a solution for EEG-based visual object classification; (2) We carried out extensive experiments, and achieved excellent performance <b>compared</b> with the results reported in the existing literatures; (3) Inspired by the brain ...", "dateLastCrawled": "2022-01-11T18:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Advances in Natural Language Processing</b> | by Nilesh Barla ...", "url": "https://medium.com/perceptronai/advances-in-natural-language-processing-c1cf005b3595", "isFamilyFriendly": true, "displayUrl": "https://medium.com/perceptronai/<b>advances-in-natural-language-processing</b>-c1cf005b3595", "snippet": "It <b>can</b> <b>also</b> help us to explore more about <b>our</b> <b>own</b> cognition: the way we think and process <b>information</b>. We take language to be a part of a system for understanding and communicating about ...", "dateLastCrawled": "2021-09-22T02:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Can</b> you get AGI from a Transformer? - AI Alignment Forum", "url": "https://www.alignmentforum.org/posts/SkcM4hwgH3AP6iqjs/can-you-get-agi-from-a-transformer", "isFamilyFriendly": true, "displayUrl": "https://www.alignmentforum.org/posts/SkcM4hwgH3AP6iqjs/<b>can</b>-you-get-agi-from-a-transformer", "snippet": "But I don&#39;t think that&#39;s relevant to this question. Like, we don&#39;t have low-level access <b>to our</b> <b>own</b> <b>brains</b>. If you ask GPT-3 (through its API) to simulate a <b>self-attention</b> <b>layer</b>, it wouldn&#39;t do particularly well, right? So I don&#39;t think it&#39;s any evidence either way. &quot;Surpassed&quot; seems strange to me; I&#39;ll bet that the first AGI system will have a very GPT-like module, that will be critical to its performance, that will nevertheless not be &quot;the whole story.&quot; Like, by analogy to AlphaGo, the ...", "dateLastCrawled": "2022-01-30T06:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Deep Learning for AI | July 2021 | Communications of the ACM", "url": "https://cacm.acm.org/magazines/2021/7/253464-deep-learning-for-ai/fulltext", "isFamilyFriendly": true, "displayUrl": "https://cacm.acm.org/magazines/2021/7/253464", "snippet": "A significant development in deep learning, especially when it comes to sequential <b>processing</b>, is the use of multiplicative interactions, particularly in the form of soft attention. 7,32,39,78 This is a transformative addition to the neural net toolbox, in that it changes neural nets from purely vector transformation machines into architectures which <b>can</b> dynamically choose which inputs they operate on, and <b>can</b> store <b>information</b> in differentiable associative memories. A key property of such ...", "dateLastCrawled": "2022-01-31T09:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Enhancing cooperation by cognition differences and consistent ...", "url": "https://link.springer.com/article/10.1007/s10489-021-02873-7", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10489-021-02873-7", "snippet": "The attention mechanism [3, 21, 36] <b>can</b> <b>also</b> be used to collect <b>information</b> for the cooperation of agents. The ATOC ... received, and process in the form of action voltages or electric spikes, which focuses on individual agent\u2019s <b>information</b> <b>processing</b>. On the other hand, this paper tries to solve the collaborative decision-making problem of multi-agent, which focuses on a group of agents\u2019 <b>information</b> <b>processing</b>. We do not consider the SNN as the base framework of the proposed CDCR but ...", "dateLastCrawled": "2022-01-29T06:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Which is better, the <b>human brain or artificial intelligence? - Quora</b>", "url": "https://www.quora.com/Which-is-better-the-human-brain-or-artificial-intelligence", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Which-is-better-the-<b>human-brain-or-artificial-intelligence</b>", "snippet": "Answer (1 of 2): Question: Which is better, the <b>human brain or artificial intelligence</b>? Answer: This is an incomplete question. \u2018Better\u2019 is not an absolute quality. Nothing is absolutely \u2018better\u2019 full stop. Something is only better \u2018for\u2019 something. To use an extreme example, you <b>can</b>\u2019t eat arti...", "dateLastCrawled": "2022-01-24T17:55:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Attention in Psychology, Neuroscience, and <b>Machine</b> <b>Learning</b>", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7177153/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7177153", "snippet": "These weightings scale the word encodings themselves to create the next <b>layer</b> in the model, a process known as \u201c<b>self-attention</b>.\u201d This process repeats, and eventually interacts with the autoregressive decoder which <b>also</b> has attention mechanisms that allow it to flexibly focus on the encoded input (as in the standard form of attention) and on the previously generated output. The Transformer\u2014the name given to this new attention architecture\u2014outperformed many previous models and quickly ...", "dateLastCrawled": "2021-12-27T10:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Lecture 7: Transformers</b> - Deep <b>Learning</b>", "url": "https://chinmayhegde.github.io/dl-notes/notes/lecture07/", "isFamilyFriendly": true, "displayUrl": "https://chinmayhegde.github.io/dl-notes/notes/lecture07", "snippet": "<b>Self-Attention</b>. This is the point where papers-blogs-tweets-slides etc start talking about keys/values and attention mechanisms and everything goes a bit haywire. Let\u2019s just ignore all that for now, and instead talk about something <b>called</b> <b>self-attention</b>. The use of the \u201cself-\u201c prefix will become clear later on. Here is how it is defined.", "dateLastCrawled": "2022-02-03T05:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) Enhancing LSTM Models with <b>Self-attention</b> and Stateful Training ...", "url": "https://www.academia.edu/69040947/Enhancing_LSTM_Models_with_Self_attention_and_Stateful_Training", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/69040947/Enhancing_LSTM_Models_with_<b>Self_attention</b>_and_Statef...", "snippet": "<b>Self-attention</b>, <b>also</b> known as intra-attention, is an attention mechanism relat- ing di\ufb00erent positions of a sequence in order to model dependencies between dif- ferent parts of the sequence. This di\ufb00ers from general attention in that instead of seeking to discover the \u201cimportant\u201d parts of the sequence relating to the net- work output, <b>self-attention</b> seeks to \ufb01nd the \u201cimportant\u201d portions of the sequence that relate to each other. This is done in order to leverage those intra ...", "dateLastCrawled": "2022-02-03T12:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Transformers in NLP: A beginner friendly explanation | Towards Data Science", "url": "https://towardsdatascience.com/transformers-89034557de14", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>transformer</b>s-89034557de14", "snippet": "<b>Self attention</b>, sometimes <b>called</b> intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. In simpler terms, <b>self attention</b> helps us create similar connections but within the same sentence. Look at the following example: \u201cI poured water from the bottle into the cup until it was full.\u201d it =&gt; cup \u201cI poured water from the bottle into the cup until it was empty.\u201d it=&gt; bottle. By changing one word ...", "dateLastCrawled": "2022-02-02T13:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) Enhancing LSTM Models with <b>Self-attention</b> and Stateful Training", "url": "https://www.researchgate.net/publication/353690477_Enhancing_LSTM_Models_with_Self-attention_and_Stateful_Training", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/353690477_Enhancing_LSTM_Models_with_Self...", "snippet": "<b>Self-attention</b>, <b>also</b> known as in tra-attention, is an attention mec hanism re- lating di\ufb00erent positions of a sequence in order to model dependencies b etween di\ufb00erent parts of the sequence.", "dateLastCrawled": "2022-01-13T05:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Machine Learning Papers: Molecules</b> - <b>Machine Learning</b> Applied", "url": "https://machinelearningapplied.com/machine-learning-papers-molecules/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>applied.com/<b>machine-learning-papers-molecules</b>", "snippet": "A <b>self-attention</b> based message passing neural network for predicting molecular lipophilicity and aqueous solubility - Tang et al 2020 . Efficient and accurate prediction of molecular properties, such as lipophilicity and solubility, is highly desirable for rational compound design in chemical and pharmaceutical industries. To this end, we build and apply a graph-neural-network framework <b>called</b> <b>self-attention</b>-based message-passing neural network (SAMPN) to study the relationship between ...", "dateLastCrawled": "2021-12-22T02:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Attending to Attention. A summary of a revolutionary paper\u2026 | by Akash ...", "url": "https://towardsdatascience.com/attending-to-attention-eba798f0e940", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/attending-to-attention-eba798f0e940", "snippet": "The encoder is composed of a stack of N = 6 identical layers. Each <b>layer</b> has two sub-layers. The first is a multi-head <b>self-attention</b> mechanism, and the second is a simple, position-wise fully connected feed-forward network. We employ a residual connection around each of the two sub-layers, followed by <b>layer</b> normalization. That is, the output ...", "dateLastCrawled": "2022-01-25T10:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Introduction to Transformers in Machine Learning</b>", "url": "https://www.machinecurve.com/index.php/2020/12/28/introduction-to-transformers-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/.../12/28/<b>introduction-to-transformers-in-machine-learning</b>", "snippet": "<b>Machine</b> <b>Learning</b> in Natural Language Processing has traditionally been performed with recurrent neural networks. Recurrent, here, means that when a sequence is processed, the hidden state (or \u2018memory\u2019) that is used for generating a prediction for a token is <b>also</b> passed on, so that it can be used when generating the subsequent prediction. A recurrent neural network (RNN) is a class of artificial neural networks where connections between nodes form a directed graph along a temporal ...", "dateLastCrawled": "2022-02-03T03:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "How to perform Text Summarization with Python, HuggingFace Transformers ...", "url": "https://www.machinecurve.com/index.php/2020/12/21/easy-text-summarization-with-huggingface-transformers-and-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/index.php/2020/12/21/easy-text-summarization-with-hugging...", "snippet": "Summary &amp; Example: Text Summarization with Transformers. Transformers are taking the world of language processing by storm. These models, which learn to interweave the importance of tokens by means of a mechanism <b>called</b> <b>self-attention</b> and without recurrent segments, have allowed us to train larger models without all the problems of recurrent neural networks.", "dateLastCrawled": "2022-02-02T23:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Journal of Physics: Conference Series PAPER OPEN ACCESS You may <b>also</b> ...", "url": "https://iopscience.iop.org/article/10.1088/1742-6596/1651/1/012019/pdf", "isFamilyFriendly": true, "displayUrl": "https://iopscience.iop.org/article/10.1088/1742-6596/1651/1/012019/pdf", "snippet": "Different <b>machine</b> <b>learning</b> techniques have been used in this field for many years. But recently, deep <b>learning</b> has caused more and more attention in the field of education. Deep <b>learning</b> is a <b>machine</b> <b>learning</b> method based on neural network structure of multi-<b>layer</b> processing units, and it has been successfully applied to a series of problems in the field of image recognition and natural language processing[2]. With the diversified cultivation of traditional universities and the development ...", "dateLastCrawled": "2021-12-29T04:07:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(self-attention (also called self-attention layer))  is like +(our own brains processing information)", "+(self-attention (also called self-attention layer)) is similar to +(our own brains processing information)", "+(self-attention (also called self-attention layer)) can be thought of as +(our own brains processing information)", "+(self-attention (also called self-attention layer)) can be compared to +(our own brains processing information)", "machine learning +(self-attention (also called self-attention layer) AND analogy)", "machine learning +(\"self-attention (also called self-attention layer) is like\")", "machine learning +(\"self-attention (also called self-attention layer) is similar\")", "machine learning +(\"just as self-attention (also called self-attention layer)\")", "machine learning +(\"self-attention (also called self-attention layer) can be thought of as\")", "machine learning +(\"self-attention (also called self-attention layer) can be compared to\")"]}