{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Word Embedding Tutorial | Word2vec</b> Model Gensim Example", "url": "https://www.guru99.com/word-embedding-word2vec.html", "isFamilyFriendly": true, "displayUrl": "https://www.guru99.com/<b>word</b>-<b>embedding</b>-<b>word</b>2vec.html", "snippet": "<b>Word</b> <b>Embedding</b> is a <b>word</b> representation type that allows machine learning algorithms to understand <b>words</b> with similar meanings. It is a language modeling and feature learning technique to map <b>words</b> into vectors of real numbers using neural networks, probabilistic models, or dimension reduction on the <b>word</b> co-occurrence matrix. Some <b>word</b> <b>embedding</b> models are Word2vec (Google), Glove (Stanford), and fastest (Facebook).", "dateLastCrawled": "2022-02-02T04:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Machine Learning</b> \u2014 Text Processing | by Javaid Nabi | Towards Data Science", "url": "https://towardsdatascience.com/machine-learning-text-processing-1d5a2d638958", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>machine-learning</b>-text-processing-1d5a2d638958", "snippet": "<b>Word</b> <b>Embedding</b>: It is a representation of text where <b>words</b> that have the same meaning have a similar representation. In other <b>words</b> it represents <b>words</b> in a <b>coordinate</b> <b>system</b> where related <b>words</b>, based on a corpus of relationships, are placed closer together. Let us discuss some of the well known models of <b>word</b> <b>embedding</b>: Word2Vec", "dateLastCrawled": "2022-02-03T04:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Geospatial and Semantic <b>Mapping</b> Platform for Massive COVID-19 ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7815194/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7815194", "snippet": "The <b>system</b> has been deployed as a web service for public access. Keywords ... an unsupervised learning algorithm for obtaining vector representations for <b>words</b>. With GLOVE, we can use pre-trained <b>word</b> <b>embedding</b> to represent <b>words</b>. The training is carried out on the aggregated global <b>word</b>-<b>word</b> co-occurrence statistics derived from a corpus, and the resulting representations showcase interesting linear substructures of the <b>word</b> vector space. Then, we can combine the meaning of each <b>word</b> in an ...", "dateLastCrawled": "2021-12-04T04:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "What does the <b>word</b> &#39;<b>embedding&#39; mean in the context</b> of Machine ... - Quora", "url": "https://www.quora.com/What-does-the-word-embedding-mean-in-the-context-of-Machine-Learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-does-the-<b>word</b>-<b>embedding-mean-in-the-context-of-Machine-Learning</b>", "snippet": "Answer (1 of 17): Embeddings are the only way one can transform discrete feature into a vector form. All machine learning algorithms take a vector and return a prediction. Therefore if you have a categorical feature, the only way you can use it in a ML model is by <b>embedding</b> it into a vector. Th...", "dateLastCrawled": "2021-12-19T03:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "PSDVec A toolbox for incremental and scalable <b>word</b> <b>embedding</b>", "url": "http://ml.cs.tsinghua.edu.cn/~jun/pub/PSDVec-2016.pdf", "isFamilyFriendly": true, "displayUrl": "ml.cs.tsinghua.edu.cn/~jun/pub/PSDVec-2016.pdf", "snippet": "PSDVec is a Python/Perl toolbox that learns <b>word</b> embeddings, i.e. the <b>mapping</b> of <b>words</b> in a natural language to continuous vectors which encode the semantic/syntactic regularities between the <b>words</b>. PSDVec implements a <b>word</b> <b>embedding</b> learning method based on a weighted low-rank positive semi-de\ufb01nite approximation. To scale up the learning process, we implement a blockwise online learning algorithm to learn the embeddings incrementally. This strategy greatly reduces the learning time of ...", "dateLastCrawled": "2021-11-21T07:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "deep learning - what actually <b>word</b> <b>embedding</b> dimensions values ...", "url": "https://datascience.stackexchange.com/questions/30075/what-actually-word-embedding-dimensions-values-represent", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/30075", "snippet": "Here I understand because each <b>word</b> is <b>mapping</b> to each column category label, But in the GloVe there are no columns labels for 50 columns, it just returns 50 values vector, so what actually these vectors represent and what i can do with it? I am trying to find this since 4-5 hours but everyone/every tutorial on the internet explaining what are <b>word</b> <b>embedding</b> and how they looks <b>like</b> but no one explaining what actually these weights represent? deep-learning nlp word2vec lstm <b>word</b>-embeddings ...", "dateLastCrawled": "2022-01-09T04:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "python - Semantically weighted mean of <b>word</b> embeddings - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/49059089/semantically-weighted-mean-of-word-embeddings", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/49059089", "snippet": "Given a list of <b>word</b> <b>embedding</b> vectors I&#39;m trying to calculate an average <b>word</b> <b>embedding</b> where some <b>words</b> are more meaningful than others. In other <b>words</b>, I want to calculate a semantically weighted <b>word</b> <b>embedding</b>. All the stuff I found is on just finding the mean vector (which is quite trivial of course) which represents the average meaning of the list OR some kind of weighted average of <b>words</b> for document representation, however that is not what I want. For example, given <b>word</b> vectors for ...", "dateLastCrawled": "2022-01-09T23:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "models.<b>word2vec</b> \u2013 <b>Word2vec</b> embeddings \u2014 <b>gensim</b>", "url": "https://radimrehurek.com/gensim/models/word2vec.html", "isFamilyFriendly": true, "displayUrl": "https://radimrehurek.com/<b>gensim</b>/models/<b>word2vec</b>.html", "snippet": "This object essentially contains the <b>mapping</b> between <b>words</b> and embeddings. After training, it can be used directly to query those embeddings in various ways. See the module level docstring for examples. Type . KeyedVectors. add_lifecycle_event (event_name, log_level = 20, ** event) \u00b6 Append an event into the lifecycle_events attribute of this object, and also optionally log the event at log_level. Events are important moments during the object\u2019s life, such as \u201cmodel created\u201d, \u201cmodel ...", "dateLastCrawled": "2022-02-02T21:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Is it possible to <b>use pre-computed word embeddings</b> with word2vec in ...", "url": "https://www.quora.com/Is-it-possible-to-use-pre-computed-word-embeddings-with-word2vec-in-Java-How-do-you-load-them", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Is-it-possible-to-<b>use-pre-computed-word-embeddings</b>-with-<b>word</b>2vec...", "snippet": "Answer: Approach from polygot is similar to same implemented in Python gensim. Coming directly to answer, yes you can load vectors from pickle object. Sample Python ...", "dateLastCrawled": "2022-01-14T10:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Embeddings | Machine Learning Crash Course | <b>Google Developers</b>", "url": "https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/machine-learning/crash-course/<b>embedding</b>", "snippet": "An <b>embedding</b> is a relatively low-dimensional space into which you can translate high-dimensional vectors. Embeddings make it easier to do machine learning on large inputs <b>like</b> sparse vectors representing <b>words</b>. Ideally, an <b>embedding</b> captures some of the semantics of the input by placing semantically similar inputs close together in the <b>embedding</b> space.", "dateLastCrawled": "2022-01-30T18:55:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Word Embedding Tutorial | Word2vec</b> Model Gensim Example", "url": "https://www.guru99.com/word-embedding-word2vec.html", "isFamilyFriendly": true, "displayUrl": "https://www.guru99.com/<b>word</b>-<b>embedding</b>-<b>word</b>2vec.html", "snippet": "<b>Word</b> <b>Embedding</b> is a <b>word</b> representation type that allows machine learning algorithms to understand <b>words</b> with <b>similar</b> meanings. It is a language modeling and feature learning technique to map <b>words</b> into vectors of real numbers using neural networks, probabilistic models, or dimension reduction on the <b>word</b> co-occurrence matrix. Some <b>word</b> <b>embedding</b> models are Word2vec (Google), Glove (Stanford), and fastest (Facebook).", "dateLastCrawled": "2022-02-02T04:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Machine Learning</b> \u2014 Text Processing | by Javaid Nabi | Towards Data Science", "url": "https://towardsdatascience.com/machine-learning-text-processing-1d5a2d638958", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>machine-learning</b>-text-processing-1d5a2d638958", "snippet": "<b>Word</b> <b>Embedding</b>: It is a representation of text where <b>words</b> that have the same meaning have a <b>similar</b> representation. In other <b>words</b> it represents <b>words</b> in a <b>coordinate</b> <b>system</b> where related <b>words</b>, based on a corpus of relationships, are placed closer together. Let us discuss some of the well known models of <b>word</b> <b>embedding</b>: Word2Vec. Word2vec takes as its input a large corpus of text and produces a vector space with each unique <b>word</b> being assigned a corresponding vector in the space. <b>Word</b> ...", "dateLastCrawled": "2022-02-03T04:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Text Classification Using <b>Word</b> <b>Embedding</b> in Based Methodologies: A ...", "url": "https://www.researchgate.net/profile/Alok-Mishra-34/publication/329270081_Text_Classification_Using_Word_Embedding_in_Rule-Based_Methodologies_A_Systematic_Mapping/links/5bffd57745851523d153964e/Text-Classification-Using-Word-Embedding-in-Rule-Based-Methodologies-A-Systematic-Mapping.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/profile/Alok-Mishra-34/publication/329270081_Text...", "snippet": "TEM Journal. Volume 7, Issue 4, Pages 902-914, ISSN 2217-8309, DOI: 10.18421/TEM74-31, November 2018. 902 TEM Journal \u2013 Volume 7 / Number 4 / 2018", "dateLastCrawled": "2021-11-19T16:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Measuring Semantic Changes Using Temporal <b>Word</b> <b>Embedding</b>", "url": "https://towardsdatascience.com/measuring-semantic-changes-using-temporal-word-embedding-6fc3f16cfdb4", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/measuring-semantic-changes-using-temporal-<b>word</b>...", "snippet": "Distributional semantics is built on the assumption that <b>words</b> occurring in the same contexts tend to have <b>similar</b> meanings. Linguist John Firth, a pioneer in this field, described it as such in his famous statement: \u201cYou shall know a <b>word</b> by the company it keeps\u201d (Firth, 1957). The example below shows the evolution of three <b>words</b> over several decades, described in the paper written by Hamilton et al. (2016). We can evaluate the contextual <b>words</b> most commonly associated with a keyword at ...", "dateLastCrawled": "2022-02-02T13:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Research Article Fracture Mechanics Method for <b>Word</b> <b>Embedding</b> ...", "url": "https://downloads.hindawi.com/journals/cin/2016/3506261.pdf", "isFamilyFriendly": true, "displayUrl": "https://downloads.hindawi.com/journals/cin/2016/3506261.pdf", "snippet": "<b>Word</b> <b>embedding</b> is a <b>word</b> numerical representation that uses a continuousvector space to represent a group of <b>words</b> [] . In the <b>word</b> vector space, each <b>word</b> corresponds to a uniquepoint.Intuitively,thosepointsthathavesimilarmean-ingsshouldbeputclose,whilethosewhoaredistantinmean-ingshouldbeputfaraway.Basedonthespace,thedegree", "dateLastCrawled": "2021-09-17T21:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Text Processing/NLP using Python - GitHub", "url": "https://github.com/reshma78611/Text-processing-NLP-using-Python", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/reshma78611/Text-processing-NLP-using-Python", "snippet": "To solve this problem we use another approach called <b>Word</b> <b>Embedding</b>. 3. <b>Word</b> <b>Embedding</b>: It is a representation of text where <b>words</b> that have the same meaning have a <b>similar</b> representation. In other <b>words</b> it represents <b>words</b> in a <b>coordinate</b> <b>system</b> where related <b>words</b>, based on a corpus of relationships, are placed closer together. One of the <b>word</b> <b>embedding</b> method is Word2Vec. Word2Vec: Word2vec takes as its input a large corpus of text and produces a vector space with each unique <b>word</b> being ...", "dateLastCrawled": "2022-02-02T13:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "NLP with R part 2: Training <b>Word</b> <b>Embedding</b> models and visualize results ...", "url": "https://medium.com/broadhorizon-cmotions/nlp-with-r-part-2-training-word-embedding-models-and-visualize-results-ae444043e234", "isFamilyFriendly": true, "displayUrl": "https://medium.com/broadhorizon-cmotions/nlp-with-r-part-2-training-<b>word</b>-<b>embedding</b>...", "snippet": "The result is a <b>coordinate</b> <b>system</b> where related <b>words</b> are placed closer together. We are going to use two different <b>word</b> <b>embedding</b> techniques on data we gathered on restaurant reviews. This image ...", "dateLastCrawled": "2022-01-29T01:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "python - Semantically weighted mean of <b>word</b> embeddings - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/49059089/semantically-weighted-mean-of-word-embeddings", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/49059089", "snippet": "Given a list of <b>word</b> <b>embedding</b> vectors I&#39;m trying to calculate an average <b>word</b> <b>embedding</b> where some <b>words</b> are more meaningful than others. In other <b>words</b>, I want to calculate a semantically weighted <b>word</b> <b>embedding</b>. All the stuff I found is on just finding the mean vector (which is quite trivial of course) which represents the average meaning of the list OR some kind of weighted average of <b>words</b> for document representation, however that is not what I want. For example, given <b>word</b> vectors for ...", "dateLastCrawled": "2022-01-09T23:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "models.<b>word2vec</b> \u2013 <b>Word2vec</b> embeddings \u2014 <b>gensim</b>", "url": "https://radimrehurek.com/gensim/models/word2vec.html", "isFamilyFriendly": true, "displayUrl": "https://radimrehurek.com/<b>gensim</b>/models/<b>word2vec</b>.html", "snippet": "&gt;&gt;&gt; vector = model. wv [&#39;computer&#39;] # get numpy vector of a <b>word</b> &gt;&gt;&gt; sims = model. wv. most_<b>similar</b> (&#39;computer&#39;, topn = 10) # get other <b>similar</b> <b>words</b> The reason for separating the trained vectors into KeyedVectors is that if you don\u2019t need the full model state any more (don\u2019t need to continue training), its state can discarded, keeping just the vectors and their keys proper.", "dateLastCrawled": "2022-02-02T21:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "In layman terms, how would you explain the Skip-Gram <b>word</b> <b>embedding</b> ...", "url": "https://www.quora.com/In-layman-terms-how-would-you-explain-the-Skip-Gram-word-embedding-model-in-natural-language-processing-NLP", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/In-layman-terms-how-would-you-explain-the-Skip-Gram-<b>word</b>...", "snippet": "Answer: The goal of <b>word</b> <b>embedding</b> models is to learn a high-dimensional dense representation for each vocabulary term in which the similarity between <b>embedding</b> vectors shows the semantic or syntactic similarity between the corresponding <b>words</b>. Skip-gram is a model for learning <b>word</b> <b>embedding</b> alg...", "dateLastCrawled": "2022-01-11T22:10:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Predicting the pandemic: sentiment evaluation and predictive analysis ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8007226/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8007226", "snippet": "The sophisticated mechanism behind the <b>word</b> level <b>embedding</b> is to <b>mapping</b> n number of <b>words</b> in an arbitrary dimensional space, while the <b>words</b> conveying similar contextual semantic meaning are the neighbors of those <b>words</b> . Though the closeness of the <b>words</b> implies different contextual meanings in cross-lingual applications, and for that several vector spaces <b>can</b> be applied concurrently with each other for forming a semantic combination. Also, for cross-lingual vocabulary, two similar ...", "dateLastCrawled": "2022-01-09T20:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "keras - Merging a <b>word</b> <b>embedding</b> trained on a specialized topic to pre ...", "url": "https://stackoverflow.com/questions/47059510/merging-a-word-embedding-trained-on-a-specialized-topic-to-pre-trained-word-embe", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/47059510/merging-a-<b>word</b>-<b>embedding</b>-trained-on-a...", "snippet": "The pretrained vectors contain more <b>words</b>, but my <b>word</b> vectors have better representations for medical terms. I was to fusion the two set of embeddings. Glove (200d) has 4 million terms, and about 10% of these are also found in my own <b>embedding</b> (also 200d). Instead of something simple like concatenating the two (which would result in a lot of 0s), I was wondering if creating a neural network that maps a vector from the Glove space to my own <b>embedding</b> space would help. Specially: from keras ...", "dateLastCrawled": "2022-01-23T10:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "smallscaled: <b>Lorenz, Takens, and Forecasting</b>", "url": "https://www.smallscaled.com/2018/07/lorenz-takens-and-forecasting.html", "isFamilyFriendly": true, "displayUrl": "https://www.smallscaled.com/2018/07/<b>lorenz-takens-and-forecasting</b>.html", "snippet": "This operation of <b>mapping</b> the scalar time series on an \\(m\\) dimensional space is called <b>embedding</b>, with <b>embedding</b> dimension being \\(m\\). Although the <b>word</b> &quot;diffeomorphism&quot; sounds a bit scary, it is actually quite common in image processing. As an example, a fish eye filter <b>can</b> <b>be thought</b> as a <b>mapping</b> from a disc to a sphere, is a ...", "dateLastCrawled": "2022-01-21T11:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Using <b>word</b> embeddings to generate data-driven human agent decision ...", "url": "https://link.springer.com/article/10.1007/s10707-019-00345-2", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10707-019-00345-2", "snippet": "<b>Word</b> embeddings are vector-based representations of <b>words</b> based on <b>word</b> co-occurrence ... VSMs in general and <b>word</b> embeddings in particular are an attempt to solve a representational problem \u2013 how <b>can</b> language, a highly nonlinear <b>system</b>, be functionally mapped into a linear <b>system</b> that is easily used by a computer while maintaining the semantic relationships between <b>words</b> as understood by the <b>word</b> users? While undoubtedly still a simplification compared to human interpretation of textual ...", "dateLastCrawled": "2022-01-05T21:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Lecture Notes on General Relativity - S. Carroll", "url": "https://ned.ipac.caltech.edu/level5/March01/Carroll3/Carroll2.html", "isFamilyFriendly": true, "displayUrl": "https://ned.ipac.caltech.edu/level5/March01/Carroll3/Carroll2.html", "snippet": "A chart or <b>coordinate</b> <b>system</b> consists of a subset U of a set M, along with a one-to-one map : U, such that the image (U) ... In fact any n-dimensional manifold <b>can</b> be embedded in (&quot;Whitney&#39;s <b>embedding</b> theorem&quot;), and sometimes we will make use of this fact (such as in our definition of the sphere above). But it&#39;s important to recognize that the manifold has an individual existence independent of any <b>embedding</b>. We have no reason to believe, for example, that four-dimensional spacetime is stuck ...", "dateLastCrawled": "2022-02-03T08:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Learning to link images with their descriptions | by Heuritech ...", "url": "https://medium.com/heuritech/learning-to-link-images-with-their-descriptions-d05f8c703f20?source=post_internal_links---------5----------------------------", "isFamilyFriendly": true, "displayUrl": "https://medium.com/heuritech/learning-to-link-images-with-their-descriptions-d05f8c703...", "snippet": "If we pair each <b>word</b> with an <b>embedding</b> (such as Word2Vec), the sum of all embeddings in a sentence, A learned combination of <b>word</b> embeddings. We focus on the last bullet-point. Skip-<b>thought</b> ...", "dateLastCrawled": "2021-05-21T09:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Semantic Concept Spaces: Guided Topic Model Refinement using <b>Word</b> ...", "url": "https://www.researchgate.net/publication/335354969_Semantic_Concept_Spaces_Guided_Topic_Model_Refinement_using_Word-Embedding_Projections", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/335354969_Semantic_Concept_Spaces_Guided...", "snippet": "These tasks are supported by an interactive visual analytics workspace that uses <b>word</b>-<b>embedding</b> projections to define concept regions which <b>can</b> then be refined. The user-refined concepts are ...", "dateLastCrawled": "2021-12-29T17:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "What is <b>embedding matrix in deep learning</b>? - Quora", "url": "https://www.quora.com/What-is-embedding-matrix-in-deep-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>embedding-matrix-in-deep-learning</b>", "snippet": "Answer (1 of 2): In problems involving inputs from discrete domains (<b>words</b> in a sentence, nodes in a network), we usually use one-of-k encoding to represent our inputs. To give a quick example, consider the sentence: I like dog and a very small dictionary, defined on the fly, to represent our s...", "dateLastCrawled": "2022-01-22T19:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "differential geometry - Are measurements using the first fundamental ...", "url": "https://math.stackexchange.com/questions/3608934/are-measurements-using-the-first-fundamental-form-preserved-by-re-parameterizati", "isFamilyFriendly": true, "displayUrl": "https://math.stackexchange.com/questions/3608934/are-measurements-using-the-first...", "snippet": "A <b>mapping</b> of a portion of a manifold M to a portion of a manifold N is called isometric, if the length of any curve on N is the same as the length of its pre-image on M. In other <b>words</b>, arclength is preserved by isometric transformations. Spherical map projections are provably not isometric ...", "dateLastCrawled": "2022-01-18T01:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>What is supply chain management</b>? | <b>IBM</b>", "url": "https://www.ibm.com/topics/supply-chain-management", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ibm.com</b>/topics", "snippet": "<b>Coordinate</b> customer orders, schedule deliveries, dispatch loads, invoice customers and receive payments. Returning Create a network or process to take back defective, excess or unwanted products. Why is supply chain management important? Effective supply chain management systems minimize cost, waste and time in the production cycle. The industry standard has become a just-in-time supply chain where retail sales automatically signal replenishment orders to manufacturers. Retail shelves <b>can</b> ...", "dateLastCrawled": "2022-02-02T23:34:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Measuring Semantic Changes Using Temporal <b>Word</b> <b>Embedding</b>", "url": "https://towardsdatascience.com/measuring-semantic-changes-using-temporal-word-embedding-6fc3f16cfdb4", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/measuring-semantic-changes-using-temporal-<b>word</b>...", "snippet": "You <b>can</b>\u2019t average the embeddings across models becasue they are not in the same <b>embedding</b> space, but you <b>can</b> look at the nearest neighbors for the same <b>words</b> across different <b>embedding</b> spaces. For example, if you have trained on a corpus of newspaper articles about the pandemic and are interested in how the <b>word</b> \u201cmask\u201d has evolved over time, you <b>can</b> determine the top 50 nearest neighbors for the <b>word</b> \u201cmask\u201d for each model, then calculate the Jaccard similarity across these nearest ...", "dateLastCrawled": "2022-02-02T13:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Text Classification Using <b>Word</b> <b>Embedding</b> in Based Methodologies: A ...", "url": "https://www.researchgate.net/profile/Alok-Mishra-34/publication/329270081_Text_Classification_Using_Word_Embedding_in_Rule-Based_Methodologies_A_Systematic_Mapping/links/5bffd57745851523d153964e/Text-Classification-Using-Word-Embedding-in-Rule-Based-Methodologies-A-Systematic-Mapping.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/profile/Alok-Mishra-34/publication/329270081_Text...", "snippet": "In this paper, a systematic <b>mapping</b> study is a way to survey all the primary studies on <b>word</b> <b>embedding</b> to rule-based and machine learning of automatic text classification. The search procedure ...", "dateLastCrawled": "2021-11-19T16:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A deep learning framework combined with <b>word</b> <b>embedding</b> to identify DNA ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7804333/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7804333", "snippet": "In addition, the principal component analysis (PCA) 49 and the T-distributed neighborhood <b>embedding</b> algorithm (T-SNE) 50 <b>can</b> be adopted to further reduce the dimension of the distributed representation in the <b>word</b> <b>embedding</b> space, thereby realizing the visualization of <b>word</b> <b>embedding</b> and <b>word</b> meaning induction. In view of this, <b>word</b> <b>embedding</b> technology is utilized in this paper to realize the distribution representation.", "dateLastCrawled": "2022-01-18T14:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Fracture Mechanics Method for <b>Word</b> <b>Embedding</b> Generation of Neural ...", "url": "https://www.hindawi.com/journals/cin/2016/3506261/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.hindawi.com</b>/journals/cin/2016/3506261", "snippet": "<b>Word</b> <b>embedding</b>, a lexical vector representation generated via the neural linguistic model (NLM), is empirically demonstrated to be appropriate for improvement of the performance of traditional language model. However, the supreme dimensionality that is inherent in NLM contributes to the problems of hyperparameters and long-time training in modeling. Here, we propose a force-directed method to improve such problems for simplifying the generation of <b>word</b> <b>embedding</b>. In this framework, each <b>word</b> ...", "dateLastCrawled": "2021-12-24T03:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "LogEvent2vec: LogEvent-to-Vector Based Anomaly Detection for Large ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7249657/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7249657", "snippet": "Bertero et al. consider logs as regular text and first apply a <b>word</b> <b>embedding</b> technique based on Google\u2019s word2vec algorithm, in which logfiles\u2019 <b>words</b> are mapped to a high dimensional metric space. Then, the <b>coordinate</b> of the <b>word</b> is transformed into the log event vector, and the <b>coordinate</b> of the log event vector is transformed into the log sequence vector. Meng et al. propose LogAnomaly, a framework to model a log stream as a natural language sequence. They propose a novel, simple ...", "dateLastCrawled": "2022-01-28T04:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Research Article Fracture Mechanics Method for <b>Word</b> <b>Embedding</b> ...", "url": "https://downloads.hindawi.com/journals/cin/2016/3506261.pdf", "isFamilyFriendly": true, "displayUrl": "https://downloads.hindawi.com/journals/cin/2016/3506261.pdf", "snippet": "is substituted by a particle <b>system</b>, which <b>can</b> simulate the correlatingprocess of semanticity between <b>words</b>. Our speci c contributionsare as follows: (i) We propose a force-directed model that is based on fracture mechanics to generate <b>word</b> <b>embedding</b>. A linear elastic fracture model is introduced to control the varying progress of <b>word</b> semantic.", "dateLastCrawled": "2021-09-17T21:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "python - Semantically weighted mean of <b>word</b> embeddings - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/49059089/semantically-weighted-mean-of-word-embeddings", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/49059089", "snippet": "Given a list of <b>word</b> <b>embedding</b> vectors I&#39;m trying to calculate an average <b>word</b> <b>embedding</b> where some <b>words</b> are more meaningful than others. In other <b>words</b>, I want to calculate a semantically weighted <b>word</b> <b>embedding</b>. All the stuff I found is on just finding the mean vector (which is quite trivial of course) which represents the average meaning of the list OR some kind of weighted average of <b>words</b> for document representation, however that is not what I want. For example, given <b>word</b> vectors for ...", "dateLastCrawled": "2022-01-09T23:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "What does the <b>word</b> &#39;<b>embedding&#39; mean in the context</b> of Machine ... - Quora", "url": "https://www.quora.com/What-does-the-word-embedding-mean-in-the-context-of-Machine-Learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-does-the-<b>word</b>-<b>embedding-mean-in-the-context-of-Machine-Learning</b>", "snippet": "Answer (1 of 17): Embeddings are the only way one <b>can</b> transform discrete feature into a vector form. All machine learning algorithms take a vector and return a prediction. Therefore if you have a categorical feature, the only way you <b>can</b> use it in a ML model is by <b>embedding</b> it into a vector. Th...", "dateLastCrawled": "2021-12-19T03:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Lexicon Creation for Financial Sentiment Analysis Using Network <b>Embedding</b>", "url": "https://www.scirp.org/Journal/PaperInformation.aspx?PaperID=80395", "isFamilyFriendly": true, "displayUrl": "https://www.scirp.org/Journal/PaperInformation.aspx?PaperID=80395", "snippet": "Figure 2 is a <b>mapping</b> of the distributed representation of <b>words</b> obtained by LINE on a two-dimensional plane. As <b>can</b> be seen, the mapped distributed representation is divided into upper and lower parts. The <b>words</b> in the upper part include <b>words</b>, such as adverbs and numbers that are likely to be depended from the other <b>words</b>. In other <b>words</b>, the ...", "dateLastCrawled": "2022-02-02T09:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Embedding geographic information for anomalous trajectory detection</b> ...", "url": "https://link.springer.com/article/10.1007/s11280-020-00812-z", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11280-020-00812-z", "snippet": "Trajectory <b>Embedding</b> is an extended field of <b>word</b> <b>embedding</b> [20, 29] which represents a <b>word</b> as a fixed length numerical vector and the co-occurrence of <b>words</b> <b>can</b> be learned from the embedded vector. A large number of remarkable works have extended <b>word</b> <b>embedding</b> to other fields such as network <b>embedding</b> [43, 45].", "dateLastCrawled": "2022-01-21T18:41:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Word</b> Embeddings, Analogies, and <b>Machine</b> <b>Learning</b>: Beyond king - man ...", "url": "https://aclanthology.org/C16-1332.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/C16-1332.pdf", "snippet": "<b>Word</b> Embeddings, Analogies, and <b>Machine</b> <b>Learning</b>: Beyond King - Man + Woman = Queen Aleksandr Drozd y, Anna Gladkova z, Satoshi Matsuoka y yTokyo Institute of Technology, Meguro-ku, Tokyo 152-8550, Japan alex@smg.is.titech.ac.jp, matsu@is.titech.ac.jp z The University of Tokyo, Meguro-ku, Tokyo 153-8902 Japan gladkova@phiz.c.u-tokyo.ac.jp Abstract Solving <b>word</b> analogies became one of the most popular benchmarks for <b>word</b> embeddings on the assumption that linear relations between <b>word</b> pairs ...", "dateLastCrawled": "2022-01-20T00:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Word</b> Embeddings, Analogies, and <b>Machine</b> <b>Learning</b>: Beyond king - man ...", "url": "https://aclanthology.org/C16-1332/", "isFamilyFriendly": true, "displayUrl": "https://acl<b>anthology</b>.org/C16-1332", "snippet": "\ufeff%0 Conference Proceedings %T <b>Word</b> Embeddings, Analogies, and <b>Machine</b> <b>Learning</b>: Beyond king - man + woman = queen %A Drozd, Aleksandr %A Gladkova, Anna %A Matsuoka, Satoshi %S Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers %D 2016 %8 dec %I The COLING 2016 Organizing Committee %C Osaka, Japan %F drozd-etal-2016-<b>word</b> %X Solving <b>word</b> analogies became one of the most popular benchmarks for <b>word</b> embeddings on the assumption that ...", "dateLastCrawled": "2022-01-17T09:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>Word</b> Embeddings, Analogies, and <b>Machine</b> <b>Learning</b>: Beyond King ...", "url": "https://www.researchgate.net/publication/311843169_Word_Embeddings_Analogies_and_Machine_Learning_Beyond_King_-_Man_Woman_Queen", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/311843169_<b>Word</b>_<b>Embeddings</b>_Analogies_and...", "snippet": "<b>Word</b> Embeddings, Analogies, and <b>Machine</b> <b>Learning</b>: Beyond King - Man+ Woman= Queen December 2016 Conference: Proceedings of COLING 2016, the 26th International Conference on Computational ...", "dateLastCrawled": "2021-11-25T14:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>What Are Word Embeddings</b> for Text? - <b>Machine</b> <b>Learning</b> Mastery", "url": "https://machinelearningmastery.com/what-are-word-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>what-are-word-embeddings</b>", "snippet": "<b>Word</b> embeddings are a type of <b>word</b> representation that allows words with similar meaning to have a similar representation. They are a distributed representation for text that is perhaps one of the key breakthroughs for the impressive performance of deep <b>learning</b> methods on challenging natural language processing problems. In this post, you will discover the <b>word</b> <b>embedding</b> approach for representing text data. After", "dateLastCrawled": "2022-01-30T18:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Finding the <b>Word</b> <b>Analogy</b> from given words using Word2Vec embeddings ...", "url": "https://www.geeksforgeeks.org/finding-the-word-analogy-from-given-words-using-word2vec-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/finding-the-<b>word</b>-<b>analogy</b>-from-given-<b>words</b>-using-<b>word</b>2vec...", "snippet": "What if we can use a <b>Machine</b> <b>Learning</b> algorithm to automate this task of finding the <b>word</b> <b>analogy</b>. In ... Overview of <b>Word</b> <b>Embedding</b> using Embeddings from Language Models (ELMo) 16, Mar 21. <b>Word</b> Embeddings in NLP. 11, Oct 20. Implement your own word2vec(skip-gram) model in Python. 18, Jan 19. Scraping And Finding Ordered Words In A Dictionary using Python. 23, Jul 17 . Python - Replace all words except the given <b>word</b>. 25, Sep 20. Python | Finding &#39;n&#39; Character Words in a Text File. 15, Oct ...", "dateLastCrawled": "2022-01-26T14:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "How to Solve <b>Analogies</b> with Word2Vec | by Khuyen Tran | Towards Data ...", "url": "https://towardsdatascience.com/how-to-solve-analogies-with-word2vec-6ebaf2354009", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/how-to-solve-<b>analogies</b>-with-<b>word</b>2vec-6ebaf2354009", "snippet": "To make words understood by <b>machine</b> <b>learning</b> algorithms, <b>word</b> <b>embedding</b> is used to map words into vectors of real numbers. There are various <b>word</b> <b>embedding</b> models and word2vec is one of them. In simple words, word2vec is a group of related models that are used to produce <b>word</b> embeddings. These models are trained to construct the linguistic contexts of words. Word2vec takes a large corpus of text and produces a vector space, with each unique <b>word</b> in the corpus being assigned to a ...", "dateLastCrawled": "2022-01-30T16:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "GitHub - jungsoh/<b>word</b>-embeddings-<b>word</b>-<b>analogy</b>-by-document-similarity ...", "url": "https://github.com/jungsoh/word-embeddings-word-analogy-by-document-similarity", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/jungsoh/<b>word</b>-<b>embeddings</b>-<b>word</b>-<b>analogy</b>-by-document-similarity", "snippet": "An example of a <b>word</b> <b>analogy</b> problem is to fill in the blank: Man is to Woman as King is to _____`. Because <b>word</b> embeddings are very computationally expensive to train, most <b>machine</b> <b>learning</b> practitioners will load a pre-trained set of embeddings. We will load a collection of pre-trained embeddings and measure similarity between <b>word</b> embeddings ...", "dateLastCrawled": "2022-01-25T02:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Word2Vec in Gensim Explained for Creating <b>Word</b> <b>Embedding</b> Models ...", "url": "https://machinelearningknowledge.ai/word2vec-in-gensim-explained-for-creating-word-embedding-models-pretrained-and-custom/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>knowledge.ai/<b>word</b>2vec-in-gensim-explained-for-creating-<b>word</b>...", "snippet": "What is <b>Word</b> Embeddings? <b>Machine</b> <b>learning</b> and ... This is another way putting that word2vec can draw the <b>analogy</b> that if Man is to Woman then Kind is to Queen! The publicly released model of word2vec by Google consists of 300 features and the model is trained in the Google news dataset. The vocabulary size of the model is around 1.6 billion words. However, this might have taken a huge time for the model to be trained on but they have applied a method of simple subsampling approach to ...", "dateLastCrawled": "2022-02-02T15:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Introduction to <b>Word Embeddings</b>. What is a <b>word</b> <b>embedding</b>? | by Hunter ...", "url": "https://towardsdatascience.com/introduction-to-word-embeddings-4cf857b12edc", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/introduction-to-<b>word-embeddings</b>-4cf857b12edc", "snippet": "A very basic definition of a <b>word</b> <b>embedding</b> is a real number, vector representation of a <b>word</b>. Typically, these days, words with similar meaning will have vector representations that are close together in the <b>embedding</b> space (though this hasn\u2019t always been the case). When constructing a <b>word</b> <b>embedding</b> space, typically the goal is to capture some sort of relationship in that space, be it meaning, morphology, context, or some other kind of relationship. By encoding <b>word embeddings</b> in a ...", "dateLastCrawled": "2022-01-30T04:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>machine</b> <b>learning</b> - <b>Text classification</b> using <b>word</b> embeddings - Stack ...", "url": "https://stackoverflow.com/questions/60929359/text-classification-using-word-embeddings", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/60929359/<b>text-classification</b>-using-<b>word</b>-<b>embeddings</b>", "snippet": "A quick <b>word</b> of encouragement, I recall feeling precisely the same way; insanely frustrated when I started <b>learning</b> this field. It really does get easier! <b>Word</b> embeddings are created by unsupervised <b>learning</b>. However, you can use a trained <b>embedding</b> layer within a supervised projected, like you&#39;re doing. In other words, your project is one of supervised <b>learning</b>, and one of the layers is using weights that were acquired by an unsupervised training technique.", "dateLastCrawled": "2022-01-24T14:57:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Word Embeddings Explained. What is <b>Word Embedding</b> ? | by Ashwin Prasad ...", "url": "https://medium.com/analytics-vidhya/word-embeddings-explained-62c046f7c79e", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/word-<b>embedding</b>s-explained-62c046f7c79e", "snippet": "<b>Word Embedding</b> is a technique in Natural Language Processing which is used to represent words in a Deep <b>Learning</b> environment. The main advantage of using <b>word embedding</b> is that it allows words of\u2026", "dateLastCrawled": "2022-01-24T11:15:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Survey and challenges of story generation models - A multimodal ...", "url": "https://www.sciencedirect.com/science/article/pii/S156625352030378X", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S156625352030378X", "snippet": "Pang et al. used the deep Bolzmann <b>machine</b> (DBM), which is a joint density model for the visual, auditory, and textual modalities, for <b>learning</b> highly non-linear relations between low-level features across different modalities for emotional prediction. It is trained using joint representation over multimodal inputs; thus, it can handle training samples, which is absent from certain modality. It can be used for emotional prediction and retrieval on any combination of modalities.", "dateLastCrawled": "2022-01-24T04:42:00.0000000Z", "language": "en", "isNavigational": false}], [], []], "all_bing_queries": ["+(word embedding)  is like +(mapping words to a coordinate system)", "+(word embedding) is similar to +(mapping words to a coordinate system)", "+(word embedding) can be thought of as +(mapping words to a coordinate system)", "+(word embedding) can be compared to +(mapping words to a coordinate system)", "machine learning +(word embedding AND analogy)", "machine learning +(\"word embedding is like\")", "machine learning +(\"word embedding is similar\")", "machine learning +(\"just as word embedding\")", "machine learning +(\"word embedding can be thought of as\")", "machine learning +(\"word embedding can be compared to\")"]}