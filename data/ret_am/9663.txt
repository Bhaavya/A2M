{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Counterfactual</b> <b>Fairness</b>: Unidentification, Bound and <b>Algorithm</b> (Journal ...", "url": "https://par.nsf.gov/biblio/10126321-counterfactual-fairness-unidentification-bound-algorithm", "isFamilyFriendly": true, "displayUrl": "https://par.nsf.gov/biblio/10126321", "snippet": "<b>Fairness</b>-aware <b>learning</b> studies the problem of building <b>machine</b> <b>learning</b> models that are subject to <b>fairness</b> requirements. <b>Counterfactual</b> <b>fairness</b> is a notion of <b>fairness</b> derived from Pearl&#39;s causal model, which considers a model is fair if for a particular individual or group its prediction in the real world is the same as that in the <b>counterfactual</b> world where the individual(s) had belonged to a different demographic group.", "dateLastCrawled": "2022-01-21T17:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Counterfactual Reasoning in Algorithmic Fairness</b>", "url": "http://fairware.cs.umass.edu/slides/silva.pdf", "isFamilyFriendly": true, "displayUrl": "fairware.cs.umass.edu/slides/silva.pdf", "snippet": "\u2022 We would <b>like</b> to predict Y in a \u201cfair\u201d way, meaning that our predictions should not be \u201cbiased\u201d against particular instances of A. Primitives ... \u2022 Choose any <b>machine</b> <b>learning</b> <b>algorithm</b> of interest, any black-box that takes as inputs observed and unobserved variables in your domain. \u2013 Select a set of variables based on which sets respect <b>counterfactual</b> <b>fairness</b>. \u2013 If necessary, infer unobserved variables from the observed ones. Some Words of Caution \u2022 Structural equations ...", "dateLastCrawled": "2021-11-20T16:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Counterfactual Fairness</b> | DeepAI", "url": "https://deepai.org/publication/counterfactual-fairness", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>counterfactual-fairness</b>", "snippet": "One simple but important implication of the definition of <b>counterfactual fairness</b> is the following: Lemma 1. Let G be the causal graph of the given model (U,V,F). Then ^Y will be counterfactually fair if it is a function of the non-descendants of A. Proof. Let W be any non-descendant of A in G. Then W A\u2190a(U) and W A\u2190a.", "dateLastCrawled": "2022-01-26T15:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Algorithmic Bias: A <b>Counterfactual</b> Perspective", "url": "https://bitlab.cas.msu.edu/trustworthy-algorithms/whitepapers/Bo%20Cowgill.pdf", "isFamilyFriendly": true, "displayUrl": "https://bitlab.cas.msu.edu/trustworthy-<b>algorithm</b>s/whitepapers/Bo Cowgill.pdf", "snippet": "We discuss an alternative approach to measuring bias and <b>fairness</b> in <b>machine</b> <b>learning</b>: <b>Counterfactual</b> evaluation. In many practical settings, the alternative to a biased <b>algorithm</b> is not an unbiased one, but another decision method such as another <b>algorithm</b> or human discretion. We discuss statistical techniques necessary for <b>counterfactual</b> comparisons, which enable researchers to quantify relative biases without access to the underlying <b>algorithm</b> or its training data. We close by discussing ...", "dateLastCrawled": "2022-01-31T05:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) <b>Counterfactual Fairness</b> - ResearchGate", "url": "https://www.researchgate.net/publication/315454664_Counterfactual_Fairness", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/315454664_<b>Counterfactual_Fairness</b>", "snippet": "way of assessing an existing decision making process, it is not as natural as <b>counterfactual fairness</b> in. the context of <b>machine</b> <b>learning</b>. Approximate <b>fairness</b> and model validation. The notion of ...", "dateLastCrawled": "2022-01-26T14:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Counterfactual</b> Explanations for <b>Machine</b> <b>Learning</b>: A Review", "url": "https://ml-retrospectives.github.io/neurips2020/camera_ready/5.pdf", "isFamilyFriendly": true, "displayUrl": "https://ml-retrospectives.github.io/neurips2020/camera_ready/5.pdf", "snippet": "<b>Counterfactual</b> Explanations for <b>Machine</b> <b>Learning</b>: A Review Sahil Verma Arthur AI, University of Washington Washington D.C., USA vsahil@cs.washington.edu John Dickerson Arthur AI Washington D.C., USA john@arthur.ai Keegan Hines Arthur AI Washington D.C., USA keegan@arthur.ai Abstract <b>Machine</b> <b>learning</b> plays a role in many deployed decision systems, often in ways that are dif\ufb01cult or impossible to understand by human stakeholders. Explaining, in a human-understandable way, the relationship ...", "dateLastCrawled": "2022-01-29T04:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) <b>Counterfactual Fairness</b> - ResearchGate", "url": "https://www.researchgate.net/publication/324600593_Counterfactual_Fairness", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/324600593_<b>Counterfactual_Fairness</b>", "snippet": "the <b>machine</b> <b>learning</b> literature, the goal of this experiment is to quantify how our <b>algorithm</b> behav es with \ufb01nite sample sizes while assuming ground truth compatible with a synthetic model ...", "dateLastCrawled": "2021-12-11T00:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Reviews: <b>Counterfactual</b> <b>Fairness</b>", "url": "https://papers.nips.cc/paper/2017/file/a486cd07e4ac3d270571622f4f316ec5-Reviews.html", "isFamilyFriendly": true, "displayUrl": "https://papers.nips.cc/paper/2017/file/a486cd07e4ac3d270571622f4f316ec5-Reviews.html", "snippet": "This paper presents an interesting and valuable contribution to the small but growing literature on <b>fairness</b> in <b>machine</b> <b>learning</b>. Specifically, it provides at least three contributions: (1) a definition of <b>counter factual</b> <b>fairness</b>; (2) an <b>algorithm</b> for <b>learning</b> a model under <b>counter factual</b> <b>fairness</b>; and (3) experiments with that <b>algorithm</b>. The value and convincingness of each of these contributions declines steadily. The value of the contributions of the current paper is sufficient for ...", "dateLastCrawled": "2021-11-21T05:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A Tutorial on <b>Fairness</b> in <b>Machine</b> <b>Learning</b> | by Ziyuan Zhong | Towards ...", "url": "https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-tutorial-on-<b>fairness</b>-in-<b>machine</b>-<b>learning</b>-3ff8ba1040cb", "snippet": "<b>Machine</b> <b>Learning</b>, the most widely used AI techniques, relies heavily on data. It is a common misconception that AI is absolutely objective. AI is objective only in the sense of <b>learning</b> what human teaches. The data provided by human can be highly-biased. It has been found in 2016 that COMPAS, the <b>algorithm</b> used for recidivism prediction produces much higher false positive rate for black people than white people(see Fig2,", "dateLastCrawled": "2022-02-01T18:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "9.3 <b>Counterfactual</b> Explanations | Interpretable <b>Machine</b> <b>Learning</b>", "url": "https://christophm.github.io/interpretable-ml-book/counterfactual.html", "isFamilyFriendly": true, "displayUrl": "https://christophm.github.io/interpretable-ml-book/<b>counterfactual</b>.html", "snippet": "In interpretable <b>machine</b> <b>learning</b>, <b>counterfactual</b> explanations can be used to explain predictions of individual instances. The \u201cevent\u201d is the predicted outcome of an instance, the \u201ccauses\u201d are the particular feature values of this instance that were input to the model and \u201ccaused\u201d a certain prediction. Displayed as a graph, the relationship between the inputs and the prediction is very simple: The feature values cause the prediction. FIGURE 9.9: The causal relationships between ...", "dateLastCrawled": "2022-02-03T02:12:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Counterfactual Fairness</b> - NIPS", "url": "https://papers.nips.cc/paper/2017/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://papers.nips.cc/paper/2017/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf", "snippet": "<b>Counterfactual Fairness</b> Matt Kusner The Alan Turing Institute and University of Warwick mkusner@turing.ac.uk Joshua Loftus New York University loftus@nyu.edu Chris Russell The Alan Turing Institute and University of Surrey crussell@turing.ac.uk Ricardo Silva The Alan Turing Institute and University College London ricardo@stats.ucl.ac.uk Abstract <b>Machine</b> <b>learning</b> can impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending ...", "dateLastCrawled": "2022-01-26T15:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Counterfactual Fairness</b> | DeepAI", "url": "https://deepai.org/publication/counterfactual-fairness", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>counterfactual-fairness</b>", "snippet": "In this paper, we introduce the first explicitly causal approach to address <b>fairness</b>. Specifically, we leverage the causal framework of Pearl to model the relationship between protected attributes and data. We describe how techniques from causal inference can be effective tools for designing fair algorithms and argue, as in DeDeo (), that it is essential to properly address causality in <b>fairness</b>.In perhaps the most closely related prior work, Johnson et al. make <b>similar</b> arguments but from a ...", "dateLastCrawled": "2022-01-26T15:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Counterfactual</b> <b>Fairness</b> \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/1703.06856/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1703.06856", "snippet": "<b>Machine</b> <b>learning</b> has matured to the point to where it is now being considered to automate decisions in loan lending, employee hiring, and predictive policing. In many of these scenarios however, previous decisions have been made that are unfairly biased against certain subpopulations (e.g., those of a particular race, gender, or sexual orientation). Because this past data is often biased, <b>machine</b> <b>learning</b> predictors must account for this to avoid perpetuating discriminatory practices (or ...", "dateLastCrawled": "2021-12-15T09:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Privacy and <b>Fairness</b>: two very different <b>machine</b> <b>learning</b> ideals? | Revue", "url": "https://newsletter.mukulrathi.com/issues/privacy-and-fairness-two-very-different-machine-learning-ideals-717119", "isFamilyFriendly": true, "displayUrl": "https://newsletter.mukulrathi.com/issues/privacy-and-<b>fairness</b>-two-very-different...", "snippet": "<b>Counterfactual</b> <b>fairness</b> could be <b>similar</b> for <b>fairness</b>, where you can guarantee <b>fairness</b> given your assumptions about causality. There\u2019s a catch It appears that current methods for privacy and <b>fairness</b> seem to be conflicting.", "dateLastCrawled": "2022-01-30T17:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) <b>Counterfactual Fairness</b> - ResearchGate", "url": "https://www.researchgate.net/publication/324600593_Counterfactual_Fairness", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/324600593_<b>Counterfactual_Fairness</b>", "snippet": "the <b>machine</b> <b>learning</b> literature, the goal of this experiment is to quantify how our <b>algorithm</b> behav es with \ufb01nite sample sizes while assuming ground truth compatible with a synthetic model ...", "dateLastCrawled": "2021-12-11T00:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Counterfactual</b> Explanations for <b>Machine</b> <b>Learning</b>: A Review", "url": "https://ml-retrospectives.github.io/neurips2020/camera_ready/5.pdf", "isFamilyFriendly": true, "displayUrl": "https://ml-retrospectives.github.io/neurips2020/camera_ready/5.pdf", "snippet": "<b>Counterfactual</b> Explanations for <b>Machine</b> <b>Learning</b>: A Review Sahil Verma Arthur AI, University of Washington Washington D.C., USA vsahil@cs.washington.edu John Dickerson Arthur AI Washington D.C., USA john@arthur.ai Keegan Hines Arthur AI Washington D.C., USA keegan@arthur.ai Abstract <b>Machine</b> <b>learning</b> plays a role in many deployed decision systems, often in ways that are dif\ufb01cult or impossible to understand by human stakeholders. Explaining, in a human-understandable way, the relationship ...", "dateLastCrawled": "2022-01-29T04:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "9.3 <b>Counterfactual</b> Explanations | Interpretable <b>Machine</b> <b>Learning</b>", "url": "https://christophm.github.io/interpretable-ml-book/counterfactual.html", "isFamilyFriendly": true, "displayUrl": "https://christophm.github.io/interpretable-ml-book/<b>counterfactual</b>.html", "snippet": "In interpretable <b>machine</b> <b>learning</b>, <b>counterfactual</b> explanations can be used to explain predictions of individual instances. The \u201cevent\u201d is the predicted outcome of an instance, the \u201ccauses\u201d are the particular feature values of this instance that were input to the model and \u201ccaused\u201d a certain prediction. Displayed as a graph, the relationship between the inputs and the prediction is very simple: The feature values cause the prediction. FIGURE 9.9: The causal relationships between ...", "dateLastCrawled": "2022-02-03T02:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A Tutorial on <b>Fairness</b> in <b>Machine</b> <b>Learning</b> | by Ziyuan Zhong | Towards ...", "url": "https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-tutorial-on-<b>fairness</b>-in-<b>machine</b>-<b>learning</b>-3ff8ba1040cb", "snippet": "<b>Machine</b> <b>Learning</b>, the most widely used AI techniques, relies heavily on data. It is a common misconception that AI is absolutely objective. AI is objective only in the sense of <b>learning</b> what human teaches. The data provided by human can be highly-biased. It has been found in 2016 that COMPAS, the <b>algorithm</b> used for recidivism prediction produces much higher false positive rate for black people than white people(see Fig2,", "dateLastCrawled": "2022-02-01T18:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Verifying Individual <b>Fairness</b> in <b>Machine</b> <b>Learning</b> Models", "url": "http://proceedings.mlr.press/v124/george-john20a/george-john20a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v124/george-john20a/george-john20a.pdf", "snippet": "the existing work on verifying bias/<b>fairness</b> in <b>machine</b> <b>learning</b> models considers notions of group <b>fairness</b>/bias (Albarghouthi et al. 2017; Bastani et al. 2019). An individual <b>fairness</b> property considers the worst case (<b>fairness</b> for all <b>similar</b> input pairs, biased if there exists a bad input pair), rather than the average case (with high probability, some notion of parity is maintained between different groups) considered in the group <b>fairness</b> de\ufb01nitions. Hence, the existing techniques for ...", "dateLastCrawled": "2022-02-03T15:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "[2109.10458] Achieving <b>Counterfactual</b> <b>Fairness</b> for Causal Bandit", "url": "https://arxiv.org/abs/2109.10458", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/abs/2109.10458", "snippet": "Title: Achieving <b>Counterfactual</b> <b>Fairness</b> for Causal Bandit. Authors: Wen Huang, Lu Zhang, Xintao Wu. Download PDF Abstract: In online recommendation, customers arrive in a sequential and stochastic manner from an underlying distribution and the online decision model recommends a chosen item for each arriving individual based on some strategy. We study how to recommend an item at each step to maximize the expected reward while achieving user-side <b>fairness</b> for customers, i.e., customers who ...", "dateLastCrawled": "2021-12-24T07:10:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Counterfactual Fairness</b> - ResearchGate", "url": "https://www.researchgate.net/publication/315454664_Counterfactual_Fairness", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/315454664_<b>Counterfactual_Fairness</b>", "snippet": "way of assessing an existing decision making process, it is not as natural as <b>counterfactual fairness</b> in. the context of <b>machine</b> <b>learning</b>. Approximate <b>fairness</b> and model validation. The notion of ...", "dateLastCrawled": "2022-01-26T14:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Counterfactual Fairness</b> - ResearchGate", "url": "https://www.researchgate.net/publication/324600593_Counterfactual_Fairness", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/324600593_<b>Counterfactual_Fairness</b>", "snippet": "<b>Machine</b> <b>learning</b> <b>can</b> impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing.", "dateLastCrawled": "2021-12-11T00:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Counterfactual</b> <b>Fairness</b> \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/1703.06856/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1703.06856", "snippet": "In large part, the initial work on <b>fairness</b> in <b>machine</b> <b>learning</b> has focused on formalizing <b>fairness</b> into quantitative definitions and using them to solve a discrimination problem in a certain dataset. Unfortunately, for a practitioner, law-maker, judge, or anyone else who is interested in implementing algorithms that control for discrimination, it <b>can</b> be difficult to decide which definition of <b>fairness</b> to choose for the task at hand. Indeed, we demonstrate that depending on the relationship ...", "dateLastCrawled": "2021-12-15T09:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Counterfactual Fairness</b> | DeepAI", "url": "https://deepai.org/publication/counterfactual-fairness", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>counterfactual-fairness</b>", "snippet": "<b>Machine</b> <b>learning</b> <b>can</b> impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing. In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation.", "dateLastCrawled": "2022-01-26T15:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Trustworthy <b>Machine</b> <b>Learning</b> - Kush R. Varshney - Chapter 10: <b>Fairness</b>", "url": "http://www.trustworthymachinelearning.com/trustworthymachinelearning-10.htm", "isFamilyFriendly": true, "displayUrl": "www.trustworthy<b>machinelearning</b>.com/trustworthy<b>machinelearning</b>-10.htm", "snippet": "\u00a7 compare and contrast definitions of <b>fairness</b> in a <b>machine</b> <b>learning</b> context, \u00a7 ... but this is just a <b>thought</b> experiment.) <b>Counterfactual</b> <b>fairness</b> <b>can</b> be tested using treatment effect estimation methods from chapter 8. Protected attributes causing different outcomes across groups is an important consideration in many laws and regulations. [11] Suppose you have a full-blown causal graph of all the variables given to you or you discover one from data using the methods of chapter 8. In that ...", "dateLastCrawled": "2022-01-07T03:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>What if Algorithms Could be Fair</b>? - <b>Human Readable Magazine</b>", "url": "https://humanreadablemag.com/issues/0/articles/what-if-algorithms-could-be-fair/", "isFamilyFriendly": true, "displayUrl": "https://humanreadablemag.com/issues/0/articles/<b>what-if-algorithms-could-be-fair</b>", "snippet": "<b>Counterfactual</b> <b>fairness</b> is a new approach to <b>fairness</b> in <b>machine</b> <b>learning</b>, statistical models, and algorithms. It draws on the new field of causality to go beyond statistical relationships and correlations and to model the root causes of differences between protected groups. A mathematized notion of <b>fairness</b> removes the fluff and emotion from <b>fairness</b>, allowing us to clearly model and compare our beliefs about causal relationships in the world, and to empirically test our claims about bias ...", "dateLastCrawled": "2022-01-04T00:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "The Use and Misuse of Counterfactuals in Ethical <b>Machine</b> <b>Learning</b> ...", "url": "https://www.arxiv-vanity.com/papers/2102.05085/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2102.05085", "snippet": "The use of counterfactuals for considerations of algorithmic <b>fairness</b> and explainability is gaining prominence within the <b>machine</b> <b>learning</b> community and industry. This paper argues for more caution with the use of counterfactuals when the facts to be considered are social categories such as race or gender. We review a broad body of papers from philosophy and social sciences on social ontology and the semantics of counterfactuals, and we conclude that the <b>counterfactual</b> approach in <b>machine</b> ...", "dateLastCrawled": "2021-11-18T21:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Algorithmic Fairness in Mortgage Lending: from Absolute Conditions</b> to ...", "url": "https://link.springer.com/article/10.1007/s11023-020-09529-4", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11023-020-09529-4", "snippet": "<b>Counterfactual</b> <b>fairness</b> condition is whether the loan decision is the same in the actual world as it would be in a <b>counterfactual</b> world in which the individual belonged to a different racial group (Kusner et al. 2017). This metric posits that given a causal model (U, V, F) with a set of observable variables V, a set of latent background variables U not caused by V, and a set of functions F, the <b>counterfactual</b> of belonging to a protected class is independent of the outcome. Where X represents ...", "dateLastCrawled": "2022-01-29T15:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Fairness in machine learning with tractable models</b> - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S0950705120308443", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0950705120308443", "snippet": "The focus of this paper is taking steps towards the application of tractable probabilistic models to <b>fairness</b> in <b>machine</b> <b>learning</b>. Tractable probabilistic models have recently emerged that guarantee that conditional marginal <b>can</b> be computed in time linear in the size of the model. In particular, we show that sum product networks (SPNs) enable an effective technique for determining the statistical relationships between protected attributes and other training variables. We will also motivate ...", "dateLastCrawled": "2022-01-20T21:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Feature Choice and Fairness: Less May</b> be More | by Valerie Carey ...", "url": "https://towardsdatascience.com/feature-choice-and-fairness-less-may-be-more-7809ec11772e", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>feature-choice-and-fairness-less-may</b>-be-more-7809ec11772e", "snippet": "<b>Machine</b> <b>learning</b> models are based on correlation, and any feature associated with an outcome <b>can</b> be used as a decision basis; there is reason for concern. However, the risks of such a scenario occurring depend on the information available to the model and on the specific <b>algorithm</b> used. Here, I will use sample data to illustrate differences in incorporation of incidental information in random forest vs. XGBoost models, and discuss the importance of considering missing information ...", "dateLastCrawled": "2022-01-25T06:50:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Counterfactual Fairness</b> - ResearchGate", "url": "https://www.researchgate.net/publication/324600593_Counterfactual_Fairness", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/324600593_<b>Counterfactual_Fairness</b>", "snippet": "<b>Machine</b> <b>learning</b> <b>can</b> impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing.", "dateLastCrawled": "2021-12-11T00:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Counterfactual Fairness</b> | DeepAI", "url": "https://deepai.org/publication/counterfactual-fairness", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>counterfactual-fairness</b>", "snippet": "<b>Machine</b> <b>learning</b> <b>can</b> impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing. In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation.", "dateLastCrawled": "2022-01-26T15:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Reviews: <b>Counterfactual</b> <b>Fairness</b>", "url": "https://papers.nips.cc/paper/2017/file/a486cd07e4ac3d270571622f4f316ec5-Reviews.html", "isFamilyFriendly": true, "displayUrl": "https://papers.nips.cc/paper/2017/file/a486cd07e4ac3d270571622f4f316ec5-Reviews.html", "snippet": "As <b>machine</b> <b>learning</b> takes a more prominent role in decision making related to people, the question of <b>fairness</b> becomes increasingly important. The proposed definition is compelling and, in my opinion, better captures human conceptions of <b>fairness</b> <b>compared</b> to prior, non-<b>counterfactual</b> definitions. Additionally, the paper is well written, easy to follow, and technically correct.", "dateLastCrawled": "2021-11-21T05:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Algorithmic Bias: A <b>Counterfactual</b> Perspective", "url": "https://bitlab.cas.msu.edu/trustworthy-algorithms/whitepapers/Bo%20Cowgill.pdf", "isFamilyFriendly": true, "displayUrl": "https://bitlab.cas.msu.edu/trustworthy-<b>algorithm</b>s/whitepapers/Bo Cowgill.pdf", "snippet": "We discuss an alternative approach to measuring bias and <b>fairness</b> in <b>machine</b> <b>learning</b>: <b>Counterfactual</b> evaluation. In many practical settings, the alternative to a biased <b>algorithm</b> is not an unbiased one, but another decision method such as another <b>algorithm</b> or human discretion. We discuss statistical techniques necessary for <b>counterfactual</b> comparisons, which enable researchers to quantify relative biases without access to the underlying <b>algorithm</b> or its training data. We close by discussing ...", "dateLastCrawled": "2022-01-31T05:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Counterfactual</b> <b>Fairness</b>: Unidentification, Bound and <b>Algorithm</b> ...", "url": "https://www.researchgate.net/publication/334843895_Counterfactual_Fairness_Unidentification_Bound_and_Algorithm", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/334843895_<b>Counterfactual</b>_<b>Fairness</b>_Un...", "snippet": "<b>Fairness</b>-aware <b>learning</b> studies the problem of building <b>machine</b> <b>learning</b> models that are subject to <b>fairness</b> requirements. <b>Counterfactual</b> <b>fairness</b> is a notion of <b>fairness</b> derived from Pearl&#39;s ...", "dateLastCrawled": "2021-12-23T12:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Adversarial <b>Learning</b> for <b>Counterfactual</b> <b>Fairness</b> | DeepAI", "url": "https://deepai.org/publication/adversarial-learning-for-counterfactual-fairness", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/adversarial-<b>learning</b>-for-<b>counterfactual</b>-<b>fairness</b>", "snippet": "Adversarial <b>Learning</b> for <b>Counterfactual</b> <b>Fairness</b>. 08/30/2020 \u2219 by Vincent Grari, et al. \u2219 AXA \u2219 Laboratoire d&#39;Informatique de Paris 6 \u2219 0 \u2219 share . In recent years, <b>fairness</b> has become an important topic in the <b>machine</b> <b>learning</b> research community. In particular, <b>counterfactual</b> <b>fairness</b> aims at building prediction models which ensure <b>fairness</b> at the most individual level.", "dateLastCrawled": "2021-12-11T12:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Counterfactual</b> Explanations for <b>Machine</b> <b>Learning</b>: A Review", "url": "https://ml-retrospectives.github.io/neurips2020/camera_ready/5.pdf", "isFamilyFriendly": true, "displayUrl": "https://ml-retrospectives.github.io/neurips2020/camera_ready/5.pdf", "snippet": "choose the most appropriate <b>algorithm</b> given the set of assumptions they have and the speed and quality of the generation they want to achieve. 2. 3. Comprehensive and lucid introduction for beginners in the area of <b>counterfactual</b> explana- tions for <b>machine</b> <b>learning</b>. 2 Background This section gives the background about the social implications of <b>machine</b> <b>learning</b>, explainability research in <b>machine</b> <b>learning</b>, and some prior studies about <b>counterfactual</b> explanations. 2.1 Social Implications of ...", "dateLastCrawled": "2022-01-29T04:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "9.3 <b>Counterfactual</b> Explanations | Interpretable <b>Machine</b> <b>Learning</b>", "url": "https://christophm.github.io/interpretable-ml-book/counterfactual.html", "isFamilyFriendly": true, "displayUrl": "https://christophm.github.io/interpretable-ml-book/<b>counterfactual</b>.html", "snippet": "In interpretable <b>machine</b> <b>learning</b>, <b>counterfactual</b> explanations <b>can</b> be used to explain predictions of individual instances. The \u201cevent\u201d is the predicted outcome of an instance, the \u201ccauses\u201d are the particular feature values of this instance that were input to the model and \u201ccaused\u201d a certain prediction. Displayed as a graph, the relationship between the inputs and the prediction is very simple: The feature values cause the prediction. FIGURE 9.9: The causal relationships between ...", "dateLastCrawled": "2022-02-03T02:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Etiq.ai Debiasing Solutions - How <b>fairness</b> metrics <b>can</b> be misleading", "url": "https://etiq.ai/research/how-fairness-metrics-can-be-misleading/", "isFamilyFriendly": true, "displayUrl": "https://etiq.ai/research/how-<b>fairness</b>-metrics-<b>can</b>-be-misleading", "snippet": "Decision makers <b>can</b> rely upon <b>machine</b> <b>learning</b> models for guidance. However, sometimes these models <b>can</b> contain hidden bias, where certain people or groups are treated unfairly by the model due to individual features. To capture this discrimination, a variety of <b>fairness</b> metrics are used. Each of these measures <b>can</b> generally be classified as either a group or individual/<b>counterfactual</b> <b>fairness</b> metric, which is determined by what they are attempting to measure. For example, a group <b>fairness</b> ...", "dateLastCrawled": "2022-01-04T14:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Verifying Individual <b>Fairness</b> in <b>Machine</b> <b>Learning</b> Models", "url": "http://proceedings.mlr.press/v124/george-john20a/george-john20a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v124/george-john20a/george-john20a.pdf", "snippet": "Verifying Individual <b>Fairness</b> in <b>Machine</b> <b>Learning</b> Models Philips George John, Deepak Vijaykeerthy, Diptikalyan Saha IBM Research AI Bengaluru 560 045, India Abstract We consider the problem of whether a given decision model, working with structured data, has individual <b>fairness</b>. Following the work of Dwork, a model is individually biased (or unfair) if there is a pair of valid inputs which are close to each other (according to an ap-propriate metric) but are treated differently by the model ...", "dateLastCrawled": "2022-02-03T15:31:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Counterfactual Fairness</b> - ResearchGate", "url": "https://www.researchgate.net/publication/315454664_Counterfactual_Fairness", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/315454664_<b>Counterfactual_Fairness</b>", "snippet": "way of assessing an existing decision making process, it is not as natural as <b>counterfactual fairness</b> in. the context of <b>machine</b> <b>learning</b>. Approximate <b>fairness</b> and model validation. The notion of ...", "dateLastCrawled": "2022-01-26T14:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Responsible AI practices \u2013 Google AI", "url": "https://ai.google/responsibilities/responsible-ai-practices/?category=fairness", "isFamilyFriendly": true, "displayUrl": "https://ai.google/responsibilities/responsible-ai-practices/?category=<b>fairness</b>", "snippet": "A case-study on the application of <b>fairness</b> in <b>machine</b> <b>learning</b> research to a production classification system, and new insights in how to measure and address algorithmic <b>fairness</b> issues. Research paper <b>Counterfactual</b> <b>fairness</b> in text classification through robustness Provides and compares multiple approaches for addressing <b>counterfactual</b> <b>fairness</b> issues in text models. Research paper Model Cards for Model Reporting Proposes a framework to encourage transparent model reporting. Research ...", "dateLastCrawled": "2022-02-02T14:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Towards Analogy-Based Explanations in Machine Learning</b> | DeepAI", "url": "https://deepai.org/publication/towards-analogy-based-explanations-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>towards-analogy-based-explanations-in-machine-learning</b>", "snippet": "More specifically, we take the view that an <b>analogy</b>-based approach is a viable alternative to existing approaches in the realm of explainable AI and interpretable <b>machine</b> <b>learning</b>, and that <b>analogy</b>-based explanations of the predictions produced by a <b>machine</b> <b>learning</b> algorithm can complement similarity-based explanations in a meaningful way. To corroborate these claims, we outline the basic idea of an <b>analogy</b>-based explanation and illustrate its potential usefulness by means of some examples.", "dateLastCrawled": "2022-01-10T12:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Human-centric Approach to <b>Fairness</b> in AI", "url": "https://www.timlrx.com/blog/a-human-centric-approach-to-fairness-in-ai", "isFamilyFriendly": true, "displayUrl": "https://www.timlrx.com/blog/a-human-centric-approach-to-<b>fairness</b>-in-ai", "snippet": "A lot of what is discussed in the <b>machine</b> <b>learning</b> literature touches on <b>fairness</b> (or rather equivalence in certain outcomes) between groups, yet this narrowly constricts <b>fairness</b> to the notion of equality. Of course, we should think about <b>fairness</b> in the context of prejudiced groups, but we should also ask whether it is fair to an individual. Adding constraints in models might lead to worse outcomes for other individuals. If the decision making processs has serious consequences e.g. a fraud ...", "dateLastCrawled": "2022-02-03T02:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Fairness in machine learning with tractable models</b> - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S0950705120308443", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0950705120308443", "snippet": "In recent years, the topic of <b>fairness</b> has become an increasingly important issue within the field of <b>machine</b> <b>learning</b>, both in academic circles , , , and more recently in the public and political domains , . However, even amid pressing concerns that algorithms currently in use may exhibit racial biases, there remains a lack of agreement about how to effectively implement fair <b>machine</b> <b>learning</b> algorithms within the academic community.", "dateLastCrawled": "2022-01-20T21:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Counterfactual</b> Explanation of <b>Machine</b> <b>Learning</b> Survival Models - IOS Press", "url": "https://content.iospress.com/articles/informatica/infor468", "isFamilyFriendly": true, "displayUrl": "https://content.iospress.com/articles/informatica/infor468", "snippet": "A method for <b>counterfactual</b> explanation of <b>machine</b> <b>learning</b> survival models is proposed. One of the difficulties of solving the <b>counterfactual</b> explanation problem is that the classes of examples are implicitly defined through outcomes of a <b>machine</b> <b>learning</b> survival model in the form of survival functions. A condition that establishes the difference between survival functions of the original example and the <b>counterfactual</b> is introduced. This condition is based on using a distance between mean ...", "dateLastCrawled": "2022-01-15T22:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "dimensions of <b>machine</b> Causality and the normative", "url": "https://maxkasy.github.io/home/files/other/ML_inequality_conference/slides_loftus.pdf", "isFamilyFriendly": true, "displayUrl": "https://maxkasy.github.io/home/files/other/ML_inequality_conference/slides_loftus.pdf", "snippet": "dimensions of <b>machine</b> <b>learning</b> Joshua Loftus (LSE Statistics) High level intro Causality, what is it good for? Causal <b>fairness</b> In prediction and ranking tasks, and with intersectionality Designing interventions Optimal fair policies, causal interference Concluding thoughts 2 / 27. Tech solutionism, using ML/AI in every situation 3 / 27. Imagination Albert Einstein: Imagination is more important than knowledge. For knowledge is limited, whereas imagination [...] stimulat[es] progress, giving ...", "dateLastCrawled": "2022-01-11T12:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Stable <b>Learning</b> and its Causal Implication", "url": "http://pengcui.thumedialab.com/papers/Stable%20Learning-tutorial-valse2021.pdf", "isFamilyFriendly": true, "displayUrl": "pengcui.thumedialab.com/papers/Stable <b>Learning</b>-tutorial-valse2021.pdf", "snippet": "Application --- <b>counterfactual</b> visual explanations ... Goyal, Yash, et al. &quot;<b>Counterfactual</b> visual explanations.&quot; International Conference on <b>Machine</b> <b>Learning</b>. PMLR, 2019. Explainability with Causality Application --- causal recommendation 17 He et al. \u201dCollaborative Causal Filtering for Out-of-Distribution Recommendation.&quot; Under review. Caual structure among user features and item features Example . Explainability and OOD \u2022Explainability would be a side product when pursuing OOD with ...", "dateLastCrawled": "2022-01-28T03:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Economic Methodology Meets Interpretable Machine Learning</b> - Part I ...", "url": "https://bcmullins.github.io/economic_methodology_interpretable_ml_blackboxes/", "isFamilyFriendly": true, "displayUrl": "https://bcmullins.github.io/economic_methodology_interpretable_ml_blackboxes", "snippet": "The ends in question for <b>machine</b> <b>learning</b> models are <b>fairness</b>, privacy, stability, causality, and so forth. With the focus on interpretability, we may be (implicitly) assuming both that interpretability is a means at getting at these ends and, more strongly, that interpretability is the only means to get at these ends. At the very least, this should be suspect, since interpretability is domain-specific and lacks a unified definition.", "dateLastCrawled": "2022-01-22T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Shortcut <b>learning</b> in deep neural networks | Nature <b>Machine</b> Intelligence", "url": "https://www.nature.com/articles/s42256-020-00257-z", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s42256-020-00257-z", "snippet": "In <b>analogy</b> to <b>machine</b> <b>learning</b>, we have a striking discrepancy between intended and actual <b>learning</b> outcome. Shortcut <b>learning</b> in education (surface <b>learning</b>) Alice loves history\u2014but at this ...", "dateLastCrawled": "2022-02-03T07:59:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Counterfactual Fairness \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/1703.06856/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1703.06856", "snippet": "<b>Machine</b> <b>learning</b> has matured to the point to where it is now being considered to automate decisions in loan lending, employee hiring, and predictive policing. In many of these scenarios however, previous decisions have been made that are unfairly biased against certain subpopulations (e.g., those of a particular race, gender, or sexual orientation). Because this past data is often biased, <b>machine</b> <b>learning</b> predictors must account for this to avoid perpetuating discriminatory practices (or ...", "dateLastCrawled": "2021-12-15T09:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Counterfactual Fairness</b> - ResearchGate", "url": "https://www.researchgate.net/publication/315454664_Counterfactual_Fairness", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/315454664_<b>Counterfactual_Fairness</b>", "snippet": "<b>Machine</b> <b>learning</b> has matured to the point to where it is now being considered to automate decisions in loan lending, employee hiring, and predictive policing. In many of these scenarios however ...", "dateLastCrawled": "2022-01-26T14:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Abstract - arXiv", "url": "https://arxiv.org/pdf/1703.06856v3.pdf", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/pdf/1703.06856v3.pdf", "snippet": "<b>machine</b> <b>learning</b> predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our de\ufb01nition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it is the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real ...", "dateLastCrawled": "2020-08-09T05:32:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(counterfactual fairness)  is like +(machine learning algorithm)", "+(counterfactual fairness) is similar to +(machine learning algorithm)", "+(counterfactual fairness) can be thought of as +(machine learning algorithm)", "+(counterfactual fairness) can be compared to +(machine learning algorithm)", "machine learning +(counterfactual fairness AND analogy)", "machine learning +(\"counterfactual fairness is like\")", "machine learning +(\"counterfactual fairness is similar\")", "machine learning +(\"just as counterfactual fairness\")", "machine learning +(\"counterfactual fairness can be thought of as\")", "machine learning +(\"counterfactual fairness can be compared to\")"]}