{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Explaining <b>Bayesian</b> <b>Neural</b> Networks | DeepAI", "url": "https://deepai.org/publication/explaining-bayesian-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/explaining-<b>bayesian</b>-<b>neural</b>-<b>networks</b>", "snippet": "<b>Bayesian</b> approaches such as <b>Bayesian</b> <b>Neural</b> Networks (BNNs) so far have a limited form of transparency (model transparency) already built-in through their prior weight distribution, but notably, they lack explanations of their predictions for given instances. In this work, we bring together these two perspectives of transparency into a holistic explanation framework for explaining BNNs. Within the <b>Bayesian</b> framework, the <b>network</b> weights follow a", "dateLastCrawled": "2022-01-26T17:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Bayesian Neural</b> Networks - Presenters", "url": "https://csc2541-f17.github.io/slides/lec03.pdf", "isFamilyFriendly": true, "displayUrl": "https://csc2541-f17.github.io/slides/lec03.pdf", "snippet": "Standard <b>Neural</b> Net <b>Bayesian Neural</b> Net ... Retains the same topology of <b>regular</b> <b>Neural</b> Nets, however we assume a prior distribution over the weights and we follow an iterative procedure for estimating the hyperparameters. 16. <b>Bayesian Neural</b> Networks Overview 17 Prior distribution over target variables Prior distribution over weights Likelihood function (assuming iid) Then Which is not Gaussian because the function y is a <b>neural</b> <b>network</b> whose dependence on w is non-linear \u2192 We will build ...", "dateLastCrawled": "2022-02-02T14:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A <b>quick intro to Bayesian neural networks</b> - matthewmcateer.me", "url": "https://matthewmcateer.me/blog/a-quick-intro-to-bayesian-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://matthewmcateer.me/blog/a-<b>quick-intro-to-bayesian-neural-networks</b>", "snippet": "<b>Bayesian</b> <b>neural</b> networks are different from <b>regular</b> <b>neural</b> networks due to the fact that their states are described by probability distributions instead of single 1D float values for each parameter. Such probability distributions reflect weight and bias uncertainties, and therefore can be used to convey predictive uncertainty. Instead of typical direct backpropagation, these weight distribution parameters are learned through variational inference. In this post, I go over some of the ...", "dateLastCrawled": "2021-05-30T11:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "An Example of a <b>Bayesian</b> <b>Neural</b> <b>Network</b> Using PyTorch | James D. McCaffrey", "url": "https://jamesmccaffrey.wordpress.com/2021/08/30/an-example-of-a-bayesian-neural-network-using-pytorch/", "isFamilyFriendly": true, "displayUrl": "https://jamesmccaffrey.wordpress.com/2021/08/30/an-example-of-a-<b>bayesian</b>-<b>neural</b>...", "snippet": "A <b>regular</b> <b>neural</b> <b>network</b> has a set of numeric constants called weights which determine the <b>network</b> output. If you feed the same input to a <b>regular</b> trained <b>neural</b> <b>network</b>, you will get the same output every time. In a <b>Bayesian</b> <b>neural</b> <b>network</b>, each weight is probability distribution instead of a fixed value. Each time you feed an input to a ...", "dateLastCrawled": "2022-02-02T11:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Bayesian</b> <b>Neural</b> Networks: 3 <b>Bayesian</b> CNN | by Adam Woolf | Towards Data ...", "url": "https://towardsdatascience.com/bayesian-neural-networks-3-bayesian-cnn-6ecd842eeff3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>bayesian</b>-<b>neural</b>-<b>networks</b>-3-<b>bayesian</b>-cnn-6ecd842eeff3", "snippet": "Understand how parameter problems of <b>Bayesian</b> <b>neural</b> networks influence training; As we\u2019ve discovered in earlier articles, <b>Bayesian</b> analysis deals in distributions and not single values. We\u2019ve seen it with normal distributions where we\u2019re getting a continuous floating point back with the most likely return value of the mean. The distribution for a categorical becomes the discreet (piano key rather than violin string). For probabilities we\u2019ll get a specific result such as a class, an ...", "dateLastCrawled": "2022-01-30T07:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Bayesian</b> <b>neural</b> <b>networks for flight trajectory prediction and safety</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0167923620300014", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0167923620300014", "snippet": "Next, we build two different types of <b>Bayesian</b> <b>neural</b> <b>network</b> models: in the first model, deep feedforward <b>Bayesian</b> <b>neural</b> networks (DNN) are trained with historical data for one-step-ahead prediction on the deviation between actual trajectory and target flight trajectory. In the second model, LSTM <b>neural</b> networks are trained with historical flight trajectory data to make longer-term predictions on the future flight trajectory. Afterwards, the two models are blended through a discrepancy ...", "dateLastCrawled": "2022-01-29T16:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "[D] <b>advantages of bayesian neural network over regular</b> <b>neural</b> networks ...", "url": "https://www.reddit.com/r/statistics/comments/mrfc9x/d_advantages_of_bayesian_neural_network_over/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/.../comments/mrfc9x/d_<b>advantages_of_bayesian_neural_network</b>_over", "snippet": "This is a really good video that explains the differences between <b>regular</b> <b>neural</b> networks and <b>bayesian</b> <b>neural</b> networks. Apparently in <b>bayesian</b> <b>neural</b> networks, the weights aren&#39;t assigned a fixed value but instead each weight is assigned a probability distribution (e.g. each weight is given a normal probability distribution and we calculate the mean and variance).", "dateLastCrawled": "2021-04-15T14:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Bayesian</b> CNN model on MNIST data using <b>Tensorflow</b>-probability (compared ...", "url": "https://medium.com/python-experiments/bayesian-cnn-model-on-mnist-data-using-tensorflow-probability-compared-to-cnn-82d56a298f45", "isFamilyFriendly": true, "displayUrl": "https://medium.com/python-experiments/<b>bayesian</b>-cnn-model-on-mnist-data-using-tensor...", "snippet": "The <b>regular</b> CNN takes a shorter time to run and achieves better accuracy, compared to the <b>Bayesian</b> CNN using the same model structure. However, the one advantage that <b>Bayesian</b> CNN brings in is an ...", "dateLastCrawled": "2022-02-03T00:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Bayesian Neural</b> <b>Network</b> Ensembles", "url": "http://bayesiandeeplearning.org/2018/papers/78.pdf", "isFamilyFriendly": true, "displayUrl": "<b>bayesian</b>deeplearning.org/2018/papers/78.pdf", "snippet": "<b>Bayesian Neural</b> <b>Network</b> Ensembles Tim Pearce Department of Engineering University of Cambridge tp424@cam.ac.uk Mohamed Zaki Department of Engineering University of Cambridge Andy Neely Department of Engineering University of Cambridge 1 Introduction Ensembles of <b>neural</b> networks (NNs) have long been used to estimate predictive uncertainty (Tib-shirani, 1996; Heskes, 1996); a small number of NNs are trained from different initialisations and sometimes on differing versions of the dataset. The ...", "dateLastCrawled": "2021-11-19T11:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Soft Computing MCQ (Multiple Choice Questions</b>) - JavaTpoint", "url": "https://www.javatpoint.com/soft-computing-mcq", "isFamilyFriendly": true, "displayUrl": "https://www.javatpoint.com/<b>soft-computing</b>-mcq", "snippet": "Answer: d) A single layer feed-forward <b>neural</b> <b>network</b> with pre-processing . Explanation: A perceptron is a single-layer <b>neural</b> <b>network</b> that consists of input values, weights, bias, net sum followed by an activation function.", "dateLastCrawled": "2022-02-02T09:41:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Explaining <b>Bayesian</b> <b>Neural</b> Networks | DeepAI", "url": "https://deepai.org/publication/explaining-bayesian-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/explaining-<b>bayesian</b>-<b>neural</b>-<b>networks</b>", "snippet": "<b>Bayesian</b> approaches such as <b>Bayesian</b> <b>Neural</b> Networks (BNNs) so far have a limited form of transparency (model transparency) already built-in through their prior weight distribution, but notably, they lack explanations of their predictions for given instances. In this work, we bring together these two perspectives of transparency into a holistic explanation framework for explaining BNNs. Within the <b>Bayesian</b> framework, the <b>network</b> weights follow a", "dateLastCrawled": "2022-01-26T17:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A <b>quick intro to Bayesian neural networks</b> - matthewmcateer.me", "url": "https://matthewmcateer.me/blog/a-quick-intro-to-bayesian-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://matthewmcateer.me/blog/a-<b>quick-intro-to-bayesian-neural-networks</b>", "snippet": "<b>Bayesian</b> <b>neural</b> networks are different from <b>regular</b> <b>neural</b> networks due to the fact that their states are described by probability distributions instead of single 1D float values for each parameter. Such probability distributions reflect weight and bias uncertainties, and therefore can be used to convey predictive uncertainty. Instead of typical direct backpropagation, these weight distribution parameters are learned through variational inference. In this post, I go over some of the ...", "dateLastCrawled": "2021-05-30T11:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Bayesian Recurrent Neural Networks</b> | DeepAI", "url": "https://deepai.org/publication/bayesian-recurrent-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>bayesian-recurrent-neural-networks</b>", "snippet": "This technique is not exclusive to recurrent <b>neural</b> networks and can be applied more widely to train <b>Bayesian</b> <b>neural</b> networks. ... RNN parameters are learnt in much the same way as in a feedforward <b>neural</b> <b>network</b>. A loss (typically after further layers) is applied to the states s 1: T of the RNN, and then backpropagation is used to update the weights of the <b>network</b>. Crucially, the weights at each of the unrolled steps are shared. Thus each weight of the RNN core receives T gradient ...", "dateLastCrawled": "2022-01-29T18:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Bayesian</b> <b>neural</b> <b>networks for flight trajectory prediction and safety</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0167923620300014", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0167923620300014", "snippet": "Next, we build two different types of <b>Bayesian</b> <b>neural</b> <b>network</b> models: in the first model, deep feedforward <b>Bayesian</b> <b>neural</b> networks (DNN) are trained with historical data for one-step-ahead prediction on the deviation between actual trajectory and target flight trajectory. In the second model, LSTM <b>neural</b> networks are trained with historical flight trajectory data to make longer-term predictions on the future flight trajectory. Afterwards, the two models are blended through a discrepancy ...", "dateLastCrawled": "2022-01-29T16:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Combining <b>Bayesian</b> <b>Neural Networks</b> and Ensemble techniques | by Dylan ...", "url": "https://towardsdatascience.com/combining-bayesian-neural-networks-and-ensemble-techniques-a4a3a9072e79", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/combining-<b>bayesian</b>-<b>neural-networks</b>-and-ensemble...", "snippet": "MAP Ensemble techniques <b>Bayesian</b> <b>Neural Networks</b> Randomized MAP sampling Gaussian Mixture Models. Examples \u2018An Ensemble of <b>Bayesian</b> <b>Neural Networks</b> for Exoplanetary Atmospheric Retrieval\u2019 \u2018Uncertainty in <b>Neural Networks</b>: Approximately <b>Bayesian</b> Ensembling\u2019 \u2018Using Stacking to Average <b>Bayesian</b> Predictive Distributions\u2019 Terminology. f\u1d42(X) \u2014 <b>neural</b> <b>network</b> model p(W|X,Y) \u2014 posterior probability of NN weights W given data X,Y p(Y|X,W) \u2014 maximum likelihood of NN weights W from ...", "dateLastCrawled": "2022-02-01T05:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "What Are <b>Bayesian</b> <b>Neural</b> <b>Network</b> Posteriors Really Like?", "url": "http://proceedings.mlr.press/v139/izmailov21a/izmailov21a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v139/izmailov21a/izmailov21a.pdf", "snippet": "of prior scale, and relatively <b>similar</b> for diagonal Gaussian, mixture of Gaussian, and logistic priors; (5) <b>Bayesian</b> <b>neural</b> networks show surprisingly poor generalization under domain shift; (6) while cheaper alternatives such as deep ensembles and SGMCMC can provide good generalization, their predictive distributions are distinct from HMC. Notably, deep ensemble predictive distributions are similarly close to HMC as standard SGLD, and closer than standard variational inference. 1 ...", "dateLastCrawled": "2022-01-29T18:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Bayesian</b> CNN model on MNIST data using <b>Tensorflow</b>-probability (compared ...", "url": "https://medium.com/python-experiments/bayesian-cnn-model-on-mnist-data-using-tensorflow-probability-compared-to-cnn-82d56a298f45", "isFamilyFriendly": true, "displayUrl": "https://medium.com/python-experiments/<b>bayesian</b>-cnn-model-on-mnist-data-using-tensor...", "snippet": "The <b>regular</b> CNN takes a shorter time to run and achieves better accuracy, compared to the <b>Bayesian</b> CNN using the same model structure. However, the one advantage that <b>Bayesian</b> CNN brings in is an ...", "dateLastCrawled": "2022-02-03T00:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Bayesian Regularization of Neural Networks</b>", "url": "https://www.researchgate.net/publication/45539220_Bayesian_Regularization_of_Neural_Networks", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/.../45539220_<b>Bayesian_Regularization_of_Neural_Networks</b>", "snippet": "The <b>Bayesian</b> regularization <b>neural</b> <b>network</b> is constructed to predict the near-optimal equivalent factor online, while the backpropagation <b>neural</b> <b>network</b> is designed to forecast the engine on/off ...", "dateLastCrawled": "2022-02-01T04:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Uncertainty</b> via 3D <b>Bayesian</b> Deep Learning | Towards Data Science", "url": "https://towardsdatascience.com/deep-learning-segmentation-with-uncertainty-via-3d-bayesian-convolutional-neural-networks-6b1c7277b078", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/deep-learning-segmentation-with-<b>uncertainty</b>-via-3d...", "snippet": "\u00b7 Measuring <b>uncertainty</b> is not possible in a <b>regular</b> deep <b>neural</b> <b>network</b>, but it is extremely important for interpretability and validation \u00b7 <b>Bayesian</b> <b>neural</b> networks learn probability distributions rather than point estimates, allowing them to measure <b>uncertainty</b> \u00b7 We designed the first-ever successful <b>Bayesian</b> convolutional <b>neural</b> <b>network</b> (BCNN) architecture for 3D segmentation \u00b7 Our BCNN beats the current state-of-the-art <b>neural</b> <b>network</b> in <b>uncertainty</b> quantification while achieving ...", "dateLastCrawled": "2022-01-14T02:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What is the relationship between Deep Belief <b>Network</b> (DBN) and <b>Bayesian</b> ...", "url": "https://www.quora.com/What-is-the-relationship-between-Deep-Belief-Network-DBN-and-Bayesian-Network-or-Belief-Network-I-know-what-is-Bayesian-network-My-question-is-that-is-deep-belief-network-a-kind-of-belief-network-or-it-is-just-a-similar-name", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-the-relationship-between-Deep-Belief-<b>Network</b>-DBN-and...", "snippet": "Answer (1 of 2): A deep belief <b>network</b> is nowadays referred to a deep <b>neural</b> <b>network</b>. At the time of development it was difficult to get <b>neural</b> <b>network</b> papers published. I think that this name was a rebranding to work around that issue. In a <b>Bayesian</b> <b>network</b>, links model probabilistic dependence...", "dateLastCrawled": "2022-01-06T20:45:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Bayesian Neural</b> Networks - Presenters", "url": "https://csc2541-f17.github.io/slides/lec03.pdf", "isFamilyFriendly": true, "displayUrl": "https://csc2541-f17.github.io/slides/lec03.pdf", "snippet": "Retains the same topology of <b>regular</b> <b>Neural</b> Nets, however we assume a prior distribution over the weights and we follow an iterative procedure for estimating the hyperparameters. 16. <b>Bayesian Neural</b> Networks Overview 17 Prior distribution over target variables Prior distribution over weights Likelihood function (assuming iid) Then Which is not Gaussian because the function y is a <b>neural</b> <b>network</b> whose dependence on w is non-linear \u2192 We will build a Gaussian Approximation to the log ...", "dateLastCrawled": "2022-02-02T14:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "An Example of a <b>Bayesian</b> <b>Neural</b> <b>Network</b> Using PyTorch | James D. McCaffrey", "url": "https://jamesmccaffrey.wordpress.com/2021/08/30/an-example-of-a-bayesian-neural-network-using-pytorch/", "isFamilyFriendly": true, "displayUrl": "https://jamesmccaffrey.wordpress.com/2021/08/30/an-example-of-a-<b>bayesian</b>-<b>neural</b>...", "snippet": "A <b>regular</b> <b>neural</b> <b>network</b> has a set of numeric constants called weights which determine the <b>network</b> output. If you feed the same input to a <b>regular</b> trained <b>neural</b> <b>network</b>, you will get the same output every time. In a <b>Bayesian</b> <b>neural</b> <b>network</b>, each weight is probability distribution instead of a fixed value. Each time you feed an input to a ...", "dateLastCrawled": "2022-02-02T11:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Bayesian</b> <b>Neural</b> Networks: 3 <b>Bayesian</b> CNN | by Adam Woolf | Towards Data ...", "url": "https://towardsdatascience.com/bayesian-neural-networks-3-bayesian-cnn-6ecd842eeff3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>bayesian</b>-<b>neural</b>-<b>networks</b>-3-<b>bayesian</b>-cnn-6ecd842eeff3", "snippet": "It\u2019s an example of a number of areas of <b>neural</b> <b>network</b> theory we often think we understand but that\u2019ll demand a review of our beliefs. We usually think of batch size as of predominant importance to training speed. Some people also appreciate the reduced variance a larger batch brings. However with <b>Bayesian</b> models batch size directly influences training performance. Have a look and see by running the same model repeatedly with a batch size of 5 and with 50. You\u2019ll notice that when the ...", "dateLastCrawled": "2022-01-30T07:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Graph Neural Processes Towards Bayesian Graph Neural</b> Networks", "url": "https://andrewnc.github.io/Graph_Neural_Processes.pdf", "isFamilyFriendly": true, "displayUrl": "https://andrewnc.github.io/Graph_<b>Neural</b>_Processes.pdf", "snippet": "In <b>Bayesian</b> <b>neural</b> networks, uncertainty estimates are used at the weight level, or at the output of the <b>network</b>. These extensions to typical deep learning give insight into what the model is learning, and where it may encounter failure modes. In this work, we use some of this progress to impute the value distribution on an edge in graph-structured data. In this work we propose a novel architecture and training mechanism which we call Graph <b>Neural</b> Processes (GNP). This architecture is based ...", "dateLastCrawled": "2022-01-31T14:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Learning Efficiency of Redundant Neural Networks in Bayesian Estimation</b> ...", "url": "https://www.researchgate.net/publication/228543402_Learning_Efficiency_of_Redundant_Neural_Networks_in_Bayesian_Estimation_TNN_687-Rev", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/228543402_Learning_Efficiency_of_Redundant...", "snippet": "<b>Bayesian</b> Learning for <b>Neural</b> Networks shows that <b>Bayesian</b> methods allow complex <b>neural</b> <b>network</b> models to be used without fear of the &quot;overfitting&quot; that <b>can</b> occur with traditional <b>neural</b> <b>network</b> ...", "dateLastCrawled": "2022-01-10T07:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "On the relative expressiveness of <b>Bayesian</b> and <b>neural</b> networks ...", "url": "https://www.sciencedirect.com/science/article/pii/S0888613X19301835", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0888613X19301835", "snippet": "Next, <b>Bayesian</b> <b>network</b> queries (and, hence, ACs) <b>can</b> be trained discriminatively using labeled data and gradient descent methods, leading to a realization of the function-based approach currently practiced using <b>neural</b> networks\u2014except that a <b>neural</b> <b>network</b> represents only one function, while a <b>Bayesian</b> <b>network</b> represents many functions (one for each query). Finally, the functions induced by <b>Bayesian</b> <b>network</b> queries <b>can</b> integrate background knowledge of various forms, suggesting a more ...", "dateLastCrawled": "2021-11-28T12:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Results for Training <b>Neural</b> Networks with <b>Bayesian</b> Linear Regression ...", "url": "https://peterbarnett.org/2021/03/05/bnet-results1/", "isFamilyFriendly": true, "displayUrl": "https://peterbarnett.org/2021/03/05/bnet-results1", "snippet": "In this post I will demonstrate results from training <b>neural</b> nets using approximate <b>Bayesian</b> updating. The algorithm ... coordinates. This was fit using data where the coordinates were from a <b>regular</b> grid which was then shuffled, and data were the points were sampled randomly. The hidden layers had sizes 128, 256, 256, 128. On the left are the radial cosine functions used for training, with the red dots representing the points used for training. On the right are the learned functions. MNIST ...", "dateLastCrawled": "2021-12-25T07:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Bayesian</b> Nonparametric Deep Learning", "url": "http://bayesiandeeplearning.org/2018/papers/50.pdf", "isFamilyFriendly": true, "displayUrl": "<b>bayesian</b>deeplearning.org/2018/papers/50.pdf", "snippet": "1 Formalizing the <b>Bayesian</b> Nonparametric Deep Generative Model We consider a layerless formulation of <b>neural</b> networks where connections are not constrained by layers and units <b>can</b> connect to any units below them with some probability. In particular, we use the Nonlinear Gaussian Belief <b>Network</b> (NLGBN) as our generative model [4]. In this model ...", "dateLastCrawled": "2022-01-18T09:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "How Optimistic Do You Want to <b>Be? Bayesian Neural Network Regression</b> ...", "url": "https://blog.wolfram.com/2018/05/31/how-optimistic-do-you-want-to-be-bayesian-neural-network-regression-with-prediction-errors/", "isFamilyFriendly": true, "displayUrl": "https://blog.wolfram.com/2018/05/31/how-optimistic-do-you-want-to-be-<b>bayesian</b>-<b>neural</b>...", "snippet": "<b>Bayesian Neural Network Regression with Prediction</b> Errors May 31, 2018. <b>Neural</b> networks are very well known for their uses in machine learning, but <b>can</b> be used as well in other, more specialized topics, like regression. Many people would probably first associate regression with statistics, but let me show you the ways in which <b>neural</b> networks <b>can</b> be helpful in this field. They are especially useful if the data you\u2019re interested in doesn\u2019t follow an obvious underlying trend you <b>can</b> ...", "dateLastCrawled": "2022-01-29T01:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "300+ TOP <b>Neural Networks Multiple Choice Questions and Answers</b>", "url": "https://engineeringinterviewquestions.com/neural-networks-multiple-choice-questions-and-answers/", "isFamilyFriendly": true, "displayUrl": "https://engineeringinterviewquestions.com/<b>neural-networks-multiple-choice-questions</b>...", "snippet": "Which of the following is not the promise of artificial <b>neural</b> <b>network</b>? a) It <b>can</b> explain result b) It <b>can</b> survive the failure of some nodes c) It has inherent parallelism d) It <b>can</b> handle noise. Answer: a Explanation: The artificial <b>Neural</b> <b>Network</b> (ANN) cannot explain result. 15. <b>Neural</b> Networks are complex _____ with many parameters. a) Linear Functions b) Nonlinear Functions c) Discrete Functions d) Exponential Functions. Answer: a Explanation: <b>Neural</b> networks are complex linear functions ...", "dateLastCrawled": "2022-02-02T15:24:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Explaining <b>Bayesian</b> <b>Neural</b> Networks | DeepAI", "url": "https://deepai.org/publication/explaining-bayesian-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/explaining-<b>bayesian</b>-<b>neural</b>-<b>networks</b>", "snippet": "<b>Bayesian</b> approaches such as <b>Bayesian</b> <b>Neural</b> Networks (BNNs) so far have a limited form of transparency (model transparency) already built-in through their prior weight distribution, but notably, they lack explanations of their predictions for given instances. In this work, we bring together these two perspectives of transparency into a holistic explanation framework for explaining BNNs. Within the <b>Bayesian</b> framework, the <b>network</b> weights follow a", "dateLastCrawled": "2022-01-26T17:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Bayesian</b> CNN model on MNIST data using <b>Tensorflow</b>-probability (<b>compared</b> ...", "url": "https://medium.com/python-experiments/bayesian-cnn-model-on-mnist-data-using-tensorflow-probability-compared-to-cnn-82d56a298f45", "isFamilyFriendly": true, "displayUrl": "https://medium.com/python-experiments/<b>bayesian</b>-cnn-model-on-mnist-data-using-tensor...", "snippet": "The <b>regular</b> CNN takes a shorter time to run and achieves better accuracy, <b>compared</b> to the <b>Bayesian</b> CNN using the same model structure. However, the one advantage that <b>Bayesian</b> CNN brings in is an ...", "dateLastCrawled": "2022-02-03T00:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Bayesian Recurrent Neural Networks</b> | DeepAI", "url": "https://deepai.org/publication/bayesian-recurrent-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>bayesian-recurrent-neural-networks</b>", "snippet": "In this work we explore a straightforward variational Bayes scheme for Recurrent <b>Neural</b> Networks.Firstly, we show that a simple adaptation of truncated backpropagation through time <b>can</b> yield good quality uncertainty estimates and superior regularisation at only a small extra computational cost during training. Secondly, we demonstrate how a novel kind of posterior approximation yields further improvements to the performance of <b>Bayesian</b> RNNs.", "dateLastCrawled": "2022-01-29T18:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "9.7. <b>Bayesian</b> <b>neural</b> networks \u2014 Learning from data", "url": "https://furnstahl.github.io/Physics-8820/notebooks/Machine_learning/Bayesian_neural_networks_tif285.html", "isFamilyFriendly": true, "displayUrl": "https://furnstahl.github.io/.../Machine_learning/<b>Bayesian</b>_<b>neural</b>_<b>networks</b>_tif285.html", "snippet": "A <b>Bayesian</b> <b>neural</b> <b>network</b> <b>can</b> be viewed as probabilistic model in which we want to infer \\(p(y \\lvert \\boldsymbol{x},\\mathcal{D})\\) ... The predictions for a <b>Bayesian</b> (left panel) and <b>regular</b> (right panel) binary classifier that has been learning from ten training data (circles) with a weight decay \\ (\\alpha = 1.0\\). The decision boundary (\\(y=0.5\\), i.e. the activation \\(a=0\\)) is shown together with the levels 0.12,0.27,0.73,0.88 (corresponding to the activation \\(a=\\pm1,\\pm2\\)). Test data ...", "dateLastCrawled": "2021-12-21T06:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "[D] <b>advantages of bayesian neural network over regular</b> <b>neural</b> networks ...", "url": "https://www.reddit.com/r/statistics/comments/mrfc9x/d_advantages_of_bayesian_neural_network_over/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/.../comments/mrfc9x/d_<b>advantages_of_bayesian_neural_network</b>_over", "snippet": "This is a really good video that explains the differences between <b>regular</b> <b>neural</b> networks and <b>bayesian</b> <b>neural</b> networks. Apparently in <b>bayesian</b> <b>neural</b> networks, the weights aren&#39;t assigned a fixed value but instead each weight is assigned a probability distribution (e.g. each weight is given a normal probability distribution and we calculate the mean and variance).", "dateLastCrawled": "2021-04-15T14:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Comparing <b>Bayesian</b> and <b>neural</b> <b>network</b> supported lithotype prediction ...", "url": "https://www.pdgm.com/resource-library/articles-and-papers/2020/comparing-bayesian-and-neural-network-supported-li/", "isFamilyFriendly": true, "displayUrl": "https://www.pdgm.com/.../2020/comparing-<b>bayesian</b>-and-<b>neural</b>-<b>network</b>-supported-li", "snippet": "Comparing <b>Bayesian</b> and <b>neural</b> <b>network</b> supported lithotype prediction from seismic data Sabine Klarner1*, Dmitriy Kirnos 1, Natalya Ivanova, Aleksey Gritsenko and Olga Malinovskaya2 benchmark advanced <b>neural</b> <b>network</b> algorithms against standard probabilistic lithology classifications from seismic data to find out which approach works best and under which circumstances. Introduction In the past few years there has been increasing interest in the application of machine learning in the industry ...", "dateLastCrawled": "2022-01-07T07:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Combining <b>Bayesian</b> <b>Neural Networks</b> and Ensemble techniques | by Dylan ...", "url": "https://towardsdatascience.com/combining-bayesian-neural-networks-and-ensemble-techniques-a4a3a9072e79", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/combining-<b>bayesian</b>-<b>neural-networks</b>-and-ensemble...", "snippet": "MAP Ensemble techniques <b>Bayesian</b> <b>Neural Networks</b> Randomized MAP sampling Gaussian Mixture Models. Examples \u2018An Ensemble of <b>Bayesian</b> <b>Neural Networks</b> for Exoplanetary Atmospheric Retrieval\u2019 \u2018Uncertainty in <b>Neural Networks</b>: Approximately <b>Bayesian</b> Ensembling\u2019 \u2018Using Stacking to Average <b>Bayesian</b> Predictive Distributions\u2019 Terminology. f\u1d42(X) \u2014 <b>neural</b> <b>network</b> model p(W|X,Y) \u2014 posterior probability of NN weights W given data X,Y p(Y|X,W) \u2014 maximum likelihood of NN weights W from ...", "dateLastCrawled": "2022-02-01T05:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "[2107.08461] Differentially Private <b>Bayesian</b> <b>Neural</b> Networks on ...", "url": "https://arxiv.org/abs/2107.08461", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/abs/2107.08461", "snippet": "<b>Bayesian</b> <b>neural</b> <b>network</b> (BNN) allows for uncertainty quantification in prediction, offering an advantage over <b>regular</b> <b>neural</b> networks that has not been explored in the differential privacy (DP) framework. We fill this important gap by leveraging recent development in <b>Bayesian</b> deep learning and privacy accounting to offer a more precise analysis of the trade-off between privacy and accuracy in BNN. We propose three DP-BNNs that characterize the weight uncertainty for the same <b>network</b> ...", "dateLastCrawled": "2021-07-20T04:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Bayesian</b> <b>neural</b> <b>networks for flight trajectory prediction and safety</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0167923620300014", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0167923620300014", "snippet": "Next, we build two different types of <b>Bayesian</b> <b>neural</b> <b>network</b> models: in the first model, deep feedforward <b>Bayesian</b> <b>neural</b> networks (DNN) are trained with historical data for one-step-ahead prediction on the deviation between actual trajectory and target flight trajectory. In the second model, LSTM <b>neural</b> networks are trained with historical flight trajectory data to make longer-term predictions on the future flight trajectory. Afterwards, the two models are blended through a discrepancy ...", "dateLastCrawled": "2022-01-29T16:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>GitHub</b> - <b>LukasRinder/bayesian-neural-networks</b>: Different ...", "url": "https://github.com/LukasRinder/bayesian-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/LukasRinder/<b>bayesian</b>-<b>neural</b>-<b>networks</b>", "snippet": "Furthermore, the uncertainty estimates from the variational <b>Bayesian</b> <b>neural</b> networks are used to perform approximate Thompson sampling within a deep Q-<b>network</b> (DQN) for efficient exploration. The approaches are <b>compared</b> against each other and against the well known epsilon-greedy strategy.", "dateLastCrawled": "2021-09-19T15:55:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A <b>machine</b> <b>learning</b> approach to <b>Bayesian</b> parameter estimation | npj ...", "url": "https://www.nature.com/articles/s41534-021-00497-w", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41534-021-00497-w", "snippet": "The parameter estimation discussed in this manuscript is divided in two parts: i) a <b>neural</b> <b>network</b> is trained and ii) <b>Bayesian</b> estimation performed on a test set, which we detail below.", "dateLastCrawled": "2022-02-03T06:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "9.7. <b>Bayesian</b> <b>neural</b> networks \u2014 <b>Learning</b> from data", "url": "https://furnstahl.github.io/Physics-8820/notebooks/Machine_learning/Bayesian_neural_networks_tif285.html", "isFamilyFriendly": true, "displayUrl": "https://furnstahl.github.io/.../<b>Machine</b>_<b>learning</b>/<b>Bayesian</b>_<b>neural</b>_<b>networks</b>_tif285.html", "snippet": "<b>Bayesian</b> <b>neural</b> networks differ from plain <b>neural</b> networks in that their weights are assigned a probability distribution instead of a single value or point estimate. These probability distributions describe the uncertainty in weights and can be used to estimate uncertainty in predictions. Training a <b>Bayesian</b> <b>neural</b> <b>network</b> via variational inference learns the parameters of these distributions instead of the weights directly.", "dateLastCrawled": "2021-12-21T06:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Researchers Explore <b>Bayesian</b> <b>Neural</b> Networks -- Pure AI", "url": "https://pureai.com/articles/2021/09/07/bayesian-neural-networks.aspx", "isFamilyFriendly": true, "displayUrl": "https://pureai.com/articles/2021/09/07/<b>bayesian</b>-<b>neural</b>-<b>networks</b>.aspx", "snippet": "<b>Bayesian</b> <b>neural</b> networks are best explained using an <b>analogy</b> example. Suppose that instead of a <b>neural</b> <b>network</b>, you have a prediction equation y = (8.5 * x1) + (9.5 * x2) + 2.5 where y is the predicted income of an employee, x1 is normalized age, and x2 is years of job tenure. The predicted income of a 30-year old who has been on the job for 4 years would be y = (8.5 * 3.0) + (9.5 * 4.0) + 2.5 = 64.5 = $64,500. If you feed the same (age, tenure) input of (3.0, 4.0) to the prediction equation ...", "dateLastCrawled": "2022-01-30T04:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Bayesian</b> <b>Neural Network</b> Series Post 2: Background Knowledge | by Kumar ...", "url": "https://medium.com/neuralspace/bayesian-neural-network-series-post-2-background-knowledge-fdec6ac62d43", "isFamilyFriendly": true, "displayUrl": "https://medium.com/<b>neural</b>space/<b>bayesian</b>-<b>neural-network</b>-series-post-2-background...", "snippet": "I will try to brief the <b>neural</b> networks <b>analogy</b> with the brain and will spend more time explaining the Probabilistic <b>Machine</b> <b>Learning</b> segments that we will work on in future. Brain Analogies. A ...", "dateLastCrawled": "2022-01-30T12:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep <b>neural</b> <b>network</b> models, and it has been used for conducting ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Bayesian Belief Network in Artificial Intelligence</b> - Javatpoint", "url": "https://www.javatpoint.com/bayesian-belief-network-in-artificial-intelligence", "isFamilyFriendly": true, "displayUrl": "https://www.javatpoint.com/<b>bayesian-belief-network-in-artificial-intelligence</b>", "snippet": "<b>Bayesian Belief Network in artificial intelligence</b>. <b>Bayesian</b> belief <b>network</b> is key computer technology for dealing with probabilistic events and to solve a problem which has uncertainty. We can define a <b>Bayesian</b> <b>network</b> as: &quot;A <b>Bayesian</b> <b>network</b> is a probabilistic graphical model which represents a set of variables and their conditional ...", "dateLastCrawled": "2022-02-02T21:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine Learning by Analogy</b> - SlideShare", "url": "https://www.slideshare.net/ColleenFarrelly/machine-learning-by-analogy-59094152", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ColleenFarrelly/<b>machine-learning-by-analogy</b>-59094152", "snippet": "<b>Machine Learning by Analogy</b> 1. Colleen M. Farrelly 2. Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics behind these ...", "dateLastCrawled": "2022-01-31T07:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PPT) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> | Colleen Farrelly - Academia.edu", "url": "https://www.academia.edu/30404581/Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/30404581/<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "<b>Machine</b> <b>Learning</b> by <b>Analogy</b> Colleen M. Farrelly Overview of Problem Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics ...", "dateLastCrawled": "2022-01-02T06:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "ML MCQ all 5 - <b>Machine</b> <b>Learning</b> MCQ&#39;s - KCS 052 - StuDocu", "url": "https://www.studocu.com/in/document/dr-apj-abdul-kalam-technical-university/machine-learning-techniques/ml-mcq-all-5-machine-learning-mcqs/16412586", "isFamilyFriendly": true, "displayUrl": "https://www.studocu.com/.../ml-mcq-all-5-<b>machine</b>-<b>learning</b>-mcqs/16412586", "snippet": "Which of the following is not numerical functions in the various function representation of <b>Machine</b> <b>Learning</b>? (A) <b>Neural</b> <b>Network</b> (B) Support Vector Machines (C) Case-based (D) Linear Regression. Answer Correct option is C . FIND-S Algorithm starts from the most specific hypothesis and generalize it by considering only ____ examples. (A) Negative (B) Positive (C) Negative or Positive (D) None of the above; Answer Correct option is B. FIND-S algorithm ignores ___ examples. (A) Negative (B ...", "dateLastCrawled": "2022-02-03T04:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Neural Networks and Learning Machines</b> - uniba.sk", "url": "https://dai.fmph.uniba.sk/courses/NN/haykin.neural-networks.3ed.2009.pdf", "isFamilyFriendly": true, "displayUrl": "https://dai.fmph.uniba.sk/courses/NN/haykin.<b>neural</b>-<b>networks</b>.3ed.2009.pdf", "snippet": "<b>Neural Networks and Learning Machines</b> Third Edition Simon Haykin McMaster University Hamilton, Ontario, Canada New York Boston San Francisco London Toronto Sydney Tokyo Singapore Madrid Mexico City Munich Paris Cape Town Hong Kong Montreal. Library of Congress Cataloging-in-Publication Data Haykin, Simon <b>Neural networks and learning machines</b> / Simon Haykin.\u20143rd ed. p. cm. Rev. ed of: <b>Neural</b> networks. 2nd ed., 1999. Includes bibliographical references and index. ISBN-13: 978-0-13-147139-9 ...", "dateLastCrawled": "2022-02-02T00:48:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(bayesian neural network)  is like +(regular neural network)", "+(bayesian neural network) is similar to +(regular neural network)", "+(bayesian neural network) can be thought of as +(regular neural network)", "+(bayesian neural network) can be compared to +(regular neural network)", "machine learning +(bayesian neural network AND analogy)", "machine learning +(\"bayesian neural network is like\")", "machine learning +(\"bayesian neural network is similar\")", "machine learning +(\"just as bayesian neural network\")", "machine learning +(\"bayesian neural network can be thought of as\")", "machine learning +(\"bayesian neural network can be compared to\")"]}