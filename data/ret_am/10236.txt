{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) Deciding when to intervene: a <b>Markov decision process</b> approach ...", "url": "https://www.academia.edu/14366848/Deciding_when_to_intervene_a_Markov_decision_process_approach", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/14366848/Deciding_when_to_intervene_a_<b>Markov_decision_process</b>...", "snippet": "The main elements of a <b>Markov decision process</b>. instants; each of the resulting intervals is 2.2. IVs called \u2018<b>Markov</b> cycle\u2019. This value is chosen according to the problem at hand and repre- IVs are direct acyclic graphs providing a sents a plausible timing for patient representation of a single transition of a monitoring. <b>MDP</b> and, as defined in [8] can include the Thus, it is assumed that, at each time following nodes: point, the <b>decision</b> maker may observe the current state of the <b>Markov</b> ...", "dateLastCrawled": "2022-01-07T21:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>A Markov Decision Process Approach to Vacant</b> Taxi Routing with E ...", "url": "https://www.researchgate.net/publication/328013734_A_Markov_Decision_Process_Approach_to_Vacant_Taxi_Routing_with_E-hailing", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/328013734_<b>A_Markov_Decision_Process_Approach</b>...", "snippet": "Abstract. The optimal routing of a vacant taxi is formulated as a <b>Markov Decision Process</b> (<b>MDP</b>) problem to account for long-term profit over the full working period. The state is defined by the ...", "dateLastCrawled": "2022-01-05T03:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Decision</b> prioritization and causal reasoning in <b>decision</b> hierarchies", "url": "https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1009688", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1009688", "snippet": "For small problems the usual approach is to transform the POMDP over latent states into a fully-observable <b>Markov decision process</b> (<b>MDP</b>) over belief states\u2013a probability distribution over the latent states\u2013and solve it using Bellman\u2019s equation [15, 16]. However, this solution works only for problems with few states and actions and a short planning horizon; for more complex problems, efficient planning must rely on heuristic strategies and relaxations that are less well characterized ...", "dateLastCrawled": "2022-01-05T03:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Deep reinforcement learning in medical imaging: A literature review ...", "url": "https://www.sciencedirect.com/science/article/pii/S1361841521002395", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1361841521002395", "snippet": "The standard theory of RL is defined by a <b>Markov Decision Process</b> (<b>MDP</b>) in which rewards depend on the last state and action only. However, most of <b>real</b>-world <b>decision</b>-<b>making</b> is based non-Markovian model in which the next state depends on more than the current state and action. This work only focuses on <b>MDP</b>, however, the readers can learn more of the recent research on non-<b>MDP</b> in", "dateLastCrawled": "2022-02-07T08:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Recent advances in leveraging human guidance for sequential <b>decision</b> ...", "url": "https://link.springer.com/article/10.1007/s10458-021-09514-w", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10458-021-09514-w", "snippet": "<b>Markov</b> <b>decision</b> processes (MDPs) A standard reinforcement learning problem is formalized as a <b>Markov decision process</b> (<b>MDP</b>), defined as a tuple \\(\\langle \\mathcal {S}, \\mathcal {A}, \\mathcal {P}, \\mathcal {R}, \\gamma \\rangle\\) , where \\(\\mathcal {S}\\) is a set of environment states which encodes relevant information for an agent\u2019s <b>decision</b>. \\(\\mathcal {A}\\) is a set of agent actions. \\(\\mathcal {P}\\) is the state transition function which describes \\(p(s&#39;|s,a)\\), i.e., the probability of ...", "dateLastCrawled": "2022-01-31T18:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Decision Making Under Deep Uncertainty</b>: Climate Change and ...", "url": "https://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780190455811.001.0001/oxfordhb-9780190455811-e-50", "isFamilyFriendly": true, "displayUrl": "https://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780190455811.001.0001/oxfordhb...", "snippet": "A generalization of an <b>MDP</b> is the partially observable <b>Markov decision process</b> (POMDP). A POMDP models a situation where the system dynamics are determined by a <b>Markov decision process</b>, but the <b>decision</b> maker cannot directly observe these. Instead, the <b>decision</b> maker has yet to decide what path the future will take, and assigns a probability to a number of future states, based on best available information. The POMDP framework is general enough to model a variety of <b>real</b>-world sequential ...", "dateLastCrawled": "2022-01-28T06:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Top Artificial Intelligence Interview Questions and Answers (2022 ...", "url": "https://www.interviewbit.com/artificial-intelligence-interview-questions/", "isFamilyFriendly": true, "displayUrl": "https://www.interviewbit.com/artificial-intelligence-interview-questions", "snippet": "The Hidden <b>Markov</b> model is a probabilistic model which is used to identify the probabilistic <b>character</b> of any event. It says that an observed event is related to a set of probability distributions. If a system is being modeled into a <b>Markov</b>\u2019s chain, then the main goal of HMM is to identify the hidden layers of the <b>Markov</b>\u2019s chain. Hidden means that the particular state is not observable to the observer. It is generally used for temporal data. HMM finds its application in reinforcement ...", "dateLastCrawled": "2022-01-30T19:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Counterfactual state explanations for reinforcement learning agents via ...", "url": "https://www.sciencedirect.com/science/article/pii/S0004370221000060", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0004370221000060", "snippet": "Underlying a RL agent is the mathematical framework of a <b>Markov Decision Process</b> (<b>MDP</b>) [Puterman ], which models an agent <b>making</b> a sequence of decisions as it interacts with a stochastic environment. In the notation to follow in this section and in the rest of the manuscript, vectors, matrices and sets are in boldface while scalars are not. Formally, a <b>MDP</b> is a tuple", "dateLastCrawled": "2022-01-29T14:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "One Hundred Solved Exercises for the subject: Stochastic Processes I", "url": "https://www.stat.berkeley.edu/~aldous/150/takis_exercises.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.stat.berkeley.edu/~aldous/150/takis_exercises.pdf", "snippet": "Start with a rabbit of given <b>character</b> (GG, Gg, or gg) and mate it with a hybrid. The o\ufb00spring produced is again mated with a hybrid, and the <b>process</b> is repeated through a number of generations, always mating with a hybrid. (i) Write down the transition probabilities of the <b>Markov</b> chain thus de\ufb01ned. (ii) Assume that we start with a hybrid rabbit. Let \u00b5n be the probability dis-tribution of the <b>character</b> of the rabbit of the n-th generation. In other words, \u00b5n(GG),\u00b5n(Gg),\u00b5n(gg) are the ...", "dateLastCrawled": "2022-01-30T13:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "A gentle introduction to Deep <b>Reinforcement Learning</b> | by Jordi TORRES ...", "url": "https://towardsdatascience.com/drl-01-a-gentle-introduction-to-deep-reinforcement-learning-405b79866bf4", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/drl-01-a-gentle-introduction-to-deep-reinforcement...", "snippet": "DRL agents can sometimes control hazardous <b>real</b>-<b>life</b> Environments, <b>like</b> robots or cars, which increases the risk of <b>making</b> incorrect <b>choices</b>. There is an important field called safe RL that attempts to deal with this risk, for instance, learning a policy that maximizes rewards while operating within predefined safety constraints.", "dateLastCrawled": "2022-02-03T04:20:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Top Artificial Intelligence Interview Questions and Answers (2022 ...", "url": "https://www.interviewbit.com/artificial-intelligence-interview-questions/", "isFamilyFriendly": true, "displayUrl": "https://www.interviewbit.com/artificial-intelligence-interview-questions", "snippet": "<b>Markov</b>\u2019s <b>decision</b> <b>process</b> (<b>MDP</b>) is a mathematical approach for reinforcement learning. <b>Markov</b>&#39;s <b>decision</b> <b>process</b> (<b>MDP</b>) is a mathematical framework used to solve problems where outcomes are partially random and partly controlled. To solve a complex problem using <b>Markov</b>\u2019s <b>decision</b> <b>process</b>, the following basic things are needed-", "dateLastCrawled": "2022-01-30T19:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>A Markov Decision Process Approach to Vacant</b> Taxi Routing with E ...", "url": "https://www.researchgate.net/publication/328013734_A_Markov_Decision_Process_Approach_to_Vacant_Taxi_Routing_with_E-hailing", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/328013734_<b>A_Markov_Decision_Process_Approach</b>...", "snippet": "Abstract. The optimal routing of a vacant taxi is formulated as a <b>Markov Decision Process</b> (<b>MDP</b>) problem to account for long-term profit over the full working period. The state is defined by the ...", "dateLastCrawled": "2022-01-05T03:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) Deciding when to intervene: a <b>Markov decision process</b> approach ...", "url": "https://www.academia.edu/14366848/Deciding_when_to_intervene_a_Markov_decision_process_approach", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/14366848/Deciding_when_to_intervene_a_<b>Markov_decision_process</b>...", "snippet": "The main elements of a <b>Markov decision process</b>. instants; each of the resulting intervals is 2.2. IVs called \u2018<b>Markov</b> cycle\u2019. This value is chosen according to the problem at hand and repre- IVs are direct acyclic graphs providing a sents a plausible timing for patient representation of a single transition of a monitoring. <b>MDP</b> and, as defined in [8] can include the Thus, it is assumed that, at each time following nodes: point, the <b>decision</b> maker may observe the current state of the <b>Markov</b> ...", "dateLastCrawled": "2022-01-07T21:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Recent advances in leveraging human guidance for sequential <b>decision</b> ...", "url": "https://link.springer.com/article/10.1007/s10458-021-09514-w", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10458-021-09514-w", "snippet": "<b>Markov</b> <b>decision</b> processes (MDPs) A standard reinforcement learning problem is formalized as a <b>Markov decision process</b> (<b>MDP</b>), defined as a tuple \\(\\langle \\mathcal {S}, \\mathcal {A}, \\mathcal {P}, \\mathcal {R}, \\gamma \\rangle\\) , where \\(\\mathcal {S}\\) is a set of environment states which encodes relevant information for an agent\u2019s <b>decision</b>. \\(\\mathcal {A}\\) is a set of agent actions. \\(\\mathcal {P}\\) is the state transition function which describes \\(p(s&#39;|s,a)\\), i.e., the probability of ...", "dateLastCrawled": "2022-01-31T18:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Decision</b> prioritization and causal reasoning in <b>decision</b> hierarchies", "url": "https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1009688", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1009688", "snippet": "For small problems the usual approach is to transform the POMDP over latent states into a fully-observable <b>Markov decision process</b> (<b>MDP</b>) over belief states\u2013a probability distribution over the latent states\u2013and solve it using Bellman\u2019s equation [15, 16]. However, this solution works only for problems with few states and actions and a short planning horizon; for more complex problems, efficient planning must rely on heuristic strategies and relaxations that are less well characterized ...", "dateLastCrawled": "2022-01-05T03:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) Planning for success : The social approach to building Bayesian ...", "url": "https://www.academia.edu/67525519/Planning_for_success_The_social_approach_to_building_Bayesian_models", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/67525519/Planning_for_success_The_social_approach_to_building...", "snippet": "We introduce a new variant of <b>Markov</b> <b>decision</b> processes called MDPs with action results, and a variant of dynamic Bayesian networks called bowties, for modeling the effects of stochastic actions. Bowties grew out of our work on <b>decision</b>-support . \u00d7 Close Log In. Log in with Facebook Log in with Google. or. Email. Password. Remember me on this computer. or reset password ...", "dateLastCrawled": "2022-01-12T19:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "One Hundred Solved Exercises for the subject: Stochastic Processes I", "url": "https://www.stat.berkeley.edu/~aldous/150/takis_exercises.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.stat.berkeley.edu/~aldous/150/takis_exercises.pdf", "snippet": "Start with a rabbit of given <b>character</b> (GG, Gg, or gg) and mate it with a hybrid. The o\ufb00spring produced is again mated with a hybrid, and the <b>process</b> is repeated through a number of generations, always mating with a hybrid. (i) Write down the transition probabilities of the <b>Markov</b> chain thus de\ufb01ned. (ii) Assume that we start with a hybrid rabbit. Let \u00b5n be the probability dis-tribution of the <b>character</b> of the rabbit of the n-th generation. In other words, \u00b5n(GG),\u00b5n(Gg),\u00b5n(gg) are the ...", "dateLastCrawled": "2022-01-30T13:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Counterfactual state explanations for reinforcement learning agents via ...", "url": "https://www.sciencedirect.com/science/article/pii/S0004370221000060", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0004370221000060", "snippet": "Underlying a RL agent is the mathematical framework of a <b>Markov Decision Process</b> (<b>MDP</b>) [Puterman ], which models an agent <b>making</b> a sequence of decisions as it interacts with a stochastic environment. In the notation to follow in this section and in the rest of the manuscript, vectors, matrices and sets are in boldface while scalars are not. Formally, a <b>MDP</b> is a tuple", "dateLastCrawled": "2022-01-29T14:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A gentle introduction to Deep <b>Reinforcement Learning</b> | by Jordi TORRES ...", "url": "https://towardsdatascience.com/drl-01-a-gentle-introduction-to-deep-reinforcement-learning-405b79866bf4", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/drl-01-a-gentle-introduction-to-deep-reinforcement...", "snippet": "It is also the most trending type of Machine Learning because it can solve a wide range of complex <b>decision</b>-<b>making</b> tasks that were previously out of reach for a machine to solve <b>real</b>-world problems with human-like intelligence. Today I\u2019m starting a series about Deep <b>Reinfor c ement Learning</b> that will bring the topic closer to the reader. The purpose is to review the field from specialized terms and jargons to fundamental concepts and classical algorithms in the area, that newbies would not ...", "dateLastCrawled": "2022-02-03T04:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Can reinforcement learning (RL) be applied to solve any problem we can ...", "url": "https://www.quora.com/Can-reinforcement-learning-RL-be-applied-to-solve-any-problem-we-can-think-of-What-problems-cannot-be-solved-by-RL-What-are-the-defining-characteristics-of-problems-which-cannot-be-solved-by-RL", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Can-reinforcement-learning-RL-be-applied-to-solve-any-problem-we...", "snippet": "Answer: Situations that are not explicitly state driven in a discrete time frame - or does not lend itself unto that - can be considered to be less attractive to RL. Problems that cannot be solved by RL - may include: Problems that would have a way too large allocation of memory to compute/expl...", "dateLastCrawled": "2022-01-21T23:14:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Dialogue Management</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/computer-science/dialogue-management", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/computer-science/<b>dialogue-management</b>", "snippet": "With utility maximization the idea is to specify priorities for the system in terms of a <b>real</b>-valued objective function; ... In the first applications of RL in spoken dialogue systems, dialogue was formalized as a <b>Markov decision process</b> (<b>MDP</b>) (see, for example, [44]); more recently partially observable <b>Markov</b> <b>decision</b> processes (POMDPs) have been used to handle the various uncertainties inherent in dialogue interactions (see, for example, [24]). The advantages and disadvantages of data ...", "dateLastCrawled": "2022-01-26T04:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Decision</b> prioritization and causal reasoning in <b>decision</b> hierarchies", "url": "https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1009688", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1009688", "snippet": "For small problems the usual approach is to transform the POMDP over latent states into a fully-observable <b>Markov decision process</b> (<b>MDP</b>) over belief states\u2013a probability distribution over the latent states\u2013and solve it using Bellman\u2019s equation [15, 16]. However, this solution works only for problems with few states and actions and a short planning horizon; for more complex problems, efficient planning must rely on heuristic strategies and relaxations that are less well characterized ...", "dateLastCrawled": "2022-01-05T03:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Glossary - <b>Lone Star Analysis</b>", "url": "https://www.lone-star.com/insights/glossary-2/", "isFamilyFriendly": true, "displayUrl": "https://www.lone-star.com/insights/glossary-2", "snippet": "<b>Markov Decision Process</b>. <b>Markov</b> <b>Decision</b> Processes (MDPs) are used to model decisions which are the result of both random, uncontrolled causes (like weather) as well as <b>choices</b> made by one or more <b>decision</b> makers. <b>MDP</b> is named for the Markovs, a family of mathematicians, and in particular, Andrey <b>Markov</b>. MDPs are related to a number of analysis and optimization topics, including dynamic programming, learning, robotics, automated control, economics, manufacturing, and other <b>decision</b> processes ...", "dateLastCrawled": "2022-01-30T04:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A step-by-step tutorial on active inference and its application to ...", "url": "https://www.sciencedirect.com/science/article/pii/S0022249621000973", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0022249621000973", "snippet": "A <b>Markov decision process</b> describes beliefs about abstract states of the world, how they are expected to change over time, and how actions are selected to seek out preferred outcomes or rewards based on beliefs about states. This class of models assumes the \u2018<b>Markov</b> property\u2019, which simply means that beliefs about the current state of the world are all that matter for an agent when deciding which actions to take (i.e., that all knowledge about past states is implicitly \u2018packed into ...", "dateLastCrawled": "2022-02-05T08:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Deep Active Inference and Scene Construction", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7861336/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7861336", "snippet": "Then we detail the <b>Markov Decision Process</b> generative model that our active inference agents entertain, and describe the belief-updating procedures used to invert generative models, given observed sensory data. Having appropriately set up our scene construction task, we then report the results of simulations, with differential effects of sensory uncertainty and prior belief strength appearing in several aspects of active evidence accumulation in this hierarchical environment. These ...", "dateLastCrawled": "2021-05-26T14:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Contextual perception under active inference | Scientific Reports", "url": "https://www.nature.com/articles/s41598-021-95510-9", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41598-021-95510-9", "snippet": "We used a <b>Markov decision process</b> (<b>MDP</b>) formulation of active inference for the mental state attribution task. <b>MDP</b> models describe the statistical nature of an environment in terms of probability ...", "dateLastCrawled": "2021-12-24T16:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A gentle introduction to Deep <b>Reinforcement Learning</b> | by Jordi TORRES ...", "url": "https://towardsdatascience.com/drl-01-a-gentle-introduction-to-deep-reinforcement-learning-405b79866bf4", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/drl-01-a-gentle-introduction-to-deep-reinforcement...", "snippet": "It is also the most trending type of Machine Learning because it <b>can</b> solve a wide range of complex <b>decision</b>-<b>making</b> tasks that were previously out of reach for a machine to solve <b>real</b>-world problems with human-like intelligence. Today I\u2019m starting a series about Deep <b>Reinfor c ement Learning</b> that will bring the topic closer to the reader. The purpose is to review the field from specialized terms and jargons to fundamental concepts and classical algorithms in the area, that newbies would not ...", "dateLastCrawled": "2022-02-03T04:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Artificial Intelligence \u2013 Harder <b>Choices</b> Archive", "url": "https://harderchoices.wordpress.com/tag/artificial-intelligence/", "isFamilyFriendly": true, "displayUrl": "https://harder<b>choices</b>.wordpress.com/tag/artificial-intelligence", "snippet": "I wanted to avoid <b>making</b> this post as there will be zero code. But as I assumed my series will be stand-alone I have to write it. So to move further I have to first establish a definition of Finite <b>Markov Decision Process</b>. It is a crucial assumption. Solving the problem of Finite <b>Markov Decision Process</b> (or finite <b>MDP</b>) is our main goal in most reinforcement learning. What? Who? When? \ud83e\udd14. Very (very) high-level definition of <b>MDP</b> for me is: <b>Markov Decision Process</b> is a framework allowing us ...", "dateLastCrawled": "2021-12-25T08:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 8, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Artificial neural network</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Artificial_neural_network", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Artificial_neural_network</b>", "snippet": "This <b>can</b> <b>be thought</b> of as learning with a &quot;teacher&quot;, in the form of a function that provides continuous feedback on the quality of solutions obtained thus far. Unsupervised learning. In unsupervised learning, input data is given along with the cost function, some function of the data and the network&#39;s output. The cost function is dependent on the task (the model domain) and any a priori assumptions (the implicit properties of the model, its parameters and the observed variables). As a ...", "dateLastCrawled": "2022-02-07T09:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Artificial Intelligence By Example.pdf [5wgl5d599xo7]", "url": "https://vbook.pub/documents/artificial-intelligence-by-examplepdf-5wgl5d599xo7", "isFamilyFriendly": true, "displayUrl": "https://vbook.pub/documents/artificial-intelligence-by-examplepdf-5wgl5d599xo7", "snippet": "It <b>can</b> also be applied to <b>decision</b>-<b>making</b> in games or <b>real</b> <b>life</b>. However, in a <b>real</b>-<b>life</b> case study problem (such as defining the reward matrix in a warehouse for the AGV, for example), the difficulty will be to design a matrix that everybody agrees with. This means many meetings with the IT department to obtain data, the SME and reinforcement learning experts. An AGV requires information coming from different sources: daily forecasts and <b>real</b>-time warehouse flows. At one point, the project ...", "dateLastCrawled": "2022-01-27T23:13:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) Deciding when to intervene: a <b>Markov decision process</b> approach ...", "url": "https://www.academia.edu/14366848/Deciding_when_to_intervene_a_Markov_decision_process_approach", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/14366848/Deciding_when_to_intervene_a_<b>Markov_decision_process</b>...", "snippet": "<b>MDP</b> and, as defined in [8] <b>can</b> include the Thus, it is assumed that, at each time following nodes: point, the <b>decision</b> maker may observe the current state of the <b>Markov</b> <b>process</b> being 2.2.1. State nodes controlled. At the same time, the <b>decision</b> Each state node represents a variable ob- maker takes one action, from a finite set of tained from the factorization 1 of the <b>MDP</b> possible actions, relying on the current state space state. Since the IV specifies a <b>MDP</b> itself. Therefore the stochastic ...", "dateLastCrawled": "2022-01-07T21:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>A Markov Decision Process Approach to Vacant</b> Taxi Routing with E ...", "url": "https://www.researchgate.net/publication/328013734_A_Markov_Decision_Process_Approach_to_Vacant_Taxi_Routing_with_E-hailing", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/328013734_<b>A_Markov_Decision_Process_Approach</b>...", "snippet": "Abstract. The optimal routing of a vacant taxi is formulated as a <b>Markov Decision Process</b> (<b>MDP</b>) problem to account for long-term profit over the full working period. The state is defined by the ...", "dateLastCrawled": "2022-01-05T03:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Dialogue Management</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/computer-science/dialogue-management", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/computer-science/<b>dialogue-management</b>", "snippet": "In the first applications of RL in spoken dialogue systems, dialogue was formalized as a <b>Markov decision process</b> (<b>MDP</b>) ... more recently partially observable <b>Markov</b> <b>decision</b> processes (POMDPs) have been used to handle the various uncertainties inherent in dialogue interactions (see, for example, [24 ]). The advantages and disadvantages of data-driven approaches are the subject of much active research. Their role in applications for ambient environments will be discussed further in Section 9 ...", "dateLastCrawled": "2022-01-26T04:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Decision</b> prioritization and causal reasoning in <b>decision</b> hierarchies", "url": "https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1009688", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1009688", "snippet": "For small problems the usual approach is to transform the POMDP over latent states into a fully-observable <b>Markov decision process</b> (<b>MDP</b>) over belief states\u2013a probability distribution over the latent states\u2013and solve it using Bellman\u2019s equation [15, 16]. However, this solution works only for problems with few states and actions and a short planning horizon; for more complex problems, efficient planning must rely on heuristic strategies and relaxations that are less well characterized ...", "dateLastCrawled": "2022-01-05T03:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) Planning for success : The social approach to building Bayesian ...", "url": "https://www.academia.edu/67525519/Planning_for_success_The_social_approach_to_building_Bayesian_models", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/67525519/Planning_for_success_The_social_approach_to_building...", "snippet": "We introduce a new variant of <b>Markov</b> <b>decision</b> processes called MDPs with action results, and a variant of dynamic Bayesian networks called bowties, for modeling the effects of stochastic actions. Bowties grew out of our work on <b>decision</b>-support . \u00d7 Close Log In. Log in with Facebook Log in with Google. or. Email. Password. Remember me on this computer. or reset password ...", "dateLastCrawled": "2022-01-12T19:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Contextual perception under active inference | Scientific Reports", "url": "https://www.nature.com/articles/s41598-021-95510-9", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41598-021-95510-9", "snippet": "We used a <b>Markov decision process</b> (<b>MDP</b>) formulation of active inference for the mental state attribution task. <b>MDP</b> models describe the statistical nature of an environment in terms of probability ...", "dateLastCrawled": "2021-12-24T16:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A probabilistic argumentation framework for reinforcement learning ...", "url": "https://link.springer.com/article/10.1007/s10458-019-09404-2", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10458-019-09404-2", "snippet": "In this section, we outline <b>Markov</b> <b>decision</b> processes (MDPs) (Sect. 3.1), a mathematical framework widely used to model reinforcement learning problems.Then, we introduce a popular RL algorithm, SARSA (Sect. 3.2), and we briefly illustrate its use in an <b>MDP</b> (Sect. 3.2). <b>Markov</b> <b>decision</b> processes. An <b>MDP</b> is a mathematical framework for modelling <b>decision</b> <b>making</b> when outcomes of actions are partly unknown.", "dateLastCrawled": "2022-01-12T07:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "International Journal of Computational Intelligence Systems - volume 14 ...", "url": "https://www.atlantis-press.com/journals/ijcis/issue/498", "isFamilyFriendly": true, "displayUrl": "https://www.atlantis-press.com/journals/ijcis/issue/498", "snippet": "In this paper, a <b>Markov decision process</b> (<b>MDP</b>) model was established to study emergency medical material scheduling strategies for public health emergencies such as COVID-19. Within the constraints of dispatchable supplies, the priority of each medical node complicates the problem of deciding which hospital...", "dateLastCrawled": "2022-02-03T18:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Decision Making Under Deep Uncertainty</b>: Climate Change and ...", "url": "https://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780190455811.001.0001/oxfordhb-9780190455811-e-50", "isFamilyFriendly": true, "displayUrl": "https://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780190455811.001.0001/oxfordhb...", "snippet": "One option is to consider applying a method where the <b>decision</b> maker <b>can</b> make a choice each year based on the results of the prior year, or a <b>Markov decision process</b> (<b>MDP</b>) (Howard, 1960) This is like a set of <b>decision</b> trees (specifically, the chance nodes) that have been strung together over time. The homeowner might use these to create a set of possible future of no flooding vs. a catastrophic flood.", "dateLastCrawled": "2022-01-28T06:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Keras Reinforcement Learning Projects - VSIP.INFO", "url": "https://vsip.info/keras-reinforcement-learning-projects-pdf-free.html", "isFamilyFriendly": true, "displayUrl": "https://vsip.info/keras-reinforcement-learning-projects-pdf-free.html", "snippet": "<b>Markov Decision Process</b> To avoid load problems and computational difficulties, the agentenvironment interaction is considered an <b>MDP</b>. An <b>MDP</b> is a discrete-time stochastic control <b>process</b>. Stochastic processes are mathematical models used to study the evolution of phenomena following random or probabilistic laws. It is known that in all natural phenomena, both by their very nature and by observational errors, a random or accidental component is present. This component causes the following: at ...", "dateLastCrawled": "2021-12-20T20:27:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Why does <b>Markov Decision Process</b> matter in Reinforcement <b>Learning</b>? | by ...", "url": "https://towardsdatascience.com/why-does-malkov-decision-process-matter-in-reinforcement-learning-b111b46b41bd", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/why-does-malkov-<b>decision</b>-<b>process</b>-matter-in...", "snippet": "It is named by <b>analogy</b> to \u201cone-armed bandit\u201d(= a slot <b>machine</b>) although the framework has k levers instead of one. ... we introduce <b>Markov Decision Process</b>(<b>MDP</b>) to solve such a problem. An <b>MDP</b> consists of two elements; the agent and the environment. The agent is a learner or <b>decision</b>-maker. In the above example, the agent is the rabbit. The environment is everything surrounding the agent. In the example, the environment includes everything in the field where the rabbit is with food and ...", "dateLastCrawled": "2022-01-31T01:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Markov Decision Process</b>: How Does Value Iteration Work? | Baeldung on ...", "url": "https://www.baeldung.com/cs/mdp-value-iteration", "isFamilyFriendly": true, "displayUrl": "https://www.baeldung.com/cs/<b>mdp</b>-value-iteration", "snippet": "From this point, we can make an <b>analogy</b> with the <b>Markov</b> model since the solution for this problem is a sequence of actions. A <b>Markov Decision Process</b> is used to model the agent, considering that the agent itself generates a series of actions. In the real world, we can have observable, hidden, or partially observed states, depending on the ...", "dateLastCrawled": "2022-01-30T20:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Introduction to Machine Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/24/introduction-to-machine-learning-2/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/24/<b>introduction-to-machine-learning</b>-2", "snippet": "<b>Markov decision process</b> (<b>MDP</b>): In reinforcement <b>learning</b>, <b>MDP</b> is a mathematical framework for modeling <b>decision</b>-making of an agent in situations or environments where outcomes are partly random and partly under control. In this model, environment is modeled as a set of states and actions that can be performed by an agent to control the system\u2019s state. The objective is to control the system in such a way that the agent\u2019s total payoff is maximized.", "dateLastCrawled": "2022-01-17T05:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "CSE599i: Online and Adaptive <b>Machine</b> <b>Learning</b> Winter 2018 Lecture 19 ...", "url": "https://courses.cs.washington.edu/courses/cse599i/18wi/resources/lecture19/lecture19.pdf", "isFamilyFriendly": true, "displayUrl": "https://courses.cs.washington.edu/courses/cse599i/18wi/resources/lecture19/lecture19.pdf", "snippet": "1.1Summary of <b>Markov</b> <b>Decision</b> Processes A <b>Markov Decision Process</b> (<b>MDP</b>) is a probabilistic model for reward-incentivized, memoryless, sequential <b>decision</b>-making. An <b>MDP</b> models a scenario in which an agent (the <b>decision</b> maker) iteratively observes the", "dateLastCrawled": "2021-09-07T09:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Markov decision process</b>: value iteration with code implementation | by ...", "url": "https://medium.com/@ngao7/markov-decision-process-value-iteration-2d161d50a6ff", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@ngao7/<b>markov-decision-process</b>-value-iteration-2d161d50a6ff", "snippet": "<b>Markov decision process</b>, <b>MDP</b>, value iteration, policy iteration, policy evaluation, policy improvement, sweep, iterative policy evaluation, policy, optimal policy ...", "dateLastCrawled": "2022-01-08T01:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Markov decision process</b>: policy iteration with code implementation | by ...", "url": "https://medium.com/@ngao7/markov-decision-process-policy-iteration-42d35ee87c82?source=post_internal_links---------0-------------------------------", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@ngao7/<b>markov-decision-process</b>-policy-iteration-42d35ee87c82?source=...", "snippet": "<b>Markov decision process</b>: policy iteration with code implementation . Nan. Dec 19, 2021 \u00b7 16 min read. In today\u2019s story we focus on policy iteration of <b>MDP</b>. We are still using the grid world ...", "dateLastCrawled": "2022-01-22T13:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Meta-<b>Learning</b> as a <b>Markov Decision Process</b>", "url": "https://hal.archives-ouvertes.fr/tel-02422144v2/document", "isFamilyFriendly": true, "displayUrl": "https://hal.archives-ouvertes.fr/tel-02422144v2/document", "snippet": "Meta-<b>Learning</b> as a <b>Markov Decision Process</b>. <b>Machine</b> <b>Learning</b> [cs.LG]. Uni-versit\u00e9 Paris Saclay (COmUE), 2019. English. \uffffNNT: 2019SACLS588\uffff. \ufffftel-02422144v2\uffff I would like to dedicate this thesis to my loving parents doctorat CLS588 Meta-<b>Learning</b> as a <b>Markov Decision Process</b> Th\u00e8se de doctorat de l\u2019Universit\u00e9 Paris-Saclay pr\u00e9par\u00e9e \u00e0 l\u2019Universit\u00e9 Paris-Sud Ecole doctorale n 580 Sciences et Technologies de l\u2019Information et de la Communication (STIC) Sp\u00e9cialit\u00e9 de doctorat ...", "dateLastCrawled": "2022-01-13T07:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>machine</b> <b>learning</b> - Why is the <b>optimal policy</b> in <b>Markov Decision Process</b> ...", "url": "https://stats.stackexchange.com/questions/132890/why-is-the-optimal-policy-in-markov-decision-process-mdp-independent-of-the-i", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/132890", "snippet": "The intuition behind the argument saying that the <b>optimal policy</b> is independent of initial state is the following: The <b>optimal policy</b> is defined by a function that selects an action for every possible state and actions in different states are independent.. Formally speaking, for an unknown initial distribution, the value function to maximize would be the following (not conditioned on initial state)", "dateLastCrawled": "2022-01-25T23:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Machine</b> <b>learning</b> and AI <b>in marketing \u2013 Connecting computing power to</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0167811620300410", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0167811620300410", "snippet": "These tasks are often formulated as a <b>Markov decision process</b> (<b>MDP</b>), a structure familiar to marketing researchers who investigate forward-looking behaviors using dynamic programming models. The <b>learning</b> algorithm needs to determine the actions to take to both learn the environment&#39;s characteristics and craft optimal policy of actions given the states. This type of <b>machine</b> <b>learning</b> tasks has received heightened attention due to recent methodological advancement and increasing usages in the ...", "dateLastCrawled": "2022-01-12T18:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Real-life <b>examples</b> of <b>Markov</b> <b>Decision</b> Processes - Cross Validated", "url": "https://stats.stackexchange.com/questions/145122/real-life-examples-of-markov-decision-processes", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/145122", "snippet": "A Markovian <b>Decision</b> <b>Process</b> indeed has to do with going from one state to another and is mainly used for planning and <b>decision</b> making. The theory. Just repeating the theory quickly, an <b>MDP</b> is: $$\\text{<b>MDP</b>} = \\langle S,A,T,R,\\gamma \\rangle$$", "dateLastCrawled": "2022-01-24T10:32:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Overview: Representation Techniques", "url": "https://webcms3.cse.unsw.edu.au/static/uploads/course/COMP4418/17s2/9c0e19e0e02df66fffb5d0bd4c20697922f5ffbf9a602b66bec3f74ac83fb77c/DecisionMaking.pdf", "isFamilyFriendly": true, "displayUrl": "https://webcms3.cse.unsw.edu.au/static/uploads/course/COMP4418/17s2/9c0e19e0e02df66...", "snippet": "<b>Markov Decision Process MDP is like</b> a Markov process, except every round we make a decision Transition probabilities depend on actions taken P(St+1 = S&#39; | St = s, At = a) = P(S, a, S&#39;) Rewards for every state, action pair u(St = s, At = a) Discount factor \u03b4 Example. A <b>machine</b> can be in one of three states: good, deteriorating, broken Can take ...", "dateLastCrawled": "2022-01-21T05:20:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], []], "all_bing_queries": ["+(markov decision process (mdp))  is like +(making choices for a real life character)", "+(markov decision process (mdp)) is similar to +(making choices for a real life character)", "+(markov decision process (mdp)) can be thought of as +(making choices for a real life character)", "+(markov decision process (mdp)) can be compared to +(making choices for a real life character)", "machine learning +(markov decision process (mdp) AND analogy)", "machine learning +(\"markov decision process (mdp) is like\")", "machine learning +(\"markov decision process (mdp) is similar\")", "machine learning +(\"just as markov decision process (mdp)\")", "machine learning +(\"markov decision process (mdp) can be thought of as\")", "machine learning +(\"markov decision process (mdp) can be compared to\")"]}