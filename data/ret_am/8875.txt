{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "The Man, The Machine, And The <b>Black</b> <b>Box</b>: ML Observability", "url": "https://blog.re-work.co/the-man-the-machine-and-the-black-box-ml-observability/", "isFamilyFriendly": true, "displayUrl": "https://blog.re-work.co/the-man-the-machine-and-the-<b>black</b>-<b>box</b>-ml-observability", "snippet": "There&#39;s other ways, such as <b>equalized</b> <b>odds</b>, which means that you have two different groups, with equal chances of those within the groups to be qualified individuals to be approved or rejected. It&#39;s really interesting to try to understand where this one works better than something <b>like</b> individual fairness, through which we&#39;re doing high-level covers, and engaging counterfactual fairness. If you have one individual who&#39;s part of one group and they were evaluated and there was a label given to ...", "dateLastCrawled": "2022-01-21T07:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "The <b>AIF360 team adds compatibility with scikit-learn</b> \u2013 IBM Developer", "url": "https://developer.ibm.com/blogs/the-aif360-team-adds-compatibility-with-scikit-learn/", "isFamilyFriendly": true, "displayUrl": "https://developer.ibm.com/blogs/the-<b>aif360-team-adds-compatibility-with-scikit-learn</b>", "snippet": "The Calibrated <b>Equalized</b> <b>Odds</b> post-processor also requires a workaround. Post-processors train on predictions from <b>a black</b>-<b>box</b> estimator and ground-truth values to produce fairer predictions. This alone is without precedent in scikit-learn. Furthermore, to avoid data leakage, the training set for the post-processor should differ from the training set of the estimator. The", "dateLastCrawled": "2021-11-18T00:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "GitHub - rtikyani/CS634_IBM_Fairness_Pipeline: Detecting and mitigating ...", "url": "https://github.com/rtikyani/CS634_IBM_Fairness_Pipeline", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/rtikyani/CS634_IBM_Fairness_Pipeline", "snippet": "The post-processing treats the learned model as <b>a black</b> <b>box</b>. This algorithm should be the only algorithm used if the training data or learning algorithm cannot be modified. There are three bias mitigation algorithms: <b>equalized</b> <b>odds</b> postprocessing, calibrated <b>equalized</b> <b>odds</b> postprocessing, and reject option classification. [Favorable Label]: Value corresponds to an advantageous outcome [Protected Attribute:] Partitions a population into groups with different amounts of benefits, ie) gender ...", "dateLastCrawled": "2022-01-04T19:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Multiaccuracy: <b>Black</b>-<b>Box</b> <b>Post-Processing</b> for Fairness in Classi cation", "url": "https://cs.stanford.edu/~mpkim/pubs/multiaccuracy.pdf", "isFamilyFriendly": true, "displayUrl": "https://cs.stanford.edu/~mpkim/pubs/multiaccuracy.pdf", "snippet": "<b>black</b>-<b>box</b> framework allows for improved fairness and accountability of predictions, even when the predictor is minimally transparent. We prove that Multiaccuracy Boost converges e ciently and show that if the initial model is accurate on an identi able subgroup, then the post-processed model will be also. We experimentally demonstrate the e ectiveness of the approach to improve the accuracy among minority subgroups in diverse applications (image classi cation, nance, population health ...", "dateLastCrawled": "2021-12-22T09:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>BIAS MITIGATION POST-PROCESSING FOR INDIVIDUAL AND GROUP</b> FAIRNESS - <b>IBM</b>", "url": "https://www.ibm.com/downloads/cas/WM4MWDOE", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ibm.com</b>/downloads/cas/WM4MWDOE", "snippet": "systematically explores the decision space of any <b>black</b> <b>box</b> classi-\ufb01er to generate test samples that have an enhanced chance of being biased. The method uses two kinds of search: (a) a global search which explores the decision space such that diverse areas are cov-ered, and (b) a local search which generates test cases by intelli-gently perturbing the values of non-protected features of an already found individually-biased sample. The key idea is to use dynamic symbolic execution, an ...", "dateLastCrawled": "2022-02-01T21:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Programming Fairness in Algorithms</b> | by Matthew Stewart, PhD Researcher ...", "url": "https://towardsdatascience.com/programming-fairness-in-algorithms-4943a13dd9f8", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>programming-fairness-in-algorithms</b>-4943a13dd9f8", "snippet": "This is a combination of statistical parity for true positives and false positives simultaneously and is also know as <b>equalized</b> <b>odds</b>. Illustration of positive rate parity (<b>equalized</b> <b>odds</b>). Notice that in the first group, all those with Y=1 (blue boxes) were classified as positives (C=1).", "dateLastCrawled": "2022-01-23T21:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Fairness improvement for <b>black</b>-<b>box</b> classifiers with Gaussian process ...", "url": "https://www.sciencedirect.com/science/article/pii/S0020025521006861", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0020025521006861", "snippet": "However, <b>like</b> humans, ... Illustration of post-processing approach for fairness improvement. <b>A black</b>-<b>box</b> pre-trained classifier f (x) (e.g. a home loan approval system) achieves a high prediction performance (accuracy = 0.80) on a test set D = [X, y] (e.g. a list of applicants), but suffers from a high discrimination between \u201cmale\u201d and \u201cfemale\u201d applicants (fairness = 0.00). Specifically, f (x) rejects all applications by women and accepts all applications by men. A post-processing ...", "dateLastCrawled": "2022-01-28T12:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>xGEMs: Generating Examplars to Explain Black</b>-<b>Box</b> Models | DeepAI", "url": "https://deepai.org/publication/xgems-generating-examplars-to-explain-black-box-models", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>xgems-generating-examplars-to-explain-black</b>-<b>box</b>-models", "snippet": "<b>Black</b>-<b>box</b> Classifier Accuracy Confounding metric f 1 \u03d5 0.9933 0.1704 f 2 \u03d5 0.9155 0.4323 Table 2: Confounding metric Target label <b>Black</b>-<b>box</b> <b>Black</b> Hair Blond Hair f 1 \u03d5 Male:0.4550 Male:0.1432 Female:0.0159 Female:0.0484 Overall:0.2430 Overall:0.0539 f 2 \u03d5 Male:0.7716 Male:0.1475 Female:0.0045 Female:0.5024 Overall:0.4012 Overall:0.4821 Table 3: Confounding metric by gender", "dateLastCrawled": "2021-12-23T20:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Achieving Trusted AI With Model Interpretability - Dataiku Community", "url": "https://community.dataiku.com/t5/General-Discussion/Achieving-Trusted-AI-With-Model-Interpretability/m-p/21793", "isFamilyFriendly": true, "displayUrl": "https://community.dataiku.com/t5/General-Discussion/Achieving-Trusted-AI-With-Model...", "snippet": "<b>Black</b>-<b>box</b> models have attracted the ire of consumers and regulators, so applying deep learning and other complex models in applications that impact people could be severely curtailed by government regulations, <b>like</b> the recent EU proposed Artificial Intelligence Act, unless we can develop tools that allow us to peer into these models and explain decisions in human terms. While the science of model interpretation progresses, implementing highly interpretable white-<b>box</b> models and using tooling ...", "dateLastCrawled": "2022-01-14T06:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>I\u2019ve Been Equalized</b> . . | ANGLO AMERICAN PICTURES", "url": "https://angloamericanpictures.wordpress.com/2014/10/02/ive-been-equalized/", "isFamilyFriendly": true, "displayUrl": "https://angloamericanpictures.wordpress.com/2014/10/02/<b>ive-been-equalized</b>", "snippet": "<b>I\u2019ve Been Equalized</b> . . 02 Oct. Watching Denzel Washington acting in almost any movie is rewarding. He\u2019s now starring in a remake of the TV series, The Equalizer. Comparing this movie to the Television show would be <b>like</b> comparing Metallica to The Dave Clark Five, both play music however the difference is extensive. Robert McCall as the Equalizer takes time developing what you know is coming, and when it does it comes at you <b>like</b> a punch to the gut followed by a kick to the head. No ...", "dateLastCrawled": "2022-01-21T11:04:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Fairness improvement for <b>black</b>-<b>box</b> classifiers with Gaussian process ...", "url": "https://www.sciencedirect.com/science/article/pii/S0020025521006861", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0020025521006861", "snippet": "Post-processing approaches are widely considered as successful tools to improve the fairness of <b>black</b>-<b>box</b> ML classifiers. ... , calibration, or <b>equalized</b> <b>odds</b>. Among them, demographic parity is one of the most widely-used. Individual fairness aims to ensure that <b>similar</b> individuals are treated similarly (i.e. they should receive <b>similar</b> classification outcomes) . When checking for the individual fairness, one major challenge is to define a notion of the distance between two individuals to ...", "dateLastCrawled": "2022-01-28T12:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>A Reductions Approach to Fair</b> Classification", "url": "http://proceedings.mlr.press/v80/agarwal18a/agarwal18a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v80/agarwal18a/agarwal18a.pdf", "snippet": "si\ufb01cation method as a \u201c<b>black</b> <b>box</b>,\u201d while implementing a wrapper that either works by pre-processing the data or post-processing the classi\ufb01er\u2019s predictions (e.g.,Kamiran &amp; Calders,2012;Feldman et al.,2015;Hardt et al.,2016; Calmon et al.,2017). Existing pre-processing approaches are speci\ufb01c to particular de\ufb01nitions of fairness and typically seek to come up with a single transformed data set that will work across all learning algorithms, which, in practice, leads to classi\ufb01ers ...", "dateLastCrawled": "2022-02-02T14:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Rawlsian Fair <b>Adaptation of Deep Learning Classifiers</b>", "url": "https://mllab.csa.iisc.ac.in/downloads/AIES-camera-ready-paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://mllab.csa.iisc.ac.in/downloads/AIES-camera-ready-paper.pdf", "snippet": "e.g., <b>similar</b> accuracy [11, 15, 32], statistical parity [17], <b>equalized</b> <b>odds</b> [22], <b>similar</b> false positive rates [22]. A popular objective in group-fair classification is accuracy maximization subject to equal or near-equal predictive performance on different sensitive sub-populations. What makes any fairness objective well founded? In this pa- per, we consider the basic principles of Pareto-efficiencyand least-difference, following the work of Rawls on distributive justice [34\u2013 36]. A ...", "dateLastCrawled": "2022-02-03T00:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>BIAS MITIGATION POST-PROCESSING FOR INDIVIDUAL AND GROUP</b> FAIRNESS - <b>IBM</b>", "url": "https://www.ibm.com/downloads/cas/WM4MWDOE", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ibm.com</b>/downloads/cas/WM4MWDOE", "snippet": "The algorithm proposed by [19], <b>equalized</b> <b>odds</b> post-processing (EOP), is targeted to a different group fairness measure: <b>equalized</b> <b>odds</b> rather than disparate impact. Perfect <b>equalized</b> <b>odds</b> requires the privileged and unprivileged groups to have the same false neg-ative rate and same false positive rate. The algorithm solves an op-", "dateLastCrawled": "2022-02-01T21:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "ALGORITHMIC FAIRNESS: MEASURES, METHODS AND REPRESENTATIONS", "url": "https://www.cs.utah.edu/~suresh/static/files/tutorial.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.utah.edu/~suresh/static/files/tutorial.pdf", "snippet": "<b>equalized</b> <b>odds</b> [HPS16] ... and fairness is making sure people of <b>similar</b> ability are treated similarly \u2022 Demographic parity: Group identity should have nothing to do with selection for a task. \u2022 <b>Equalized</b> <b>odds</b>: Groups may have different innate skill levels, but we should make mistakes equally. MORE HIDDEN ASSUMPTIONS. MORE HIDDEN ASSUMPTIONS \u2022 All groups are equivalent and unfair treatment of one is the same as unfair treatment of another. MORE HIDDEN ASSUMPTIONS \u2022 All groups are ...", "dateLastCrawled": "2022-02-03T18:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "GitHub - rtikyani/CS634_IBM_Fairness_Pipeline: Detecting and mitigating ...", "url": "https://github.com/rtikyani/CS634_IBM_Fairness_Pipeline", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/rtikyani/CS634_IBM_Fairness_Pipeline", "snippet": "The post-processing treats the learned model as a <b>black</b> <b>box</b>. This algorithm should be the only algorithm used if the training data or learning algorithm cannot be modified. There are three bias mitigation algorithms: <b>equalized</b> <b>odds</b> postprocessing, calibrated <b>equalized</b> <b>odds</b> postprocessing, and reject option classification. [Favorable Label]: Value corresponds to an advantageous outcome [Protected Attribute:] Partitions a population into groups with different amounts of benefits, ie) gender ...", "dateLastCrawled": "2022-01-04T19:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Discriminative but Not Discriminatory: A Comparison of Fairness ...", "url": "https://anthropocentricai.github.io/blog/2018/08/30/yeom2018discriminative/", "isFamilyFriendly": true, "displayUrl": "https://anthropocentricai.github.io/blog/2018/08/30/yeom2018discriminative", "snippet": "We mathematically compare three competing definitions of group-level nondiscrimination: demographic parity, <b>equalized</b> <b>odds</b>, and calibration. Using the theoretical framework of Friedler et al., we study the properties of each definition under various worldviews, which are assumptions about how, if at all, the observed data is biased. We prove that different worldviews call for different definitions of fairness, and we specify when it is appropriate to use demographic parity and <b>equalized</b> <b>odds</b> ...", "dateLastCrawled": "2021-09-01T00:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Fairwashing: the risk of rationalization</b>", "url": "http://proceedings.mlr.press/v97/aivodji19a/aivodji19a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v97/aivodji19a/aivodji19a.pdf", "snippet": "<b>black</b> <b>box</b> through a process that we coin as rationalization. In particular, we propose a systematic rationalization tech-nique that, given <b>black</b>-<b>box</b> access to a model f, produces an ensemble Cof interpretable models c\u02c7fthat are fairer than the <b>black</b>-<b>box</b> according to a prede\ufb01ned fairness metric. From this set of plausible explanations, a dishonest entity can pick a model to achieve fairwashing. One of the strength of our approach is that it is agnostic to both the <b>black</b>-<b>box</b> model and the ...", "dateLastCrawled": "2022-01-24T20:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "2. Algorithms for Bias Mitigation \u2013 AI Fairness \u2013 Dev Guis", "url": "http://devguis.com/2-algorithms-for-bias-mitigation-ai-fairness.html", "isFamilyFriendly": true, "displayUrl": "devguis.com/2-algorithms-for-bias-mitigation-ai-fairness.html", "snippet": "If you need to treat the learned model as a <b>black</b> <b>box</b> and cannot modify the training data or learning algorithm, you will need to use the post-processing algorithms. Figure 2-1. Where can you intervene in the pipeline? Pre-Processing Algorithms. Pre-processing is the optimal time to mitigate bias given that most bias is intrinsic to the data. With pre-processing algorithms, you attempt to reduce bias by manipulating the training data before training the algorithm. Although this is ...", "dateLastCrawled": "2022-01-12T12:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Achieving Trusted AI With Model Interpretability - Dataiku Community", "url": "https://community.dataiku.com/t5/General-Discussion/Achieving-Trusted-AI-With-Model-Interpretability/m-p/21793", "isFamilyFriendly": true, "displayUrl": "https://community.dataiku.com/t5/General-Discussion/Achieving-Trusted-AI-With-Model...", "snippet": "<b>Black</b>-<b>box</b> models have attracted the ire of consumers and regulators, so applying deep learning and other complex models in applications that impact people could be severely curtailed by government regulations, like the recent EU proposed Artificial Intelligence Act, unless we can develop tools that allow us to peer into these models and explain decisions in human terms. While the science of model interpretation progresses, implementing highly interpretable white-<b>box</b> models and using tooling ...", "dateLastCrawled": "2022-01-14T06:58:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Reprogramming Fairness: Affirmative Action in Algorithmic Criminal</b> ...", "url": "http://hrlr.law.columbia.edu/hrlr-online/reprogramming-fairness-affirmative-action-in-algorithmic-criminal-sentencing/", "isFamilyFriendly": true, "displayUrl": "hrlr.law.columbia.edu/hrlr-online/<b>reprogramming-fairness-affirmative-action-in</b>...", "snippet": "In this scenario, for the definition of <b>equalized</b> <b>odds</b> to be satisfied, <b>black</b> and white defendants must have the same <b>odds</b> of being misclassified as High Risk, and the same <b>odds</b> of being misclassified as Low Risk. To make this more concrete, it is helpful to examine what group of defendants the definition of <b>equalized</b> <b>odds</b> most benefits.", "dateLastCrawled": "2022-02-01T07:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>A Reductions Approach to Fair</b> Classification", "url": "http://proceedings.mlr.press/v80/agarwal18a/agarwal18a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v80/agarwal18a/agarwal18a.pdf", "snippet": "that <b>can</b> be formalized via linear inequalities on conditional moments, such as demographic parity or <b>equalized</b> <b>odds</b> (see Section2.1). We show how binary classi\ufb01cation subject to these constraints <b>can</b> be reduced to a sequence of cost-sensitive classi\ufb01cation problems. We require only <b>black</b>-<b>box</b> access to a cost-sensitive classi\ufb01cation algorithm,", "dateLastCrawled": "2022-02-02T14:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Ensuring Fairness under Prior Probability Shifts", "url": "https://suvamm.github.io/papers/aies21.pdf", "isFamilyFriendly": true, "displayUrl": "https://suvamm.github.io/papers/aies21.pdf", "snippet": "Statistical Parity (SP) [15, 31, 50], <b>Equalized</b> <b>Odds</b> (EO) [26, 33, 46], and Disparate Mistreatment [11, 47]. These group fairness notions have been used extensively to audit <b>black</b>-<b>box</b> classifiers for dis-crimination [7, 43]. An inherent requirement of such audits is to have a test dataset of (statistically) significant size.", "dateLastCrawled": "2021-12-22T21:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Advancing health equity with artificial intelligence", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8607970/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8607970", "snippet": "A glossary of key terms <b>can</b> be found in <b>Box</b> ... <b>Equalized</b> <b>odds</b>: No difference in sensitivity and specificity across all groups: Predictive parity: No difference in positive predictive value rates across all groups: Demographic parity : No difference in positive outcome rates across all groups: Validation (AI lifecycle) Evaluation of model performance prior to formal implementation: Interpretability: The degree to which the decision process of AI is understandable to humans: Continuously ...", "dateLastCrawled": "2021-12-23T22:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) EqGNN: <b>Equalized</b> Node Opportunity in Graphs", "url": "https://www.researchgate.net/publication/354020699_EqGNN_Equalized_Node_Opportunity_in_Graphs", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/354020699_EqGNN_<b>Equalized</b>_Node_Opportunity_in...", "snippet": "PDF | Graph neural networks (GNNs), has been widely used for supervised learning tasks in graphs reaching state-of-the-art results. However, little work... | Find, read and cite all the research ...", "dateLastCrawled": "2021-12-10T20:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "An Information-Theoretic Quanti\ufb01cation of Discrimination with Exempt ...", "url": "https://users.ece.cmu.edu/~sanghamd/publications/Fairness_Exemptions_AAAI2020.pdf", "isFamilyFriendly": true, "displayUrl": "https://users.ece.cmu.edu/~sanghamd/publications/Fairness_Exemptions_AAAI2020.pdf", "snippet": "extend the idea of \u201cproxy-use\u201d [22] from white-<b>box</b> models to <b>black</b>-<b>box</b> models, where we regard a model as being discriminatory if a virtual component (P) is formed inside the model that has high mutual information about Z (i.e., Pis a virtual proxy of Z) and that also causally in\ufb02uences the \ufb01nal output Y^. Interestingly, note that this discrimination may not exhibit itself entirely in I(Z;Y^), which is the \u201cstatistically visible\u201d information about Zin Y^ because of \u201cstatistical ...", "dateLastCrawled": "2021-07-14T07:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Achieving Trusted AI With Model Interpretability - Dataiku Community", "url": "https://community.dataiku.com/t5/General-Discussion/Achieving-Trusted-AI-With-Model-Interpretability/m-p/21793", "isFamilyFriendly": true, "displayUrl": "https://community.dataiku.com/t5/General-Discussion/Achieving-Trusted-AI-With-Model...", "snippet": "<b>Black</b>-<b>box</b> models have attracted the ire of consumers and regulators, so applying deep learning and other complex models in applications that impact people could be severely curtailed by government regulations, like the recent EU proposed Artificial Intelligence Act, unless we <b>can</b> develop tools that allow us to peer into these models and explain decisions in human terms. While the science of model interpretation progresses, implementing highly interpretable white-<b>box</b> models and using tooling ...", "dateLastCrawled": "2022-01-14T06:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Foucault on Power | Philosophy Talk", "url": "https://www.philosophytalk.org/blog/foucault-power/", "isFamilyFriendly": true, "displayUrl": "https://www.philosophytalk.org/blog/foucault-power", "snippet": "And one consequence is that we <b>can</b> start thinking of injustice as something systemic or structural: injustice is not just a set of acts that bad people do, but a system that even otherwise good people <b>can</b> end up perpetuating, often in spite of themselves. This is a deep insight, and one that, it seems to me, has been powerfully put into practice by the <b>Black</b> Lives Matter movement. As this movement recognizes, there\u2019s no single tyrant we could depose from office in order to end the problem ...", "dateLastCrawled": "2022-02-02T22:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>An Intersectional Definition of Fairness</b> | DeepAI", "url": "https://deepai.org/publication/an-intersectional-definition-of-fairness", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>an-intersectional-definition-of-fairness</b>", "snippet": "We then extend the definition to measuring bias in data, e.g. from a <b>black</b>-<b>box</b> system, in Section 4. We present worked examples which illustrate the calculation of <b>differential fairness</b> in Section 5, followed by a case study in Section 6. Finally, we discuss related work in Section 7 and conclude in Section 8. 2 Algorithmic Fairness and its Relation to Intersectionality. Fairness is a complicated socio-technical construct with a multitude of political and legal facets, which may be in ...", "dateLastCrawled": "2022-01-29T05:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Bias and Fairness in Machine Learning, Part 2: building a baseline ...", "url": "https://freecontent.manning.com/bias-and-fairness-in-machine-learning-part-2-building-a-baseline-model-and-features/", "isFamilyFriendly": true, "displayUrl": "https://freecontent.manning.com/bias-and-fairness-in-machine-learning-part-2-building...", "snippet": "Building a Baseline Model. Check out part 1 for an intro to the dataset and how bias influences machine learning models, making fairness important to consider when dealing with data.. It\u2019s time to build our baseline ML model. For our first pass at our model, we will apply a bit of feature engineering to ensure our model interprets all of our data correctly and spend time analyzing the fairness/performance results of our model.", "dateLastCrawled": "2022-02-03T06:46:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Bounding the Fairness and Accuracy of Classi\ufb01ers from Population Statistics", "url": "http://proceedings.mlr.press/v119/sabato20a/sabato20a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v119/sabato20a/sabato20a.pdf", "snippet": "as a <b>black</b>-<b>box</b>, is impossible. Instead, only ag-gregate statistics on the rate of positive predic-tions in each of several sub-populations are avail- able, as well as the true rates of positive labels in each of these sub-populations. We show that these aggregate statistics <b>can</b> be used to lower-bound the discrepancy of a classi\ufb01er, which is a measure that balances inaccuracy and unfair-ness. To this end, we de\ufb01ne a new measure of unfairness, equal to the fraction of the population on ...", "dateLastCrawled": "2021-12-25T21:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Reprogramming Fairness: Affirmative Action in Algorithmic Criminal</b> ...", "url": "http://hrlr.law.columbia.edu/hrlr-online/reprogramming-fairness-affirmative-action-in-algorithmic-criminal-sentencing/", "isFamilyFriendly": true, "displayUrl": "hrlr.law.columbia.edu/hrlr-online/<b>reprogramming-fairness-affirmative-action-in</b>...", "snippet": "In this scenario, for the definition of <b>equalized</b> <b>odds</b> to be satisfied, <b>black</b> and white defendants must have the same <b>odds</b> of being misclassified as High Risk, and the same <b>odds</b> of being misclassified as Low Risk. To make this more concrete, it is helpful to examine what group of defendants the definition of <b>equalized</b> <b>odds</b> most benefits.", "dateLastCrawled": "2022-02-01T07:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Multiaccuracy: <b>Black</b>-<b>Box</b> <b>Post-Processing</b> for Fairness in Classi cation", "url": "https://cs.stanford.edu/~mpkim/pubs/multiaccuracy.pdf", "isFamilyFriendly": true, "displayUrl": "https://cs.stanford.edu/~mpkim/pubs/multiaccuracy.pdf", "snippet": "<b>black</b>-<b>box</b> framework allows for improved fairness and accountability of predictions, even when the predictor is minimally transparent. We prove that Multiaccuracy Boost converges e ciently and show that if the initial model is accurate on an identi able subgroup, then the post-processed model will be also. We experimentally demonstrate the e ectiveness of the approach to improve the accuracy among minority subgroups in diverse applications (image classi cation, nance, population health ...", "dateLastCrawled": "2021-12-22T09:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Rawlsian Fair <b>Adaptation of Deep Learning Classifiers</b>", "url": "https://mllab.csa.iisc.ac.in/downloads/AIES-camera-ready-paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://mllab.csa.iisc.ac.in/downloads/AIES-camera-ready-paper.pdf", "snippet": "a feature representation is often computed by a <b>black</b>-<b>box</b> model that has been useful but unfair. Our second contribution is practi-cal Rawlsian fair adaptation of any given <b>black</b>-<b>box</b> deep learning model, without changing the score or feature representation it computes. Given any score function or feature representation and only its second-order statistics on the sensitive sub-populations, we seek a threshold classifier on the given score or a linear thresh-old classifier on the given feature ...", "dateLastCrawled": "2022-02-03T00:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Navigating the Sea of Explainability</b> | by Shir Meir Lador | Towards ...", "url": "https://towardsdatascience.com/navigating-the-sea-of-explainability-649672aa7bdd", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>navigating-the-sea-of-explainability</b>-649672aa7bdd", "snippet": "Interpretability methods fall into two major categories based on whether the model being interpreted is: (a) <b>black</b> <b>box</b> (unintelligible) or (b) glass <b>box</b> (intelligible). In the following section, we will explain and compare each of the approaches. We will also describe how we <b>can</b> use intelligible models to better understand our data. Then we will review a method to detect high-performing intelligible models for any use case (Rashomon curves). Finally, we will compare local and global ...", "dateLastCrawled": "2022-01-24T16:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>BIAS MITIGATION POST-PROCESSING FOR INDIVIDUAL AND GROUP</b> FAIRNESS - <b>IBM</b>", "url": "https://www.ibm.com/downloads/cas/WM4MWDOE", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ibm.com</b>/downloads/cas/WM4MWDOE", "snippet": "algorithms operate in a <b>black</b>-<b>box</b> fashion, meaning that they do not need access to the internals of models, their derivatives, etc., and are therefore applicable to any machine learning model (or amalgama-tion of models) [14]. The vast majority of bias mitigation algorithms address group fairness, but a few address individual fairness [15, 16]. Some pre-processing algorithms address both group and individual fairness [17, 18, 6], but to the best of our knowledge, all existing post-processing ...", "dateLastCrawled": "2022-02-01T21:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) Blackbox Post-Processing for Multiclass Fairness", "url": "https://www.researchgate.net/publication/357790861_Blackbox_Post-Processing_for_Multiclass_Fairness", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/357790861_<b>Blackbox</b>_Post-Processing_for_Multi...", "snippet": "<b>box</b> models whose internal parameters <b>can</b> be either inacces-sible or too costly to train. In this paper, we address the case . where outcomes are multiclass and the user has received a. pre-trained ...", "dateLastCrawled": "2022-01-25T23:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Explainable AI: A Review of Machine Learning Interpretability Methods", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7824368/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7824368", "snippet": "Brendel et al. highlighted the lack of scientific studies regarding decision-based adversarial attacks and pinpointed to the benefits and the versatility of such attacks, namely that they <b>can</b> be used against any <b>black</b>-<b>box</b> model, require only the observing of the model\u2019s final decisions, are easier to implement <b>compared</b> to transfer-based attacks, and, at the same time, are more effective against simple defences when <b>compared</b> to gradient-based or score-based attacks. To support their ...", "dateLastCrawled": "2022-01-29T03:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Bias and Fairness in Machine Learning, Part 2: building a baseline ...", "url": "https://freecontent.manning.com/bias-and-fairness-in-machine-learning-part-2-building-a-baseline-model-and-features/", "isFamilyFriendly": true, "displayUrl": "https://freecontent.manning.com/bias-and-fairness-in-machine-learning-part-2-building...", "snippet": "<b>Equalized</b> <b>Odds</b> \u2013 modifying predicted labels using a separate optimization objective to make the predictions fairer. Calibrated <b>Equalized</b> <b>Odds</b> \u2013 modifying the classifier\u2019s scores to make for fairer results. That\u2019s all for now. In part 3, we will see how to build a bias-aware model.", "dateLastCrawled": "2022-02-03T06:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>xGEMs: Generating Examplars to Explain Black</b>-<b>Box</b> Models | DeepAI", "url": "https://deepai.org/publication/xgems-generating-examplars-to-explain-black-box-models", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>xgems-generating-examplars-to-explain-black</b>-<b>box</b>-models", "snippet": "<b>Black</b>-<b>box</b> Classifier Accuracy Confounding metric f 1 \u03d5 0.9933 0.1704 f 2 \u03d5 0.9155 0.4323 Table 2: Confounding metric Target label <b>Black</b>-<b>box</b> <b>Black</b> Hair Blond Hair f 1 \u03d5 Male:0.4550 Male:0.1432 Female:0.0159 Female:0.0484 Overall:0.2430 Overall:0.0539 f 2 \u03d5 Male:0.7716 Male:0.1475 Female:0.0045 Female:0.5024 Overall:0.4012 Overall:0.4821 Table 3: Confounding metric by gender", "dateLastCrawled": "2021-12-23T20:58:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine Learning</b> Glossary | <b>Google Developers</b>", "url": "https://developers.google.com/machine-learning/glossary/", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine-learning</b>/glossary", "snippet": "A <b>machine learning</b> technique that iteratively combines a set of simple and not very accurate classifiers ... Contrast with <b>equalized</b> <b>odds</b> and equality of opportunity, which permit classification results in aggregate to depend on sensitive attributes, but do not permit classification results for certain specified ground-truth labels to depend on sensitive attributes. See &quot;Attacking discrimination with smarter <b>machine learning</b>&quot; for a visualization exploring the tradeoffs when optimizing for ...", "dateLastCrawled": "2022-02-03T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Residual Unfairness in Fair <b>Machine</b> <b>Learning</b> from Prejudiced Data", "url": "http://proceedings.mlr.press/v80/kallus18a/kallus18a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v80/kallus18a/kallus18a.pdf", "snippet": "chine <b>learning</b> has considered some subcomponents of the overall problem we study of <b>learning</b> fair policies from bi-ased datasets. Hardt et al. (2016) formalize the criteria of equal opportunity and <b>equalized</b> <b>odds</b>. Lum &amp; Isaac (2016) show that a predictive policing algorithm for drug enforce-ment in Oakland, trained on police records, will ...", "dateLastCrawled": "2022-01-31T07:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Fighting Money Laundering with Statistics and <b>Machine</b> <b>Learning</b>: An ...", "url": "https://deepai.org/publication/fighting-money-laundering-with-statistics-and-machine-learning-an-introduction-and-review", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/fighting-money-laundering-with-statistics-and-<b>machine</b>...", "snippet": "Third, we present recent <b>machine</b> <b>learning</b> concepts that have the potential to improve AML. The remainder of the paper is organized as follows. Section 2 introduces AML in banks. Section 3 presents our terminology. Sections 4 and 5 review the literature on client risk profiling and suspicious behavior flagging, respectively. Section 6 provides a discussion on future research directions and section 7 concludes the paper. 2 Anti-Money Laundering in Banks. The international framework for AML is ...", "dateLastCrawled": "2022-01-28T21:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Survey on Bias and Fairness in <b>Machine</b> <b>Learning</b> \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/1908.09635/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1908.09635", "snippet": "With the widespread use of AI systems and applications in our everyday lives, it is important to take fairness issues into consideration while designing and engineering these types of systems. Such systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that the decisions do not reflect discriminatory behavior toward certain groups or populations. We have recently seen work in <b>machine</b> <b>learning</b>, natural language ...", "dateLastCrawled": "2021-11-15T17:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A Human-centric Approach to Fairness in AI", "url": "https://www.timlrx.com/blog/a-human-centric-approach-to-fairness-in-ai", "isFamilyFriendly": true, "displayUrl": "https://www.timlrx.com/blog/a-human-centric-approach-to-fairness-in-ai", "snippet": "We embed the evaluation of AI fairness within the best practices of <b>machine</b> <b>learning</b> development and operations such as version control, ... This includes measures such as Demographic Parity / Statistical Parity (Dwork et al., 2012), <b>Equalized</b> <b>Odds</b> Metric (Hardt et al., 2016) and Calibration within Groups (Chouldechova, 2017). They are all statistical measures derived from the predictions of a classification model and differ in terms of which element(s) of the confusion matrix they are ...", "dateLastCrawled": "2022-02-03T02:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Mitigating Unwanted Biases with Adversarial Learning</b>", "url": "https://www.aies-conference.com/2018/contents/papers/main/AIES_2018_paper_162.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.aies-conference.com/2018/contents/papers/main/AIES_2018_paper_162.pdf", "snippet": "<b>Machine</b> <b>learning</b> is a tool for building models that accurately represent input training data. When undesired biases concern- ing demographic groups are in the training data, well-trained models will re\ufb02ect those biases. We present a framework for mitigating such biases by including a variable for the group of interest and simultaneously <b>learning</b> a predictor and an ad-versary. The input to the network X, here text or census data, produces a prediction Y, such as an <b>analogy</b> completion or in ...", "dateLastCrawled": "2021-12-17T22:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "RStudio AI Blog: Starting to think about AI Fairness", "url": "https://blogs.rstudio.com/ai/posts/2021-07-15-ai-fairness/", "isFamilyFriendly": true, "displayUrl": "https://blogs.rstudio.com/ai/posts/2021-07-15-ai-fairness", "snippet": "Papers on fairness in <b>machine</b> <b>learning</b>, as is common in fields like computer science, abound with formulae. Even the papers referenced here, though selected not for their theorems and proofs but for the ideas they harbor, are no exception. But to start thinking about fairness as it might apply to an ML process at hand, common language \u2013 and common sense \u2013 will do just fine. If, after analyzing your use case, you judge that the more technical results are relevant to the process in ...", "dateLastCrawled": "2022-01-31T09:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Classification - Fairness and <b>machine</b> <b>learning</b>", "url": "https://fairmlbook.org/classification.html", "isFamilyFriendly": true, "displayUrl": "https://fairmlbook.org/classification.html", "snippet": "Simply put, the goal of classification is to determine a plausible value for an unknown variable Y given an observed variable X.For example, we might try to predict whether a loan applicant will pay back her loan by looking at various characteristics such as credit history, income, and net worth. Classification also applies in situations where the variable Y does not refer to an event that lies in the future. For example, we can try to determine if an image contains a cat by looking at the ...", "dateLastCrawled": "2022-02-03T07:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "On Predicting Recidivism: Epistemic Risk, Tradeoffs, and Values in ...", "url": "https://www.cambridge.org/core/journals/canadian-journal-of-philosophy/article/on-predicting-recidivism-epistemic-risk-tradeoffs-and-values-in-machine-learning/7E541FA03E78C3141A65EA99A0CA6E9A", "isFamilyFriendly": true, "displayUrl": "https://www.cambridge.org/core/journals/canadian-journal-of-philosophy/article/on...", "snippet": "This paper examines the role of value judgments in the design of <b>machine</b>-<b>learning</b> (ML) systems generally and in recidivism-prediction algorithms specifically. Drawing on work on inductive and epistemic risk, the paper argues that ML systems are value laden in ways similar to human decision making, because the development and design of ML systems requires human decisions that involve tradeoffs that reflect values. In many cases, these decisions have significant\u2014and, in some cases, disparate ...", "dateLastCrawled": "2022-01-26T20:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Measuring discrimination in algorithmic <b>decision making</b> | SpringerLink", "url": "https://link.springer.com/article/10.1007/s10618-017-0506-1", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10618-017-0506-1", "snippet": "A <b>machine</b> <b>learning</b> algorithm is a procedure used for producing a predictive model from historical data. A model is a collection of decision rules used for <b>decision making</b> for new incoming data. The model would take personal characteristics as inputs (for example, income, credit history, employment status), and produce a prediction (for example, credit risk level). Fig. 1. A typical <b>machine</b> <b>learning</b> setting. Full size image. <b>Learning</b> algorithms as such cannot discriminate, because they are ...", "dateLastCrawled": "2022-01-29T20:54:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(equalized odds)  is like +(a black box)", "+(equalized odds) is similar to +(a black box)", "+(equalized odds) can be thought of as +(a black box)", "+(equalized odds) can be compared to +(a black box)", "machine learning +(equalized odds AND analogy)", "machine learning +(\"equalized odds is like\")", "machine learning +(\"equalized odds is similar\")", "machine learning +(\"just as equalized odds\")", "machine learning +(\"equalized odds can be thought of as\")", "machine learning +(\"equalized odds can be compared to\")"]}