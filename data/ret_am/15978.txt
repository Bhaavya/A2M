{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Solving Markov Decision Processes via Simulation</b>", "url": "https://web.mst.edu/~gosavia/gosavi_rev3.pdf", "isFamilyFriendly": true, "displayUrl": "https://web.mst.edu/~gosavia/gosavi_rev3.pdf", "snippet": "<b>Solving Markov Decision Processes via Simulation</b> Abhijit Gosavi* Abstract This chapter presents an overview of simulation-based techniques use- ful for solving <b>Markov</b> <b>decision</b> problems/processes (MDPs). MDPs are problems of sequential <b>decision</b>-making in which decisions made in each state collectively affect the trajectory of the states visited by the system \u2014 over a time horizon of interest to the analyst. The trajectory in turn, usually, affects the performance of the system, which is ...", "dateLastCrawled": "2022-02-03T11:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Simulating Markov Decision Processes</b>", "url": "https://ece.engin.umich.edu/event/simulating-markov-decision-processes", "isFamilyFriendly": true, "displayUrl": "https://ece.engin.umich.edu/event/<b>simulating-markov-decision-processes</b>", "snippet": "Many complex systems can be modeled as <b>Markov</b> <b>decision</b> processes and games. Despite impressive advances in computational technology and methods, often, our only recourse to their analysis and design is through computer simulations. In many problems, even the system parameters may be unknown. In such cases, we would <b>like</b> to obtain uniform estimates of quantities of interest such as the value function. The value function of a <b>Markov decision process</b> assigns to each policy its expected ...", "dateLastCrawled": "2022-01-24T14:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Markov Decision Processes and Reinforcement Learning</b>", "url": "https://danieltakeshi.github.io/2015-08-02-markov-decision-processes-and-reinforcement-learning/", "isFamilyFriendly": true, "displayUrl": "https://danieltakeshi.github.io/2015-08-02-<b>markov-decision-processes-and-reinforcement</b>...", "snippet": "This is the Partially Observable <b>Markov Decision Process</b> (POMDP) case. We augment the <b>MDP</b> with a sensor model P ( e \u2223 s) and treat states as belief states. In a discrete <b>MDP</b> with n states, the belief state vector b would be an n -dimensional vector with components representing the probabilities of being in a particular state.", "dateLastCrawled": "2022-01-30T21:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Artificial intelligence framework for <b>simulating</b> clinical <b>decision</b> ...", "url": "https://www.researchgate.net/publication/234050168_Artificial_intelligence_framework_for_simulating_clinical_decision-making_A_Markov_decision_process_approach", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/234050168_Artificial_intelligence_framework...", "snippet": "The traditional RL approach [13], which is formulated based on <b>Markov decision process</b> (<b>MDP</b>) [8,12,14, 15], enables a single agent (or <b>a decision</b> maker) to interact with its operating environment ...", "dateLastCrawled": "2021-12-12T07:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Reinforcement Learning Tutorial</b> - Javatpoint", "url": "https://www.javatpoint.com/reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://www.javatpoint.com/reinforcement-learning", "snippet": "<b>Markov Decision Process</b>. <b>Markov Decision Process</b> or <b>MDP</b>, is used to formalize the reinforcement learning problems. If the environment is completely observable, then its dynamic can be modeled as a <b>Markov</b> <b>Process</b>. In <b>MDP</b>, the agent constantly interacts with the environment and performs actions; at each action, the environment responds and generates a new state. <b>MDP</b> is used to describe the environment for the RL, and almost all the RL problem can be formalized using <b>MDP</b>. <b>MDP</b> contains a tuple ...", "dateLastCrawled": "2022-02-02T06:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "SeaLevelRiseMDPProblem_OnlineMaterial.pdf - A <b>Markov Decision Process</b> ...", "url": "https://www.coursehero.com/file/111793932/SeaLevelRiseMDPProblem-OnlineMaterialpdf/", "isFamilyFriendly": true, "displayUrl": "https://www.coursehero.com/file/111793932/SeaLevelRise<b>MDP</b>Problem-OnlineMaterialpdf", "snippet": "An <b>MDP</b> Model for Socio-Economic Systems Impacted by Climate Change natural and socioeconomic systems under the impacts of climate change. <b>MDP</b> provides a suitable theoretical framework for creating agent-based scenarios (Howard, 1960; Bellman, 1957; Fi-lar &amp; Vrieze, 2012).The <b>MDP</b> agent (government in our case) interacts with the environment (nature and residents in our case) by taking action at each time and receiving a reward/cost from the environment in return. The objective of the agent is ...", "dateLastCrawled": "2022-01-26T15:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Improving Real-Time Bidding <b>Using a Constrained Markov Decision Process</b>", "url": "http://wnzhang.net/share/rtb-papers/rtb-cmdp.pdf", "isFamilyFriendly": true, "displayUrl": "wnzhang.net/share/rtb-papers/rtb-c<b>mdp</b>.pdf", "snippet": "<b>a Constrained Markov Decision Process</b> Manxing Du1(B), Redouane Sassioui 1, Georgios Varisteas1, Radu State , Mats Brorsson2, and Omar Cherkaoui3 1 University of Luxembourg, Luxembourg City, Luxembourg {manxing.du,redouane.sassioui,georgios.varisteas,radu.state}@uni.lu 2 Royal Institute of Technology (KTH), Stockholm, Sweden matsbror@kth.se 3 University of Quebec in Montreal, Montreal, Canada cherkaoui.omar@uqam.ca Abstract. Online advertising is increasingly switching to real-time bid-ding ...", "dateLastCrawled": "2022-01-27T09:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Modeling and <b>Simulating</b> of a Mobile <b>Decision</b> <b>Process</b>, International ...", "url": "https://www.deepdyve.com/lp/de-gruyter/modeling-and-simulating-of-a-mobile-decision-process-l2C0gKhMTE", "isFamilyFriendly": true, "displayUrl": "https://www.<b>deepdyve</b>.com/lp/de-gruyter/modeling-and-<b>simulating</b>-of-a-mobile-<b>decision</b>...", "snippet": "For vehicle traffic and wireless communication traffic constrains in mobile environment, we give a novel method for the mobile <b>process</b> modeling and the total cost optimizing by <b>Markov decision process</b>. Some simulations are discussed in the end. Keywords: vehicle traffic gain, wireless communication traffic gain, <b>MDP</b>, mobile <b>decision</b> support <b>process</b>. 1 Introduction With the telecommunication technologies, especially mobile communication technologies development, the <b>decision</b> support system ...", "dateLastCrawled": "2020-06-12T01:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Is <b>Markov Decision Process</b> a step <b>for reinforcement learning</b> or ... - Quora", "url": "https://www.quora.com/Is-Markov-Decision-Process-a-step-for-reinforcement-learning-or-a-model-approach-for-reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Is-<b>Markov-Decision-Process</b>-a-step-<b>for-reinforcement-learning</b>-or...", "snippet": "Answer: No. A <b>Markov decision process</b> (<b>MDP</b>) is the problem that <b>reinforcement learning</b>(RL) tries to solve. We often use \u201c<b>Markov</b> <b>decision</b> problem\u201d to clarify that the <b>process</b> is not an algorithm, but rather that it represents a problem to be solved (the reason it\u2019s called a <b>process</b> is because in s...", "dateLastCrawled": "2022-01-11T19:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Optimal Blackjack Strategy Using MDP Solvers</b> \u2013 Kunal Menda", "url": "https://kunalmenda.com/2016/08/20/optimal-blackjack-strategy-using-mdp-solvers/", "isFamilyFriendly": true, "displayUrl": "https://kunalmenda.com/2016/08/20/<b>optimal-blackjack-strategy-using-mdp-solvers</b>", "snippet": "A <b>Markov Decision Process</b> (<b>MDP</b>) is a powerful way to allow AI to make decisions in uncertain environments. In this project, I used the algorithm to provide the optimal strategy for Blackjack. I then compared the strategy to a widely available \u201cBlackjack Cheat Sheet\u201d, and found the strategy provided by <b>MDP</b> and the cheat sheet to have comparable performances of expecting to lose only 5\u00a2 on a $10 bet, which is around the \u201chouse advantage\u201d of 0.5% to the casino, indicating that it is ...", "dateLastCrawled": "2022-01-22T06:09:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Reinforcement Learning Tutorial</b> - Javatpoint", "url": "https://www.javatpoint.com/reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://www.javatpoint.com/reinforcement-learning", "snippet": "<b>Markov Decision Process</b> or <b>MDP</b>, is used to formalize the reinforcement learning problems. If the environment is completely observable, then its dynamic can be modeled as a <b>Markov</b> <b>Process</b> . In <b>MDP</b>, the agent constantly interacts with the environment and performs actions; at each action, the environment responds and generates a new state.", "dateLastCrawled": "2022-02-02T06:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Markov decision process model for patient admission decision</b> at an ...", "url": "https://link.springer.com/article/10.1007%2Fs10696-017-9276-8", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10696-017-9276-8", "snippet": "A finite horizon <b>Markov decision process</b> (<b>MDP</b>) model is formulated to determine patient admission decisions. In particular, our model considers the time-dependent arrival of patients and time-dependent reward function. We also consider a policy restriction that immediate-patients should be admitted as long as there is available beds. The <b>MDP</b> model has a continuous state space, and we solve the model by using a state discretization technique and obtain numerical solutions. Structural ...", "dateLastCrawled": "2022-02-01T16:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Finding Markov Decision Processes related reference</b>", "url": "https://www.researchgate.net/post/Finding-Markov-Decision-Processes-related-reference", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/post/<b>Finding-Markov-Decision-Processes-related-reference</b>", "snippet": "<b>Markov Decision Process</b>. Share . Facebook. Twitter. LinkedIn. Reddit . Get help with your research. Join ResearchGate to ask questions, get input, and advance your work. Join for free. Log in ...", "dateLastCrawled": "2022-01-19T08:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Reinforcement Learning using a <b>Recurrent Neural</b> Network \u2013 Oleg Sushkov ...", "url": "https://osushkov.github.io/rnn-rl/", "isFamilyFriendly": true, "displayUrl": "https://osushkov.github.io/rnn-rl", "snippet": "A <b>Markov Decision Process</b> (<b>MDP</b>) is a stochastic control <b>process</b>, where you are in some state s and you can perform some action a. This action will result in a transition into a new state s\u2019 and a reward r. Both of these are dependent only on the tuple (s, a), rather than on the history of states and actions (this is known as the <b>Markov</b> Property).", "dateLastCrawled": "2022-01-31T05:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "What <b>functions is the &#39;Markov decision process&#39; used for</b> in machine ...", "url": "https://www.quora.com/What-functions-is-the-Markov-decision-process-used-for-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-<b>functions-is-the-Markov-decision-process-used-for</b>-in...", "snippet": "Answer: A mathematical representation of a complex <b>decision</b> making <b>process</b> is \u201c<b>Markov</b> <b>Decision</b> Processes\u201d (<b>MDP</b>). <b>MDP</b> is defined by: * A state S, which represents every state that one could be in, within a defined world. * A model or transition function T; which is a function of the current st...", "dateLastCrawled": "2022-01-17T17:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Artificial intelligence framework for <b>simulating</b> clinical <b>decision</b> ...", "url": "https://www.researchgate.net/publication/234050168_Artificial_intelligence_framework_for_simulating_clinical_decision-making_A_Markov_decision_process_approach", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/234050168_Artificial_intelligence_framework...", "snippet": "The traditional RL approach [13], which is formulated based on <b>Markov decision process</b> (<b>MDP</b>) [8,12,14, 15], enables a single agent (or <b>a decision</b> maker) to interact with its operating environment ...", "dateLastCrawled": "2021-12-12T07:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Markov Chains in Python</b> with Model Examples - DataCamp", "url": "https://www.datacamp.com/community/tutorials/markov-chains-python-tutorial", "isFamilyFriendly": true, "displayUrl": "https://www.datacamp.com/community/tutorials/<b>markov</b>-chains-python-tutorial", "snippet": "A <b>Markov</b> chain is a random <b>process</b> with the <b>Markov</b> property. A random <b>process</b> or often called stochastic property is a mathematical object defined as a collection of random variables. A <b>Markov</b> chain has either discrete state space (set of possible values of the random variables) or discrete index set (often representing time) - given the fact, many variations for a <b>Markov</b> chain exists. Usually the term &quot;<b>Markov</b> chain&quot; is reserved for a <b>process</b> with a discrete set of times, that is a Discrete ...", "dateLastCrawled": "2022-02-02T09:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Improving Real-Time Bidding <b>Using a Constrained Markov Decision Process</b>", "url": "http://wnzhang.net/share/rtb-papers/rtb-cmdp.pdf", "isFamilyFriendly": true, "displayUrl": "wnzhang.net/share/rtb-papers/rtb-c<b>mdp</b>.pdf", "snippet": "problem \ufb01ts perfectly into the framework of <b>a Constrained Markov Decision Process</b> (CMDP) [2], which allows to maximize one criterion while keeping another criterion below a given threshold. In this paper, we cast the optimization of the sequential bid requests as a CMDP. This is done in order to \ufb01nd the optimal bid price under budget con-straints for each auction. A CMDP is de\ufb01ned by the tuple &lt;S,A,P,R,C,V &gt;, which correspondingly represents state, action, state transition probability ...", "dateLastCrawled": "2022-01-27T09:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "What <b>is the mathematical backbone behind Markov</b> <b>Decision</b> Processes? - Quora", "url": "https://www.quora.com/What-is-the-mathematical-backbone-behind-Markov-Decision-Processes", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-<b>is-the-mathematical-backbone-behind-Markov</b>-<b>Decision</b>-<b>Process</b>es", "snippet": "Answer (1 of 4): In order to understand the <b>Markov Decision process</b>, it helps to understand Stochastic <b>Process</b> with &#39; State space&#39; and &#39; Parameter Space&#39;. Difference between a Discrete Stochastic <b>Process</b> and a Continuous Stochastic <b>Process</b>. The difference between a <b>Markov</b> Chain and a <b>Markov</b> Proce...", "dateLastCrawled": "2022-01-18T17:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Markov Chain</b> Analysis and Simulation using Python | by Herman Scheepers ...", "url": "https://towardsdatascience.com/markov-chain-analysis-and-simulation-using-python-4507cee0b06e", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>markov-chain</b>-analysis-and-simulation-using-python-4507...", "snippet": "A <b>Markov chain</b> is a discrete-time stochastic <b>process</b> that progresses from one state to another with certain probabilities that can be represented by a graph and state transition matrix P as indicated\u2026 Get started. Open in app. Sign in. Get started. Follow. 618K Followers \u00b7 Editors&#39; Picks Features Deep Dives Grow Contribute. About. Get started. Open in app. <b>Markov Chain</b> Analysis and Simulation using Python. Solving real-world problems with probabilities. Herman Scheepers. Nov 20, 2019 \u00b7 8 ...", "dateLastCrawled": "2022-02-02T22:05:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Markov Decision Processes and Reinforcement Learning</b>", "url": "https://danieltakeshi.github.io/2015-08-02-markov-decision-processes-and-reinforcement-learning/", "isFamilyFriendly": true, "displayUrl": "https://danieltakeshi.github.io/2015-08-02-<b>markov-decision-processes-and-reinforcement</b>...", "snippet": "This is the Partially Observable <b>Markov Decision Process</b> (POMDP) case. We augment the <b>MDP</b> with a sensor model P ( e \u2223 s) and treat states as belief states. In a discrete <b>MDP</b> with n states, the belief state vector b would be an n -dimensional vector with components representing the probabilities of being in a particular state.", "dateLastCrawled": "2022-01-30T21:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Abstractive Meeting Summarization as a <b>Markov Decision Process</b>", "url": "https://www.ufv.ca/media/assets/computer-information-systems/gabriel-murray/publications/canadian-ai-2015-1.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.ufv.ca/media/assets/computer-information-systems/gabriel-murray/...", "snippet": "<b>Markov Decision Process</b>. Value Iteration is used to determine the opti-mal policy for natural language generation. While the approach is gen-eral, in this work we apply the system to the problem of automatically summarizing meeting conversations. The generated abstracts are supe-rior to generated extracts according to intrinsic measures. Keywords: abstractive summarization, natural language generation, <b>markov decision process</b>, value iteration 1 Introduction This paper represents ongoing work ...", "dateLastCrawled": "2022-02-03T14:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Artificial intelligence framework for <b>simulating</b> clinical <b>decision</b> ...", "url": "https://www.researchgate.net/publication/234050168_Artificial_intelligence_framework_for_simulating_clinical_decision-making_A_Markov_decision_process_approach", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/234050168_Artificial_intelligence_framework...", "snippet": "The traditional RL approach [13], which is formulated based on <b>Markov decision process</b> (<b>MDP</b>) [8,12,14, 15], enables a single agent (or <b>a decision</b> maker) to interact with its operating environment ...", "dateLastCrawled": "2021-12-12T07:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Reinforcement Learning using a <b>Recurrent Neural</b> Network \u2013 Oleg Sushkov ...", "url": "https://osushkov.github.io/rnn-rl/", "isFamilyFriendly": true, "displayUrl": "https://osushkov.github.io/rnn-rl", "snippet": "A <b>Markov Decision Process</b> (<b>MDP</b>) is a stochastic control <b>process</b>, where you are in some state s and you <b>can</b> perform some action a. This action will result in a transition into a new state s\u2019 and a reward r. Both of these are dependent only on the tuple (s, a), rather than on the history of states and actions (this is known as the <b>Markov</b> Property). A POMDP is a generalisation of MDPs in that it introduces the concept of observations and the possibility that the current state s <b>can</b> only be ...", "dateLastCrawled": "2022-01-31T05:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>PlanningWithDynamicsAndUncertainty</b>", "url": "http://motion.cs.illinois.edu/RoboticSystems/PlanningWithDynamicsAndUncertainty.html", "isFamilyFriendly": true, "displayUrl": "motion.cs.illinois.edu/RoboticSystems/<b>PlanningWithDynamicsAndUncertainty</b>.html", "snippet": "A <b>Markov Decision Process</b> (<b>MDP</b>) is a principled method for representing <b>decision</b>-making problems under probabilistic movement uncertainty. Using MDPs, we <b>can</b> calculate safer navigation functions that give suitable trade-offs between optimality and collision risk. MDPs are also general-purpose, so they have been useful in modeling uncertainty in obstacle motion, wind gusts for UAVs, traffic delays, human behavior, as well as many other applications outside of robotics.", "dateLastCrawled": "2022-01-30T01:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Activity, Plan, and Goal Recognition: A Review", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8141730/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8141730", "snippet": "A different probabilistic approach to those already mentioned is modeling the observed agent as a <b>Markov decision process</b> (<b>MDP</b>) (Stuart and Norvig, 2016). <b>Markov</b> <b>decision</b> processes model an agent <b>decision</b> <b>process</b> in a system where the transition between states, as well as the rewards obtained by the agent, depend probabilistically on the actions taken. This type of approach is sometimes referred to as inverse reinforcement learning. Oh et al. proposed a proactive assistant agent for domains ...", "dateLastCrawled": "2022-01-04T03:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "6 <b>Technical Approaches For Building Conversational AI</b>", "url": "https://www.topbots.com/building-conversational-ai/", "isFamilyFriendly": true, "displayUrl": "https://www.topbots.com/building-conversational-ai", "snippet": "The <b>process</b> would work well if the <b>decision</b> <b>process</b> <b>can</b> be modeled as a <b>Markov Decision Process</b> (<b>MDP</b>), in which all of the information that the system needs for making the optimal next action is contained in the present state, making the preceding states irrelevant to the <b>decision</b>-making <b>process</b>. While Go is a great example of finite game of perfect information, conversation is not, and there is no guarantee even after <b>simulating</b> sample responses millions and millions of times that a system ...", "dateLastCrawled": "2022-01-31T14:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Integrating Models of Interval Timing and Reinforcement Learning</b> - Cell", "url": "https://www.cell.com/trends/cognitive-sciences/pdf/S1364-6613(18)30193-1.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cell.com/trends/cognitive-sciences/pdf/S1364-6613(18)30193-1.pdf", "snippet": "<b>Markov decision process</b> (<b>MDP</b>): a mathematical framework for describing environments in reinforcement learning. An <b>MDP</b> consists of states (s), actions (a), a function that probabilistically maps state\u2013action pairs to reward, and a state transition function that probabilistically maps state\u2013action pairs to the next state. This de\ufb01nition", "dateLastCrawled": "2021-11-10T18:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Is <b>Markov Decision Process</b> a step <b>for reinforcement learning</b> or ... - Quora", "url": "https://www.quora.com/Is-Markov-Decision-Process-a-step-for-reinforcement-learning-or-a-model-approach-for-reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Is-<b>Markov-Decision-Process</b>-a-step-<b>for-reinforcement-learning</b>-or...", "snippet": "Answer: No. A <b>Markov decision process</b> (<b>MDP</b>) is the problem that <b>reinforcement learning</b>(RL) tries to solve. We often use \u201c<b>Markov</b> <b>decision</b> problem\u201d to clarify that the <b>process</b> is not an algorithm, but rather that it represents a problem to be solved (the reason it\u2019s called a <b>process</b> is because in s...", "dateLastCrawled": "2022-01-11T19:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Markov Chains in Python</b> with Model Examples - DataCamp", "url": "https://www.datacamp.com/community/tutorials/markov-chains-python-tutorial", "isFamilyFriendly": true, "displayUrl": "https://www.datacamp.com/community/tutorials/<b>markov</b>-chains-python-tutorial", "snippet": "A <b>Markov</b> chain is a random <b>process</b> with the <b>Markov</b> property. A random <b>process</b> or often called stochastic property is a mathematical object defined as a collection of random variables. A <b>Markov</b> chain has either discrete state space (set of possible values of the random variables) or discrete index set (often representing time) - given the fact, many variations for a <b>Markov</b> chain exists. Usually the term &quot;<b>Markov</b> chain&quot; is reserved for a <b>process</b> with a discrete set of times, that is a Discrete ...", "dateLastCrawled": "2022-02-02T09:33:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Markov</b> <b>Decision</b> Processes - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/mathematics/markov-decision-processes", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/mathematics/<b>markov</b>-<b>decision</b>-<b>process</b>es", "snippet": "<b>Markov</b> <b>Decision</b> Processes. A finite <b>MDP</b> <b>can</b> be defined using the four-tuple (S,A,P,R), where S is a finite set of states of the system or environment, A is a finite set of possible actions when in the states sk\u2208S, P represents a state transition probability matrix, Pak(sk,sk+1) is the probability that the state sk\u2208S at k transits to the state sk+1 at k + 1 with an action ak\u2208A at k, and R represents a reward that assesses the advantage of an action ak\u2208A for all sk\u2208S.", "dateLastCrawled": "2022-01-06T10:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Markov Decision Process</b> Model for Patient Admission <b>Decision</b> at an ...", "url": "https://link.springer.com/chapter/10.1007/978-3-319-35132-2_18", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/chapter/10.1007/978-3-319-35132-2_18", "snippet": "We use <b>Markov Decision Process</b> (<b>MDP</b>) to make admission decisions for a finite horizon. It models the time-dependent arrival of disaster victims and their time-dependent survival probabilities. We numerically solve the <b>MDP</b> model using a discretization technique. The results of experiments conducted using virtual patient arrival data, in which the efficiency of our <b>MDP</b> solution was <b>compared</b> with that of other operating schemes, indicate that our proposed <b>MDP</b> model <b>can</b> improve the efficiency of ...", "dateLastCrawled": "2022-01-06T19:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Markov decision process model for patient admission decision</b> at an ...", "url": "https://link.springer.com/article/10.1007%2Fs10696-017-9276-8", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10696-017-9276-8", "snippet": "We study an admission control problem for patients arriving at an emergency department in the aftermath of a mass casualty incident. A finite horizon <b>Markov decision process</b> (<b>MDP</b>) model is formulated to determine patient admission decisions. In particular, our model considers the time-dependent arrival of patients and time-dependent reward function. We also consider a policy restriction that immediate-patients should be admitted as long as there is available beds. The <b>MDP</b> model has a ...", "dateLastCrawled": "2022-02-01T16:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Chapter 5: Simulation-Based <b>Markov</b> <b>Decision</b> Processes \u2013 Hands-On ...", "url": "https://w3sdev.com/chapter-5-simulation-based-markov-decision-processes-hands-on-simulation-modeling-with-python.html", "isFamilyFriendly": true, "displayUrl": "https://w3sdev.com/chapter-5-simulation-based-<b>markov</b>-<b>decision</b>-<b>process</b>es-hands-on...", "snippet": "<b>Markov</b> <b>Decision</b> Processes (MDPs) model <b>decision</b>-making in situations where outcomes are partly random and partly under the control of <b>a decision</b> maker. An <b>MDP</b> is a stochastic <b>process</b> characterized by five elements: <b>decision</b> epochs, states, actions, transition probability, and reward. The characteristic elements of a Markovian <b>process</b> are the states in which the system finds itself and the available actions that the <b>decision</b> maker <b>can</b> carry out on those states. These elements identify two ...", "dateLastCrawled": "2021-11-20T01:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Is <b>Markov Decision Process</b> a step <b>for reinforcement learning</b> or ... - Quora", "url": "https://www.quora.com/Is-Markov-Decision-Process-a-step-for-reinforcement-learning-or-a-model-approach-for-reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Is-<b>Markov-Decision-Process</b>-a-step-<b>for-reinforcement-learning</b>-or...", "snippet": "Answer: No. A <b>Markov decision process</b> (<b>MDP</b>) is the problem that <b>reinforcement learning</b>(RL) tries to solve. We often use \u201c<b>Markov</b> <b>decision</b> problem\u201d to clarify that the <b>process</b> is not an algorithm, but rather that it represents a problem to be solved (the reason it\u2019s called a <b>process</b> is because in s...", "dateLastCrawled": "2022-01-11T19:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Markov Decision Processes and Its Applications in Healthcare</b>", "url": "https://www.researchgate.net/publication/281272258_Markov_Decision_Processes_and_Its_Applications_in_Healthcare", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/281272258_<b>Markov_Decision_Processes_and_Its</b>...", "snippet": "This need not be the case when the redeployment problem is formulated as a <b>Markov Decision Process</b> (<b>MDP</b>) [21], as other information related to the system state <b>can</b> be captured in the <b>decision</b> ...", "dateLastCrawled": "2022-01-21T22:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Evaluating the <b>Markov</b> Assumption in <b>Markov</b> <b>Decision</b> Processes for ...", "url": "https://www.jstor.org/stable/30200558", "isFamilyFriendly": true, "displayUrl": "https://www.jstor.org/stable/30200558", "snippet": "<b>Markov Decision Process</b> (<b>MDP</b>), which assumes that the state of the dialogue depends only on the previous state and action. In this article, we investigate whether constraining the state space by the <b>Markov</b> assumption, especially when the struc-ture of the state space may be unknown, truly affords the highest reward. In simu-lation experiments conducted in the context of a dialogue system for interacting with a speech-enabled web browser, models under the <b>Markov</b> assumption did not per-form as ...", "dateLastCrawled": "2021-12-15T19:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Optimal Blackjack Strategy Using MDP Solvers</b> \u2013 Kunal Menda", "url": "https://kunalmenda.com/2016/08/20/optimal-blackjack-strategy-using-mdp-solvers/", "isFamilyFriendly": true, "displayUrl": "https://kunalmenda.com/2016/08/20/<b>optimal-blackjack-strategy-using-mdp-solvers</b>", "snippet": "A <b>Markov Decision Process</b> (<b>MDP</b>) is a powerful way to allow AI to make decisions in uncertain environments. In this project, I used the algorithm to provide the optimal strategy for Blackjack. I then <b>compared</b> the strategy to a widely available \u201cBlackjack Cheat Sheet\u201d, and found the strategy provided by <b>MDP</b> and the cheat sheet to have comparable performances of expecting to lose only 5\u00a2 on a $10 bet, which is around the \u201chouse advantage\u201d of 0.5% to the casino, indicating that it is ...", "dateLastCrawled": "2022-01-22T06:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Transportation infrastructure asset management modeling using <b>Markov</b> ...", "url": "https://www.emerald.com/insight/content/doi/10.1108/SRT-11-2020-0026/full/html", "isFamilyFriendly": true, "displayUrl": "https://www.emerald.com/insight/content/doi/10.1108/SRT-11-2020-0026/full/html", "snippet": "We use <b>Markov decision process</b> (<b>MDP</b>) optimization techniques to compute the best out of all possible combinations of MR&amp;R <b>decision</b> scenarios for a number of assets over a dynamic, multi-year time horizon. On top of the inherent stochasticity in infrastructure deterioration, we proposed a simulation model to account for the probability distribution and variances of model parameters. Literature review. The current infrastructure asset management literature develops and optimizes an integrated ...", "dateLastCrawled": "2022-01-19T23:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "A Simulation Model of Intermittently Controlled Point-and-Click Behaviour", "url": "https://minsukchang.com/assets/pdf/chi2021-pointanclick.pdf", "isFamilyFriendly": true, "displayUrl": "https://minsukchang.com/assets/pdf/chi2021-pointanclick.pdf", "snippet": "click action in a given environment <b>can</b> be expressed as a <b>Markov decision process</b> (<b>MDP</b>), a natural framework for formulating se-quential <b>decision</b>-making processes. For each j-th <b>decision</b> step, there are two variables that represent the simulated user\u2019s action aj (prediction horizonTh and click <b>decision</b> K), and six variables that represent the environmental states sj that the user perceives (position and velocity of cursor and target, radius of target and hand position). As a result of the ...", "dateLastCrawled": "2022-02-03T04:09:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Why does <b>Markov Decision Process</b> matter in Reinforcement <b>Learning</b>? | by ...", "url": "https://towardsdatascience.com/why-does-malkov-decision-process-matter-in-reinforcement-learning-b111b46b41bd", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/why-does-malkov-<b>decision</b>-<b>process</b>-matter-in...", "snippet": "It is named by <b>analogy</b> to \u201cone-armed bandit\u201d(= a slot <b>machine</b>) although the framework has k levers instead of one. ... we introduce <b>Markov Decision Process</b>(<b>MDP</b>) to solve such a problem. An <b>MDP</b> consists of two elements; the agent and the environment. The agent is a learner or <b>decision</b>-maker. In the above example, the agent is the rabbit. The environment is everything surrounding the agent. In the example, the environment includes everything in the field where the rabbit is with food and ...", "dateLastCrawled": "2022-01-31T01:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Markov Decision Process</b>: How Does Value Iteration Work? | Baeldung on ...", "url": "https://www.baeldung.com/cs/mdp-value-iteration", "isFamilyFriendly": true, "displayUrl": "https://www.baeldung.com/cs/<b>mdp</b>-value-iteration", "snippet": "From this point, we can make an <b>analogy</b> with the <b>Markov</b> model since the solution for this problem is a sequence of actions. A <b>Markov Decision Process</b> is used to model the agent, considering that the agent itself generates a series of actions. In the real world, we can have observable, hidden, or partially observed states, depending on the ...", "dateLastCrawled": "2022-01-30T20:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Introduction to Machine Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/24/introduction-to-machine-learning-2/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/24/<b>introduction-to-machine-learning</b>-2", "snippet": "<b>Markov decision process</b> (<b>MDP</b>): In reinforcement <b>learning</b>, <b>MDP</b> is a mathematical framework for modeling <b>decision</b>-making of an agent in situations or environments where outcomes are partly random and partly under control. In this model, environment is modeled as a set of states and actions that can be performed by an agent to control the system\u2019s state. The objective is to control the system in such a way that the agent\u2019s total payoff is maximized.", "dateLastCrawled": "2022-01-17T05:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Reinforcement <b>Learning</b> | Ioannis Anifantakis | Analytics Vidhya", "url": "https://medium.com/analytics-vidhya/reinforcement-learning-basic-understanding-4fcb91ba4e4", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/reinforcement-<b>learning</b>-basic-understanding-4fcb91ba4e4", "snippet": "<b>Markov Decision Process</b> (<b>MDP</b>) The States, Actions, Rewards, their mechanics (known as One-Step Dynamics ), together with the discount rate (\u03b3) define a <b>Markov Decision Process</b> (<b>MDP</b>) .", "dateLastCrawled": "2021-08-02T19:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Markov decision process</b>: value iteration with code implementation | by ...", "url": "https://medium.com/@ngao7/markov-decision-process-value-iteration-2d161d50a6ff", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@ngao7/<b>markov-decision-process</b>-value-iteration-2d161d50a6ff", "snippet": "<b>Markov decision process</b>, <b>MDP</b>, value iteration, policy iteration, policy evaluation, policy improvement, sweep, iterative policy evaluation, policy, optimal policy ...", "dateLastCrawled": "2022-01-08T01:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Lecture <b>Reinforcement Learning</b> - MIT OpenCourseWare", "url": "https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-s897-machine-learning-for-healthcare-spring-2019/lecture-notes/MIT6_S897S19_lec16note.pdf", "isFamilyFriendly": true, "displayUrl": "https://<b>ocw.mit.edu</b>/courses/electrical-engineering-and-computer-science/6-s897-<b>machine</b>...", "snippet": "4.1 Examples of <b>decision</b> processes. A <b>Markov decision process</b> (<b>MDP</b>) is a well-known type of <b>decision</b> <b>process</b>, where the states follow the <b>Markov</b> assumption that the state transitions, rewards, and actions depend only on the most recent state-action pair. See Figure 3(a) for an illustration. Algebraically, this means the states, actions and reward", "dateLastCrawled": "2022-02-03T08:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "18.1. <b>Markov Decision Process</b> (<b>MDP</b>) \u2014 Dive into Deep <b>Learning</b> 0.17.0 ...", "url": "http://preview.d2l.ai.s3-website-us-west-2.amazonaws.com/d2l-en/master/chapter_reinforcement_learning/mdp.html", "isFamilyFriendly": true, "displayUrl": "preview.d2l.ai.s3-website-us-west-2.amazonaws.com/...reinforcement_<b>learning</b>/<b>mdp</b>.html", "snippet": "In this section, we will discuss how to formulate reinforcement <b>learning</b> problems using <b>Markov</b> <b>decision</b> processes (MDPs) and describe in detail various components of MDPs. Definition of an <b>MDP</b> \u00b6 A <b>Markov decision process</b> (<b>MDP</b>) is a model for how the state of a system evolves as different actions are applied to it.", "dateLastCrawled": "2022-01-27T05:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Meta-<b>Learning</b> as a <b>Markov Decision Process</b>", "url": "https://hal.archives-ouvertes.fr/tel-02422144v2/document", "isFamilyFriendly": true, "displayUrl": "https://hal.archives-ouvertes.fr/tel-02422144v2/document", "snippet": "Meta-<b>Learning</b> as a <b>Markov Decision Process</b>. <b>Machine</b> <b>Learning</b> [cs.LG]. Uni-versit\u00e9 Paris Saclay (COmUE), 2019. English. \uffffNNT: 2019SACLS588\uffff. \ufffftel-02422144v2\uffff I would like to dedicate this thesis to my loving parents doctorat CLS588 Meta-<b>Learning</b> as a <b>Markov Decision Process</b> Th\u00e8se de doctorat de l\u2019Universit\u00e9 Paris-Saclay pr\u00e9par\u00e9e \u00e0 l\u2019Universit\u00e9 Paris-Sud Ecole doctorale n 580 Sciences et Technologies de l\u2019Information et de la Communication (STIC) Sp\u00e9cialit\u00e9 de doctorat ...", "dateLastCrawled": "2022-01-13T07:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>machine</b> <b>learning</b> - Why is the <b>optimal policy</b> in <b>Markov Decision Process</b> ...", "url": "https://stats.stackexchange.com/questions/132890/why-is-the-optimal-policy-in-markov-decision-process-mdp-independent-of-the-i", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/132890", "snippet": "The intuition behind the argument saying that the <b>optimal policy</b> is independent of initial state is the following: The <b>optimal policy</b> is defined by a function that selects an action for every possible state and actions in different states are independent.. Formally speaking, for an unknown initial distribution, the value function to maximize would be the following (not conditioned on initial state)", "dateLastCrawled": "2022-01-25T23:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Real-life <b>examples</b> of <b>Markov</b> <b>Decision</b> Processes - Cross Validated", "url": "https://stats.stackexchange.com/questions/145122/real-life-examples-of-markov-decision-processes", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/145122", "snippet": "A Markovian <b>Decision</b> <b>Process</b> indeed has to do with going from one state to another and is mainly used for planning and <b>decision</b> making. The theory. Just repeating the theory quickly, an <b>MDP</b> is: $$\\text{<b>MDP</b>} = \\langle S,A,T,R,\\gamma \\rangle$$", "dateLastCrawled": "2022-01-24T10:32:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Overview: Representation Techniques", "url": "https://webcms3.cse.unsw.edu.au/static/uploads/course/COMP4418/17s2/9c0e19e0e02df66fffb5d0bd4c20697922f5ffbf9a602b66bec3f74ac83fb77c/DecisionMaking.pdf", "isFamilyFriendly": true, "displayUrl": "https://webcms3.cse.unsw.edu.au/static/uploads/course/COMP4418/17s2/9c0e19e0e02df66...", "snippet": "<b>Markov Decision Process MDP is like</b> a Markov process, except every round we make a decision Transition probabilities depend on actions taken P(St+1 = S&#39; | St = s, At = a) = P(S, a, S&#39;) Rewards for every state, action pair u(St = s, At = a) Discount factor \u03b4 Example. A <b>machine</b> can be in one of three states: good, deteriorating, broken Can take ...", "dateLastCrawled": "2022-01-21T05:20:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], []], "all_bing_queries": ["+(markov decision process (mdp))  is like +(simulating a decision process)", "+(markov decision process (mdp)) is similar to +(simulating a decision process)", "+(markov decision process (mdp)) can be thought of as +(simulating a decision process)", "+(markov decision process (mdp)) can be compared to +(simulating a decision process)", "machine learning +(markov decision process (mdp) AND analogy)", "machine learning +(\"markov decision process (mdp) is like\")", "machine learning +(\"markov decision process (mdp) is similar\")", "machine learning +(\"just as markov decision process (mdp)\")", "machine learning +(\"markov decision process (mdp) can be thought of as\")", "machine learning +(\"markov decision process (mdp) can be compared to\")"]}