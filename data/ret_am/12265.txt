{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Dictionary</b> based representations \u2014 <b>sparse</b>-plex v2019.02", "url": "https://sparse-plex.readthedocs.io/en/latest/book/sparse_signal_models/dictionary_representations.html", "isFamilyFriendly": true, "displayUrl": "https://<b>sparse</b>-plex.readthedocs.io/.../<b>dictionary</b>_<b>representations</b>.html", "snippet": "Thus such a <b>dictionary</b> is able provide multiple representations to same vector \\(x\\). We call such dictionaries redundant dictionaries or over-complete dictionaries. In contrast a basis with \\(D=N\\) is called a complete <b>dictionary</b>. A special class of signals is those signals which have a <b>sparse</b> <b>representation</b> in a given <b>dictionary</b> \\(\\mathcal{D}\\).", "dateLastCrawled": "2022-02-02T21:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Dictionary Training for Sparse Representation as Generalization of</b> K ...", "url": "https://ieeexplore.ieee.org/document/6504716", "isFamilyFriendly": true, "displayUrl": "https://ieeexplore.ieee.org/document/6504716", "snippet": "Recent <b>dictionary</b> training algorithms for <b>sparse</b> <b>representation</b> <b>like</b> K-SVD, MOD, and their variation are reminiscent of K-means clustering, and this letter investigates such algorithms from that viewpoint. It shows: though K-SVD is sequential <b>like</b> K-means, it fails to simplify to K-means by destroying the structure in the <b>sparse</b> coefficients. In contrast, MOD can be viewed as a parallel generalization of K-means, which simplifies to K-means without perturbing the <b>sparse</b> coefficients. Keeping ...", "dateLastCrawled": "2021-12-09T19:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Dictionary</b> Learning Algorithms for <b>Sparse</b> <b>Representation</b>", "url": "https://papers.cnl.salk.edu/PDFs/Dictionary%20Learning%20Algorithms%20for%20Sparse%20Representation%202003-3696.pdf", "isFamilyFriendly": true, "displayUrl": "https://papers.cnl.salk.edu/PDFs/<b>Dictionary</b> Learning Algorithms for <b>Sparse</b>...", "snippet": "to a <b>sparse</b> set of <b>dictionary</b> words or vectors can be related to entropy min- ... 1981). Finding a <b>sparse</b> <b>representation</b> (based on the use of a \u201cfew\u201d code or <b>dictio-nary</b> words) can also be viewed as a generalization of vector quantization where a match to a single \u201ccode vector\u201d (word) is always sought (taking \u201ccode book\u201d = \u201c<b>dictionary</b>\u201d).4 Indeed, we can refer to a <b>sparse</b> solution, x, as a <b>sparse</b> coding of the signal instantiation, y. 1.1 Stochastic Models. It is well known ...", "dateLastCrawled": "2022-01-31T18:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Sparse</b> Matrix and its representations | Set 1 (Using Arrays and Linked ...", "url": "https://www.geeksforgeeks.org/sparse-matrix-representation/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/<b>sparse</b>-matrix-<b>representation</b>", "snippet": "<b>Sparse</b> Matrix and its representations | Set 2 (Using List of Lists and <b>Dictionary</b> of keys) This article is contributed by Akash Gupta.If you <b>like</b> <b>GeeksforGeeks</b> and would <b>like</b> to contribute, you can also write an article using write.<b>geeksforgeeks</b>.org or mail your article to review-team@<b>geeksforgeeks</b>.org. See your article appearing on the ...", "dateLastCrawled": "2022-01-31T00:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Dictionary</b> training for <b>sparse</b> <b>representation</b> as generalization of K ...", "url": "https://dr.ntu.edu.sg/bitstream/10356/96655/1/SGK.pdf", "isFamilyFriendly": true, "displayUrl": "https://dr.ntu.edu.sg/bitstream/10356/96655/1/SGK.pdf", "snippet": "<b>Dictionary</b> Training for <b>Sparse</b> <b>Representation</b> as Generalization of K-means Clustering Sujit Kumar Sahoo, Member, IEEE and Anamitra Makur, Senior Member, IEEE Abstract\u2014Recent <b>dictionary</b> training algorithms for <b>sparse</b> <b>representation</b> <b>like</b> K-SVD, MOD, and their variation are rem-iniscent of K-means clustering, and this letter investigates such algorithms from that viewpoint. It shows: though K-SVD is sequential <b>like</b> K-means, it fails to simplify to K-means by de-stroying the structure in the ...", "dateLastCrawled": "2022-01-13T18:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "What are the <b>Types of Sparse Dictionary Learning Algorithms</b>? | FinsliQ", "url": "https://www.finsliqblog.com/ai-and-machine-learning/what-are-the-types-of-sparse-dictionary-learning-algorithms/", "isFamilyFriendly": true, "displayUrl": "https://www.finsliqblog.com/ai-and-machine-learning/what-are-the-types-of-<b>sparse</b>...", "snippet": "<b>Sparse</b> Dictionaries: This method mainly focuses on not only giving a <b>sparse</b> <b>representation</b> but also constructing <b>dictionary</b> learning algorithms which is enforced by the equation . where . is some predefined analytical <b>dictionary</b> with desirable properties <b>like</b> fast computation and . is a <b>sparse</b> matrix. These formulation helps to directly combine the fast implementation of the analytical dictionaries with the flexibility of <b>sparse</b> techniques.", "dateLastCrawled": "2021-12-30T02:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 6, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Sparse dictionary learning</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Sparse_dictionary_learning", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Sparse_dictionary_learning</b>", "snippet": "<b>Sparse</b> coding is a <b>representation</b> learning method which aims at finding a <b>sparse</b> <b>representation</b> of the input data (also known as <b>sparse</b> coding) in the form of a linear combination of basic elements as well as those basic elements themselves.These elements are called atoms and they compose a <b>dictionary</b>.Atoms in the <b>dictionary</b> are not required to be orthogonal, and they may be an over-complete spanning set.This problem setup also allows the dimensionality of the signals being represented to be ...", "dateLastCrawled": "2022-01-27T21:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Overcomplete <b>Representation and Dictionary Learning</b> \u2013 Vidya", "url": "https://myblogs806213342.wordpress.com/2018/03/23/overcomplete-representation-and-dictionary-learning/", "isFamilyFriendly": true, "displayUrl": "https://myblogs806213342.wordpress.com/2018/03/23/overcomplete-<b>representation</b>-and...", "snippet": "<b>Dictionary</b> (D) is learned in a manner that any signal x can be represented as where M&gt;&gt;N (overcomplete) and is the <b>sparse</b> code. If the sparsity of is K it means that out of the M entries only K is non-zero. Next, we will see how to create the <b>dictionary</b> and the <b>sparse</b> code provided a set training data X. <b>Dictionary</b> learning is done using the ...", "dateLastCrawled": "2022-01-15T01:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "python - Convert <b>sparse</b> <b>dictionary</b> <b>representation</b> into a dense ...", "url": "https://stackoverflow.com/questions/25943870/convert-sparse-dictionary-representation-into-a-dense-dataframe", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/25943870", "snippet": "I would <b>like</b> to convert a list of dictionaries which sparsely represent individual observations of features into a dense data structure (e.g. a dataframe). Each observation is a <b>dictionary</b> with", "dateLastCrawled": "2022-01-06T20:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "python - <b>Dictionary To Sparse Vector help- sort dictionary</b>? [SOLVED ...", "url": "https://www.daniweb.com/programming/software-development/threads/409992/dictionary-to-sparse-vector-help-sort-dictionary", "isFamilyFriendly": true, "displayUrl": "https://<b>www.daniweb.com</b>/.../409992/<b>dictionary-to-sparse-vector-help-sort-dictionary</b>", "snippet": "A <b>sparse</b> vector is a vector whose entries are almost all zero, <b>like</b> [1, 0, 0, 0, 0, 0, 0, 2, 0]. Storing all those zeros wastes memory and dictionaries are commonly used to keep track of just the nonzero entries. For example, the vector shown earlier can be represented as {0:1, 7:2}, since the vector it is meant to represent has the value 1 at index 0 and the value 2 at index 7. Write a function that converts a <b>dictionary</b> back to its sparese vector <b>representation</b>.", "dateLastCrawled": "2022-01-12T02:58:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Dictionary</b> Learning with Uniform <b>Sparse</b> Representations for Anomaly ...", "url": "https://deepai.org/publication/dictionary-learning-with-uniform-sparse-representations-for-anomaly-detection", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>dictionary</b>-learning-with-uniform-<b>sparse</b>-<b>representations</b>...", "snippet": "The Joint <b>Sparse</b> <b>Representation</b> (JSR) model analyzed in [ZhaLi:13, LiZha:15] assumes a multi-class partitioning of the input, where each class spans a low-dimensional subspace. In [AdlEla:15], \u2113 1 penalty-based JSR is used for detecting noisy anomalies with prefixed <b>dictionary</b>. Although their formulations are <b>similar</b> to ours, the authors aim ...", "dateLastCrawled": "2022-02-03T18:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Latent <b>Dictionary</b> Learning for <b>Sparse</b> <b>Representation</b> based Classification", "url": "https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Yang_Latent_Dictionary_Learning_2014_CVPR_paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cv-foundation.org/.../Yang_Latent_<b>Dictionary</b>_Learning_2014_CVPR_paper.pdf", "snippet": "proposed <b>sparse</b> <b>representation</b> and <b>dictionary</b> learning approaches for action, gender and face recognition. 1. Introduction With the inspiration of <b>sparse</b> coding mechanism of human vision system [3][4], <b>sparse</b> coding by representing a signal as a <b>sparse</b> linear combination of <b>representation</b> bases (i.e., a <b>dictionary</b> of atoms) has been successfully applied to image restoration [1][2], image classification [5][6], to name a few. The <b>dictionary</b>, which should faithfully and discriminatively ...", "dateLastCrawled": "2022-01-08T23:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Sparse</b> Analysis Model Based <b>Dictionary</b> Learning for Signal Declipping ...", "url": "https://ieeexplore.ieee.org/abstract/document/9324931/similar", "isFamilyFriendly": true, "displayUrl": "https://ieeexplore.ieee.org/abstract/document/9324931/<b>similar</b>", "snippet": "<b>Sparse</b> <b>representation</b> has underpinned several algorithms developed recently for reconstruction of the original signal from clipped observations. However, these declipping algorithms are often built on a synthesis model, where the signal is represented by a <b>dictionary</b> weighted by <b>sparse</b> coding coefficients. In contrast to these works, we propose a <b>sparse</b> analysis-model-based declipping (SAD) method, where the declipping model is formulated on an analysis (i.e. transform) <b>dictionary</b>, and ...", "dateLastCrawled": "2022-01-15T20:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Group <b>Sparse</b> <b>Representation</b> Based <b>Dictionary</b> Learning for SAR Image ...", "url": "https://doaj.org/article/f92f3e519ead4be380199f53598b54d1", "isFamilyFriendly": true, "displayUrl": "https://doaj.org/article/f92f3e519ead4be380199f53598b54d1", "snippet": "Different from traditional patch-based <b>sparse</b> <b>representation</b> theory, GSR is able to sparsely represent images in the domain of group which contains the image patches with <b>similar</b> structure. Based on the multiplicative speckle noise model, a novel <b>dictionary</b> learning algorithm based on GSR (GSR-DL) for SAR image despeckling is proposed. The proposed algorithm mainly consists of three steps. First, in order to realize the recovery of despeckled SAR image by the GSR model, a mean filter is ...", "dateLastCrawled": "2022-01-15T20:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Image Super-Resolution via <b>Sparse</b> <b>Representation</b>", "url": "http://www.columbia.edu/~jw2966/papers/YWHM10-TIP.pdf", "isFamilyFriendly": true, "displayUrl": "<b>www.columbia.edu</b>/~jw2966/papers/YWHM10-TIP.pdf", "snippet": "an appropriately chosen over-complete <b>dictionary</b>. Inspired by this observation, we seek a <b>sparse</b> <b>representation</b> for each patch of the low-resolution input, and then use the coef\ufb01cients of this <b>representation</b> to generate the high-resolution output. Theoretical results from compressed sensing suggest that under mild condi-tions, the <b>sparse</b> <b>representation</b> can be correctly recovered from the downsampled signals. By jointly training two dictionaries for the low- and high-resolution image ...", "dateLastCrawled": "2022-02-01T22:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Image restoration <b>based on sparse representation</b> using feature ...", "url": "https://jivp-eurasipjournals.springeropen.com/articles/10.1186/s13640-020-00531-5", "isFamilyFriendly": true, "displayUrl": "https://jivp-eurasipjournals.springeropen.com/articles/10.1186/s13640-020-00531-5", "snippet": "In recent years, with the continuous improvement of <b>sparse</b> <b>representation</b> theory, researchers have applied <b>sparse</b> <b>representation</b> to image restoration, and gradually formed the third kind of method, that is, the <b>sparse</b>-<b>representation</b>-based method [15,16,17,18]. Its basic idea is to calculate the <b>sparse</b> coding of the damaged patch on the over-complete <b>dictionary</b>, and then reconstruct the damaged patch using the <b>sparse</b> coding and the over-complete <b>dictionary</b>. Since the <b>sparse</b> <b>representation</b> ...", "dateLastCrawled": "2022-01-27T18:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "K-<b>Means, Sparse Coding, Dictionary Learning and All</b> That \u2014 Data, ML ...", "url": "https://bugra.github.io/posts/2015/2/10/k-means-sparse-coding-dictionary-learning-and-all-that/", "isFamilyFriendly": true, "displayUrl": "https://bugra.github.io/posts/2015/2/10/k-<b>means-sparse-coding-dictionary-learning-and</b>...", "snippet": "Moreover, after building these dictionaries one could only pass the <b>dictionary</b> along with the <b>representation</b> of the vector to enable robust recovery <b>similar</b> to FFT. <b>Dictionary</b> learning is very <b>similar</b> to <b>sparse</b> coding in terms of it tries to represent the data, it tries to find good atoms from a \u201c<b>dictionary</b>\u201d where the <b>dictionary</b> atoms are learned from the training set. Especially, for classification tasks, as long as the user has a good <b>dictionary</b>, one could build very efficient vectors ...", "dateLastCrawled": "2022-01-01T11:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Dictionary</b> training for <b>sparse</b> <b>representation</b> as generalization of K ...", "url": "https://dr.ntu.edu.sg/bitstream/10356/96655/1/SGK.pdf", "isFamilyFriendly": true, "displayUrl": "https://dr.ntu.edu.sg/bitstream/10356/96655/1/SGK.pdf", "snippet": "<b>representation</b> error, a VQ codebook is typically trained using K-means clustering algorithm. It is an iterative process <b>similar</b> <b>to dictionary</b> training which alternates between \ufb01nding X and updating D. 1) <b>Sparse</b> coding (encoding) stage: This stage involves \ufb01nding the index k = argmin j ky i D(t)e jk2 2, so that the <b>sparse</b> <b>representation</b> for ...", "dateLastCrawled": "2022-01-13T18:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Dictionary</b> Learning Based Applications in Image Processing using Convex ...", "url": "http://home.iitk.ac.in/~saurabhk/EE609A_12011_12807637_.pdf", "isFamilyFriendly": true, "displayUrl": "home.iitk.ac.in/~saurabhk/EE609A_12011_12807637_.pdf", "snippet": "<b>similar</b> performances with the state of the art methods. Keywords\u2014<b>Sparse</b>, OMP, K-SVD, DCT. I. Introduction Natural signals such as images admit a <b>sparse</b> <b>represen-tation</b> i.e. they can be represented as a linear combination of few vectors (atoms of a learned <b>dictionary</b>). It has been established that there is much redundancy in natural signals. It can be represented in terms of few basis vectors. Since the <b>sparse</b> <b>representation</b> consists of mostly zeros, we only have to store only the non-zero ...", "dateLastCrawled": "2022-01-17T20:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>GitHub</b> - <b>tiepvupsu/DICTOL</b>: DICTOL - A <b>Dictionary</b> Learning Toolbox in ...", "url": "https://github.com/tiepvupsu/DICTOL", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/<b>tiepvupsu/DICTOL</b>", "snippet": "K: total number of <b>dictionary</b> bases. D_range: <b>similar</b> to Y_range but used for <b>dictionary</b> without the shared <b>dictionary</b>. <b>Sparse</b> <b>Representation</b>-based classification (SRC) <b>Sparse</b> <b>Representation</b>-based classification implementation . Classification based on SRC. Syntax: [pred, X] = SRC_pred(Y, D, D_range, opts) INPUT: Y: test samples.", "dateLastCrawled": "2022-01-31T17:15:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A Study of the K-SVD <b>Algorithm for Designing Overcomplete Dictionaries</b> ...", "url": "https://www.caam.rice.edu/~optimization/L1/optseminar/K-SVD_talk_lijun.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.caam.rice.edu/~optimization/L1/optseminar/K-SVD_talk_lijun.pdf", "snippet": "The arbitrary approximation in norm <b>can</b> <b>be thought</b> as a <b>representation</b> somehow. \u2022In recent years there has been a growing interest in the study of <b>sparse</b> <b>representation</b> of signals which is based on an overcomplete <b>dictionary</b>. \u2022Applications that use <b>sparse</b> <b>representation</b> are many and include compression, regularization in inverse problems, feature extraction, and more. \u2022Sparsity in overcomplete dictionaries is the basis for a wide variety of highly effective signal and image processing ...", "dateLastCrawled": "2022-01-27T02:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Research Article A <b>Sparse</b> <b>Representation</b> Based Method to Classify ...", "url": "https://downloads.hindawi.com/journals/cmmm/2015/567932.pdf", "isFamilyFriendly": true, "displayUrl": "https://downloads.hindawi.com/journals/cmmm/2015/567932.pdf", "snippet": "<b>dictionary</b>. It is <b>thought</b> that the <b>sparse</b> <b>representation</b> <b>can</b> improve the performance of the image classi cation [ ]. Firstly, the images could be treated as a distribution of a set of representative features, so the <b>sparse</b> <b>representation</b> <b>can</b> encode the semantic information of the images. Secondly, the number of atoms in the <b>dictionary</b> is greater than the dimensionality of the input examples, which means that the approximation of the example is not unique. So, it <b>can</b> nd a relative better ...", "dateLastCrawled": "2021-09-27T08:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "US8484023B2 - <b>Sparse representation features</b> for speech recognition ...", "url": "https://patents.google.com/patent/US8484023B2/en", "isFamilyFriendly": true, "displayUrl": "https://patents.google.com/patent/US8484023B2/en", "snippet": "H <b>can</b> <b>be thought</b> of as an over-complete <b>dictionary</b> where m&lt;&lt;N. We <b>can</b> then write test vector y as a linear combination of all training examples, i.e., y=H\u03b2. Ideally, the optimal \u03b2 should be <b>sparse</b>, and only be non-zero for the elements in H which belong to the same class as y. Thus, ideally, y will assign itself to lie in the linear span of examples from the training set of the true class to which it belongs. For example, assume that H is comprised of features from five classes and y ...", "dateLastCrawled": "2021-12-19T02:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Sparse</b> Matrix and its representations | Set 1 (Using Arrays and Linked ...", "url": "https://www.geeksforgeeks.org/sparse-matrix-representation/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/<b>sparse</b>-matrix-<b>representation</b>", "snippet": "<b>Sparse</b> Matrix Representations <b>can</b> be done in many ways following are two common representations: Array <b>representation</b>. Linked list <b>representation</b>. Method 1: Using Arrays: 2D array is used to represent a <b>sparse</b> matrix in which there are three rows named as. Row: Index of row, where non-zero element is located.", "dateLastCrawled": "2022-01-31T00:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "12.5. <b>Sparse</b> Matrices \u2014 How to Think like a Computer Scientist ...", "url": "https://runestone.academy/ns/books/published/thinkcspy/Dictionaries/Sparsematrices.html", "isFamilyFriendly": true, "displayUrl": "https://runestone.academy/ns/books/published/thinkcspy/Dictionaries/<b>Sparse</b>matrices.html", "snippet": "<b>Sparse</b> Matrices\u00b6 A matrix is a two dimensional collection, typically <b>thought</b> of as having rows and columns of data. One of the easiest ways to create a matrix is to use a list of lists. For example, consider the matrix shown below. We <b>can</b> represent this collection as five rows, each row having five columns. Using a list of lists <b>representation</b>, we will have a list of five items, each of which is a list of five items. The outer items represent the rows and the items in the nested lists ...", "dateLastCrawled": "2022-01-14T06:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) <b>Sparse Representation and Dictionary Learning Model</b> Incorporating ...", "url": "https://www.researchgate.net/publication/341886352_Sparse_Representation_and_Dictionary_Learning_Model_Incorporating_Group_Sparsity_and_Incoherence_to_Extract_Abnormal_Brain_Regions_Associated_With_Schizophrenia", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/341886352_<b>Sparse</b>_<b>Representation</b>_and...", "snippet": "Schizophrenia is a complex mental illness, the mechanism of which is currently unclear. Using <b>sparse representation and dictionary learning</b> (SDL) model to analyze functional magnetic resonance ...", "dateLastCrawled": "2021-08-28T12:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Sparse and Redundant Representation Modeling \u2014 What Next</b>?", "url": "https://elad.cs.technion.ac.il/wp-content/uploads/2018/02/Paper-IEEE-SPL-Sparsity-Final.pdf", "isFamilyFriendly": true, "displayUrl": "https://elad.cs.technion.ac.il/wp-content/uploads/2018/02/Paper-IEEE-SPL-Sparsity...", "snippet": "<b>dictionary</b>. The forward transform is a highly non-linear one, (P 0) ^ = argmin k k 0 s:t: x = D ; (2) searching for the sparsest explanation for the signal x. The \u20180 cost function kk 0 counts the non-zero entries in this vector, and we expect a <b>sparse</b> outcome, k k 0 = k \u02ddd. This model <b>can</b> be interpreted as a chemistry of data sources: the ...", "dateLastCrawled": "2021-12-09T16:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "python - Convert <b>sparse</b> <b>dictionary</b> <b>representation</b> into a dense ...", "url": "https://stackoverflow.com/questions/25943870/convert-sparse-dictionary-representation-into-a-dense-dataframe", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/25943870", "snippet": "I would like to convert this list of dictionaries into a dense dataframe such that the columns contain all possible keys. I began writing some code, but <b>thought</b> that I&#39;d ask first if this functionality actually exists in a package somewhere. Thanks.", "dateLastCrawled": "2022-01-06T20:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Sparse</b> <b>representation</b> using overcomplete <b>dictionary</b> - when is L1 norm ...", "url": "https://math.stackexchange.com/questions/881699/sparse-representation-using-overcomplete-dictionary-when-is-l1-norm-not-good-e", "isFamilyFriendly": true, "displayUrl": "https://math.stackexchange.com/questions/881699/<b>sparse</b>-<b>representation</b>-using-over...", "snippet": "Stack Exchange network consists of 178 Q&amp;A communities including Stack Overflow, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers.. Visit Stack Exchange", "dateLastCrawled": "2021-12-24T14:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How to start to learn <b>sparse</b> coding or <b>dictionary</b> learning, such that I ...", "url": "https://www.quora.com/How-do-I-start-to-learn-sparse-coding-or-dictionary-learning-such-that-I-can-write-my-own-optimization-equations", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/How-do-I-start-to-learn-<b>sparse</b>-coding-or-<b>dictionary</b>-learning...", "snippet": "Answer: I think it depends on you. The <b>sparse</b> <b>representation</b> has huge literature. If we just restrict our discussion on learning based methods, you will need to have a good understanding of optimization and linear algebra. You need to have experience with some of the well-known optimization appro...", "dateLastCrawled": "2022-01-11T05:09:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Active <b>Dictionary</b> Learning in <b>Sparse</b> <b>Representation</b> Based ...", "url": "https://deepai.org/publication/active-dictionary-learning-in-sparse-representation-based-classification", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/active-<b>dictionary</b>-learning-in-<b>sparse</b>-<b>representation</b>...", "snippet": "The property of the <b>dictionary</b> <b>can</b> affect the <b>sparse</b> <b>representation</b> significantly. How to construct a proper <b>dictionary</b> is important for <b>sparse</b> <b>representation</b>. There are two major approaches for <b>dictionary</b> learning. First is the analytic approach, in which DCT bases, wavelets, curvelets and other nonadaptive functions are used as atoms to construct the dictionaries. Second is the learning-based approaches, such as the", "dateLastCrawled": "2022-01-11T02:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Image Super-Resolution via <b>Sparse</b> <b>Representation</b>", "url": "http://www.columbia.edu/~jw2966/papers/YWHM10-TIP.pdf", "isFamilyFriendly": true, "displayUrl": "<b>www.columbia.edu</b>/~jw2966/papers/YWHM10-TIP.pdf", "snippet": "an appropriately chosen over-complete <b>dictionary</b>. Inspired by this observation, we seek a <b>sparse</b> <b>representation</b> for each patch of the low-resolution input, and then use the coef\ufb01cients of this <b>representation</b> to generate the high-resolution output. Theoretical results from compressed sensing suggest that under mild condi-tions, the <b>sparse</b> <b>representation</b> <b>can</b> be correctly recovered from the downsampled signals. By jointly training two dictionaries for the low- and high-resolution image ...", "dateLastCrawled": "2022-02-01T22:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Sparse-Representation-Based Classi\ufb01cation with Structure</b>-Preserving ...", "url": "https://www.ele.uri.edu/faculty/he/PDFfiles/sparserepresentation.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.ele.uri.edu/faculty/he/PDFfiles/<b>sparserepresentation</b>.pdf", "snippet": "In recent years, <b>sparse</b> <b>representation</b> (or <b>sparse</b> coding) has received a lot of attentions. The key idea is to search for the least number of basis vectors (or atoms) in a <b>dictionary</b> A 2 Rm n to characterize a signal y 2 Rm (A has n atoms and each atom is a vector with m elements). Therefore, the signal <b>can</b> be represented as the <b>sparse</b> vectors ...", "dateLastCrawled": "2022-02-02T13:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Submodular <b>Dictionary</b> Selection for <b>Sparse</b> <b>Representation</b>", "url": "https://las.inf.ethz.ch/files/krause10submodular.pdf", "isFamilyFriendly": true, "displayUrl": "https://las.inf.ethz.ch/files/krause10submodular.pdf", "snippet": "that only a few <b>dictionary</b> elements, <b>compared</b> to the ambient signal dimension, <b>can</b> exactly repre-sent or well-approximate the signals of interest. We formulate both the selection of the <b>dictionary</b> columns and the <b>sparse</b> <b>representation</b> of signals as a joint combinatorial optimization problem. The proposed combinatorial objective maximizes variance reduction over the set of training signals by constraining the size of the <b>dictionary</b> as well as the number of <b>dictionary</b> columns that <b>can</b> be used ...", "dateLastCrawled": "2021-12-25T22:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) Greedy <b>Dictionary Selection for Sparse Representation</b>", "url": "https://www.researchgate.net/publication/220373119_Greedy_Dictionary_Selection_for_Sparse_Representation", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/220373119_Greedy_<b>Dictionary</b>_Selection_for...", "snippet": "By <b>sparse</b> representa- tion, we mean that only a few <b>dictionary</b> elements, <b>compared</b> to the ambient signal dimension, <b>can</b> be used to well-approximate the signals. We formulate both the selection of ...", "dateLastCrawled": "2021-10-18T03:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A novel <b>dictionary learning method for sparse representation with</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0925231220312157", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0925231220312157", "snippet": "In this work, we used the nonconvex GMC as the <b>sparse</b> constraint to formulate a <b>dictionary</b> learning problem, that is, learning a <b>dictionary</b> from the sampled signals to a <b>sparse</b> <b>representation</b> of signals . GMC <b>sparse</b> regularization <b>can</b> maintain the convexity of the cost function, thereby avoiding the presence of spurious local minima in the cost function. In this work, we consider using the forward\u2013backward splitting (FBS) algorithm to address the <b>sparse</b> coding", "dateLastCrawled": "2022-01-17T04:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Image restoration <b>based on sparse representation</b> using feature ...", "url": "https://jivp-eurasipjournals.springeropen.com/articles/10.1186/s13640-020-00531-5", "isFamilyFriendly": true, "displayUrl": "https://jivp-eurasipjournals.springeropen.com/articles/10.1186/s13640-020-00531-5", "snippet": "The method <b>based on sparse representation</b> calculates the <b>sparse</b> coding of the damaged patch on the over-complete <b>dictionary</b>, and then reconstruct the damaged patch using the <b>sparse</b> coding and the over-complete <b>dictionary</b>, while the method based on deep learning uses a large number of real images to train the generative model and the discriminative model, so that the deep network <b>can</b> learn the feature distribution of the real images, and then uses the generative model to automatically ...", "dateLastCrawled": "2022-01-27T18:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Sparse</b> <b>Representation</b> of Electrodermal Activity With Knowledge-Driven ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4362752/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC4362752", "snippet": "<b>Sparse</b> <b>representation</b> techniques model a signal as a linear combination of a small number of atoms chosen from an overcomplete <b>dictionary</b> aiming to reveal certain structures of a signal and represent them in a compact way . Since psychophysiological signals, such as EDA, show typical patterns over time, their <b>sparse</b> decomposition <b>can</b> yield accurate representations of scientific and translational value and contribute to scalable implementations (e.g., on mobile devices). Noting that the ...", "dateLastCrawled": "2017-01-14T08:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Sparse</b> <b>representation</b> based computed tomography images reconstruction ...", "url": "https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/iet-ipr.2019.1312", "isFamilyFriendly": true, "displayUrl": "https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/iet-ipr.2019.1312", "snippet": "<b>Sparse</b> <b>representation</b> of LR and HR image patches corresponding to each other are considered as pairs, for better <b>representation</b> of <b>sparse</b> atoms, coupled-KSVD is employed for training the dictionaries. Similar to the training phase, in the SR reconstruction phase, bicubic interpolation is applied to LR images to amplify the dimensions of the corresponding HR images. Extract the patches and vectorise each patch to reconstruct the HR image. <b>Sparse</b> <b>representation</b> coefficients for extracted ...", "dateLastCrawled": "2022-01-21T14:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Sparse</b> Matrix and its representations | Set 1 (Using Arrays and Linked ...", "url": "https://www.geeksforgeeks.org/sparse-matrix-representation/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/<b>sparse</b>-matrix-<b>representation</b>", "snippet": "<b>Sparse</b> Matrix Representations <b>can</b> be done in many ways following are two common representations: Array <b>representation</b>. Linked list <b>representation</b>. Method 1: Using Arrays: 2D array is used to represent a <b>sparse</b> matrix in which there are three rows named as. Row: Index of row, where non-zero element is located.", "dateLastCrawled": "2022-01-31T00:07:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine Learning by Analogy</b> - SlideShare", "url": "https://www.slideshare.net/ColleenFarrelly/machine-learning-by-analogy-59094152", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ColleenFarrelly/<b>machine-learning-by-analogy</b>-59094152", "snippet": "<b>Machine Learning by Analogy</b> 1. Colleen M. Farrelly 2. Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics behind these ...", "dateLastCrawled": "2022-01-31T07:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Neural Networks: Analogies. When our brains form analogies, they\u2026 | by ...", "url": "https://towardsdatascience.com/neural-networks-analogies-7ebeb3ac5d5e", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/neural-networks-analogies-7ebeb3ac5d5e", "snippet": "I\u2019ll outline a potential route to artificial neural networks which exhibit transfer <b>learning</b>: First, <b>Sparse</b> Distributed Representations. Numenta\u2019s Hierarchical Te m poral Memory, along with other techniques, relies upon a <b>sparse</b> distributed <b>representation</b>. An example of this is a very long string of ones and zeroes, where almost all the values are zero \u2014 there is a <b>sparse</b> distribution of the ones. If each digit represented a different thing, like \u2018pointy ears\u2019, \u2018tail ...", "dateLastCrawled": "2022-01-28T03:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "TensorFlow is an interface for expressing <b>machine</b> <b>learning</b> algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PPT) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> | Colleen Farrelly - Academia.edu", "url": "https://www.academia.edu/30404581/Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/30404581/<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "<b>Machine</b> <b>Learning</b> by <b>Analogy</b> Colleen M. Farrelly Overview of Problem Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics ...", "dateLastCrawled": "2022-01-02T06:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine</b> <b>Learning</b> by <b>Analogy</b> II - slideshare.net", "url": "https://www.slideshare.net/ColleenFarrelly/machine-learning-by-analogy-ii", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ColleenFarrelly/<b>machine</b>-<b>learning</b>-by-<b>analogy</b>-ii", "snippet": "Updated <b>Machine</b> <b>Learning</b> by <b>Analogy</b> presentation that builds to more advanced methods (TensorFlow, geometry/topology-based methods...) and adds a section on ti\u2026", "dateLastCrawled": "2021-12-31T12:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Conceptualization as a Basis for Cognition \u2014 Human and <b>Machine</b> | by ...", "url": "https://towardsdatascience.com/conceptualization-as-a-basis-for-cognition-human-and-machine-345d9e687e3c", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/conceptualization-as-a-basis-for-cognition-human-and...", "snippet": "Abstraction and <b>analogy</b> allow concepts to be re-applied in new domains. There are many, often conflicting, ... <b>Machine</b>-<b>learning</b> systems must learn to conceptualize to reach the goal of creating machines with higher intelligence. To substantiate this claim, let\u2019s first examine what generalization in artificial intelligence means specifically in the context of artificial intelligence/<b>machine</b> <b>learning</b> (as opposed to the layman\u2019s use of the term), and then explore how that differs from ...", "dateLastCrawled": "2022-01-20T17:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Compressed Sensing Meets <b>Machine</b> <b>Learning</b>: Classification via <b>Sparse</b> ...", "url": "https://nuit-blanche.blogspot.com/2008/05/cs-mini-course-classification-via.html", "isFamilyFriendly": true, "displayUrl": "https://nuit-blanche.blogspot.com/2008/05/cs-mini-course-classification-via.html", "snippet": "Compressed Sensing Meets <b>Machine</b> <b>Learning</b>: Classification via <b>Sparse</b> <b>Representation</b> and Distributed Pattern Recognition This Spring, Allen Yang has given a mini course at Berkeley entitled Compressed Sensing Meets <b>Machine</b> <b>Learning</b>. The three lectures are listed here (it includes accompanying code): lecture 1: Classification via <b>Sparse</b> <b>Representation</b>; lecture 2: Classification of Mixture Subspace Models via <b>Sparse</b> <b>Representation</b>, lecture 3: Distributed Pattern Recognition; The third lecture ...", "dateLastCrawled": "2022-01-25T10:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Word Embedding: Syntactics or Semantics</b> \u00b7 Shengbin&#39;s Studio", "url": "https://wushbin.github.io/2017/10/09/Word-Embedding-Syntactics-or-Semantics/", "isFamilyFriendly": true, "displayUrl": "https://wushbin.github.io/2017/10/09/<b>Word-Embedding-Syntactics-or-Semantics</b>", "snippet": "<b>Sparse</b> Vector <b>Representation</b>. The co-occurrence matrix in represented each cell by the raw frequency of the co-occurrence of two words. The raw frequency in a matrix may be skewed. Pointwise mutual information PPMI is a good measure for association between words which can tell us how much often the two words occur. The pointwise mutual information is a measure of how often two events x and y occur, compared with what we would expect if they were independent: PMI between two words is ...", "dateLastCrawled": "2022-01-09T18:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>What Are Word Embeddings</b> for Text? - <b>Machine</b> <b>Learning</b> Mastery", "url": "https://machinelearningmastery.com/what-are-word-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>what-are-word-embeddings</b>", "snippet": "Word embeddings are a type of word <b>representation</b> that allows words with similar meaning to have a similar <b>representation</b>. They are a distributed <b>representation</b> for text that is perhaps one of the key breakthroughs for the impressive performance of deep <b>learning</b> methods on challenging natural language processing problems. In this post, you will discover the word embedding approach for representing text data. After", "dateLastCrawled": "2022-01-30T18:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Word Embeddings in NLP - GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/word-embeddings-in-nlp/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/word-embeddings-in-nlp", "snippet": "Word Embeddings are a method of extracting features out of text so that we can input those features into a <b>machine</b> <b>learning</b> model to work with text data. They try to preserve syntactical and semantic information. The methods such as Bag of Words(BOW), CountVectorizer and TFIDF rely on the word count in a sentence but do not save any syntactical or semantic information. In these algorithms, the size of the vector is the number of elements in the vocabulary. We can get a <b>sparse</b> matrix if most ...", "dateLastCrawled": "2022-01-30T08:38:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Sparse Adaptive Local Machine Learning</b> Algorithms for Sensing and Analytics", "url": "https://pdxscholar.library.pdx.edu/cgi/viewcontent.cgi?article=1000&context=mcecs_mentoring", "isFamilyFriendly": true, "displayUrl": "https://pdxscholar.library.pdx.edu/cgi/viewcontent.cgi?article=1000&amp;context=mcecs...", "snippet": "Fig. 2: A <b>sparse representation can be thought of as</b> the dot product of a dictionary vector and a sparse code vector. Given a . dictionary . of general components, we can use a . sparse code. to select as few of them as possible to reconstruct an image of interest (Fig. 2). This reconstruction is called a . sparse representation. Sparse Coding. Image processing is expensive. Instead of working with the original image, we can identify its most relevant components and discard the rest. This ...", "dateLastCrawled": "2021-08-31T12:20:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(sparse representation)  is like +(dictionary)", "+(sparse representation) is similar to +(dictionary)", "+(sparse representation) can be thought of as +(dictionary)", "+(sparse representation) can be compared to +(dictionary)", "machine learning +(sparse representation AND analogy)", "machine learning +(\"sparse representation is like\")", "machine learning +(\"sparse representation is similar\")", "machine learning +(\"just as sparse representation\")", "machine learning +(\"sparse representation can be thought of as\")", "machine learning +(\"sparse representation can be compared to\")"]}