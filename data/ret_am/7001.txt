{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "least squares - Is minimizing <b>squared</b> <b>error</b> equivalent to minimizing ...", "url": "https://stats.stackexchange.com/questions/147001/is-minimizing-squared-error-equivalent-to-minimizing-absolute-error-why-squared", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/<b>question</b>s/147001", "snippet": "<b>Twice</b> as far from the mean would therefore result in <b>twice</b> the penalty. The more common approach is to consider a <b>squared</b> proportional relationship between deviations from the mean and the corresponding penalty. This will make sure that the further you are away from the mean, the proportionally more you will be <b>penalized</b>.", "dateLastCrawled": "2022-02-02T20:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Hands-On <b>Convolutional Neural Networks with TensorFlow</b>: Solve computer ...", "url": "https://dokumen.pub/hands-on-convolutional-neural-networks-with-tensorflow-solve-computer-vision-problems-with-modeling-in-tensorflow-and-python-9781789132823-1789132827.html", "isFamilyFriendly": true, "displayUrl": "https://dokumen.pub/hands-on-<b>convolutional-neural-networks-with-tensorflow</b>-solve...", "snippet": "For this model, we will use the <b>hinge</b> <b>loss</b>. <b>Hinge</b> <b>loss</b> is the <b>loss</b> function used when creating a support vector machine (SVM). <b>Hinge</b> <b>loss</b> heavily punishes incorrect predictions. For one given example, , where and is its label, the <b>hinge</b> <b>loss</b> for it will be as follows: is a feature vector of a datapoint To this, the following will apply: In simple words, this equation takes the raw output of the classifier. In our model, that&#39;s three output scores, and ensures that the score of the target ...", "dateLastCrawled": "2022-01-24T16:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "No, this post is not 30 <b>days early: Psychological Science backs away</b> ...", "url": "https://statmodeling.stat.columbia.edu/2016/03/02/no-this-post-is-not-30-days-early-psychological-science-backs-away-from-null-hypothesis-significance-testing/", "isFamilyFriendly": true, "displayUrl": "https://statmodeling.stat.columbia.edu/2016/03/02/no-this-post-is-not-30-days-early...", "snippet": "This upsets some people\u2014they don\u2019t <b>like</b> to be <b>penalized</b>, as it were, for analyses they didn\u2019t do\u2014but, sorry, that\u2019s the logic of p-values. As Eric and I explain in our paper, the p-value is necessarily defined based on what you would\u2019ve done. If you don\u2019t want outsiders speculating on what you would\u2019ve done, had the data been different, you can preregister or you can use other statistical methods. If you want to play the p-value game, you gotta play by the rules. Anyway, I th", "dateLastCrawled": "2022-01-25T10:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "pdf.pdf", "url": "https://www.slideshare.net/zanaveen1/pdfpdf-76753543", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/zanaveen1/pdfpdf-76753543", "snippet": "viii 0 Contents 7.1.1 A Regularized Risk Minimization Viewpoint 170 7.1.2 An Exponential Family Interpretation 170 7.1.3 Specialized Algorithms for Training SVMs 172 7.2 Extensions 177 7.2.1 The \u03bd trick 177 7.2.2 <b>Squared</b> <b>Hinge</b> <b>Loss</b> 179 7.2.3 Ramp <b>Loss</b> 180 7.3 Support Vector Regression 181 7.3.1 Incorporating General <b>Loss</b> Functions 184 7.3.2 Incorporating the \u03bd Trick 186 7.4 Novelty Detection 186 7.5 Margins and Probability 189 7.6 Beyond Binary Classi\ufb01cation 189 7.6.1 Multiclass ...", "dateLastCrawled": "2021-12-22T14:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) Hands-<b>On Machine Learning with Scikit-Learn &amp; TensorFlow</b> | sonia ...", "url": "https://www.academia.edu/42041768/Hands_On_Machine_Learning_with_Scikit_Learn_and_TensorFlow", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/42041768/Hands_<b>On_Machine_Learning_with_Scikit_Learn</b>_and...", "snippet": "Academia.edu is a platform for academics to share research papers.", "dateLastCrawled": "2022-01-28T06:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Robust Regression</b> \u00b7 Issue #141 \u00b7 scikit-learn-contrib/py-earth - <b>GitHub</b>", "url": "https://github.com/scikit-learn-contrib/py-earth/issues/141", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/scikit-learn-contrib/py-earth/issues/141", "snippet": "I <b>like</b> this idea. I&#39;ve thought a lot about ways to use MARS with different <b>loss</b> functions. The conclusion I&#39;ve come too is that gradient boosting is the way to do it, similarly to scikit-learn&#39;s GradientBoostingRegressor but with MARS basis functions instead of trees. It&#39;s on my road map but it&#39;s a lot of work to implement, so probably won&#39;t happen for some time.", "dateLastCrawled": "2021-08-26T22:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "50 Data Scientist Interview Questions (ANSWERED with PDF) To Crack Next ...", "url": "https://www.mlstack.cafe/blog/data-scientist-interview-questions", "isFamilyFriendly": true, "displayUrl": "https://www.mlstack.cafe/blog/data-scientist-interview-<b>questions</b>", "snippet": "Decision trees is a tool that uses a tree-<b>like</b> model of decisions and their possible consequences. If an algorithm only contains conditional control statements, decision trees can model that algorithm really well.; Decision trees are a non-parametric, supervised learning method.; Decision trees are used for classification and regression tasks.; The diagram below shows an example of a decision tree (the dataset used is the Titanic dataset to predict whether a passenger survived or not):", "dateLastCrawled": "2022-02-03T06:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Short <b>Question</b> | Short <b>Question</b> Online Test - Avatto", "url": "https://avatto.com/interview-questions/short-question/", "isFamilyFriendly": true, "displayUrl": "https://avatto.com/interview-<b>questions</b>/short-<b>question</b>", "snippet": "b. Generate research <b>question</b> from data. c. Identify variables present in data. Also, identify important variables or variables to be analyzed as such. d. Generate hypothesis. e. Analyze data using graph data <b>like</b> histogram for example. f. Fit a model from analyzed data. g. Accept or reject the hypothesis. h. Research <b>question</b> answer found.", "dateLastCrawled": "2022-01-26T21:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Loss</b> Function For Imbalanced Classification Keras", "url": "https://apindustria.padova.it/Loss_Function_For_Imbalanced_Classification_Keras.html", "isFamilyFriendly": true, "displayUrl": "https://apindustria.padova.it/<b>Loss</b>_Function_For_Imbalanced_Classification_Keras.html", "snippet": "use <b>hinge</b> <b>loss</b> (or a related function, such as <b>squared</b> <b>hinge</b> <b>loss</b>). By misclassifying the minority class, a higher <b>loss</b> is incurred by the model since the minority class has a higher weight. For the <b>loss</b> function, I chose binary crossentropy. For example. It is responsible for calculating <b>loss</b> and predictions. The focal <b>loss</b> is designed to address class imbalance by down-weighting inliers (easy examples) such that their contribution to the total <b>loss</b> is small even if their number is large ...", "dateLastCrawled": "2022-02-02T00:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Unlearning descriptive statistics | Hacker News", "url": "https://news.ycombinator.com/item?id=13541589", "isFamilyFriendly": true, "displayUrl": "https://news.ycombinator.com/item?id=13541589", "snippet": "If you stack a bunch of shitty <b>penalized</b> regressions (which is what linear/logistic + relU <b>hinge</b> <b>loss</b> represents) you now have one gigantic shitty regression which is harder to debug. If your early steps are thrown out of whack by outliers, your later steps will be too. Dropout is an attempt to remedy this, but you tend to lose power when you shrink your dataset or model, so (per usual) there really is no such thing as a free lunch. But most of the tradeoffs make more sense when you are able ...", "dateLastCrawled": "2021-08-17T20:37:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A review on instance ranking problems in statistical learning ...", "url": "https://link.springer.com/article/10.1007/s10994-021-06122-3", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10994-021-06122-3", "snippet": "A <b>similar</b> argumentation has been made in Werner in the context of risk -based ... (l_1\\)-regularization term and use the <b>squared</b> <b>Hinge</b> <b>loss</b>. They solve the problem by invoking Fenchel duality (hence the name FenchelRank) and prove convergence of the solution. After experiments on real data sets for document retrieval, they conclude sparsity of the solutions as well as superiority of FenchelRank to non-sparse algorithms. They implement their method in MATLAB. An iterative gradient procedure ...", "dateLastCrawled": "2021-12-27T02:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "pdf.pdf", "url": "https://www.slideshare.net/zanaveen1/pdfpdf-76753543", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/zanaveen1/pdfpdf-76753543", "snippet": "viii 0 Contents 7.1.1 A Regularized Risk Minimization Viewpoint 170 7.1.2 An Exponential Family Interpretation 170 7.1.3 Specialized Algorithms for Training SVMs 172 7.2 Extensions 177 7.2.1 The \u03bd trick 177 7.2.2 <b>Squared</b> <b>Hinge</b> <b>Loss</b> 179 7.2.3 Ramp <b>Loss</b> 180 7.3 Support Vector Regression 181 7.3.1 Incorporating General <b>Loss</b> Functions 184 7.3.2 Incorporating the \u03bd Trick 186 7.4 Novelty Detection 186 7.5 Margins and Probability 189 7.6 Beyond Binary Classi\ufb01cation 189 7.6.1 Multiclass ...", "dateLastCrawled": "2021-12-22T14:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "50 Data Scientist Interview Questions (ANSWERED with PDF) To Crack Next ...", "url": "https://www.mlstack.cafe/blog/data-scientist-interview-questions", "isFamilyFriendly": true, "displayUrl": "https://www.mlstack.cafe/blog/data-scientist-interview-<b>questions</b>", "snippet": "Companies need data scientists. They need people who are able to take large amounts of data and make it usable. The national average salary for a Data Scientist in the United States is $117,212. Data Scientist roles in Australia were typically advertised between $110k and $140k in the last 3 months. Follow along and learn the 50 most common and advanced Data Scientist Interview Questions and Answers (PDF download ready) you must know before your next Machine Learning and Data Science interview.", "dateLastCrawled": "2022-02-03T06:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A review on ranking problems in statistical learning - arxiv-vanity.com", "url": "https://www.arxiv-vanity.com/papers/1909.02998/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1909.02998", "snippet": "Ranking problems define a widely spread class of statistical learning problems with many applications, including fraud detection, document ranking, medicine, credit risk screening, image ranking or media memorability. In this article, we systematically describe different types of non-probabilistic supervised ranking problems, i.e., ranking problems that require the prediction of an order of the response variables, and the corresponding <b>loss</b> functions resp. goodness criteria. We discuss the ...", "dateLastCrawled": "2021-09-29T14:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Robust Regression</b> \u00b7 Issue #141 \u00b7 scikit-learn-contrib/py-earth - <b>GitHub</b>", "url": "https://github.com/scikit-learn-contrib/py-earth/issues/141", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/scikit-learn-contrib/py-earth/issues/141", "snippet": "I like this idea. I&#39;ve thought a lot about ways to use MARS with different <b>loss</b> functions. The conclusion I&#39;ve come too is that gradient boosting is the way to do it, similarly to scikit-learn&#39;s GradientBoostingRegressor but with MARS basis functions instead of trees. It&#39;s on my road map but it&#39;s a lot of work to implement, so probably won&#39;t happen for some time.", "dateLastCrawled": "2021-08-26T22:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) <b>Basic Ship Theory</b> | Sdjsk Mdjksjn - Academia.edu", "url": "https://www.academia.edu/6983209/Basic_Ship_Theory", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/6983209/<b>Basic_Ship_Theory</b>", "snippet": "Academia.edu is a platform for academics to share research papers.", "dateLastCrawled": "2022-02-01T02:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) In Defense of the Triplet <b>Loss for Person Re-Identification</b>", "url": "https://www.researchgate.net/publication/315514719_In_Defense_of_the_Triplet_Loss_for_Person_Re-Identification", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/315514719_In_Defense_of_the_Triplet_<b>Loss</b>_for...", "snippet": "With this. realization, we propose an organizational modi\ufb01cation to. the classic way of using the triplet <b>loss</b>: the core idea is to. form batches by randomly sampling P classes (person iden ...", "dateLastCrawled": "2022-01-23T20:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Python Machine Learning: Machine Learning and Deep Learning with Python ...", "url": "https://ebin.pub/python-machine-learning-machine-learning-and-deep-learning-with-python-scikit-learn-and-tensorflow-2-3rd-edition-3nbsped-9781789955750-1789955750.html", "isFamilyFriendly": true, "displayUrl": "https://ebin.pub/python-machine-learning-machine-learning-and-deep-learning-with...", "snippet": "In some literature, the term &quot;<b>loss</b>&quot; refers to the <b>loss</b> measured for a single data point, and the cost is a measurement that computes the <b>loss</b> (average or summed) over the entire dataset. A roadmap for building machine learning systems In previous sections, we discussed the basic concepts of machine learning and the three different types of learning.", "dateLastCrawled": "2022-01-31T17:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Loss</b> Function For Imbalanced Classification Keras", "url": "https://apindustria.padova.it/Loss_Function_For_Imbalanced_Classification_Keras.html", "isFamilyFriendly": true, "displayUrl": "https://apindustria.padova.it/<b>Loss</b>_Function_For_Imbalanced_Classification_Keras.html", "snippet": "use <b>hinge</b> <b>loss</b> (or a related function, such as <b>squared</b> <b>hinge</b> <b>loss</b>). By misclassifying the minority class, a higher <b>loss</b> is incurred by the model since the minority class has a higher weight. For the <b>loss</b> function, I chose binary crossentropy. For example. It is responsible for calculating <b>loss</b> and predictions. The focal <b>loss</b> is designed to address class imbalance by down-weighting inliers (easy examples) such that their contribution to the total <b>loss</b> is small even if their number is large ...", "dateLastCrawled": "2022-02-02T00:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": ". The <b>formula for a regression equation</b> is Y\u2019 = 2X + 9. \u2013 versedwriters", "url": "https://versedwriters.com/the-formula-for-a-regression-equation-is-y-2x-9/", "isFamilyFriendly": true, "displayUrl": "https://versedwriters.com/the-<b>formula-for-a-regression-equation</b>-is-y-2x-9", "snippet": "if you choose an M&amp;M at random, the probability of <b>getting</b>, say, a brown M&amp;M is equal to the proportion of M&amp;M\u2019s that are brown (0.30). 41 Color Frequency Brown 17 Red 18 Yellow 7 Green 7 Blue 2 Orange 4 Color Proportion Brown 0.30 Red 0.20 Yellow 0.20 Green 0.10 Blue 0.10 Orange 0.10 0 0.10 0.20 0.30 Brown Red Yellow Green Blue Orange ...", "dateLastCrawled": "2022-02-03T00:48:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "pdf.pdf", "url": "https://www.slideshare.net/zanaveen1/pdfpdf-76753543", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/zanaveen1/pdfpdf-76753543", "snippet": "viii 0 Contents 7.1.1 A Regularized Risk Minimization Viewpoint 170 7.1.2 An Exponential Family Interpretation 170 7.1.3 Specialized Algorithms for Training SVMs 172 7.2 Extensions 177 7.2.1 The \u03bd trick 177 7.2.2 <b>Squared</b> <b>Hinge</b> <b>Loss</b> 179 7.2.3 Ramp <b>Loss</b> 180 7.3 Support Vector Regression 181 7.3.1 Incorporating General <b>Loss</b> Functions 184 7.3.2 Incorporating the \u03bd Trick 186 7.4 Novelty Detection 186 7.5 Margins and Probability 189 7.6 Beyond Binary Classi\ufb01cation 189 7.6.1 Multiclass ...", "dateLastCrawled": "2021-12-22T14:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Understanding Machine Learning: From Theory</b> to Algorithms | Keep ...", "url": "https://www.academia.edu/40679311/Understanding_Machine_Learning_From_Theory_to_Algorithms", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/40679311/<b>Understanding_Machine_Learning_From_Theory</b>_to_Algorithms", "snippet": "Academia.edu is a platform for academics to share research papers.", "dateLastCrawled": "2022-01-23T01:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A review on ranking problems in statistical learning - arxiv-vanity.com", "url": "https://www.arxiv-vanity.com/papers/1909.02998/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1909.02998", "snippet": "Ranking problems define a widely spread class of statistical learning problems with many applications, including fraud detection, document ranking, medicine, credit risk screening, image ranking or media memorability. In this article, we systematically describe different types of non-probabilistic supervised ranking problems, i.e., ranking problems that require the prediction of an order of the response variables, and the corresponding <b>loss</b> functions resp. goodness criteria. We discuss the ...", "dateLastCrawled": "2021-09-29T14:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "least squares - Is minimizing <b>squared</b> <b>error</b> equivalent to minimizing ...", "url": "https://stats.stackexchange.com/questions/147001/is-minimizing-squared-error-equivalent-to-minimizing-absolute-error-why-squared", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/<b>question</b>s/147001", "snippet": "Anybody <b>can</b> ask <b>a question</b> Anybody <b>can</b> answer The best answers are voted up and rise to the top Home ... <b>Twice</b> as far from the mean would therefore result in <b>twice</b> the penalty. The more common approach is to consider a <b>squared</b> proportional relationship between deviations from the mean and the corresponding penalty. This will make sure that the further you are away from the mean, the proportionally more you will be <b>penalized</b>. Using this penalty function, outliers (far away from the mean) are ...", "dateLastCrawled": "2022-02-02T20:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Hands-On <b>Convolutional Neural Networks with TensorFlow</b>: Solve computer ...", "url": "https://dokumen.pub/hands-on-convolutional-neural-networks-with-tensorflow-solve-computer-vision-problems-with-modeling-in-tensorflow-and-python-9781789132823-1789132827.html", "isFamilyFriendly": true, "displayUrl": "https://dokumen.pub/hands-on-<b>convolutional-neural-networks-with-tensorflow</b>-solve...", "snippet": "Losses <b>can</b> be found in the tf.losses module. For this model, we will use the <b>hinge</b> <b>loss</b>. <b>Hinge</b> <b>loss</b> is the <b>loss</b> function used when creating a support vector machine (SVM). <b>Hinge</b> <b>loss</b> heavily punishes incorrect predictions. For one given example, , where and is its label, the <b>hinge</b> <b>loss</b> for it will be as follows: is a feature vector of a datapoint To this, the following will apply: In simple words, this equation takes the raw output of the classifier. In our model, that&#39;s three output scores ...", "dateLastCrawled": "2022-01-24T16:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Robust Regression</b> \u00b7 Issue #141 \u00b7 scikit-learn-contrib/py-earth - <b>GitHub</b>", "url": "https://github.com/scikit-learn-contrib/py-earth/issues/141", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/scikit-learn-contrib/py-earth/issues/141", "snippet": "I like this idea. I&#39;ve <b>thought</b> a lot about ways to use MARS with different <b>loss</b> functions. The conclusion I&#39;ve come too is that gradient boosting is the way to do it, similarly to scikit-learn&#39;s GradientBoostingRegressor but with MARS basis functions instead of trees. It&#39;s on my road map but it&#39;s a lot of work to implement, so probably won&#39;t happen for some time.", "dateLastCrawled": "2021-08-26T22:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "No, this post is not 30 <b>days early: Psychological Science backs away</b> ...", "url": "https://statmodeling.stat.columbia.edu/2016/03/02/no-this-post-is-not-30-days-early-psychological-science-backs-away-from-null-hypothesis-significance-testing/", "isFamilyFriendly": true, "displayUrl": "https://statmodeling.stat.columbia.edu/2016/03/02/no-this-post-is-not-30-days-early...", "snippet": "This upsets some people\u2014they don\u2019t like to be <b>penalized</b>, as it were, for analyses they didn\u2019t do\u2014but, sorry, that\u2019s the logic of p-values. As Eric and I explain in our paper, the p-value is necessarily defined based on what you would\u2019ve done. If you don\u2019t want outsiders speculating on what you would\u2019ve done, had the data been different, you <b>can</b> preregister or you <b>can</b> use other statistical methods. If you want to play the p-value game, you gotta play by the rules. Anyway, I th", "dateLastCrawled": "2022-01-25T10:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Short <b>Question</b> | Short <b>Question</b> Online Test - Avatto", "url": "https://avatto.com/interview-questions/short-question/", "isFamilyFriendly": true, "displayUrl": "https://avatto.com/interview-<b>questions</b>/short-<b>question</b>", "snippet": "A data frame <b>can</b> contain vectors with different inputs and a matrix cannot. (You <b>can</b> have a data frame of characters, integers, and even other data frames, but you <b>can</b>&#39;t do that with a matrix. A matrix must be all the same type.) So, the data frame <b>can</b> have a different vector of character, numbers, logic, etc. and it is still cool. But, for a ...", "dateLastCrawled": "2022-01-26T21:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Unlearning descriptive statistics | Hacker News", "url": "https://news.ycombinator.com/item?id=13541589", "isFamilyFriendly": true, "displayUrl": "https://news.ycombinator.com/item?id=13541589", "snippet": "If you look at a linear fit plus a relU, you&#39;re just tacking a <b>hinge</b> <b>loss</b> onto a linear/logistic fit. Stack a bunch of these on top of each other and you have a universal function approximator, for which the goodness of fit is limited by the data. If you don&#39;t have a crapton of data, the fit isn&#39;t likely to be a lot better than linear or transformed linear. If you do have a crapton of data with nonlinear relationships, the implicit structure <b>can</b> be better captured by the flexibility of an NN ...", "dateLastCrawled": "2021-08-17T20:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": ". The <b>formula for a regression equation</b> is Y\u2019 = 2X + 9. \u2013 versedwriters", "url": "https://versedwriters.com/the-formula-for-a-regression-equation-is-y-2x-9/", "isFamilyFriendly": true, "displayUrl": "https://versedwriters.com/the-<b>formula-for-a-regression-equation</b>-is-y-2x-9", "snippet": "Here we <b>can</b> say the base of 10 is raised to the second power. Here is an example of a log: Log10(100) = 2 This <b>can</b> be read as: The log base ten of 100 equals 2. The result is the power that the base of 10 has to be raised to in order to equal the value (100). Similarly, Log10(1000) = 3 since 10 has to be raised to the third power in order to ...", "dateLastCrawled": "2022-02-03T00:48:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>CMA Part 2 Financial Decision Making</b> | Amit <b>Hinge</b> - Academia.edu", "url": "https://www.academia.edu/42219334/CMA_Part_2_Financial_Decision_Making", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/42219334/<b>CMA_Part_2_Financial_Decision_Making</b>", "snippet": "Amit <b>Hinge</b>. Saquib Nawaz. Fas Mo. Download Download PDF. Full PDF Package Download Full PDF Package. This Paper. A short summary of this paper. 1 Full PDF related to this paper. Read Paper. Download Download PDF. Download Full PDF Package. Translate PDF ...", "dateLastCrawled": "2022-01-24T04:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A review on instance ranking problems in statistical learning ...", "url": "https://link.springer.com/article/10.1007/s10994-021-06122-3", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10994-021-06122-3", "snippet": "We insist to once more take a look on the U-statistics that arise for the hard and the localized ranking problem. Cl\u00e9men\u00e7on et al. already mentioned that these pair-wise <b>loss</b> functions <b>can</b> be generalized to <b>loss</b> functions with m input arguments.This leads to U-statistics of order m.But if the whole permutations that represent the ordering of the response values should <b>be compared</b> at once (i.e., \\(m=n\\)), then this again boils down to a U-statistic of order 2.Let \\(\\pi\\), \\({\\hat{\\pi }} \\in ...", "dateLastCrawled": "2021-12-27T02:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "pdf.pdf", "url": "https://www.slideshare.net/zanaveen1/pdfpdf-76753543", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/zanaveen1/pdfpdf-76753543", "snippet": "viii 0 Contents 7.1.1 A Regularized Risk Minimization Viewpoint 170 7.1.2 An Exponential Family Interpretation 170 7.1.3 Specialized Algorithms for Training SVMs 172 7.2 Extensions 177 7.2.1 The \u03bd trick 177 7.2.2 <b>Squared</b> <b>Hinge</b> <b>Loss</b> 179 7.2.3 Ramp <b>Loss</b> 180 7.3 Support Vector Regression 181 7.3.1 Incorporating General <b>Loss</b> Functions 184 7.3.2 Incorporating the \u03bd Trick 186 7.4 Novelty Detection 186 7.5 Margins and Probability 189 7.6 Beyond Binary Classi\ufb01cation 189 7.6.1 Multiclass ...", "dateLastCrawled": "2021-12-22T14:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A review on ranking problems in statistical learning - arxiv-vanity.com", "url": "https://www.arxiv-vanity.com/papers/1909.02998/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1909.02998", "snippet": "Ranking problems define a widely spread class of statistical learning problems with many applications, including fraud detection, document ranking, medicine, credit risk screening, image ranking or media memorability. In this article, we systematically describe different types of non-probabilistic supervised ranking problems, i.e., ranking problems that require the prediction of an order of the response variables, and the corresponding <b>loss</b> functions resp. goodness criteria. We discuss the ...", "dateLastCrawled": "2021-09-29T14:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) In Defense of the Triplet <b>Loss for Person Re-Identification</b>", "url": "https://www.researchgate.net/publication/315514719_In_Defense_of_the_Triplet_Loss_for_Person_Re-Identification", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/315514719_In_Defense_of_the_Triplet_<b>Loss</b>_for...", "snippet": "With this. realization, we propose an organizational modi\ufb01cation to. the classic way of using the triplet <b>loss</b>: the core idea is to. form batches by randomly sampling P classes (person iden ...", "dateLastCrawled": "2022-01-23T20:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Hands-On <b>Convolutional Neural Networks with TensorFlow</b>: Solve computer ...", "url": "https://dokumen.pub/hands-on-convolutional-neural-networks-with-tensorflow-solve-computer-vision-problems-with-modeling-in-tensorflow-and-python-9781789132823-1789132827.html", "isFamilyFriendly": true, "displayUrl": "https://dokumen.pub/hands-on-<b>convolutional-neural-networks-with-tensorflow</b>-solve...", "snippet": "Losses <b>can</b> be found in the tf.losses module. For this model, we will use the <b>hinge</b> <b>loss</b>. <b>Hinge</b> <b>loss</b> is the <b>loss</b> function used when creating a support vector machine (SVM). <b>Hinge</b> <b>loss</b> heavily punishes incorrect predictions. For one given example, , where and is its label, the <b>hinge</b> <b>loss</b> for it will be as follows: is a feature vector of a datapoint To this, the following will apply: In simple words, this equation takes the raw output of the classifier. In our model, that&#39;s three output scores ...", "dateLastCrawled": "2022-01-24T16:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Online learning</b>: A comprehensive survey - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S0925231221006706", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0925231221006706", "snippet": "Based on the feedback, the learner <b>can</b> measure the <b>loss</b> suffered, depending on the difference between the prediction and the answer. Finally, the learner updates its prediction model by some strategy so as to improve predictive performance on future received instances. Consider spam email detection as a running example of online binary classification, where the learner answers every <b>question</b> in binary: yes or no. The task is supervised binary classification from a machine learning ...", "dateLastCrawled": "2022-01-27T04:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Short <b>Question</b> | Short <b>Question</b> Online Test - Avatto", "url": "https://avatto.com/interview-questions/short-question/", "isFamilyFriendly": true, "displayUrl": "https://avatto.com/interview-<b>questions</b>/short-<b>question</b>", "snippet": "A data frame <b>can</b> contain vectors with different inputs and a matrix cannot. (You <b>can</b> have a data frame of characters, integers, and even other data frames, but you <b>can</b>&#39;t do that with a matrix. A matrix must be all the same type.) So, the data frame <b>can</b> have a different vector of character, numbers, logic, etc. and it is still cool. But, for a ...", "dateLastCrawled": "2022-01-26T21:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": ". The <b>formula for a regression equation</b> is Y\u2019 = 2X + 9. \u2013 versedwriters", "url": "https://versedwriters.com/the-formula-for-a-regression-equation-is-y-2x-9/", "isFamilyFriendly": true, "displayUrl": "https://versedwriters.com/the-<b>formula-for-a-regression-equation</b>-is-y-2x-9", "snippet": "if you choose an M&amp;M at random, the probability of <b>getting</b>, say, a brown M&amp;M is equal to the proportion of M&amp;M\u2019s that are brown (0.30). 41 Color Frequency Brown 17 Red 18 Yellow 7 Green 7 Blue 2 Orange 4 Color Proportion Brown 0.30 Red 0.20 Yellow 0.20 Green 0.10 Blue 0.10 Orange 0.10 0 0.10 0.20 0.30 Brown Red Yellow Green Blue Orange ...", "dateLastCrawled": "2022-02-03T00:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Loss</b> Function For Imbalanced Classification Keras", "url": "https://apindustria.padova.it/Loss_Function_For_Imbalanced_Classification_Keras.html", "isFamilyFriendly": true, "displayUrl": "https://apindustria.padova.it/<b>Loss</b>_Function_For_Imbalanced_Classification_Keras.html", "snippet": "use <b>hinge</b> <b>loss</b> (or a related function, such as <b>squared</b> <b>hinge</b> <b>loss</b>). By misclassifying the minority class, a higher <b>loss</b> is incurred by the model since the minority class has a higher weight. For the <b>loss</b> function, I chose binary crossentropy. For example. It is responsible for calculating <b>loss</b> and predictions. The focal <b>loss</b> is designed to address class imbalance by down-weighting inliers (easy examples) such that their contribution to the total <b>loss</b> is small even if their number is large ...", "dateLastCrawled": "2022-02-02T00:40:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Introduction to Machine Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/24/introduction-to-machine-learning-2/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/24/<b>introduction-to-machine-learning</b>-2", "snippet": "<b>Squared</b> <b>loss</b> (for regression) <b>Hinge</b> <b>loss</b> (SVM) Logistic/log <b>loss</b> (logistic regression) Some <b>loss</b> functions are as follows: When to stop tuning <b>machine</b> <b>learning</b> models. When to stop tuning the hyperparameters in a <b>machine</b> <b>learning</b> model is a million-dollar question. This problem can be mostly solved by keeping tabs on training and testing errors. While increasing the complexity of a model, the following stages occur: Stage 1: Underfitting stage \u2013 high train and high test errors (or low ...", "dateLastCrawled": "2022-01-17T05:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Data Science: <b>Support Vector Machines (SVM</b>)", "url": "https://www.datasciencesmachinelearning.com/2019/01/support-vector-machines-svm.html", "isFamilyFriendly": true, "displayUrl": "https://www.datasciences<b>machinelearning</b>.com/2019/01/<b>support-vector-machines-svm</b>.html", "snippet": "In this case, <b>squared</b> <b>hinge</b> <b>loss</b> function (as against <b>hinge</b> <b>loss</b> function) and l2 penalty are the major changes compared to the earlier three methods. This method is useful for when sample size is larger.", "dateLastCrawled": "2022-01-28T08:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Introduction to <b>Machine</b> <b>Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/30/introduction-to-machine-learning-3/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/30/introduction-to-<b>machine</b>-<b>learning</b>-3", "snippet": "However, in <b>machine</b> <b>learning</b> methodology, <b>squared</b> <b>loss</b> will be minimized with respect to ... <b>Squared</b> <b>loss</b> (for regression) <b>Hinge</b> <b>loss</b> (SVM) Logistic/log <b>loss</b> (logistic regression) Some <b>loss</b> functions are as follows: When to stop tuning <b>machine</b> <b>learning</b> models. When to stop tuning the hyperparameters in a <b>machine</b> <b>learning</b> model is a million-dollar question. This problem can be mostly solved by keeping tabs on training and testing errors. While increasing the complexity of a model, the ...", "dateLastCrawled": "2022-01-28T18:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A study on L2-<b>loss (Squared Hinge-Loss) multiclass SVM</b> | Request PDF", "url": "https://www.researchgate.net/publication/235884495_A_study_on_L2-loss_Squared_Hinge-Loss_multiclass_SVM", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/235884495_A_study_on_L2-<b>loss</b>_<b>Squared</b>_<b>Hinge</b>...", "snippet": "Taking the <b>analogy</b> to classification task, it has been previously studied [13] that using the <b>squared</b> <b>hinge</b> <b>loss</b> in SVM would yield better accuracy when \u03bb is large. In this case, underfitting ...", "dateLastCrawled": "2021-12-14T01:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Metrics to Evaluate Classification and Regression Algorithms | by ...", "url": "https://medium.com/@poojitha.penmethsa/metrics-to-evaluate-classification-and-regression-algorithms-1554f1e00a75", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@poojitha.penmethsa/metrics-to-evaluate-classification-and...", "snippet": "<b>Hinge</b> <b>loss</b>. cross-entropy <b>loss</b> / log <b>loss</b>. likelihood <b>loss</b>. MSE / Quadratic <b>loss</b> / L2 <b>loss</b>: Mean <b>Squared</b> Error, or MSE <b>loss</b> is the default <b>loss</b> to use for regression problems. Mathematically, it ...", "dateLastCrawled": "2022-01-17T01:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Understanding the 3 most common <b>loss</b> functions for <b>Machine</b> <b>Learning</b> ...", "url": "https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-the-3-most-common-<b>loss</b>-functions-for...", "snippet": "A <b>loss function</b> in <b>Machine</b> <b>Learning</b> is a measure of how accurately your ML model is able to predict the expected outcome i.e the ground truth. The <b>loss function</b> will take two items as input: the output value of our model and the ground truth expected value. The output of the <b>loss function</b> is called the <b>loss</b> which is a measure of how well our model did at predicting the outcome. A high value for the <b>loss</b> means our model performed very poorly. A low value for the <b>loss</b> means our model performed ...", "dateLastCrawled": "2022-02-02T13:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and-sample-codes ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/README.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "Models 1.1 Support vector <b>machine</b> 1.1.1 Principle 1.1.2 Kernel 1.1.3 Soft margin SVM 1.1.4 <b>Hinge</b> <b>loss</b> view 1.1.5 Multi-class SVM 1.1.6 Extensions 1.2 Tree-based models 1.2.1 Decision tree 1.2.2 Random forest 1.2.3 Gradient boosted decision trees 1.2.4 Tools 1.3 EM Principle 1.4 MaxEnt 1.4.1 Entropy 1.5 Model selection 1.5.1 Under-fitting / Over-fitting 1.5.2 Model ensemble, sklearn 2.", "dateLastCrawled": "2021-09-22T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Fast generalization rates for distance metric</b> <b>learning</b> - <b>Machine</b> <b>Learning</b>", "url": "https://link.springer.com/article/10.1007/s10994-018-5734-0", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10994-018-5734-0", "snippet": "For example, the <b>squared</b> <b>hinge</b> <b>loss</b> \\(\\ell _s^1(x) = \\max (1-x, 0) ... <b>Analogy</b>-preserving semantic embedding for visual object categorization. In Proceedings of the 30th international conference on <b>machine</b> <b>learning</b>, Atlanta, GA (pp. 639\u2013647). Jin, R., Wang, S., &amp; Zhou, Y. (2010). Regularized distance metric <b>learning</b>: Theory and algorithm. Advances in neural information processing systems (Vol. 23, pp. 862\u2013870). Cambridge, MA: MIT Press. Google Scholar Kulis, B. (2012). Metric <b>learning</b>: A ...", "dateLastCrawled": "2021-12-28T14:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>MLP for regression with TensorFlow 2 and</b> Keras \u2013 MachineCurve", "url": "https://www.machinecurve.com/index.php/2019/07/30/creating-an-mlp-for-regression-with-keras/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/index.php/2019/07/30/creating-an-mlp-for-regression-with...", "snippet": "Last Updated on 30 March 2021. <b>Machine</b> <b>learning</b> is a wide field and <b>machine</b> <b>learning</b> problems come in many flavors. If, say, you wish to group data based on similarities, you would choose an unsupervised approach called clustering.If you have a fixed number of classes which you wish to assign new data to, you\u2019ll choose a supervised approach named classification.If, however, you don\u2019t have a fixed number, but wish to estimate a real value \u2013 your approach will still be supervised, but ...", "dateLastCrawled": "2022-02-03T02:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "CPSC 340: Data Mining <b>Machine</b> <b>Learning</b>", "url": "https://www.cs.ubc.ca/~schmidtm/Courses/340-F16/L35.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.ubc.ca/~schmidtm/Courses/340-F16/L35.pdf", "snippet": "<b>Machine</b> <b>Learning</b> and Data Mining Course Review/Preview Fall 2016 Some images from this lecture are taken from Google Image Search. Admin \u2022Assignment 6: \u20131 late day to hand in next Monday, 2 for Wednesday, 3 for Friday. \u2022Final: \u2013December 12 (8:30am \u2013HEBB 100) \u2013Covers Assignments 1-6. \u2013List of topics posted. \u2013Final from last year will be posted after class. \u2013Closed-book, cheat sheet: 4-pages each double-sided. Last Time: Semi-Supervised <b>Learning</b> \u2022In semi-supervised <b>learning</b> ...", "dateLastCrawled": "2021-11-22T01:45:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>About loss and loss functions</b> \u2013 MachineCurve", "url": "https://www.machinecurve.com/index.php/2019/10/04/about-loss-and-loss-functions/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/index.php/2019/10/04/<b>about-loss-and-loss-functions</b>", "snippet": "We\u2019re then using <b>machine</b> <b>learning</b> for ... The <b>squared hinge loss is like</b> the hinge formula displayed above, but then the \\(max()\\) function output is squared. This helps achieving two things: Firstly, it makes the loss value more sensitive to outliers, just as we saw with MSE vs MAE. Large errors will add to the loss more significantly than smaller errors. Note that simiarly, this may also mean that you\u2019ll need to inspect your dataset for the presence of such outliers first. Secondly ...", "dateLastCrawled": "2022-01-25T13:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "\u635f\u5931\u51fd\u6570 - \u7b97\u6cd5\u6742\u8d27\u94fa - bjmsong.github.io", "url": "https://bjmsong.github.io/2020/02/21/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/", "isFamilyFriendly": true, "displayUrl": "https://bjmsong.github.io/2020/02/21/\u635f\u5931\u51fd\u6570", "snippet": "the training data is fed into the <b>machine</b> <b>learning</b> model; Loss : compare between some actual targets and predicted targets; the lower the loss, the more the set of targets and the set of predictions resemble each other; the more they resemble each other, the better the <b>machine</b> <b>learning</b> model performs. Backward pass", "dateLastCrawled": "2021-12-27T11:43:00.0000000Z", "language": "zh_chs", "isNavigational": false}], [], [], [], []], "all_bing_queries": ["+(squared hinge loss)  is like +(getting penalized twice for getting a question wrong)", "+(squared hinge loss) is similar to +(getting penalized twice for getting a question wrong)", "+(squared hinge loss) can be thought of as +(getting penalized twice for getting a question wrong)", "+(squared hinge loss) can be compared to +(getting penalized twice for getting a question wrong)", "machine learning +(squared hinge loss AND analogy)", "machine learning +(\"squared hinge loss is like\")", "machine learning +(\"squared hinge loss is similar\")", "machine learning +(\"just as squared hinge loss\")", "machine learning +(\"squared hinge loss can be thought of as\")", "machine learning +(\"squared hinge loss can be compared to\")"]}