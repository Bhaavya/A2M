{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) The Brownian motion in the transformer model", "url": "https://www.researchgate.net/publication/353208969_The_Brownian_motion_in_the_transformer_model", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/353208969_The_Brownian_motion_in_the...", "snippet": "Abstract. Transformer is the state of the art model for many language and visual tasks. In this paper, we give a deep analysis of its <b>multi-head</b> <b>self-attention</b> (MHSA) module and find that: 1) Each ...", "dateLastCrawled": "2022-01-12T05:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Sequence</b> models \u2014 ML Compiled", "url": "https://ml-compiled.readthedocs.io/en/latest/sequence.html", "isFamilyFriendly": true, "displayUrl": "https://ml-compiled.readthedocs.io/en/latest/<b>sequence</b>.html", "snippet": "<b>Multi-head</b> attention\u00b6 Concatenates the output of <b>multiple</b> parallel attention layers. Each layer has the <b>same</b> inputs (Q, K and V) but different weights. Vaswani et al. (2017) use 8 layers in each <b>multi-head</b> attention component but reduce the dimensionality of each from 512 to 64, which keeps the computational cost the <b>same</b> overall.", "dateLastCrawled": "2022-01-17T01:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "The <b>Illustrated Transformer</b> - Visualizing <b>machine</b> <b>learning</b> one concept ...", "url": "http://jalammar.github.io/illustrated-transformer/", "isFamilyFriendly": true, "displayUrl": "jalammar.github.io/<b>illustrated-transformer</b>", "snippet": "Discussions: Hacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments) Translations: Chinese (Simplified), French, Japanese, Korean, Russian, Spanish, Vietnamese Watch: MIT\u2019s Deep <b>Learning</b> State of the Art lecture referencing this post In the previous post, we looked at Attention \u2013 a ubiquitous method in modern deep <b>learning</b> models. Attention is a concept that helped improve the performance of neural <b>machine</b> translation applications. In this post, we will look ...", "dateLastCrawled": "2022-02-03T07:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Applied Sciences | Free Full-<b>Text</b> | Age Estimation from fMRI <b>Data</b> Using ...", "url": "https://www.mdpi.com/2076-3417/12/2/749/html", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/2076-3417/12/2/749/html", "snippet": "The first is the <b>multi-head</b> <b>self-attention</b> mechanism: we set the number of <b>heads</b> to 2. The second is a position-wise feed-forward layer that sets its network dimension to 8 times the input dimension. We optimize other RNN-based models (vanilla RNN and LSTM) and achieve the <b>same</b> hyperparameters as the GRU-based model. The MLP network consists of three FC layers. Standard dropout probability", "dateLastCrawled": "2022-01-29T05:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "The Annotated <b>Transformer</b> - Harvard University", "url": "http://nlp.seas.harvard.edu/2018/04/03/attention.html", "isFamilyFriendly": true, "displayUrl": "nlp.seas.harvard.edu/2018/04/03/attention", "snippet": "<b>Self-attention</b> has been used successfully in a variety of tasks including <b>reading</b> comprehension, abstractive summarization, textual entailment and <b>learning</b> task-independent sentence representations. End- to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple- language question answering and language modeling tasks.", "dateLastCrawled": "2022-01-31T02:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and ...", "url": "https://aclanthology.org/volumes/W19-48/", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/volumes/W19-48", "snippet": "We inspect the <b>multi-head</b> <b>self-attention</b> in Transformer NMT encoders for three source languages, looking for patterns that could have a syntactic interpretation. In many of the attention <b>heads</b>, we frequently find sequences of consecutive states attending to the <b>same</b> position, which resemble syntactic phrases. We propose a transparent deterministic method of quantifying the amount of syntactic information present in the self-attentions, based on automatically building and evaluating phrase ...", "dateLastCrawled": "2022-01-31T17:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "How to easily do Handwriting Recognition using Deep <b>Learning</b>", "url": "https://nanonets.com/blog/handwritten-character-recognition/", "isFamilyFriendly": true, "displayUrl": "https://nanonets.com/blog/handwritten-character-recognition", "snippet": "The initial approaches of solving handwriting recognition involved <b>Machine</b> <b>Learning</b> methods <b>like</b> Hidden Markov Models(HMM), SVM etc. Once the initial <b>text</b> is pre-processed, feature extraction is performed to identify key information such as loops, inflection points, aspect ratio etc. of an individual character. These generated features are now fed to a classifier say HMM to get the results. The performance of <b>machine</b> <b>learning</b> models is pretty limited due to manual feature extraction phase ...", "dateLastCrawled": "2022-02-03T01:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>The Annotated GPT-2</b> | Committed towards better future", "url": "https://amaarora.github.io/2020/02/18/annotatedGPT2.html", "isFamilyFriendly": true, "displayUrl": "https://amaarora.github.io/2020/02/18/<b>annotatedGPT2</b>.html", "snippet": "Natural language processing tasks, such as question answering, <b>machine</b> translation, <b>reading</b> comprehension, and summarization, are typically approached with supervised <b>learning</b> on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language ...", "dateLastCrawled": "2022-02-03T18:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) Deep <b>Learning for NLP and Speech Recognition</b> | William Jacome ...", "url": "https://www.academia.edu/43190210/Deep_Learning_for_NLP_and_Speech_Recognition", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/43190210/Deep_<b>Learning_for_NLP_and_Speech_Recognition</b>", "snippet": "Deep <b>Learning for NLP and Speech Recognition</b>. William Jacome. Download PDF. Download Full PDF Package. This paper. A short summary of this paper. 37 Full PDFs related to this paper. READ PAPER. Deep <b>Learning for NLP and Speech Recognition</b> . Download. Deep <b>Learning for NLP and Speech Recognition</b>. William Jacome ...", "dateLastCrawled": "2022-02-02T18:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Sujay S Kumar \u00b7 Masters in Intelligent Information Systems (MIIS)", "url": "https://sujayskumar.com/", "isFamilyFriendly": true, "displayUrl": "https://sujayskumar.com", "snippet": "The most common misconception among software developers and <b>data</b> scientists is that a <b>machine</b> <b>learning</b> project lifecycle consists of just training a single successful model and deploying it in a service. The real world scenario is much more complicated than this. In fact, assuming that there would always be a single model in service for a particular problem statement is fallacious. While conventional software architectures take into account that <b>multiple</b> <b>versions</b> of services will be in ...", "dateLastCrawled": "2022-01-31T03:24:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "How to easily do Handwriting Recognition using Deep <b>Learning</b>", "url": "https://nanonets.com/blog/handwritten-character-recognition/", "isFamilyFriendly": true, "displayUrl": "https://nanonets.com/blog/handwritten-character-recognition", "snippet": "This result is then passed to a <b>Multi-Head</b> Language <b>Self-Attention</b> module which <b>is similar</b> to attention module in Visual encoder. The <b>text</b> features generated along the visual features from visual encoder are passed to a mutual-attention module whose task is to align and combine the learned features from both images and the <b>text</b> inputs. The output is passed through a softmax function to get the final result. When evaluating on test <b>data</b>, the transcriptions are not available. Thus only the ...", "dateLastCrawled": "2022-02-03T01:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>The Annotated GPT-2</b> | Committed towards better future", "url": "https://amaarora.github.io/2020/02/18/annotatedGPT2.html", "isFamilyFriendly": true, "displayUrl": "https://amaarora.github.io/2020/02/18/<b>annotatedGPT2</b>.html", "snippet": "Our model largely follows the original transformer work. We trained a 12-layer decoder-only transformer with masked <b>self-attention</b> <b>heads</b> (768 dimensional states and 12 attention <b>heads</b>). For the position-wise feed-forward networks, we used 3072 dimensional inner states. We used the Adam optimization scheme with a max <b>learning</b> rate of 2.5e-4. The ...", "dateLastCrawled": "2022-02-03T18:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "The <b>Illustrated Transformer</b> - Visualizing <b>machine</b> <b>learning</b> one concept ...", "url": "https://jalammar.github.io/illustrated-transformer/", "isFamilyFriendly": true, "displayUrl": "https://jalammar.github.io/<b>illustrated-transformer</b>", "snippet": "Discussions: Hacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments) Translations: Chinese (Simplified), French, Japanese, Korean, Russian, Spanish, Vietnamese Watch: MIT\u2019s Deep <b>Learning</b> State of the Art lecture referencing this post In the previous post, we looked at Attention \u2013 a ubiquitous method in modern deep <b>learning</b> models. Attention is a concept that helped improve the performance of neural <b>machine</b> translation applications. In this post, we will look ...", "dateLastCrawled": "2022-02-03T04:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "The Annotated <b>Transformer</b> - Harvard University", "url": "http://nlp.seas.harvard.edu/2018/04/03/attention.html", "isFamilyFriendly": true, "displayUrl": "nlp.seas.harvard.edu/2018/04/03/attention", "snippet": "<b>Self-attention</b> has been used successfully in a variety of tasks including <b>reading</b> comprehension, abstractive summarization, textual entailment and <b>learning</b> task-independent sentence representations. End- to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple- language question answering and language modeling tasks.", "dateLastCrawled": "2022-01-31T02:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Applied Sciences | Free Full-<b>Text</b> | Age Estimation from fMRI <b>Data</b> Using ...", "url": "https://www.mdpi.com/2076-3417/12/2/749/html", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/2076-3417/12/2/749/html", "snippet": "The first is the <b>multi-head</b> <b>self-attention</b> mechanism: we set the number of <b>heads</b> to 2. The second is a position-wise feed-forward layer that sets its network dimension to 8 times the input dimension. We optimize other RNN-based models (vanilla RNN and LSTM) and achieve the <b>same</b> hyperparameters as the GRU-based model. The MLP network consists of three FC layers. Standard dropout probability", "dateLastCrawled": "2022-01-29T05:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "notes-1/Deep <b>Learning</b>.md at master \u00b7 kirk86/notes-1 \u00b7 <b>GitHub</b>", "url": "https://github.com/kirk86/notes-1/blob/master/Deep%20Learning.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/kirk86/notes-1/blob/master/Deep <b>Learning</b>.md", "snippet": "&quot;A very simple way to improve the performance of almost any <b>machine</b> <b>learning</b> <b>algorithm</b> is to train many different models on the <b>same</b> <b>data</b> and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in ...", "dateLastCrawled": "2021-12-24T03:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) Deep <b>Learning for NLP and Speech Recognition</b> | William Jacome ...", "url": "https://www.academia.edu/43190210/Deep_Learning_for_NLP_and_Speech_Recognition", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/43190210/Deep_<b>Learning_for_NLP_and_Speech_Recognition</b>", "snippet": "Deep <b>Learning for NLP and Speech Recognition</b>. William Jacome. Download PDF. Download Full PDF Package. This paper. A short summary of this paper. 37 Full PDFs related to this paper. READ PAPER. Deep <b>Learning for NLP and Speech Recognition</b> . Download. Deep <b>Learning for NLP and Speech Recognition</b>. William Jacome ...", "dateLastCrawled": "2022-02-02T18:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Deep Learning Based</b> <b>Text Classification</b>: A Comprehensive Review \u2013 arXiv ...", "url": "https://www.arxiv-vanity.com/papers/2004.03705/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2004.03705", "snippet": "Using pre-labeled examples as training <b>data</b>, a <b>machine</b> <b>learning</b> <b>algorithm</b> can learn the inherent associations between pieces of <b>text</b> and their labels. Thus, <b>machine</b> <b>learning</b> based methods can detect hidden patterns in the <b>data</b>, are more scalable, and can be applied to various tasks. This is in contrast to rule-based methods, which need different sets of rules for different tasks. Hybrid methods, as the name suggests, use a combination of rule-based and <b>machine</b> <b>learning</b> methods to make ...", "dateLastCrawled": "2022-02-03T05:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Breaking <b>BERT</b> Down. What is <b>BERT</b>? | by Shreya Ghelani | Towards <b>Data</b> ...", "url": "https://towardsdatascience.com/breaking-bert-down-430461f60efb", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/breaking-<b>bert</b>-down-430461f60efb", "snippet": "A mini-batch consists of <b>multiple</b> examples with the <b>same</b> number of features. Mini-batches are matrices \u2014 or tensors if each input is multi-dimensional \u2014 where one axis corresponds to the batch and the other axis \u2014 or axes \u2014 correspond to the feature dimensions. Batch normalization normalizes the input features across the batch dimension. The key feature of layer normalization is that it normalizes the inputs across the features. In batch normalization, the statistics are computed ...", "dateLastCrawled": "2022-01-31T22:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Aman&#39;s AI Journal</b> \u2022 <b>Aman&#39;s AI Journal \u2022 Reading List</b>", "url": "https://aman.ai/read/", "isFamilyFriendly": true, "displayUrl": "https://aman.ai/read", "snippet": "This paper by Hinton et al. from Google in NIPS 2014 introduces a very simple way to improve the performance of almost any <b>machine</b> <b>learning</b> <b>algorithm</b> by training many different models on the <b>same</b> <b>data</b> and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets.", "dateLastCrawled": "2022-02-02T09:57:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Multi-Granularity Self-Attention for Neural Machine Translation</b> ...", "url": "https://www.researchgate.net/publication/336998986_Multi-Granularity_Self-Attention_for_Neural_Machine_Translation", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/336998986_Multi-Granularity_<b>Self-Attention</b>...", "snippet": "Michel et al. (2019) mask some <b>heads</b> in <b>multi-head</b> attention modules in BERT, and then evaluate the performance on the <b>machine</b> translation task. Similarly, Hao et al. (2019) eliminates certain ...", "dateLastCrawled": "2021-12-20T22:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>The Annotated GPT-2</b> | Committed towards better future", "url": "https://amaarora.github.io/2020/02/18/annotatedGPT2.html", "isFamilyFriendly": true, "displayUrl": "https://amaarora.github.io/2020/02/18/<b>annotatedGPT2</b>.html", "snippet": "Natural language processing tasks, such as question answering, <b>machine</b> translation, <b>reading</b> comprehension, and summarization, are typically approached with supervised <b>learning</b> on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language ...", "dateLastCrawled": "2022-02-03T18:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A Knowledge-Enhanced Pretraining Model for Commonsense Story Generation ...", "url": "https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00302/43540/A-Knowledge-Enhanced-Pretraining-Model-for", "isFamilyFriendly": true, "displayUrl": "https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00302/43540/A-Knowledge...", "snippet": "Story generation is a strong indicator of <b>machine</b> <b>understanding</b> of natural language. It is often approached as selecting a sequence of events to form a story with a reasonable logic or plot. Although existing generative models (Roemmele, 2016; Fan et al., 2018; Fan et al., 2019) <b>can</b> generate stories with good local coherence, they are still struggling to plan a coherent plot and maintain a reasonable event sequence throughout the story, or they are often biased towards generating a limited ...", "dateLastCrawled": "2022-01-24T19:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Deep Learning Based</b> <b>Text Classification</b>: A Comprehensive Review \u2013 arXiv ...", "url": "https://www.arxiv-vanity.com/papers/2004.03705/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2004.03705", "snippet": "Using pre-labeled examples as training <b>data</b>, a <b>machine</b> <b>learning</b> <b>algorithm</b> <b>can</b> learn the inherent associations between pieces of <b>text</b> and their labels. Thus, <b>machine</b> <b>learning</b> based methods <b>can</b> detect hidden patterns in the <b>data</b>, are more scalable, and <b>can</b> be applied to various tasks. This is in contrast to rule-based methods, which need different sets of rules for different tasks. Hybrid methods, as the name suggests, use a combination of rule-based and <b>machine</b> <b>learning</b> methods to make ...", "dateLastCrawled": "2022-02-03T05:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A <b>Knowledge-Enhanced Pretraining Model for Commonsense Story</b> ... - DeepAI", "url": "https://deepai.org/publication/a-knowledge-enhanced-pretraining-model-for-commonsense-story-generation", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/a-<b>knowledge-enhanced-pretraining-model-for-commonsense</b>...", "snippet": "Story generation is a strong indicator of <b>machine</b> <b>understanding</b> of natural language. It is often approached as selecting a sequence of events to form a story with a reasonable logic or plot. While existing generative models Roemmele ; Fan et al. (2018, 2019) <b>can</b> generate stories with good local coherence, they are still struggling to plan a coherent plot and maintain a reasonable event sequence throughout the story, or they are often biased towards generating a limited set of stories with ...", "dateLastCrawled": "2022-01-16T17:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) Deep <b>Learning for NLP and Speech Recognition</b> | William Jacome ...", "url": "https://www.academia.edu/43190210/Deep_Learning_for_NLP_and_Speech_Recognition", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/43190210/Deep_<b>Learning_for_NLP_and_Speech_Recognition</b>", "snippet": "Deep <b>Learning for NLP and Speech Recognition</b>. William Jacome. Download PDF. Download Full PDF Package. This paper. A short summary of this paper. 37 Full PDFs related to this paper. READ PAPER. Deep <b>Learning for NLP and Speech Recognition</b> . Download. Deep <b>Learning for NLP and Speech Recognition</b>. William Jacome ...", "dateLastCrawled": "2022-02-02T18:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Mastering Transformers | Packt", "url": "https://www.packtpub.com/product/mastering-transformers/9781801077651", "isFamilyFriendly": true, "displayUrl": "https://www.packtpub.com/product/mastering-transformers/9781801077651", "snippet": "<b>Multi-head</b> <b>self-attention</b>; Positional encoding to case word order; Parallelizable architectures that make for faster training and fine-tuning ; Model compression (distillation, quantization, and so on) TL (cross-lingual, multitask <b>learning</b>) For many years, we used traditional NLP approaches such as n-gram language models, TF-IDF-based information retrieval models, and one-hot encoded document-term matrices. All these approaches have contributed a lot to the solution of many NLP problems such ...", "dateLastCrawled": "2022-01-25T19:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) <b>Deep Learning Based Text Classification: A Comprehensive Review</b>", "url": "https://www.researchgate.net/publication/340523298_Deep_Learning_Based_Text_Classification_A_Comprehensive_Review", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/340523298_Deep_<b>Learning</b>_Based_<b>Text</b>...", "snippet": "as training <b>data</b>, a <b>machine</b> <b>learning</b> <b>algorithm</b> <b>can</b> learn the inherent associations between pieces of <b>text</b> and . their labels. Thus, <b>machine</b> <b>learning</b> based methods <b>can</b> detect hidden patterns in the ...", "dateLastCrawled": "2022-01-06T16:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Proceedings of the 2020 Conference on Empirical Methods in Natural ...", "url": "https://aclanthology.org/volumes/2020.emnlp-main/", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/volumes/2020.emnlp-main", "snippet": "<b>Machine</b> <b>learning</b> models are trained to find patterns in <b>data</b>. NLP models <b>can</b> inadvertently learn socially undesirable patterns when training on gender biased <b>text</b>. In this work, we propose a novel, general framework that decomposes gender bias in <b>text</b> along several pragmatic and semantic dimensions: bias from the gender of the person being spoken about, bias from the gender of the person being spoken to, and bias from the gender of the speaker. Using this fine-grained framework, we ...", "dateLastCrawled": "2022-02-02T12:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "notes-1/Deep <b>Learning</b>.md at master \u00b7 kirk86/notes-1 \u00b7 <b>GitHub</b>", "url": "https://github.com/kirk86/notes-1/blob/master/Deep%20Learning.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/kirk86/notes-1/blob/master/Deep <b>Learning</b>.md", "snippet": "&quot;A very simple way to improve the performance of almost any <b>machine</b> <b>learning</b> <b>algorithm</b> is to train many different models on the <b>same</b> <b>data</b> and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in ...", "dateLastCrawled": "2021-12-24T03:06:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) The Brownian motion in the transformer model", "url": "https://www.researchgate.net/publication/353208969_The_Brownian_motion_in_the_transformer_model", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/353208969_The_Brownian_motion_in_the...", "snippet": "Abstract. Transformer is the state of the art model for many language and visual tasks. In this paper, we give a deep analysis of its <b>multi-head</b> <b>self-attention</b> (MHSA) module and find that: 1) Each ...", "dateLastCrawled": "2022-01-12T05:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "How to easily do Handwriting Recognition using Deep <b>Learning</b>", "url": "https://nanonets.com/blog/handwritten-character-recognition/", "isFamilyFriendly": true, "displayUrl": "https://nanonets.com/blog/handwritten-character-recognition", "snippet": "In this work the authors proposed usage of a transformer based architecture using multi-headed attention <b>self-attention</b> layers at both visual and <b>text</b> stages and thus <b>can</b> learn both character recognition as well as language-related dependencies of the character sequences to be decoded. Since the language knowledge is embedded into the model itself, there is no need for any additional post-processing step using a language model and hence has the capability to predicts outputs which are not ...", "dateLastCrawled": "2022-02-03T01:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) Sarcasm Detection Using <b>Multi-Head</b> Attention Based Bidirectional LSTM", "url": "https://www.researchgate.net/publication/338379498_Sarcasm_Detection_Using_Multi-Head_Attention_Based_Bidirectional_LSTM", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/338379498_Sarcasm_Detection_Using_<b>Multi-Head</b>...", "snippet": "A <b>multi-head</b> attention-based bidirectional long-short memory (MHA-BiLSTM) deep neural network is proposed by Kumar et al. [21] to detect sarcasm in the SARC <b>data</b> set corpus which is a self ...", "dateLastCrawled": "2022-01-31T12:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and ...", "url": "https://aclanthology.org/volumes/W19-48/", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/volumes/W19-48", "snippet": "We inspect the <b>multi-head</b> <b>self-attention</b> in Transformer NMT encoders for three source languages, looking for patterns that could have a syntactic interpretation. In many of the attention <b>heads</b>, we frequently find sequences of consecutive states attending to the <b>same</b> position, which resemble syntactic phrases. We propose a transparent deterministic method of quantifying the amount of syntactic information present in the self-attentions, based on automatically building and evaluating phrase ...", "dateLastCrawled": "2022-01-31T17:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Part-of-speech <b>tagging of building codes empowered by deep learning</b> and ...", "url": "https://www.sciencedirect.com/science/article/pii/S1474034620302044", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1474034620302044", "snippet": "Like any <b>machine</b> <b>learning</b> models, neural networks predict categories of given inputs. In the context of POS tagging, ... The <b>multi-head</b> attention is the concatenation of <b>multiple</b> <b>self-attention</b> matrices. The <b>multi-head</b> attention is used to capture different dependencies in a sentence. The first step to calculate the <b>self-attention</b> Z is to calculate: the Query Q, Key K, and Value V matrices with the embedding matrix X, the weight of Query W Q, the weight of Key W k, and the weight of Value W ...", "dateLastCrawled": "2021-10-30T23:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) Deep <b>Learning for NLP and Speech Recognition</b> | William Jacome ...", "url": "https://www.academia.edu/43190210/Deep_Learning_for_NLP_and_Speech_Recognition", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/43190210/Deep_<b>Learning_for_NLP_and_Speech_Recognition</b>", "snippet": "Deep <b>Learning for NLP and Speech Recognition</b>. William Jacome. Download PDF. Download Full PDF Package. This paper. A short summary of this paper. 37 Full PDFs related to this paper. READ PAPER. Deep <b>Learning for NLP and Speech Recognition</b> . Download. Deep <b>Learning for NLP and Speech Recognition</b>. William Jacome ...", "dateLastCrawled": "2022-02-02T18:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Long Short-Term Memory-Networks for Machine Reading</b> | Request PDF", "url": "https://www.researchgate.net/publication/312416396_Long_Short-Term_Memory-Networks_for_Machine_Reading", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/312416396_<b>Long_Short-Term_Memory-Networks</b>_for...", "snippet": "<b>Self-attention</b> has shown improvements for a wide variety of NLP tasks such as <b>reading</b> comprehension [6], abstractive summarization [41], neural <b>machine</b> translation [53] and representation <b>learning</b> ...", "dateLastCrawled": "2022-01-29T19:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Deep Learning Based</b> <b>Text Classification</b>: A Comprehensive Review \u2013 arXiv ...", "url": "https://www.arxiv-vanity.com/papers/2004.03705/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2004.03705", "snippet": "Using pre-labeled examples as training <b>data</b>, a <b>machine</b> <b>learning</b> <b>algorithm</b> <b>can</b> learn the inherent associations between pieces of <b>text</b> and their labels. Thus, <b>machine</b> <b>learning</b> based methods <b>can</b> detect hidden patterns in the <b>data</b>, are more scalable, and <b>can</b> be applied to various tasks. This is in contrast to rule-based methods, which need different sets of rules for different tasks. Hybrid methods, as the name suggests, use a combination of rule-based and <b>machine</b> <b>learning</b> methods to make ...", "dateLastCrawled": "2022-02-03T05:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Graph neural networks: A review of methods and applications</b> - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S2666651021000012", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2666651021000012", "snippet": "1. Introduction. Graphs are a kind of <b>data</b> structure which models a set of objects (nodes) and their relationships (edges). Recently, researches on analyzing graphs with <b>machine</b> <b>learning</b> have been receiving more and more attention because of the great expressive power of graphs, i.e. graphs <b>can</b> be used as denotation of a large number of systems across various areas including social science (social networks (Wu et al., 2020), natural science (physical systems (Sanchez et al., 2018; Battaglia ...", "dateLastCrawled": "2022-02-02T10:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(PDF) <b>Deep Learning Based Text Classification: A Comprehensive Review</b>", "url": "https://www.researchgate.net/publication/340523298_Deep_Learning_Based_Text_Classification_A_Comprehensive_Review", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/340523298_Deep_<b>Learning</b>_Based_<b>Text</b>...", "snippet": "as training <b>data</b>, a <b>machine</b> <b>learning</b> <b>algorithm</b> <b>can</b> learn the inherent associations between pieces of <b>text</b> and . their labels. Thus, <b>machine</b> <b>learning</b> based methods <b>can</b> detect hidden patterns in the ...", "dateLastCrawled": "2022-01-06T16:53:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "10.5. <b>Multi-Head Attention</b> \u2014 Dive into Deep <b>Learning</b> 0.17.2 documentation", "url": "http://d2l.ai/chapter_attention-mechanisms/multihead-attention.html", "isFamilyFriendly": true, "displayUrl": "d2l.ai/chapter_attention-mechanisms/<b>multihead-attention</b>.html", "snippet": "This design is called <b>multi-head attention</b>, where each of the \\(h\\) attention pooling outputs is a head [Vaswani et al., 2017]. Using fully-connected layers to perform learnable linear transformations, Fig. 10.5.1 describes <b>multi-head attention</b>.", "dateLastCrawled": "2022-02-02T01:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Introduction to Transformers in Machine Learning</b>", "url": "https://www.machinecurve.com/index.php/2020/12/28/introduction-to-transformers-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/.../12/28/<b>introduction-to-transformers-in-machine-learning</b>", "snippet": "The masked <b>multi-head</b> attention segment, which performs <b>multi-head</b> <b>self-attention</b> on the outputs, but does so in a masked way, so that positions depend on the past only. The <b>multi-head</b> attention segment , which performs <b>multi-head</b> <b>self-attention</b> on a combination of the ( encoded ) inputs and the outputs, so that the model learns to correlate encoded inputs with desired outputs.", "dateLastCrawled": "2022-02-03T03:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Machine</b> <b>Learning</b> Glossary: Language Evaluation | Google Developers", "url": "https://developers.google.com/machine-learning/glossary/language", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine</b>-<b>learning</b>/glossary/language", "snippet": "Refer also to <b>self-attention</b> and <b>multi-head</b> <b>self-attention</b>, which are the building blocks of Transformers. B. bag of words. #language. A representation of the words in a phrase or passage, irrespective of order. For example, bag of words represents the following three phrases identically: the dog jumps; jumps the dog; dog jumps the; Each word is mapped to an index in a sparse vector, where the vector has an index for every word in the vocabulary. For example, the phrase the dog jumps is ...", "dateLastCrawled": "2022-01-29T02:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Attending to Attention. A summary of a revolutionary paper\u2026 | by Akash ...", "url": "https://towardsdatascience.com/attending-to-attention-eba798f0e940", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/attending-to-attention-eba798f0e940", "snippet": "The first is a <b>multi-head</b> <b>self-attention</b> mechanism, and the second is a simple, position-wise fully connected feed-forward network. We employ a residual connection around each of the two sub-layers, followed by layer normalization. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of ...", "dateLastCrawled": "2022-01-25T10:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Redesigning the Transformer Architecture with Insights from Multi ...", "url": "https://ui.adsabs.harvard.edu/abs/2021arXiv210915142D/abstract", "isFamilyFriendly": true, "displayUrl": "https://ui.adsabs.harvard.edu/abs/2021arXiv210915142D/abstract", "snippet": "The Transformer and its variants have been proven to be efficient sequence learners in many different domains. Despite their staggering success, a critical issue has been the enormous number of parameters that must be trained (ranging from $10^7$ to $10^{11}$) along with the quadratic complexity of dot-product attention. In this work, we investigate the problem of approximating the two central components of the Transformer -- <b>multi-head</b> <b>self-attention</b> and point-wise feed-forward ...", "dateLastCrawled": "2021-11-07T15:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Intuitive Introduction to BERT \u2013 MachineCurve</b>", "url": "https://www.machinecurve.com/index.php/2021/01/04/intuitive-introduction-to-bert/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/index.php/2021/01/04/<b>intuitive-introduction-to-bert</b>", "snippet": "From our article about GPT: \u201cThe input is then served to a masked <b>multi-head</b> attention segment, which computes <b>self-attention</b> in a unidirectional way.Here, the residual is added and the result is layer normalized.\u201d Indeed, GPT (which uses the Transformer decoder segment autoregressively during pretraining) and the original Transformer (which performs Seq2Seq), apply a mask in one of the attention modules \u2013 the masked <b>multi-head</b> <b>self-attention</b> subsegment in the decoder segment.. For any ...", "dateLastCrawled": "2022-01-30T22:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Capturing Multi-Resolution Context by Dilated <b>Self-Attention</b>", "url": "https://www.merl.com/publications/docs/TR2021-036.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.merl.com/publications/docs/TR2021-036.pdf", "snippet": "to <b>machine</b> translation or language modeling, where close-by words are more likely to have a dependent relationship, while only a few distant words or word groups are relevant to trace the semantic con-text and syntax of a sentence [15]. This hypothesis is investigated in this work by combining re-stricted (or time-restricted) <b>self-attention</b> with a dilation mechanism, whereby a high <b>self-attention</b> resolution for neighboring frames and a lower <b>self-attention</b> resolution for distant information ...", "dateLastCrawled": "2021-12-02T00:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Fighting Money Laundering with Statistics and <b>Machine</b> <b>Learning</b>: An ...", "url": "https://deepai.org/publication/fighting-money-laundering-with-statistics-and-machine-learning-an-introduction-and-review", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/fighting-money-laundering-with-statistics-and-<b>machine</b>...", "snippet": "Third, we present recent <b>machine</b> <b>learning</b> concepts that have the potential to improve AML. The remainder of the paper is organized as follows. Section 2 introduces AML in banks. Section 3 presents our terminology. Sections 4 and 5 review the literature on client risk profiling and suspicious behavior flagging, respectively. Section 6 provides a discussion on future research directions and section 7 concludes the paper. 2 Anti-Money Laundering in Banks. The international framework for AML is ...", "dateLastCrawled": "2022-01-28T21:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Transformers in NLP: A beginner friendly explanation | Towards Data Science", "url": "https://towardsdatascience.com/transformers-89034557de14", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>transformer</b>s-89034557de14", "snippet": "If you are looking for an <b>analogy</b> between <b>self attention</b> and attention, think of z serving the purpose of context vectors and not global alignment weights. The <b>Transformer</b> . \u26a0\ufe0f A word of caution: the contents of this image may appear exponentially more complicated than they are. We will break this scary beast down into small baby beasts and it will all make sense. (I promise #2) (left) The <b>Transformer</b> architecture. Source: paper. (right) An abstracted version of the same for better ...", "dateLastCrawled": "2022-02-02T13:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "The Illustrated GPT-2 (Visualizing Transformer Language Models) \u2013 Jay ...", "url": "http://jalammar.github.io/illustrated-gpt2/", "isFamilyFriendly": true, "displayUrl": "jalammar.github.io/illustrated-gpt2", "snippet": "Discussions: Hacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments) Translations: Korean, Russian This year, we saw a dazzling application of <b>machine</b> <b>learning</b>. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn\u2019t a particularly novel architecture \u2013 it\u2019s architecture is very similar to the decoder-only transformer.", "dateLastCrawled": "2022-02-01T04:25:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(multi-head self-attention)  is like +(machine learning algorithm reading and understanding multiple versions, or \"heads,\" of the same text data)", "+(multi-head self-attention) is similar to +(machine learning algorithm reading and understanding multiple versions, or \"heads,\" of the same text data)", "+(multi-head self-attention) can be thought of as +(machine learning algorithm reading and understanding multiple versions, or \"heads,\" of the same text data)", "+(multi-head self-attention) can be compared to +(machine learning algorithm reading and understanding multiple versions, or \"heads,\" of the same text data)", "machine learning +(multi-head self-attention AND analogy)", "machine learning +(\"multi-head self-attention is like\")", "machine learning +(\"multi-head self-attention is similar\")", "machine learning +(\"just as multi-head self-attention\")", "machine learning +(\"multi-head self-attention can be thought of as\")", "machine learning +(\"multi-head self-attention can be compared to\")"]}