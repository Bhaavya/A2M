{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Deep Q-Networks (DQN</b>) \u2014 GenRL 0.1 documentation", "url": "https://genrl.readthedocs.io/en/latest/usage/tutorials/Deep/DQN.html", "isFamilyFriendly": true, "displayUrl": "https://genrl.readthedocs.io/en/latest/usage/tutorials/<b>Deep</b>/<b>DQN</b>.html", "snippet": "<b>DQN</b> uses a <b>neural</b> <b>network</b> as a function approximator and objective is to get as close to the Bellman Expectation of the Q-value function as possible. This is done by minimising the loss function which is defined as. E ( s, a, s \u2032, r) \u223c D [ r + \u03b3 m a x a \u2032 Q ( s \u2032, a \u2032; \u03b8 i \u2212) \u2212 Q ( s, a; \u03b8 i)] 2. Unlike in regular Q-learning ...", "dateLastCrawled": "2022-01-31T11:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Reinforcement Learning Explained Visually (Part</b> 5): <b>Deep</b> Q Networks ...", "url": "https://towardsdatascience.com/reinforcement-learning-explained-visually-part-5-deep-q-networks-step-by-step-5a5317197f4b", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>reinforcement-learning-explained-visually-part</b>-5-<b>deep</b>-q...", "snippet": "The underlying principle of a <b>Deep</b> <b>Q Network</b> is very similar to the Q Learning algorithm. It starts with arbitrary Q-value estimates and explores the environment using the \u03b5-greedy policy. And at its core, it uses the same notion of dual actions, a current action with a current Q-value and a target action with a target Q-value, for its update logic to improve its Q-value estimates. <b>DQN</b> Architecture Components. The <b>DQN</b> architecture has two <b>neural</b> nets, the <b>Q network</b> and the Target networks ...", "dateLastCrawled": "2022-01-31T02:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Deep Q-Network</b> - Papers With Code", "url": "https://paperswithcode.com/method/dqn", "isFamilyFriendly": true, "displayUrl": "https://paperswithcode.com/method/<b>dqn</b>", "snippet": "A <b>DQN</b>, or <b>Deep Q-Network</b>, approximates a state-value function in a Q-Learning framework with a <b>neural</b> <b>network</b>. In the Atari Games case, they take in several frames of the game as an input and output state values for each action as an output. It is usually used in conjunction with Experience Replay, for storing the episode steps in memory for off-policy learning, where samples are drawn from the replay memory at random.Additionally, the <b>Q-Network</b> is usually optimized towards a frozen target ...", "dateLastCrawled": "2022-02-03T03:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Deep</b> <b>Q Network</b>(<b>DQN</b>)- Applying <b>Neural</b> <b>Network</b> as a functional ...", "url": "https://medium.com/intro-to-artificial-intelligence/deep-q-network-dqn-applying-neural-network-as-a-functional-approximation-in-q-learning-6ffe3b0a9062", "isFamilyFriendly": true, "displayUrl": "https://<b>medium</b>.com/intro-to-artificial-intelligence/<b>deep</b>-<b>q-network</b>-<b>dqn</b>-applying-<b>neural</b>...", "snippet": "<b>Deep</b> <b>Q Network</b>(<b>DQN</b>)- Applying <b>Neural</b> <b>Network as a functional approximation in Q-learning</b> Q-learning Please follow this link to understand the basics of Q-learning.", "dateLastCrawled": "2022-02-03T05:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Deep</b> <b>Q-Network</b> (<b>DQN</b>)-II. Experience Replay and Target Networks | by ...", "url": "https://towardsdatascience.com/deep-q-network-dqn-ii-b6bf911b6b2c", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>deep</b>-<b>q-network</b>-<b>dqn</b>-ii-b6bf911b6b2c", "snippet": "This is the second post devoted to <b>Deep</b> <b>Q-Network</b> (<b>DQN</b>), in the \u201c<b>Deep</b> Reinforcement Learning Explained\u201d series, in which we will analyse some challenges that appear when we apply <b>Deep</b> Learning to Reinforcement Learning. We will also present in detail the code that solves the OpenAI Gym Pong game using the <b>DQN</b> <b>network</b> introduced in the previous post.. Spanish version of this publication", "dateLastCrawled": "2022-02-02T11:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Build <b>Deep</b> <b>Q-Network</b> - <b>Reinforcement Learning</b> Code Project - deeplizard", "url": "https://deeplizard.com/learn/video/PyQNfsGUnQA", "isFamilyFriendly": true, "displayUrl": "https://<b>deep</b>lizard.com/learn/video/PyQNfsGUnQA", "snippet": "<b>Deep</b> <b>Q-network</b>. Let&#39;s start first with our <b>deep</b> <b>Q-network</b>. This is where PyTorch comes into play. To build a <b>neural</b> <b>network</b> in PyTorch, we use the torch.nn package, which we gave the alias nn when we imported it earlier. This package contains all of the typical components needed to build <b>neural</b> networks.", "dateLastCrawled": "2022-01-26T13:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Reinforcement Learning (<b>DQN</b>) Tutorial \u2014 <b>PyTorch</b> Tutorials 1.10.1+cu102 ...", "url": "https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html", "isFamilyFriendly": true, "displayUrl": "https://<b>pytorch</b>.org/tutorials/intermediate/reinforcement_q_learning.html", "snippet": "This tutorial shows how to use <b>PyTorch</b> to train a <b>Deep</b> Q Learning (<b>DQN</b>) agent on the CartPole-v0 task from the OpenAI Gym. Task. The agent has to decide between two actions - moving the cart left or right - so that the pole attached to it stays upright. You can find an official leaderboard with various algorithms and visualizations at the Gym website. cartpole. As the agent observes the current state of the environment and chooses an action, the environment transitions to a new state, and ...", "dateLastCrawled": "2022-02-03T02:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Deep Recurrent Q-Network</b> | Multi-Agent Reinforcement Learning", "url": "https://marl-ieee-nitk.github.io/deep-reinforcement-learning/2019/01/06/DRQN.html", "isFamilyFriendly": true, "displayUrl": "https://marl-ieee-nitk.github.io/<b>deep</b>-reinforcement-learning/2019/01/06/DRQN.html", "snippet": "<b>Deep Recurrent Q-Network</b> Jan 6, 2019 \u2022 Madhuparna Bhowmik In this article, we will learn about <b>Deep recurrent Q</b>-learning and POMDP and find out why DRQN works better in case of POMDP than <b>DQN</b>.", "dateLastCrawled": "2022-01-26T15:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "What are <b>Deep Q-Networks</b>? - Definition from Techopedia", "url": "https://www.techopedia.com/definition/34032/deep-q-networks", "isFamilyFriendly": true, "displayUrl": "https://<b>www.techopedia.com</b>/definition/34032", "snippet": "<b>Deep Q Networks</b> (<b>DQN</b>) are <b>neural</b> networks (and/or related tools) that utilize <b>deep</b> Q learning in order to provide models such as the simulation of intelligent video game play. Rather than being a specific name for a specific <b>neural</b> <b>network</b> build, <b>Deep Q Networks</b> may be composed of convolutional <b>neural</b> networks and other structures that use specific methods to learn about various processes. Advertisement. Techopedia Explains <b>Deep Q-Networks</b>. The method of <b>deep</b> Q learning typically uses ...", "dateLastCrawled": "2022-02-02T17:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Deep</b> Q Learning and <b>Deep</b> Q Networks (<b>DQN</b>) Intro ... - Python Programming", "url": "https://pythonprogramming.net/deep-q-learning-dqn-reinforcement-learning-python-tutorial/", "isFamilyFriendly": true, "displayUrl": "https://pythonprogramming.net/<b>deep</b>-q-learning-<b>dqn</b>-reinforcement-learning-python-tutorial", "snippet": "<b>Deep</b> Q Learning and <b>Deep</b> Q Networks (<b>DQN</b>) Intro and Agent - Reinforcement Learning w/ Python Tutorial p.5 . Hello and welcome to the first video about <b>Deep</b> Q-Learning and <b>Deep</b> Q Networks, or DQNs. <b>Deep</b> Q Networks are the <b>deep</b> learning/<b>neural</b> <b>network</b> versions of Q-Learning. With DQNs, instead of a Q Table to look up values, you have a model that you inference (make predictions from), and rather than updating the Q table, you fit (train) your model. A typical <b>DQN</b> model might look something ...", "dateLastCrawled": "2022-01-30T03:42:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Reinforcement Learning Explained Visually (Part</b> 5): <b>Deep</b> Q Networks ...", "url": "https://towardsdatascience.com/reinforcement-learning-explained-visually-part-5-deep-q-networks-step-by-step-5a5317197f4b", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>reinforcement-learning-explained-visually-part</b>-5-<b>deep</b>-q...", "snippet": "The underlying principle of a <b>Deep</b> <b>Q Network</b> is very <b>similar</b> to the Q Learning algorithm. It starts with arbitrary Q-value estimates and explores the environment using the \u03b5-greedy policy. And at its core, it uses the same notion of dual actions, a current action with a current Q-value and a target action with a target Q-value, for its update logic to improve its Q-value estimates. <b>DQN</b> Architecture Components. The <b>DQN</b> architecture has two <b>neural</b> nets, the <b>Q network</b> and the Target networks ...", "dateLastCrawled": "2022-01-31T02:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Reinforcement Learning Explained Visually - <b>Deep</b> Q Networks, step-by ...", "url": "https://ketanhdoshi.github.io/Reinforcement-Learning-Deep-Q-Network/", "isFamilyFriendly": true, "displayUrl": "https://ketanhdoshi.github.io/Reinforcement-Learning-<b>Deep</b>-<b>Q-Network</b>", "snippet": "The underlying principle of a <b>Deep</b> <b>Q Network</b> is very <b>similar</b> to the Q Learning algorithm. It starts with arbitrary Q-value estimates and explores the environment using the \u03b5-greedy policy. And at its core, it uses the same notion of dual actions, a current action with a current Q-value and a target action with a target Q-value, for its update logic to improve its Q-value estimates. <b>DQN</b> Architecture Components. The <b>DQN</b> architecture has two <b>neural</b> nets, the <b>Q network</b> and the Target networks ...", "dateLastCrawled": "2022-02-01T13:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Deep</b> Q-Learning with Recurrent <b>Neural</b> Networks", "url": "http://cs229.stanford.edu/proj2016/report/ChenYingLaird-DeepQLearningWithRecurrentNeuralNetwords-report.pdf", "isFamilyFriendly": true, "displayUrl": "cs229.stanford.edu/proj2016/report/ChenYingLaird-<b>Deep</b>QLearningWithRecurrent<b>Neural</b>Net...", "snippet": "rent <b>neural</b> <b>network</b> (RNN) [6] and a <b>deep</b> <b>Q-network</b> (<b>DQN</b>) <b>similar</b> to [5] 1. The idea being that the RNN will be able to retain information from states further back in time and incorporate that into predicting better Qvalues and thus performing better on games that require long term planning. Figure 1: Q*bert, Seaquest, SpaceInvaders and Montesuma\u2019s Revenge In addition to vanilla RNN architectures, we also examine augmented RNN architectures such as attention RNNs. Recent achievements of ...", "dateLastCrawled": "2022-01-29T02:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Deep</b> <b>Q-Network</b> (<b>DQN</b>)-II. Experience Replay and Target Networks | by ...", "url": "https://towardsdatascience.com/deep-q-network-dqn-ii-b6bf911b6b2c", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>deep</b>-<b>q-network</b>-<b>dqn</b>-ii-b6bf911b6b2c", "snippet": "This is the second post devoted to <b>Deep</b> <b>Q-Network</b> (<b>DQN</b>), in the \u201c<b>Deep</b> Reinforcement Learning Explained\u201d series, in which we will analyse some challenges that appear when we apply <b>Deep</b> Learning to Reinforcement Learning. We will also present in detail the code that solves the OpenAI Gym Pong game using the <b>DQN</b> <b>network</b> introduced in the previous post.. Spanish version of this publication", "dateLastCrawled": "2022-02-02T11:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Deep Q</b> Networks (<b>DQN</b>) \u00b7 <b>Deep</b> Reinforcement Learning", "url": "https://stevenschmatz.gitbooks.io/deep-reinforcement-learning/content/deep-q-networks.html", "isFamilyFriendly": true, "displayUrl": "https://stevenschmatz.gitbooks.io/<b>deep</b>-reinforcement-learning/content/<b>deep-q</b>-<b>networks</b>.html", "snippet": "<b>Deep Q</b> Networks (<b>DQN</b>) <b>DQN</b> Extensions Double <b>DQN</b> ... We will use this gradient to update the weights of our Q Q <b>Q-network</b>, because it will drive our <b>network</b> weights to producing the optimal Q Q Q-function and hence the optimal policy \u03c0 \u2217 \\pi^* \u03c0 \u2217 . The <b>neural</b> <b>network</b> architecture. Since the function is approximating a Q function, we require that the input to the <b>neural</b> <b>network</b> be the state variables, and the output be the predicted Q-values. Preprocessing. For most games, only ...", "dateLastCrawled": "2022-01-30T04:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Deep</b> <b>Q-Network</b> (<b>DQN</b>) [12], an example of standard <b>Deep</b> RL <b>neural</b> ...", "url": "https://researchgate.net/figure/Deep-Q-Network-DQN-12-an-example-of-standard-Deep-RL-neural-network-architecture_fig2_328280687", "isFamilyFriendly": true, "displayUrl": "https://researchgate.net/figure/<b>Deep</b>-<b>Q-Network</b>-<b>DQN</b>-12-an-example-of-standard-<b>Deep</b>-RL...", "snippet": "The major breakthrough work combining <b>deep</b> learning with Q-learning was the <b>Deep</b> <b>Q-Network</b> (<b>DQN</b>) [13]. <b>DQN</b> uses a <b>deep</b> <b>neural</b> <b>network</b> for function approximation [87] 6 (see Figure 2) and maintains ...", "dateLastCrawled": "2021-06-30T05:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Q-<b>targets, Double DQN and Dueling DQN</b> | AI Summer", "url": "https://theaisummer.com/Taking_Deep_Q_Networks_a_step_further/", "isFamilyFriendly": true, "displayUrl": "https://theaisummer.com/Taking_<b>Deep</b>_Q_<b>Networks</b>_a_step_further", "snippet": "Double <b>Deep</b> <b>Q Network</b>. To address maximization bias, we use two <b>Deep</b> Q Networks. On the one hand, the <b>DQN</b> is responsible for the selection of the next action (the one with the maximum value) as always. On the other hand, the Target <b>network</b> is responsible for the evaluation of that action.", "dateLastCrawled": "2022-02-02T19:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "[<b>Deep</b> <b>Q-Network</b>]How to exclude ops at auto-differential of Tensorflow ...", "url": "https://stackoverflow.com/questions/45937774/deep-q-networkhow-to-exclude-ops-at-auto-differential-of-tensorflow", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/45937774", "snippet": "I am trying to create a <b>Deep</b> <b>Q-Network</b> (<b>DQN</b>) <b>similar</b> to Deepmind DQN3.0 using Tensorflow, but I am having some difficulties. I think that the cause is TensorFlow&#39;s auto-differential approach. Plea...", "dateLastCrawled": "2022-01-19T19:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Deep</b> <b>Q Network</b> vs Policy Gradients - An Experiment on VizDoom with ...", "url": "https://flyyufelix.github.io/2017/10/12/dqn-vs-pg.html", "isFamilyFriendly": true, "displayUrl": "https://flyyufelix.github.io/2017/10/12/<b>dqn</b>-vs-pg.html", "snippet": "In practice, we usually use a <b>deep</b> <b>neural</b> <b>network</b> as the Q function approximator and apply gradient descent to minimize the objective function \\(L\\). This is known as <b>Deep</b> Q Learning (<b>DQN</b>) . A close variant called Double <b>DQN</b> (DDQN) basically uses 2 <b>neural</b> networks to perform the Bellman iteration, one for generating the prediction term and the other for generating the target term.", "dateLastCrawled": "2022-01-31T19:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Deepmind Deep Q Network (DQN) 3D Convolution</b> - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/34692318/deepmind-deep-q-network-dqn-3d-convolution", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/34692318", "snippet": "<b>Deepmind Deep Q Network (DQN) 3D Convolution</b>. Ask Question Asked 5 years, 11 months ago. Active 5 years, 11 months ago. Viewed 712 times 3 1. I was reading the deepmind nature paper on <b>DQN</b> <b>network</b>. I almost got everything about it except one. I don&#39;t know why no one has asked this question before but it seems a little odd to me anyway. My question: Input to <b>DQN</b> is a 84*84*4 image. The first convolution layer consists of 32 filters of 8*8 with stide 4. I want to know what is the result of ...", "dateLastCrawled": "2022-01-03T21:50:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>DeepMellow: Removing the Need for</b> a Target <b>Network</b> in <b>Deep</b> Q-Learning", "url": "https://www.ijcai.org/Proceedings/2019/0379.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.ijcai.org/Proceedings/2019/0379.pdf", "snippet": "complex domains when combined with <b>deep</b> <b>neural</b> networks. <b>Deep</b> <b>Q-Network</b> (or simply <b>DQN</b>)[Mnih et al., 2015] was the rst algorithm that successfully instantiated this combination; it yields human-level performance in high-dimensional large-scale domains like Atari video games [Bellemareet al., 2013]. An important component of <b>DQN</b> is the use of a target <b>network</b>, which was introduced to stabilize learning. In Q-learning, the agent updates the value of executing an action in the current state ...", "dateLastCrawled": "2022-01-11T11:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Qrash Course: Reinforcement Learning 101 &amp; <b>Deep Q</b> Networks in 10 ...", "url": "https://towardsdatascience.com/qrash-course-deep-q-networks-from-the-ground-up-1bbda41d3677", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/qrash-course-<b>deep-q</b>-<b>networks</b>-from-the-ground-up-1bbda41...", "snippet": "Introducing: Double <b>Deep Q Network</b>, which uses semi-constant labels during training. How? We keep two copies of the <b>Q Network</b>, but only one is being updated \u2014 the other one remains still. Every once in a while though, we replace the constant <b>network</b> with a copy of the trained <b>Q Network</b>, hence the reason we call it \u201csemi-constant\u201d. And so:", "dateLastCrawled": "2022-01-30T05:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Deep</b> Q-Networks (<b>DQN</b>) \u2013 Oleg Sushkov \u2013 Research Engineer", "url": "https://osushkov.github.io/deepq/", "isFamilyFriendly": true, "displayUrl": "https://osushkov.github.io/<b>deep</b>q", "snippet": "I <b>can</b> also conclude that the trained <b>q-network</b> is much better than random (a low bar, I know), and is approximately equal to a min-max player with a look-ahead of depth 2 (I only plotted the winning percentage here, without accounting for draws). Against look-ahead 2 the <b>q-network</b> wins approximately 45% of games, loses 45% of games, and draws approximately 10%. Against a depth 3 look-ahead player it wins about 40% of the time, and loses about 58% of the time. Not too bad given that the q ...", "dateLastCrawled": "2021-09-27T15:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Episodic Memory <b>and Deep Q-Networks - GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/episodic-memory-and-deep-q-networks/", "isFamilyFriendly": true, "displayUrl": "https://www.geeksforgeeks.org/episodic-memory-and-<b>deep</b>-q-<b>networks</b>", "snippet": "<b>Deep</b> Q-Networks (<b>DQN</b>): A well-established technique to perform the above task is Q-learning, where we decide on a function called Q-function which is important for the success of the algorithm. <b>DQN</b> uses the <b>neural</b> networks as Q-function to approximate the action values Q(s, a, \\theta) where the parameter of <b>network</b> and (s,a) represents the state-action pair .", "dateLastCrawled": "2022-01-16T05:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Action <b>Branching Architectures for Deep Reinforcement Learning</b> | DeepAI", "url": "https://deepai.org/publication/action-branching-architectures-for-deep-reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://<b>deep</b>ai.org/publication/action-<b>branching-architectures-for-deep-reinforcement</b>...", "snippet": "Notable examples include the <b>Deep</b> <b>Q-Network</b> (<b>DQN</b>) (Mnih et al., 2013, 2015) ... This approach, which <b>can</b> <b>be thought</b> of as an adaptation of the dueling <b>network</b> into the action branching architecture, generally yields a better performance. The use of the dueling architecture with action branching is particularly an interesting augmentation for learning in large action spaces. This is due to the fact that the dueling architecture <b>can</b> more rapidly identify action redundancies and generalize more ...", "dateLastCrawled": "2022-01-29T23:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Going Deeper Into Reinforcement Learning: Understanding Deep</b>-Q-Networks", "url": "https://danieltakeshi.github.io/2016/12/01/going-deeper-into-reinforcement-learning-understanding-dqn/", "isFamilyFriendly": true, "displayUrl": "https://danieltakeshi.github.io/2016/12/01/<b>going-deeper-into-reinforcement-learning</b>...", "snippet": "The <b>Deep</b> <b>Q-Network</b> (<b>DQN</b>) algorithm, as introduced by DeepMind in a NIPS 2013 workshop paper, and later published in Nature 2015 <b>can</b> be credited with revolutionizing reinforcement learning. In this post, therefore, I would like to give a guide to a subset of the <b>DQN</b> algorithm. This is a continuation of an earlier reinforcement learning article about linear function approximators. My contribution here will be orthogonal to my previous post about the preprocessing steps for game frames. Before ...", "dateLastCrawled": "2022-02-03T15:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Q-<b>targets, Double DQN and Dueling DQN</b> | AI Summer", "url": "https://theaisummer.com/Taking_Deep_Q_Networks_a_step_further/", "isFamilyFriendly": true, "displayUrl": "https://theaisummer.com/Taking_<b>Deep</b>_Q_<b>Networks</b>_a_step_further", "snippet": "Double <b>Deep</b> <b>Q Network</b>. To address maximization bias, we use two <b>Deep</b> Q Networks. On the one hand, the <b>DQN</b> is responsible for the selection of the next action (the one with the maximum value) as always. On the other hand, the Target <b>network</b> is responsible for the evaluation of that action.", "dateLastCrawled": "2022-02-02T19:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "How <b>and when should we update the</b> Q-<b>target in deep Q-learning</b> ...", "url": "https://ai.stackexchange.com/questions/21485/how-and-when-should-we-update-the-q-target-in-deep-q-learning", "isFamilyFriendly": true, "displayUrl": "https://ai.stackexchange.com/questions/21485/how-<b>and-when-should-we-update-the</b>-q...", "snippet": "The update form $\\theta^{\\prime} \\leftarrow \\tau \\theta+(1-\\tau) \\theta^{\\prime}$ (where $\\theta&#39;$ and $\\theta$ represent the weights of the target <b>network</b> and the current <b>network</b>, respectively) does exist and is correct.. It is called soft update and it has been used in the <b>Deep</b> Deterministic Policy Gradient (DDPG) paper, which uses the concept of a target <b>network</b> like <b>DQN</b>. The authors of the paper state that: The weights of these target networks are then updated by having them slowly track ...", "dateLastCrawled": "2022-01-20T16:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "GitHub - rishavb123/MineRL: Applies the <b>Deep</b> Q Learning algorithm using ...", "url": "https://github.com/rishavb123/MineRL", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/rishavb123/MineRL", "snippet": "We used a Convolutional <b>Deep</b> <b>Q Network</b> to take in the image input and output what action(s) to take. One of the ways that Project Malmo allowed our agent to \u201csee\u201d in the Minecraft world was through images, so using a convolutional <b>neural</b> <b>network</b> made logical sense. Similar to most CNNs, we started with the CNN workflow (Convolution, Max Pooling, Activation) and then used some fully connected layers. We also used a replay buffer to allow the agent to have \u201cmemory,\u201d giving the agent a ...", "dateLastCrawled": "2022-01-23T20:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How <b>can</b> I <b>use RNN in DQN? - Quora</b>", "url": "https://www.quora.com/How-can-I-use-RNN-in-DQN", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/How-<b>can</b>-I-use-RNN-in-<b>DQN</b>", "snippet": "Answer: The mechanism is called <b>Deep</b> Recurrent <b>Q-Network</b>. Let\u2019s say you have programmed a robot to solve a maze. It\u2019s at point A and it needs to go to point B. But the robot doesn\u2019t know it\u2019s position in the maze, it only <b>can</b> measure its distance from point B. We call these measurements \u201cObserva...", "dateLastCrawled": "2022-01-19T00:39:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A <b>pair of interrelated neural networks in</b> <b>Deep</b> <b>Q-Network</b> | by Rafael ...", "url": "https://towardsdatascience.com/a-pair-of-interrelated-neural-networks-in-dqn-f0f58e09b3c4", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-<b>pair-of-interrelated-neural-networks-in</b>-<b>dqn</b>-f0f58e09b3c4", "snippet": "A <b>pair of interrelated neural networks in</b> <b>Deep</b> <b>Q-Network</b>. In <b>DQN</b> and Double <b>DQN</b> models, comparing two interrelated <b>neural</b> networks is crucial. Rafael Stekolshchik. Mar 18, 2020 \u00b7 10 min read. source: 123rf.com. We will follow a few steps that have been taken in the fight against correlations and overestimations in the development of the <b>DQN</b> and Double <b>DQN</b> algorithms. As an example of the <b>DQN</b> and Double <b>DQN</b> applications, we present the training results for the CartPole-v0 and CartPole-v1 ...", "dateLastCrawled": "2022-01-29T13:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Deep</b> <b>Q Network</b> vs Policy Gradients - An Experiment on VizDoom with ...", "url": "https://flyyufelix.github.io/2017/10/12/dqn-vs-pg.html", "isFamilyFriendly": true, "displayUrl": "https://flyyufelix.github.io/2017/10/12/<b>dqn</b>-vs-pg.html", "snippet": "<b>Deep</b> <b>Q Network</b> vs Policy Gradients - An Experiment on VizDoom with Keras. October 12, 2017 After a brief stint with several interesting computer vision projects, include this and this, I\u2019ve recently decided to take a break from computer vision and explore reinforcement learning, another exciting field.Similar to computer vision, the field of reinforcement learning has experienced several important breakthroughs made possible by the <b>deep</b> learning revolution.", "dateLastCrawled": "2022-01-31T19:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Q-<b>targets, Double DQN and Dueling DQN</b> | AI Summer", "url": "https://theaisummer.com/Taking_Deep_Q_Networks_a_step_further/", "isFamilyFriendly": true, "displayUrl": "https://theaisummer.com/Taking_<b>Deep</b>_Q_<b>Networks</b>_a_step_further", "snippet": "Double <b>Deep</b> <b>Q Network</b>. To address maximization bias, we use two <b>Deep</b> Q Networks. On the one hand, the <b>DQN</b> is responsible for the selection of the next action (the one with the maximum value) as always. On the other hand, the Target <b>network</b> is responsible for the evaluation of that action.", "dateLastCrawled": "2022-02-02T19:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Deep</b> Q Networks for Visual Fighting Game AI", "url": "http://www.cig2017.com/wp-content/uploads/2017/08/paper_90.pdf", "isFamilyFriendly": true, "displayUrl": "www.cig2017.com/wp-content/uploads/2017/08/paper_90.pdf", "snippet": "MCTS-based agents. Especially, we adopted the <b>Deep</b> Q-learning <b>Network</b> (<b>DQN</b>) successfully demonstrated in Atari Games, and Visual Doom AI competitions [2] Fig. 1. Picture of Fighting Game <b>Compared</b> to previous studies on <b>DQN</b>, the fighting game AI platform suffers from the relatively large number of actions. For example, Atari platform usually has 4~8 actions per each game and the Visual Doom platform requires left, right, shoot, and change weapons. In the fighting game platform, it defines ...", "dateLastCrawled": "2021-11-22T12:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Deep</b> <b>Q-network</b>-based <b>multi-criteria decision-making framework</b> for ...", "url": "https://link.springer.com/article/10.1007/s00521-020-04918-3", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00521-020-04918-3", "snippet": "This paper proposes a <b>deep</b> <b>Q-network</b> (<b>DQN</b>)-based <b>multi-criteria decision-making framework</b> for virtual agents in real time to automatically select goals based on motivations in virtual simulation environments and to plan relevant behaviors to achieve those goals. All motivations are classified according to the five-level Maslow\u2019s hierarchy of needs, and the virtual agents train a double <b>DQN</b> by big social data, select optimal goals depending on motivations, and perform behaviors relying on a ...", "dateLastCrawled": "2021-12-22T08:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Applying a <b>Deep</b> <b>Q Network</b> for OpenAI\u2019s <b>Car Racing</b> Game | by Ali Fakhry ...", "url": "https://towardsdatascience.com/applying-a-deep-q-network-for-openais-car-racing-game-a642daf58fc9", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/applying-a-<b>deep</b>-<b>q-network</b>-for-openais-<b>car-racing</b>-game-a...", "snippet": "Abstract. Using a classic environment from OpenAI, <b>CarRacing</b>-v0, a 2D autonomous vehicle environment, alongside a custom based modification of the environment, a <b>Deep</b> <b>Q-Network</b> (<b>DQN</b>) was created to solve both the classic and custom environments. Through the use of a Resnet18 pre-trained architecture and a custom made convolutional <b>neural</b> <b>network</b> structure, these models were used to solve the classic and modified environments.", "dateLastCrawled": "2022-02-02T19:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Building a <b>DQN</b> in PyTorch: Balancing <b>Cart Pole</b> with <b>Deep</b> RL | by Mohit ...", "url": "https://blog.gofynd.com/building-a-deep-q-network-in-pytorch-fa1086aa5435", "isFamilyFriendly": true, "displayUrl": "https://blog.gofynd.com/building-a-<b>deep</b>-<b>q-network</b>-in-pytorch-fa1086aa5435", "snippet": "The Policy Evaluation step gives us the loss value of the current policy <b>network</b>. With this information, we <b>can</b> use Gradient Descent to optimize the weights of the policy <b>network</b> to minimize this loss. In this way, the policy <b>network</b> <b>can</b> be improved. <b>Deep</b> <b>Q-Network</b>. A <b>DQN</b> is a Q-value function approximator. At each time step, we pass the ...", "dateLastCrawled": "2022-01-31T19:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "NEUROSTUDIO: <b>Deep Reinforcement Learning with</b> <b>Neural</b> Networks \u2013 Machine ...", "url": "https://unrealai.wordpress.com/2018/05/08/deep-rl-with-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://unrealai.wordpress.com/2018/05/08/<b>deep</b>-rl-with-<b>neural</b>-<b>networks</b>", "snippet": "The <b>Deep</b> Q <b>Neural</b> <b>Network</b> Learning Engine *: The Learning Engine provided in this package is a <b>Deep</b> Q <b>Neural</b> <b>Network</b>. The number of hidden layers , neurons, learning rate and training epochs used by the <b>neural</b> <b>network</b> are all configurable via blueprints. The <b>network</b> uses a sequential model with a linear stack of layers. The Learning Engine generates an action prediction across a given set of states that the agent <b>can</b> occupy. It is an off-policy approach that breaks learning into an ...", "dateLastCrawled": "2022-01-24T22:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Playing Atari with <b>Deep</b> Reinforcement Learning", "url": "https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.toronto.edu/~vmnih/docs/<b>dqn</b>.pdf", "snippet": "We refer to a <b>neural</b> <b>network</b> function approximator with weights as a <b>Q-network</b>. A <b>Q-network</b> <b>can</b> be trained by minimising a sequence of loss functions L i( i) that changes at each iteration i, L i( i) = E s;a\u02d8\u02c6( ) h (y i Q(s;a; i)) 2 i; (2) where y i = E s0\u02d8E[r+ max a0 Q(s0;a0; i 1)js;a] is the target for iteration iand \u02c6(s;a) is a probability distribution over sequences sand actions athat we refer to as the behaviour distribution. The parameters from the previous iteration i 1 are held ...", "dateLastCrawled": "2022-02-02T16:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "In a <b>deep</b> <b>Q-network</b>, what are the advantages/disadvantages of ...", "url": "https://www.quora.com/In-a-deep-Q-network-what-are-the-advantages-disadvantages-of-enumerating-all-possible-action-states-even-if-actions-are-independent", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/In-a-<b>deep</b>-<b>Q-network</b>-what-are-the-advantages-disadvantages-of...", "snippet": "Answer (1 of 3): Q learning requires the creation of a two-dimensional array. The first dimension is the number of possible states, while the second dimension is the number of possible actions. Each state-action pair maintains a Q value, which is the expected discounted long-term reward for selec...", "dateLastCrawled": "2022-01-14T00:33:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>DQN</b> Algorithm: A father-son tale. The <b>Deep</b> <b>Q-Network</b> (<b>DQN</b> ...", "url": "https://medium.com/analytics-vidhya/dqn-algorithm-a-father-son-tale-b4bf6ff1ae2f", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/<b>dqn</b>-algorithm-a-father-son-tale-b4bf6ff1ae2f", "snippet": "The <b>Deep</b> <b>Q-Network</b> (<b>DQN</b>) Reinforcement <b>learning</b> algorithm has a surprisingly simple and real life <b>analogy</b> with which it can be explained. It helps understand the sequence of operations involved by ...", "dateLastCrawled": "2022-01-13T23:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Deep Q-Learning with Keras and Gym</b> \u00b7 Keon&#39;s Blog", "url": "https://keon.github.io/deep-q-learning/", "isFamilyFriendly": true, "displayUrl": "https://keon.github.io/<b>deep</b>-q-<b>learning</b>", "snippet": "If we use the <b>analogy</b> of the bicycle, we can define reward as the distance from the original starting point. ## <b>Deep</b> Reinforcement <b>Learning</b> Google\u2019s DeepMind published its famous paper Playing Atari with <b>Deep</b> Reinforcement <b>Learning</b>, in which they introduced a new algorithm called <b>Deep</b> <b>Q Network</b> (<b>DQN</b> for short) in 2013. It demonstrated how an ...", "dateLastCrawled": "2022-02-01T12:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Graying the Black Box: Understanding DQNs</b> | the morning paper", "url": "https://blog.acolyer.org/2016/03/02/graying-the-black-box-understanding-dqns/", "isFamilyFriendly": true, "displayUrl": "https://blog.acolyer.org/2016/03/02/<b>graying-the-black-box-understanding-dqns</b>", "snippet": "<b>Deep</b> Reinforcement <b>Learning</b> (DRL) applies <b>Deep</b> Neural Networks to reinforcement <b>learning</b>. The <b>Deep</b> Mind team used a DRL algorithm called <b>Deep</b> <b>Q-Network</b> (<b>DQN</b>) to learn how to play the Atari games. In \u2018Graying the Black Box,\u2019 Zahavy et al. look at three of those games \u2013 Breakout, Pacman, and Seaquest \u2013 and develop a new visualization and interaction approach that helps to shed insight on what it is that <b>DQN</b> is actually <b>learning</b>.", "dateLastCrawled": "2022-01-20T19:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Using <b>Keras and Deep Q-Network to Play FlappyBird</b> | Ben Lau", "url": "https://yanpanlau.github.io/2016/07/10/FlappyBird-Keras.html", "isFamilyFriendly": true, "displayUrl": "https://yanpanlau.github.io/2016/07/10/FlappyBird-Keras.html", "snippet": "What is <b>Deep</b> <b>Q-Network</b>? <b>Deep</b> <b>Q-Network</b> is a <b>learning</b> algorithm developed by Google DeepMind to play Atari games. They demonstrated how a computer learned to play Atari 2600 video games by observing just the screen pixels and receiving a reward when the game score increased. The result was remarkable because it demonstrates the algorithm is generic enough to play various games. The following post is a must-read for those who are interested in <b>deep</b> reinforcement <b>learning</b>. Demystifying <b>Deep</b> ...", "dateLastCrawled": "2022-01-30T05:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Reinforcement Learning for On-Demand Logistics</b> - <b>DoorDash Engineering Blog</b>", "url": "https://doordash.engineering/2018/09/10/reinforcement-learning-for-on-demand-logistics/", "isFamilyFriendly": true, "displayUrl": "https://doordash.engineering/2018/09/10/<b>reinforcement-learning-for-on-demand-logistics</b>", "snippet": "This approach is known as <b>Deep</b> <b>Q-Network</b> (<b>DQN</b>) and is very useful when feature dimensionality is high and data volume is also high. Reinforcement learned assignment . Now we will discuss how we applied reinforcement <b>learning</b> to the DoorDash assignment problem. To formulate the assignment problem in a way that\u2019s suitable for reinforcement <b>learning</b>, we made the following definitions. State: The outstanding deliveries and working Dashers, since they represent the current status of the world ...", "dateLastCrawled": "2022-01-18T18:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Guide to Reinforcement <b>Learning with Python and TensorFlow</b>", "url": "https://rubikscode.net/2021/07/13/deep-q-learning-with-python-and-tensorflow-2-0/", "isFamilyFriendly": true, "displayUrl": "https://rubikscode.net/2021/07/13/<b>deep</b>-q-<b>learning-with-python-and-tensorflow</b>-2-0", "snippet": "In the previous two articles we started exploring the interesting universe of reinforcement <b>learning</b>.First we went through the basics of third paradigm within <b>machine</b> <b>learning</b> \u2013 reinforcement <b>learning</b>.Just to freshen up our memory, we saw that approach of this type of <b>learning</b> is unlike the previously explored supervised and unsupervised <b>learning</b>. In reinforcement <b>learning</b>, self-<b>learning</b> agent learns how to interact with the environment and solve a problem within it. In this article, we ...", "dateLastCrawled": "2022-02-03T13:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Solving Combinatorial Problems with Machine Learning Methods</b> | Request PDF", "url": "https://www.researchgate.net/publication/333525406_Solving_Combinatorial_Problems_with_Machine_Learning_Methods", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/333525406_Solving_Combinatorial_Problems_with...", "snippet": "Next we discuss <b>Deep</b> <b>Q-Network</b> (<b>DQN</b>) and its extensions, asynchronous methods, policy optimization, reward, and planning. After that, we talk about attention and memory, unsupervised <b>learning</b>, and ...", "dateLastCrawled": "2022-01-23T14:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Deep</b> Reinforcement <b>Learning</b> for Crowdsourced Urban Delivery - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S0191261521001636", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0191261521001636", "snippet": "We propose a new <b>deep</b> reinforcement <b>learning</b> (DRL)-based approach to tackling this assignment problem. A <b>deep</b> <b>Q network</b> (<b>DQN</b>) algorithm is trained which entails two salient features of experience replay and target network that enhance the efficiency, convergence, and stability of DRL training. More importantly, this paper makes three methodological contributions: 1) presenting a comprehensive and novel characterization of crowdshipping system states that encompasses spatial-temporal and ...", "dateLastCrawled": "2022-01-19T19:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Reinforcement Learning for Formula 1</b> Race Strategy | by Ashref Maiza ...", "url": "https://towardsdatascience.com/reinforcement-learning-for-formula-1-race-strategy-7f29c966472a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>reinforcement-learning-for-formula-1</b>-race-strategy-7f29...", "snippet": "Reinforcement <b>Learning</b> (RL) is an advanced <b>machine</b> <b>learning</b> (ML) technique which takes a very different approach to training models than other <b>machine</b> <b>learning</b> methods. Its super power is that it learns very complex behaviors without requiring any labeled training data, and can make short term decisions while optimizing for a longer term goal. RL in the context of Formula 1 racing. In RL, an agent learns the optimal behavior to perform a certain task by interacting directly with the ...", "dateLastCrawled": "2022-02-02T11:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Applications of <b>Reinforcement Learning</b> in Real World | by garychl ...", "url": "https://towardsdatascience.com/applications-of-reinforcement-learning-in-real-world-1a94955bcd12", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/applications-of-<b>reinforcement-learning</b>-in-real-world-1a...", "snippet": "RL, known as a semi-supervised <b>learning</b> model in <b>machine</b> <b>learning</b>, is a technique to allow an agent to take actions and interact with an environment so as to maximize the total rewards. RL is usually modeled as a Markov Decision Process (MDP). Source: <b>Reinforcement Learning</b>:An Introduction. Imagine a baby is given a TV remote control at your home (environment). In simple terms, the baby (agent) will first observe and construct his/her own representation of the environment (state). Then the ...", "dateLastCrawled": "2022-02-02T20:37:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(deep q-network (dqn))  is like +(neural network)", "+(deep q-network (dqn)) is similar to +(neural network)", "+(deep q-network (dqn)) can be thought of as +(neural network)", "+(deep q-network (dqn)) can be compared to +(neural network)", "machine learning +(deep q-network (dqn) AND analogy)", "machine learning +(\"deep q-network (dqn) is like\")", "machine learning +(\"deep q-network (dqn) is similar\")", "machine learning +(\"just as deep q-network (dqn)\")", "machine learning +(\"deep q-network (dqn) can be thought of as\")", "machine learning +(\"deep q-network (dqn) can be compared to\")"]}