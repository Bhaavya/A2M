{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Understanding MT Quality: <b>BLEU</b> Scores", "url": "https://blog.modernmt.com/understanding-mt-quality-bleu-scores/", "isFamilyFriendly": true, "displayUrl": "https://blog.modernmt.com/understanding-mt-quality-<b>bleu</b>-scores", "snippet": "Very simply stated, <b>BLEU</b> is a \u201cquality metric\u201d score for an MT system that is attempting to measure the correspondence between <b>a machine</b> <b>translation</b> output and that of a <b>human</b> with the understanding that &quot;the closer <b>a machine</b> <b>translation</b> <b>is to a professional</b> <b>human</b> <b>translation</b>, the better it is&quot; \u2013 this is the central idea behind <b>BLEU</b>. This is also the central idea behind the other &quot;improved&quot; metrics.", "dateLastCrawled": "2022-02-03T17:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Understanding MT Quality: <b>BLEU</b> Scores | by K Vashee | Medium", "url": "https://kvashee.medium.com/understanding-mt-quality-bleu-scores-9a19ed20526d", "isFamilyFriendly": true, "displayUrl": "https://kvashee.medium.com/understanding-mt-quality-<b>bleu</b>-scores-9a19ed20526d", "snippet": "Very simply stated, <b>BLEU</b> is a \u201cquality metric\u201d score for an MT system that is attempting to measure the correspondence between <b>a machine</b> <b>translation</b> output and that of a <b>human</b> with the understanding that \u201cthe closer <b>a machine</b> <b>translation</b> <b>is to a professional</b> <b>human</b> <b>translation</b>, the better it is\u201d \u2014 this is the central idea behind <b>BLEU</b>. Scores are calculated for individual MT translated segments-generally sentences-by comparing them with a set of good quality <b>human</b> reference ...", "dateLastCrawled": "2022-01-31T18:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "What is <b>BLEU</b> Score? (And How Does it Affect <b>Translation</b>?) - Asian ...", "url": "https://asianabsolute.co.uk/blog/2021/01/28/what-is-bleu-score/", "isFamilyFriendly": true, "displayUrl": "https://asianabsolute.co.uk/blog/2021/01/28/what-is-<b>bleu</b>-score", "snippet": "<b>BLEU</b> stands for <b>BiLingual</b> <b>Evaluation</b> <b>Understudy</b>. A <b>BLEU</b> score is a quality metric assigned to a text which has been translated by <b>a Machine</b> <b>Translation</b> engine. The goal with MT is to produce results equal to those of a <b>professional</b> <b>human</b> translator. <b>BLEU</b> is an algorithm which is designed to measure exactly that. Because it is fast and cheap to carry out, <b>BLEU</b> has stayed one of the most popular automated metrics for determining the quality of <b>Machine</b> <b>Translation</b>. Even though it was one of the ...", "dateLastCrawled": "2022-01-30T21:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) <b>BLEU</b>: a Method for <b>Automatic Evaluation of Machine Translation</b>", "url": "https://www.researchgate.net/publication/2588204_BLEU_a_Method_for_Automatic_Evaluation_of_Machine_Translation", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/2588204_", "snippet": "We describe two metrics for <b>automatic evaluation of machine trans-lation</b> quality. These metrics, <b>BLEU</b> and NEE, are compared to <b>hu-man</b> judgment of quality of <b>translation</b> of Arabic, Chinese, French ...", "dateLastCrawled": "2022-02-01T20:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Why Evaluating Machine Translation Quality is</b> Hard | Waygo", "url": "http://blog.waygoapp.com/why-evaluating-machine-translation-quality-is-hard/", "isFamilyFriendly": true, "displayUrl": "blog.waygoapp.com/<b>why-evaluating-machine-translation-quality-is</b>-hard", "snippet": "<b>Measuring</b> <b>Translation</b> Quality. <b>BLEU</b> (an acronym for <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) is an algorithm for evaluating the quality of text which has been <b>machine</b>-translated from one language to another. The idea behind <b>BLEU</b> is that \u201cthe closer <b>a machine</b> <b>translation</b> <b>is to a professional</b> <b>human</b> <b>translation</b>, the better it is\u201d. This sounds about the same as our first idea above \u2013 to let a translator evaluate the quality every time. The key difference is that before <b>a machine</b> gets involved ...", "dateLastCrawled": "2021-11-28T05:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "How to Perform <b>Machine</b> <b>Translation</b> <b>Evaluation</b> - Defined.ai", "url": "https://www.defined.ai/blog/machine-translation-101-part-3/", "isFamilyFriendly": true, "displayUrl": "https://www.defined.ai/blog/<b>machine</b>-<b>translation</b>-101-part-3", "snippet": "<b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) is the most common automatic approach to <b>machine</b> <b>translation</b> <b>evaluation</b> at present. It focuses on precision-based features. <b>BLEU</b> works by comparing the <b>machine</b> <b>translation</b> to a number of reference translations. It gives a higher score (on a scale of 0 to 1) when the <b>machine</b> <b>translation</b> text shares a lot of strings with the reference <b>translation</b> text. When the score is closer to 1, the more similar the <b>machine</b> <b>translation</b> is to a <b>human</b> <b>translation</b>. But ...", "dateLastCrawled": "2022-01-08T09:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "2021 <b>Measuring</b> and Comparing <b>Machine</b> <b>Translation</b> Quality", "url": "https://resources.lilt.com/hubfs/Whitepapers/Measuring_and_Comparing_MT_Quality_2021.pdf", "isFamilyFriendly": true, "displayUrl": "https://resources.lilt.com/hubfs/Whitepapers/<b>Measuring</b>_and_Comparing_MT_Quality_2021.pdf", "snippet": "Automatic <b>Evaluation</b> with <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b> (<b>BLEU</b>) The <b>BLEU</b> score has long been the standard for evaluating fully automated MT. It compares <b>a machine</b> <b>translation</b> output with that of a <b>human</b> <b>translation</b>. The <b>close</b> the MT output is to a <b>human</b> <b>translation</b>, the higher the score. To calculate an automatic score, you need one or more <b>human</b> translations for reference, then test the MT output against those reference points. <b>BLEU</b> scores are used frequently because they correlate ...", "dateLastCrawled": "2022-01-23T17:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) Enhanced <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>", "url": "https://www.researchgate.net/publication/264043325_Enhanced_Bilingual_Evaluation_Understudy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/.../264043325_Enhanced_<b>Bilingual</b>_<b>Evaluation</b>_<b>Understudy</b>", "snippet": "<b>Understudy</b> (<b>BLEU</b>) <b>evaluation</b> technique for statistical <b>machine</b> <b>translation</b> to make it more adjustable and robust . We in tend to adapt it to resemble <b>human</b> <b>evaluatio n</b> mor e.", "dateLastCrawled": "2021-12-11T10:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Detecting errors in <b>machine</b> <b>translation</b> using residuals and metrics of ...", "url": "https://content.iospress.com/articles/journal-of-intelligent-and-fuzzy-systems/ifs169504", "isFamilyFriendly": true, "displayUrl": "https://content.iospress.com/articles/journal-of-intelligent-and-fuzzy-systems/ifs169504", "snippet": "<b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>, ) is a geometric mean of n-gram precisions ... Guerberof Arenas A. , Correlations between productivity and quality when post-editing in a <b>professional</b> context, <b>Machine</b> <b>Translation</b> 28(3-4) (2014), 165\u2013186. [2] Han A.L.-F. , Wong D.F. and Chao L.S. , A robust <b>evaluation</b> metric for <b>machine</b> <b>translation</b> with augmented factors, Proceedings of COLING (2012), 441\u2013450. [3] Chen B. , Kuhn R. and Foster G. , Improving amber, an mt <b>evaluation</b> metric ...", "dateLastCrawled": "2022-01-30T14:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Machine</b> <b>Translation</b> Quality Assessment - Pulse of Asia", "url": "https://www.1stopasia.com/blog/machine-translation-quality-assessment/", "isFamilyFriendly": true, "displayUrl": "https://www.1stopasia.com/blog/<b>machine</b>-<b>translation</b>-quality-assessment", "snippet": "The closer the MT comes to a <b>human</b> <b>translation</b>, the higher the <b>BLEU</b> score. A score of 1 usually means that the <b>translation</b> is identical to a <b>human</b> <b>translation</b>, while a score of 0 usually means that the MT has no matches with the <b>human</b> <b>translation</b>. Ultimately, the main goal is to produce translations with the \u201chighest degree of accuracy, and not to imitate the provided references.\u201d", "dateLastCrawled": "2022-01-29T18:14:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Understanding MT Quality: <b>BLEU</b> Scores", "url": "https://blog.modernmt.com/understanding-mt-quality-bleu-scores/", "isFamilyFriendly": true, "displayUrl": "https://blog.modernmt.com/understanding-mt-quality-<b>bleu</b>-scores", "snippet": "Very simply stated, <b>BLEU</b> is a \u201cquality metric\u201d score for an MT system that is attempting to measure the correspondence between <b>a machine</b> <b>translation</b> output and that of a <b>human</b> with the understanding that &quot;the closer <b>a machine</b> <b>translation</b> <b>is to a professional</b> <b>human</b> <b>translation</b>, the better it is&quot; \u2013 this is the central idea behind <b>BLEU</b>. This is also the central idea behind the other &quot;improved&quot; metrics.", "dateLastCrawled": "2022-02-03T17:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Understanding MT Quality: <b>BLEU</b> Scores | by K Vashee | Medium", "url": "https://kvashee.medium.com/understanding-mt-quality-bleu-scores-9a19ed20526d", "isFamilyFriendly": true, "displayUrl": "https://kvashee.medium.com/understanding-mt-quality-<b>bleu</b>-scores-9a19ed20526d", "snippet": "Very simply stated, <b>BLEU</b> is a \u201cquality metric\u201d score for an MT system that is attempting to measure the correspondence between <b>a machine</b> <b>translation</b> output and that of a <b>human</b> with the understanding that \u201cthe closer <b>a machine</b> <b>translation</b> <b>is to a professional</b> <b>human</b> <b>translation</b>, the better it is\u201d \u2014 this is the central idea behind <b>BLEU</b>. Scores are calculated for individual MT translated segments-generally sentences-by comparing them with a set of good quality <b>human</b> reference ...", "dateLastCrawled": "2022-01-31T18:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "How to Perform <b>Machine</b> <b>Translation</b> <b>Evaluation</b> - Defined.ai", "url": "https://www.defined.ai/blog/machine-translation-101-part-3/", "isFamilyFriendly": true, "displayUrl": "https://www.defined.ai/blog/<b>machine</b>-<b>translation</b>-101-part-3", "snippet": "<b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) is the most common automatic approach to <b>machine</b> <b>translation</b> <b>evaluation</b> at present. It focuses on precision-based features. <b>BLEU</b> works by comparing the <b>machine</b> <b>translation</b> to a number of reference translations. It gives a higher score (on a scale of 0 to 1) when the <b>machine</b> <b>translation</b> text shares a lot of strings with the reference <b>translation</b> text. When the score is closer to 1, the more <b>similar</b> the <b>machine</b> <b>translation</b> is to a <b>human</b> <b>translation</b>. But ...", "dateLastCrawled": "2022-01-08T09:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) <b>BLEU</b>: a Method for <b>Automatic Evaluation of Machine Translation</b>", "url": "https://www.researchgate.net/publication/2588204_BLEU_a_Method_for_Automatic_Evaluation_of_Machine_Translation", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/2588204_", "snippet": "We describe two metrics for <b>automatic evaluation of machine trans-lation</b> quality. These metrics, <b>BLEU</b> and NEE, are compared to <b>hu-man</b> judgment of quality of <b>translation</b> of Arabic, Chinese, French ...", "dateLastCrawled": "2022-02-01T20:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) Enhanced <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>", "url": "https://www.researchgate.net/publication/264043325_Enhanced_Bilingual_Evaluation_Understudy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/.../264043325_Enhanced_<b>Bilingual</b>_<b>Evaluation</b>_<b>Understudy</b>", "snippet": "<b>Understudy</b> (<b>BLEU</b>) <b>evaluation</b> technique for statistical <b>machine</b> <b>translation</b> to make it more adjustable and robust . We in tend to adapt it to resemble <b>human</b> <b>evaluatio n</b> mor e.", "dateLastCrawled": "2021-12-11T10:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Automated MT Evaluation Metrics</b> - TAUS", "url": "https://blog.taus.net/automated-mt-evaluation-metrics", "isFamilyFriendly": true, "displayUrl": "https://blog.taus.net/<b>automated-mt-evaluation-metrics</b>", "snippet": "The <b>BLEU</b> (<b>Bi-Lingual</b> <b>Evaluation</b> <b>Understudy</b>) score was first proposed in 2002 paper \u201c<b>BLEU</b>: a Method for Automatic <b>Evaluation</b> of <b>Machine</b> <b>Translation</b>\u201c(Kishore Papineni, et al.) and it is still the most widely used metric for MT <b>evaluation</b>, due to its presumed high correlation with <b>human</b> rankings of MT output that has often been brought into question. It is a segment-level algorithm that judges translations on a per-word basis.", "dateLastCrawled": "2022-02-03T14:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "2021 <b>Measuring</b> and Comparing <b>Machine</b> <b>Translation</b> Quality", "url": "https://resources.lilt.com/hubfs/Whitepapers/Measuring_and_Comparing_MT_Quality_2021.pdf", "isFamilyFriendly": true, "displayUrl": "https://resources.lilt.com/hubfs/Whitepapers/<b>Measuring</b>_and_Comparing_MT_Quality_2021.pdf", "snippet": "importantly, can be trained with much less <b>human</b> effort. Neural <b>Machine</b> <b>Translation</b> (NMT) ... Automatic <b>Evaluation</b> with <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b> (<b>BLEU</b>) The <b>BLEU</b> score has long been the standard for evaluating fully automated MT. It compares <b>a machine</b> <b>translation</b> output with that of a <b>human</b> <b>translation</b>. The <b>close</b> the MT output is to a <b>human</b> <b>translation</b>, the higher the score. To calculate an automatic score, you need one or more <b>human</b> translations for reference, then test the MT output ...", "dateLastCrawled": "2022-01-23T17:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Machine translation systems and quality assessment: a systematic review</b> ...", "url": "https://link.springer.com/article/10.1007/s10579-021-09537-5", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10579-021-09537-5", "snippet": "The most popular metric is <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b> (<b>BLEU</b>), a precision measurement carried out at the level of n-grams, indivisible language units. It employs a modified precision that takes into account the maximum number of each n-gram appearance in the reference <b>translation</b> and applies a brevity penalty that is added to the measurement calculation (Papineni et al., 2002 ).", "dateLastCrawled": "2022-01-27T23:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Machine</b> <b>translation</b> systems and quality assessment: a systematic review", "url": "https://www.readkong.com/page/machine-translation-systems-and-quality-assessment-a-6818843", "isFamilyFriendly": true, "displayUrl": "https://www.readkong.com/page/<b>machine</b>-<b>translation</b>-systems-and-quality-assessment-a-6818843", "snippet": "The most popular metric is <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b> (<b>BLEU</b>), a precision measurement carried out at the level of n-grams, 123. <b>Machine</b> <b>translation</b> systems and quality assessment\u2026 indivisible language units. It employs a modified precision that takes into account the maximum number of each n-gram appearance in the reference <b>translation</b> and applies a brevity penalty that is added to the measurement calculation (Papineni et al., 2002). This measurement became very popular as it showed ...", "dateLastCrawled": "2022-01-14T23:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Detecting errors in <b>machine</b> <b>translation</b> using residuals and metrics of ...", "url": "https://content.iospress.com/articles/journal-of-intelligent-and-fuzzy-systems/ifs169504", "isFamilyFriendly": true, "displayUrl": "https://content.iospress.com/articles/journal-of-intelligent-and-fuzzy-systems/ifs169504", "snippet": "<b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>, ) is a geometric mean of n-gram precisions ... Guerberof Arenas A. , Correlations between productivity and quality when post-editing in a <b>professional</b> context, <b>Machine</b> <b>Translation</b> 28(3-4) (2014), 165\u2013186. [2] Han A.L.-F. , Wong D.F. and Chao L.S. , A robust <b>evaluation</b> metric for <b>machine</b> <b>translation</b> with augmented factors, Proceedings of COLING (2012), 441\u2013450. [3] Chen B. , Kuhn R. and Foster G. , Improving amber, an mt <b>evaluation</b> metric ...", "dateLastCrawled": "2022-01-30T14:38:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Why Evaluating Machine Translation Quality is</b> Hard | Waygo", "url": "http://blog.waygoapp.com/why-evaluating-machine-translation-quality-is-hard/", "isFamilyFriendly": true, "displayUrl": "blog.waygoapp.com/<b>why-evaluating-machine-translation-quality-is</b>-hard", "snippet": "<b>Measuring</b> <b>Translation</b> Quality. <b>BLEU</b> (an acronym for <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) is an algorithm for evaluating the quality of text which has been <b>machine</b>-translated from one language to another. The idea behind <b>BLEU</b> is that \u201cthe closer <b>a machine</b> <b>translation</b> <b>is to a professional</b> <b>human</b> <b>translation</b>, the better it is\u201d. This sounds about the same as our first idea above \u2013 to let a translator evaluate the quality every time. The key difference is that before <b>a machine</b> gets involved ...", "dateLastCrawled": "2021-11-28T05:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) Enhanced <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>", "url": "https://www.researchgate.net/publication/264043325_Enhanced_Bilingual_Evaluation_Understudy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/.../264043325_Enhanced_<b>Bilingual</b>_<b>Evaluation</b>_<b>Understudy</b>", "snippet": "<b>Understudy</b> (<b>BLEU</b>) <b>evaluation</b> technique for statistical <b>machine</b> <b>translation</b> to make it more adjustable and robust . We in tend to adapt it to resemble <b>human</b> <b>evaluatio n</b> mor e.", "dateLastCrawled": "2021-12-11T10:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Human</b> <b>Evaluation</b> Of <b>Machine</b> <b>Translation</b>", "url": "https://groups.google.com/g/ug5zfjhnv/c/JfGeRYENjvg", "isFamilyFriendly": true, "displayUrl": "https://groups.google.com/g/ug5zfjhnv/c/JfGeRYENjvg", "snippet": "Together, tests <b>can</b> salt be useful to shield how any system evolves over time. Ibm research has some of <b>bleu</b> <b>evaluation</b> of <b>human</b> <b>translation</b> quality varies greatly leveraged by automatic metrics instead of automatic cover photo as resources. The eminent of <b>Machine</b> <b>Translation</b> <b>Evaluation</b> is therefore central to supply <b>Machine</b> <b>translation</b> systems. Your information will connect be shared with third parties. Just how effective are language learning apps? While overall scores, and a fair around ...", "dateLastCrawled": "2022-01-22T08:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Detecting errors in <b>machine</b> <b>translation</b> using residuals and metrics of ...", "url": "https://content.iospress.com/articles/journal-of-intelligent-and-fuzzy-systems/ifs169504", "isFamilyFriendly": true, "displayUrl": "https://content.iospress.com/articles/journal-of-intelligent-and-fuzzy-systems/ifs169504", "snippet": "<b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>, ) is a geometric mean of n-gram precisions ... Guerberof Arenas A. , Correlations between productivity and quality when post-editing in a <b>professional</b> context, <b>Machine</b> <b>Translation</b> 28(3-4) (2014), 165\u2013186. [2] Han A.L.-F. , Wong D.F. and Chao L.S. , A robust <b>evaluation</b> metric for <b>machine</b> <b>translation</b> with augmented factors, Proceedings of COLING (2012), 441\u2013450. [3] Chen B. , Kuhn R. and Foster G. , Improving amber, an mt <b>evaluation</b> metric ...", "dateLastCrawled": "2022-01-30T14:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Informatics Research Review: A Review of Neural Network Based Image ...", "url": "https://www.ukessays.com/essays/computer-science/informatics-research-review-a-review-of-neural-network-based-image-captioning.php", "isFamilyFriendly": true, "displayUrl": "https://www.ukessays.com/essays/computer-science/informatics-research-review-a-review...", "snippet": "<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b> (<b>BLEU</b>) [11] represents a score for comparing a generated string to a target string. It was initially developed for evaluating text outputs generated by <b>machine</b> <b>translation</b> (MT) models. While it was originally developed for MT, it has since been used for <b>evaluation</b> of various NLP tasks, image captioning being one of them. The score ranges from [0,1], where 1.0 represents a perfect match and 0.0 a non-existent one. Higher scores <b>can</b> generally be interpreted as ...", "dateLastCrawled": "2022-01-29T22:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Evaluation</b> of <b>Text Generation</b>: A Survey | DeepAI", "url": "https://deepai.org/publication/evaluation-of-text-generation-a-survey", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>evaluation</b>-of-<b>text-generation</b>-a-survey", "snippet": "The <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b> (<b>bleu</b>) is one of the first metrics used to measure the similarity between two sentences (Papineni et al., 2002). Originally proposed for <b>machine</b> <b>translation</b>, it compares a candidate <b>translation</b> of text to one or more reference translations. <b>bleu</b>. is a weighted geometric mean of . n-gram precision scores ...", "dateLastCrawled": "2022-02-03T15:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) Understanding Terminologies of CAT Tools and <b>Machine</b> <b>Translation</b> ...", "url": "https://www.academia.edu/69654553/Understanding_Terminologies_of_CAT_Tools_and_Machine_Translation_Applications_Authors_Details", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/69654553/Understanding_Terminologies_of_CAT_Tools_and_<b>Machine</b>...", "snippet": "<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b> (<b>BLEU</b>) Scores evaluates the fluency of the <b>translation</b> correlation with <b>human</b> <b>evaluation</b>, 3. Terminology (Ter) Scores estimates the amount of post-editing effort required, and 4. Post-editing efficiency measures the amount of engine efficiency of post-editing work. According to Lopez (14), the first campaign for the Automatic Language Processing Advisory Committee (ALPAC) already revealed the wide gap between <b>human</b> and <b>machine</b> translators. MTs are still ...", "dateLastCrawled": "2022-02-01T01:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) <b>Measuring Difficulty in Translation and Post</b>-editing: A Review", "url": "https://www.researchgate.net/publication/328561530_Measuring_Difficulty_in_Translation_and_Post-editing_A_Review", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/328561530_<b>Measuring_Difficulty_in_Translation</b>...", "snippet": "idea is that \u201c [t]he closer <b>a machine</b> <b>translation</b> <b>is to a professional</b> <b>human</b> <b>translation</b>, the better it is \u201d (p. 311). It counts the n -gram (or word sequences) matches between", "dateLastCrawled": "2022-01-28T17:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A Survey on <b>Machine</b> Reading Comprehension\u2014Tasks, <b>Evaluation</b> Metrics and ...", "url": "https://www.mdpi.com/2076-3417/10/21/7640/htm", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/2076-3417/10/21/7640/htm", "snippet": "So the <b>evaluation</b> metrics of <b>machine</b> <b>translation</b> tasks <b>can</b> also be used for MRC tasks. In the following sections, we will give detailed calculation methods of these <b>evaluation</b> metrics. 3.2. Accuracy . Accuracy represents the percentage of the questions that a MRC system accurately answers. For example, suppose a MRC task contains N questions, each question corresponds to one correct answer, the answers <b>can</b> be a word, a phrases, or a sentence, and the number of questions that the system ...", "dateLastCrawled": "2022-01-31T08:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Machine</b> <b>Translation</b>: Game Changer or Marketing Hype? \u2013 Picea Consulting ...", "url": "https://www.piceasmart.com/2020/09/machine-translation-game-changer-or-marketing-hype/", "isFamilyFriendly": true, "displayUrl": "https://www.piceasmart.com/2020/09/<b>machine</b>-<b>translation</b>-game-changer-or-marketing-hype", "snippet": "<b>Machine</b> <b>Translation</b> (MT) has seen promising advancements in the past few years. Particularly Neural MT (NMT) ... The Challenge of <b>Measuring</b> <b>Translation</b> Quality. The potential cost and time improvements of MT are undeniable. Furthermore, not all translations require the same level of quality to be useful. However, most content that has a business application or is needed in a <b>professional</b> setting requires higher quality. Therefore, closing the quality gap between MT output and <b>human</b> ...", "dateLastCrawled": "2021-12-30T15:22:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Understanding MT Quality: <b>BLEU</b> Scores | by K Vashee | Medium", "url": "https://kvashee.medium.com/understanding-mt-quality-bleu-scores-9a19ed20526d", "isFamilyFriendly": true, "displayUrl": "https://kvashee.medium.com/understanding-mt-quality-<b>bleu</b>-scores-9a19ed20526d", "snippet": "Very simply stated, <b>BLEU</b> is a \u201cquality metric\u201d score for an MT system that is attempting to measure the correspondence between <b>a machine</b> <b>translation</b> output and that of a <b>human</b> with the understanding that \u201cthe closer <b>a machine</b> <b>translation</b> <b>is to a professional</b> <b>human</b> <b>translation</b>, the better it is\u201d \u2014 this is the central idea behind <b>BLEU</b>. Scores are calculated for individual MT translated segments-generally sentences-by comparing them with a set of good quality <b>human</b> reference ...", "dateLastCrawled": "2022-01-31T18:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Understanding MT Quality: <b>BLEU</b> Scores", "url": "https://blog.modernmt.com/understanding-mt-quality-bleu-scores/", "isFamilyFriendly": true, "displayUrl": "https://blog.modernmt.com/understanding-mt-quality-<b>bleu</b>-scores", "snippet": "Very simply stated, <b>BLEU</b> is a \u201cquality metric\u201d score for an MT system that is attempting to measure the correspondence between <b>a machine</b> <b>translation</b> output and that of a <b>human</b> with the understanding that &quot;the closer <b>a machine</b> <b>translation</b> <b>is to a professional</b> <b>human</b> <b>translation</b>, the better it is&quot; \u2013 this is the central idea behind <b>BLEU</b>. This is also the central idea behind the other &quot;improved&quot; metrics.", "dateLastCrawled": "2022-02-03T17:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>BLEU</b>: a Method for <b>Automatic Evaluation of Machine Translation</b>", "url": "https://www.researchgate.net/publication/2588204_BLEU_a_Method_for_Automatic_Evaluation_of_Machine_Translation", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/2588204_", "snippet": "We describe two metrics for <b>automatic evaluation of machine trans-lation</b> quality. These metrics, <b>BLEU</b> and NEE, are <b>compared</b> to <b>hu-man</b> judgment of quality of <b>translation</b> of Arabic, Chinese, French ...", "dateLastCrawled": "2022-02-01T20:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "What is <b>BLEU</b> Score? (And How Does it Affect <b>Translation</b>?) - Asian ...", "url": "https://asianabsolute.co.uk/blog/2021/01/28/what-is-bleu-score/", "isFamilyFriendly": true, "displayUrl": "https://asianabsolute.co.uk/blog/2021/01/28/what-is-<b>bleu</b>-score", "snippet": "<b>BLEU</b> stands for <b>BiLingual</b> <b>Evaluation</b> <b>Understudy</b>. A <b>BLEU</b> score is a quality metric assigned to a text which has been translated by <b>a Machine</b> <b>Translation</b> engine. The goal with MT is to produce results equal to those of a <b>professional</b> <b>human</b> translator. <b>BLEU</b> is an algorithm which is designed to measure exactly that. Because it is fast and cheap to carry out, <b>BLEU</b> has stayed one of the most popular automated metrics for determining the quality of <b>Machine</b> <b>Translation</b>. Even though it was one of the ...", "dateLastCrawled": "2022-01-30T21:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "2021 <b>Measuring</b> and Comparing <b>Machine</b> <b>Translation</b> Quality", "url": "https://resources.lilt.com/hubfs/Whitepapers/Measuring_and_Comparing_MT_Quality_2021.pdf", "isFamilyFriendly": true, "displayUrl": "https://resources.lilt.com/hubfs/Whitepapers/<b>Measuring</b>_and_Comparing_MT_Quality_2021.pdf", "snippet": "Automatic <b>Evaluation</b> with <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b> (<b>BLEU</b>) ... <b>a machine</b> <b>translation</b> output with that of a <b>human</b> <b>translation</b>. The <b>close</b> the MT output is to a <b>human</b> <b>translation</b>, the higher the score. To calculate an automatic score, you need one or more <b>human</b> translations for reference, then test the MT output against those reference points. <b>BLEU</b> scores are used frequently because they correlate reasonably well with <b>human</b> judgments of quality when comparing similar types of MT systems ...", "dateLastCrawled": "2022-01-23T17:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) Enhanced <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>", "url": "https://www.researchgate.net/publication/264043325_Enhanced_Bilingual_Evaluation_Understudy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/.../264043325_Enhanced_<b>Bilingual</b>_<b>Evaluation</b>_<b>Understudy</b>", "snippet": "<b>Understudy</b> (<b>BLEU</b>) <b>evaluation</b> technique for statistical <b>machine</b> <b>translation</b> to make it more adjustable and robust . We in tend to adapt it to resemble <b>human</b> <b>evaluatio n</b> mor e.", "dateLastCrawled": "2021-12-11T10:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Automated MT Evaluation Metrics</b> - TAUS", "url": "https://blog.taus.net/automated-mt-evaluation-metrics", "isFamilyFriendly": true, "displayUrl": "https://blog.taus.net/<b>automated-mt-evaluation-metrics</b>", "snippet": "The <b>BLEU</b> (<b>Bi-Lingual</b> <b>Evaluation</b> <b>Understudy</b>) score was first proposed in 2002 paper \u201c<b>BLEU</b>: a Method for Automatic <b>Evaluation</b> of <b>Machine</b> <b>Translation</b>\u201c(Kishore Papineni, et al.) and it is still the most widely used metric for MT <b>evaluation</b>, due to its presumed high correlation with <b>human</b> rankings of MT output that has often been brought into question. It is a segment-level algorithm that judges translations on a per-word basis.", "dateLastCrawled": "2022-02-03T14:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Machine translation systems and quality assessment: a systematic review</b> ...", "url": "https://link.springer.com/article/10.1007/s10579-021-09537-5", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10579-021-09537-5", "snippet": "The most popular metric is <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b> (<b>BLEU</b>), a precision measurement carried out at the level of n-grams, indivisible language units. It employs a modified precision that takes into account the maximum number of each n-gram appearance in the reference <b>translation</b> and applies a brevity penalty that is added to the measurement calculation (Papineni et al., 2002 ).", "dateLastCrawled": "2022-01-27T23:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A survey on metrics for <b>the evaluation of user simulations</b> | The ...", "url": "https://www.cambridge.org/core/journals/knowledge-engineering-review/article/survey-on-metrics-for-the-evaluation-of-user-simulations/602976EC6417B5BAA1719D0876FB5611", "isFamilyFriendly": true, "displayUrl": "https://www.cambridge.org/core/journals/knowledge-engineering-review/article/survey-on...", "snippet": "The <b>bleu</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) score (Papineni et al., Reference Papineni, Roukos, Ward and Zhu 2002) is widely used in <b>machine</b> <b>translation</b>. It is a metric that compares two semantically equivalent sentences. Generally, it is used to compare an automatically translated sentence to several reference sentences generated in the target language by <b>human</b> experts. Papineni", "dateLastCrawled": "2022-02-02T10:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Detecting errors in <b>machine</b> <b>translation</b> using residuals and metrics of ...", "url": "https://content.iospress.com/articles/journal-of-intelligent-and-fuzzy-systems/ifs169504", "isFamilyFriendly": true, "displayUrl": "https://content.iospress.com/articles/journal-of-intelligent-and-fuzzy-systems/ifs169504", "snippet": "<b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>, ) is a geometric mean of n-gram precisions ... Guerberof Arenas A. , Correlations between productivity and quality when post-editing in a <b>professional</b> context, <b>Machine</b> <b>Translation</b> 28(3-4) (2014), 165\u2013186. [2] Han A.L.-F. , Wong D.F. and Chao L.S. , A robust <b>evaluation</b> metric for <b>machine</b> <b>translation</b> with augmented factors, Proceedings of COLING (2012), 441\u2013450. [3] Chen B. , Kuhn R. and Foster G. , Improving amber, an mt <b>evaluation</b> metric ...", "dateLastCrawled": "2022-01-30T14:38:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine</b> <b>Learning</b> Glossary: Language <b>Evaluation</b> | Google Developers", "url": "https://developers.google.com/machine-learning/glossary/language", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine</b>-<b>learning</b>/glossary/language", "snippet": "<b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) #language. A score between 0.0 and 1.0, inclusive, indicating the quality of a translation between two human languages (for example, between English and Russian). A <b>BLEU</b> score of 1.0 indicates a perfect translation; a <b>BLEU</b> score of 0.0 indicates a terrible translation. C. causal language model. #language. Synonym for unidirectional language model. See bidirectional language model to contrast different directional approaches in language modeling. crash ...", "dateLastCrawled": "2022-01-29T02:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Evaluation</b> of an <b>NLP</b> model \u2014 latest benchmarks | by Ria Kulshrestha ...", "url": "https://towardsdatascience.com/evaluation-of-an-nlp-model-latest-benchmarks-90fd8ce6fae5", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>evaluation</b>-of-an-<b>nlp</b>-model-latest-benchmarks-90fd8ce6fae5", "snippet": "<b>BLEU</b> Score \u2014 <b>BiLingual</b> <b>Evaluation</b> <b>Understudy</b>. As the name suggests, it was originally used to evaluate translations from one language to another. How to calculate <b>BLEU</b> score? Calculating unigram precision: Step 1: Look at each word in the output sentence and assign it a score of 1 if it shows up in any of the reference sentences and 0 if it doesn\u2019t. Step 2: Normalize that count, so that it\u2019s always between 0 and 1, by dividing the number of words that showed up in one of the reference ...", "dateLastCrawled": "2022-01-28T07:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "The <b>Computational</b> Limits of Deep <b>Learning</b> | DeepAI", "url": "https://deepai.org/publication/the-computational-limits-of-deep-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/the-<b>computational</b>-limits-of-deep-<b>learning</b>", "snippet": "<b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) [papineni2002bleu] score is a metric for translation and computes the similarity between human translation and <b>machine</b> translation based on n-gram. An n-gram is a continuous sequence of n items from a given text. The score is based on precision, brevity penalty, and clipping. The modified n-gram precision ...", "dateLastCrawled": "2022-01-28T07:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Natrual language processing basic concepts - language model - word ...", "url": "https://shuffleai.blog/blog/nlp_concepts_part_1.html", "isFamilyFriendly": true, "displayUrl": "https://shuffleai.blog/blog/nlp_concepts_part_1.html", "snippet": "<b>BLEU</b> stands for <b>bilingual</b> <b>evaluation</b> <b>understudy</b>. It&#39;s an automatic metric to evaluate how close a sequence of text generated by a language model is to a reference. At first, it&#39;s used to evaluate the quality of <b>machine</b> translation text. Now other natural language processing tasks such as task-oriented dialogue generation adopt it as well. For a reference &quot;The man returned to the store&quot;, a generated text &quot;the the man the&quot; would get a BLUE score as below. For each word in the generated text ...", "dateLastCrawled": "2021-12-24T07:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "9.7. <b>Sequence</b> to <b>Sequence</b> <b>Learning</b> \u2014 Dive into Deep <b>Learning</b> 0.17.2 ...", "url": "https://d2l.ai/chapter_recurrent-modern/seq2seq.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_recurrent-modern/seq2seq.html", "snippet": "<b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>), though originally proposed for evaluating <b>machine</b> translation results [Papineni et al., 2002], has been extensively used in measuring the quality of output sequences for different applications.", "dateLastCrawled": "2022-01-26T00:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) pathak2019.pdf | Aditya Kumar Pathak and Priyankit Acharya ...", "url": "https://www.academia.edu/38228943/pathak2019_pdf", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/38228943/pathak2019_pdf", "snippet": "The standard metric people are using for <b>evaluation</b> of MT systems is <b>BLEU</b> score.<b>Bilingual</b> <b>evaluation</b> <b>understudy</b> (<b>BLEU</b>) is the algorithm to determine the quality of text translated by a <b>machine</b> translation. Quality is the comparison between <b>machine</b>-translated output to that of human-generated output; the closer <b>machine</b> translation is to human-generated translation, the better is the <b>BLEU</b> score. <b>BLEU</b> score is a n-gram overlap of <b>machine</b> translation to that of reference translation.<b>BLEU</b> \u00bc min ...", "dateLastCrawled": "2021-02-16T17:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Sequence Models - Deep <b>Learning</b> Specialization 5 - Yuet&#39;s Blog", "url": "https://yestinyang.github.io/2018/02/19/DLS-5-Sequence-Models.html", "isFamilyFriendly": true, "displayUrl": "https://yestinyang.github.io/2018/02/19/DLS-5-Sequence-Models.html", "snippet": "<b>Bleu</b> Score: <b>bilingual</b> <b>evaluation</b> <b>understudy</b>. Evaluate \u2018accuracy\u2019 of a model predicting multiply equally good answers, being a substitute for human evaluating each output Attention Model. Counter the problem of long sentence, which requires the ability of memory but not badly need a NN to do this kind of job. Instead of \u2018remembering\u2019 the ...", "dateLastCrawled": "2022-01-22T18:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>The Evolution of Machine Translation</b> | LLM Law Review", "url": "https://www.llmlawreview.com/2018/01/26/the-evolution-of-machine-translation/", "isFamilyFriendly": true, "displayUrl": "https://www.llmlawreview.com/2018/01/26/<b>the-evolution-of-machine-translation</b>", "snippet": "Using the <b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) method to score the outcome, they found that NMT scored consistently higher than PBSMT for accuracy. In addition, human translators whose native language was Catalan but who were also fluent in English, evaluated sections of the MT translation in three of the books. Once again, the NMT outperformed its rival. It was estimated that \u201cbetween 17% and 34% of the translations \u2026 are perceived by native speakers of the target language to be of ...", "dateLastCrawled": "2022-01-19T23:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Smoothing a <b>PBSMT Model by Factoring Out Adjuncts</b>", "url": "https://www.researchgate.net/publication/254810876_Smoothing_a_PBSMT_Model_by_Factoring_Out_Adjuncts", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/254810876_Smoothing_a_PBSMT_Model_by...", "snippet": "We evaluated the automated <b>machine</b> translated outputs using <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b> (<b>BLEU</b>). The EBMT approach produced the highest accuracy of 84. 21% whereas the accuracy of the online ...", "dateLastCrawled": "2021-09-24T13:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What I Learned from <b>Writing a Data Science Article Every Week for</b> a ...", "url": "https://towardsdatascience.com/what-i-learned-from-writing-a-data-science-article-every-week-for-a-year-201c0357e0ce", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/what-i-learned-from-<b>writing-a-data-science-article</b>...", "snippet": "The best <b>analogy</b> for the payoff of writing/working consistently is making regular contributions to a savings account. On a day-to-day basis, you don\u2019t notice much change, but then one day you wake up and your initial investment has tripled (or you\u2019ve written 100 data science articles). It can be difficult to see the benefits if you look only at the progress from yesterday to today, but if you can take a step back and view the long term then you see the value of making a little progress ...", "dateLastCrawled": "2022-01-27T04:11:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(bleu (bilingual evaluation understudy))  is like +(measuring how close a machine translation is to a professional human translation)", "+(bleu (bilingual evaluation understudy)) is similar to +(measuring how close a machine translation is to a professional human translation)", "+(bleu (bilingual evaluation understudy)) can be thought of as +(measuring how close a machine translation is to a professional human translation)", "+(bleu (bilingual evaluation understudy)) can be compared to +(measuring how close a machine translation is to a professional human translation)", "machine learning +(bleu (bilingual evaluation understudy) AND analogy)", "machine learning +(\"bleu (bilingual evaluation understudy) is like\")", "machine learning +(\"bleu (bilingual evaluation understudy) is similar\")", "machine learning +(\"just as bleu (bilingual evaluation understudy)\")", "machine learning +(\"bleu (bilingual evaluation understudy) can be thought of as\")", "machine learning +(\"bleu (bilingual evaluation understudy) can be compared to\")"]}