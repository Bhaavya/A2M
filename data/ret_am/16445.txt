{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Learning</b> a <b>sparse</b> <b>representation</b> for object detection | Dan Roth ...", "url": "https://www.academia.edu/2661632/Learning_a_sparse_representation_for_object_detection", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/2661632/<b>Learning</b>_a_<b>sparse</b>_<b>representation</b>_for_object_detection", "snippet": "We present an approach for <b>learning</b> to detect <b>objects</b> in still gray images, that is based on a <b>sparse</b>, part-based <b>representation</b> of <b>objects</b>. A vocabulary of information-rich object parts is automatically constructed from a set of sample images of the", "dateLastCrawled": "2022-01-11T03:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Learning a Sparse Representation for Object Detection</b> | Request PDF", "url": "https://www.researchgate.net/publication/221304142_Learning_a_Sparse_Representation_for_Object_Detection", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221304142_<b>Learning</b>_a_<b>Sparse</b>_<b>Representation</b>...", "snippet": "Abstract. We present an approach for <b>learning</b> to detect <b>objects</b> in still gray images, that is based on a <b>sparse</b>, part-based <b>representation</b> of <b>objects</b>. A vocabulary of information-rich object parts ...", "dateLastCrawled": "2021-08-27T02:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Sparse Representation</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/computer-science/sparse-representation", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/computer-science/<b>sparse-representation</b>", "snippet": "A <b>sparse representation</b> of image structures such as edges, corners, and textures requires using a large dictionary of vectors. Section 5.5.1 describes redundant dictionaries of directional wavelets and curvelets. Matching pursuit decompositions over two-dimensional directional Gabor wavelets are introduced in [105].They are constructed with a separable product of Gaussian windows g j [n] in (12.76), with angle directions \u03b8 = k \u03c0 / C where C is typically 4 or 8:", "dateLastCrawled": "2022-02-02T22:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Learning</b> to detect <b>objects</b> in images via a <b>sparse</b> Part-base <b>representation</b>", "url": "https://www.researchgate.net/publication/8198113_Learning_to_detect_objects_in_images_via_a_sparse_Part-base_representation", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/8198113_<b>Learning</b>_to_detect_<b>objects</b>_in_images...", "snippet": "Abstract. We study the problem of detecting <b>objects</b> in still, gray-scale images. Our primary focus is the development of a <b>learning</b>-based approach to the problem that makes use of a <b>sparse</b>, part ...", "dateLastCrawled": "2022-01-10T23:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Theoretical Foundations of Deep <b>Learning</b> via <b>Sparse</b> Representations", "url": "https://elad.cs.technion.ac.il/wp-content/uploads/2018/06/IEEESPM-DeepLearningTheory.pdf", "isFamilyFriendly": true, "displayUrl": "https://elad.cs.technion.ac.il/wp-content/uploads/2018/06/IEEESPM-Deep<b>Learning</b>Theory.pdf", "snippet": "image processing and machine <b>learning</b>. <b>Sparse</b> <b>representation</b> theory (we shall refer to it as Sparseland) puts forward an emerg - ing, highly effective, and universal model. Its core idea is the description of data as a linear combination of <b>few</b> atoms taken from a dictionary of such fundamental elements. Our prime objective in this article is to review a recently introduced [1] model-based explanation of deep <b>learning</b>, which relies on <b>sparse</b> modeling of data. We start by presenting the ...", "dateLastCrawled": "2022-01-30T03:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Theoretical Foundations of Deep <b>Learning</b> via <b>Sparse</b> Representations", "url": "https://elad.cs.technion.ac.il/wp-content/uploads/2018/06/theoretical-foundations-deep-learning-sparse-representation.pdf", "isFamilyFriendly": true, "displayUrl": "https://elad.cs.technion.ac.il/wp-content/uploads/2018/06/theoretical-foundations-deep...", "snippet": "and machine <b>learning</b>. <b>Sparse</b> <b>representation</b> theory (Sparseland, in our language) puts forward an emerging, highly e ective, and universal model. Its core idea is the description of data as a linear combination of <b>few</b> atoms taken from a dictionary of such fundamental elements. Our prime objective in this paper is to review a recently introduced [1] model-based explanation of deep <b>learning</b>, which relies on <b>sparse</b> modeling of data. We start by presenting the general story of Sparseland ...", "dateLastCrawled": "2022-01-21T21:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Learning to recognize objects</b>: Trends in Cognitive Sciences", "url": "https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(98)01261-3", "isFamilyFriendly": true, "displayUrl": "https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(98)01261-3", "snippet": "Evidence from neurophysiological and psychological studies is coming together to shed light on how we represent and <b>recognize</b> <b>objects</b>. This review describes evidence supporting two major hypotheses: the first is that <b>objects</b> are represented in a mosaic-<b>like</b> form in which <b>objects</b> are encoded by combinations of complex, reusable features, rather than two-dimensional templates, or three-dimensional models. The second hypothesis is that transform-invariant representations of <b>objects</b> are learnt ...", "dateLastCrawled": "2021-11-17T19:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Learning sparse and meaningful representations through embodiment</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0893608020303890", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0893608020303890", "snippet": "<b>Sparse</b> <b>representation</b> of the high dimensional image input have been shown to be more robust and stable to noise (Ahmad &amp; Scheinkman, 2019) and is has been demonstrated that <b>sparse</b> representations in deep reinforcement <b>learning</b> agents lead to better performance in several environments (Fernando Hernandez-Garcia and Sutton, 2019, Liu et al., 2019, Rafati and Noelle, 2019).", "dateLastCrawled": "2021-10-14T09:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Neural Networks: Analogies. When our brains form analogies, they\u2026 | by ...", "url": "https://towardsdatascience.com/neural-networks-analogies-7ebeb3ac5d5e", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/neural-networks-analogies-7ebeb3ac5d5e", "snippet": "I\u2019ll outline a potential route to artificial neural networks which exhibit transfer <b>learning</b>: First, <b>Sparse</b> Distributed Representations. Numenta\u2019s Hierarchical Te m poral Memory, along with other techniques, relies upon a <b>sparse</b> distributed <b>representation</b>. An example of this is a very long string of ones and zeroes, where almost all the values are zero \u2014 there is a <b>sparse</b> distribution of the ones. If each digit represented a different thing, <b>like</b> \u2018pointy ears\u2019, \u2018tail ...", "dateLastCrawled": "2022-01-28T03:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Andrew Ng: What are <b>the advantages of sparse coding over</b> <b>sparse</b> ...", "url": "https://www.quora.com/Andrew-Ng-What-are-the-advantages-of-sparse-coding-over-sparse-autoencoders-and-vice-versa", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Andrew-Ng-What-are-<b>the-advantages-of-sparse-coding-over</b>-<b>sparse</b>...", "snippet": "Answer (1 of 2): I understand, that there are many methods to achieve <b>sparse</b> coding, in essence, it is <b>learning</b> a <b>sparse</b> dictionary that when the elements are combined in a particular order way we are able to recreate the original data. <b>Sparse</b> AE is just one of the ways to achieve that, however,...", "dateLastCrawled": "2022-01-22T11:42:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Learning</b> a <b>sparse</b> <b>representation</b> for object detection | Dan Roth ...", "url": "https://www.academia.edu/2661632/Learning_a_sparse_representation_for_object_detection", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/2661632/<b>Learning</b>_a_<b>sparse</b>_<b>representation</b>_for_object_detection", "snippet": "We present an approach for <b>learning</b> to detect <b>objects</b> in still gray images, that is based on a <b>sparse</b>, part-based <b>representation</b> of <b>objects</b>. A vocabulary of information-rich object parts is automatically constructed from a set of sample images of the", "dateLastCrawled": "2022-01-11T03:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Sparse Representation</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/computer-science/sparse-representation", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/computer-science/<b>sparse-representation</b>", "snippet": "A <b>sparse representation</b> of image structures such as edges, corners, and textures requires using a large dictionary of vectors. Section 5.5.1 describes redundant dictionaries of directional wavelets and curvelets. Matching pursuit decompositions over two-dimensional directional Gabor wavelets are introduced in [105].They are constructed with a separable product of Gaussian windows g j [n] in (12.76), with angle directions \u03b8 = k \u03c0 / C where C is typically 4 or 8:", "dateLastCrawled": "2022-02-02T22:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Learning</b> to detect <b>objects</b> in images via a <b>sparse</b> Part-base <b>representation</b>", "url": "https://www.researchgate.net/publication/8198113_Learning_to_detect_objects_in_images_via_a_sparse_Part-base_representation", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/8198113_<b>Learning</b>_to_detect_<b>objects</b>_in_images...", "snippet": "Abstract. We study the problem of detecting <b>objects</b> in still, gray-scale images. Our primary focus is the development of a <b>learning</b>-based approach to the problem that makes use of a <b>sparse</b>, part ...", "dateLastCrawled": "2022-01-10T23:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Learning a Sparse Representation for Object Detection</b> | Request PDF", "url": "https://www.researchgate.net/publication/221304142_Learning_a_Sparse_Representation_for_Object_Detection", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221304142_<b>Learning</b>_a_<b>Sparse</b>_<b>Representation</b>...", "snippet": "Abstract. We present an approach for <b>learning</b> to detect <b>objects</b> in still gray images, that is based on a <b>sparse</b>, part-based <b>representation</b> of <b>objects</b>. A vocabulary of information-rich object parts ...", "dateLastCrawled": "2021-08-27T02:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Symbolic play connects to language through visual object recognition", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3482824/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC3482824", "snippet": "Here we show that object substitutions depend on developmental changes in visual object recognition: 18- to 30-month old children (n=63) substitute <b>objects</b> in play after they have developed the adult-like ability <b>to recognize</b> common <b>objects</b> from <b>sparse</b> models of their geometric structure. These developmental changes in object recognition are a better predictor of object substitutions than language or age. A developmental pathway connecting visual object recognition, object name <b>learning</b>, and ...", "dateLastCrawled": "2022-01-28T22:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 26 ...", "url": "http://host.robots.ox.ac.uk/pascal/VOC/pubs/agarwal04.pdf", "isFamilyFriendly": true, "displayUrl": "host.robots.ox.ac.uk/pascal/VOC/pubs/agarwal04.pdf", "snippet": "<b>Learning</b> to Detect <b>Objects</b> in Images via a <b>Sparse</b>, Part-Based <b>Representation</b> Shivani Agarwal, Aatif Awan, and Dan Roth,Member, IEEE Computer Society Abstract\u2014We study the problem of detecting <b>objects</b> in still, gray-scale images. Our primary focus is the development of a <b>learning</b>-based approach to the problem that makes use of a <b>sparse</b>, part-based <b>representation</b>. A vocabulary of distinctive object parts is automatically constructedfrom a set of sample images of the object class of interest ...", "dateLastCrawled": "2021-08-28T00:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>For Peer Review Only</b>", "url": "https://cogdev.sitehost.iu.edu/labwork/Augustine.Revision2.Jan17.pdf", "isFamilyFriendly": true, "displayUrl": "https://cogdev.sitehost.iu.edu/labwork/Augustine.Revision2.Jan17.pdf", "snippet": "The ability <b>to recognize</b> common <b>objects</b> from <b>sparse</b> information about geometric shape emerges during the same period in which children learn object names and object categories. Hummel and Biederman\u2019s (1992) theory of object recognition proposes that the geometric shapes of <b>objects</b> have two components \u2013 geometric volumes representing major object parts, and the spatial relations among those parts. In the present research, 18- to 30-month-old children\u2019s ability to use separate ...", "dateLastCrawled": "2021-09-20T17:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Chapter 6 <b>Learning</b> Image Patch Similarity", "url": "https://home.ttic.edu/~gregory/thesis/thesisChapter6.pdf", "isFamilyFriendly": true, "displayUrl": "https://home.ttic.edu/~gregory/thesis/thesisChapter6.pdf", "snippet": "Figure 6-1 shows a <b>few</b> <b>examples</b> of groups of patches that are <b>similar</b> to each other. Such a de\ufb01nition may not be broad enough, that is, some patches that do not \ufb01t this description still may be considered <b>similar</b> in the context of a given task. For instance, if the goal is to categorize an object, rather than <b>recognize</b> a speci\ufb01c instance, patches that look visually dissimilar at this low lever may still correspond to semantically matching parts.1 However, we believe that for many ...", "dateLastCrawled": "2022-01-25T08:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Neural Networks: Analogies. When our brains form analogies, they\u2026 | by ...", "url": "https://towardsdatascience.com/neural-networks-analogies-7ebeb3ac5d5e", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/neural-networks-analogies-7ebeb3ac5d5e", "snippet": "I\u2019ll outline a potential route to artificial neural networks which exhibit transfer <b>learning</b>: First, <b>Sparse</b> Distributed Representations. Numenta\u2019s Hierarchical Te m poral Memory, along with other techniques, relies upon a <b>sparse</b> distributed <b>representation</b>. An example of this is a very long string of ones and zeroes, where almost all the values are zero \u2014 there is a <b>sparse</b> distribution of the ones. If each digit represented a different thing, like \u2018pointy ears\u2019, \u2018tail ...", "dateLastCrawled": "2022-01-28T03:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "View-<b>dependent object recognition</b> by monkeys", "url": "http://cbcl.mit.edu/publications/ps/logothetis-poggio-current-biology-1994.pdf", "isFamilyFriendly": true, "displayUrl": "cbcl.mit.edu/publications/ps/logothetis-poggio-current-biology-1994.pdf", "snippet": "that, in the early stages of <b>learning</b> <b>to recognize</b> a pre-viously unfamiliar object, the monkeys build two-dimensional, viewer-centered object representa- tions, rather than a three-dimensional model of the object. When the animals were trained with as <b>few</b> as three views of the object, 1200 apart, they could often <b>recognize</b> all the views of the object resulting from rotations around the same axis. Conclusion: Our experiments show that recognition of three-dimensional novel <b>objects</b> is a ...", "dateLastCrawled": "2021-09-14T12:12:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Symbolic play connects to language through visual object recognition", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3482824/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC3482824", "snippet": "Here we show that object substitutions depend on developmental changes in visual object recognition: 18- to 30-month old children (n=63) substitute <b>objects</b> in play after they have developed the adult-like ability <b>to recognize</b> common <b>objects</b> from <b>sparse</b> models of their geometric structure. These developmental changes in object recognition are a better predictor of object substitutions than language or age. A developmental pathway connecting visual object recognition, object name <b>learning</b>, and ...", "dateLastCrawled": "2022-01-28T22:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Sparse</b> representations based attribute <b>learning</b> for flower ...", "url": "https://www.sciencedirect.com/science/article/pii/S0925231214005827", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0925231214005827", "snippet": "Attribute <b>learning</b> method vs. non-attribute <b>learning</b> method: In the experiment, we compare our attribute <b>learning</b> method to some non-attribute <b>learning</b> methods such as CG-Boost and SVM. As one <b>can</b> see in Fig. 7 , although our method does not provide a significant gain, it delivers a good semantic explanation and an ability <b>to recognize</b> new categories from purely textual descriptions.", "dateLastCrawled": "2022-01-12T18:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "How does the brain solve visual object recognition?", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3306444/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC3306444", "snippet": "Mounting evidence suggests that \u201ccore object recognition,\u201d the ability to rapidly <b>recognize</b> <b>objects</b> despite substantial appearance variation, is solved in the brain via a cascade of reflexive, largely feedforward computations that culminate in a powerful neuronal <b>representation</b> in the inferior temporal cortex. However, the algorithm that produces this solution remains little-understood. Here we review evidence ranging from individual neurons, to neuronal populations, to behavior, to ...", "dateLastCrawled": "2022-02-03T07:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) <b>Learning</b> <b>to Recognize</b> Three-Dimensional <b>Objects</b>", "url": "https://www.researchgate.net/publication/11395425_Learning_to_Recognize_Three-Dimensional_Objects", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/11395425", "snippet": "<b>Learning</b> <b>to Recognize</b> Three-Dimensional <b>Objects</b> 1093 there also needs to be a computational approach that is able to learn ef- \ufb01ciently (in terms of both computation and number of <b>examples</b>) in the", "dateLastCrawled": "2021-12-17T05:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Real-world <b>underwater fish recognition and identification, using sparse</b> ...", "url": "https://www.researchgate.net/publication/259163342_Real-world_underwater_fish_recognition_and_identification_using_sparse_representation", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/259163342_Real-world_underwater_fish...", "snippet": "A <b>sparse</b> <b>representation</b>-based framework was used and was able to achieve a species recognition rate of 81.8%, which is much lower than the results reported in this paper. Note that the dataset ...", "dateLastCrawled": "2021-12-22T17:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A causal view of compositional zero-shot recognition - NIPS", "url": "https://papers.nips.cc/paper/2020/file/1010cedf85f6a7e24b087e63235dc12e-Paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://papers.nips.cc/paper/2020/file/1010cedf85f6a7e24b087e63235dc12e-Paper.pdf", "snippet": "Compositional zero-shot recognition is the problem of <b>learning</b> <b>to recognize</b> new combinations of known components. People seamlessly <b>recognize</b> and generate new compositions from known elements and Compositional Reasoning is considered a hallmark of human intelligence [33, 34, 6, 4]. As a simple example, people <b>can</b> <b>recognize</b> a purple cauli\ufb02ower even if they have never seen one, based on their familiarity with cauli\ufb02owers and with other purple <b>objects</b> (Figure 1b). Unfortunately, although ...", "dateLastCrawled": "2022-01-29T06:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Can</b> We Copy the <b>Brain</b>?. A review of IEEE Spectrum Special\u2026 | by Eugenio ...", "url": "https://towardsdatascience.com/can-we-copy-the-brain-9ddbff5e0dde", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>can</b>-we-copy-the-<b>brain</b>-9ddbff5e0dde", "snippet": "But we are making strides. We started by <b>learning</b> <b>to recognize</b> <b>objects</b> (and faces, and road-scenes). ... Of course we are <b>learning</b> a <b>sparse</b> <b>representation</b>. All deep neural networks do this \u2014 see PS2. And of course we need to learn to act in an environment (embodiment), we already do this by <b>learning</b> to play video games and drive cars. But of course it does not say how to tackle real-world tasks, because Numenta is still stuck in its peculiar business model where it does not help itself and ...", "dateLastCrawled": "2022-01-30T23:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "What is the advantage of <b>sparse</b> autoencoder than the usual autoencoder ...", "url": "https://www.quora.com/What-is-the-advantage-of-sparse-autoencoder-than-the-usual-autoencoder-the-number-of-nodes-in-the-hidden-layer-is-less-than-the-number-of-inputs", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-the-advantage-of-<b>sparse</b>-autoencoder-than-the-usual-auto...", "snippet": "Answer (1 of 4): That\u2019s not the definition of a <b>sparse</b> autoencoder! Every autoencoder should have less nodes in the hidden layer compared to the input layer, the idea for this is to create a compact <b>representation</b> of the input as correctly stated in other answers it is a method of dimensionality...", "dateLastCrawled": "2022-01-14T20:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Sparse</b> Coding Using Biologically Plausible Local <b>Learning</b> Rules ...", "url": "https://fenrirllc.com/2021/09/17/sparse-coding-using-biologically-plausible-local-learning-rules/", "isFamilyFriendly": true, "displayUrl": "https://fenrirllc.com/.../<b>sparse</b>-coding-using-biologically-plausible-local-<b>learning</b>-rules", "snippet": "this leaves <b>representation</b> of the data in terms of <b>sparse</b> activities, right? Only a <b>few</b>. Why? Because of this guy, right? On average, you only get a <b>few</b> active neurons at a time. This is unconstrained optimization, just to hit that point again. Okay. So here is what Olshausen. and Field found out in 1996. Turns out that each of these little ...", "dateLastCrawled": "2022-01-02T16:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Models of object recognition</b> | Nature Neuroscience", "url": "https://www.nature.com/articles/nn1100_1199", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/nn1100_1199", "snippet": "<b>Models of object recognition</b>. We propose a model ( Fig. 3) that extends several existing models 5, 39, 40, 42, 43. A view-based module, whose final stage consists of units tuned to specific views ...", "dateLastCrawled": "2022-02-02T06:40:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Learning</b> a <b>sparse</b> <b>representation</b> for object detection | Dan Roth ...", "url": "https://www.academia.edu/2661632/Learning_a_sparse_representation_for_object_detection", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/2661632/<b>Learning</b>_a_<b>sparse</b>_<b>representation</b>_for_object_detection", "snippet": "We present an approach for <b>learning</b> to detect <b>objects</b> in still gray images, that is based on a <b>sparse</b>, part-based <b>representation</b> of <b>objects</b>. A vocabulary of information-rich object parts is automatically constructed from a set of sample images of the . \u00d7 Close Log In. Log in with Facebook Log in with Google. or. Email. Password. Remember me on this computer. or reset password. Enter the email address you signed up with and we&#39;ll email you a reset link. ...", "dateLastCrawled": "2022-01-11T03:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Learning sparse and meaningful representations through embodiment</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0893608020303890", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0893608020303890", "snippet": "<b>Sparse</b> <b>representation</b> of the high dimensional image input have been shown to be more robust and stable to noise (Ahmad &amp; Scheinkman, 2019) and is has been demonstrated that <b>sparse</b> representations in deep reinforcement <b>learning</b> agents lead to better performance in several environments (Fernando Hernandez-Garcia and Sutton, 2019, Liu et al., 2019, Rafati and Noelle, 2019).", "dateLastCrawled": "2021-10-14T09:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Learning</b> to detect <b>objects</b> in images via a <b>sparse</b> Part-base <b>representation</b>", "url": "https://www.researchgate.net/publication/8198113_Learning_to_detect_objects_in_images_via_a_sparse_Part-base_representation", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/8198113_<b>Learning</b>_to_detect_<b>objects</b>_in_images...", "snippet": "Abstract. We study the problem of detecting <b>objects</b> in still, gray-scale images. Our primary focus is the development of a <b>learning</b>-based approach to the problem that makes use of a <b>sparse</b>, part ...", "dateLastCrawled": "2022-01-10T23:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Neural Networks: Analogies. When our brains form analogies, they\u2026 | by ...", "url": "https://towardsdatascience.com/neural-networks-analogies-7ebeb3ac5d5e", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/neural-networks-analogies-7ebeb3ac5d5e", "snippet": "<b>Sparse</b> Distributed Representations allow a <b>few</b> neat tricks. Because they have only a <b>few</b> ones and many zeroes, two representations <b>can</b> be combined by addition, and when a new <b>representation</b> is <b>compared</b> to this combination, it will usually match only if the new input matches one of the old ones. For example, if the network is presented an image ...", "dateLastCrawled": "2022-01-28T03:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) <b>Learning to recognize 3D objects with SNoW</b> | Ming-Hsuan Yang ...", "url": "https://www.academia.edu/2739310/Learning_to_recognize_3D_objects_with_SNoW", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/2739310/<b>Learning_to_recognize_3D_objects_with_SNoW</b>", "snippet": "<b>Learning to Recognize 3D Objects with SNoW</b> Ming-Hsuan Yang, Dan Roth, and Narendra Ahuja Department of Computer Science and Beckman Institute University of Illinois at Urbana-Champaign, Urbana, IL 61801 mhyang@vison.ai.uiuc.edu danr@cs.uiuc.edu ahuja@vision.ai.uiuc.edu Abstract. This paper describes a novel view-based <b>learning</b> algorithm for 3D ...", "dateLastCrawled": "2022-01-24T22:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "What is the advantage of <b>sparse</b> autoencoder than the usual autoencoder ...", "url": "https://www.quora.com/What-is-the-advantage-of-sparse-autoencoder-than-the-usual-autoencoder-the-number-of-nodes-in-the-hidden-layer-is-less-than-the-number-of-inputs", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-the-advantage-of-<b>sparse</b>-autoencoder-than-the-usual-auto...", "snippet": "Answer (1 of 4): That\u2019s not the definition of a <b>sparse</b> autoencoder! Every autoencoder should have less nodes in the hidden layer <b>compared</b> to the input layer, the idea for this is to create a compact <b>representation</b> of the input as correctly stated in other answers it is a method of dimensionality...", "dateLastCrawled": "2022-01-14T20:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 26 ...", "url": "http://host.robots.ox.ac.uk/pascal/VOC/pubs/agarwal04.pdf", "isFamilyFriendly": true, "displayUrl": "host.robots.ox.ac.uk/pascal/VOC/pubs/agarwal04.pdf", "snippet": "<b>Learning</b> to Detect <b>Objects</b> in Images via a <b>Sparse</b>, Part-Based <b>Representation</b> Shivani Agarwal, Aatif Awan, and Dan Roth,Member, IEEE Computer Society Abstract\u2014We study the problem of detecting <b>objects</b> in still, gray-scale images. Our primary focus is the development of a <b>learning</b>-based approach to the problem that makes use of a <b>sparse</b>, part-based <b>representation</b>. A vocabulary of distinctive object parts is automatically constructedfrom a set of sample images of the object class of interest ...", "dateLastCrawled": "2021-08-28T00:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Andrew Ng: What are <b>the advantages of sparse coding over</b> <b>sparse</b> ...", "url": "https://www.quora.com/Andrew-Ng-What-are-the-advantages-of-sparse-coding-over-sparse-autoencoders-and-vice-versa", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Andrew-Ng-What-are-<b>the-advantages-of-sparse-coding-over</b>-<b>sparse</b>...", "snippet": "Answer (1 of 2): I understand, that there are many methods to achieve <b>sparse</b> coding, in essence, it is <b>learning</b> a <b>sparse</b> dictionary that when the elements are combined in a particular order way we are able to recreate the original data. <b>Sparse</b> AE is just one of the ways to achieve that, however,...", "dateLastCrawled": "2022-01-22T11:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Can</b> We Copy the <b>Brain</b>?. A review of IEEE Spectrum Special\u2026 | by Eugenio ...", "url": "https://towardsdatascience.com/can-we-copy-the-brain-9ddbff5e0dde", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>can</b>-we-copy-the-<b>brain</b>-9ddbff5e0dde", "snippet": "But we are making strides. We started by <b>learning</b> <b>to recognize</b> <b>objects</b> (and faces, and road-scenes). ... Of course we are <b>learning</b> a <b>sparse</b> <b>representation</b>. All deep neural networks do this \u2014 see PS2. And of course we need to learn to act in an environment (embodiment), we already do this by <b>learning</b> to play video games and drive cars. But of course it does not say how to tackle real-world tasks, because Numenta is still stuck in its peculiar business model where it does not help itself and ...", "dateLastCrawled": "2022-01-30T23:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Learning</b> low-dimensional representations via the usage of multiple ...", "url": "https://forums.cs.tau.ac.il/~nin/papers/net97.pdf", "isFamilyFriendly": true, "displayUrl": "https://forums.cs.tau.ac.il/~nin/papers/net97.pdf", "snippet": "<b>Learning</b> <b>to recognize</b> visual <b>objects</b> from <b>examples</b> requires the ability to \ufb01nd meaningful patterns in spaces of very high dimensionality. We present a method for dimensionality reduction which effectively biases the <b>learning</b> system by combining multiple constraints via the use of class labels. The use of extensive class labels steers the resulting low-dimensional <b>representation</b> to become invariant to those directions of variation in the input space that are irrelevant to classi\ufb01cation ...", "dateLastCrawled": "2022-01-18T16:18:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine Learning by Analogy</b> - SlideShare", "url": "https://www.slideshare.net/ColleenFarrelly/machine-learning-by-analogy-59094152", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ColleenFarrelly/<b>machine-learning-by-analogy</b>-59094152", "snippet": "<b>Machine Learning by Analogy</b> 1. Colleen M. Farrelly 2. Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics behind these ...", "dateLastCrawled": "2022-01-31T07:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Neural Networks: Analogies. When our brains form analogies, they\u2026 | by ...", "url": "https://towardsdatascience.com/neural-networks-analogies-7ebeb3ac5d5e", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/neural-networks-analogies-7ebeb3ac5d5e", "snippet": "I\u2019ll outline a potential route to artificial neural networks which exhibit transfer <b>learning</b>: First, <b>Sparse</b> Distributed Representations. Numenta\u2019s Hierarchical Te m poral Memory, along with other techniques, relies upon a <b>sparse</b> distributed <b>representation</b>. An example of this is a very long string of ones and zeroes, where almost all the values are zero \u2014 there is a <b>sparse</b> distribution of the ones. If each digit represented a different thing, like \u2018pointy ears\u2019, \u2018tail ...", "dateLastCrawled": "2022-01-28T03:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "TensorFlow is an interface for expressing <b>machine</b> <b>learning</b> algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PPT) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> | Colleen Farrelly - Academia.edu", "url": "https://www.academia.edu/30404581/Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/30404581/<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "<b>Machine</b> <b>Learning</b> by <b>Analogy</b> Colleen M. Farrelly Overview of Problem Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics ...", "dateLastCrawled": "2022-01-02T06:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine</b> <b>Learning</b> by <b>Analogy</b> II - slideshare.net", "url": "https://www.slideshare.net/ColleenFarrelly/machine-learning-by-analogy-ii", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ColleenFarrelly/<b>machine</b>-<b>learning</b>-by-<b>analogy</b>-ii", "snippet": "Updated <b>Machine</b> <b>Learning</b> by <b>Analogy</b> presentation that builds to more advanced methods (TensorFlow, geometry/topology-based methods...) and adds a section on ti\u2026", "dateLastCrawled": "2021-12-31T12:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Conceptualization as a Basis for Cognition \u2014 Human and <b>Machine</b> | by ...", "url": "https://towardsdatascience.com/conceptualization-as-a-basis-for-cognition-human-and-machine-345d9e687e3c", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/conceptualization-as-a-basis-for-cognition-human-and...", "snippet": "Abstraction and <b>analogy</b> allow concepts to be re-applied in new domains. There are many, often conflicting, ... <b>Machine</b>-<b>learning</b> systems must learn to conceptualize to reach the goal of creating machines with higher intelligence. To substantiate this claim, let\u2019s first examine what generalization in artificial intelligence means specifically in the context of artificial intelligence/<b>machine</b> <b>learning</b> (as opposed to the layman\u2019s use of the term), and then explore how that differs from ...", "dateLastCrawled": "2022-01-20T17:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Compressed Sensing Meets <b>Machine</b> <b>Learning</b>: Classification via <b>Sparse</b> ...", "url": "https://nuit-blanche.blogspot.com/2008/05/cs-mini-course-classification-via.html", "isFamilyFriendly": true, "displayUrl": "https://nuit-blanche.blogspot.com/2008/05/cs-mini-course-classification-via.html", "snippet": "Compressed Sensing Meets <b>Machine</b> <b>Learning</b>: Classification via <b>Sparse</b> <b>Representation</b> and Distributed Pattern Recognition This Spring, Allen Yang has given a mini course at Berkeley entitled Compressed Sensing Meets <b>Machine</b> <b>Learning</b>. The three lectures are listed here (it includes accompanying code): lecture 1: Classification via <b>Sparse</b> <b>Representation</b>; lecture 2: Classification of Mixture Subspace Models via <b>Sparse</b> <b>Representation</b>, lecture 3: Distributed Pattern Recognition; The third lecture ...", "dateLastCrawled": "2022-01-25T10:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Word Embedding: Syntactics or Semantics</b> \u00b7 Shengbin&#39;s Studio", "url": "https://wushbin.github.io/2017/10/09/Word-Embedding-Syntactics-or-Semantics/", "isFamilyFriendly": true, "displayUrl": "https://wushbin.github.io/2017/10/09/<b>Word-Embedding-Syntactics-or-Semantics</b>", "snippet": "<b>Sparse</b> Vector <b>Representation</b>. The co-occurrence matrix in represented each cell by the raw frequency of the co-occurrence of two words. The raw frequency in a matrix may be skewed. Pointwise mutual information PPMI is a good measure for association between words which can tell us how much often the two words occur. The pointwise mutual information is a measure of how often two events x and y occur, compared with what we would expect if they were independent: PMI between two words is ...", "dateLastCrawled": "2022-01-09T18:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>What Are Word Embeddings</b> for Text? - <b>Machine</b> <b>Learning</b> Mastery", "url": "https://machinelearningmastery.com/what-are-word-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>what-are-word-embeddings</b>", "snippet": "Word embeddings are a type of word <b>representation</b> that allows words with similar meaning to have a similar <b>representation</b>. They are a distributed <b>representation</b> for text that is perhaps one of the key breakthroughs for the impressive performance of deep <b>learning</b> methods on challenging natural language processing problems. In this post, you will discover the word embedding approach for representing text data. After", "dateLastCrawled": "2022-01-30T18:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Word Embeddings in NLP - GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/word-embeddings-in-nlp/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/word-embeddings-in-nlp", "snippet": "Word Embeddings are a method of extracting features out of text so that we can input those features into a <b>machine</b> <b>learning</b> model to work with text data. They try to preserve syntactical and semantic information. The methods such as Bag of Words(BOW), CountVectorizer and TFIDF rely on the word count in a sentence but do not save any syntactical or semantic information. In these algorithms, the size of the vector is the number of elements in the vocabulary. We can get a <b>sparse</b> matrix if most ...", "dateLastCrawled": "2022-01-30T08:38:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Sparse Adaptive Local Machine Learning</b> Algorithms for Sensing and Analytics", "url": "https://pdxscholar.library.pdx.edu/cgi/viewcontent.cgi?article=1000&context=mcecs_mentoring", "isFamilyFriendly": true, "displayUrl": "https://pdxscholar.library.pdx.edu/cgi/viewcontent.cgi?article=1000&amp;context=mcecs...", "snippet": "Fig. 2: A <b>sparse representation can be thought of as</b> the dot product of a dictionary vector and a sparse code vector. Given a . dictionary . of general components, we can use a . sparse code. to select as few of them as possible to reconstruct an image of interest (Fig. 2). This reconstruction is called a . sparse representation. Sparse Coding. Image processing is expensive. Instead of working with the original image, we can identify its most relevant components and discard the rest. This ...", "dateLastCrawled": "2021-08-31T12:20:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(sparse representation)  is like +(learning to recognize objects from a few examples)", "+(sparse representation) is similar to +(learning to recognize objects from a few examples)", "+(sparse representation) can be thought of as +(learning to recognize objects from a few examples)", "+(sparse representation) can be compared to +(learning to recognize objects from a few examples)", "machine learning +(sparse representation AND analogy)", "machine learning +(\"sparse representation is like\")", "machine learning +(\"sparse representation is similar\")", "machine learning +(\"just as sparse representation\")", "machine learning +(\"sparse representation can be thought of as\")", "machine learning +(\"sparse representation can be compared to\")"]}