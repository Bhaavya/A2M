{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Artificial Intelligence and <b>Machine</b> <b>Learning</b> for HIV Prevention ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7260108/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7260108", "snippet": "<b>Using</b> a dataset from Twitter, <b>Young</b> et al. applied <b>machine</b> <b>learning</b> to predict tweets defined by a content expert as being related to HIV risk behaviors , suggesting that social media data could augment traditional epidemiologic surveillance by identifying HIV risk behaviors in real time. Although these studies are a proof of concept, the feasibility and <b>impact</b> of AI for HIV prevention in this context are as yet unknown.", "dateLastCrawled": "2022-01-29T01:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Computational Fairness: Preventing <b>Machine</b>-Learned Discrimination", "url": "https://scholarship.tricolib.brynmawr.edu/bitstream/handle/10066/17628/2015FeldmanM.pdf?sequence=1", "isFamilyFriendly": true, "displayUrl": "https://scholarship.tricolib.brynmawr.edu/bitstream/handle/10066/17628/2015FeldmanM...", "snippet": "It may use <b>a machine</b> <b>learning</b> <b>algorithm</b> to Portions of this work were included in Certifying and Removing <b>Disparate</b> <b>Impact</b> [5]. 1. simplify its hiring process. <b>Machine</b> <b>learning</b> algorithms can take data and produce models that can make predictions on future input based on past training data. Models that make discrete outcomes, such as hiring or not hiring an applicant, are called classi ers. The company may take hiring data from previous years { attributes of each applicant and whether or not ...", "dateLastCrawled": "2022-01-19T02:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "AI, <b>Machine</b> <b>Learning</b> &amp; Big Data Laws and Regulations | 3 Artificial ...", "url": "https://www.globallegalinsights.com/practice-areas/ai-machine-learning-and-big-data-laws-and-regulations/3-artificial-intelligence-employment-law-risks-and-considerations", "isFamilyFriendly": true, "displayUrl": "https://www.globallegalinsights.com/practice-areas/ai-<b>machine</b>-<b>learning</b>-and-big-data...", "snippet": "If an <b>algorithm</b> begins producing <b>biased</b> results after reviewing faulty or <b>biased</b> training data consisting of prior decisions made by humans, a ... bring discrimination claims <b>using</b> a <b>disparate</b> <b>impact</b> theory. To succeed on such a claim, the plaintiff must demonstrate that a facially neutral employment practice had a disproportionate effect on a protected group. 68 Unlike a <b>disparate</b> treatment claim, a plaintiff alleging <b>disparate</b> <b>impact</b> need not demonstrate that discriminatory animus ...", "dateLastCrawled": "2022-01-03T21:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Bias and Discrimination in AI: A Cross-Disciplinary Perspective - IEEE ...", "url": "https://technologyandsociety.org/bias-and-discrimination-in-ai-a-cross-disciplinary-perspective/", "isFamilyFriendly": true, "displayUrl": "https://technologyandsociety.org/bias-and-discrimination-in-ai-a-cross-disciplinary...", "snippet": "For instance, consider a possible case of algorithmic bias in usage, in which an <b>algorithm</b> is <b>biased</b> toward hiring <b>young</b> <b>people</b>. At first glance, it can be considered that the <b>algorithm</b> is discriminating <b>against</b> older <b>people</b>. However, this (<b>biased</b>) <b>algorithm</b> should only be considered to discriminate if the context in which it is intended to be deployed does not justify hiring more <b>young</b> <b>people</b> than older <b>people</b>. Therefore, statistically reductionist approaches, such as estimating the ratio ...", "dateLastCrawled": "2022-01-30T19:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A New Metric for Quantifying <b>Machine</b> <b>Learning</b> Fairness in Healthcare", "url": "https://www.closedloop.ai/post/a-new-metric-for-quantifying-machine-learning-fairness-in-healthcare", "isFamilyFriendly": true, "displayUrl": "https://www.closedloop.ai/post/a-new-metric-for-quantifying-<b>machine</b>-<b>learning</b>-fairness...", "snippet": "The value for <b>disparate</b> <b>impact</b> is .569, well below the standard threshold of .8; this would mean that by this standard this model is <b>biased</b> <b>against</b> males. If instead, group benefit equality is calculated, the score of .875 is observed. While there is room for improvement in servicing this sub-population, one would not classify this model as <b>biased</b> <b>against</b> men.", "dateLastCrawled": "2022-02-01T00:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Programming Fairness in Algorithms</b> | by Matthew Stewart, PhD Researcher ...", "url": "https://towardsdatascience.com/programming-fairness-in-algorithms-4943a13dd9f8", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>programming-fairness-in-algorithms</b>-4943a13dd9f8", "snippet": "<b>Machine</b> <b>learning</b> fairness is a <b>young</b> subfield of <b>machine</b> <b>learning</b> that has been growing in popularity over the last few years in response to the rapid integration of <b>machine</b> <b>learning</b> into social realms. Computer scientists, unlike doctors, are not necessarily trained to consider the ethical implications of their actions. It is only relatively recently (one could argue since the advent of social media) that the designs or inventions of computer scientists were able to take on an ethical ...", "dateLastCrawled": "2022-01-23T21:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Artificial Intelligence and Bias</b> - <b>Regulatory Transparency Project</b>", "url": "https://regproject.org/video/artificial-intelligence-and-bias/", "isFamilyFriendly": true, "displayUrl": "https://regproject.org/video/<b>artificial-intelligence-and-bias</b>", "snippet": "You\u2019re going to get a determination that every group should be protected <b>against</b> <b>disparate</b> <b>impact</b> and not just every group, but every intersection of groups. That requires a lot of shimming of the data to achieve, but once you\u2019ve achieved it, and <b>people</b> are doing that now much more often with synthetic data. They\u2019re just making up data \u2014 a lot of good reasons to make up data, but if you make up the data or if you run <b>a machine</b> <b>learning</b> system in which you say to the <b>machine</b> go ...", "dateLastCrawled": "2022-01-31T09:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Computer algorithms can be biased</b>, but U. team says it can fix problem ...", "url": "https://www.deseret.com/2015/9/8/20571781/computer-algorithms-can-be-biased-but-u-team-says-it-can-fix-problem", "isFamilyFriendly": true, "displayUrl": "https://www.deseret.com/2015/9/8/20571781/<b>computer-algorithms-can-be-biased</b>-but-u-team...", "snippet": "The research determines whether the software algorithms could be prejudiced through the legal definition of <b>disparate</b> <b>impact</b> \u2014 the legal theory that a policy may be considered discriminatory if it has an adverse <b>impact</b> on any group based on race, religion, gender, sexual orientation or other protected status. The team\u2019s research revealed that algorithms could be tested to determine possible bias, ironically <b>using</b> another <b>machine</b>-<b>learning</b> <b>algorithm</b>. If the test reveals a possible problem ...", "dateLastCrawled": "2022-01-24T13:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "The ethics of <b>algorithms</b>: key problems and solutions | SpringerLink", "url": "https://link.springer.com/article/10.1007/s00146-021-01154-8", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00146-021-01154-8", "snippet": "Actions (1) and (2) may be performed by (semi-)autonomous <b>algorithms</b>\u2014such as <b>machine</b> <b>learning</b> (ML) <b>algorithms</b>\u2014and this complicates, (3) the attribution of responsibility for the effects of actions that an <b>algorithm</b> may trigger. Here, ML is of particular interest, as a field which includes deep <b>learning</b> architectures. Computer systems deploying ML <b>algorithms</b> may be described as \u201cautonomous\u201d or \u201csemi-autonomous\u201d, to the extent that their outputs are induced from data and thus, non ...", "dateLastCrawled": "2022-01-30T20:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Machine Learning and the Police: Asking the Right Questions</b> | Policing ...", "url": "https://academic.oup.com/policing/article/15/1/44/5518992", "isFamilyFriendly": true, "displayUrl": "https://academic.oup.com/policing/article/15/1/44/5518992", "snippet": "1 A \u2018model\u2019 is the system of weights that will be trained <b>using</b> <b>learning</b> data and the <b>learning</b> <b>algorithm</b>. The weights are numerical and are used to calculate predictions when given new data. They can be as simple as Y = bX (where b is the weight of input data feature X), or as complex as millions of weights connected to each other through convolutional or recurrent networks and including functions that transform the output of these systems. Commonly throughout this article, we will use ...", "dateLastCrawled": "2022-01-20T01:18:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Artificial Intelligence and <b>Machine</b> <b>Learning</b> for HIV Prevention ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7260108/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7260108", "snippet": "<b>Using</b> a dataset from Twitter, <b>Young</b> et al. applied <b>machine</b> <b>learning</b> to predict tweets defined by a content expert as being related to HIV risk behaviors , suggesting that social media data could augment traditional epidemiologic surveillance by identifying HIV risk behaviors in real time. Although these studies are a proof of concept, the feasibility and <b>impact</b> of AI for HIV prevention in this context are as yet unknown.", "dateLastCrawled": "2022-01-29T01:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Computational Fairness: Preventing <b>Machine</b>-Learned Discrimination", "url": "https://scholarship.tricolib.brynmawr.edu/bitstream/handle/10066/17628/2015FeldmanM.pdf?sequence=1", "isFamilyFriendly": true, "displayUrl": "https://scholarship.tricolib.brynmawr.edu/bitstream/handle/10066/17628/2015FeldmanM...", "snippet": "It may use <b>a machine</b> <b>learning</b> <b>algorithm</b> to Portions of this work were included in Certifying and Removing <b>Disparate</b> <b>Impact</b> [5]. 1. simplify its hiring process. <b>Machine</b> <b>learning</b> algorithms can take data and produce models that can make predictions on future input based on past training data. Models that make discrete outcomes, such as hiring or not hiring an applicant, are called classi ers. The company may take hiring data from previous years { attributes of each applicant and whether or not ...", "dateLastCrawled": "2022-01-19T02:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "AI, <b>Machine</b> <b>Learning</b> &amp; Big Data Laws and Regulations | 3 Artificial ...", "url": "https://www.globallegalinsights.com/practice-areas/ai-machine-learning-and-big-data-laws-and-regulations/3-artificial-intelligence-employment-law-risks-and-considerations", "isFamilyFriendly": true, "displayUrl": "https://www.globallegalinsights.com/practice-areas/ai-<b>machine</b>-<b>learning</b>-and-big-data...", "snippet": "If an <b>algorithm</b> begins producing <b>biased</b> results after reviewing faulty or <b>biased</b> training data consisting of prior decisions made by humans, a ... bring discrimination claims <b>using</b> a <b>disparate</b> <b>impact</b> theory. To succeed on such a claim, the plaintiff must demonstrate that a facially neutral employment practice had a disproportionate effect on a protected group. 68 Unlike a <b>disparate</b> treatment claim, a plaintiff alleging <b>disparate</b> <b>impact</b> need not demonstrate that discriminatory animus ...", "dateLastCrawled": "2022-01-03T21:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A New Metric for Quantifying <b>Machine</b> <b>Learning</b> Fairness in Healthcare", "url": "https://www.closedloop.ai/post/a-new-metric-for-quantifying-machine-learning-fairness-in-healthcare", "isFamilyFriendly": true, "displayUrl": "https://www.closedloop.ai/post/a-new-metric-for-quantifying-<b>machine</b>-<b>learning</b>-fairness...", "snippet": "The value for <b>disparate</b> <b>impact</b> is .569, well below the standard threshold of .8; this would mean that by this standard this model is <b>biased</b> <b>against</b> males. If instead, group benefit equality is calculated, the score of .875 is observed. While there is room for improvement in servicing this sub-population, one would not classify this model as <b>biased</b> <b>against</b> men.", "dateLastCrawled": "2022-02-01T00:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A New Metric for Quantifying <b>Machine</b> <b>Learning</b> Fairness in Healthcare ...", "url": "https://towardsdatascience.com/a-new-metric-for-quantifying-machine-learning-fairness-in-healthcare-closedloop-ai-fc07b9c83487", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-new-metric-for-quantifying-<b>machine</b>-<b>learning</b>-fairness...", "snippet": "<b>Disparate</b> <b>impact</b> occurs when the predicted outcomes are different for different groups. Some examples of when this metric is used are recidivism[4], hiring[5][6], and loan applications[2]. This standard metric accounts for only one factor: the rate at which the <b>algorithm</b> predicts a person should benefit from a particular classification. In the context of healthcare, the standard of <b>disparate</b> <b>impact</b> is entirely inappropriate. The above examples have a common characteristic; every individual ...", "dateLastCrawled": "2022-01-17T14:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Programming Fairness in Algorithms</b> | by Matthew Stewart, PhD Researcher ...", "url": "https://towardsdatascience.com/programming-fairness-in-algorithms-4943a13dd9f8", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>programming-fairness-in-algorithms</b>-4943a13dd9f8", "snippet": "<b>Machine</b> <b>learning</b> fairness is a <b>young</b> subfield of <b>machine</b> <b>learning</b> that has been growing in popularity over the last few years in response to the rapid integration of <b>machine</b> <b>learning</b> into social realms. Computer scientists, unlike doctors, are not necessarily trained to consider the ethical implications of their actions. It is only relatively recently (one could argue since the advent of social media) that the designs or inventions of computer scientists were able to take on an ethical ...", "dateLastCrawled": "2022-01-23T21:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Certifying and removing disparate impact</b> | DeepAI", "url": "https://deepai.org/publication/certifying-and-removing-disparate-impact", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>certifying-and-removing-disparate-impact</b>", "snippet": "The two problems we consider address identifying and removing <b>disparate</b> <b>impact</b>. The <b>disparate</b> <b>impact</b> certification problem is to guarantee that, given D, any classification <b>algorithm</b> aiming to predict some C \u2032 (which is potentially different from the given C) from Y would not have <b>disparate</b> <b>impact</b>. By certifying any outcomes C \u2032, and not the process by which they were reached, we follow legal precedent in making no judgment on the <b>algorithm</b> itself, and additionally ensure that ...", "dateLastCrawled": "2022-01-22T13:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Artificial Intelligence and Bias</b> - <b>Regulatory Transparency Project</b>", "url": "https://regproject.org/video/artificial-intelligence-and-bias/", "isFamilyFriendly": true, "displayUrl": "https://regproject.org/video/<b>artificial-intelligence-and-bias</b>", "snippet": "<b>People</b> have been looking at <b>machine</b> <b>learning</b> since the \u201990s if not earlier. It\u2019s just that in the past few years, we\u2019ve had a revolution in the capability of <b>machine</b> <b>learning</b>, not because the underlying algorithms have gotten better but because of a quirk of happenstance. And that is that the underlying problem behind <b>machine</b> <b>learning</b>, what we call dense matrix multiply \u2014 take a whole bunch of numbers, multiply them together to get a whole bunch of numbers \u2014 is something that works ...", "dateLastCrawled": "2022-01-31T09:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "AI is convicting criminals and determining jail time, but is it fair ...", "url": "https://www.weforum.org/agenda/2018/11/algorithms-court-criminals-jail-time-fair/", "isFamilyFriendly": true, "displayUrl": "https://www.weforum.org/agenda/2018/11/<b>algorithms</b>-court-criminals-jail-time-fair", "snippet": "<b>Machine</b> <b>learning</b> systems need to avoid unjust effects on individuals, especially impacts related to social and physical vulnerabilities, and other sensitive attributes. These could include race, ethnicity, gender, nationality, sexual orientation, religion and political beliefs. The overall fairness of an <b>algorithm</b> must be judged by how it impacts the most vulnerable <b>people</b> affected by it.", "dateLastCrawled": "2022-01-30T23:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Machine Learning and the Police: Asking the Right Questions</b> | Policing ...", "url": "https://academic.oup.com/policing/article/15/1/44/5518992", "isFamilyFriendly": true, "displayUrl": "https://academic.oup.com/policing/article/15/1/44/5518992", "snippet": "1 A \u2018model\u2019 is the system of weights that will be trained <b>using</b> <b>learning</b> data and the <b>learning</b> <b>algorithm</b>. The weights are numerical and are used to calculate predictions when given new data. They can be as simple as Y = bX (where b is the weight of input data feature X), or as complex as millions of weights connected to each other through convolutional or recurrent networks and including functions that transform the output of these systems. Commonly throughout this article, we will use ...", "dateLastCrawled": "2022-01-20T01:18:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Certifying and removing disparate impact</b> | DeepAI", "url": "https://deepai.org/publication/certifying-and-removing-disparate-impact", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>certifying-and-removing-disparate-impact</b>", "snippet": "The two problems we consider address identifying and removing <b>disparate</b> <b>impact</b>. The <b>disparate</b> <b>impact</b> certification problem is to guarantee that, given D, any classification <b>algorithm</b> aiming to predict some C \u2032 (which is potentially different from the given C) from Y would not have <b>disparate</b> <b>impact</b>. By certifying any outcomes C \u2032, and not the process by which they were reached, we follow legal precedent in making no judgment on the <b>algorithm</b> itself, and additionally ensure that ...", "dateLastCrawled": "2022-01-22T13:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Review into bias in algorithmic decision-making</b> - <b>GOV.UK</b>", "url": "https://www.gov.uk/government/publications/cdei-publishes-review-into-bias-in-algorithmic-decision-making/main-report-cdei-review-into-bias-in-algorithmic-decision-making", "isFamilyFriendly": true, "displayUrl": "https://<b>www.gov.uk</b>/government/publications/cdei-publishes-review-into-bias-in...", "snippet": "Second, <b>A machine</b> <b>learning</b> <b>algorithm</b> is chosen, and uses historical data (e.g. a set of past input data (e.g. a set of past input data, the decisions reached) to build a model, optimising <b>against</b> ...", "dateLastCrawled": "2022-02-03T02:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "What does <b>it mean for an algorithm to be fair</b>? \u2013 Math \u2229 Programming", "url": "https://jeremykun.com/2015/07/13/what-does-it-mean-for-an-algorithm-to-be-fair/", "isFamilyFriendly": true, "displayUrl": "https://jeremykun.com/2015/07/13/what-does-<b>it-mean-for-an-algorithm-to-be-fair</b>", "snippet": "In the US the existing legal theory is called <b>disparate</b> <b>impact</b>, ... Part of the misunderstanding is that <b>people</b> don\u2019t realize <b>machine</b> <b>learning</b> relies on data which is labeled (at the end of some long chain) by humans making decisions. And <b>a machine</b> <b>learning</b> <b>algorithm</b> is designed to find majority trends in data and spit them out as hypotheses. So anyone with a basic understanding of <b>machine</b> <b>learning</b> already knows that algorithms <b>can</b> encode (this is what I mean by encode, perpetuate) human ...", "dateLastCrawled": "2022-01-14T21:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) <b>Causation in Antidiscrimination Law: Beyond Intent Versus</b> <b>Impact</b>", "url": "https://www.researchgate.net/publication/228166192_Causation_in_Antidiscrimination_Law_Beyond_Intent_Versus_Impact", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/228166192_Causation_in_Antidiscrimination_Law...", "snippet": "The discovery of discriminatory bias in human or automated decision making is a task of increasing importance and difficulty, exacerbated by the pervasive use of <b>machine</b> <b>learning</b> and data mining.", "dateLastCrawled": "2022-01-08T07:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Artificial Intelligence and Bias</b> - <b>Regulatory Transparency Project</b>", "url": "https://regproject.org/video/artificial-intelligence-and-bias/", "isFamilyFriendly": true, "displayUrl": "https://regproject.org/video/<b>artificial-intelligence-and-bias</b>", "snippet": "And it\u2019s really a process known as <b>machine</b> <b>learning</b>. And this is actually a very old <b>thought</b>. <b>People</b> have been looking at <b>machine</b> <b>learning</b> since the \u201990s if not earlier. It\u2019s just that in the past few years, we\u2019ve had a revolution in the capability of <b>machine</b> <b>learning</b>, not because the underlying algorithms have gotten better but because of a quirk of happenstance. And that is that the underlying problem behind <b>machine</b> <b>learning</b>, what we call dense matrix multiply \u2014 take a whole ...", "dateLastCrawled": "2022-01-31T09:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "AI is convicting criminals and determining jail time, but is it fair ...", "url": "https://www.weforum.org/agenda/2018/11/algorithms-court-criminals-jail-time-fair/", "isFamilyFriendly": true, "displayUrl": "https://www.weforum.org/agenda/2018/11/<b>algorithms</b>-court-criminals-jail-time-fair", "snippet": "This is problematic, because <b>machine</b> <b>learning</b> models are only as reliable as the data they\u2019re trained on.If the underlying data is <b>biased</b> in any form, there is a risk that structural inequalities and unfair biases are not just replicated, but also amplified.In this regard, AI engineers must be especially wary of their blind spots and implicit assumptions; it is not just the choice of <b>machine</b> <b>learning</b> techniques that matters, but also all the small decisions about finding, organising and ...", "dateLastCrawled": "2022-01-30T23:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Affirmative</b> Algorithms: The Legal Grounds for Fairness as Awareness ...", "url": "https://lawreviewblog.uchicago.edu/2020/10/30/aa-ho-xiang/", "isFamilyFriendly": true, "displayUrl": "https://lawreviewblog.uchicago.edu/2020/10/30/aa-ho-xiang", "snippet": "A substantial literature has emerged within <b>machine</b> <b>learning</b> around remedies for algorithmic bias, with the consensus position being that <b>machine</b> bias is best addressed through awareness of such \u201cprotected groups.\u201d 3 We use \u201cprotected groups\u201d to refer to groups conventionally protected under various antidiscrimination provisions, such as the Constitution\u2019s Equal Protection Clause, Title VII of the Civil Rights Act, the Fair Housing Act, and the like. An important debate concerns ...", "dateLastCrawled": "2022-01-31T02:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Recommendations and future directions for supervised <b>machine</b> <b>learning</b> ...", "url": "https://www.nature.com/articles/s41398-019-0607-2", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41398-019-0607-2", "snippet": "<b>A machine</b> <b>learning</b> pipeline <b>can</b> <b>be thought</b> of as an object that sequentially chains together a list of transformers and a final estimator into one object. This sequential chaining has three ...", "dateLastCrawled": "2021-08-13T03:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "The ethics of <b>algorithms</b>: key problems and solutions | SpringerLink", "url": "https://link.springer.com/article/10.1007/s00146-021-01154-8", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00146-021-01154-8", "snippet": "Actions (1) and (2) may be performed by (semi-)autonomous <b>algorithms</b>\u2014such as <b>machine</b> <b>learning</b> (ML) <b>algorithms</b>\u2014and this complicates, (3) the attribution of responsibility for the effects of actions that an <b>algorithm</b> may trigger. Here, ML is of particular interest, as a field which includes deep <b>learning</b> architectures. Computer systems deploying ML <b>algorithms</b> may be described as \u201cautonomous\u201d or \u201csemi-autonomous\u201d, to the extent that their outputs are induced from data and thus, non ...", "dateLastCrawled": "2022-01-30T20:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(PDF) <b>Machine</b> <b>Learning</b>: The New &#39;<b>Big Thing&#39; for Competitive Advantage</b>", "url": "https://www.researchgate.net/publication/327215281_Machine_Learning_The_New_'Big_Thing'_for_Competitive_Advantage", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/327215281_<b>Machine</b>_<b>Learning</b>_The_New_", "snippet": "Abstract: While <b>machine</b> <b>learning</b> (ML) has existed for a long time, the. business world\u2019s focus on it may seem like an overnigh t develop ment. The. technology has been steadily growing since the ...", "dateLastCrawled": "2022-02-03T07:02:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A New Metric for Quantifying <b>Machine</b> <b>Learning</b> Fairness in Healthcare ...", "url": "https://towardsdatascience.com/a-new-metric-for-quantifying-machine-learning-fairness-in-healthcare-closedloop-ai-fc07b9c83487", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-new-metric-for-quantifying-<b>machine</b>-<b>learning</b>-fairness...", "snippet": "<b>Disparate</b> <b>impact</b> occurs when the predicted outcomes are different for different groups. Some examples of when this metric is used are recidivism[4], hiring[5][6], and loan applications[2]. This standard metric accounts for only one factor: the rate at which the <b>algorithm</b> predicts a person should benefit from a particular classification. In the context of healthcare, the standard of <b>disparate</b> <b>impact</b> is entirely inappropriate. The above examples have a common characteristic; every individual ...", "dateLastCrawled": "2022-01-17T14:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A New Metric for Quantifying <b>Machine</b> <b>Learning</b> Fairness in Healthcare", "url": "https://www.closedloop.ai/post/a-new-metric-for-quantifying-machine-learning-fairness-in-healthcare", "isFamilyFriendly": true, "displayUrl": "https://www.closedloop.ai/post/a-new-metric-for-quantifying-<b>machine</b>-<b>learning</b>-fairness...", "snippet": "The value for <b>disparate</b> <b>impact</b> is .569, well below the standard threshold of .8; this would mean that by this standard this model is <b>biased</b> <b>against</b> males. If instead, group benefit equality is calculated, the score of .875 is observed. While there is room for improvement in servicing this sub-population, one would not classify this model as <b>biased</b> <b>against</b> men.", "dateLastCrawled": "2022-02-01T00:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Artificial Intelligence and <b>Machine</b> <b>Learning</b> for HIV Prevention ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7260108/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7260108", "snippet": "<b>Using</b> a dataset from Twitter, <b>Young</b> et al. applied <b>machine</b> <b>learning</b> to predict tweets defined by a content expert as being related to HIV risk behaviors , suggesting that social media data could augment traditional epidemiologic surveillance by identifying HIV risk behaviors in real time. Although these studies are a proof of concept, the feasibility and <b>impact</b> of AI for HIV prevention in this context are as yet unknown.", "dateLastCrawled": "2022-01-29T01:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Bias and Discrimination in AI: A Cross-Disciplinary Perspective - IEEE ...", "url": "https://technologyandsociety.org/bias-and-discrimination-in-ai-a-cross-disciplinary-perspective/", "isFamilyFriendly": true, "displayUrl": "https://technologyandsociety.org/bias-and-discrimination-in-ai-a-cross-disciplinary...", "snippet": "For instance, consider a possible case of algorithmic bias in usage, in which an <b>algorithm</b> is <b>biased</b> toward hiring <b>young</b> <b>people</b>. At first glance, it <b>can</b> be considered that the <b>algorithm</b> is discriminating <b>against</b> older <b>people</b>. However, this (<b>biased</b>) <b>algorithm</b> should only be considered to discriminate if the context in which it is intended to be deployed does not justify hiring more <b>young</b> <b>people</b> than older <b>people</b>. Therefore, statistically reductionist approaches, such as estimating the ratio ...", "dateLastCrawled": "2022-01-30T19:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Certifying and removing disparate impact</b> | DeepAI", "url": "https://deepai.org/publication/certifying-and-removing-disparate-impact", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>certifying-and-removing-disparate-impact</b>", "snippet": "The two problems we consider address identifying and removing <b>disparate</b> <b>impact</b>. The <b>disparate</b> <b>impact</b> certification problem is to guarantee that, given D, any classification <b>algorithm</b> aiming to predict some C \u2032 (which is potentially different from the given C) from Y would not have <b>disparate</b> <b>impact</b>. By certifying any outcomes C \u2032, and not the process by which they were reached, we follow legal precedent in making no judgment on the <b>algorithm</b> itself, and additionally ensure that ...", "dateLastCrawled": "2022-01-22T13:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Proxy Discrimination in the <b>Age of Artificial Intelligence and Big Data</b> ...", "url": "https://ilr.law.uiowa.edu/print/volume-105-issue-3/proxy-discrimination-in-the-age-of-artificial-intelligence-and-big-data/", "isFamilyFriendly": true, "displayUrl": "https://ilr.law.uiowa.edu/print/volume-105-issue-3/proxy-discrimination-in-the-age-of...", "snippet": "For arguments about the desirability of <b>disparate</b> <b>impact</b> in insurance, see generally Matthew Jordan Cochran, Fairness in Disparity: Challenging the Application of <b>Disparate</b> <b>Impact</b> Theory in Fair Housing Claims <b>Against</b> Insurers, 21 Geo. Mason U. C.R. L.J. 159 (2011) (discussing the use of <b>disparate</b> <b>impact</b> theory under Title VII and potential applicability to Fair Housing Act claims <b>against</b> insurers); Dana L. Kaersvang, Note, The Fair Housing Act and <b>Disparate</b> <b>Impact</b> in Homeowners Insurance ...", "dateLastCrawled": "2022-02-02T02:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) <b>The Biased Algorithm: Disparate Impact for Hispanics</b>", "url": "https://www.researchgate.net/publication/327745793_The_Biased_Algorithm_Disparate_Impact_for_Hispanics", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/327745793_The_<b>Biased</b>_<b>Algorithm</b>_<b>Disparate</b>...", "snippet": "Working Paper] The <b>Biased</b> <b>Algorithm</b> 11 for at least a fe w of the recidivism outc omes tested. 68 Despite the seeming importance of these findings, the authors do not expound upon these results.", "dateLastCrawled": "2021-12-15T21:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) <b>Digital Discrimination</b> - ResearchGate", "url": "https://www.researchgate.net/publication/336792693_Digital_Discrimination", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/336792693_<b>Digital_Discrimination</b>", "snippet": "o Usage: a non-discriminatory <b>machine</b> <b>learning</b> <b>algorithm</b> <b>can</b> also lead to discrimination when it is used in a situation for which it was not intended. For example, an <b>algorithm</b>", "dateLastCrawled": "2022-02-02T17:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Artificial Intelligence and Bias</b> - <b>Regulatory Transparency Project</b>", "url": "https://regproject.org/video/artificial-intelligence-and-bias/", "isFamilyFriendly": true, "displayUrl": "https://regproject.org/video/<b>artificial-intelligence-and-bias</b>", "snippet": "Nicholas Weaver: The other problem is is there is enough evidence out there that there is a tendency to like that <b>machine</b> <b>learning</b> <b>can</b> launder stuff and <b>can</b> find confounding variables very easily. So say your training data explicitly excludes race but includes zip code of residence. That\u2019s going to be a very strong proxy. If it also includes name as well as zip code of residence, that is going to be a really strong proxy. And the <b>machine</b> learner is going to instantly figure out the proxies ...", "dateLastCrawled": "2022-01-31T09:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Machine Learning and the Police: Asking the Right Questions</b> | Policing ...", "url": "https://academic.oup.com/policing/article/15/1/44/5518992", "isFamilyFriendly": true, "displayUrl": "https://academic.oup.com/policing/article/15/1/44/5518992", "snippet": "It found that <b>using</b> the PredPol <b>algorithm</b>, \u2018black <b>people</b> would be targeted by predictive policing at roughly twice the rate of whites\u2019, despite estimates showing roughly equal levels of drug use (Lum and Isaac, 2016, p. 18). Low-income <b>people</b> and non-Whites other than Blacks would also be disproportionately targeted, that is, over-policed. This example shows how input data used to train machines and humans alike <b>can</b> lead to invalid models and unfair practice. In this case, the invalid ...", "dateLastCrawled": "2022-01-20T01:18:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A New Metric for Quantifying <b>Machine</b> <b>Learning</b> Fairness in Healthcare", "url": "https://www.closedloop.ai/post/a-new-metric-for-quantifying-machine-learning-fairness-in-healthcare", "isFamilyFriendly": true, "displayUrl": "https://www.closedloop.ai/post/a-new-metric-for-quantifying-<b>machine</b>-<b>learning</b>-fairness...", "snippet": "Often, the approach to fairness draws upon the legal standard of <b>disparate</b> <b>impact</b>[2][3]. <b>Disparate</b> <b>impact</b> occurs when the predicted outcomes are different for different groups. Some examples of when this metric is used are recidivism[4], hiring[5][6], and loan applications[2]. This standard metric accounts for only one factor: the rate at which the algorithm predicts a person should benefit from a particular classification.", "dateLastCrawled": "2022-02-01T00:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A New Metric for Quantifying <b>Machine</b> <b>Learning</b> Fairness in Healthcare ...", "url": "https://towardsdatascience.com/a-new-metric-for-quantifying-machine-learning-fairness-in-healthcare-closedloop-ai-fc07b9c83487", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-new-metric-for-quantifying-<b>machine</b>-<b>learning</b>-fairness...", "snippet": "<b>Disparate</b> <b>impact</b> occurs when the predicted outcomes are different for different groups. Some examples of when this metric is used are recidivism[4], hiring[5][6], and loan applications[2]. This standard metric accounts for only one factor: the rate at which the algorithm predicts a person should benefit from a particular classification. In the context of healthcare, the standard of <b>disparate</b> <b>impact</b> is entirely inappropriate. The above examples have a common characteristic; every individual ...", "dateLastCrawled": "2022-01-17T14:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Responsible machine learning</b> protects intellectual property | World ...", "url": "https://www.weforum.org/agenda/2021/03/responsible-machine-learning-that-protects-intellectual-property/", "isFamilyFriendly": true, "displayUrl": "https://www.weforum.org/agenda/2021/03/<b>responsible-machine-learning</b>-that-protects...", "snippet": "Given <b>machine</b> <b>learning</b>\u2019s complexity and interdisciplinary nature, executives should employ a wide variety of approaches to manage the associated risks, which include building risk management into model development and applying holistic risk frameworks that leverage and adapt principles used in managing other types of enterprise risk. Executives must also simply step back regularly to consider the broad implications for employees and society when using ML.", "dateLastCrawled": "2022-01-29T18:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Assessing <b>Disparate</b> <b>Impact</b> of Personalized Interventions ...", "url": "https://proceedings.neurips.cc/paper/8603-assessing-disparate-impact-of-personalized-interventions-identifiability-and-bounds.pdf", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/8603-assessing-<b>disparate</b>-<b>impact</b>-of-personalized...", "snippet": "result in <b>disparate</b> <b>impact</b> (with regards to social welfare) for the same reasons that these disparities occur in <b>machine</b> <b>learning</b> classi\ufb01cation models [21]. (See Appendix C for an expanded discussion on our use of the term \u201c<b>disparate</b> <b>impact</b>.\u201d) However, in the problem of personalized interventions, the \u201cfundamental problem of causal inference,\u201d that outcomes are not observed for interventions not administered, poses a fundamental challenge for evaluating the fairness of any ...", "dateLastCrawled": "2021-09-17T16:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Feature Engineering for Machine Learning</b>: Why and How | by ...", "url": "https://medium.com/analytics-vidhya/feature-engineering-for-machine-learning-stem-to-shtem-submission-76903112e437", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/<b>feature-engineering-for-machine-learning</b>-stem-to...", "snippet": "Here\u2019s a simple <b>analogy</b>: a student named Timmy, analogous to a supervised <b>machine</b> <b>learning</b> model, has spent the last few weeks studying for a math test so that he can answer questions correctly ...", "dateLastCrawled": "2021-09-13T20:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Machine Learning</b> Glossary | <b>Google Developers</b>", "url": "https://developers.google.com/machine-learning/glossary/", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine-learning</b>/glossary", "snippet": "A <b>machine learning</b> technique that iteratively combines a set of simple and not very accurate classifiers ... <b>disparate</b> <b>impact</b>. #fairness. Making decisions about people that <b>impact</b> different population subgroups disproportionately. This usually refers to situations where an algorithmic decision-making process harms or benefits some subgroups more than others. For example, suppose an algorithm that determines a Lilliputian&#39;s eligibility for a miniature-home loan is more likely to classify them ...", "dateLastCrawled": "2022-02-03T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Algorithmic injustice: a relational ethics approach", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7892355/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7892355", "snippet": "<b>Machine</b> classification and prediction are practices that act directly upon the world and result in tangible <b>impact</b>.64 Various companies, institutes, and governments use <b>machine</b>-<b>learning</b> systems across a variety of areas. These systems process people&#39;s behaviors, actions, and the social world at large. The <b>machine</b>-detected patterns often provide \u201canswers\u201d to fuzzy, contingent, and open-ended questions. These \u201canswers\u201d neither reveal any causal relations nor provide explanation on why ...", "dateLastCrawled": "2022-01-26T01:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Classification - Fairness and <b>machine</b> <b>learning</b>", "url": "https://fairmlbook.org/classification.html", "isFamilyFriendly": true, "displayUrl": "https://fairmlbook.org/classification.html", "snippet": "However, if such a model of long-term <b>impact</b> model were available, directly optimizing for long-term benefit may be a more effective intervention than to impose a general and crude demographic criterion. Liu et al., \u201cDelayed <b>Impact</b> of Fair <b>Machine</b> <b>Learning</b>,\u201d in Proc. 35 Th ICML, 2018, 3156\u201364.", "dateLastCrawled": "2022-02-03T07:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Using analogies to develop new products: What if a locomotive</b> was like ...", "url": "https://sloanreview.mit.edu/article/using-analogies-to-develop-new-products-what-if-a-locomotive-was-like-a-lego-construction/", "isFamilyFriendly": true, "displayUrl": "https://sloanreview.mit.edu/article/<b>using-analogies-to-develop-new-products</b>-what-if-a...", "snippet": "The results ranged from transferring technical solutions from one similar realm to another \u201d what the researchers call \u201cnear analogies\u201d \u201d to analogies between very <b>disparate</b> things, such as thinking about an egg as an <b>analogy</b> for the kind of protection fork-lift truck cabins should provide their drivers \u201d or shark skin as an <b>analogy</b> for what a bathing suit should be like. These the authors term \u201cfar analogies.\u201d They found that design teams tended to use \u201cfar analogies\u201d when ...", "dateLastCrawled": "2022-01-09T22:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Democratizing Algorithmic Fairness</b> | SpringerLink", "url": "https://link.springer.com/article/10.1007/s13347-019-00355-w", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s13347-019-00355-w", "snippet": "<b>Machine</b> <b>learning</b> algorithms can now identify patterns and correlations in (big) datasets and predict outcomes based on the identified patterns and correlations. They can then generate decisions in accordance with the outcomes predicted, and decision-making processes can thereby be automated. Algorithms can inherit questionable values from datasets and acquire biases in the course of (<b>machine</b>) <b>learning</b>. While researchers and developers have taken the problem of algorithmic bias seriously, the ...", "dateLastCrawled": "2022-02-03T18:15:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(disparate impact)  is like +(using a machine learning algorithm that is biased against young people)", "+(disparate impact) is similar to +(using a machine learning algorithm that is biased against young people)", "+(disparate impact) can be thought of as +(using a machine learning algorithm that is biased against young people)", "+(disparate impact) can be compared to +(using a machine learning algorithm that is biased against young people)", "machine learning +(disparate impact AND analogy)", "machine learning +(\"disparate impact is like\")", "machine learning +(\"disparate impact is similar\")", "machine learning +(\"just as disparate impact\")", "machine learning +(\"disparate impact can be thought of as\")", "machine learning +(\"disparate impact can be compared to\")"]}