{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Naive Bayes and Text Classification - Dr. Sebastian Raschka", "url": "https://sebastianraschka.com/Articles/2014_naive_bayes_1.html", "isFamilyFriendly": true, "displayUrl": "https://sebastianraschka.com/Articles/2014_naive_bayes_1.html", "snippet": "One <b>assumption</b> that Bayes classifiers make is that the samples are <b>i.i.d</b>. The abbreviation <b>i.i.d</b>. stands for \u201cindependent and identically distributed\u201d and describes random variables that are independent from one another and are <b>drawn</b> from a similar probability <b>distribution</b>. Independence means that the probability of one observation does not ...", "dateLastCrawled": "2022-01-31T01:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Google Colab", "url": "https://colab.research.google.com/github/d2l-ai/d2l-en-colab/blob/master/chapter_multilayer-perceptrons/underfit-overfit.ipynb", "isFamilyFriendly": true, "displayUrl": "https://colab.research.google.com/github/d2l-ai/d2l-en-colab/blob/master/chapter...", "snippet": "This is commonly called the <b>i.i.d</b>. <b>assumption</b>, which means that the process that samples our <b>data</b> has no memory. In other words, the second example <b>drawn</b> and the third <b>drawn</b> are no more correlated than the second and the two-millionth sample <b>drawn</b>.", "dateLastCrawled": "2021-12-30T21:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "4.4. Model Selection, <b>Underfitting</b>, and Overfitting \u2014 Dive into Deep ...", "url": "https://d2l.ai/chapter_multilayer-perceptrons/underfit-overfit.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_multilayer-perceptrons/underfit-overfit.html", "snippet": "This is commonly called the <b>i.i.d</b>. <b>assumption</b>, which means that the process that samples our <b>data</b> has no memory. In other words, the second example <b>drawn</b> and the third <b>drawn</b> are no more correlated than the second and the two-millionth sample <b>drawn</b>.", "dateLastCrawled": "2022-01-30T17:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 3, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Independent and identically distributed</b> random variables - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Independent_and_identically_distributed</b>_random_variables", "snippet": "The <b>i.i.d</b>. <b>assumption</b> is also used in central limit theorem, which states that the probability <b>distribution</b> of the sum (or average) of <b>i.i.d</b>. variables with finite variance approaches a <b>normal</b> <b>distribution</b>. Often the <b>i.i.d</b>. <b>assumption</b> arises in the context of sequences of random variables. Then &quot;<b>independent and identically distributed</b>&quot; implies that an element in the sequence is independent of the random variables that came before it. In this way, an <b>i.i.d</b>. sequence is different from a Markov ...", "dateLastCrawled": "2022-02-03T02:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Bayes\u2019 classifier with <b>Maximum Likelihood</b> ... - Towards <b>Data</b> Science", "url": "https://towardsdatascience.com/bayes-classifier-with-maximum-likelihood-estimation-4b754b641488", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/bayes-classifier-with-<b>maximum-likelihood</b>-estimation-4b...", "snippet": "When initial <b>data</b> are given, <b>assumption</b> here is that <b>data</b> are picked INDEPENDENTLY and IDENTICALLY DISTRIBUTED (<b>i.i.d</b>.) Then the <b>data</b> type is checked to decide what probability model can be used. For example, if the <b>data</b> is coin tosses, Bernoulli model is used, if it\u2019s dice rolls, multinomial model can be used. In my example below, Gaussian model, which is most common phenomenon, is used. In order to make sure the <b>distribution</b> is <b>normal</b>, the normality test is often done. In the learning ...", "dateLastCrawled": "2022-01-31T03:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "9 | PROBABILISTIC M - CIML", "url": "http://ciml.info/dl/v0_99/ciml-v0_99-ch09.pdf", "isFamilyFriendly": true, "displayUrl": "ciml.info/dl/v0_99/ciml-v0_99-ch09.pdf", "snippet": "with the <b>assumption</b> that all the <b>training</b> <b>data</b> <b>is drawn</b> from the same <b>distribution</b> Dleads to the <b>i.i.d</b>. <b>assumption</b> or independently and identically distributed <b>assumption</b>. This is a key <b>assumption</b> in al-most all of machine learning. 9.2 Statistical Estimation Suppose you need to model a coin that is possibly biased (you can think of this as modeling the label in a binary classi\ufb01cation problem), and that you observe <b>data</b> HHTH (where H means a \ufb02ip came up heads. 118 a course in machine ...", "dateLastCrawled": "2022-01-29T00:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A Course in Machine Learning", "url": "http://ciml.info/dl/v0_9/ciml-v0_9-ch07.pdf", "isFamilyFriendly": true, "displayUrl": "ciml.info/dl/v0_9/ciml-v0_9-ch07.pdf", "snippet": "with the <b>assumption</b> that all the <b>training</b> <b>data</b> <b>is drawn</b> from the same <b>distribution</b> Dleads to the <b>i.i.d</b>. <b>assumption</b> or independently and identically distributed <b>assumption</b>. This is a key <b>assumption</b> in al-most all of machine learning. 7.2 Statistical Estimation Suppose you need to model a coin that is possibly biased (you can think of this as modeling the label in a binary classi\ufb01cation problem), and that you observe <b>data</b> HHTH (where H means a \ufb02ip came up heads and T means it came up tails ...", "dateLastCrawled": "2022-01-11T04:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Can machine learning find <b>extraordinary materials</b>? - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S0927025619307979", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0927025619307979", "snippet": "For a materials scientist, the <b>i.i.d</b>. <b>assumption</b> implies the <b>training</b> <b>data</b> fairly represents the full diversity of reality. This is clearly not the case due to <b>data</b> set bias. For example, some compounds are easier to synthesize and simulate or may be of more interest to researchers due to cost, performance in applications, or novelty. For this reason one must ask: do we have the information necessary, on a physical level, to even determine whether a material is extraordinary given highly ...", "dateLastCrawled": "2021-12-06T16:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Statistical process monitoring</b> as a big <b>data</b> analytics tool for smart ...", "url": "https://www.sciencedirect.com/science/article/pii/S0959152417301257", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0959152417301257", "snippet": "The underlying <b>assumption</b> made for the PCA and PLS based MSPM methods is that the <b>data</b> are independent and identically distributed (<b>i.i.d</b>.) samples <b>drawn</b> from a multivariate Gaussian <b>distribution</b>, which implies that the process <b>data</b> with the following characteristics would violate this <b>assumption</b>: (1) multimodal <b>distribution</b>; (2) dynamics; (3) nonlinear relationships between variables; (4) non-Gaussianity; (5) time-varying characteristics; (6) other characteristics such as outliers, gross ...", "dateLastCrawled": "2022-01-15T19:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>10-701 Machine Learning: Assignment 1</b>", "url": "https://cs.cmu.edu/~aarti/Class/10701_Spring14/assignments/hw1_solution.pdf", "isFamilyFriendly": true, "displayUrl": "https://cs.cmu.edu/~aarti/Class/10701_Spring14/assignments/hw1_solution.pdf", "snippet": "(a) (1 pts) This problem is equivalent to estimating the mean parameter of a Bernoulli <b>distribution</b> from <b>i.i.d</b>. <b>data</b>. Therefore, the MLE estimation is ^ = n 1 N, where n 1 is the number of students who answered Yes and Nis the total number of students. (b) (4 pts) Let X i = 1 if a student answered yes, and let X i = 0 if the answer was no. [If ...", "dateLastCrawled": "2022-02-02T13:00:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Naive Bayes and Text Classification - Dr. Sebastian Raschka", "url": "https://sebastianraschka.com/Articles/2014_naive_bayes_1.html", "isFamilyFriendly": true, "displayUrl": "https://sebastianraschka.com/Articles/2014_naive_bayes_1.html", "snippet": "One <b>assumption</b> that Bayes classifiers make is that the samples are <b>i.i.d</b>. The abbreviation <b>i.i.d</b>. stands for \u201cindependent and identically distributed\u201d and describes random variables that are independent from one another and are <b>drawn</b> from a <b>similar</b> probability <b>distribution</b>. Independence means that the probability of one observation does not ...", "dateLastCrawled": "2022-01-31T01:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Learning Classifiers When the <b>Training</b> <b>Data</b> Is Not <b>IID</b>", "url": "https://www.ijcai.org/Proceedings/07/Papers/121.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.ijcai.org/Proceedings/07/Papers/121.pdf", "snippet": "Relaxing this <b>i.i.d</b>. <b>assumption</b>, we consider algorithms from the statistics litera-ture for the more realistic situation where batches or sub-groups of <b>training</b> samples may have inter- nal correlations, although the samples from dif-ferent batches may be considered to be uncorre-lated. Next, we propose simpler (more ef\ufb01cient) variantsthatscale well to largedatasets; theoretical results from the literature are provided to support their validity. Experimental results from real-life computer ...", "dateLastCrawled": "2022-02-03T16:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 2, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Independent and identically distributed</b> random variables - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Independent_and_identically_distributed</b>_random_variables", "snippet": "The <b>i.i.d</b>. <b>assumption</b> is also used in central limit theorem, which states that the probability <b>distribution</b> of the sum (or average) of <b>i.i.d</b>. variables with finite variance approaches a <b>normal</b> <b>distribution</b>. Often the <b>i.i.d</b>. <b>assumption</b> arises in the context of sequences of random variables. Then &quot;<b>independent and identically distributed</b>&quot; implies that an element in the sequence is independent of the random variables that came before it. In this way, an <b>i.i.d</b>. sequence is different from a Markov ...", "dateLastCrawled": "2022-02-03T02:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "9 | PROBABILISTIC M - CIML", "url": "http://ciml.info/dl/v0_99/ciml-v0_99-ch09.pdf", "isFamilyFriendly": true, "displayUrl": "ciml.info/dl/v0_99/ciml-v0_99-ch09.pdf", "snippet": "with the <b>assumption</b> that all the <b>training</b> <b>data</b> <b>is drawn</b> from the same <b>distribution</b> Dleads to the <b>i.i.d</b>. <b>assumption</b> or independently and identically distributed <b>assumption</b>. This is a key <b>assumption</b> in al-most all of machine learning. 9.2 Statistical Estimation Suppose you need to model a coin that is possibly biased (you can think of this as modeling the label in a binary classi\ufb01cation problem), and that you observe <b>data</b> HHTH (where H means a \ufb02ip came up heads. 118 a course in machine ...", "dateLastCrawled": "2022-01-29T00:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Bayes\u2019 classifier with <b>Maximum Likelihood</b> ... - Towards <b>Data</b> Science", "url": "https://towardsdatascience.com/bayes-classifier-with-maximum-likelihood-estimation-4b754b641488", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/bayes-classifier-with-<b>maximum-likelihood</b>-estimation-4b...", "snippet": "When initial <b>data</b> are given, <b>assumption</b> here is that <b>data</b> are picked INDEPENDENTLY and IDENTICALLY DISTRIBUTED (<b>i.i.d</b>.) Then the <b>data</b> type is checked to decide what probability model can be used. For example, if the <b>data</b> is coin tosses, Bernoulli model is used, if it\u2019s dice rolls, multinomial model can be used. In my example below, Gaussian model, which is most common phenomenon, is used. In order to make sure the <b>distribution</b> is <b>normal</b>, the normality test is often done. In the learning ...", "dateLastCrawled": "2022-01-31T03:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A Course in Machine Learning", "url": "http://ciml.info/dl/v0_9/ciml-v0_9-ch07.pdf", "isFamilyFriendly": true, "displayUrl": "ciml.info/dl/v0_9/ciml-v0_9-ch07.pdf", "snippet": "with the <b>assumption</b> that all the <b>training</b> <b>data</b> <b>is drawn</b> from the same <b>distribution</b> Dleads to the <b>i.i.d</b>. <b>assumption</b> or independently and identically distributed <b>assumption</b>. This is a key <b>assumption</b> in al-most all of machine learning. 7.2 Statistical Estimation Suppose you need to model a coin that is possibly biased (you can think of this as modeling the label in a binary classi\ufb01cation problem), and that you observe <b>data</b> HHTH (where H means a \ufb02ip came up heads and T means it came up tails ...", "dateLastCrawled": "2022-01-11T04:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Under review as a conference paper at ICLR 2019", "url": "https://openreview.net/pdf?id=S1giro05t7", "isFamilyFriendly": true, "displayUrl": "https://openreview.net/pdf?id=S1giro05t7", "snippet": "different from the <b>training</b> <b>data</b>. 1 INTRODUCTION In machine learning and computer vision, the <b>i.i.d</b>. <b>assumption</b>, that <b>training</b> and test sets are sampled from the same <b>distribution</b> (henceforth \u201cfamiliar\u201d <b>distribution</b>), is so prevalent as to be left unwritten. In experiments, it is easy to satisfy the <b>i.i.d</b>. condition by randomly sampling <b>training</b> and test <b>data</b> from a single pool, such as photos of employees\u2019 faces. But in real-life applications, test samples are often sampled ...", "dateLastCrawled": "2022-01-31T22:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Statistical process monitoring</b> as a big <b>data</b> analytics tool for smart ...", "url": "https://www.sciencedirect.com/science/article/pii/S0959152417301257", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0959152417301257", "snippet": "The underlying <b>assumption</b> made for the PCA and PLS based MSPM methods is that the <b>data</b> are independent and identically distributed (<b>i.i.d</b>.) samples <b>drawn</b> from a multivariate Gaussian <b>distribution</b>, which implies that the process <b>data</b> with the following characteristics would violate this <b>assumption</b>: (1) multimodal <b>distribution</b>; (2) dynamics; (3) nonlinear relationships between variables; (4) non-Gaussianity; (5) time-varying characteristics; (6) other characteristics such as outliers, gross ...", "dateLastCrawled": "2022-01-15T19:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Lecture 5: <b>Bayes Classifier and Naive Bayes</b>", "url": "https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote05.html", "isFamilyFriendly": true, "displayUrl": "https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote05.html", "snippet": "Here, the <b>data</b> is emails and the label is spam or not-spam. The Naive Bayes <b>assumption</b> implies that the words in an email are conditionally independent, given that you know that an email is spam or not. Clearly this is not true. Neither the words of spam or not-spam emails are <b>drawn</b> independently at random. However, the resulting classifiers ...", "dateLastCrawled": "2022-01-30T13:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Homework 1 Solutions</b>", "url": "https://www.cs.cmu.edu/~bapoczos/Classes/ML10715_2015Fall/assignments/hw1_sol.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.cmu.edu/~bapoczos/Classes/ML10715_2015Fall/assignments/hw1_sol.pdf", "snippet": "1.(3pts) An exponential <b>distribution</b> with parameter follows a <b>distribution</b> p(x) = e x. Given some <b>i.i.d</b>. <b>data</b> fx ign i=1 \u02d8Exp( ), derive the maximum likelihood estimate (MLE) ^ MLE. Is this estimator biased? Solution: The log likelihood is l( ) = X i log x i= nlog X i x i Set the derivative to 0: n= X i x i= 0 ) = 1 x This is biased.", "dateLastCrawled": "2022-02-03T04:58:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Learning Classifiers When the <b>Training</b> <b>Data</b> Is Not <b>IID</b>", "url": "https://www.ijcai.org/Proceedings/07/Papers/121.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.ijcai.org/Proceedings/07/Papers/121.pdf", "snippet": "Relaxing this <b>i.i.d</b>. <b>assumption</b>, we consider algorithms from the statistics litera-ture for the more realistic situation where batches or sub-groups of <b>training</b> samples may have inter- nal correlations, although the samples from dif-ferent batches may be considered to be uncorre-lated. Next, we propose simpler (more ef\ufb01cient) variantsthatscale well to largedatasets; theoretical results from the literature are provided to support their validity. Experimental results from real-life computer ...", "dateLastCrawled": "2022-02-03T16:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 1, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Independent and identically distributed</b> random variables - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Independent_and_identically_distributed</b>_random_variables", "snippet": "The <b>i.i.d</b>. <b>assumption</b> is also used in central limit theorem, which states that the probability <b>distribution</b> of the sum (or average) of <b>i.i.d</b>. variables with finite variance approaches a <b>normal</b> <b>distribution</b>. Often the <b>i.i.d</b>. <b>assumption</b> arises in the context of sequences of random variables. Then &quot;<b>independent and identically distributed</b>&quot; implies that an element in the sequence is independent of the random variables that came before it. In this way, an <b>i.i.d</b>. sequence is different from a Markov ...", "dateLastCrawled": "2022-02-03T02:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Lecture 2: More on linear methods for regression", "url": "https://www.cs.mcgill.ca/~dprecup/courses/Fall2009/ML/Lectures/ml-lecture02.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.mcgill.ca/~dprecup/courses/Fall2009/ML/Lectures/ml-lecture02.pdf", "snippet": "distributed (<b>i.i.d</b>.) from a unique underlying probability <b>distribution</b> P(hx;yi) The goal of the analysis is to compute, for an arbitrary new point x, E P (y h(x))2 where yis the value of x that could be present in a <b>data</b> set, and the expectation is over all all <b>training</b> sets <b>drawn</b> according to P We will decompose this expectation into three ...", "dateLastCrawled": "2021-11-21T23:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Gaussian Mixture</b> Models and Expectation ... - Towards <b>Data</b> Science", "url": "https://towardsdatascience.com/gaussian-mixture-models-and-expectation-maximization-a-full-explanation-50fa94111ddd", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/<b>gaussian-mixture</b>-models-and-expectation-maximization-a...", "snippet": "Note that the full joint probability of the <b>data</b> set <b>can</b> be vectorized (ie., decomposed as a product of individual probabilities, with the \u03a0 operator) only in the <b>assumption</b> that the observations are <b>drawn</b> <b>i.i.d</b> (identically independently distributed). When they are, the events are independent and the probability of observing one <b>data</b> point is not influenced by the other probabilities. Instead of the likelihood, we usually maximize the log-likelihood, in part because it turns the product of ...", "dateLastCrawled": "2022-02-01T07:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "4.4. Model Selection, <b>Underfitting</b>, and Overfitting \u2014 Dive into Deep ...", "url": "https://d2l.ai/chapter_multilayer-perceptrons/underfit-overfit.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_multilayer-perceptrons/underfit-overfit.html", "snippet": "Sometimes we <b>can</b> get away with minor violations of the <b>i.i.d</b>. <b>assumption</b> and our models will continue to work remarkably well. After all, nearly every real-world application involves at least some minor violation of the <b>i.i.d</b>. <b>assumption</b>, and yet we have many useful tools for various applications such as face recognition, speech recognition, and language translation.", "dateLastCrawled": "2022-01-30T17:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Can</b> machine learning find <b>extraordinary materials</b>? - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S0927025619307979", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0927025619307979", "snippet": "For a materials scientist, the <b>i.i.d</b>. <b>assumption</b> implies the <b>training</b> <b>data</b> fairly represents the full diversity of reality. This is clearly not the case due to <b>data</b> set bias. For example, some compounds are easier to synthesize and simulate or may be of more interest to researchers due to cost, performance in applications, or novelty. For this reason one must ask: do we have the information necessary, on a physical level, to even determine whether a material is extraordinary given highly ...", "dateLastCrawled": "2021-12-06T16:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Stationarity</b> in time series analysis | by Shay Palachy | Towards <b>Data</b> ...", "url": "https://towardsdatascience.com/stationarity-in-time-series-analysis-90c94f27322", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/<b>stationarity</b>-in-time-series-analysis-90c94f27322", "snippet": "For example, all <b>i.i.d</b>. stochastic processes are <b>stationary</b>. ... If, additionally, every variable x\u1d62 follows a <b>normal</b> <b>distribution</b> with zero mean and the same variance \u03c3\u00b2, then the process is said to be a Gaussian white noise process. N-th order <b>stationarity</b>. Very close to the definition of strong <b>stationarity</b>, N-th order <b>stationarity</b> demands the shift-invariance (in time) of the <b>distribution</b> of any n samples of the stochastic process, for all n up to order N. Thus, the same condition is ...", "dateLastCrawled": "2022-02-02T06:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Chapter 6 Hierarchical models</b> | Bayesian Inference 2019", "url": "https://vioshyvo.github.io/Bayesian_inference/hierarchical-models.html", "isFamilyFriendly": true, "displayUrl": "https://vioshyvo.github.io/Bayesian_inference/hierarchical-models.html", "snippet": "We <b>can</b> derive the posterior for the common true <b>training</b> effect \\(\\theta\\) with a computation almost identical to one performed in Example 5.2.1, in which we derived a posterior for one observation from the <b>normal</b> <b>distribution</b> with known variance: \\[ p(\\theta|\\mathbf{y}) = N\\left( \\frac{\\sum_{j=1}^J \\frac{1}{\\sigma^2_j} y_j}{\\sum_{j=1}^J \\frac{1}{\\sigma^2_j}},\\,\\, \\frac{1}{\\sum_{j=1}^J \\frac{1}{\\sigma^2_j}} \\right) \\] The posterior <b>distribution</b> is a <b>normal</b> <b>distribution</b> whose precision is the ...", "dateLastCrawled": "2022-02-02T18:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Meaning and mining: the impact of implicit assumptions <b>in data mining</b> ...", "url": "https://academic.oup.com/dsh/article-abstract/23/4/409/1036906", "isFamilyFriendly": true, "displayUrl": "https://academic.oup.com/dsh/article-abstract/23/4/409/1036906", "snippet": "3.1 Examples are <b>drawn</b> from a fixed <b>distribution</b>. Learning theory assumes that the <b>data</b> is produced by some process with constant probabilistic qualities. Examples are <b>drawn</b> identically and independently <b>i.i.d</b>.) from this <b>distribution</b>. The key is that the <b>distribution</b>&#39;s; probabilistic behavior does not change over time, and that it will continue to produce as many examples as requested. This <b>assumption</b> enables a formalization of the idea of generalization, and allows bounds to be proved ...", "dateLastCrawled": "2021-11-19T06:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Transfer learning for activity recognition: a survey | SpringerLink", "url": "https://link.springer.com/article/10.1007/s10115-013-0665-3", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10115-013-0665-3", "snippet": "The traditional approaches to activity recognition make the strong <b>assumption</b> <b>that the training</b> and test <b>data</b> are <b>drawn</b> from identical distributions. Many real-world applications cannot be represented in this setting, and thus, the baseline activity recognition approaches have to be modified to work in these realistic settings. Transfer-based activity recognition is one conduit for achieving this. Transfer learning. The ability to identify deep, subtle connections, what we term transfer ...", "dateLastCrawled": "2022-02-02T18:29:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Chapter 7 Resampling-based Statistical Assessment | <b>Data</b> Analysis and ...", "url": "https://gagneurlab.github.io/dataviz/resampling-stat.html", "isFamilyFriendly": true, "displayUrl": "https://gagneurlab.github.io/<b>data</b>viz/resampling-stat.html", "snippet": "The <b>i.i.d</b>. <b>assumption</b> if often taken in Hypothesis testing. It is however a tricky one. For instance if you have longitudinal <b>data</b> or confounders (hidden groups in the <b>data</b>). In our case, if the measurement of growth was done in separate day for the segregants of distinct genotypes, the <b>i.i.d</b> <b>assumption</b> could have not held. It is important in real applications to question this <b>assumption</b>, and if possible, to address it, for instance by stratifying the <b>data</b>.", "dateLastCrawled": "2022-01-31T05:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 1, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Independent and identically distributed</b> random variables - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Independent_and_identically_distributed</b>_random_variables", "snippet": "The <b>i.i.d</b>. <b>assumption</b> is also used in central limit theorem, which states that the probability <b>distribution</b> of the sum (or average) of <b>i.i.d</b>. variables with finite variance approaches a <b>normal</b> <b>distribution</b>. Often the <b>i.i.d</b>. <b>assumption</b> arises in the context of sequences of random variables. Then &quot;<b>independent and identically distributed</b>&quot; implies that an element in the sequence is independent of the random variables that came before it. In this way, an <b>i.i.d</b>. sequence is different from a Markov ...", "dateLastCrawled": "2022-02-03T02:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Unraveling the heterogeneity in Alzheimer&#39;s disease progression across ...", "url": "https://alz-journals.onlinelibrary.wiley.com/doi/10.1002/alz.12387", "isFamilyFriendly": true, "displayUrl": "https://alz-journals.onlinelibrary.wiley.com/doi/10.1002/alz.12387", "snippet": "Consequently, this indicates that <b>training</b> and validation <b>data</b> must be independently and identically distributed (<b>i.i.d</b>.) samples. 15 As such, a well-trained model should show similar performance on a validation dataset that was <b>drawn</b> from the identical statistical <b>distribution</b> as the <b>training</b> <b>data</b>, while an overfitted model would fail such validation.", "dateLastCrawled": "2022-02-03T02:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>10-701 Machine Learning: Assignment 1</b>", "url": "https://cs.cmu.edu/~aarti/Class/10701_Spring14/assignments/hw1_solution.pdf", "isFamilyFriendly": true, "displayUrl": "https://cs.cmu.edu/~aarti/Class/10701_Spring14/assignments/hw1_solution.pdf", "snippet": "Each student <b>can</b> only answer this question once, and we assume that the <b>distribution</b> of the answers is <b>i.i.d</b>. (a)What is the MLE estimation of ? (1 pts) (b)Let the above estimator be denoted by ^ . How many students should the instructors ask if they want the estimated value ^ to be so close to the unknown such that P(j^ j&gt;0:1) &lt;0:05; (4pts) (a) (1 pts) This problem is equivalent to estimating the mean parameter of a Bernoulli <b>distribution</b> from <b>i.i.d</b>. <b>data</b>. Therefore, the MLE estimation is ...", "dateLastCrawled": "2022-02-02T13:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Knowledge transfer across different domain <b>data</b> with multiple views ...", "url": "https://link.springer.com/article/10.1007/s00521-013-1432-9", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00521-013-1432-9", "snippet": "Traditional machine learning approaches make a basic <b>assumption</b> <b>that the training</b> and test <b>data</b> should be <b>drawn</b> from the same feature space with the identical <b>distribution</b>. It has been extensively demonstrated in the literatures that traditional leaning models perform drastically worse when the independent and identically distributed (<b>i.i.d</b>.) <b>assumption</b> no longer holds ...", "dateLastCrawled": "2021-12-01T12:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Quantile forecasting and <b>data</b>-driven inventory management under ...", "url": "https://www.sciencedirect.com/science/article/pii/S0167637718301366", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0167637718301366", "snippet": "The first category holds a strong <b>assumption</b> that all observations <b>can</b> be considered independent and identically distributed (<b>i.i.d</b>.) sample <b>drawn</b> from a real underlying <b>distribution</b>. Next, a standard treatment is to first estimate the <b>distribution</b> and then to replace the <b>distribution</b> with its estimation in the decision-making step (see [15] , [24] , [44] for a review).", "dateLastCrawled": "2022-01-10T18:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Naive Bayes and Text Classification - Dr. Sebastian Raschka", "url": "https://sebastianraschka.com/Articles/2014_naive_bayes_1.html", "isFamilyFriendly": true, "displayUrl": "https://sebastianraschka.com/Articles/2014_naive_bayes_1.html", "snippet": "The patterns from the first class (\\(\\omega_1=\\text{blue}\\)) are <b>drawn</b> <b>from a normal</b> <b>distribution</b> with mean \\(x=4\\) and a standard deviation \\(\\sigma=1\\). The probability <b>distribution</b> of the second class (\\(\\omega_2=\\text{green}\\)) is centered at x=10 with a similar standard deviation of \\(\\sigma=1\\). The bell-curves denote the probability densities of the samples that were <b>drawn</b> from the two different <b>normal</b> distributions. Considering only the class conditional probabilities, the maximum ...", "dateLastCrawled": "2022-01-31T01:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Homework 1 Solutions</b>", "url": "https://www.cs.cmu.edu/~bapoczos/Classes/ML10715_2015Fall/assignments/hw1_sol.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.cmu.edu/~bapoczos/Classes/ML10715_2015Fall/assignments/hw1_sol.pdf", "snippet": "1.(3pts) An exponential <b>distribution</b> with parameter follows a <b>distribution</b> p(x) = e x. Given some <b>i.i.d</b>. <b>data</b> fx ign i=1 \u02d8Exp( ), derive the maximum likelihood estimate (MLE) ^ MLE. Is this estimator biased? Solution: The log likelihood is l( ) = X i log x i= nlog X i x i Set the derivative to 0: n= X i x i= 0 ) = 1 x This is biased.", "dateLastCrawled": "2022-02-03T04:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Transfer learning for activity recognition: a survey | SpringerLink", "url": "https://link.springer.com/article/10.1007/s10115-013-0665-3", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10115-013-0665-3", "snippet": "The traditional approaches to activity recognition make the strong <b>assumption</b> <b>that the training</b> and test <b>data</b> are <b>drawn</b> from identical distributions. Many real-world applications cannot be represented in this setting, and thus, the baseline activity recognition approaches have to be modified to work in these realistic settings. Transfer-based activity recognition is one conduit for achieving this. Transfer learning. The ability to identify deep, subtle connections, what we term transfer ...", "dateLastCrawled": "2022-02-02T18:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Machine learning based approach to exam cheating detection", "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0254340", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0254340", "snippet": "They are based on the <b>assumption</b> that the <b>normal</b> <b>data</b> is generated according to some statistical <b>distribution</b> [32, 33]. The <b>distribution</b> parameters such as the mean and standard deviation are calculated based on the sample <b>data</b>. More sophisticated approaches use kernel functions to estimate the underlying <b>distribution</b> of <b>data</b> . Then the points with low probability are deemed to be outliers. Distance-based approaches operate on the <b>assumption</b> that abnormal points are far from the main cluster ...", "dateLastCrawled": "2021-11-14T14:54:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "What is <b>Learning</b>? <b>Machine Learning: Introduction and Unsupervised Learning</b>", "url": "http://pages.cs.wisc.edu/~bgibson/cs540/handouts/learning_intro.pdf", "isFamilyFriendly": true, "displayUrl": "pages.cs.wisc.edu/~bgibson/cs540/handouts/<b>learning</b>_intro.pdf", "snippet": "<b>learning</b> process \u2022x i = (x i1, . . . , x iD) \u2022Assume these instances are sampled independently from an unknown (population) distribution, P(x) \u2022We denote this by x i \u223c P(x), where <b>i.i.d</b>. stands for independent and identically distributed <b>i.i.d</b>. Training Sample \u2022A training sample is the \u201cexperience\u201d given to a <b>learning</b> algorithm", "dateLastCrawled": "2021-08-25T13:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "What is <b>Learning</b>? <b>Machine</b> <b>Learning</b>: Introduction and Unsupervised <b>Learning</b>", "url": "https://pages.cs.wisc.edu/~dyer/cs540/notes/08_learning-intro.pdf", "isFamilyFriendly": true, "displayUrl": "https://pages.cs.wisc.edu/~dyer/cs540/notes/08_<b>learning</b>-intro.pdf", "snippet": "<b>Machine</b> <b>Learning</b>: Introduction and Unsupervised <b>Learning</b> Chapter 18.1, 18.2, 18.8.1 and \u201cIntroduction to Statistical <b>Machine</b> <b>Learning</b>\u201d 1 What is <b>Learning</b>? \u2022\u201c<b>Learning</b> is making useful changes in our minds\u201d \u2013Marvin Minsky \u2022\u201c<b>Learning</b> is constructing or modifying representations of what is being experienced\u201c \u2013RyszardMichalski \u2022\u201c<b>Learning</b> denotes changes in a system that ... enable a system to do the same task more efficiently the next time\u201d \u2013Herbert Simon 3 Why do Mach", "dateLastCrawled": "2022-02-03T16:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Machine</b> <b>Learning</b> for Drug <b>Discovery</b> in a Nutshell \u2014 Part I | by Stefan ...", "url": "https://medium.com/atomwise/machine-learning-for-drug-discovery-in-a-nutshell-part-i-24ae3f65c135", "isFamilyFriendly": true, "displayUrl": "https://medium.com/atomwise/<b>machine</b>-<b>learning</b>-for-drug-<b>discovery</b>-in-a-nutshell-part-i...", "snippet": "Classical <b>machine</b> <b>learning</b> literature spends little attention to this aspect. Most often, the underlying assumption is that training and test examples are drawn <b>i.i.d</b>. from the same distribution ...", "dateLastCrawled": "2022-01-26T14:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "11._Intro_to_<b>Machine</b>_<b>Learning</b>.pdf - CMPSC 442 Artificial Intelligence ...", "url": "https://www.coursehero.com/file/121916721/11-Intro-to-Machine-Learningpdf/", "isFamilyFriendly": true, "displayUrl": "https://www.coursehero.com/file/121916721/11-Intro-to-<b>Machine</b>-<b>Learning</b>pdf", "snippet": "Outline Data-Driven Problem Solving Types of <b>Machine</b> <b>Learning</b> <b>I.I.D</b> Assumption and Generalization The Fundamental Tradeoff between Bias and Variance Bias and Variance Overfitting and Underfitting Regularization Hyperparameters, Three-fold split, Cross-Validation Example of Polynomial Regression 2. Minimum Spanning Tree A classical problem in algorithm design: Minimum Spanning Tree Input: A graph with cost for edges Output: A spanning tree with minimum cost Prim&#39;s algorithm, Kruskal&#39;s ...", "dateLastCrawled": "2022-01-15T13:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Lecture 16: Reinforcement <b>Learning</b>, Part 1 | Lecture Videos | <b>Machine</b> ...", "url": "https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-s897-machine-learning-for-healthcare-spring-2019/lecture-videos/lecture-16-reinforcement-learning-part-1/", "isFamilyFriendly": true, "displayUrl": "https://<b>ocw.mit.edu</b>/courses/electrical-engineering-and-computer-science/6-s897-<b>machine</b>...", "snippet": "So that S0, 1, et cetera, up to St are all <b>i.i.d</b>. draws of the same distribution. Then we have, essentially, a model for t different patients with a single time step or single action, instead of them being dependent in some way. So we can see that by going backwards through my slides, this is essentially what we had last week.", "dateLastCrawled": "2022-02-02T22:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Shortcut <b>learning</b> in deep neural networks | Nature <b>Machine</b> Intelligence", "url": "https://www.nature.com/articles/s42256-020-00257-z", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s42256-020-00257-z", "snippet": "In <b>analogy</b> to <b>machine</b> <b>learning</b>, we have a striking discrepancy between intended and actual <b>learning</b> outcome. Shortcut <b>learning</b> in education (surface <b>learning</b>) Alice loves history\u2014but at this ...", "dateLastCrawled": "2022-02-03T07:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Learning from positive</b> and unlabeled data: a survey - <b>Machine</b> <b>Learning</b>", "url": "https://link.springer.com/article/10.1007/s10994-020-05877-5", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10994-020-05877-5", "snippet": "<b>Learning from positive</b> and unlabeled data or PU <b>learning</b> is the setting where a learner only has access to positive examples and unlabeled data. The assumption is that the unlabeled data can contain both positive and negative examples. This setting has attracted increasing interest within the <b>machine</b> <b>learning</b> literature as this type of data naturally arises in applications such as medical diagnosis and knowledge base completion. This article provides a survey of the current state of the art ...", "dateLastCrawled": "2022-02-02T03:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Time Series Forecasting as Supervised Learning</b> - <b>Machine</b> <b>Learning</b> Mastery", "url": "https://machinelearningmastery.com/time-series-forecasting-supervised-learning/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/time-series-forecasting-su", "snippet": "Time series forecasting can be framed as a supervised <b>learning</b> problem. This re-framing of your time series data allows you access to the suite of standard linear and nonlinear <b>machine</b> <b>learning</b> algorithms on your problem. In this post, you will discover how you can re-frame your time series problem as a supervised <b>learning</b> problem for <b>machine</b> <b>learning</b>. After reading this post, you", "dateLastCrawled": "2022-02-02T19:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "[1910.07796] Overcoming Forgetting in <b>Federated Learning</b> on Non-<b>IID</b> Data", "url": "https://arxiv.org/abs/1910.07796", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/abs/1910.07796", "snippet": "We tackle the problem of <b>Federated Learning</b> in the non <b>i.i.d</b>. case, in which local models drift apart, inhibiting <b>learning</b>. Building on an <b>analogy</b> with Lifelong <b>Learning</b>, we adapt a solution for catastrophic forgetting to <b>Federated Learning</b>. We add a penalty term to the loss function, compelling all local models to converge to a shared optimum. We show that this can be done efficiently for communication (adding no further privacy risks), scaling with the number of nodes in the distributed ...", "dateLastCrawled": "2022-01-28T12:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "econometrics - Principle of <b>Analogy</b> and Method of Moments - Cross Validated", "url": "https://stats.stackexchange.com/questions/272803/principle-of-analogy-and-method-of-moments", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/272803/principle-of-<b>analogy</b>-and-method-of...", "snippet": "The basic principle of MoM is to choose the parameter estimate so that the corresponding sample moments are also zero. This is the &quot;matching&quot; part of the population moments with those of sample. The impetus for the evolution of MoM is that Hansen thought if there are more orthogonality conditions than parameters, then the system may not have a ...", "dateLastCrawled": "2022-01-25T20:40:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Pearson\u2019s Correlation, Linear Regression, And Why \u2018Beta\u2019 Grossly ...", "url": "https://medium.com/kxytechnologies/https-medium-com-pit-ai-technologies-the-black-swans-in-your-market-neutral-portfolios-part-1-e17fc18a42a7", "isFamilyFriendly": true, "displayUrl": "https://medium.com/kxytechnologies/https-medium-com-pit-ai-technologies-the-black...", "snippet": "To <b>machine</b> <b>learning</b> researchers, the Gaussian distribution arises naturally as the solution to some important optimization problems over probability distributions. One such problem is the maximum ...", "dateLastCrawled": "2021-05-27T10:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Pearson\u2019s <b>Correlation</b>, Linear Regression, And Why \u2018Beta\u2019 Grossly ...", "url": "https://towardsdatascience.com/the-black-swans-in-your-market-neutral-portfolios-part-i-7521683a7317", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/the-black-swans-in-your-market-neutral-portfolios-part...", "snippet": "To <b>machine</b> <b>learning</b> researchers, the Gaussian distribution arises naturally as the solution to some important optimization problems over probability distributions. One such problem is the maximum-entropy problem , which aims at finding among all probability distributions that are consistent with observed empirical evidence, the one the is the most ignorant about everything else.", "dateLastCrawled": "2022-02-02T09:46:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], []], "all_bing_queries": ["+(i.i.d.)  is like +(assumption that the training data is drawn from a Normal distribution)", "+(i.i.d.) is similar to +(assumption that the training data is drawn from a Normal distribution)", "+(i.i.d.) can be thought of as +(assumption that the training data is drawn from a Normal distribution)", "+(i.i.d.) can be compared to +(assumption that the training data is drawn from a Normal distribution)", "machine learning +(i.i.d. AND analogy)", "machine learning +(\"i.i.d. is like\")", "machine learning +(\"i.i.d. is similar\")", "machine learning +(\"just as i.i.d.\")", "machine learning +(\"i.i.d. can be thought of as\")", "machine learning +(\"i.i.d. can be compared to\")"]}