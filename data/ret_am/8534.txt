{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Dictionary</b> based representations \u2014 <b>sparse</b>-plex v2019.02", "url": "https://sparse-plex.readthedocs.io/en/latest/book/sparse_signal_models/dictionary_representations.html", "isFamilyFriendly": true, "displayUrl": "https://<b>sparse</b>-plex.readthedocs.io/.../<b>dictionary</b>_<b>representations</b>.html", "snippet": "Thus such a <b>dictionary</b> is able provide multiple representations to same vector \\(x\\). We call such dictionaries redundant dictionaries or over-complete dictionaries. In contrast a basis with \\(D=N\\) is called a complete <b>dictionary</b>. A special class of signals is those signals which have a <b>sparse</b> <b>representation</b> in a given <b>dictionary</b> \\(\\mathcal{D}\\).", "dateLastCrawled": "2022-02-02T21:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Dictionary Training for Sparse Representation as Generalization of</b> K ...", "url": "https://ieeexplore.ieee.org/document/6504716", "isFamilyFriendly": true, "displayUrl": "https://ieeexplore.ieee.org/document/6504716", "snippet": "Recent <b>dictionary</b> training algorithms for <b>sparse</b> <b>representation</b> <b>like</b> K-SVD, MOD, and their variation are reminiscent of K-means clustering, and this letter investigates such algorithms from that viewpoint. It shows: though K-SVD is sequential <b>like</b> K-means, it fails to simplify to K-means by destroying the structure in the <b>sparse</b> coefficients. In contrast, MOD can be viewed as a parallel generalization of K-means, which simplifies to K-means without perturbing the <b>sparse</b> coefficients. Keeping ...", "dateLastCrawled": "2021-12-09T19:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Dictionary</b> Learning Algorithms for <b>Sparse</b> <b>Representation</b>", "url": "https://papers.cnl.salk.edu/PDFs/Dictionary%20Learning%20Algorithms%20for%20Sparse%20Representation%202003-3696.pdf", "isFamilyFriendly": true, "displayUrl": "https://papers.cnl.salk.edu/PDFs/<b>Dictionary</b> Learning Algorithms for <b>Sparse</b>...", "snippet": "to a <b>sparse</b> set of <b>dictionary</b> words or vectors can be related to entropy min- ... 1981). Finding a <b>sparse</b> <b>representation</b> (based on the use of a \u201cfew\u201d code or <b>dictio-nary</b> words) can also be viewed as a generalization of vector quantization where a match to a single \u201ccode vector\u201d (word) is always sought (taking \u201ccode book\u201d = \u201c<b>dictionary</b>\u201d).4 Indeed, we can refer to a <b>sparse</b> solution, x, as a <b>sparse</b> coding of the signal instantiation, y. 1.1 Stochastic Models. It is well known ...", "dateLastCrawled": "2022-01-31T18:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "What are the <b>Types of Sparse Dictionary Learning Algorithms</b>? | FinsliQ", "url": "https://www.finsliqblog.com/ai-and-machine-learning/what-are-the-types-of-sparse-dictionary-learning-algorithms/", "isFamilyFriendly": true, "displayUrl": "https://www.finsliqblog.com/ai-and-machine-learning/what-are-the-types-of-<b>sparse</b>...", "snippet": "<b>Sparse</b> Dictionaries: This method mainly focuses on not only giving a <b>sparse</b> <b>representation</b> but also constructing <b>dictionary</b> learning algorithms which is enforced by the equation . where . is some predefined analytical <b>dictionary</b> with desirable properties <b>like</b> fast computation and . is a <b>sparse</b> matrix. These formulation helps to directly combine the fast implementation of the analytical dictionaries with the flexibility of <b>sparse</b> techniques.", "dateLastCrawled": "2021-12-30T02:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Sparse</b> Matrix and its representations | Set 1 (Using Arrays and Linked ...", "url": "https://www.geeksforgeeks.org/sparse-matrix-representation/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/<b>sparse</b>-matrix-<b>representation</b>", "snippet": "<b>Sparse</b> Matrix and its representations | Set 2 (Using List of Lists and <b>Dictionary</b> of keys) This article is contributed by Akash Gupta.If you <b>like</b> <b>GeeksforGeeks</b> and would <b>like</b> to contribute, you can also write an article using write.<b>geeksforgeeks</b>.org or mail your article to review-team@<b>geeksforgeeks</b>.org. See your article appearing on the ...", "dateLastCrawled": "2022-01-31T00:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Dictionary</b> training for <b>sparse</b> <b>representation</b> as generalization of K ...", "url": "https://dr.ntu.edu.sg/bitstream/10356/96655/1/SGK.pdf", "isFamilyFriendly": true, "displayUrl": "https://dr.ntu.edu.sg/bitstream/10356/96655/1/SGK.pdf", "snippet": "<b>Dictionary</b> Training for <b>Sparse</b> <b>Representation</b> as Generalization of K-means Clustering Sujit Kumar Sahoo, Member, IEEE and Anamitra Makur, Senior Member, IEEE Abstract\u2014Recent <b>dictionary</b> training algorithms for <b>sparse</b> <b>representation</b> <b>like</b> K-SVD, MOD, and their variation are rem-iniscent of K-means clustering, and this letter investigates such algorithms from that viewpoint. It shows: though K-SVD is sequential <b>like</b> K-means, it fails to simplify to K-means by de-stroying the structure in the ...", "dateLastCrawled": "2022-01-13T18:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Overcomplete <b>Representation and Dictionary Learning</b> \u2013 Vidya", "url": "https://myblogs806213342.wordpress.com/2018/03/23/overcomplete-representation-and-dictionary-learning/", "isFamilyFriendly": true, "displayUrl": "https://myblogs806213342.wordpress.com/2018/03/23/overcomplete-<b>representation</b>-and...", "snippet": "<b>Dictionary</b> (D) is learned in a manner that any signal x can be represented as where M&gt;&gt;N (overcomplete) and is the <b>sparse</b> code. If the sparsity of is K it means that out of the M entries only K is non-zero. Next, we will see how to create the <b>dictionary</b> and the <b>sparse</b> code provided a set training data X. <b>Dictionary</b> learning is done using the ...", "dateLastCrawled": "2022-01-15T01:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 7, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Sparse dictionary learning</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Sparse_dictionary_learning", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Sparse_dictionary_learning</b>", "snippet": "<b>Sparse</b> coding is a <b>representation</b> learning method which aims at finding a <b>sparse</b> <b>representation</b> of the input data (also known as <b>sparse</b> coding) in the form of a linear combination of basic elements as well as those basic elements themselves.These elements are called atoms and they compose a <b>dictionary</b>.Atoms in the <b>dictionary</b> are not required to be orthogonal, and they may be an over-complete spanning set.This problem setup also allows the dimensionality of the signals being represented to be ...", "dateLastCrawled": "2022-01-27T21:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "python - <b>Dictionary To Sparse Vector help- sort dictionary</b>? [SOLVED ...", "url": "https://www.daniweb.com/programming/software-development/threads/409992/dictionary-to-sparse-vector-help-sort-dictionary", "isFamilyFriendly": true, "displayUrl": "https://<b>www.daniweb.com</b>/.../409992/<b>dictionary-to-sparse-vector-help-sort-dictionary</b>", "snippet": "A <b>sparse</b> vector is a vector whose entries are almost all zero, <b>like</b> [1, 0, 0, 0, 0, 0, 0, 2, 0]. Storing all those zeros wastes memory and dictionaries are commonly used to keep track of just the nonzero entries. For example, the vector shown earlier can be represented as {0:1, 7:2}, since the vector it is meant to represent has the value 1 at index 0 and the value 2 at index 7. Write a function that converts a <b>dictionary</b> back to its sparese vector <b>representation</b>.", "dateLastCrawled": "2022-01-12T02:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "python - Convert <b>sparse</b> <b>dictionary</b> <b>representation</b> into a dense ...", "url": "https://stackoverflow.com/questions/25943870/convert-sparse-dictionary-representation-into-a-dense-dataframe", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/25943870", "snippet": "I would <b>like</b> to convert a list of dictionaries which sparsely represent individual observations of features into a dense data structure (e.g. a dataframe). Each observation is a <b>dictionary</b> with", "dateLastCrawled": "2022-01-06T20:34:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Dictionary</b> Learning with Uniform <b>Sparse</b> Representations for Anomaly ...", "url": "https://deepai.org/publication/dictionary-learning-with-uniform-sparse-representations-for-anomaly-detection", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>dictionary</b>-learning-with-uniform-<b>sparse</b>-<b>representations</b>...", "snippet": "The Joint <b>Sparse</b> <b>Representation</b> (JSR) model analyzed in [ZhaLi:13, LiZha:15] assumes a multi-class partitioning of the input, where each class spans a low-dimensional subspace. In [AdlEla:15], \u2113 1 penalty-based JSR is used for detecting noisy anomalies with prefixed <b>dictionary</b>. Although their formulations are <b>similar</b> to ours, the authors aim ...", "dateLastCrawled": "2022-02-03T18:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Latent <b>Dictionary</b> Learning for <b>Sparse</b> <b>Representation</b> based Classification", "url": "https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Yang_Latent_Dictionary_Learning_2014_CVPR_paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cv-foundation.org/.../Yang_Latent_<b>Dictionary</b>_Learning_2014_CVPR_paper.pdf", "snippet": "proposed <b>sparse</b> <b>representation</b> and <b>dictionary</b> learning approaches for action, gender and face recognition. 1. Introduction With the inspiration of <b>sparse</b> coding mechanism of human vision system [3][4], <b>sparse</b> coding by representing a signal as a <b>sparse</b> linear combination of <b>representation</b> bases (i.e., a <b>dictionary</b> of atoms) has been successfully applied to image restoration [1][2], image classification [5][6], to name a few. The <b>dictionary</b>, which should faithfully and discriminatively ...", "dateLastCrawled": "2022-01-08T23:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "K-<b>Means, Sparse Coding, Dictionary Learning and All</b> That \u2014 Data, ML ...", "url": "https://bugra.github.io/posts/2015/2/10/k-means-sparse-coding-dictionary-learning-and-all-that/", "isFamilyFriendly": true, "displayUrl": "https://bugra.github.io/posts/2015/2/10/k-<b>means-sparse-coding-dictionary-learning-and</b>...", "snippet": "Moreover, after building these dictionaries one could only pass the <b>dictionary</b> along with the <b>representation</b> of the vector to enable robust recovery <b>similar</b> to FFT. <b>Dictionary</b> learning is very <b>similar</b> to <b>sparse</b> coding in terms of it tries to represent the data, it tries to find good atoms from a \u201c<b>dictionary</b>\u201d where the <b>dictionary</b> atoms are learned from the training set. Especially, for classification tasks, as long as the user has a good <b>dictionary</b>, one could build very efficient vectors ...", "dateLastCrawled": "2022-01-01T11:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Dictionary</b> training for <b>sparse</b> <b>representation</b> as generalization of K ...", "url": "https://dr.ntu.edu.sg/bitstream/10356/96655/1/SGK.pdf", "isFamilyFriendly": true, "displayUrl": "https://dr.ntu.edu.sg/bitstream/10356/96655/1/SGK.pdf", "snippet": "<b>representation</b> error, a VQ codebook is typically trained using K-means clustering algorithm. It is an iterative process <b>similar</b> <b>to dictionary</b> training which alternates between \ufb01nding X and updating D. 1) <b>Sparse</b> coding (encoding) stage: This stage involves \ufb01nding the index k = argmin j ky i D(t)e jk2 2, so that the <b>sparse</b> <b>representation</b> for ...", "dateLastCrawled": "2022-01-13T18:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) <b>Improved online dictionary learning for sparse signal representation</b>", "url": "https://www.researchgate.net/publication/269303211_Improved_online_dictionary_learning_for_sparse_signal_representation", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/269303211_Improved_online_<b>dictionary</b>_learning...", "snippet": "<b>Similar</b> to many <b>dictionary</b> learning algorithms, the proposed algorithm alternates between two stages. First, <b>sparse</b> coding stage uses the current <b>dictionary</b> to obtain the <b>sparse</b> <b>representation</b> ...", "dateLastCrawled": "2021-11-18T10:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Dictionary</b> Learning Based Applications in Image Processing using Convex ...", "url": "http://home.iitk.ac.in/~saurabhk/EE609A_12011_12807637_.pdf", "isFamilyFriendly": true, "displayUrl": "home.iitk.ac.in/~saurabhk/EE609A_12011_12807637_.pdf", "snippet": "<b>similar</b> performances with the state of the art methods. Keywords\u2014<b>Sparse</b>, OMP, K-SVD, DCT. I. Introduction Natural signals such as images admit a <b>sparse</b> <b>represen-tation</b> i.e. they can be represented as a linear combination of few vectors (atoms of a learned <b>dictionary</b>). It has been established that there is much redundancy in natural signals. It can be represented in terms of few basis vectors. Since the <b>sparse</b> <b>representation</b> consists of mostly zeros, we only have to store only the non-zero ...", "dateLastCrawled": "2022-01-17T20:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Submodular <b>Dictionary</b> Selection for <b>Sparse</b> <b>Representation</b>", "url": "https://las.inf.ethz.ch/files/krause10submodular.pdf", "isFamilyFriendly": true, "displayUrl": "https://las.inf.ethz.ch/files/krause10submodular.pdf", "snippet": "to learn a <b>dictionary</b> for <b>sparse</b> <b>representation</b> di-rectly from data using techniques such as regulariza-tion, clustering, and nonparametric Bayesian infer-ence. Regularization-based approaches de ne an ob- jective function that minimize the data error, regular-ized by the \u2018 1 or the total variation (TV) norms to enforce sparsity under the <b>dictionary</b> <b>representation</b>. The proposed objective function is then jointly opti-mized in the <b>dictionary</b> entries and the <b>sparse</b> coe -cients (Olshausen ...", "dateLastCrawled": "2021-12-25T22:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Research on image sentiment analysis technology based on <b>sparse</b> ...", "url": "https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/cit2.12074", "isFamilyFriendly": true, "displayUrl": "https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/cit2.12074", "snippet": "Under normal circumstances, in order to make the <b>dictionary</b> have good <b>representation</b>, researchers will choose large-scale dictionaries, which greatly increases the time required for the <b>sparse</b> <b>representation</b> process and greatly reduces the efficiency of the experiment; the FDL proposed in this article is obtained through the K-SVD after going to the complete <b>dictionary</b>, combine important parameters derived from the SVD and the extracted <b>sparse</b> features with the neural network model, on the ...", "dateLastCrawled": "2022-02-02T20:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>GitHub</b> - <b>tiepvupsu/DICTOL</b>: DICTOL - A <b>Dictionary</b> Learning Toolbox in ...", "url": "https://github.com/tiepvupsu/DICTOL", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/<b>tiepvupsu/DICTOL</b>", "snippet": "K: total number of <b>dictionary</b> bases. D_range: <b>similar</b> to Y_range but used for <b>dictionary</b> without the shared <b>dictionary</b>. <b>Sparse</b> <b>Representation</b>-based classification (SRC) <b>Sparse</b> <b>Representation</b>-based classification implementation . Classification based on SRC. Syntax: [pred, X] = SRC_pred(Y, D, D_range, opts) INPUT: Y: test samples.", "dateLastCrawled": "2022-01-31T17:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Sparse</b> Dictionaries for Semantic Segmentation", "url": "http://vision.jhu.edu/assets/TaoECCV14.pdf", "isFamilyFriendly": true, "displayUrl": "vision.jhu.edu/assets/TaoECCV14.pdf", "snippet": "<b>sparse</b> <b>dictionary</b> learning. Although <b>similar</b> approaches have been explored in image classi cation tasks [20,32,2,1] and shown good performance, they have not been used to model top-down information in semantic labeling. 2.A new algorithm for jointly learning a <b>sparse</b> <b>dictionary</b> and the CRF param-eters, which makes the learned <b>dictionary</b> more discriminative and speci - cally trained for the segmentation task. Prior work in this area either learned the <b>dictionary</b> beforehand or used energies ...", "dateLastCrawled": "2022-01-29T01:51:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A Study of the K-SVD <b>Algorithm for Designing Overcomplete Dictionaries</b> ...", "url": "https://www.caam.rice.edu/~optimization/L1/optseminar/K-SVD_talk_lijun.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.caam.rice.edu/~optimization/L1/optseminar/K-SVD_talk_lijun.pdf", "snippet": "The arbitrary approximation in norm <b>can</b> <b>be thought</b> as a <b>representation</b> somehow. \u2022In recent years there has been a growing interest in the study of <b>sparse</b> <b>representation</b> of signals which is based on an overcomplete <b>dictionary</b>. \u2022Applications that use <b>sparse</b> <b>representation</b> are many and include compression, regularization in inverse problems, feature extraction, and more. \u2022Sparsity in overcomplete dictionaries is the basis for a wide variety of highly effective signal and image processing ...", "dateLastCrawled": "2022-01-27T02:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Research Article A <b>Sparse</b> <b>Representation</b> Based Method to Classify ...", "url": "https://downloads.hindawi.com/journals/cmmm/2015/567932.pdf", "isFamilyFriendly": true, "displayUrl": "https://downloads.hindawi.com/journals/cmmm/2015/567932.pdf", "snippet": "<b>dictionary</b>. It is <b>thought</b> that the <b>sparse</b> <b>representation</b> <b>can</b> improve the performance of the image classi cation [ ]. Firstly, the images could be treated as a distribution of a set of representative features, so the <b>sparse</b> <b>representation</b> <b>can</b> encode the semantic information of the images. Secondly, the number of atoms in the <b>dictionary</b> is greater than the dimensionality of the input examples, which means that the approximation of the example is not unique. So, it <b>can</b> nd a relative better ...", "dateLastCrawled": "2021-09-27T08:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "K-SVD: <b>Dictionary</b> Developing Algorithms for <b>Sparse</b> <b>Representation</b> of Signal", "url": "https://www.irjet.net/archives/V6/i6/IRJET-V6I6610.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.irjet.net/archives/V6/i6/IRJET-V6I6610.pdf", "snippet": "norm <b>can</b> <b>be thought</b> as a <b>representation</b> somehow. K-SVD algorithm for studying dictionaries D. We explained its development and analysis, and formalized applications to establish its usability and the advantage of trained dictionaries D. Diversities of the K-SVD algorithm for learning structural constrained dictionaries are also showcased. Out of those constraints are the non-negativity of the <b>dictionary</b> and shift invariance property. K-SVD deals with development of a state-of-the art image ...", "dateLastCrawled": "2021-09-17T13:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) <b>Dictionary</b> Learning Algorithms for <b>Sparse</b> <b>Representation</b> ...", "url": "https://www.academia.edu/14314030/Dictionary_Learning_Algorithms_for_Sparse_Representation", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/14314030/<b>Dictionary</b>_Learning_Algorithms_for_<b>Sparse</b>_<b>Representation</b>", "snippet": "<b>Dictionary</b> Learning Algorithms for <b>Sparse</b> <b>Representation</b>. Neural Computation, 2003. Kenneth Kreutz-delgado. Terrence Sejnowski. Joseph Murray. Bhaskar Rao. Te-won Lee. Kenneth Kreutz-delgado. Terrence Sejnowski . Joseph Murray. Bhaskar Rao. Te-won Lee. Download PDF. Download Full PDF Package. This paper. A short summary of this paper. 37 Full PDFs related to this paper. READ PAPER. <b>Dictionary</b> Learning Algorithms for <b>Sparse</b> <b>Representation</b>. Download. <b>Dictionary</b> Learning Algorithms for <b>Sparse</b> ...", "dateLastCrawled": "2021-08-18T00:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Sparse</b> Matrix and its representations | Set 1 (Using Arrays and Linked ...", "url": "https://www.geeksforgeeks.org/sparse-matrix-representation/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/<b>sparse</b>-matrix-<b>representation</b>", "snippet": "<b>Sparse</b> Matrix Representations <b>can</b> be done in many ways following are two common representations: Array <b>representation</b>. Linked list <b>representation</b>. Method 1: Using Arrays: 2D array is used to represent a <b>sparse</b> matrix in which there are three rows named as. Row: Index of row, where non-zero element is located.", "dateLastCrawled": "2022-01-31T00:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Sparse and Redundant Representation Modeling \u2014 What Next</b>?", "url": "https://elad.cs.technion.ac.il/wp-content/uploads/2018/02/Paper-IEEE-SPL-Sparsity-Final.pdf", "isFamilyFriendly": true, "displayUrl": "https://elad.cs.technion.ac.il/wp-content/uploads/2018/02/Paper-IEEE-SPL-Sparsity...", "snippet": "<b>dictionary</b>. The forward transform is a highly non-linear one, (P 0) ^ = argmin k k 0 s:t: x = D ; (2) searching for the sparsest explanation for the signal x. The \u20180 cost function kk 0 counts the non-zero entries in this vector, and we expect a <b>sparse</b> outcome, k k 0 = k \u02ddd. This model <b>can</b> be interpreted as a chemistry of data sources: the ...", "dateLastCrawled": "2021-12-09T16:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) <b>Dictionary</b> Learning Algorithms for <b>Sparse</b> <b>Representation</b>", "url": "https://www.researchgate.net/publication/10896448_Dictionary_Learning_Algorithms_for_Sparse_Representation", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/10896448", "snippet": "signal to a <b>sparse</b> set of <b>dictionary</b> words or vectors <b>can</b> be related to entropy minimization as a means of elucidating statistical structure (Watanabe, 1981). Finding a <b>sparse</b> <b>representation</b>", "dateLastCrawled": "2022-01-05T22:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "12.5. <b>Sparse</b> Matrices \u2014 How to Think like a Computer Scientist ...", "url": "https://runestone.academy/ns/books/published//thinkcspy/Dictionaries/Sparsematrices.html", "isFamilyFriendly": true, "displayUrl": "https://runestone.academy/ns/books/published//thinkcspy/Dictionaries/<b>Sparse</b>matrices.html", "snippet": "<b>Sparse</b> Matrices\u00b6 A matrix is a two dimensional collection, typically <b>thought</b> of as having rows and columns of data. One of the easiest ways to create a matrix is to use a list of lists. For example, consider the matrix shown below. We <b>can</b> represent this collection as five rows, each row having five columns. Using a list of lists <b>representation</b>, we will have a list of five items, each of which is a list of five items. The outer items represent the rows and the items in the nested lists ...", "dateLastCrawled": "2022-02-03T05:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Sparse</b> <b>representation</b> using overcomplete <b>dictionary</b> - when is L1 norm ...", "url": "https://math.stackexchange.com/questions/881699/sparse-representation-using-overcomplete-dictionary-when-is-l1-norm-not-good-e", "isFamilyFriendly": true, "displayUrl": "https://math.stackexchange.com/questions/881699/<b>sparse</b>-<b>representation</b>-using-over...", "snippet": "Stack Exchange network consists of 178 Q&amp;A communities including Stack Overflow, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers.. Visit Stack Exchange", "dateLastCrawled": "2021-12-24T14:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "python - Convert <b>sparse</b> <b>dictionary</b> <b>representation</b> into a dense ...", "url": "https://stackoverflow.com/questions/25943870/convert-sparse-dictionary-representation-into-a-dense-dataframe", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/25943870", "snippet": "I would like to convert this list of dictionaries into a dense dataframe such that the columns contain all possible keys. I began writing some code, but <b>thought</b> that I&#39;d ask first if this functionality actually exists in a package somewhere. Thanks.", "dateLastCrawled": "2022-01-06T20:34:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) Greedy <b>dictionary</b> selection for <b>sparse</b> <b>representation</b> | Andreas ...", "url": "https://www.academia.edu/2804714/Greedy_dictionary_selection_for_sparse_representation", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/2804714/Greedy_<b>dictionary</b>_selection_for_<b>sparse</b>_<b>representation</b>", "snippet": "By <b>sparse</b>, we mean that only a few <b>dictionary</b> elements, <b>compared</b> to the. Abstract We develop an efficient learning framework to construct signal dictionaries for <b>sparse</b> <b>representation</b> by selecting the <b>dictionary</b> columns from multiple candidate bases. By <b>sparse</b>, we mean that only a few <b>dictionary</b> elements, <b>compared</b> to the . \u00d7 Close Log In. Log in with Facebook Log in with Google. or. Email. Password. Remember me on this computer ...", "dateLastCrawled": "2022-01-15T17:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Active <b>Dictionary</b> Learning in <b>Sparse</b> <b>Representation</b> Based ...", "url": "https://deepai.org/publication/active-dictionary-learning-in-sparse-representation-based-classification", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/active-<b>dictionary</b>-learning-in-<b>sparse</b>-<b>representation</b>...", "snippet": "The property of the <b>dictionary</b> <b>can</b> affect the <b>sparse</b> <b>representation</b> significantly. How to construct a proper <b>dictionary</b> is important for <b>sparse</b> <b>representation</b>. There are two major approaches for <b>dictionary</b> learning. First is the analytic approach, in which DCT bases, wavelets, curvelets and other nonadaptive functions are used as atoms to construct the dictionaries. Second is the learning-based approaches, such as the", "dateLastCrawled": "2022-01-11T02:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A <b>sparse</b> <b>representation</b> and <b>dictionary</b> learning based algorithm for ...", "url": "https://pubmed.ncbi.nlm.nih.gov/30214129/", "isFamilyFriendly": true, "displayUrl": "https://pubmed.ncbi.nlm.nih.gov/30214129", "snippet": "In order to avoid producing the artifact, this paper presents a new de-noising model based on <b>sparse</b> <b>representation</b> and <b>dictionary</b> learning. The Split Bregman Iteration strategy is employed to implement the model. Furthermore, an appropriate <b>dictionary</b> is designed by the use of the Kernel Singular Value Decomposition method, resulting in a new Rician noise removal algorithm. <b>Compared</b> with other de-noising algorithms, the presented new algorithm <b>can</b> achieve superior performance, in terms of ...", "dateLastCrawled": "2020-12-29T10:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Image super-resolution via <b>sparse</b> <b>representation</b>", "url": "https://pubmed.ncbi.nlm.nih.gov/20483687/", "isFamilyFriendly": true, "displayUrl": "https://pubmed.ncbi.nlm.nih.gov/20483687", "snippet": "Therefore, the <b>sparse</b> <b>representation</b> of a low resolution image patch <b>can</b> be applied with the high resolution image patch <b>dictionary</b> to generate a high resolution image patch. The learned <b>dictionary</b> pair is a more compact <b>representation</b> of the patch pairs, <b>compared</b> to previous approaches, which simply sample a large amount of image patch pairs, reducing the computational cost substantially. The effectiveness of such a sparsity prior is demonstrated for both general image super-resolution and ...", "dateLastCrawled": "2021-12-11T18:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Image Super-Resolution via <b>Sparse</b> <b>Representation</b>", "url": "http://www.columbia.edu/~jw2966/papers/YWHM10-TIP.pdf", "isFamilyFriendly": true, "displayUrl": "<b>www.columbia.edu</b>/~jw2966/papers/YWHM10-TIP.pdf", "snippet": "an appropriately chosen over-complete <b>dictionary</b>. Inspired by this observation, we seek a <b>sparse</b> <b>representation</b> for each patch of the low-resolution input, and then use the coef\ufb01cients of this <b>representation</b> to generate the high-resolution output. Theoretical results from compressed sensing suggest that under mild condi-tions, the <b>sparse</b> <b>representation</b> <b>can</b> be correctly recovered from the downsampled signals. By jointly training two dictionaries for the low- and high-resolution image ...", "dateLastCrawled": "2022-02-01T22:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Sparse-Representation-Based Classi\ufb01cation with Structure</b>-Preserving ...", "url": "https://www.ele.uri.edu/faculty/he/PDFfiles/sparserepresentation.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.ele.uri.edu/faculty/he/PDFfiles/<b>sparserepresentation</b>.pdf", "snippet": "In recent years, <b>sparse</b> <b>representation</b> (or <b>sparse</b> coding) has received a lot of attentions. The key idea is to search for the least number of basis vectors (or atoms) in a <b>dictionary</b> A 2 Rm n to characterize a signal y 2 Rm (A has n atoms and each atom is a vector with m elements). Therefore, the signal <b>can</b> be represented as the <b>sparse</b> vectors ...", "dateLastCrawled": "2022-02-02T13:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Dictionary</b> based Image Compression via <b>Sparse</b> <b>Representation</b>", "url": "https://www.slideshare.net/IJECEIAES/dictionary-based-image-compression-via-sparse-representation", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/IJECEIAES/<b>dictionary</b>-based-image-compression-via-<b>sparse</b>...", "snippet": "The minimization problem to find the best <b>dictionary</b> for <b>sparse</b> <b>representation</b> of Y in the given sparsity constraint T0 <b>can</b> be represented by: (4) The <b>dictionary</b> is trained to provide a better <b>representation</b> of the actual signal when the number of <b>dictionary</b> elements used to represent it is less than or equal to T0.Various algorithms have been developed to train over complete dictionaries for <b>sparse</b> signal <b>representation</b>. The K-SVD algorithm [14] is very efficient and it works well with ...", "dateLastCrawled": "2022-01-30T11:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Sparse</b> <b>Representation</b> of Electrodermal Activity With Knowledge-Driven ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4362752/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC4362752", "snippet": "<b>Sparse</b> <b>representation</b> techniques model a signal as a linear combination of a small number of atoms chosen from an overcomplete <b>dictionary</b> aiming to reveal certain structures of a signal and represent them in a compact way . Since psychophysiological signals, such as EDA, show typical patterns over time, their <b>sparse</b> decomposition <b>can</b> yield accurate representations of scientific and translational value and contribute to scalable implementations (e.g., on mobile devices). Noting that the ...", "dateLastCrawled": "2017-01-14T08:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Sparse</b> <b>representation</b> based computed tomography images reconstruction ...", "url": "https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/iet-ipr.2019.1312", "isFamilyFriendly": true, "displayUrl": "https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/iet-ipr.2019.1312", "snippet": "<b>Sparse</b> <b>representation</b> of LR and HR image patches corresponding to each other are considered as pairs, for better <b>representation</b> of <b>sparse</b> atoms, coupled-KSVD is employed for training the dictionaries. Similar to the training phase, in the SR reconstruction phase, bicubic interpolation is applied to LR images to amplify the dimensions of the corresponding HR images. Extract the patches and vectorise each patch to reconstruct the HR image. <b>Sparse</b> <b>representation</b> coefficients for extracted ...", "dateLastCrawled": "2022-01-21T14:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Sparse</b> Matrix and its representations | Set 1 (Using Arrays and Linked ...", "url": "https://www.geeksforgeeks.org/sparse-matrix-representation/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/<b>sparse</b>-matrix-<b>representation</b>", "snippet": "<b>Sparse</b> Matrix Representations <b>can</b> be done in many ways following are two common representations: Array <b>representation</b>. Linked list <b>representation</b>. Method 1: Using Arrays: 2D array is used to represent a <b>sparse</b> matrix in which there are three rows named as. Row: Index of row, where non-zero element is located.", "dateLastCrawled": "2022-01-31T00:07:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine Learning by Analogy</b> - SlideShare", "url": "https://www.slideshare.net/ColleenFarrelly/machine-learning-by-analogy-59094152", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ColleenFarrelly/<b>machine-learning-by-analogy</b>-59094152", "snippet": "<b>Machine Learning by Analogy</b> 1. Colleen M. Farrelly 2. Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics behind these ...", "dateLastCrawled": "2022-01-31T07:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "TensorFlow is an interface for expressing <b>machine</b> <b>learning</b> algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Machine</b> <b>Learning</b> by <b>Analogy</b> II - slideshare.net", "url": "https://www.slideshare.net/ColleenFarrelly/machine-learning-by-analogy-ii", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ColleenFarrelly/<b>machine</b>-<b>learning</b>-by-<b>analogy</b>-ii", "snippet": "Updated <b>Machine</b> <b>Learning</b> by <b>Analogy</b> presentation that builds to more advanced methods (TensorFlow, geometry/topology-based methods...) and adds a section on ti\u2026", "dateLastCrawled": "2021-12-31T12:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Neural Networks: Analogies. When our brains form analogies, they\u2026 | by ...", "url": "https://towardsdatascience.com/neural-networks-analogies-7ebeb3ac5d5e", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/neural-networks-analogies-7ebeb3ac5d5e", "snippet": "I\u2019ll outline a potential route to artificial neural networks which exhibit transfer <b>learning</b>: First, <b>Sparse</b> Distributed Representations. Numenta\u2019s Hierarchical Te m poral Memory, along with other techniques, relies upon a <b>sparse</b> distributed <b>representation</b>. An example of this is a very long string of ones and zeroes, where almost all the values are zero \u2014 there is a <b>sparse</b> distribution of the ones. If each digit represented a different thing, like \u2018pointy ears\u2019, \u2018tail ...", "dateLastCrawled": "2022-01-28T03:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PPT) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> | Colleen Farrelly - Academia.edu", "url": "https://www.academia.edu/30404581/Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/30404581/<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "<b>Machine</b> <b>Learning</b> by <b>Analogy</b> Colleen M. Farrelly Overview of Problem Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics ...", "dateLastCrawled": "2022-01-02T06:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "On <b>Machine</b> <b>Learning</b> \u2014 Data, ML &amp; Leadership", "url": "https://bugra.github.io/posts/2014/8/23/on-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://bugra.github.io/posts/2014/8/23/on-<b>machine</b>-<b>learning</b>", "snippet": "<b>Sparse</b> Colorful Filters. Recently, I wrote how we do classification at CB Insights.The post outlines some of the things that I have been thinking about how to apply <b>machine</b> <b>learning</b> for a given problem along with the process that we adopted for the classification problem at CB Insights, but also gave me a good opportunity to reflect even further about the <b>machine</b> <b>learning</b> process; shortcomings of papers, books and even traditional education system when it comes to teach the <b>machine</b> <b>learning</b>.", "dateLastCrawled": "2021-12-10T17:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Compressed Sensing Meets <b>Machine</b> <b>Learning</b>: Classification via <b>Sparse</b> ...", "url": "https://nuit-blanche.blogspot.com/2008/05/cs-mini-course-classification-via.html", "isFamilyFriendly": true, "displayUrl": "https://nuit-blanche.blogspot.com/2008/05/cs-mini-course-classification-via.html", "snippet": "Compressed Sensing Meets <b>Machine</b> <b>Learning</b>: Classification via <b>Sparse</b> <b>Representation</b> and Distributed Pattern Recognition This Spring, Allen Yang has given a mini course at Berkeley entitled Compressed Sensing Meets <b>Machine</b> <b>Learning</b>. The three lectures are listed here (it includes accompanying code): lecture 1: Classification via <b>Sparse</b> <b>Representation</b>; lecture 2: Classification of Mixture Subspace Models via <b>Sparse</b> <b>Representation</b>, lecture 3: Distributed Pattern Recognition; The third lecture ...", "dateLastCrawled": "2022-01-25T10:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Word Embedding: Syntactics or Semantics</b> \u00b7 Shengbin&#39;s Studio", "url": "https://wushbin.github.io/2017/10/09/Word-Embedding-Syntactics-or-Semantics/", "isFamilyFriendly": true, "displayUrl": "https://wushbin.github.io/2017/10/09/<b>Word-Embedding-Syntactics-or-Semantics</b>", "snippet": "<b>Sparse</b> Vector <b>Representation</b>. The co-occurrence matrix in represented each cell by the raw frequency of the co-occurrence of two words. The raw frequency in a matrix may be skewed. Pointwise mutual information PPMI is a good measure for association between words which can tell us how much often the two words occur. The pointwise mutual information is a measure of how often two events x and y occur, compared with what we would expect if they were independent: PMI between two words is ...", "dateLastCrawled": "2022-01-09T18:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>What Are Word Embeddings</b> for Text? - <b>Machine</b> <b>Learning</b> Mastery", "url": "https://machinelearningmastery.com/what-are-word-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>what-are-word-embeddings</b>", "snippet": "Word embeddings are a type of word <b>representation</b> that allows words with similar meaning to have a similar <b>representation</b>. They are a distributed <b>representation</b> for text that is perhaps one of the key breakthroughs for the impressive performance of deep <b>learning</b> methods on challenging natural language processing problems. In this post, you will discover the word embedding approach for representing text data. After", "dateLastCrawled": "2022-01-30T18:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Accelerating Innovation Through <b>Analogy</b> Mining", "url": "http://hyadatalab.com/papers/analogy-kdd17.pdf", "isFamilyFriendly": true, "displayUrl": "hyadatalab.com/papers/<b>analogy</b>-kdd17.pdf", "snippet": "<b>machine</b> <b>learning</b> models that develop similarity metrics suited for <b>analogy</b> mining. We demonstrate that <b>learning</b> purpose and mechanism representations allows us to \u2022nd analogies with higher precision and recall than traditional information-retrieval methods based on TF-IDF, LSA, LDA and GlOVe, in challenging noisy set-tings. Furthermore, we ...", "dateLastCrawled": "2022-01-29T02:29:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Sparse Adaptive Local Machine Learning</b> Algorithms for Sensing and Analytics", "url": "https://pdxscholar.library.pdx.edu/cgi/viewcontent.cgi?article=1000&context=mcecs_mentoring", "isFamilyFriendly": true, "displayUrl": "https://pdxscholar.library.pdx.edu/cgi/viewcontent.cgi?article=1000&amp;context=mcecs...", "snippet": "Fig. 2: A <b>sparse representation can be thought of as</b> the dot product of a dictionary vector and a sparse code vector. Given a . dictionary . of general components, we can use a . sparse code. to select as few of them as possible to reconstruct an image of interest (Fig. 2). This reconstruction is called a . sparse representation. Sparse Coding. Image processing is expensive. Instead of working with the original image, we can identify its most relevant components and discard the rest. This ...", "dateLastCrawled": "2021-08-31T12:20:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(sparse representation)  is like +(dictionary)", "+(sparse representation) is similar to +(dictionary)", "+(sparse representation) can be thought of as +(dictionary)", "+(sparse representation) can be compared to +(dictionary)", "machine learning +(sparse representation AND analogy)", "machine learning +(\"sparse representation is like\")", "machine learning +(\"sparse representation is similar\")", "machine learning +(\"just as sparse representation\")", "machine learning +(\"sparse representation can be thought of as\")", "machine learning +(\"sparse representation can be compared to\")"]}