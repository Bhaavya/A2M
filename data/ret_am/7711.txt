{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Embedding Maps</b> - iShare Help - Confluence", "url": "https://astuntech.atlassian.net/wiki/spaces/ISHAREHELP/pages/28803122/Embedding+Maps", "isFamilyFriendly": true, "displayUrl": "https://astuntech.atlassian.net/wiki/<b>spaces</b>/ISHAREHELP/pages/28803122/<b>Embedding+Maps</b>", "snippet": "<b>Embedding Maps</b>. Created by Kim Stimpson. Last updated: Oct 14, 2021. Overview. iShare has a variety of <b>different</b> ways of <b>embedding</b> a map directly into a web page. Originally you could only embed a static Location map within a web page using iShare. Direct <b>embedding</b> without JavaScript; With the introduction of iShare v4.2 you now have the power to embed a map in any web page without the need to display &quot;iShare - My <b>Maps</b>&quot;. In all of the following scenarios, as well as displaying a map, the ...", "dateLastCrawled": "2022-01-06T16:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "tensorflow - What is meant by visualizing an <b>embedding</b> <b>space</b>(neural ...", "url": "https://stackoverflow.com/questions/46802480/what-is-meant-by-visualizing-an-embedding-spaceneural-network", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/46802480", "snippet": "The <b>embedding</b> <b>space</b> is the <b>space</b> of the features produced by some learning algorithm. In the specific case of a (convolutional) neural network, this usually means one of the output feature <b>maps</b> (flattened) at some predefined layer or the output of one of the fully connected layers. What one would visualize is not the weight matrix, but the values of the produced features for some input test data. For example one takes the full test set and passes it through the network and computes the ...", "dateLastCrawled": "2022-01-09T05:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Neural Network Embeddings Explained | by Will Koehrsen | Towards Data ...", "url": "https://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/neural-network-<b>embeddings</b>-explained-4d028e6f0526", "snippet": "This process takes discrete entities and <b>maps</b> each observation to a vector of 0s and a single 1 signaling the specific category. The one-hot encoding technique has two main drawbacks: For high-cardinality variables \u2014 those with many unique categories \u2014 the dimensionality of the transformed vector becomes unmanageable. The mapping is completely uninformed: \u201csimilar\u201d categories are not placed closer to each other in <b>embedding</b> <b>space</b>. The first problem is well-understood: for each ...", "dateLastCrawled": "2022-02-03T06:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Divide and Conquer the <b>Embedding</b> <b>Space</b> for Metric Learning", "url": "https://openaccess.thecvf.com/content_CVPR_2019/papers/Sanakoyeu_Divide_and_Conquer_the_Embedding_Space_for_Metric_Learning_CVPR_2019_paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://openaccess.thecvf.com/content_CVPR_2019/papers/Sanakoyeu_Divide_and_Conquer...", "snippet": "coming from <b>different</b> classes in the learned <b>embedding</b> <b>space</b>. An <b>embedding</b> <b>space</b> with the desired properties is learned by optimizing loss functions based on pairs of images from the same or <b>different</b> class [13, 3], triplets of images [38, 45, 17] or tuples of larger number of images [20, 43, 41, 1], which express positive or negative relationships in the dataset. Existing deep metric learning approaches usually learn a single distance metric for all samples from the given data distribution ...", "dateLastCrawled": "2022-02-02T23:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Exploring Deep <b>Embeddings</b>. Visualizing Pytorch Models with\u2026 | by ...", "url": "https://shairozsohail.medium.com/exploring-deep-embeddings-fa677f0e7c90", "isFamilyFriendly": true, "displayUrl": "https://shairozsohail.medium.com/exploring-deep-<b>embeddings</b>-fa677f0e7c90", "snippet": "The goal of this article will be to explore what this vector <b>space</b> looks <b>like</b> for <b>different</b> models and build a tool that will allow us to take any deep learning model and visualize its vector <b>space</b> using Tensorboard\u2019s <b>Embedding</b> Projector, TensorboardX, and Pytorch. *All of the code for this guide is available on the Github repo here*", "dateLastCrawled": "2022-01-31T06:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Document <b>Embedding</b> Techniques. A review of notable literature on the ...", "url": "https://towardsdatascience.com/document-embedding-techniques-fed3e7a6a25d", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/document-<b>embedding</b>-techniques-fed3e7a6a25d", "snippet": "Topic modelling While this is not usually the main application for topic modeling techniques <b>like</b> LDA and PLSI, they inherently generate a document <b>embedding</b> <b>space</b> meant to model and explain word distribution in the corpus and where dimensions can be seen as latent semantic structures hidden in the data, and are thus useful in our context. I don\u2019t really cover this approach in this post (except a brief intro to LDA), since I think that it is both well represented by LDA and well known ...", "dateLastCrawled": "2022-02-03T01:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Poincar\u00e9 <b>maps</b> for analyzing complex hierarchies in single-cell data ...", "url": "https://www.nature.com/articles/s41467-020-16822-4/", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41467-020-16822-4", "snippet": "Monocle 2 15 forces a tree-<b>like</b> topology on the data using &quot;reversed graph <b>embedding</b>\u201d in a low-dimensional Euclidean <b>space</b>. However, similar to UMAP, such a representation might not exist for ...", "dateLastCrawled": "2022-01-30T02:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "ag.algebraic geometry - <b>Maps to projective space determined by</b> a line ...", "url": "https://mathoverflow.net/questions/4567/maps-to-projective-space-determined-by-a-line-bundle", "isFamilyFriendly": true, "displayUrl": "https://<b>mathoverflow</b>.net/questions/4567", "snippet": "The general definition of projective <b>space</b> in the functorial approach to algebraic geometry (see e.g. EGA I) just says that <b>maps</b> into projective <b>space</b> are given by invertible sheaves + global generators. The description of the points follows from it (plug in fields). If you pick a universal element, i.e. the serre twist $\\mathcal{O}(1)$ with its chosen generators, then the bijection is on field-valued points exactly what is described above, but it should not be confused with a complete ...", "dateLastCrawled": "2022-01-26T19:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "javascript - <b>Embedding</b> Google-<b>Maps</b> iFrame by User-Defined Coordinates ...", "url": "https://stackoverflow.com/questions/31878591/embedding-google-maps-iframe-by-user-defined-coordinates", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/31878591", "snippet": "This is how it looks right now: When the user clicks the button underneath the Google-<b>Maps</b>-iFrame, the current View should get saved and an iFrame with exactly the same coordinates, zoom, tilt, maptype should be created and inserted into a contenteditable iframe. The function called after clicking the button only does this:", "dateLastCrawled": "2022-01-25T19:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "[D] How is the MDP Homomorphism approach in the below paper <b>different</b> ...", "url": "https://www.reddit.com/r/MachineLearning/comments/s410dw/d_how_is_the_mdp_homomorphism_approach_in_the/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/MachineLearning/comments/s410dw/d_how_is_the_mdp_homomorphism...", "snippet": "I don&#39;t <b>like</b> having to walk around and hope I picked up all of it. Where I live it snows a lot, and poops get lost in the snow come new snowfall. I found some cool concept gadgets that people have made, but nothing that worked with just a security cam. So I built this poop detector and made a video about it. When some code I wrote detects my dog pooping it will remember the location and draw a circle where my dog pooped on a picture of my backyard.", "dateLastCrawled": "2022-01-18T16:09:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Neural Network Embeddings Explained | by Will Koehrsen | Towards Data ...", "url": "https://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/neural-network-<b>embeddings</b>-explained-4d028e6f0526", "snippet": "Embeddings. An <b>embedding</b> is a mapping of a discrete \u2014 categorical \u2014 variable to a vector of continuous numbers. In the context of neural networks, embeddings are low-dimensional, learned continuous vector representations of discrete variables. Neural network embeddings are useful because they can reduce the dimensionality of categorical variables and meaningfully represent categories in the transformed <b>space</b>.. Neural network embeddings have 3 primary purposes:", "dateLastCrawled": "2022-02-03T06:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 1, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Embedding</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Embedding", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Embedding</b>", "snippet": "(On the other hand, this notation is sometimes reserved for inclusion <b>maps</b>.) Given X and Y, several <b>different</b> embeddings of X in Y may be possible. In many cases of interest there is a standard (or &quot;canonical&quot;) <b>embedding</b>, like those of the natural numbers in the integers, the integers in the rational numbers, the rational numbers in the real numbers, and the real numbers in the complex numbers. In such cases it is common to identify the domain X with its image f(X) contained in Y, so that f ...", "dateLastCrawled": "2022-02-02T15:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Node <b>Representation</b> Learning - SNAP", "url": "https://snap-stanford.github.io/cs224w-notes/machine-learning-with-networks/node-representation-learning", "isFamilyFriendly": true, "displayUrl": "https://snap-stanford.github.io/cs224w-notes/machine-learning-with-networks/node...", "snippet": "The goal of node <b>embedding</b> is to encode nodes so that similarity in the <b>embedding</b> <b>space</b> (e.g., dot product) approximates similarity in the original network, the node <b>embedding</b> algorithms we will explore generally consist of three basic stages: Define an encoder (i.e., a mapping from nodes to embeddings). Below we include a diagram to illustrate the process, encoder <b>maps</b> node and to low-dimensional vector and : Define a node similarity function (i.e., a measure of similarity in the original ...", "dateLastCrawled": "2022-01-30T23:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Word2Vec vs GloVe - A Comparative Guide to Word <b>Embedding</b> Techniques", "url": "https://analyticsindiamag.com/word2vec-vs-glove-a-comparative-guide-to-word-embedding-techniques/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/word2vec-vs-glove-a-comparative-guide-to-word-<b>embedding</b>...", "snippet": "This unsupervised learning algorithm <b>maps</b> the words into <b>space</b> where the semantic similarity between the words is observed by the distance between the words. These algorithms perform the Training of a corpus consisting of the aggregated global word-word co-occurrence statistics, and the result of the training usually represents the subspace of the words in which our interest lies. It is developed as an open-source project at Stanford and was launched in 2014.", "dateLastCrawled": "2022-01-29T03:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Modeling Semantic Similarities in Multiple Maps</b>", "url": "https://lvdmaaten.github.io/publications/papers/TR_Multiple_Maps_2009.pdf", "isFamilyFriendly": true, "displayUrl": "https://lvdmaaten.github.io/publications/papers/TR_Multiple_<b>Maps</b>_2009.pdf", "snippet": "association data. We present visualizations of the multiple <b>maps</b> that show how the model is able to, e.g., identify <b>different</b> senses of words and modeling them in <b>different</b> <b>maps</b>. Also, we show that the use of multiple <b>maps</b> if bene\ufb01cial in generalization tasks: a multiple map model is better at predicting word association than a single map model.", "dateLastCrawled": "2022-01-31T20:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "5: The char-CNN-RNN encoder <b>maps</b> images to a common <b>embedding</b> <b>space</b> ...", "url": "https://www.researchgate.net/figure/The-char-CNN-RNN-encoder-maps-images-to-a-common-embedding-space-Images-and_fig4_324783775", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/figure/The-char-CNN-RNN-encoder-<b>maps</b>-images-to-a-common...", "snippet": "5: The char-CNN-RNN encoder <b>maps</b> images to a common <b>embedding</b> <b>space</b>. Images and descriptions which match are closer to each other. Here the <b>embedding</b> <b>space</b> is R 2 to make visualisation easier.", "dateLastCrawled": "2022-01-26T09:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Multi-Adversarial Learning for Cross-Lingual Word Embeddings", "url": "https://aclanthology.org/2021.naacl-main.39.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/2021.naacl-main.39.pdf", "snippet": "<b>different</b> languages are represented by <b>similar</b> vec-tors. Following the observation ofMikolov et al. (2013) that the geometric positions of <b>similar</b> words in two <b>embedding</b> spaces of <b>different</b> languages ap-pear to be related by a linear relation, the most common method aims to map between two pre-trained monolingual <b>embedding</b> spaces by learn-", "dateLastCrawled": "2022-01-22T12:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Diffusion <b>maps</b> <b>embedding</b> and transition matrix analysis of the large ...", "url": "https://iopscience.iop.org/article/10.1088/1361-6544/ab6a76/pdf", "isFamilyFriendly": true, "displayUrl": "https://iopscience.iop.org/article/10.1088/1361-6544/ab6a76/pdf", "snippet": "By utilizing diffusion <b>maps</b> <b>embedding</b> and transition matrix analysis we investigate sparse temperature measurement time-series data from Rayleigh\u2013 B\u00e9nard convection experiments in a cylindrical container of aspect ratio \u0393=D/L = 0.5 between its diameter (D) and height (L). We consider the two cases of a cylinder at rest and rotating around its cylinder axis. We find that the relative amplitude of the large-scale circulation (LSC) and its orientation inside the container at <b>different</b> ...", "dateLastCrawled": "2021-07-19T09:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Exploring Deep <b>Embeddings</b>. Visualizing Pytorch Models with\u2026 | by ...", "url": "https://shairozsohail.medium.com/exploring-deep-embeddings-fa677f0e7c90", "isFamilyFriendly": true, "displayUrl": "https://shairozsohail.medium.com/exploring-deep-<b>embeddings</b>-fa677f0e7c90", "snippet": "The goal of this article will be to explore what this vector <b>space</b> looks like for <b>different</b> models and build a tool that will allow us to take any deep learning model and visualize its vector <b>space</b> using Tensorboard\u2019s <b>Embedding</b> Projector, TensorboardX, and Pytorch. *All of the code for this guide is available on the Github repo here*", "dateLastCrawled": "2022-01-31T06:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "[D] How is the MDP Homomorphism approach in the below paper <b>different</b> ...", "url": "https://www.reddit.com/r/MachineLearning/comments/s410dw/d_how_is_the_mdp_homomorphism_approach_in_the/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/MachineLearning/comments/s410dw/d_how_is_the_mdp_homomorphism...", "snippet": "For example, if you take an action and use the contrastive <b>embedding</b> versus taking the <b>embedding</b> then performing the action you should get the same (or <b>similar</b> if there is some lossy compression) <b>embedding</b> vector.", "dateLastCrawled": "2022-01-18T16:09:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Poincar\u00e9 <b>maps: Hyperbolic embeddings to understand how</b> cells develop", "url": "https://ai.facebook.com/blog/poincare-maps-hyperbolic-embeddings-to-understand-how-cells-develop/", "isFamilyFriendly": true, "displayUrl": "https://ai.facebook.com/blog/poincare-<b>maps-hyperbolic-embeddings-to-understand-how</b>...", "snippet": "Hyperbolic <b>space</b>, which <b>can</b> <b>be thought</b> of as a continuous version of trees, offers promising advantages for this task, including representational efficiency, small distortion of hierarchical relationships, and easy interpretability. To exploit these properties for the discovery of hierarchies from noisy measurements (as are common in current single-cell data), we developed an algorithm that connects hyperbolic embeddings with manifold learning and pseudo-temporal ordering. In our approach ...", "dateLastCrawled": "2022-01-27T14:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "5: The char-CNN-RNN encoder <b>maps</b> images to a common <b>embedding</b> <b>space</b> ...", "url": "https://www.researchgate.net/figure/The-char-CNN-RNN-encoder-maps-images-to-a-common-embedding-space-Images-and_fig4_324783775", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/figure/The-char-CNN-RNN-encoder-<b>maps</b>-images-to-a-common...", "snippet": "5: The char-CNN-RNN encoder <b>maps</b> images to a common <b>embedding</b> <b>space</b>. Images and descriptions which match are closer to each other. Here the <b>embedding</b> <b>space</b> is R 2 to make visualisation easier.", "dateLastCrawled": "2022-01-26T09:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Embed <b>a Story Map within a Story Map</b> - Esri", "url": "https://www.esri.com/arcgis-blog/products/story-maps/mapping/embedding-a-story-map-within-a-story-map/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.esri.com</b>/<b>arcgis-blog</b>/products/story-<b>maps</b>/mapping/<b>embedding</b>-<b>a-story-map</b>...", "snippet": "<b>Embedding</b> <b>a story map within a story map</b> <b>can</b> create a powerful addition to your story, however, it is not always desirable. It <b>can</b> detract from your story, and create a more complicated experience for the viewer by presenting too many options when a more streamlined approach might be better. Also, if you embed too many <b>different</b> kinds of story <b>maps</b>, the juxtaposition from one user experience to another might confuse the user, and be a jarring distraction. But used prudently and wisely ...", "dateLastCrawled": "2022-01-27T02:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Document <b>Embedding</b> Techniques. A review of notable literature on the ...", "url": "https://towardsdatascience.com/document-embedding-techniques-fed3e7a6a25d", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/document-<b>embedding</b>-techniques-fed3e7a6a25d", "snippet": "The <b>different</b> self-supervised techniques covered above extended the distributional hypothesis in <b>different</b> ways, with skip-<b>thought</b> and quick-<b>thought</b> modeling a strong relation between sentences/paragraphs based on their distance in a document. This perhaps applies trivially for books, articles and social media posts, but might not apply as strongly to other sequences of texts, especially structured ones, and might thus project your documents into an <b>embedding</b> <b>space</b> which does not apply to ...", "dateLastCrawled": "2022-02-03T01:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A Multi-View <b>Embedding</b> <b>Space</b> for Modeling Internet Images, Tags, and ...", "url": "http://slazebni.cs.illinois.edu/publications/yunchao_cca13.pdf", "isFamilyFriendly": true, "displayUrl": "slazebni.cs.illinois.edu/publications/yunchao_cca13.pdf", "snippet": "a classic technique that <b>maps</b> two views, given by visual and and textual features, into a common latent <b>space</b> where the correlation between the two views is maximized (Hotelling, cross-modal, in the sense that embed-ded vectors representing visual and textual information are treated as the same class of citizens, and thus image-to-image, text-to-image, and image-to-text retrieval tasks <b>can</b> in prin-ciple all be handled in exactly the same way. While CCA is very attractive in its simplicity ...", "dateLastCrawled": "2022-01-21T15:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>The Unreasonable Effectiveness Of Neural Network Embeddings</b> | by Peter ...", "url": "https://medium.com/aquarium-learning/the-unreasonable-effectiveness-of-neural-network-embeddings-93891acad097", "isFamilyFriendly": true, "displayUrl": "https://medium.com/aquarium-learning/<b>the-unreasonable-effectiveness-of-neural-network</b>...", "snippet": "Targeted data mining <b>can</b> <b>be thought</b> of as a search and retrieval problem that <b>can</b> be solved with neural network embeddings. <b>Embedding</b>-based search systems work by comparing a \u201cquery\u201d <b>embedding</b> ...", "dateLastCrawled": "2022-01-28T00:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Joint embedding: A scalable alignment to compare individuals in</b> a ...", "url": "https://www.sciencedirect.com/science/article/pii/S1053811920307187", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1053811920307187", "snippet": "The spatial patterns of the significance <b>maps</b> were highly similar to their own embeddings <b>maps</b> (component 1: r = 0.90; component 2 r = 0.92; component 3 r = 0.85), indicating that the apex or nadir of <b>embedding</b> components in functional <b>space</b> situate at the regions that change the most across the lifespan.", "dateLastCrawled": "2022-01-29T00:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "5 Mind <b>Maps</b> to Boost Your Podcast\u2019s <b>Thought</b> Leadership Potential - Focus", "url": "https://www.mindmeister.com/blog/5-mind-maps-to-boost-your-podcasts-thought-leadership-potential/", "isFamilyFriendly": true, "displayUrl": "https://www.mindmeister.com/blog/5-mind-<b>maps</b>-to-boost-your-podcasts-<b>thought</b>-leadership...", "snippet": "Welcome back to the final blog post of my three-part series on mind mapping for your new podcast. We\u2019ve covered a lot of ground so far. My first article explored how MindMeister <b>can</b> help you establish the \u201c3 P\u2019s\u201d of Podcasting \u2014 Purpose, Process and Production \u2014 while my second explained the \u201cPodcasting M&amp;Ms\u201d of Marketing and Monetization.In this final post, we\u2019ll examine ways to establish yourself as a <b>thought</b> leader with podcasting and look closely at the mind <b>maps</b> that ...", "dateLastCrawled": "2022-01-20T17:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Visualizing feature <b>vectors</b>/embeddings using t-SNE and PCA | by ...", "url": "https://towardsdatascience.com/visualizing-feature-vectors-embeddings-using-pca-and-t-sne-ef157cea3a42", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/visualizing-feature-<b>vectors</b>-<b>embeddings</b>-using-pca-and-t...", "snippet": "Visualization is a very powerful tool and <b>can</b> provide invaluable information. In this post, I\u2019ll be discussing two very powerful techniques that <b>can</b> help you visualise higher dimensional data in a lower-dimensional <b>space</b> to find trends and patterns, namely PCA and t-SNE. We will be taking a CNN based example and inject noise in the test dataset to do our visualization investigation. Introduction. I\u2019ll briefly talk about the two techniques before diving into how to use them. Principal ...", "dateLastCrawled": "2022-02-03T13:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "A Multi-View <b>Embedding</b> <b>Space</b> for <b>Modeling Internet Images</b>, Tags, and ...", "url": "https://link.springer.com/article/10.1007/s11263-013-0658-4", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11263-013-0658-4", "snippet": "In the latent CCA <b>space</b>, points from <b>different</b> views are directly comparable, so we <b>can</b> do I2I, image-to-tag, and T2I retrieval by nearest neighbor search. In the implementation, we select the <b>embedding</b> dimensionality \\(d\\) by measuring the retrieval accuracy in embedded spaces with <b>different</b> values of \\(d\\) on validation images set aside from each of our datasets (details will be given in Sects. 6\u20138).", "dateLastCrawled": "2022-01-14T11:42:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Context Attention Heterogeneous Network Embedding</b>", "url": "https://pubmed.ncbi.nlm.nih.gov/31531010/", "isFamilyFriendly": true, "displayUrl": "https://pubmed.ncbi.nlm.nih.gov/31531010", "snippet": "Network <b>embedding</b> (NE), which <b>maps</b> nodes into a low-dimensional latent Euclidean <b>space</b> to represent effective features of each node in the network, has obtained considerable attention in recent years. Many popular NE methods, such as DeepWalk, Node2vec, and LINE, are capable of handling homogeneous networks. However, nodes are always fully accompanied by heterogeneous information (e.g., text descriptions, node properties, and hashtags) in the real-world network, which remains a great ...", "dateLastCrawled": "2021-05-06T18:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "5: The char-CNN-RNN encoder <b>maps</b> images to a common <b>embedding</b> <b>space</b> ...", "url": "https://www.researchgate.net/figure/The-char-CNN-RNN-encoder-maps-images-to-a-common-embedding-space-Images-and_fig4_324783775", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/figure/The-char-CNN-RNN-encoder-<b>maps</b>-images-to-a-common...", "snippet": "5: The char-CNN-RNN encoder <b>maps</b> images to a common <b>embedding</b> <b>space</b>. Images and descriptions which match are closer to each other. Here the <b>embedding</b> <b>space</b> is R 2 to make visualisation easier.", "dateLastCrawled": "2022-01-26T09:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Heterogeneous Network Embedding via Deep Architectures</b>", "url": "https://www.eecs.ucf.edu/~gqi/publications/kdd2015_heterogeneous.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.eecs.ucf.edu/~gqi/publications/kdd2015_heterogeneous.pdf", "snippet": "work <b>Embedding</b> (HNE), which jointly considers both the content as well as the relational information. HNE <b>maps</b> <b>different</b> hetero-geneous objects into a uni\ufb01ed latent <b>space</b> so that objects from <b>dif-ferent</b> spaces <b>can</b> be directly <b>compared</b>. Unlike to traditional linear <b>embedding</b> models [29, 44, 50], the", "dateLastCrawled": "2022-02-03T12:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Divide and Conquer the <b>Embedding</b> <b>Space</b> for Metric Learning", "url": "https://openaccess.thecvf.com/content_CVPR_2019/papers/Sanakoyeu_Divide_and_Conquer_the_Embedding_Space_for_Metric_Learning_CVPR_2019_paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://openaccess.thecvf.com/content_CVPR_2019/papers/Sanakoyeu_Divide_and_Conquer...", "snippet": "coming from <b>different</b> classes in the learned <b>embedding</b> <b>space</b>. An <b>embedding</b> <b>space</b> with the desired properties is learned by optimizing loss functions based on pairs of images from the same or <b>different</b> class [13, 3], triplets of images [38, 45, 17] or tuples of larger number of images [20, 43, 41, 1], which express positive or negative relationships in the dataset. Existing deep metric learning approaches usually learn a single distance metric for all samples from the given data distribution ...", "dateLastCrawled": "2022-02-02T23:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Joint embedding: A scalable alignment to compare individuals in</b> a ...", "url": "https://www.sciencedirect.com/science/article/pii/S1053811920307187", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1053811920307187", "snippet": "The spatial patterns of the significance <b>maps</b> were highly similar to their own embeddings <b>maps</b> (component 1: r = 0.90; component 2 r = 0.92; component 3 r = 0.85), indicating that the apex or nadir of <b>embedding</b> components in functional <b>space</b> situate at the regions that change the most across the lifespan.", "dateLastCrawled": "2022-01-29T00:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Diffusion <b>maps</b> <b>embedding</b> and transition matrix analysis of the large ...", "url": "https://iopscience.iop.org/article/10.1088/1361-6544/ab6a76/pdf", "isFamilyFriendly": true, "displayUrl": "https://iopscience.iop.org/article/10.1088/1361-6544/ab6a76/pdf", "snippet": "Diffusion <b>maps</b> <b>embedding</b> and transition matrix analysis of the large-scale flow structure in turbulent Rayleigh\u2013B\u00e9nard convection ... inside the container at <b>different</b> points in time are associated to prominent geometric features in the <b>embedding</b> <b>space</b> spanned by the two dominant diffusion-<b>maps</b> eigenvectors. From this two-dimensional <b>embedding</b> we <b>can</b> measure azimuthal drift and diffusion rates, as well as coherence times of the LSC. In addition, we <b>can</b> distinguish from the data clearly ...", "dateLastCrawled": "2021-07-19T09:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "general topology - <b>Can</b> two <b>different</b> topological spaces cover each ...", "url": "https://math.stackexchange.com/questions/1883064/can-two-different-topological-spaces-cover-each-other", "isFamilyFriendly": true, "displayUrl": "https://math.stackexchange.com/questions/1883064/<b>can</b>-two-<b>different</b>-topological-<b>spaces</b>...", "snippet": "I have a basic understanding of covering <b>space</b> theory as its taught in school. I was inspired by this question Two covering spaces covering each other are equivalent? I have done some of the things you would do, looking at what happens to fundamental groups.", "dateLastCrawled": "2022-01-09T14:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Word2Vec vs GloVe - A Comparative Guide to Word <b>Embedding</b> Techniques", "url": "https://analyticsindiamag.com/word2vec-vs-glove-a-comparative-guide-to-word-embedding-techniques/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/word2vec-vs-glove-a-comparative-guide-to-word-<b>embedding</b>...", "snippet": "The one noticeable thing about the word2vec generated word <b>embedding</b> is that it <b>can</b> hold the word vectors such as \u201cking\u201d \u2013 \u201cman\u201d + \u201cwoman\u201d -&gt; \u201cqueen\u201d or \u201cbetter\u201d \u2013 \u201cgood\u201d + \u201cbad\u201d -&gt; \u201cworse\u201d together or close in the vector <b>space</b> where the GloVe <b>can</b> not understand such linear relationship between the words in the vector <b>space</b>. Somehow now we are able to make GloVe understand such linear relationships.", "dateLastCrawled": "2022-01-29T03:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A survey <b>of cross-lingual word embedding models</b>", "url": "https://ruder.io/cross-lingual-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://ruder.io/cross-lingual-<b>embeddings</b>", "snippet": "The ease of constructing a shared <b>embedding</b> <b>space</b> between languages and consequently the success of cross-lingual transfer is intuitively proportional to the similarity of the languages: An <b>embedding</b> <b>space</b> shared between Spanish and Portuguese tends to capture more linguistic nuances of meaning than an <b>embedding</b> <b>space</b> populated with English and Chinese representations. Furthermore, if two languages are too dissimilar, cross-linguistic transfer might not be possible at all -- similar to the ...", "dateLastCrawled": "2022-02-01T17:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "[D] How is the MDP Homomorphism approach in the below paper <b>different</b> ...", "url": "https://www.reddit.com/r/MachineLearning/comments/s410dw/d_how_is_the_mdp_homomorphism_approach_in_the/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/MachineLearning/comments/s410dw/d_how_is_the_mdp_homomorphism...", "snippet": "For example, if you take an action and use the contrastive <b>embedding</b> versus taking the <b>embedding</b> then performing the action you should get the same (or similar if there is some lossy compression) <b>embedding</b> vector.", "dateLastCrawled": "2022-01-18T16:09:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Finding the Word <b>Analogy</b> from given words using Word2Vec embeddings ...", "url": "https://www.geeksforgeeks.org/finding-the-word-analogy-from-given-words-using-word2vec-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/finding-the-word-<b>analogy</b>-from-given-words-using-word2vec...", "snippet": "In the word <b>analogy</b> task, ... What if we can use a <b>Machine</b> <b>Learning</b> algorithm to automate this task of finding the word <b>analogy</b>. In this tutorial, we will be using Word2Vec model and a pre-trained model named \u2018GoogleNews-vectors-negative300.bin\u2018 which is trained on over 50 Billion words by Google. Each word inside the pre-trained dataset is embedded in a 300-dimensional <b>space</b> and the words which are similar in context/meaning are placed closer to each other in the <b>space</b>. Methodology to ...", "dateLastCrawled": "2022-01-26T14:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Analogical <b>Embedding</b> for <b>Analogy</b>-Based <b>Learning</b> to Rank", "url": "https://link.springer.com/chapter/10.1007/978-3-030-74251-5_7", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/chapter/10.1007/978-3-030-74251-5_7", "snippet": "A suitable representation is therefore essential for the success of <b>analogy</b>-based <b>learning</b> to rank. Therefore, we propose a method for analogical <b>embedding</b>, i.e., for <b>embedding</b> the data in a target <b>space</b> such that, in this <b>space</b>, the aforementioned <b>analogy</b> assumption is as valid and strongly pronounced as possible. This is accomplished by means of a neural network with a quadruple Siamese structure, which is trained on a suitably designed set of examples in the form of quadruples of objects ...", "dateLastCrawled": "2022-01-17T09:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Analogical <b>Embedding</b> for <b>Analogy</b>-Based <b>Learning</b> to Rank", "url": "https://homepages.uni-paderborn.de/ahmadim/IDA%202021.pdf", "isFamilyFriendly": true, "displayUrl": "https://homepages.uni-paderborn.de/ahmadim/IDA 2021.pdf", "snippet": "7 Intelligent Systems and <b>Machine</b> <b>Learning</b> <b>Embedding</b> By ignoring irrelevant or noisy features, the performance can often be improved Common feature selection techniques tailored for the case of <b>analogy</b>-based <b>learning</b> to rank. <b>Analogy</b>-based <b>learning</b> to rank (able2rank) 8 Intelligent Systems and <b>Machine</b> <b>Learning</b> Extension to feature vectors Degree of <b>analogy</b>. Analogical <b>Embedding</b> 9 Intelligent Systems and <b>Machine</b> <b>Learning</b> Positive example: preferences on both sides are coherent Negative ...", "dateLastCrawled": "2022-01-06T10:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Towards Analogy-Based Explanations in Machine Learning</b> | DeepAI", "url": "https://deepai.org/publication/towards-analogy-based-explanations-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>towards-analogy-based-explanations-in-machine-learning</b>", "snippet": "More specifically, we take the view that an <b>analogy</b>-based approach is a viable alternative to existing approaches in the realm of explainable AI and interpretable <b>machine</b> <b>learning</b>, and that <b>analogy</b>-based explanations of the predictions produced by a <b>machine</b> <b>learning</b> algorithm can complement similarity-based explanations in a meaningful way. To corroborate these claims, we outline the basic idea of an <b>analogy</b>-based explanation and illustrate its potential usefulness by means of some examples.", "dateLastCrawled": "2022-01-10T12:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Classifying and completing word analogies by <b>machine</b> <b>learning</b> ...", "url": "https://www.sciencedirect.com/science/article/abs/pii/S0888613X21000141", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/abs/pii/S0888613X21000141", "snippet": "With the emergence of word <b>embedding</b> models, a lot of progress has been made in NLP, essentially assuming that a word <b>analogy</b> like m a n: k i n g:: w o m a n: q u e e n is an instance of a parallelogram within the underlying vector <b>space</b>. In this paper, we depart from this assumption to adopt a <b>machine</b> <b>learning</b> approach, i.e., <b>learning</b> a substitute of the parallelogram model. To achieve our goal, we first review the formal modeling of analogical proportions, highlighting the properties which ...", "dateLastCrawled": "2021-11-13T01:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "How to Solve <b>Analogies</b> with Word2Vec | by Khuyen Tran | Towards Data ...", "url": "https://towardsdatascience.com/how-to-solve-analogies-with-word2vec-6ebaf2354009", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/how-to-solve-<b>analogies</b>-with-word2vec-6ebaf2354009", "snippet": "To make words understood by <b>machine</b> <b>learning</b> algorithms, word <b>embedding</b> is used to map words into vectors of real numbers. There are various word <b>embedding</b> models and word2vec is one of them. In simple words, word2vec is a group of related models that are used to produce word embeddings. These models are trained to construct the linguistic contexts of words. Word2vec takes a large corpus of text and produces a vector <b>space</b>, with each unique word in the corpus being assigned to a ...", "dateLastCrawled": "2022-01-30T16:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Graph <b>Embedding</b> for Deep <b>Learning</b> | by Flawnson Tong | Towards Data Science", "url": "https://towardsdatascience.com/overview-of-deep-learning-on-graph-embeddings-4305c10ad4a4", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/overview-of-deep-<b>learning</b>-on-graph-<b>embeddings</b>-4305c10ad4a4", "snippet": "Using an <b>analogy</b> with word2vec, if a document is made of sentences (which is then made of words), then a graph is made of sub-graphs ... Graph <b>embedding</b> techniques take graphs and embed them in a lower dimensional continuous latent <b>space</b> before passing that representation through a <b>machine</b> <b>learning</b> model. Walk <b>embedding</b> methods perform graph traversals with the goal of preserving structure and features and aggregates these traversals which can then be passed through a recurrent neural ...", "dateLastCrawled": "2022-02-01T16:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "In the word <b>embedding</b> <b>space</b>, the <b>analogy</b> pairs exhibit interesting ...", "url": "https://www.researchgate.net/figure/In-the-word-embedding-space-the-analogy-pairs-exhibit-interesting-algebraic_fig1_319370400", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/figure/In-the-word-<b>embedding</b>-<b>space</b>-the-<b>analogy</b>-pairs...", "snippet": "Download scientific diagram | In the word <b>embedding</b> <b>space</b>, the <b>analogy</b> pairs exhibit interesting algebraic relationships. from publication: Visual Exploration of Semantic Relationships in Neural ...", "dateLastCrawled": "2021-12-21T11:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>What Are Word Embeddings</b> for Text? - <b>Machine</b> <b>Learning</b> Mastery", "url": "https://machinelearningmastery.com/what-are-word-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>what-are-word-embeddings</b>", "snippet": "This approach of <b>learning</b> an <b>embedding</b> layer requires a lot of training data and can be slow, but will learn an <b>embedding</b> both targeted to the specific text data and the NLP task. 2. Word2Vec. Word2Vec is a statistical method for efficiently <b>learning</b> a standalone word <b>embedding</b> from a text corpus. It was developed by Tomas Mikolov, et al. at Google in 2013 as a response to make the neural-network-based training of the <b>embedding</b> more efficient and since then has become the de facto standard ...", "dateLastCrawled": "2022-01-30T18:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Word Embeddings in NLP - GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/word-embeddings-in-nlp/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/word-<b>embeddings</b>-in-nlp", "snippet": "Word <b>Embedding</b> or Word Vector is a numeric vector input that represents a word in a lower-dimensional <b>space</b>. It allows words with similar meaning to have a similar representation. They can also approximate meaning. A word vector with 50 values can represent 50 unique features. Features: Anything that relates words to one another. Eg: Age, Sports, Fitness, Employed etc. Each word vector has values corresponding to these features. Goal of Word Embeddings. To reduce dimensionality; To use a ...", "dateLastCrawled": "2022-01-30T08:38:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Zero-shot <b>learning</b> via discriminative representation extraction ...", "url": "https://www.sciencedirect.com/science/article/pii/S0167865517303501", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0167865517303501", "snippet": "The pioneer work in ZSL can be traced to Larochelle et al. , where it verified that when test images belong to some classes that are not available at training stage, a <b>machine</b> <b>learning</b> system can still figure out what a test image is. Due to the importance of zero-shot <b>learning</b>, the number of proposed approaches has increased steadily recently.The number of new zero-shot <b>learning</b> approaches proposed every year was increasing.", "dateLastCrawled": "2021-10-30T07:08:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A self-supervised domain-general <b>learning</b> framework for human ventral ...", "url": "https://www.nature.com/articles/s41467-022-28091-4", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41467-022-28091-4", "snippet": "On this view, the <b>embedding space can be thought of as</b> a high-fidelity perceptual interface, with useful visual primitives over which separate conceptual representational systems can operate.", "dateLastCrawled": "2022-01-25T10:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Spectral Af\ufb01ne-Kernel Embeddings</b> - NSF", "url": "https://par.nsf.gov/servlets/purl/10039348", "isFamilyFriendly": true, "displayUrl": "https://par.nsf.gov/servlets/purl/10039348", "snippet": "Since <b>machine</b> <b>learn-ing</b> algorithms struggle with high dimensions (an issue known as the curse of dimensionality in this context), one typically needs to map these data points from their high-dimensional space into a lower dimensional space without signi\ufb01cant distortion. Mapping data (living in RD with D\u02db1 but sampling a manifold of low in-trinsic dimensionality d \u02ddD) into a low-dimensional <b>embedding space can be thought of as</b> a preliminary feature extraction step in <b>machine</b> <b>learning</b> ...", "dateLastCrawled": "2022-01-29T21:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Predicting affinity ties in a surname network", "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0256603", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0256603", "snippet": "<b>Machine</b> <b>learning</b>-based approaches for knowledge graph completion To cover the broadest possible range of methods and architectures in the evaluation, we identified representative methods of different model families, taking care that these methods achieve state-of-the-art performances in knowledge graph completion and have open-source implementations that favor the reproducibility of the reported results.", "dateLastCrawled": "2021-09-03T03:18:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(embedding space)  is like +(different maps)", "+(embedding space) is similar to +(different maps)", "+(embedding space) can be thought of as +(different maps)", "+(embedding space) can be compared to +(different maps)", "machine learning +(embedding space AND analogy)", "machine learning +(\"embedding space is like\")", "machine learning +(\"embedding space is similar\")", "machine learning +(\"just as embedding space\")", "machine learning +(\"embedding space can be thought of as\")", "machine learning +(\"embedding space can be compared to\")"]}