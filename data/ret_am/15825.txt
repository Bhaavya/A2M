{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Interrater</b> reliability: the kappa statistic", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3900052/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC3900052", "snippet": "<b>Like</b> most correlation statistics, the kappa can range from \u22121 to +1. While the kappa is one of the most commonly used statistics to test <b>interrater</b> reliability, it has limitations. Judgments about what level of kappa should be acceptable for health research are questioned. Cohen\u2019s suggested interpretation may be too lenient for health related studies because it implies that a score as low as 0.41 might be acceptable. Kappa and percent <b>agreement</b> are compared, and levels for both kappa and ...", "dateLastCrawled": "2022-01-26T07:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Inter-Rater Reliability</b> in Psychology: Definition &amp; Formula - Video ...", "url": "https://study.com/academy/lesson/inter-rater-reliability-in-psychology-definition-formula-quiz.html", "isFamilyFriendly": true, "displayUrl": "https://study.com/academy/lesson/<b>inter-rater-reliability</b>-in-psychology-definition...", "snippet": "<b>Inter-rater reliability</b> (IRR) ensures that there is a level of <b>agreement</b> between judges in a given competition. Learn about Cohen&#39;s Kappa and Spearman&#39;s Rho, which are the two most commonly used ...", "dateLastCrawled": "2022-01-29T08:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Peer Review Evaluation Process of <b>Marie Curie</b> Actions under EU\u2019s ...", "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0130753", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0130753", "snippet": "Table A. <b>Inter-rater</b> <b>agreement</b> (average deviation index, AD index) for individual evaluation criteria across all evaluation panels. Table B. Distribution of proposals, across panels and type of action, where a) one rater disagrees with other two raters; b) all raters disagree with each other; c) difference between the <b>Consensus</b> Report (CR) and average Individual Evaluation Report (AVIER) score is large. Table C. Pearson\u2019s correlations between IER and CR scores for separate criteria.", "dateLastCrawled": "2021-09-03T04:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Intraclass Correlations (ICC</b>) and <b>Interrater Reliability in SPSS</b>", "url": "https://neoacademic.com/2011/11/16/computing-intraclass-correlations-icc-as-estimates-of-interrater-reliability-in-spss/", "isFamilyFriendly": true, "displayUrl": "https://neoacademic.com/2011/11/16/computing-<b>intraclass-correlations-icc</b>-as", "snippet": "I would <b>like</b> to ask you a question about how to analyze <b>inter-rater</b> <b>agreement</b>. I created a prosodic reading scale with 7 items. Each item has four possible options, each option is perfectly described.120 children were evaluated, 2 raters rate 60 children, and others 2 rater the other 60 children.", "dateLastCrawled": "2022-01-31T10:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Interrater</b> <b>Reliability</b> in Systematic Review Methodology: Exploring ...", "url": "https://journals.sagepub.com/doi/full/10.1177/0049124118799372", "isFamilyFriendly": true, "displayUrl": "https://journals.sagepub.com/doi/full/10.1177/0049124118799372", "snippet": "Differences of <b>opinion</b> have to be resolved and <b>agreement</b> reached between the research team members. Surprisingly, little attention is paid to reporting the details of <b>interrater</b> <b>reliability</b> (IRR) when multiple coders are used to make decisions at various points in the screening and data extraction stages of a study. Often IRR results are reported summarily as a percentage of <b>agreement</b> between various coders, if at all. Sometimes the <b>agreement</b> is qualified by a \u03ba or similar \u201cchance ...", "dateLastCrawled": "2022-02-01T05:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A <b>consensus</b> definition and rating scale for minimalist shoes", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4543477/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC4543477", "snippet": "A significant rank effect (p &lt; 0.001) confirmed the MI&#39;s discriminative validity. Excellent intra- and <b>inter-rater</b> reliability was found for total MI score (ICC = 0.84-0.99) and for weight, stack height, heel to toe drop and flexibility subscales (AC1 = 0.82-0.99), while good <b>inter-rater</b> reliability was found for technologies (AC1 = 0.73). Conclusion. This standardised definition of minimalist shoes developed by an international panel of experts will improve future research on minimalist ...", "dateLastCrawled": "2021-12-25T02:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>agreement</b> statistics - <b>Inter-rater</b> reliability with many non ...", "url": "https://stats.stackexchange.com/questions/14781/inter-rater-reliability-with-many-non-overlapping-raters", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/14781", "snippet": "I would <b>like</b> to calculate some measure of <b>inter-rater</b> reliability for the ratings, something better than a simply looking at <b>consensus</b>. I believe, however, that Fleiss Kappa, which is the measure I know best, would require a consistent group of raters for the entire set of items, and so I cannot use Fleiss Kappa to check IRR with my data. Is this correct? Is there another method I could use? Any advice would be much appreciated! reliability <b>agreement</b>-statistics cohens-kappa. Share. Cite ...", "dateLastCrawled": "2022-01-21T15:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Consensus</b> on the exercise and dosage variables of an exercise training ...", "url": "https://bmjopen.bmj.com/content/10/5/e037656", "isFamilyFriendly": true, "displayUrl": "https://bmjopen.bmj.com/content/10/5/e037656", "snippet": "<b>Consensus</b>: the extent to which the group of experts share the same <b>opinion</b>.55. <b>Agreement</b>: a measure of <b>inter-rater</b> <b>agreement</b> where the rating of one expert can be predicted by the rating of another.59. Stability: the consistency of responses between successive rounds.55 57", "dateLastCrawled": "2022-01-26T04:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Intercoder <b>Reliability</b> in Qualitative Research: Debates and Practical ...", "url": "https://journals.sagepub.com/doi/full/10.1177/1609406919899220", "isFamilyFriendly": true, "displayUrl": "https://journals.sagepub.com/doi/full/10.1177/1609406919899220", "snippet": "ICR is a numerical measure of the <b>agreement</b> between different coders regarding how the same data should be coded. ICR is sometimes conflated with <b>interrater</b> <b>reliability</b> (IRR), and the two terms are often used interchangeably. However, technically IRR refers to cases where data are rated on some ordinal or interval scale (e.g., the intensity of an emotion), whereas ICR is appropriate when categorizing data at a nominal level (e.g., the presence or absence of an emotion).", "dateLastCrawled": "2022-02-03T01:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>What does &#39;inter-rater reliability&#39; mean? - Quora</b>", "url": "https://www.quora.com/What-does-inter-rater-reliability-mean", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>What-does-inter-rater-reliability-mean</b>", "snippet": "Answer (1 of 3): The reliability of a test score (or any inferred statistic) refers to how consistent it is from one measurement to another. <b>Inter-rater</b> reliability is a measure of how reliable the score is when different people grade the same performance, task, test, etc. Many standardized ess...", "dateLastCrawled": "2022-01-26T14:12:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Interrater</b> reliability: the kappa statistic", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3900052/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC3900052", "snippet": "<b>Similar</b> to correlation coefficients, it can range from \u22121 to +1, where 0 represents the amount of <b>agreement</b> that can be expected from random chance, and 1 represents perfect <b>agreement</b> between the raters. While kappa values below 0 are possible, Cohen notes they are unlikely in practice . As with all correlation statistics, the kappa is a standardized value and thus is interpreted the same across multiple studies. Cohen suggested the Kappa result be interpreted as follows: values \u2264 0 as ...", "dateLastCrawled": "2022-01-26T07:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Assessing educational validity of the Morbidity and Mortality ...", "url": "https://pubmed.ncbi.nlm.nih.gov/14972297/", "isFamilyFriendly": true, "displayUrl": "https://pubmed.ncbi.nlm.nih.gov/14972297", "snippet": "Purpose: To assess <b>inter-rater</b> <b>agreement</b> in perceptions of cases presented during Morbidity &amp; Mortality conference (M&amp;M) and changes associated with initiation of a modified M&amp;M. Methods: Faculty, residents, fellows, and students at weekly M&amp;M between June 2001 and March 2002 voluntarily completed an anonymous questionnaire after each M&amp;M case presentation, which asked: if the complication was avoidable (yes/no/not sure), if <b>consensus</b> was reached among participants (yes/no/not sure), the ...", "dateLastCrawled": "2021-12-31T07:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Inter-rater and test\u2013retest reliability</b> of quality assessments by ...", "url": "https://bmjopen.bmj.com/content/2/4/e001368", "isFamilyFriendly": true, "displayUrl": "https://bmjopen.bmj.com/content/2/4/e001368", "snippet": "Similarly, differences in rater <b>opinion</b> regarding what constitutes an \u2018adequate\u2019 description of withdrawals, inclusion/exclusion criteria or adverse effects led to poor <b>agreement</b> on these questions. To improve <b>inter-rater</b> <b>agreement</b> among inexperienced raters, we suggest a pilot phase wherein raters rate the quality of a subsample of articles to allow for the identification and clarification of areas of ambiguity. We recognise that any strategy to improve reliability will be limited by ...", "dateLastCrawled": "2022-01-30T19:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Development of a Standardized MRI Scoring Tool for CNS Demyelination in ...", "url": "http://www.ajnr.org/content/34/6/1271", "isFamilyFriendly": true, "displayUrl": "<b>www.ajnr.org</b>/content/34/6/1271", "snippet": "The poor <b>inter-rater</b> <b>agreement</b> of the sixth parameter, \u201csubcortical lesions,\u201d was due to a difference in <b>opinion</b> among raters on what represented subcortical white matter. One rater referred to all supratentorial white matter extending between the cortical ribbon and the lateral ventricles as subcortical; therefore, a lesion located in the supratentorial white matter that did not abut the cortex or lateral ventricle was scored as subcortical. The other rater viewed the supratentorial ...", "dateLastCrawled": "2022-02-03T09:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Testing the face validity and <b>inter-rater</b> <b>agreement</b> of a simple ...", "url": "https://www.sciencedirect.com/science/article/pii/S1532046419302758", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1532046419302758", "snippet": "Two chance corrected <b>agreement</b> coefficients were used to assess <b>inter-rater</b> reliability on the overall sufficiency of evidence for the 5 drug pairs used in the study. Conger\u2019s Kappa was used because it applies to multiple-raters and Gwet\u2019s AC 1 was chosen because it is a paradox-resistant alternative to kappa coefficient when the overall percentage <b>agreement</b> is high [16] .", "dateLastCrawled": "2022-01-27T16:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Should you use <b>inter-rater reliability in qualitative coding</b>?", "url": "https://www.quirkos.com/blog/post/inter-rater-reliaiblity-qualitative-coding-data/", "isFamilyFriendly": true, "displayUrl": "https://www.quirkos.com/blog/post/<b>inter-rater</b>-reliaiblity-qualitative-coding-data", "snippet": "All these are methods of calculating what is called \u2018<b>inter-rater</b> reliability\u2019 (IRR or RR) \u2013 how much raters agree about something. These tests are very common in psychology where they are used for having multiple people give binary diagnostics (positive/negative diagnoses), or delivering standardised tests \u2013 both situations that are probably better suited to measures of Kappa than qualitative coding. But before qualitative researchers use any method of <b>interrater</b> reliablity, they ...", "dateLastCrawled": "2022-02-03T15:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Inter-rater</b> variability of visual interpretation and comparison with ...", "url": "https://link.springer.com/article/10.1007/s00259-016-3591-2", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00259-016-3591-2", "snippet": "<b>Inter-rater</b> <b>agreement</b> was almost perfect (\u03ba = 0.88 in ternary and \u03ba = 0.89 in binary criteria) in visual interpretation of 11 C-PiB PET images in J-ADNI study, a Japanese multicenter trial. The positive or negative decision by visual interpretation was dichotomized by a cut-off value of mcSUVR = 1.5. In addition, significant positive/negative associations were observed between mcSUVR and the number of positive/negative interpretation. As cases of disagreement among observers tended to show ...", "dateLastCrawled": "2021-12-08T07:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Automated Consensus Moderation as a</b> <b>Tool for Ensuring Reliability in</b> a ...", "url": "https://coed.asee.org/wp-content/uploads/2020/08/2-Automated-Consensus-Moderation-as-a-Tool-for-Ensuring-Reliability-in-a-Multi-Marker-Environment.pdf", "isFamilyFriendly": true, "displayUrl": "https://coed.asee.org/wp-content/uploads/2020/08/2-<b>Automated-Consensus-Moderation-as-a</b>...", "snippet": "moderation is proposed in which the <b>opinion</b> of an individual marker on a speci\ufb01c criteria point on a rubric is cast as a vote. In this process a majority vote determines the successful completion of the speci\ufb01c rubric criteria point. Tests are conducted in order to determine whether such an automated <b>consensus</b> moderation process yields more reliable results than that of individual markers. Using Krippendorf\u2019s the average level of <b>agreement</b> between individual markers on 4 programming ...", "dateLastCrawled": "2021-11-01T20:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Interrater</b> <b>Reliability</b> in Systematic Review Methodology: Exploring ...", "url": "https://journals.sagepub.com/doi/full/10.1177/0049124118799372", "isFamilyFriendly": true, "displayUrl": "https://journals.sagepub.com/doi/full/10.1177/0049124118799372", "snippet": "Differences of <b>opinion</b> have to be resolved and <b>agreement</b> reached between the research team members. Surprisingly, little attention is paid to reporting the details of <b>interrater</b> <b>reliability</b> (IRR) when multiple coders are used to make decisions at various points in the screening and data extraction stages of a study. Often IRR results are reported summarily as a percentage of <b>agreement</b> between various coders, if at all. Sometimes the <b>agreement</b> is qualified by a \u03ba or <b>similar</b> \u201cchance ...", "dateLastCrawled": "2022-02-01T05:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>agreement</b> statistics - <b>Inter-rater</b> reliability with many non ...", "url": "https://stats.stackexchange.com/questions/14781/inter-rater-reliability-with-many-non-overlapping-raters", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/14781", "snippet": "88 different raters provided judgments for the task, and no one rater completed more about 800 judgments. Most provided significantly fewer than that. I would like to calculate some measure of <b>inter-rater</b> reliability for the ratings, something better than a simply looking at <b>consensus</b>. I believe, however, that Fleiss Kappa, which is the measure ...", "dateLastCrawled": "2022-01-21T15:49:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Characteristics of EEG Interpreters Associated With Higher <b>Interrater</b> ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5784780/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC5784780", "snippet": "As the number of EEGers used for scoring is increased, the amount of change in the <b>consensus</b> <b>opinion</b> decreases steadily and is quite low as the group size approaches 10. Conclusions. The IRA among EEGers varies considerably. The EEGers must be tested before use as scorers for ET annotation research projects. The American Board of Clinical Neurophysiology certification is associated with improved performance. The optimal size for a group of experts scoring ETs in EEG is probably in the 6 to ...", "dateLastCrawled": "2021-07-31T05:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Interrater Agreement for Consensus Definitions</b> of Delayed ...", "url": "https://www.researchgate.net/publication/303798116_Interrater_Agreement_for_Consensus_Definitions_of_Delayed_Ischemic_Events_After_Aneurysmal_Subarachnoid_Hemorrhage", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/303798116_<b>Interrater</b>_<b>Agreement</b>_for_<b>Consensus</b>...", "snippet": "<b>Inter-rater agreement for consensus definitions</b> of ... This complication had long been <b>thought</b> to occur secondary to severe cerebral vasospasm, but expert <b>opinion</b> now favors a multifactorial ...", "dateLastCrawled": "2021-08-28T10:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>We you thought were utter trash. So</b>, how - Free Scholaship Essays Examples", "url": "https://finnolux.com/inter-rater-reliability-in-psychology-definition-formula/", "isFamilyFriendly": true, "displayUrl": "https://finnolux.com/<b>inter-rater</b>-reliability-in-psychology-definition-formula", "snippet": "That\u2019s where <b>inter-rater</b> reliability (IRR) comes in. <b>Inter-rater</b> reliability is a level of <b>consensus</b> among raters. In the case of our art competition, the judges are the raters. Even though there is no way to describe \u2018best,\u2019 we <b>can</b> give the judges some outside pieces that they <b>can</b> use to calibrate their judgments so that they are all in tune with each other\u2019s performances.For example, we <b>can</b> ask them to rate the pieces on aspects like \u2018originality,\u2019 \u2018caliber of technique ...", "dateLastCrawled": "2021-12-26T22:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Inter-Rater Reliability</b> in Psychology: Definition &amp; Formula - Video ...", "url": "https://study.com/academy/lesson/inter-rater-reliability-in-psychology-definition-formula-quiz.html", "isFamilyFriendly": true, "displayUrl": "https://study.com/academy/lesson/<b>inter-rater-reliability</b>-in-psychology-definition...", "snippet": "<b>Inter-rater reliability</b> (IRR) ensures that there is a level of <b>agreement</b> between judges in a given competition. Learn about Cohen&#39;s Kappa and Spearman&#39;s Rho, which are the two most commonly used ...", "dateLastCrawled": "2022-01-29T08:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Interrater</b> <b>Reliability</b> in Systematic Review Methodology: Exploring ...", "url": "https://journals.sagepub.com/doi/full/10.1177/0049124118799372", "isFamilyFriendly": true, "displayUrl": "https://journals.sagepub.com/doi/full/10.1177/0049124118799372", "snippet": "IRR or intercoder <b>agreement</b> <b>can</b> be defined as \u201cthe extent to which independent coders evaluate a characteristic of a message or artefact and reach the same conclusions\u201d (Lombard et al. 2002:589). Intrarater <b>reliability</b> on the other hand measures the extent to which one person will interpret the data in the same way and assign it the same code over time. Thus, <b>reliability</b> across multiple coders is measured by IRR and <b>reliability</b> over time for the same coder is measured by intrarater ...", "dateLastCrawled": "2022-02-01T05:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A <b>consensus</b> definition and rating scale for minimalist shoes", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4543477/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC4543477", "snippet": "A significant rank effect (p &lt; 0.001) confirmed the MI&#39;s discriminative validity. Excellent intra- and <b>inter-rater</b> reliability was found for total MI score (ICC = 0.84-0.99) and for weight, stack height, heel to toe drop and flexibility subscales (AC1 = 0.82-0.99), while good <b>inter-rater</b> reliability was found for technologies (AC1 = 0.73). Conclusion. This standardised definition of minimalist shoes developed by an international panel of experts will improve future research on minimalist ...", "dateLastCrawled": "2021-12-25T02:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Testing the face validity and <b>inter-rater</b> <b>agreement</b> of a simple ...", "url": "https://www.sciencedirect.com/science/article/pii/S1532046419302758", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1532046419302758", "snippet": "We sought to recruit 15 experts because this was <b>thought</b> to be sufficient to evaluate <b>inter-rater</b> reliability and a reasonable sample of the small population of experts whose work involves evaluating DDI evidence. Individuals who assisted in the design of DRIVE were excluded. Our recruitment strategy included sending email invitations to (1) all members of the three workgroups who participated in a <b>consensus</b> building conference series focusing on DDI evidence and decision support (3), and (2 ...", "dateLastCrawled": "2022-01-27T16:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Inter-rater reliability of the Bereavement Risk Assessment Tool</b> ...", "url": "https://www.cambridge.org/core/journals/palliative-and-supportive-care/article/abs/interrater-reliability-of-the-bereavement-risk-assessment-tool/9C91D92389F182D07376E047B0C25267", "isFamilyFriendly": true, "displayUrl": "https://www.cambridge.org/core/journals/palliative-and-supportive-care/article/abs/...", "snippet": "<b>Inter-rater</b> <b>Agreement</b>. For the 31 BRAT items used in this study, values of Fleiss\u2019 kappa ranged from 0.05 to 0.97. Six items had kappa values less than 0.4 (slight to fair <b>agreement</b>) and the remaining 25 items (81%) had kappa values above 0.4 (moderate to almost perfect <b>agreement</b>) (Landis &amp; Koch, Reference Landis and Koch 1977).", "dateLastCrawled": "2022-01-25T00:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "PERSON: A general model of interpersonal perception | Request PDF", "url": "https://www.researchgate.net/publication/8261729_PERSON_A_general_model_of_interpersonal_perception", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/8261729_PERSON_A_general_model_of...", "snippet": "<b>Consensus</b> between perceivers (i.e., <b>inter-rater</b> <b>agreement</b>) <b>can</b> be a prerequisite to establish accuracy (Funder, 1995(Funder, , 1999Kenny, 1994 Kenny, , 2004 as it psychometrically represents the ...", "dateLastCrawled": "2022-01-29T22:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Pros and Cons of <b>Consensus Decision Making</b>", "url": "https://www.thebalancecareers.com/consensus-decision-making-pros-and-cons-4178335", "isFamilyFriendly": true, "displayUrl": "https://www.thebalancecareers.com/<b>consensus-decision-making-pros-and</b>-cons-4178335", "snippet": "With 100 percent <b>agreement</b>, you <b>can</b> move forward with confidence, and you don\u2019t have to worry about another employee working to undermine your efforts. Involved Employees See a Benefits . To get everyone to agree, it generally (but not always) means that the decision made will benefit every group within the team or organization. You\u2019re not sacrificing good HR, for example, to make finance happy, or vice versa. You Present a Unified Front . Leadership teams often have to make decisions ...", "dateLastCrawled": "2022-02-03T01:02:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Comparison between <b>Inter-rater</b> Reliability and <b>Inter-rater</b> <b>Agreement</b> in ...", "url": "https://www.researchgate.net/publication/46256628_Comparison_between_Inter-rater_Reliability_and_Inter-rater_Agreement_in_Performance_Assessment", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/46256628_Comparison_between_<b>Inter-rater</b>...", "snippet": "<b>Inter-rater</b> <b>agreement</b> and <b>inter-rater</b> reliability <b>can</b> but do not necessarily coexist. The presence of one does not guarantee that of the other. <b>Inter-rater</b> <b>agreement</b> and <b>inter-rater</b> reliability ...", "dateLastCrawled": "2022-01-30T15:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Interrater</b> reliability: the kappa statistic", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3900052/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC3900052", "snippet": "For percent <b>agreement</b>, 61% <b>agreement</b> <b>can</b> immediately be seen as problematic. Almost 40% of the data in the dataset represent faulty data. In healthcare research, this could lead to recommendations for changing practice based on faulty evidence. For a clinical laboratory, having 40% of the sample evaluations being wrong would be an extremely serious quality problem. This is the reason that many texts recommend 80% <b>agreement</b> as the minimum acceptable <b>interrater</b> <b>agreement</b>. Given the reduction ...", "dateLastCrawled": "2022-01-26T07:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Assessing educational validity of the Morbidity and Mortality ...", "url": "https://pubmed.ncbi.nlm.nih.gov/14972297/", "isFamilyFriendly": true, "displayUrl": "https://pubmed.ncbi.nlm.nih.gov/14972297", "snippet": "Purpose: To assess <b>inter-rater</b> <b>agreement</b> in perceptions of cases presented during Morbidity &amp; Mortality conference (M&amp;M) and changes associated with initiation of a modified M&amp;M. Methods: Faculty, residents, fellows, and students at weekly M&amp;M between June 2001 and March 2002 voluntarily completed an anonymous questionnaire after each M&amp;M case presentation, which asked: if the complication was avoidable (yes/no/not sure), if <b>consensus</b> was reached among participants (yes/no/not sure), the ...", "dateLastCrawled": "2021-12-31T07:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Measuring <b>inter-rater</b> reliability for nominal data \u2013 which coefficients ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4974794/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC4974794", "snippet": "We <b>compared</b> the performance of Fleiss\u2019 K and Krippendorff\u2019s alpha as measures of <b>inter-rater</b> reliability. Both coefficients are highly flexible as they <b>can</b> handle two or more raters and categories. In our simulation study as well as in a case study, point estimates of Fleiss\u2019 K and Krippendorff\u2019s alpha were very similar and were not associated with over- or underestimation. The asymptotic confidence interval for Fleiss\u2019 K led to a very low coverage probability, while the standard ...", "dateLastCrawled": "2022-01-29T12:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Intraclass Correlations (ICC</b>) and <b>Interrater Reliability in SPSS</b>", "url": "https://neoacademic.com/2011/11/16/computing-intraclass-correlations-icc-as-estimates-of-interrater-reliability-in-spss/", "isFamilyFriendly": true, "displayUrl": "https://neoacademic.com/2011/11/16/computing-<b>intraclass-correlations-icc</b>-as", "snippet": "<b>Consensus</b> doesn\u2019t influence which statistic is appropriate \u2013 it should just be close to 1 if they are mostly in <b>consensus</b>. The only exception would be if they agree 100% of the time \u2013 then you would not be able to calculate ICC. I\u2019d actually say that your scale is double-barreled \u2013 i.e. you are assessing both quality (1-5) and ability to be used as a diagnostic tool (0 vs 1-5). In that case, I\u2019d probably use ICC for the quality scale and kappa for the diagnostic element (recoding ...", "dateLastCrawled": "2022-01-31T10:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "WR <b>Agreement</b> Paper - White Rose Research Online", "url": "https://eprints.whiterose.ac.uk/116973/3/WR%20agreement%20paper_JSAMA_revision_130616.pdf", "isFamilyFriendly": true, "displayUrl": "https://eprints.whiterose.ac.uk/116973/3/WR <b>agreement</b> paper_JSAMA_revision_130616.pdf", "snippet": "7 Design: Diagnostic accuracy and <b>inter-rater</b> <b>agreement</b> study 8 Methods: ... The accuracy of these ratings were <b>compared</b> to 12 <b>consensus</b> expert <b>opinion</b> by calculating mean sensitivity and specificity across raters. The 13 W\u30ee \uff97S\u250cI\uff77H\uff77\uff89\uff77\u30c7\u251e \uff97a S\uff97I\u30c7\uff97 \u30b2 SWI\uff77\u30b2\uff77\uff97\uff90\u30b2 \u2518;\u30b2 additionally assessed using raw <b>agreement</b> and Gwets AC1 14 chance corrected <b>agreement</b> coefficient. 15 Results: Forty rugby medicine doctors were included in the study. <b>Compared</b> to the expert ...", "dateLastCrawled": "2021-12-26T20:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Cone-beam Computed Tomography Uses in Clinical Endodontics: Observer ...", "url": "https://pubmed.ncbi.nlm.nih.gov/28024758/", "isFamilyFriendly": true, "displayUrl": "https://pubmed.ncbi.nlm.nih.gov/28024758", "snippet": "In addition, experience leads to better <b>inter-rater</b> reliability. In neither of these 2 categories was <b>agreement</b> found to be excellent, suggesting that more <b>can</b> be done t \u2026 Cone-beam Computed Tomography Uses in Clinical Endodontics: Observer Variability in Detecting Periapical Lesions J Endod. 2017 Feb;43(2):184-187. doi: 10.1016/j.joen.2016.10.007. Epub 2016 Dec 23. Authors Jeffrey M Parker 1 , Andr\u00e9 Mol 2 , Eric M Rivera 3 , Peter Z Tawil 3 Affiliations 1 Department of Endodontics, UNC ...", "dateLastCrawled": "2021-11-15T22:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Interobserver <b>agreement</b> and <b>consensus</b> over the esthetic evaluation of ...", "url": "https://www.researchgate.net/publication/7747185_Interobserver_agreement_and_consensus_over_the_esthetic_evaluation_of_conservative_treatment_for_breast_cancer", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/7747185", "snippet": "In contrast <b>inter-rater</b> <b>agreement</b> was only slight to fair (mk = 0.1-0.3). <b>Agreement</b> between the panel participants and the software was fair (wk = 0.24-0.45). Subjective third party assessment ...", "dateLastCrawled": "2022-01-07T18:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "The reliability and diagnostic validity of clinical manifestations of ...", "url": "https://iro.uiowa.edu/esploro/outputs/doctoral/The-reliability-and-diagnostic-validity-of/9983776899202771", "isFamilyFriendly": true, "displayUrl": "https://iro.uiowa.edu/esploro/outputs/doctoral/The-reliability-and-diagnostic-validity...", "snippet": "Briefly, these clinical manifestations were <b>compared</b> against three diagnostic criteria for CAUTI based on microbiologic and molecular methods, and their <b>inter-rater</b> reliability was examined using assessments conducted by three advanced practice nurses. Because significant microbial growth was only present in two urine samples, the diagnostic validity of these manifestations could not be established. However, it was possible to examine the <b>inter-rater</b> reliability of these manifestations. To ...", "dateLastCrawled": "2022-01-05T16:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>What does &#39;inter-rater reliability&#39; mean? - Quora</b>", "url": "https://www.quora.com/What-does-inter-rater-reliability-mean", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>What-does-inter-rater-reliability-mean</b>", "snippet": "Answer (1 of 3): The reliability of a test score (or any inferred statistic) refers to how consistent it is from one measurement to another. <b>Inter-rater</b> reliability is a measure of how reliable the score is when different people grade the same performance, task, test, etc. Many standardized ess...", "dateLastCrawled": "2022-01-26T14:12:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Understanding Interobserver <b>Agreement</b>: The Kappa Statistic", "url": "http://web2.cs.columbia.edu/~julia/courses/CS6998/Interrater_agreement.Kappa_statistic.pdf", "isFamilyFriendly": true, "displayUrl": "web2.cs.columbia.edu/~julia/courses/CS6998/<b>Interrater</b>_<b>agreement</b>.Kappa_statistic.pdf", "snippet": "call the <b>analogy</b> of a target and how close we get to the bull\u2019s-eye (Figure 1). If we actually hit the bull\u2019s-eye (representing <b>agreement</b> with the gold standard), we are accurate. If all our shots land together, we have good precision (good reliability). If all our shots land together and we hit the bull\u2019s-eye, we are accurate as well as precise. It is possible, however, to hit the bull\u2019s-eye purely by chance. Referring to Figure 1, only the center black dot in target A is accurate ...", "dateLastCrawled": "2022-01-28T23:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Leveraging Inter-rater Agreement for Audio-Visual Emotion Recognition</b>", "url": "https://www.researchgate.net/publication/283487589_Leveraging_Inter-rater_Agreement_for_Audio-Visual_Emotion_Recognition", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/283487589_Leveraging_<b>Inter-rater</b>_<b>Agreement</b>...", "snippet": "In <b>machine</b> <b>learning</b> tasks an actual \u2018ground truth\u2019 may not be available. Then, machines often have to rely on human labelling of data. This becomes challenging the more subjective the <b>learning</b> ...", "dateLastCrawled": "2021-08-28T14:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Machine</b> <b>learning</b> glossary - DataTime", "url": "https://www.dtalg.com/article-1/", "isFamilyFriendly": true, "displayUrl": "https://www.dtalg.com/article-1", "snippet": "See also Cohen\u2019s kappa, which is one of the most popular <b>inter-rater</b> <b>agreement</b> measurements. intersection over union (IoU) #image. The intersection of two sets divided by their union. In <b>machine</b>-<b>learning</b> image-detection tasks, IoU is used to measure the accuracy of the model\u2019s predicted bounding box with respect to the ground-truth bounding ...", "dateLastCrawled": "2022-01-25T17:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Recognition of Design Fixation via Body Language Using Computer Vision", "url": "https://www.hindawi.com/journals/mpe/2021/6649300/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.hindawi.com</b>/journals/mpe/2021/6649300", "snippet": "An <b>inter-rater</b> <b>agreement</b> statistic was conducted to judge the extent of <b>agreement</b> between the experts who rated the data. We calculated the percentage <b>agreement</b> for design fixation. This measure indicates how often raters who rated the fixation item on the same sketches choose the same response category. We considered the highest number of similar ratings per sketch as <b>agreement</b>, and the other ratings as nonagreement. The percentage <b>agreement</b> was calculated by dividing the number of ratings ...", "dateLastCrawled": "2022-01-28T11:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Clinician perspectives on <b>machine</b> <b>learning</b> prognostic algorithms in the ...", "url": "https://link.springer.com/article/10.1007/s00520-021-06774-w", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00520-021-06774-w", "snippet": "<b>Machine</b> <b>learning</b> algorithms may accurately predict mortality risk in cancer, but it is unclear how oncology clinicians would use such algorithms in practice. The purpose of this qualitative study was to assess oncology clinicians\u2019 perceptions on the utility and barriers of <b>machine</b> <b>learning</b> prognostic algorithms to prompt advance care planning. Participants included medical oncology physicians and advanced practice providers (APPs) practicing in tertiary and community practices within a ...", "dateLastCrawled": "2022-01-30T10:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Evaluation of Automated Hypnogram Analysis on Multi-Scored ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8521900/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8521900", "snippet": "The <b>inter-rater</b> reliability on the multi-labeled dataset (84.1%) is barely higher than the comparison between the algorithm and the expert&#39;s hypnograms (82.7%). This suggests that the algorithm reaches an almost human accuracy but due to the <b>inter-rater</b> variability this is not clear from the accuracy value itself. Studying the inter-expert variability, the epochs can be categorized based on the level of <b>agreement</b> between the experts. An accuracy of 97.8% is reached considering only epochs in ...", "dateLastCrawled": "2022-01-11T20:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Analyzing and Interpreting Data From Rating Scales</b> | by Kevin C Lee ...", "url": "https://towardsdatascience.com/analyzing-and-interpreting-data-from-rating-scales-d169d66211db", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>analyzing-and-interpreting-data-from-rating-scales</b>-d169...", "snippet": "<b>Inter-Rater</b> Reliability. In B), we plot the pairwise correlations between the students with a heatmap. Most of the correlations are &gt; 0.6 with a few exceptions. A small number of respondents showing low correlations with others is acceptable as long as most students are able to respond similarly. P.S. The use of Pearson Correlation is only ...", "dateLastCrawled": "2022-01-29T00:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) Are the Sequential Interactive Effects of Two Active <b>Learning</b> ...", "url": "https://www.academia.edu/69179129/Are_the_Sequential_Interactive_Effects_of_Two_Active_Learning_Strategies_Synergistic_The_Use_of_the_Socratic_Method_of_Questioning_and_Ability_Based_Learning_Techniques_to_Enhance_Student_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/69179129/Are_the_Sequential_Interactive_Effects_of_Two_Active...", "snippet": "The instrument is an assessment measure to facilitate <b>learning</b> based on Benjamin 376 S. Bloom\u2019s <b>Learning</b> for Mastery, which measures students\u2019 knowledge of Bloom\u2019s concepts and is 377 contained in the Handbook on Formative and Summative Evaluation of Student <b>Learning</b> (Educational 378 Testing Services, 2000). The device is described as being used for higher education, college students, 379 diagnostic testing, pretest-posttest assessment, <b>learning</b> activities, which makes the adoption of ...", "dateLastCrawled": "2022-01-28T03:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Creating and detecting fake reviews of online products - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S0969698921003374", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0969698921003374", "snippet": "Fleiss\u2019 Kappa was used to assess the <b>inter-rater</b> reliability between the three <b>machine</b> <b>learning</b> models (H2) and three raters (H3). This metric indicates the degree of <b>agreement</b> in classification over that which would be expected by chance, and it can be used to assess the <b>agreement</b> between two or more raters, thus matching our analytical goals.", "dateLastCrawled": "2022-01-28T17:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Frontiers | From What to Why, the Growing Need for a Focus Shift Toward ...", "url": "https://www.frontiersin.org/articles/10.3389/fphys.2021.821217/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/fphys.2021.821217", "snippet": "Explainable AI is far from a novel concept in the <b>machine</b> <b>learning</b> (ML) community (Goebel et al., 2018; Tosun et al., 2020a,b). While the presentation of new approaches for post-hoc explainers of deep convolutional neural networks (CNNs) is outside of the scope of this review, there are a few simple steps that can increase the interpretability and explainability of an AI-driven study ( Figure 1 ).", "dateLastCrawled": "2022-02-03T06:05:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Reliability and Learnability of Human Bandit Feedback for Sequence-to ...", "url": "https://aclanthology.org/P18-1165.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/P18-1165.pdf", "snippet": "intra- and <b>inter-rater agreement is similar</b> for both tasks, with highest inter-rater reliability for stan-dardized 5-point ratings. In a next step, we address the issue of <b>machine</b> learnability of human rewards. We use deep learn- ing models to train reward estimators by regres-sion against cardinal feedback, and by \ufb01tting a Bradley-Terry model (Bradley and Terry,1952) to ordinal feedback. Learnability is understood by a slight misuse of the <b>machine</b> <b>learning</b> notion of learnability (Shalev ...", "dateLastCrawled": "2021-12-22T00:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "arXiv:1805.10627v3 [cs.CL] 13 Dec 2018", "url": "https://www.researchgate.net/profile/Joshua-Uyheng/publication/325413588_Reliability_and_Learnability_of_Human_Bandit_Feedback_for_Sequence-to-Sequence_Reinforcement_Learning/links/5ea04de5a6fdccd7cee0eebe/Reliability-and-Learnability-of-Human-Bandit-Feedback-for-Sequence-to-Sequence-Reinforcement-Learning.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/profile/Joshua-Uyheng/publication/325413588_Reliability...", "snippet": "\ufb01ed by bandit <b>learning</b> for neural <b>machine</b> trans-lation (NMT). Our aim is to show that successful <b>learning</b> from simulated bandit feedback (Sokolov et al.,2016b;Kreutzer et al.,2017;Nguyen et al ...", "dateLastCrawled": "2021-08-22T12:54:00.0000000Z", "language": "en", "isNavigational": false}], [], [], []], "all_bing_queries": ["+(inter-rater agreement)  is like +(consensus opinion)", "+(inter-rater agreement) is similar to +(consensus opinion)", "+(inter-rater agreement) can be thought of as +(consensus opinion)", "+(inter-rater agreement) can be compared to +(consensus opinion)", "machine learning +(inter-rater agreement AND analogy)", "machine learning +(\"inter-rater agreement is like\")", "machine learning +(\"inter-rater agreement is similar\")", "machine learning +(\"just as inter-rater agreement\")", "machine learning +(\"inter-rater agreement can be thought of as\")", "machine learning +(\"inter-rater agreement can be compared to\")"]}