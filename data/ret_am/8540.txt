{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 0, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Pareto principle</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Pareto_principle", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Pareto_principle</b>", "snippet": "The <b>Pareto principle</b> states that for many outcomes, roughly 80% of consequences come from 20% of causes (the &quot;vital few&quot;). Other names for this principle are the 80/20 rule, the law of the vital few, or the principle of factor <b>sparsity</b>.. Management consultant Joseph M. Juran developed the concept in the context of quality control, and improvement, naming it after Italian economist Vilfredo Pareto, who noted the 80/20 connection while at the University of Lausanne in 1896. In his first work ...", "dateLastCrawled": "2022-02-03T06:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "BSBWOR501_PPS_P2.pdf - Establish personal work goals Set and meet own ...", "url": "https://www.coursehero.com/file/55960531/BSBWOR501-PPS-P2pdf/", "isFamilyFriendly": true, "displayUrl": "https://www.coursehero.com/file/55960531/BSBWOR501-PPS-P2pdf", "snippet": "<b>Low hanging fruit</b> <b>Low-hanging fruit</b>. ... While the phrase <b>low-hanging fruit</b> was used in a literal sense and as a metaphor in many writings for hundreds of years, the use of <b>the term</b> as an idiom first occurred in the mid-1900s. It is a popular expression in business and marketing, especially in sales.", "dateLastCrawled": "2022-01-04T18:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A phrase indicating something that gives a reward or outcome ...", "url": "https://english.stackexchange.com/questions/390852/a-phrase-indicating-something-that-gives-a-reward-or-outcome-disproportionately", "isFamilyFriendly": true, "displayUrl": "https://<b>english.stackexchange.com</b>/questions/390852/a-phrase-indicating-something-that...", "snippet": "There is a principle that has a similar connotation, to the one you have talked about in your question: Pareto principle. The Pareto principle (also known as the 80/20 rule, the law of the vital few, or the principle of factor <b>sparsity</b>) states that, for many events, roughly 80% of the effects come from 20% of the causes.", "dateLastCrawled": "2022-01-21T00:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Enhancing Sparsity by Reweighted L1 Minimization</b> \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/0711.1612/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/0711.1612", "snippet": "When the equations are linear, one would <b>like</b> to determine an object x 0 \u2208 R n from data y = \u03a6 x 0, where \u03a6 is an m \u00d7 n matrix with fewer rows than columns; i.e., m &lt; n. The problem is of course that a system with fewer equations than unknowns usually has infinitely many solutions and thus, it is apparently impossible to identify which of these candidate solutions is indeed the \u201ccorrect\u201d one without some additional information. In many instances, however, the object we wish to ...", "dateLastCrawled": "2021-12-09T03:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "CASP10, <b>and the Future of Structure in Biology</b> \u00ab Some Thoughts on a ...", "url": "https://moalquraishi.wordpress.com/2013/01/06/casp10-and-the-future-of-structure-in-biology/", "isFamilyFriendly": true, "displayUrl": "https://moalquraishi.wordpress.com/2013/01/06/casp10-<b>and-the-future-of-structure-in</b>...", "snippet": "Two obvious ones are the explosion of available sequence data, which has made sequence-based analysis ripe with <b>low-hanging fruit</b>, and the slow progress in computational structural biology itself. Our ability to predict structures has been stagnant, with most of the improvement gained coming from the availability of more structural data as opposed to fundamental theoretical advances. This stagnation has in fact been a topic of discussion for the past two CASPs at least.", "dateLastCrawled": "2022-01-08T22:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Crucial <b>Factors Affecting Listening Comprehension</b>", "url": "https://universeofmemory.com/factors-affecting-listening-comprehension/", "isFamilyFriendly": true, "displayUrl": "https://universeofmemory.com/<b>factors-affecting-listening-comprehension</b>", "snippet": "Thank you for the great comment \ud83d\ude42 <b>Like</b> I said, your ability to understand your L2 is modulated by any kind of information from your long-<b>term</b> memory. Especially your knowledge of other languages which are similar to the one you\u2019re currently learning. I don\u2019t know any Chinese but I\u2019m pretty sure there won\u2019t be any <b>low-hanging fruit</b> for me if I decide to listen to it after learning 100 characters.", "dateLastCrawled": "2022-02-02T14:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Stop Paying Attention to Likes and Shares and Track This Instead - 5 ...", "url": "https://5starmultimedia.com/stop-paying-attention-to-likes-and-shares-and-track-this-instead/", "isFamilyFriendly": true, "displayUrl": "https://5starmultimedia.com/stop-paying-attention-to-<b>likes</b>-and-shares-and-track-this...", "snippet": "Focusing on likes and shares is a lot <b>like</b> using traditional advertising metrics related to Nielsen ratings for television ads. They provide immediate gratification. In the digital age we have lost patience. We want instant results and often lose sight of the need to examine multiple data points, some of which may take time to gather and require additional investment. Related: AI Is Taking the Art Out of Sales. Looking to vanity metrics does not provide the answers we need to truly ...", "dateLastCrawled": "2022-02-01T10:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Connective issue: AI learns by doing more with less. <b>Sparsity</b> makes the ...", "url": "https://www.reddit.com/r/singularity/comments/oxbypu/connective_issue_ai_learns_by_doing_more_with/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/singularity/comments/oxbypu/connective_issue_ai_learns_by...", "snippet": "First of all, I&#39;d <b>like</b> to clarify that I am not a &#39;&#39;hater&#39;&#39; at all. In fact I have been lurking the space and this sub for many years. There is nothing I&#39;d <b>like</b> to be true more than the accelerating rate of change. I&#39;m young (20) and have no reason to be biased toward the critical side, if anything it&#39;s the opposite. I love tech and its advancements are one of the very main reasons I am optimistic about the future.", "dateLastCrawled": "2021-08-17T06:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Stop Paying Attention to Likes and Shares and Track This Instead ...", "url": "https://www.vidasvegas.com/stop-paying-attention-to-likes-and-shares-and-track-this-instead/", "isFamilyFriendly": true, "displayUrl": "https://www.vidasvegas.com/stop-paying-attention-to-<b>likes</b>-and-shares-and-track-this...", "snippet": "Focusing on likes and shares is a lot <b>like</b> using traditional advertising metrics related to Nielsen ratings for television ads. They provide immediate gratification. In the digital age we have lost patience. We want instant results and often lose sight of the need to examine multiple data points, some of which may take time to gather and require additional investment. Related: AI Is Taking the Art Out of Sales ...", "dateLastCrawled": "2021-12-27T02:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What is the <b>difference between segmentation and feature extraction? - Quora</b>", "url": "https://www.quora.com/What-is-the-difference-between-segmentation-and-feature-extraction", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-the-<b>difference-between-segmentation-and-feature-extraction</b>", "snippet": "Answer (1 of 3): Segmentation means classifying all pixels in an image into the class they belong to. It is useful for autonomous driving for example: Feature extraction means extracting a vector (or a set of vectors) that describe an image, in order to use it in a Machine Learning system. For ...", "dateLastCrawled": "2022-01-29T20:04:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 0, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Pareto principle</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Pareto_principle", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Pareto_principle</b>", "snippet": "The <b>Pareto principle</b> states that for many outcomes, roughly 80% of consequences come from 20% of causes (the &quot;vital few&quot;). Other names for this principle are the 80/20 rule, the law of the vital few, or the principle of factor <b>sparsity</b>.. Management consultant Joseph M. Juran developed the concept in the context of quality control, and improvement, naming it after Italian economist Vilfredo Pareto, who noted the 80/20 connection while at the University of Lausanne in 1896. In his first work ...", "dateLastCrawled": "2022-02-03T06:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A phrase indicating something that gives a reward or outcome ...", "url": "https://english.stackexchange.com/questions/390852/a-phrase-indicating-something-that-gives-a-reward-or-outcome-disproportionately", "isFamilyFriendly": true, "displayUrl": "https://<b>english.stackexchange.com</b>/questions/390852/a-phrase-indicating-something-that...", "snippet": "There is a principle that has a <b>similar</b> connotation, to the one you have talked about in your question: Pareto principle. The Pareto principle (also known as the 80/20 rule, the law of the vital few, or the principle of factor <b>sparsity</b>) states that, for many events, roughly 80% of the effects come from 20% of the causes.", "dateLastCrawled": "2022-01-21T00:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>ELU activation: A comprehensive analysis</b>", "url": "https://tungmphung.com/elu-activation-a-comprehensive-analysis/", "isFamilyFriendly": true, "displayUrl": "https://tungmphung.com/<b>elu-activation-a-comprehensive-analysis</b>", "snippet": "No <b>sparsity</b>. ReLU outputs 0 for negative inputs, which is bad in some senses but also good in some others, as ... L2-weight decay, momentum <b>term</b> are <b>similar</b> to the previous experiment. The initial learning rate is set to 0.01 and decreased by a factor of 10 after each 35k iterations. The mini-batch size is 100. (More details about the configuration can be found in the paper here.) Result: The result of an experiment that compares a network with ELU vs other known architectures. Verdict: It ...", "dateLastCrawled": "2022-01-31T22:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Connective issue: AI learns by doing more with less. <b>Sparsity</b> makes the ...", "url": "https://www.reddit.com/r/singularity/comments/oxbypu/connective_issue_ai_learns_by_doing_more_with/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/singularity/comments/oxbypu/connective_issue_ai_learns_by...", "snippet": "Again Chollet and the other articles do a great job of providing counter arguments, the best one being: Yes, many of the resources spent on tech and science are accelerating exponentially, but the problems are also getting exponentially harder, after all the <b>low hanging fruit</b> has been picked.", "dateLastCrawled": "2021-08-17T06:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Crucial <b>Factors Affecting Listening Comprehension</b>", "url": "https://universeofmemory.com/factors-affecting-listening-comprehension/", "isFamilyFriendly": true, "displayUrl": "https://universeofmemory.com/<b>factors-affecting-listening-comprehension</b>", "snippet": "Thank you for the great comment \ud83d\ude42 Like I said, your ability to understand your L2 is modulated by any kind of information from your long-<b>term</b> memory. Especially your knowledge of other languages which are <b>similar</b> to the one you\u2019re currently learning. I don\u2019t know any Chinese but I\u2019m pretty sure there won\u2019t be any <b>low-hanging fruit</b> for me if I decide to listen to it after learning 100 characters.", "dateLastCrawled": "2022-02-02T14:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "1.1 - Fermi estimate of future training runs", "url": "https://www.danieldewey.net/risk/estimates.html", "isFamilyFriendly": true, "displayUrl": "https://www.danieldewey.net/risk/estimates.html", "snippet": "The trend in general-purpose FLOP per dollar seems more likely to slow down than to speed up, since a lot of <b>low-hanging fruit</b> has already been plucked and some Moore\u2019s-law-like patterns have begun to level off. However, if deep learning continues to show good results, companies will spend a lot of effort looking for ways to continue to lower prices for training-specific computation, and they may succeed: for example, there may be gains from deep-learning-specific hardware, improvements in ...", "dateLastCrawled": "2022-01-08T17:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "CASP10, <b>and the Future of Structure in Biology</b> \u00ab Some Thoughts on a ...", "url": "https://moalquraishi.wordpress.com/2013/01/06/casp10-and-the-future-of-structure-in-biology/", "isFamilyFriendly": true, "displayUrl": "https://moalquraishi.wordpress.com/2013/01/06/casp10-<b>and-the-future-of-structure-in</b>...", "snippet": "Coming from a compressive sensing standpoint, I sense that they seem to follow a <b>similar</b> path to what has been going on in CS. For a while the central tenet was <b>sparsity</b>: i.e. If your signal is sparse then it should be acquired in a CS fashion and reconstructed with solvers that imposed that constraint. However, most signals are not sparse and to make CS really relevant it has to address the issue of compressible signals. The only way to tackle that has been to develop the concept of ...", "dateLastCrawled": "2022-01-08T22:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Role of CA in capacity as Virtual Entrepreneur Mentor to put your ...", "url": "https://taxguru.in/chartered-accountant/role-chartered-accountant-capacity-virtual-entrepreneur-mentor-put-entrepreneurship-ideas-action.html", "isFamilyFriendly": true, "displayUrl": "https://taxguru.in/chartered-accountant/role-chartered-accountant-capacity-virtual...", "snippet": "B The Pareto principle (also known as the 80/20 rule, the law of the vital few, or the principle of factor <b>sparsity</b>) states that 80% of consequences come from 20% of the causes. Vilfredo Pareto an Italian economist and sociologist used this principle to establish the relationship between the inputs and outputs. The principle can be applied to various economic factors like land and illustrates the notion that majority of the things are controlled by the minority of the people.", "dateLastCrawled": "2022-01-30T04:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "What is the <b>difference between segmentation and feature extraction? - Quora</b>", "url": "https://www.quora.com/What-is-the-difference-between-segmentation-and-feature-extraction", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-the-<b>difference-between-segmentation-and-feature-extraction</b>", "snippet": "Answer (1 of 3): Segmentation means classifying all pixels in an image into the class they belong to. It is useful for autonomous driving for example: Feature extraction means extracting a vector (or a set of vectors) that describe an image, in order to use it in a Machine Learning system. For ...", "dateLastCrawled": "2022-01-29T20:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What are <b>currently the most interesting research areas in applied</b> ...", "url": "https://www.quora.com/What-are-currently-the-most-interesting-research-areas-in-applied-mathematics", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-are-<b>currently-the-most-interesting-research</b>-areas-in...", "snippet": "Answer (1 of 2): What is &quot;most interesting&quot; is, by definition, in the eye of the beholder. If you want to get broad brush strokes regarding what is being studied in applied mathematics, then take a look at the topics presented in applied mathematics conferences (e.g., those sponsored by Society ...", "dateLastCrawled": "2022-01-20T18:04:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Enhancing Sparsity by Reweighted L1 Minimization</b> \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/0711.1612/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/0711.1612", "snippet": "The use of the \u2113 1 norm as a <b>sparsity</b>-promoting functional traces back several decades. A leading early application was reflection seismology, in which a sparse reflection function (indicating meaningful changes between subsurface layers) was sought from bandlimited data. In 1973, Claerbout and Muir [2] first proposed the use of \u2113 1 to deconvolve seismic traces. Over the next decade this idea was refined to better handle observation noise [3, 4], and the <b>sparsity</b>-promoting nature of \u2113 ...", "dateLastCrawled": "2021-12-09T03:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Enhancing sparsity by reweighted</b> \u2113 1 minimization | Emmanuel ...", "url": "https://www.academia.edu/1029599/Enhancing_sparsity_by_reweighted_l_1_minimization", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/1029599/<b>Enhancing_sparsity_by_reweighted</b>_l_1_minimization", "snippet": "<b>Enhancing sparsity by reweighted</b> \u2113 1 minimization. Emmanuel Candes. IntroductionWhat makes some scientific or engineering problems at once interesting and challenging is that often, one has fewer equations than unknowns. When the equations are linear, one would like to determine an object x 0 \u2208 R n from data y = \u03a6x 0 , where \u03a6 is an m \u00d7 ...", "dateLastCrawled": "2021-03-20T11:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Enhancing <b>Sparsity</b> by Reweighted \u2113 1 Minimization, Journal of Fourier ...", "url": "https://www.deepdyve.com/lp/springer-journals/enhancing-sparsity-by-reweighted-1-minimization-5wmTOTeOjb", "isFamilyFriendly": true, "displayUrl": "https://www.<b>deepdyve</b>.com/lp/springer-journals/<b>enhancing-sparsity-by-reweighted-1</b>...", "snippet": "Read &quot;Enhancing <b>Sparsity</b> by Reweighted \u2113 1 Minimization, Journal of Fourier Analysis and Applications&quot; on <b>DeepDyve</b>, the largest online rental service for scholarly research with thousands of academic publications available at your fingertips. It is now well understood that (1) it is possible to reconstruct sparse signals exactly from what appear to be highly incomplete sets of linear measurements and (2) that this <b>can</b> be done by constrained \u2113 1 minimization. In this paper, we study a ...", "dateLastCrawled": "2020-07-25T21:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Local <b>sparsity</b> control for <b>naive Bayes with extreme misclassification</b> ...", "url": "https://www.researchgate.net/publication/221653466_Local_sparsity_control_for_naive_Bayes_with_extreme_misclassification_costs", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221653466_Local_<b>sparsity</b>_control_for_naive...", "snippet": "Aside from efforts to better calibrate Naive Bayes scores, it has been shown that its accuracy depends on document <b>sparsity</b> and feature selection <b>can</b> lead to marked improvement in classification ...", "dateLastCrawled": "2021-08-07T09:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 4, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Pareto principle</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Pareto_principle", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Pareto_principle</b>", "snippet": "The <b>term</b> 80/20 is only a shorthand for the general principle at work. In individual cases, the distribution could just as well be, say, nearer to 90/10 or 70/30. There is no need for the two numbers to add up to the number 100, as they are measures of different things, (e.g., &#39;number of customers&#39; vs &#39;amount spent&#39;). However, each case in which they do not add up to 100%, is equivalent to one in which they do. For example, as noted above, the &quot;64/4 law&quot; (in which the two numbers do not add ...", "dateLastCrawled": "2022-02-03T06:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "What is unknown in using microbiota as a therapeutic? - Sung ...", "url": "https://onlinelibrary.wiley.com/doi/10.1111/jgh.15716", "isFamilyFriendly": true, "displayUrl": "https://onlinelibrary.wiley.com/doi/10.1111/jgh.15716", "snippet": "This article describes the unknowns in microbiota biology (undetected microbes, uncertain colonization, unclear mechanisms of action, uncertain indications, unsure long-<b>term</b> efficacy, or side effects). We discuss how these unknowns may affect the therapeutic uses of FMT, and the potentials and caveats of other related microbiota-based therapies. When used as an experimental therapy or last resort in difficult conditions, caution should be taken against inadvertent complications. Clear ...", "dateLastCrawled": "2021-11-14T09:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>How should we model complex systems</b>? - LessWrong 2.0 viewer", "url": "https://www.greaterwrong.com/posts/2scaKmrBeufowY9ku/how-should-we-model-complex-systems", "isFamilyFriendly": true, "displayUrl": "https://www.greaterwrong.com/posts/2scaKmrBeufowY9ku/<b>how-should-we-model-complex-systems</b>", "snippet": "This <b>can</b> <b>be thought</b> of as \u2018bottom-up\u2019 modeling. Some examples: modeling senescence, bank runs, or regional climate cycles. I\u2019ve not found anything about rational strategies to approximately model complex systems rather than derive true models. I interpret \u201capproximately model complex systems\u201d as \u2018top-down\u2019 \u2018statistical\u2019 modeling \u2013 that <b>can</b> be useful regardless, even if it\u2019s wrong, but might be reasonably accurate if the system is relatively \u2018simple\u2019. But if the ...", "dateLastCrawled": "2021-12-23T07:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Role of CA in capacity as Virtual Entrepreneur Mentor to put your ...", "url": "https://taxguru.in/chartered-accountant/role-chartered-accountant-capacity-virtual-entrepreneur-mentor-put-entrepreneurship-ideas-action.html", "isFamilyFriendly": true, "displayUrl": "https://taxguru.in/chartered-accountant/role-chartered-accountant-capacity-virtual...", "snippet": "Therefore a person <b>can</b> recover from illness just by his own mind only thinking that he is being treated. Your achievement is a consequence of how your mind thinks. A \u2018<b>Thought</b>\u2019 is the Alpha and Omega of success. Buddha said \u2018what you think, you become\u2019. Accomplishment of your goal has already started when the seed was sown in your brain ...", "dateLastCrawled": "2022-01-30T04:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "What is the <b>difference between segmentation and feature extraction? - Quora</b>", "url": "https://www.quora.com/What-is-the-difference-between-segmentation-and-feature-extraction", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-the-<b>difference-between-segmentation-and-feature-extraction</b>", "snippet": "Answer (1 of 3): Segmentation means classifying all pixels in an image into the class they belong to. It is useful for autonomous driving for example: Feature extraction means extracting a vector (or a set of vectors) that describe an image, in order to use it in a Machine Learning system. For ...", "dateLastCrawled": "2022-01-29T20:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Is <b>there anyone who does mathematical research as</b> a hobby? - Quora", "url": "https://www.quora.com/Is-there-anyone-who-does-mathematical-research-as-a-hobby", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Is-<b>there-anyone-who-does-mathematical-research-as</b>-a-hobby", "snippet": "Answer (1 of 5): It is very uncommon these days, but possible, to do high-quality mathematical research without a university job. One example that I know of is Jonathan Sondow, who has been publishing papers and presenting at professional conferences for many years without being employed as a ma...", "dateLastCrawled": "2022-01-18T17:20:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Crucial <b>Factors Affecting Listening Comprehension</b>", "url": "https://universeofmemory.com/factors-affecting-listening-comprehension/", "isFamilyFriendly": true, "displayUrl": "https://universeofmemory.com/<b>factors-affecting-listening-comprehension</b>", "snippet": "The Pareto principle (also known as the 80/20 rule, the law of the vital few, or the principle of factor <b>sparsity</b>) states that, for many events, roughly 80% of the effects come from 20% of the causes. Wiki. And, as you will shortly see, even among these two, there is one which is clearly more important. 1. The total amount of listening practice . Photo by Volkan Olmez on Unsplash . In order to increase your comprehension, you have to spend a lot of time listening to people or recordings. The ...", "dateLastCrawled": "2022-02-02T14:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "What is unknown in using microbiota as a therapeutic? - Sung ...", "url": "https://onlinelibrary.wiley.com/doi/10.1111/jgh.15716", "isFamilyFriendly": true, "displayUrl": "https://onlinelibrary.wiley.com/doi/10.1111/jgh.15716", "snippet": "However, some of these tests are used without resolving the issue of data dispersion, that may require transforming and modeling due to the high dimensionality, the association <b>sparsity</b>, and interdependence between microbial taxa. 77-79 Most assume that the microbiota data are independent, but this is probably not true as coexistence and co-exclusion of microbiota are well known. The other limit of statistical analysis is the question of inferring correlation as causality. To model causative ...", "dateLastCrawled": "2021-11-14T09:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>ELU activation: A comprehensive analysis</b>", "url": "https://tungmphung.com/elu-activation-a-comprehensive-analysis/", "isFamilyFriendly": true, "displayUrl": "https://tungmphung.com/<b>elu-activation-a-comprehensive-analysis</b>", "snippet": "Outputs <b>can</b> be either positive or negative. Studies showed that functions with 0-centered outputs help networks train faster. Although ELU\u2019s outputs are not distributed around 0, the fact that it does produce negative values makes it be preferred in this sense <b>compared</b> to ReLU. In practice, it seems that networks with ELU converge more quickly than with ReLU, even though the exponential computation in ELU", "dateLastCrawled": "2022-01-31T22:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A phrase indicating something that gives a reward or outcome ...", "url": "https://english.stackexchange.com/questions/390852/a-phrase-indicating-something-that-gives-a-reward-or-outcome-disproportionately", "isFamilyFriendly": true, "displayUrl": "https://<b>english.stackexchange.com</b>/questions/390852/a-phrase-indicating-something-that...", "snippet": "I <b>can</b> define the exact meaning of the word with the following example: A life insurance policy is an aleatory agreement because the financial benefits <b>can</b> be greater than the premiums paid. Aleatory is something depending on a contingent event, but aleatory contract is used so often with life insurance that the usage of the <b>term</b> is closely associated with an agreement where &quot;something that gives a reward or outcome disproportionately large to its effort&quot;.", "dateLastCrawled": "2022-01-21T00:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Identifying and Verifying Causes</b> | The Lean Six Sigma Pocket Toolbook A ...", "url": "https://flylib.com/books/en/2.528.1/identifying_and_verifying_causes.html", "isFamilyFriendly": true, "displayUrl": "https://flylib.com/books/en/2.528.1/<b>identifying_and_verifying_causes</b>.html", "snippet": "To increase the chances that you <b>can</b> identify the true root causes of problems, which <b>can</b> then be targeted for improvement. The tools in this chapter fall into two very different categories: Tools for identifying potential causes (starts below) are techniques for sparking creative thinking about the causes of observed problems. The emphasis is on thinking broadly about what&#39;s going on in your process. Tools for verifying potential causes (starts on p. 149) are at the opposite end of the ...", "dateLastCrawled": "2022-01-31T05:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Consumer Vulnerability - Hill - 2020 - Journal of Consumer Psychology ...", "url": "https://myscp.onlinelibrary.wiley.com/doi/full/10.1002/jcpy.1161", "isFamilyFriendly": true, "displayUrl": "https://myscp.onlinelibrary.wiley.com/doi/full/10.1002/jcpy.1161", "snippet": "The <b>term</b> consumer vulnerability is widely used to describe a variety of difficult situations that consumers face. These situations may be associated with individual characteristics (e.g., age, race, physical capabilities), social phenomena (e.g., stereotypes, prejudicial treatment), business practices (e.g., store layouts, marketer manipulations), and environmental forces (e.g., hurricanes, tsunamis) (for a review, see Baker, Gentry, &amp; Rittenburg, 2005).Despite the wide-ranging implicit or ...", "dateLastCrawled": "2021-11-19T13:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "The role of Chartered Accountant in capacity as Virtual Entrepreneur", "url": "https://www.slideshare.net/RajkumarAdukia/the-role-of-chartered-accountant-in-capacity-as-virtual-entrepreneur-mentor-to-put-your-entrepreneurship-ideas-into-action", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/RajkumarAdukia/the-role-of-chartered-accountant-in-capacity...", "snippet": "2 A virtual Entrepreneur Mentor <b>can</b> act as a trusted confidante over an extended period of time with an objective to provide advice, counselling from a fresh perspective, collaborate and help you as an entrepreneur stay focused on their long- <b>term</b> goal of making their venture a success. It is important for budding entrepreneurs to understand the right route to reach their destination while embarking on their entrepreneurial journey. A virtual entrepreneur mentor due to advantages of ...", "dateLastCrawled": "2022-01-22T09:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Role of CA in capacity as Virtual Entrepreneur Mentor to put your ...", "url": "https://taxguru.in/chartered-accountant/role-chartered-accountant-capacity-virtual-entrepreneur-mentor-put-entrepreneurship-ideas-action.html", "isFamilyFriendly": true, "displayUrl": "https://taxguru.in/chartered-accountant/role-chartered-accountant-capacity-virtual...", "snippet": "These highlight a need in the entrepreneurial community to find and connect with qualified mentors who <b>can</b> deliver solid advice to help a business over those shaky first years. A virtual Entrepreneur Mentor <b>can</b> act as a trusted confidante over an extended period of time with an objective to provide advice, counselling from a fresh perspective, collaborate and help you as an entrepreneur stay focused on their long-<b>term</b> goal of making their venture a success. It is important for budding ...", "dateLastCrawled": "2022-01-30T04:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "What is the <b>difference between segmentation and feature extraction? - Quora</b>", "url": "https://www.quora.com/What-is-the-difference-between-segmentation-and-feature-extraction", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-the-<b>difference-between-segmentation-and-feature-extraction</b>", "snippet": "Answer (1 of 3): Segmentation means classifying all pixels in an image into the class they belong to. It is useful for autonomous driving for example: Feature extraction means extracting a vector (or a set of vectors) that describe an image, in order to use it in a Machine Learning system. For ...", "dateLastCrawled": "2022-01-29T20:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Is it possible that SVMs with kernel will revive in the future ...", "url": "https://www.quora.com/Is-it-possible-that-SVMs-with-kernel-will-revive-in-the-future-competing-with-Deep-Learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Is-it-possible-that-SVMs-with-kernel-will-revive-in-the-future...", "snippet": "Answer: This question tacitly assumes that SVM is inferior to neural networks. Let me copy below my answers to other questions Prasoon Goyal&#39;s answer to Why has support vector machines fallen out of favor? &gt; Support vector machines have NOT fallen out of favor. Please don\u2019t get misled by the ...", "dateLastCrawled": "2022-01-07T21:24:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "<b>Sparsity</b> is an essential feature of many contemporary data problems. Remote sensing, various forms of automated screening and other high throughput measurement devices collect a large amount of ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Regularization \u2014 Understanding L1 and L2 regularization for Deep <b>Learning</b>", "url": "https://medium.com/analytics-vidhya/regularization-understanding-l1-and-l2-regularization-for-deep-learning-a7b9e4a409bf", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/regularization-understanding-l1-and-l2...", "snippet": "The <b>sparsity</b> feature used in L1 regularization has been used extensively as a feature selection mechanism in <b>machine</b> <b>learning</b>. Feature selection is a mechanism which inherently simplifies a ...", "dateLastCrawled": "2022-02-01T00:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "An E\ufb03cient Sparse Metric <b>Learning</b> in High ... - <b>Machine</b> <b>Learning</b>", "url": "http://machinelearning.org/archive/icml2009/papers/46.pdf", "isFamilyFriendly": true, "displayUrl": "<b>machinelearning</b>.org/archive/icml2009/papers/46.pdf", "snippet": "This <b>sparsity</b> prior of <b>learning</b> distance metric serves to regularize the com-plexity of the distance model especially in the \u201cless example number p and high dimension d\u201d setting. Theoretically, by <b>analogy</b> to the covariance estimation problem, we \ufb01nd the proposed distance <b>learning</b> algorithm has a consistent result at rate O!&quot;# m2 logd $% n &amp; to the target distance matrix with at most m nonzeros per row. Moreover, from the imple-mentation perspective, this! 1-penalized log-determinant ...", "dateLastCrawled": "2021-11-19T11:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Basic Concepts in Machine Learning</b>", "url": "https://machinelearningmastery.com/basic-concepts-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>basic-concepts-in-machine-learning</b>", "snippet": "What are the <b>basic concepts in machine learning</b>? I found that the best way to discover and get a handle on the <b>basic concepts in machine learning</b> is to review the introduction chapters to <b>machine learning</b> textbooks and to watch the videos from the first model in online courses. Pedro Domingos is a lecturer and professor on <b>machine learning</b> at the University of Washing and", "dateLastCrawled": "2022-02-02T15:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Discovering governing equations from data</b> by sparse identification of ...", "url": "https://www.pnas.org/content/pnas/113/15/3932.full.pdf", "isFamilyFriendly": true, "displayUrl": "https://<b>www.pnas.org</b>/content/<b>pnas</b>/113/15/3932.full.pdf", "snippet": "examples. In this work, we combin e <b>sparsity</b>-promoting techniques and <b>machine</b> <b>learning</b> with nonlinear dynamical systems to discover governing equations from noisy measurement data. The only as-sumption about the structureof the model is that there are onlya few important terms that govern the dy namics, so that the equations are sparse in the space of possible functions; this assumption holds for many physical systems in an appropriate basis. In particular, we use sparse regression to ...", "dateLastCrawled": "2022-01-20T20:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Dynamical <b>machine</b> <b>learning</b> volumetric reconstruction of objects ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8027224/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8027224", "snippet": "The sequence index in the angle of illumination plays the role of discrete time in the dynamical system <b>analogy</b>. Thus, the imaging problem turns into a problem of nonlinear system identification, which also suggests dynamical <b>learning</b> as a better fit to regularize the reconstructions. We devised a Recurrent Neural Network (RNN) architecture with a novel Separable-Convolution Gated Recurrent Unit (SC-GRU) as the fundamental building block. Through a comprehensive comparison of several ...", "dateLastCrawled": "2022-01-08T19:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Dynamical machine learning volumetric reconstruction of</b> objects ...", "url": "https://www.nature.com/articles/s41377-021-00512-x", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41377-021-00512-x", "snippet": "Recently, thanks to a ground-breaking observation from 2010 that <b>sparsity</b> can be learnt by a deep neural network 48, the idea of using <b>machine</b> <b>learning</b> to approximate solutions to inverse problems ...", "dateLastCrawled": "2022-02-02T23:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Regularization : What? Why? and How? (Part -1) | by Siddhant Rai ...", "url": "https://medium.com/mlearning-ai/regularization-what-why-and-how-part-1-ef6bdb6bafea", "isFamilyFriendly": true, "displayUrl": "https://medium.com/m<b>learning</b>-ai/regularization-what-why-and-how-part-1-ef6bdb6bafea", "snippet": "In general <b>machine</b> <b>learning</b> sense, it is solving an objective function to perform maximum or minimum evaluation. In reality, optimization is lot more profound in usage. Then we have two terms ...", "dateLastCrawled": "2022-01-28T00:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and-sample-codes ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/README.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "Model 1: accuracy = 92%. Model 2: accuracy = 95%. Model 2 has higher accuracy than model 1, but model 2 is useless. This is called accuracy paradox, which means the model with higher accuracy may not have better generalization power. In general, when TP &lt; FP, the accuracy will always increase when we change the classifier to always output &#39;negative&#39;.Conversely, when TN &lt; FN, the same will happen when we change the classifier to always output &#39;positive&#39; [1].. Recall@k", "dateLastCrawled": "2021-09-22T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "regression - Why L1 norm for sparse models - Cross Validated", "url": "https://stats.stackexchange.com/questions/45643/why-l1-norm-for-sparse-models", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/45643", "snippet": "There are many norms that lead to <b>sparsity</b> (e.g., as you mentioned, any Lp norm with p &lt;= 1). In general, any norm with a sharp corner at zero induces <b>sparsity</b>. So, going back to the original question - the L1 norm induces <b>sparsity</b> by having a discontinuous gradient at zero (and any other penalty with this property will do so too). $\\endgroup$", "dateLastCrawled": "2022-01-26T23:44:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Learning Neural Representations for Network Anomaly Detection</b>", "url": "https://www.researchgate.net/publication/325797465_Learning_Neural_Representations_for_Network_Anomaly_Detection", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/325797465_<b>Learning</b>_Neural_Representations_for...", "snippet": "Many <b>machine</b> <b>learning</b> algorithms have been. Manuscript received December 22, 2017; revised March 13, 2018. This. work is funded by Vietnam International Education De velopment (VIED) and. by ...", "dateLastCrawled": "2021-12-06T22:55:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Self-representation based dual-graph regularized <b>feature selection</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0925231215010759", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0925231215010759", "snippet": "<b>Just as sparsity</b> leads to sparse representation, self-similarity results in self-representation ... Her current research interests include pattern recognition and <b>machine</b> <b>learning</b>. Licheng Jiao (SM\u05f389) received the B.S. degree from Shanghai Jiaotong University, Shanghai, China, in 1982, the M.S. and Ph.D. degrees from Xi\u05f3an Jiaotong University, Xi\u05f3an, China, in 1984 and 1990, respectively. From 1990 to 1991, he was a postdoctoral Fellow in the National Key Laboratory for Radar Signal ...", "dateLastCrawled": "2021-11-22T12:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Self-representation based dual-graph regularized feature selection ...", "url": "https://web.xidian.edu.cn/rhshang/files/20160516_172953.pdf", "isFamilyFriendly": true, "displayUrl": "https://web.xidian.edu.cn/rhshang/files/20160516_172953.pdf", "snippet": "<b>machine</b> <b>learning</b> and computer vision \ufb01elds [41]. <b>Just as sparsity</b> leads to sparse representation, self-similarity results in self-representation [41]. Taking into account of manifold <b>learning</b> and feature selection, and inspired by the self-representation property and the idea of dual-regularization <b>learning</b> [44,45], we propose a novel feature selection algorithm for clustering, named self-representation based dual-graph regularized feature selection clustering (DFSC). This algorithm ...", "dateLastCrawled": "2022-02-02T03:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Unsupervised feature selection</b> by <b>regularized self-representation</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0031320314002970", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0031320314002970", "snippet": "<b>Just as sparsity</b> leads to sparse representation, self-similarity results in self-representation. With the above considerations, in this paper we propose a simple yet very effective <b>unsupervised feature selection</b> method by exploiting the self-representation ability of features. The feature matrix is represented over itself to find the representative feature components. The representation residual is minimized by L 2, 1-norm loss to reduce the effect of outlier samples. Different from the ...", "dateLastCrawled": "2022-01-24T12:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Talk <b>Archive</b> - Research on Algorithms and Incentives in Networks", "url": "https://rain.stanford.edu/schedule/archive.shtml", "isFamilyFriendly": true, "displayUrl": "https://rain.stanford.edu/schedule/<b>archive</b>.shtml", "snippet": "McFowland\u2019s research interests\u2014which lie at the intersection of Information Systems, <b>Machine</b> <b>Learning</b>, and Public Policy\u2014include the development of computationally efficient algorithms for large-scale statistical <b>machine</b> <b>learning</b> and \u201cbig data\u201d analytics. More specifically, his research seeks to demonstrate that many real-world problems faced by organizations, and society more broadly, can be reduced to the tasks of anomalous pattern detection and discovery. As a data and ...", "dateLastCrawled": "2022-01-20T11:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Talks - <b>sites.google.com</b>", "url": "https://sites.google.com/view/dssseminarseries/talks", "isFamilyFriendly": true, "displayUrl": "https://<b>sites.google.com</b>/view/dssseminarseries/talks", "snippet": "Abstracts &amp; Bios for upcoming talks", "dateLastCrawled": "2022-01-27T14:42:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Sparse representations for text categorization</b>", "url": "https://www.researchgate.net/publication/221479613_Sparse_representations_for_text_categorization", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221479613_Sparse_representations_for_text...", "snippet": "<b>Machine</b> <b>learning</b> for text classification is the cornerstone of document categorization, news filtering, document routing, and personalization. In text domains, effective feature selection is ...", "dateLastCrawled": "2021-12-10T18:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Continual Learning via Neural Pruning</b> | DeepAI", "url": "https://deepai.org/publication/continual-learning-via-neural-pruning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>continual-learning-via-neural-pruning</b>", "snippet": "Continual <b>learning</b>, the ability of models to learn to solve new tasks beyond what has previously been trained, has garnered much attention from the <b>machine</b> <b>learning</b> community in recent years. This is driven in part by the practical advantages promised by continual <b>learning</b> schemes such as improved performance on subsequent tasks as well as a more efficient use of resources in machines with memory constraints.", "dateLastCrawled": "2021-12-30T15:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Non-negative data-<b>driven mapping of structural connections</b> with ...", "url": "https://www.sciencedirect.com/science/article/pii/S105381192030759X", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S105381192030759X", "snippet": "For ICA, <b>sparsity can be thought of as</b> a proxy for independence. 3.5. In-vivo data decompositions. For real data, we decomposed group-average tractography matrices, using independent component analysis (ICA) and non-negative matrix factorisation (NMF), with a range of model orders K. ICA was initialised with regular PCA, in which the first 500 components were retained (explaining 97% of the total variance). ICA was applied to the reduced dataset using the FastICA algorithm (Hyv\u00e4rinen and ...", "dateLastCrawled": "2021-10-11T05:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) <b>Sparse Representations for Text Categorization</b> | Dimitri Kanevsky ...", "url": "https://www.academia.edu/2738730/Sparse_Representations_for_Text_Categorization", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/2738730/<b>Sparse_Representations_for_Text_Categorization</b>", "snippet": "A Comparative Study of <b>Machine</b> <b>Learning</b> Methods for Verbal Autopsy Text Classification. By Eric S Atwell and Samuel Danso. CSC435 book proposal. By Russell Frith. Higher-Order Smoothing: A Novel Semantic Smoothing Method for Text Classification. By Murat C Ganiz, Mitat Poyraz, and Zeynep Kilimci. INFORMATION RETRIEVAL. By febi k. Introduction to information retrieval. By Valeria Mesi. Download pdf. \u00d7 Close Log In. Log In with Facebook Log In with Google. Sign Up with Apple. or. Email ...", "dateLastCrawled": "2021-10-13T23:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Continual <b>Learning</b> via Neural Pruning \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/1903.04476/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1903.04476", "snippet": "We introduce Continual <b>Learning</b> via Neural Pruning (CLNP), a new method aimed at lifelong <b>learning</b> in fixed capacity models based on neuronal model sparsification. In this method, subsequent tasks are trained using the inactive neurons and filters of the sparsified network and cause zero deterioration to the performance of previous tasks. In order to deal with the possible compromise between model sparsity and performance, we formalize and incorporate the concept of graceful forgetting: the ...", "dateLastCrawled": "2021-11-07T10:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Continual <b>Learning</b> via Neural Pruning", "url": "https://openreview.net/pdf?id=Hyl_XXYLIB", "isFamilyFriendly": true, "displayUrl": "https://openreview.net/pdf?id=Hyl_XXYLIB", "snippet": "Continual <b>learning</b>, the ability of models to learn to solve new tasks beyond what has previously been trained, has garnered much at-tention from the <b>machine</b> <b>learning</b> community in recent years. The main obstacle for effective continual <b>learning</b> is the problem of cata-strophic forgetting: machines trained on new problems forget about", "dateLastCrawled": "2022-01-05T02:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Abstract - arXiv.org e-Print archive", "url": "https://arxiv.org/pdf/1903.04476", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/pdf/1903.04476", "snippet": "Continual <b>learning</b>, the ability of models to learn to solve new tasks beyond what has previously been trained, has garnered much attention from the <b>machine</b> <b>learning</b> community in recent years. This is driven in part by the practical advantages promised by continual <b>learning</b> schemes such as improved performance on subsequent tasks as well as a more ef\ufb01cient use of resources in machines with memory constraints. There is also great interest in continual <b>learning</b> from a more long term ...", "dateLastCrawled": "2021-10-25T14:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) <b>Introduction to compressed sensing</b>", "url": "https://www.researchgate.net/publication/220043734_Introduction_to_compressed_sensing", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/220043734_<b>Introduction_to_compressed_sensing</b>", "snippet": "systems control, clustering, and <b>machine</b> <b>learning</b> [14, 15, 58, 61, 89, 193, 217, 240, 244]. Low-dimensional manifolds hav e also been prop osed as approximate mod-", "dateLastCrawled": "2022-01-14T18:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) <b>Introduction to compressed sensing</b> | Marco Duarte - Academia.edu", "url": "https://www.academia.edu/1443164/Introduction_to_compressed_sensing", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/1443164/<b>Introduction_to_compressed_sensing</b>", "snippet": "<b>Introduction to Compressed Sensing</b> For any x \u2208 \u03a3k , we can associate a k-face of C n with the support and sign pattern of x. One can show that the number of k-faces of AC n is precisely the number of index sets of size k for which signals supported on them can be recovered by (1.12) with B (y) = {z : Az = y}.", "dateLastCrawled": "2022-01-21T03:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Compressed Sensing : Theory and Applications</b> | Kutyniok, Gitta Eldar ...", "url": "https://b-ok.africa/book/2086657/84a688", "isFamilyFriendly": true, "displayUrl": "https://b-ok.africa/book/2086657/84a688", "snippet": "You can write a book review and share your experiences. Other readers will always be interested in your opinion of the books you&#39;ve read. Whether you&#39;ve loved the book or not, if you give your honest and detailed thoughts then people will find new books that are right for them.", "dateLastCrawled": "2021-12-26T07:22:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(sparsity)  is like +(the term \"low hanging fruit\")", "+(sparsity) is similar to +(the term \"low hanging fruit\")", "+(sparsity) can be thought of as +(the term \"low hanging fruit\")", "+(sparsity) can be compared to +(the term \"low hanging fruit\")", "machine learning +(sparsity AND analogy)", "machine learning +(\"sparsity is like\")", "machine learning +(\"sparsity is similar\")", "machine learning +(\"just as sparsity\")", "machine learning +(\"sparsity can be thought of as\")", "machine learning +(\"sparsity can be compared to\")"]}