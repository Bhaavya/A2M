{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Deep Convolutional Neural Networks With Ensemble Learning and ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8416107/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8416107", "snippet": "These results illustrate the advantage of the GoogLeNet architecture in <b>preventing</b> <b>overfitting</b> by using auxiliary classifiers, compared to other <b>models</b>, such as AlexNet and VGG-16. ResNet is an additional successful variant of the classical CNN (He et al., 2016). The hallmarks of the ResNet model include the incorporation of residual mapping ...", "dateLastCrawled": "2022-01-29T05:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Outlier Detection Using Distributionally Robust Optimization</b> ... - DeepAI", "url": "https://deepai.org/publication/outlier-detection-using-distributionally-robust-optimization-under-the-wasserstein-metric", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>outlier-detection-using-distributionally-robust</b>...", "snippet": "We empirically explore three important aspects of the <b>Wasserstein</b> DRO formulation, including the advantages of the \u2113 1-<b>loss</b> function, the selection of a proper norm for the <b>Wasserstein</b> metric, and the implication of penalizing the extended regression coefficient (\u2212 \u03b2, 1), through comparing with a series of regression <b>models</b> on a number of synthetic datasets. We show the superiority of the <b>Wasserstein</b> DRO approach, presenting a thorough analysis, under four different experimental setups ...", "dateLastCrawled": "2022-02-03T07:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) Generative Adversarial Networks for Data Generation in Structural ...", "url": "https://www.researchgate.net/publication/357068970_Generative_Adversarial_Networks_for_Data_Generation_in_Structural_Health_Monitoring", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/357068970_Generative_Adversarial_Networks_for...", "snippet": "In this paper, 1-D <b>Wasserstein</b> <b>loss</b> Deep Convolutional Generative Adversarial Networks using Gradient Penalty (1-D WDCGAN-GP) is utilized to generate damage associated vibration datasets that are ...", "dateLastCrawled": "2022-02-02T17:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "GitHub - sboonpan/Stock.ai", "url": "https://github.com/sboonpan/Stock.ai", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/sboonpan/Stock.ai", "snippet": "Having a lot of features and neural networks we need to make sure we prevent <b>overfitting</b> and be mindful of the total <b>loss</b>. We use several techniques for <b>preventing</b> <b>overfitting</b> (not only in the LSTM, but also in the CNN and the auto-encoders): Ensuring data quality. We already performed statistical checks and made sure the data doesn&#39;t suffer ...", "dateLastCrawled": "2022-02-03T07:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Overfitting</b> in adversarially robust deep learning - arxiv-vanity.com", "url": "https://www.arxiv-vanity.com/papers/2002.11569/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2002.11569", "snippet": "3 Adversarial training and robust <b>overfitting</b>. In order to learn networks that are robust to adversarial examples, a commonly used method is adversarial training, which solves the following robust optimization problem. min \u03b8 \u2211 imax \u03b4\u2208\u0394 \u2113(f \u03b8(xi +\u03b4),yi), (1) where f \u03b8 is a network with parameters \u03b8, (xi,yi) is a training example ...", "dateLastCrawled": "2021-11-28T15:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Using the latest advancements in <b>deep learning</b> to predict stock price ...", "url": "https://towardsdatascience.com/aifortrading-2edd6fac689d", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/aifortrading-2edd6fac689d", "snippet": "WGAN uses <b>Wasserstein</b> distance, W(pr,pg)=1Ksup\u2016f\u2016L\u2264K\ud835\udd3cx\u223cpr[f(x)]\u2212\ud835\udd3cx\u223cpg[f(x)] (where sup stands for supremum), as a <b>loss</b> function (also called Earth Mover\u2019s distance, because it normally is interpreted as moving one pile of, say, sand to another one, both piles having different probability distributions, using minimum energy during the transformation). Compared to KL and JS divergences, <b>Wasserstein</b> metric gives a smooth measure (without sudden jumps in divergence). This ...", "dateLastCrawled": "2022-02-03T02:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Video Summarization: <b>Models</b>, code, and papers - CatalyzeX", "url": "https://www.catalyzex.com/s/Video%20Summarization", "isFamilyFriendly": true, "displayUrl": "https://www.catalyzex.com/s/Video Summarization", "snippet": "The GAN training problem is solved by introducing the <b>Wasserstein</b> GAN and two newly proposed video patch/score sum losses. In addition, the score sum <b>loss</b> can also relieve the model sensitivity to the varying video lengths, which is an inherent problem for most current video analysis tasks. Our method substantially lifts the performance on the ...", "dateLastCrawled": "2022-01-21T01:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Industrial Engineering &amp; Management Systems", "url": "http://www.iemsjl.org/journal/article.php?code=80452", "isFamilyFriendly": true, "displayUrl": "www.iemsjl.org/journal/article.php?code=80452", "snippet": "5.3.2 Analysis of Training <b>Loss</b> of Classification <b>Models</b> . Next, we compare the effect of the augmented training data in order to clarify why generalization performance differs between proposal method and comparing method. For that purpose, the magnitude of the <b>loss</b> of the training data during the training process is compared. The change of the <b>loss</b> of the training and test data during the training of the classification model is shown below (Figure 2 to Figure 5). Figure 2 represents the ...", "dateLastCrawled": "2022-01-26T10:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Introduction to Generative Adversarial Networks \u2013 Mastering Machine ...", "url": "https://w3sdev.com/introduction-to-generative-adversarial-networks-mastering-machine-learning-algorithms-second-edition.html", "isFamilyFriendly": true, "displayUrl": "https://w3sdev.com/introduction-to-generative-adversarial-networks-mastering-machine...", "snippet": "<b>Wasserstein</b> GANs (WGANs) We can now introduce the concept of adversarial training of neural <b>models</b>, its connection to game theory and its applications to GANs. Adversarial training. The brilliant idea of adversarial training, proposed by Goodfellow et al. (in Goodfellow I. J., Pouget-Abadie J., Mirza M., Xu B., Warde-Farley D., Ozair S., Courville A., Bengio Y., Generative Adversarial Networks, arXiv:1406.2661 [stat.ML] \u2013 although this idea has been, at least in theory, discussed earlier ...", "dateLastCrawled": "2021-10-04T19:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Python Machine Learning: Machine Learning and Deep Learning with Python ...", "url": "https://ebin.pub/python-machine-learning-machine-learning-and-deep-learning-with-python-scikit-learn-and-tensorflow-2-3rd-edition-3nbsped-9781789955750-1789955750.html", "isFamilyFriendly": true, "displayUrl": "https://ebin.pub/python-machine-learning-machine-learning-and-deep-learning-with...", "snippet": "<b>Overfitting</b> Note that we will evaluate the performance of our <b>models</b> based on the test dataset in this chapter. In Chapter 6, Learning Best Practices for Model Evaluation and Hyperparameter Tuning, you will learn about useful techniques, including graphical analysis, such as learning curves, to detect and prevent <b>overfitting</b>. <b>Overfitting</b>, which we will return to later in this chapter, means that the model captures the patterns in the training data well but fails to generalize well to unseen ...", "dateLastCrawled": "2022-01-31T17:24:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Deep Convolutional Neural Networks With Ensemble Learning and ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8416107/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8416107", "snippet": "These results illustrate the advantage of the GoogLeNet architecture in <b>preventing</b> <b>overfitting</b> by using auxiliary classifiers, compared to other <b>models</b>, such as AlexNet and VGG-16. ResNet is an additional successful variant of the classical CNN (He et al., 2016). The hallmarks of the ResNet model include the incorporation of residual mapping ...", "dateLastCrawled": "2022-01-29T05:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Improving the Improved Training</b> of <b>Wasserstein</b> GANs: A Consistency Term ...", "url": "https://deepai.org/publication/improving-the-improved-training-of-wasserstein-gans-a-consistency-term-and-its-dual-effect", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>improving-the-improved-training</b>-of-<b>wasserstein</b>-gans-a...", "snippet": "We do not observe obvious <b>overfitting</b> phenomena even when the model is trained on only 1000 images of CIFAR-10 (Krizhevsky &amp; Hinton, 2009). (3) Our approach can be seamlessly integrated with GANs to be a competitive semi-supervised training technique (Chapelle et al., 2009 ) thanks to that both inject noise to the real data points.", "dateLastCrawled": "2022-01-26T16:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Outlier Detection Using Distributionally Robust Optimization</b> ... - DeepAI", "url": "https://deepai.org/publication/outlier-detection-using-distributionally-robust-optimization-under-the-wasserstein-metric", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>outlier-detection-using-distributionally-robust</b>...", "snippet": "We empirically explore three important aspects of the <b>Wasserstein</b> DRO formulation, including the advantages of the \u2113 1-<b>loss</b> function, the selection of a proper norm for the <b>Wasserstein</b> metric, and the implication of penalizing the extended regression coefficient (\u2212 \u03b2, 1), through comparing with a series of regression <b>models</b> on a number of synthetic datasets. We show the superiority of the <b>Wasserstein</b> DRO approach, presenting a thorough analysis, under four different experimental setups ...", "dateLastCrawled": "2022-02-03T07:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) Generative Adversarial Networks for Data Generation in Structural ...", "url": "https://www.researchgate.net/publication/357068970_Generative_Adversarial_Networks_for_Data_Generation_in_Structural_Health_Monitoring", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/357068970_Generative_Adversarial_Networks_for...", "snippet": "In this paper, 1-D <b>Wasserstein</b> <b>loss</b> Deep Convolutional Generative Adversarial Networks using Gradient Penalty (1-D WDCGAN-GP) is utilized to generate damage associated vibration datasets that are ...", "dateLastCrawled": "2022-02-02T17:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>In-layer normalization techniques for training</b> very deep neural ...", "url": "https://theaisummer.com/normalization/", "isFamilyFriendly": true, "displayUrl": "https://theaisummer.com/normalization", "snippet": "It is theoretically and experimentally validated that it smooths the <b>loss</b> landscape by standardizing the weights in convolutional layers. Theoretically, WS reduces the Lipschitz constants of the <b>loss</b> and the gradients. The core idea is to keep the convolutional weights in a compact space. Hence, WS smooths the <b>loss</b> landscape and improves training. Remember that we observed a <b>similar</b> result in <b>Wasserstein</b> GANs. Some results from the official paper: For the record, they combined WS with Group ...", "dateLastCrawled": "2022-02-03T15:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Video Summarization: <b>Models</b>, code, and papers - CatalyzeX", "url": "https://www.catalyzex.com/s/Video%20Summarization", "isFamilyFriendly": true, "displayUrl": "https://www.catalyzex.com/s/Video Summarization", "snippet": "The GAN training problem is solved by introducing the <b>Wasserstein</b> GAN and two newly proposed video patch/score sum losses. In addition, the score sum <b>loss</b> can also relieve the model sensitivity to the varying video lengths, which is an inherent problem for most current video analysis tasks. Our method substantially lifts the performance on the ...", "dateLastCrawled": "2022-01-21T01:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Overfitting</b> in adversarially robust deep learning - arxiv-vanity.com", "url": "https://www.arxiv-vanity.com/papers/2002.11569/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2002.11569", "snippet": "3 Adversarial training and robust <b>overfitting</b>. In order to learn networks that are robust to adversarial examples, a commonly used method is adversarial training, which solves the following robust optimization problem. min \u03b8 \u2211 imax \u03b4\u2208\u0394 \u2113(f \u03b8(xi +\u03b4),yi), (1) where f \u03b8 is a network with parameters \u03b8, (xi,yi) is a training example ...", "dateLastCrawled": "2021-11-28T15:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Introduction to Generative Adversarial Networks \u2013 Mastering Machine ...", "url": "https://goois.net/introduction-to-generative-adversarial-networks-mastering-machine-learning-algorithms-second-edition.html", "isFamilyFriendly": true, "displayUrl": "https://goois.net/introduction-to-generative-adversarial-networks-mastering-machine...", "snippet": "In this chapter, we&#39;re going to provide a brief introduction to a family of generative <b>models</b> based on some game theory concepts. Their main peculiarity is an adversarial training procedure that is aimed at learning to distinguish between true and fake samples, driving, at the same time, another component that generates samples more and more <b>similar</b> to the training examples. In particular, we will be discussing: Adversarial training and standard Generative Adversarial Networks (GANs) Deep ...", "dateLastCrawled": "2021-09-18T23:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "GitHub - Gguinet/semisupervised-alignment", "url": "https://github.com/Gguinet/semisupervised-alignment", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/Gguinet/semisupervised-alignment", "snippet": "A pointwise <b>loss</b> will only compare one predicted label to the real one. Therefore, each item&#39;s label is not compared to any other informations. A pairwise <b>loss</b> will compare 2 scores from 2 items at the same time. With pairwise <b>loss</b>, a model will try to minimize the number of pairs that are in the wrong order relative to the true labels. A ...", "dateLastCrawled": "2021-08-09T12:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Industrial Engineering &amp; Management Systems", "url": "http://www.iemsjl.org/journal/article.php?code=80452", "isFamilyFriendly": true, "displayUrl": "www.iemsjl.org/journal/article.php?code=80452", "snippet": "5.3.2 Analysis of Training <b>Loss</b> of Classification <b>Models</b> . Next, we compare the effect of the augmented training data in order to clarify why generalization performance differs between proposal method and comparing method. For that purpose, the magnitude of the <b>loss</b> of the training data during the training process is compared. The change of the <b>loss</b> of the training and test data during the training of the classification model is shown below (Figure 2 to Figure 5). Figure 2 represents the ...", "dateLastCrawled": "2022-01-26T10:27:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Predicting Landscapes from Environmental Conditions Using</b> Generative ...", "url": "https://deepai.org/publication/predicting-landscapes-from-environmental-conditions-using-generative-networks", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>predicting-landscapes-from-environmental-conditions</b>...", "snippet": "Summary statistics for the prediction of the overall amount of vegetation for all <b>models</b> <b>can</b> be seen in table 1. As NDVI is a simple ratio between different spectral bands, these results <b>can</b> also be understood as the ability of the <b>models</b> to predict overall reflectance. The simpler <b>models</b> perform close to the GAN <b>models</b>. This effect might be due to the unnecessity of context and spatial features as NDVI is averaged across the image. While by a small margin, the more complex <b>models</b> (GAN 1 Gb ...", "dateLastCrawled": "2021-12-27T22:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Artificial intelligence with deep learning in nuclear medicine and ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8665861/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8665861", "snippet": "Therefore, dropout <b>can</b> <b>be thought</b> of as a way to create and train an ensemble of many subnetworks and thereby improve the generalization performance. Another view on why dropout has a regularizing effect is that it prevents coadaptation of different neurons. By removing different neurons at every iteration, neurons that are included should perform well regardless of which other neurons are included in the network. Hence, it forces the neurons to be relevant in many contexts. Our example in Fig.", "dateLastCrawled": "2022-01-29T17:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Short Question | Short Question Online Test - Avatto", "url": "https://avatto.com/interview-questions/short-question/", "isFamilyFriendly": true, "displayUrl": "https://avatto.com/interview-questions/short-question", "snippet": "Some of the methods used for <b>preventing</b> <b>overfitting</b> in neural networks include the following: Dropouts; Early stopping ; Regularization; Data augmentation; What could be the reason that <b>loss</b> leads to nan during training the network? Some of the most common reasons for the <b>loss</b> leading to nan during training the network includes when the learning rate is set to a high value when the gradient blows up and improper or poor <b>loss</b> function. What is data normalization? Data normalization is usually ...", "dateLastCrawled": "2022-01-26T21:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A comprehensive survey on regularization strategies in machine learning ...", "url": "https://www.sciencedirect.com/science/article/pii/S156625352100230X", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S156625352100230X", "snippet": "The penalty terms usually limit the complexity the <b>models</b> to avoid <b>overfitting</b>. ... The robust PCA problem is commonly <b>thought</b> of as a low-rank matrix recovery problem with incorporates sparse corruption. The goal of robust PCA is to enhance the robustness of PCA against outliers or corrupted observations of PCA. In fact, the data matrix A of this problem is a composite matrix of sparse and low-rank recovery which needs to be decomposed into two components such that A = L + S, where L is a ...", "dateLastCrawled": "2022-01-30T18:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Lester</b> Mackey: Research - Stanford University", "url": "https://web.stanford.edu/~lmackey/research.html", "isFamilyFriendly": true, "displayUrl": "https://web.stanford.edu/~lmackey/research.html", "snippet": "Given a <b>loss</b> function F : X -&gt; R+ that <b>can</b> be written as the sum of losses over a large set of inputs a1, ..., an, it is often desirable to approximate F by subsampling the input points. Strong theoretical guarantees require taking into account the importance of each point, measured by how much its individual <b>loss</b> contributes to F(x). Maximizing this importance over all x yields the sensitivity score of ai. Sampling with probabilities proportional to these scores gives strong guarantees ...", "dateLastCrawled": "2022-02-02T17:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "On the Suitability of Lp-Norms <b>for Creating and Preventing Adversarial</b> ...", "url": "https://www.researchgate.net/publication/329743904_On_the_Suitability_of_Lp-Norms_for_Creating_and_Preventing_Adversarial_Examples", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/329743904_On_the_Suitability_of_Lp-Norms_for...", "snippet": "We formalize the problem setting and systematically evaluate what benefits the adversary <b>can</b> gain by using substitute <b>models</b>. We show that there is an exploration-exploitation tradeoff in that ...", "dateLastCrawled": "2021-12-24T00:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) Deep <b>Learning for NLP and Speech Recognition</b> | William Jacome ...", "url": "https://www.academia.edu/43190210/Deep_Learning_for_NLP_and_Speech_Recognition", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/43190210/Deep_<b>Learning_for_NLP_and_Speech_Recognition</b>", "snippet": "Academia.edu is a platform for academics to share research papers.", "dateLastCrawled": "2022-02-02T18:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Python Machine Learning: Machine Learning and Deep Learning with Python ...", "url": "https://ebin.pub/python-machine-learning-machine-learning-and-deep-learning-with-python-scikit-learn-and-tensorflow-2-3rd-edition-3nbsped-9781789955750-1789955750.html", "isFamilyFriendly": true, "displayUrl": "https://ebin.pub/python-machine-learning-machine-learning-and-deep-learning-with...", "snippet": "<b>Overfitting</b> Note that we will evaluate the performance of our <b>models</b> based on the test dataset in this chapter. In Chapter 6, Learning Best Practices for Model Evaluation and Hyperparameter Tuning, you will learn about useful techniques, including graphical analysis, such as learning curves, to detect and prevent <b>overfitting</b>. <b>Overfitting</b>, which we will return to later in this chapter, means that the model captures the patterns in the training data well but fails to generalize well to unseen ...", "dateLastCrawled": "2022-01-31T17:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>In-layer normalization techniques for training</b> very deep neural ...", "url": "https://theaisummer.com/normalization/", "isFamilyFriendly": true, "displayUrl": "https://theaisummer.com/normalization", "snippet": "To this end, we need to develop ways to train our <b>models</b> more effectively. Effectiveness <b>can</b> be evaluated in terms of training time, performance, and stability. Below you <b>can</b> see a graph depicting the trends in normalization methods used by different papers through time. Source: papers with code", "dateLastCrawled": "2022-02-03T15:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Using the power of transfer learning for building a flower classifier ...", "url": "https://paul-mora.com/transfer-learning/clustering/python/Using-the-power-of-transfer-learning-for-building-a-flower-classifier/", "isFamilyFriendly": true, "displayUrl": "https://paul-mora.com/transfer-learning/clustering/python/Using-the-power-of-transfer...", "snippet": "In this blog-post we discuss the concept of transfer-learning and show an implementation in Python, using tensorflow. Namely, we are using the pre-trained model MobileNetV2 and apply it on the Oxford Flower 102 dataset, in order to build a flower classification model. Lastly, we deploy the trained model on an ios device to make live predictions, using the phone\u2019s camera. The Github Repository for this project <b>can</b> be found here.", "dateLastCrawled": "2022-01-09T14:46:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Wasserstein</b> GAN: Deep Generation applied on Bitcoins financial time ...", "url": "https://deepai.org/publication/wasserstein-gan-deep-generation-applied-on-bitcoins-financial-time-series", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>wasserstein</b>-gan-deep-generation-applied-on-bitcoins...", "snippet": "The <b>Wasserstein</b> <b>loss</b> <b>can</b>, for this reason, be tremendous, which is usually unsettling, due to that large numbers in neural networks should be avoided. The authors of the WGAN paper show that for the <b>Wasserstein</b> <b>loss</b> function, it also needs to place an additional constraint on the critic. Specifically, it is required that the critic is a 1-Lipschitz continuous function. To enforce the Lipschitz constraint on the critic, it proposes to clip the critic\u2019s weights to be in a compact space ...", "dateLastCrawled": "2022-01-11T22:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Deep Convolutional Neural Networks With Ensemble Learning and ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8416107/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8416107", "snippet": "These results illustrate the advantage of the GoogLeNet architecture in <b>preventing</b> <b>overfitting</b> by using auxiliary classifiers, <b>compared</b> to other <b>models</b>, such as AlexNet and VGG-16. ResNet is an additional successful variant of the classical CNN (He et al., 2016). The hallmarks of the ResNet model include the incorporation of residual mapping ...", "dateLastCrawled": "2022-01-29T05:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) Generative Adversarial Networks for Data Generation in Structural ...", "url": "https://www.researchgate.net/publication/357068970_Generative_Adversarial_Networks_for_Data_Generation_in_Structural_Health_Monitoring", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/357068970_Generative_Adversarial_Networks_for...", "snippet": "In this paper, 1-D <b>Wasserstein</b> <b>loss</b> Deep Convolutional Generative Adversarial Networks using Gradient Penalty (1-D WDCGAN-GP) is utilized to generate damage associated vibration datasets that are ...", "dateLastCrawled": "2022-02-02T17:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Outlier Detection Using Distributionally Robust Optimization</b> ... - DeepAI", "url": "https://deepai.org/publication/outlier-detection-using-distributionally-robust-optimization-under-the-wasserstein-metric", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>outlier-detection-using-distributionally-robust</b>...", "snippet": "We empirically explore three important aspects of the <b>Wasserstein</b> DRO formulation, including the advantages of the \u2113 1-<b>loss</b> function, the selection of a proper norm for the <b>Wasserstein</b> metric, and the implication of penalizing the extended regression coefficient (\u2212 \u03b2, 1), through comparing with a series of regression <b>models</b> on a number of synthetic datasets. We show the superiority of the <b>Wasserstein</b> DRO approach, presenting a thorough analysis, under four different experimental setups ...", "dateLastCrawled": "2022-02-03T07:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "[PDF] Empirical Analysis Of <b>Overfitting</b> And Mode Drop In Gan Training ...", "url": "https://www.semanticscholar.org/paper/Empirical-Analysis-Of-Overfitting-And-Mode-Drop-In-Yazici-Foo/1062091d18374f4d475a1db877c7d96e8ab5ac5b", "isFamilyFriendly": true, "displayUrl": "https://www.semanticscholar.org/paper/Empirical-Analysis-Of-<b>Overfitting</b>-And-Mode-Drop...", "snippet": "It is shown that when stochasticity is removed from the training procedure, GANs <b>can</b> overfit and exhibit almost no mode drop, providing evidence against prevailing intuitions that GAns do not memorize the training set, and that mode dropping is mainly due to properties of the GAN objective rather than how it is optimized during training. We examine two key questions in GAN training, namely <b>overfitting</b> and mode drop, from an empirical perspective. We show that when stochasticity is removed ...", "dateLastCrawled": "2021-09-18T00:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "The <b>theoretical research of generative adversarial networks</b>: an ...", "url": "https://www.sciencedirect.com/science/article/pii/S0925231220320269", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0925231220320269", "snippet": "This metric <b>can</b> detect <b>overfitting</b> and mode dropping: if the generator memorizes all training samples, the feature distance between generated samples and validation samples will be large due to the difference between the verification set and the training set; if the mode is dropping, the critic <b>can</b> easily distinguish between generated samples and validation samples. In addition, the <b>Wasserstein</b> distance <b>can</b> measure the distance between non-overlapping distributions, and the magnitude of the ...", "dateLastCrawled": "2021-12-28T12:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Generative adversarial networks for generating synthetic features for ...", "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0260308", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0260308", "snippet": "We developed, trained, and <b>compared</b> two of the most used GAN architectures: the Vanilla GAN and the <b>Wasserstein</b> GAN (WGAN). Both <b>models</b> presented satisfactory results and were able to generate synthetic data similar to the real ones. In particular, the distribution of the synthetic data overlaps the distribution of the real data for all of the considered features. Moreover, the considered generative <b>models</b> <b>can</b> reproduce the same associations observed for the synthetic features. We chose the ...", "dateLastCrawled": "2022-01-08T17:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Video Summarization: <b>Models</b>, code, and papers - CatalyzeX", "url": "https://www.catalyzex.com/s/Video%20Summarization", "isFamilyFriendly": true, "displayUrl": "https://www.catalyzex.com/s/Video Summarization", "snippet": "The GAN training problem is solved by introducing the <b>Wasserstein</b> GAN and two newly proposed video patch/score sum losses. In addition, the score sum <b>loss</b> <b>can</b> also relieve the model sensitivity to the varying video lengths, which is an inherent problem for most current video analysis tasks. Our method substantially lifts the performance on the target benchmark datasets and exceeds the current leaderboard Rank 1 state of the art CSNet (2.1% F1 score increase on TVSum and 3.1% F1 score ...", "dateLastCrawled": "2022-01-21T01:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Industrial Engineering &amp; Management Systems", "url": "http://www.iemsjl.org/journal/article.php?code=80452", "isFamilyFriendly": true, "displayUrl": "www.iemsjl.org/journal/article.php?code=80452", "snippet": "In such case, augmented data <b>can</b>\u2019t reduce <b>overfitting</b> on the original training data. Therefore, our research contributes to augment data which meets these two requirements. In this study, we propose a method to generate data by the class specific GAN with small training data and selectively add generated data to the training data set that improves classification accuracy by using the entropy of the classification model. The feature of the proposed method is that it focuses on the ...", "dateLastCrawled": "2022-01-26T10:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "GitHub - sboonpan/Stock.ai", "url": "https://github.com/sboonpan/Stock.ai", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/sboonpan/Stock.ai", "snippet": "Also, <b>compared</b> to KL and JS, <b>Wasserstein</b> distance is differentiable nearly everywhere. As we know, during backpropagation, we differentiate the <b>loss</b> function in order to create the gradients, which in turn update the weights. Therefore, having a differentiable <b>loss</b> function is quite important. Hands down, this was the toughest part of this notebook. Mixing WGAN and MHGAN took me three days. 4.4. The Generator - One layer RNN 4.4.1. LSTM or GRU As mentioned before, the generator is a LSTM ...", "dateLastCrawled": "2022-02-03T07:05:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "How to stabilize GAN training. Understand <b>Wasserstein</b> distance and ...", "url": "https://towardsdatascience.com/wasserstein-distance-gan-began-and-progressively-growing-gan-7e099f38da96", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>wasserstein</b>-distance-gan-began-and-progressively...", "snippet": "<b>Wasserstein</b> <b>loss</b> leads to a higher quality of the gradients to train G. ... Finally, one intuitive way to understand this paper is to make an <b>analogy</b> with the gradients on the history of in-layer activation functions. Specifically, the gradients of sigmoid and tanh activations that disappeared in favor of ReLUs, because of the improved gradients in the whole range of values. BEGAN (Boundary Equilibrium Generative Adversarial Networks 2017) We often see that the discriminator progresses too ...", "dateLastCrawled": "2022-01-25T18:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Learning Wasserstein Embeddings</b> | DeepAI", "url": "https://deepai.org/publication/learning-wasserstein-embeddings", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>learning-wasserstein-embeddings</b>", "snippet": "The <b>Wasserstein</b> distance received a lot of attention recently in the community of <b>machine</b> <b>learning</b>, especially for its principled way of comparing distributions. It has found numerous applications in several hard problems, such as domain adaptation, dimensionality reduction or generative models. However, its use is still limited by a heavy ...", "dateLastCrawled": "2022-01-05T07:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>Learning</b> <b>Wasserstein</b> Embeddings - ResearchGate", "url": "https://www.researchgate.net/publication/320564581_Learning_Wasserstein_Embeddings", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/320564581_<b>Learning</b>_<b>Wasserstein</b>_Embeddings", "snippet": "Designed through an <b>analogy</b> with ... Fast dictionary <b>learning</b> with a smoothed <b>wasserstein</b> <b>loss</b>. In AISTA TS, pages 630\u2013638, 2016. [32] F. Santambrogio. Introduction to optimal transport theory ...", "dateLastCrawled": "2021-12-13T04:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "deep <b>learning</b> - How can both generator and discriminator losses ...", "url": "https://datascience.stackexchange.com/questions/32699/how-can-both-generator-and-discriminator-losses-decrease", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/32699", "snippet": "In the widely used <b>analogy</b>: ... despite the WGAN having a different <b>loss</b> function, namely the <b>Wasserstein</b> distance, one should still not expect that the discriminator and generator simultaneously monotonically increase -- generally one of them &quot;wins&quot; the round and receives a lower portion of the <b>loss</b>. $\\endgroup$ \u2013 PSub. Mar 13 &#39;21 at 6:07 $\\begingroup$ @PSub You are completely misunderstanding the question. It&#39;s not a question about the small scale changes of the <b>loss</b> values. OP is asking ...", "dateLastCrawled": "2022-01-28T20:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Manifold-Valued Image Generation with <b>Wasserstein</b> Generative ...", "url": "https://ojs.aaai.org/index.php/AAAI/article/download/4277/4155", "isFamilyFriendly": true, "displayUrl": "https://ojs.aaai.org/index.php/AAAI/article/download/4277/4155", "snippet": "fundamental <b>machine</b> <b>learning</b> problems. However, few mod-ern generative models, including <b>Wasserstein</b> Generative Ad-versarial Nets (WGANs), are studied on manifold-valued im- ages that are frequently encountered in real-world applica-tions. To \ufb01ll the gap, this paper \ufb01rst formulates the problem of generating manifold-valued images and exploits three typical instances: hue-saturation-value (HSV) color image genera-tion, chromaticity-brightness (CB) color image generation, and diffusion ...", "dateLastCrawled": "2022-01-29T09:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Advanced <b>Machine</b> <b>Learning</b> - Computer Vision", "url": "https://www.vision.rwth-aachen.de/media/course/SS/2019/advanced-machine-learning/ml2_19-part17-gans-6on1.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.vision.rwth-aachen.de/media/course/SS/2019/advanced-<b>machine</b>-<b>learning</b>/ml2...", "snippet": "<b>Analogy</b>: police investigator \u2022Both generator and discriminator are deep networks We can train them with backprop. Image sources: www.bundesbank.de, weclipart.com, Kevin McGuiness 15 Advanced <b>Machine</b> <b>Learning</b> Part 17 \u2013Generative Adversarial Networks Training the Discriminator \u2022Procedure Fix generator weights Train discriminator to distinguish between real and generated images Image credit: Kevin McGuiness 16 Visual Computing Institute | Prof. Dr . Bastian Leibe Advanced <b>Machine</b> <b>Learning</b> ...", "dateLastCrawled": "2021-10-25T15:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "[D] Is the <b>Wasserstein</b> distance really what we optimize in WGAN ...", "url": "https://www.reddit.com/r/MachineLearning/comments/ew2lzs/d_is_the_wasserstein_distance_really_what_we/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/<b>MachineLearning</b>/comments/ew2lzs/d_is_the_<b>wasserstein</b>_distance...", "snippet": "The &quot;genuine&quot; <b>Wasserstein</b> <b>loss</b> relies on optimal transport, a generalization of sorting to high-dimensional feature spaces. In a nutshell: OT relies on the matrix of distances between samples to define a &quot;least action&quot; matching between any two distributions. Now, unfortunately, in spaces of images, the L2 distance is (essentially) meaningless: natural images should not be compared with each other pixel-wise. As a consequence, the baseline <b>Wasserstein</b> distance between two batches of images is ...", "dateLastCrawled": "2021-09-30T11:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A <b>Tour of Generative Adversarial Network Models</b> - <b>Machine</b> <b>Learning</b> Mastery", "url": "https://machinelearningmastery.com/tour-of-generative-adversarial-network-models/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>tour-of-generative-adversarial-network-models</b>", "snippet": "By <b>analogy</b> with auto-encoders, we propose Context Encoders \u2013 a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. \u2014 Context Encoders: Feature <b>Learning</b> by Inpainting, 2016. Example of the Context Encoders Encoder-Decoder Model Architecture. Taken from: Context Encoders: Feature <b>Learning</b> by Inpainting. The model is trained with a joint-<b>loss</b> that combines both the adversarial <b>loss</b> of generator and discriminator models ...", "dateLastCrawled": "2022-02-03T03:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "SpringerLink - <b>Machine Learning</b>", "url": "https://link.springer.com/article/10.1007/s10994-020-05924-1", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10994-020-05924-1", "snippet": "for a given set \\({\\mathcal {F}}\\) of distributions, twice differentiable and convex <b>loss</b> \\(\\ell\\), and prediction \\(f_\\theta (x)\\).The set \\({\\mathcal {F}}\\) is the set of distributions on which one would like the estimator to achieve a guaranteed performance bound.. Causal inference can be seen to be a specific instance of distributional robustness, where we take \\({\\mathcal {F}}\\) to be the class of all distributions generated under do-interventions on the predictors X (Meinshausen 2018 ...", "dateLastCrawled": "2022-01-15T22:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "A <b>Gentle Introduction to Pix2Pix Generative</b> Adversarial Network", "url": "https://machinelearningmastery.com/a-gentle-introduction-to-pix2pix-generative-adversarial-network/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/a-<b>gentle-introduction-to-pix2pix-generative</b>...", "snippet": "Image-to-image translation is the controlled conversion of a given source image to a target image. An example might be the conversion of black and white photographs to color photographs. Image-to-image translation is a challenging problem and often requires specialized models and <b>loss</b> functions for a given translation task or dataset. The Pix2Pix GAN is a general approach for image-to-image translation. It is based", "dateLastCrawled": "2022-02-02T13:26:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(wasserstein loss)  is like +(preventing the overfitting of models)", "+(wasserstein loss) is similar to +(preventing the overfitting of models)", "+(wasserstein loss) can be thought of as +(preventing the overfitting of models)", "+(wasserstein loss) can be compared to +(preventing the overfitting of models)", "machine learning +(wasserstein loss AND analogy)", "machine learning +(\"wasserstein loss is like\")", "machine learning +(\"wasserstein loss is similar\")", "machine learning +(\"just as wasserstein loss\")", "machine learning +(\"wasserstein loss can be thought of as\")", "machine learning +(\"wasserstein loss can be compared to\")"]}