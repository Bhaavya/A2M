{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Entropy | Free Full-Text | Why Do Big Data and Machine Learning Entail ...", "url": "https://www.mdpi.com/1099-4300/23/3/297/htm", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/1099-4300/23/3/297/htm", "snippet": "The <b>stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>) has been widely used in deep learning with great success because of the computational efficiency [80,81]. The <b>gradient</b> noise (GN) in the <b>SGD</b> algorithm is often considered to be Gaussian in the large data regime by assuming that the classical central limit theorem (CLT) kicks in. The machine-learning tasks are usually considered as solving the following optimization problem:", "dateLastCrawled": "2022-02-02T19:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) Why Do Big Data and <b>Machine Learning Entail the Fractional Dynamics</b>?", "url": "https://www.researchgate.net/publication/349681844_Why_Do_Big_Data_and_Machine_Learning_Entail_the_Fractional_Dynamics", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/349681844_Why_Do_Big_Data_and_Machine...", "snippet": "The <b>stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>) has been widely used in deep learning with . great success because of the computational ef\ufb01ciency [80, 81]. The <b>gradient</b> noise (GN) in. the <b>SGD</b> algorithm is ...", "dateLastCrawled": "2022-02-02T05:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Data Science Question and Answer</b> | PDF | Machine Learning | Statistical ...", "url": "https://www.scribd.com/document/354655052/Data-Science-Question-and-Answer", "isFamilyFriendly": true, "displayUrl": "https://www.scribd.com/document/354655052/<b>Data-Science-Question-and-Answer</b>", "snippet": "Tags: machine-learning (Prev Q) (Next Q), neuralnetwork (Next Q), algorithms (Next Q) Im currently working on implementing <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> (<b>SGD</b>) for neural nets using backpropagation, and while I understand its purpose I have some questions about how to choose values for the learning rate.", "dateLastCrawled": "2022-01-27T15:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Recognition of Human Activities Using Continuous Autoencoders with ...", "url": "https://europepmc.org/article/MED/26861319", "isFamilyFriendly": true, "displayUrl": "https://europepmc.org/article/MED/26861319", "snippet": "In order to shorten the training time, we propose a new fast <b>stochastic</b> <b>gradient</b> <b>descent</b> (FSGD) algorithm to update the gradients of CAE. The reconstruction of a swiss-roll dataset experiment demonstrates that the CAE can fit continuous data better than the basic autoencoder, and the training time can be reduced by an FSGD algorithm. In the experiment of human activities&#39; recognition, time and frequency domain feature extract (TFFE) method is raised to extract features from the original ...", "dateLastCrawled": "2021-07-27T17:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) Triaxial Inertial Devices for <b>Stochastic</b> Life-Log Monitoring via ...", "url": "https://www.researchgate.net/publication/346715146_Triaxial_Inertial_Devices_for_Stochastic_Life-Log_Monitoring_via_Augmented-Signal_and_a_Hierarchical_Recognizer", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/346715146_Triaxial_Inertial_Devices_for...", "snippet": "21 <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> (<b>SGD</b>) optimization algorithms with adaptive learning of(a) exercising and (b) <b>walking</b> activities using the IM-WSHA dataset. Figures - uploaded by Sheikh Badar ud din ...", "dateLastCrawled": "2022-01-05T01:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "AI 2020: Advances in Artificial Intelligence: 33rd Australasian Joint ...", "url": "https://dokumen.pub/ai-2020-advances-in-artificial-intelligence-33rd-australasian-joint-conference-ai-2020-canberra-act-australia-november-2930-2020-proceedings-9783030649838-9783030649845.html", "isFamilyFriendly": true, "displayUrl": "https://<b>dokumen</b>.pub/ai-2020-advances-in-artificial-intelligence-33rd-australasian...", "snippet": "Federated averaging (FedAvg), a <b>stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>) based optimization method [9], is massively used in federated learning. Now, momentum [13] accelerates <b>SGD</b> in the proper direction and dampens oscillations. Adaptive Moment Estimation (Adam) [6] computes adaptive learning rate for each training parameter. Adamax [6] is a variant of Adam optimizer. In adaptive federated optimization [15], the federated version of the adaptive optimizers such as Adagrad, Adam, and Yogi have been ...", "dateLastCrawled": "2021-12-08T16:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Sensors | Free Full-Text | Recognition of Human Activities Using ...", "url": "https://www.mdpi.com/1424-8220/16/2/189/htm", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/1424-8220/16/2/189/htm", "snippet": "In order to shorten the training time, we propose a new fast <b>stochastic</b> <b>gradient</b> <b>descent</b> (FSGD) algorithm to update the gradients of CAE. The reconstruction of a swiss-roll dataset experiment demonstrates that the CAE can fit continuous data better than the basic autoencoder, and the training time can be reduced by an FSGD algorithm. In the experiment of human activities\u2019 recognition, time and frequency domain feature extract (TFFE) method is raised to extract features from the original ...", "dateLastCrawled": "2021-10-20T09:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Chapter 9 Multi-user Validation | Behavior Analysis with Machine ...", "url": "https://enriquegit.github.io/behavior-free/multiuser.html", "isFamilyFriendly": true, "displayUrl": "https://enriquegit.github.io/behavior-free/multiuser.html", "snippet": "Chapter 9 Multi-user Validation | Behavior Analysis with Machine Learning Using R teaches you how to train machine learning models in the R programming language to make sense of behavioral data collected with sensors and stored in electronic records. This book introduces machine learning concepts and algorithms applied to a diverse set of behavior analysis problems by focusing on practical aspects. Some of the topics include how to: Build supervised models to predict indoor locations based ...", "dateLastCrawled": "2022-01-30T15:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>CreateSpace Data Mar 2016 ISBN 1530655277</b> | Machine Learning ...", "url": "https://www.scribd.com/document/366396894/CreateSpace-Data-Mar-2016-ISBN-1530655277-pdf", "isFamilyFriendly": true, "displayUrl": "https://www.<b>scribd</b>.com/document/366396894/<b>CreateSpace-Data-Mar-2016-ISBN-1530655277</b>-pdf", "snippet": "Tags: machine-learning (Prev Q) (Next Q), neuralnetwork (Next Q), algorithms (Next Q) Im currently working on implementing <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> (<b>SGD</b>) for neural nets using backpropagation, and while I understand its purpose I have some questions about how to choose values for the learning rate.", "dateLastCrawled": "2021-10-25T20:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Titlebot/davidjayharris.txt at master \u00b7 <b>davharris/Titlebot</b> \u00b7 <b>GitHub</b>", "url": "https://github.com/davharris/Titlebot/blob/master/data/davidjayharris.txt", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/davharris/Titlebot/blob/master/data/davidjayharris.txt", "snippet": "This is exciting! MCMC on a budget. <b>Like</b> <b>stochastic</b> <b>gradient</b> <b>descent</b>, but for MCMC. via @StatMLPapers @ZachWeiner @brianwolven Just noticed a Typo. Should have been 1.8e-5 m^3. Yikes! @Mattcademia @Mchl_Wolfe @ucfagls can you point to an example where curated data was deemed copyrightable in the US? @michaelhoffman @hylopsar @dgmacarthur @algaebarnacle @jtleek good frequency properties too. Controlling false discovery rate is useful 2/2: @michaelhoffman @hylopsar @dgmacarthur @algaebarnacle ...", "dateLastCrawled": "2021-09-07T11:07:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Recognition of Human Activities Using Continuous Autoencoders with ...", "url": "https://europepmc.org/article/MED/26861319", "isFamilyFriendly": true, "displayUrl": "https://europepmc.org/article/MED/26861319", "snippet": "In order to shorten the training time, we propose a new fast <b>stochastic</b> <b>gradient</b> <b>descent</b> (FSGD) algorithm to update the gradients of CAE. The reconstruction of a swiss-roll dataset experiment demonstrates that the CAE can fit continuous data better than the basic autoencoder, and the training time can be reduced by an FSGD algorithm. In the experiment of human activities&#39; recognition, time and frequency domain feature extract (TFFE) method is raised to extract features from the original ...", "dateLastCrawled": "2021-07-27T17:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Entropy | Free Full-Text | Why Do Big Data and Machine Learning Entail ...", "url": "https://www.mdpi.com/1099-4300/23/3/297/htm", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/1099-4300/23/3/297/htm", "snippet": "FC is used in optimal randomness in the methods of <b>stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>) and random search, and in implementing the internal model principle (IMP) . FOT is a way of thinking using FC. For example, there are non-integers between the integers; between logic 0 and logic 1, there is the fuzzy logic ; compared with integer-order splines, there are fractional-order splines ; between the high-order integer moments, there are non-integer-order moments, etc. FOT has been entailed by many ...", "dateLastCrawled": "2022-02-02T19:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Sensors | Free Full-Text | Recognition of Human Activities Using ...", "url": "https://www.mdpi.com/1424-8220/16/2/189/htm", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/1424-8220/16/2/189/htm", "snippet": "In order to shorten the training time, we propose a new fast <b>stochastic</b> <b>gradient</b> <b>descent</b> (FSGD) algorithm to update the gradients of CAE. The reconstruction of a swiss-roll dataset experiment demonstrates that the CAE can fit continuous data better than the basic autoencoder, and the training time can be reduced by an FSGD algorithm. In the experiment of human activities\u2019 recognition, time and frequency domain feature extract (TFFE) method is raised to extract features from the original ...", "dateLastCrawled": "2021-10-20T09:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) Recognition of Human Activities Using Continuous Autoencoders ...", "url": "https://www.researchgate.net/publication/293042600_Recognition_of_Human_Activities_Using_Continuous_Autoencoders_with_Wearable_Sensors", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/293042600_Recognition_of_Human_Activities...", "snippet": "In 1952, Kiefer and Wolfowitz put forward <b>stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>) algorith m [23], which was widely applied in the machine learnin g [24,25] domain . <b>SGD</b> algorit hm does not need", "dateLastCrawled": "2022-01-26T16:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Chapter 9 Multi-user Validation | Behavior Analysis with Machine ...", "url": "https://enriquegit.github.io/behavior-free/multiuser.html", "isFamilyFriendly": true, "displayUrl": "https://enriquegit.github.io/behavior-free/multiuser.html", "snippet": "Chapter 9 Multi-user Validation | Behavior Analysis with Machine Learning Using R teaches you how to train machine learning models in the R programming language to make sense of behavioral data collected with sensors and stored in electronic records. This book introduces machine learning concepts and algorithms applied to a diverse set of behavior analysis problems by focusing on practical aspects. Some of the topics include how to: Build supervised models to predict indoor locations based ...", "dateLastCrawled": "2022-01-30T15:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "AI 2020: Advances in Artificial Intelligence: 33rd Australasian Joint ...", "url": "https://dokumen.pub/ai-2020-advances-in-artificial-intelligence-33rd-australasian-joint-conference-ai-2020-canberra-act-australia-november-2930-2020-proceedings-9783030649838-9783030649845.html", "isFamilyFriendly": true, "displayUrl": "https://<b>dokumen</b>.pub/ai-2020-advances-in-artificial-intelligence-33rd-australasian...", "snippet": "Federated averaging (FedAvg), a <b>stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>) based optimization method [9], is massively used in federated learning. Now, momentum [13] accelerates <b>SGD</b> in the proper direction and dampens oscillations. Adaptive Moment Estimation (Adam) [6] computes adaptive learning rate for each training parameter. Adamax [6] is a variant of Adam optimizer. In adaptive federated optimization [15], the federated version of the adaptive optimizers such as Adagrad, Adam, and Yogi have been ...", "dateLastCrawled": "2021-12-08T16:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Data Science Question and Answer</b> | PDF | Machine Learning | Statistical ...", "url": "https://www.scribd.com/document/354655052/Data-Science-Question-and-Answer", "isFamilyFriendly": true, "displayUrl": "https://www.scribd.com/document/354655052/<b>Data-Science-Question-and-Answer</b>", "snippet": "A <b>similar</b> approach would be to count how many distinct values ... The project goal is determine what activity a person is engaging in (e.g., <b>WALKING</b>, <b>WALKING</b>_UPSTAIRS, <b>WALKING</b>_<b>DOWNSTAIRS</b>, SITTING, STANDING, LAYING) from data recorded by a smartphone (Samsung Galaxy S II) on the subjects waist. Using its embedded accelerometer and gyroscope, the data includes 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50Hz. All the data set is given in one folder with some ...", "dateLastCrawled": "2022-01-27T15:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Artificial Intelligence By Example.pdf [5wgl5d599xo7]", "url": "https://vbook.pub/documents/artificial-intelligence-by-examplepdf-5wgl5d599xo7", "isFamilyFriendly": true, "displayUrl": "https://vbook.pub/documents/artificial-intelligence-by-examplepdf-5wgl5d599xo7", "snippet": "The concept of <b>gradient</b> <b>descent</b> is minimizing loss or errors between the present result and a goal to attain. First, a cost function is needed. There are four predicates (0-0, 1-1, 1-0, 0-1) to train correctly. We simply need to find out how many are correctly trained at each epoch. The cost function will measure the difference between the training goal (4) and the result of this epoch or training iteration (result). When 0 convergence is reached, it means the training has succeeded. SFTVMU", "dateLastCrawled": "2022-01-27T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Titlebot/davidjayharris.txt at master \u00b7 <b>davharris/Titlebot</b> \u00b7 <b>GitHub</b>", "url": "https://github.com/davharris/Titlebot/blob/master/data/davidjayharris.txt", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/davharris/Titlebot/blob/master/data/davidjayharris.txt", "snippet": "This is exciting! MCMC on a budget. Like <b>stochastic</b> <b>gradient</b> <b>descent</b>, but for MCMC. via @StatMLPapers @ZachWeiner @brianwolven Just noticed a Typo. Should have been 1.8e-5 m^3. Yikes! @Mattcademia @Mchl_Wolfe @ucfagls can you point to an example where curated data was deemed copyrightable in the US? @michaelhoffman @hylopsar @dgmacarthur @algaebarnacle @jtleek good frequency properties too. Controlling false discovery rate is useful 2/2: @michaelhoffman @hylopsar @dgmacarthur @algaebarnacle ...", "dateLastCrawled": "2021-09-07T11:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>CreateSpace Data Mar 2016 ISBN 1530655277</b> | Machine Learning ...", "url": "https://www.scribd.com/document/366396894/CreateSpace-Data-Mar-2016-ISBN-1530655277-pdf", "isFamilyFriendly": true, "displayUrl": "https://www.<b>scribd</b>.com/document/366396894/<b>CreateSpace-Data-Mar-2016-ISBN-1530655277</b>-pdf", "snippet": "A <b>similar</b> approach would be to count how many distinct ... , <b>WALKING</b>, <b>WALKING</b>_UPSTAIRS, <b>WALKING</b>_<b>DOWNSTAIRS</b>, SITTING, STANDING, LAYING) from data recorded by a smartphone (Samsung Galaxy S II) on the subjects waist. Using its embedded accelerometer and gyroscope , the data includes 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50Hz. All the data set is given in one folder with some description and feature labels. The data is divided for test and train files in ...", "dateLastCrawled": "2021-10-25T20:28:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) Why Do Big Data and <b>Machine Learning Entail the Fractional Dynamics</b>?", "url": "https://www.researchgate.net/publication/349681844_Why_Do_Big_Data_and_Machine_Learning_Entail_the_Fractional_Dynamics", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/349681844_Why_Do_Big_Data_and_Machine...", "snippet": "The <b>stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>) has been widely used in deep learning with . great success because of the computational ef\ufb01ciency [80, 81]. The <b>gradient</b> noise (GN) in. the <b>SGD</b> algorithm is ...", "dateLastCrawled": "2022-02-02T05:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Entropy | Free Full-Text | Why Do Big Data and Machine Learning Entail ...", "url": "https://www.mdpi.com/1099-4300/23/3/297/htm", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/1099-4300/23/3/297/htm", "snippet": "The <b>stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>) has been widely used in deep learning with great success because of the computational efficiency [80,81]. The <b>gradient</b> noise (GN) in the <b>SGD</b> algorithm is often considered to be Gaussian in the large data regime by assuming that the classical central limit theorem (CLT) kicks in. The machine-learning tasks are usually considered as solving the following optimization problem:", "dateLastCrawled": "2022-02-02T19:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Data Science Question and Answer</b> | PDF | Machine Learning | Statistical ...", "url": "https://www.scribd.com/document/354655052/Data-Science-Question-and-Answer", "isFamilyFriendly": true, "displayUrl": "https://www.scribd.com/document/354655052/<b>Data-Science-Question-and-Answer</b>", "snippet": "Tags: machine-learning (Prev Q) (Next Q), neuralnetwork (Next Q), algorithms (Next Q) Im currently working on implementing <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> (<b>SGD</b>) for neural nets using backpropagation, and while I understand its purpose I have some questions about how to choose values for the learning rate.", "dateLastCrawled": "2022-01-27T15:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "AI 2020: Advances in Artificial Intelligence: 33rd Australasian Joint ...", "url": "https://dokumen.pub/ai-2020-advances-in-artificial-intelligence-33rd-australasian-joint-conference-ai-2020-canberra-act-australia-november-2930-2020-proceedings-9783030649838-9783030649845.html", "isFamilyFriendly": true, "displayUrl": "https://<b>dokumen</b>.pub/ai-2020-advances-in-artificial-intelligence-33rd-australasian...", "snippet": "Federated averaging (FedAvg), a <b>stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>) based optimization method [9], is massively used in federated learning. Now, momentum [13] accelerates <b>SGD</b> in the proper direction and dampens oscillations. Adaptive Moment Estimation (Adam) [6] computes adaptive learning rate for each training parameter. Adamax [6] is a variant of Adam optimizer. In adaptive federated optimization [15], the federated version of the adaptive optimizers such as Adagrad, Adam, and Yogi have been ...", "dateLastCrawled": "2021-12-08T16:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Why Do Big Data and Machine Learning Entail the ... - europepmc.org", "url": "https://europepmc.org/article/PMC/PMC7997214", "isFamilyFriendly": true, "displayUrl": "https://europepmc.org/article/PMC/PMC7997214", "snippet": "Fractional-order calculus is about the differentiation and integration of non-integer orders. Fractional calculus (FC) is based on fractional-order thinking (FOT) and has been sho", "dateLastCrawled": "2021-03-30T00:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>CreateSpace Data Mar 2016 ISBN 1530655277</b> | Machine Learning ...", "url": "https://www.scribd.com/document/366396894/CreateSpace-Data-Mar-2016-ISBN-1530655277-pdf", "isFamilyFriendly": true, "displayUrl": "https://www.<b>scribd</b>.com/document/366396894/<b>CreateSpace-Data-Mar-2016-ISBN-1530655277</b>-pdf", "snippet": "Tags: machine-learning (Prev Q) (Next Q), neuralnetwork (Next Q), algorithms (Next Q) Im currently working on implementing <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> (<b>SGD</b>) for neural nets using backpropagation, and while I understand its purpose I have some questions about how to choose values for the learning rate.", "dateLastCrawled": "2021-10-25T20:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Artificial Intelligence By Example.pdf [5wgl5d599xo7]", "url": "https://vbook.pub/documents/artificial-intelligence-by-examplepdf-5wgl5d599xo7", "isFamilyFriendly": true, "displayUrl": "https://vbook.pub/documents/artificial-intelligence-by-examplepdf-5wgl5d599xo7", "snippet": "<b>Gradient</b> <b>descent</b> measures the value of the <b>descent</b> to find the direction of the slope: up, down, or 0. Then, once you have that slope and the steepness of it, you <b>can</b> optimize the weights. A derivative is a way to know whether you are going up or down a slope. In this case, I hijacked the concept and used it to set the learning rate with a one-line function. Why not? It helped to solve <b>gradient</b> <b>descent</b> optimization in one line: JG DPOWFSHFODF X C X", "dateLastCrawled": "2022-01-27T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "PANDAS/ICMLA_2014_2015_2016_2017.csv at main \u00b7 moroeljair/PANDAS \u00b7 GitHub", "url": "https://github.com/moroeljair/PANDAS/blob/main/ICMLA_2014_2015_2016_2017.csv", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/moroeljair/PANDAS/blob/main/ICMLA_2014_2015_2016_2017.csv", "snippet": "70,Leveraging Machine Learning Algorithms to Perform Online and Offline Highway Traffic Flow Prediction,&quot;traffic prediction, online learning, flow forecast, <b>stochastic</b> <b>gradient</b> <b>descent</b>&quot;,&quot;Advanced traffic management systems (ATMS) are heavily depending on traffic flow or equivalent travel time estimation. The main goal of this paper is to accomplish two different algorithms to perform offline and online traffic flow forecasting. A multi-layer perceptron (MLP), which is trained on yearly data ...", "dateLastCrawled": "2022-01-23T11:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Titlebot/davidjayharris.txt at master \u00b7 <b>davharris/Titlebot</b> \u00b7 <b>GitHub</b>", "url": "https://github.com/davharris/Titlebot/blob/master/data/davidjayharris.txt", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/davharris/Titlebot/blob/master/data/davidjayharris.txt", "snippet": "This is exciting! MCMC on a budget. Like <b>stochastic</b> <b>gradient</b> <b>descent</b>, but for MCMC. via @StatMLPapers @ZachWeiner @brianwolven Just noticed a Typo. Should have been 1.8e-5 m^3. Yikes! @Mattcademia @Mchl_Wolfe @ucfagls <b>can</b> you point to an example where curated data was deemed copyrightable in the US? @michaelhoffman @hylopsar @dgmacarthur @algaebarnacle @jtleek good frequency properties too. Controlling false discovery rate is useful 2/2: @michaelhoffman @hylopsar @dgmacarthur @algaebarnacle ...", "dateLastCrawled": "2021-09-07T11:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Data Science: Questions and Answers</b> | George A Duckett | download", "url": "https://b-ok.africa/book/2701277/91ea70", "isFamilyFriendly": true, "displayUrl": "https://b-ok.africa/book/2701277/91ea70", "snippet": "You <b>can</b> use this book to look up commonly asked questions, browse questions on a particular topic, compare answers to common topics, check out the original source and much more. This book has been designed to be very easy to use, with many internal references set up that makes browsing in many different ways possible. Topics covered include: Machine Learning, Bigdata, Data Mining, Classification, Neuralnetwork, Statistics, Python, Clustering, R, Text Mining, NLP, Dataset, Efficiency ...", "dateLastCrawled": "2021-12-26T06:22:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) Why Do Big Data and <b>Machine Learning Entail the Fractional Dynamics</b>?", "url": "https://www.researchgate.net/publication/349681844_Why_Do_Big_Data_and_Machine_Learning_Entail_the_Fractional_Dynamics", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/349681844_Why_Do_Big_Data_and_Machine...", "snippet": "The <b>stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>) has been widely used in deep learning with . great success because of the computational ef\ufb01ciency [80, 81]. The <b>gradient</b> noise (GN) in. the <b>SGD</b> algorithm is ...", "dateLastCrawled": "2022-02-02T05:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) Recognition of Human Activities Using Continuous Autoencoders ...", "url": "https://www.researchgate.net/publication/293042600_Recognition_of_Human_Activities_Using_Continuous_Autoencoders_with_Wearable_Sensors", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/293042600_Recognition_of_Human_Activities...", "snippet": "In 1952, Kiefer and Wolfowitz put forward <b>stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>) algorith m [23], which was widely applied in the machine learnin g [24,25] domain . <b>SGD</b> algorit hm does not need", "dateLastCrawled": "2022-01-26T16:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Entropy | Free Full-Text | Why Do Big Data and Machine Learning Entail ...", "url": "https://www.mdpi.com/1099-4300/23/3/297/htm", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/1099-4300/23/3/297/htm", "snippet": "FC is used in optimal randomness in the methods of <b>stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>) and random search, and in implementing the internal model principle (IMP) . FOT is a way of thinking using FC. For example, there are non-integers between the integers; between logic 0 and logic 1, there is the fuzzy logic ; <b>compared</b> with integer-order splines, there are fractional-order splines ; between the high-order integer moments, there are non-integer-order moments, etc. FOT has been entailed by many ...", "dateLastCrawled": "2022-02-02T19:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "AI 2020: Advances in Artificial Intelligence: 33rd Australasian Joint ...", "url": "https://dokumen.pub/ai-2020-advances-in-artificial-intelligence-33rd-australasian-joint-conference-ai-2020-canberra-act-australia-november-2930-2020-proceedings-9783030649838-9783030649845.html", "isFamilyFriendly": true, "displayUrl": "https://<b>dokumen</b>.pub/ai-2020-advances-in-artificial-intelligence-33rd-australasian...", "snippet": "We have <b>compared</b> the performance of adaptive learning rate based optimizers such as Adam and Adamax with Momentum based <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> (<b>SGD</b>) and found out that Momentum <b>SGD</b> yields better results than others. Lastly, for visualization, we have used Class Activation Mapping (CAM) approaches such as Grad-CAM, Grad-CAM++, and Score-CAM to identify pneumonia a\ufb00ected regions in a chest-X-ray. Keywords: Federated learning \u00b7 Optimization \u00b7 Transfer learning Medical imagery analysis ...", "dateLastCrawled": "2021-12-08T16:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Data Science Question and Answer</b> | PDF | Machine Learning | Statistical ...", "url": "https://www.scribd.com/document/354655052/Data-Science-Question-and-Answer", "isFamilyFriendly": true, "displayUrl": "https://www.scribd.com/document/354655052/<b>Data-Science-Question-and-Answer</b>", "snippet": "Tags: machine-learning (Prev Q) (Next Q), neuralnetwork (Next Q), algorithms (Next Q) Im currently working on implementing <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> (<b>SGD</b>) for neural nets using backpropagation, and while I understand its purpose I have some questions about how to choose values for the learning rate.", "dateLastCrawled": "2022-01-27T15:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Sensors | Free Full-Text | Deep Learning for Classifying Physical ...", "url": "https://www.mdpi.com/1424-8220/21/16/5564/htm", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/1424-8220/21/16/5564/htm", "snippet": "These learning rates are tested in combination with three well known optimizers, i.e., Adagrad, Adam and <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> (<b>SGD</b>). Furthermore, these combinations of optimizers and learning rates are tested for all three types of sliding windows, i.e., three, five and ten seconds, both for the basic and specific movement categories. Considering the number of data-points available in the datasets, all experiments are stopped after 2000 training steps. Thus, reducing the chance of ...", "dateLastCrawled": "2021-09-20T06:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Chapter 9 Multi-user Validation | Behavior Analysis with Machine ...", "url": "https://enriquegit.github.io/behavior-free/multiuser.html", "isFamilyFriendly": true, "displayUrl": "https://enriquegit.github.io/behavior-free/multiuser.html", "snippet": "Chapter 9 Multi-user Validation | Behavior Analysis with Machine Learning Using R teaches you how to train machine learning models in the R programming language to make sense of behavioral data collected with sensors and stored in electronic records. This book introduces machine learning concepts and algorithms applied to a diverse set of behavior analysis problems by focusing on practical aspects. Some of the topics include how to: Build supervised models to predict indoor locations based ...", "dateLastCrawled": "2022-01-30T15:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Why Do Big Data and Machine Learning Entail the ... - europepmc.org", "url": "https://europepmc.org/article/PMC/PMC7997214", "isFamilyFriendly": true, "displayUrl": "https://europepmc.org/article/PMC/PMC7997214", "snippet": "Fractional-order calculus is about the differentiation and integration of non-integer orders. Fractional calculus (FC) is based on fractional-order thinking (FOT) and has been sho", "dateLastCrawled": "2021-03-30T00:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "PANDAS/ICMLA_2014_2015_2016_2017.csv at main \u00b7 moroeljair/PANDAS \u00b7 GitHub", "url": "https://github.com/moroeljair/PANDAS/blob/main/ICMLA_2014_2015_2016_2017.csv", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/moroeljair/PANDAS/blob/main/ICMLA_2014_2015_2016_2017.csv", "snippet": "70,Leveraging Machine Learning Algorithms to Perform Online and Offline Highway Traffic Flow Prediction,&quot;traffic prediction, online learning, flow forecast, <b>stochastic</b> <b>gradient</b> <b>descent</b>&quot;,&quot;Advanced traffic management systems (ATMS) are heavily depending on traffic flow or equivalent travel time estimation. The main goal of this paper is to accomplish two different algorithms to perform offline and online traffic flow forecasting. A multi-layer perceptron (MLP), which is trained on yearly data ...", "dateLastCrawled": "2022-01-23T11:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Titlebot/davidjayharris.txt at master \u00b7 <b>davharris/Titlebot</b> \u00b7 <b>GitHub</b>", "url": "https://github.com/davharris/Titlebot/blob/master/data/davidjayharris.txt", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/davharris/Titlebot/blob/master/data/davidjayharris.txt", "snippet": "This is exciting! MCMC on a budget. Like <b>stochastic</b> <b>gradient</b> <b>descent</b>, but for MCMC. via @StatMLPapers @ZachWeiner @brianwolven Just noticed a Typo. Should have been 1.8e-5 m^3. Yikes! @Mattcademia @Mchl_Wolfe @ucfagls <b>can</b> you point to an example where curated data was deemed copyrightable in the US? @michaelhoffman @hylopsar @dgmacarthur @algaebarnacle @jtleek good frequency properties too. Controlling false discovery rate is useful 2/2: @michaelhoffman @hylopsar @dgmacarthur @algaebarnacle ...", "dateLastCrawled": "2021-09-07T11:07:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Stochastic Gradient Descent</b> \u2014 Clearly Explained !! | by Aishwarya V ...", "url": "https://towardsdatascience.com/stochastic-gradient-descent-clearly-explained-53d239905d31", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>stochastic-gradient-descent</b>-clearly-explained-53d239905d31", "snippet": "<b>Stochastic gradient descent</b> is a very popular and common algorithm used in various <b>Machine</b> <b>Learning</b> algorithms, most importantly forms the basis of Neural Networks. In this article, I have tried my\u2026 Get started. Open in app. Sign in. Get started. Follow. 618K Followers \u00b7 Editors&#39; Picks Features Deep Dives Grow Contribute. About. Get started. Open in app. <b>Stochastic Gradient Descent</b> \u2014 Clearly Explained !! Aishwarya V Srinivasan. Sep 7, 2019 \u00b7 4 min read. <b>Stochastic gradient descent</b> is a ...", "dateLastCrawled": "2022-02-02T12:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Stochastic gradient descent</b> - The <b>Learning</b> <b>Machine</b>", "url": "https://the-learning-machine.com/article/optimization/stochastic-gradient-descent", "isFamilyFriendly": true, "displayUrl": "https://the-<b>learning</b>-<b>machine</b>.com/article/optimization/<b>stochastic-gradient-descent</b>", "snippet": "<b>Stochastic gradient descent</b> (<b>SGD</b>) is an approach for unconstrained optimization.<b>SGD</b> is the workhorse of optimization for <b>machine</b> <b>learning</b> approaches. It is used as a faster alternative for training support vector machines and is the preferred optimization routine for deep <b>learning</b> approaches.. In this article, we will motivate the formulation for <b>stochastic gradient descent</b> and provide interactive demos over multiple univariate and multivariate functions to show it in action.", "dateLastCrawled": "2022-01-26T05:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Stochastic</b> <b>Gradient</b> <b>Descent</b> in Theory and Practice", "url": "https://ai.stanford.edu/~optas/data/stanford_qual_exams.pdf", "isFamilyFriendly": true, "displayUrl": "https://ai.stanford.edu/~optas/data/stanford_qual_exams.pdf", "snippet": "<b>Stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>) is the most widely used optimization method in the <b>machine</b> <b>learning</b> community. Researchers in both academia and industry have put considerable e ort to optimize <b>SGD</b>\u2019s runtime performance and to develop a theoretical framework for its empirical success. For example, recent advancements in deep neural networks have been largely achieved because, surprisingly, <b>SGD</b> has been found adequate to train them. Here we present three works highlighting desirable ...", "dateLastCrawled": "2022-01-30T21:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Gradient</b> <b>Descent</b> for <b>Machine</b> <b>Learning</b> <b>Gradient</b> <b>Descent</b> for <b>Machine</b> <b>Learning</b>", "url": "http://www.bel.utcluj.ro/dce/didactic/eai/04_GradientDescent_ML.pdf", "isFamilyFriendly": true, "displayUrl": "www.bel.utcluj.ro/dce/didactic/eai/04_<b>GradientDescent</b>_ML.pdf", "snippet": "<b>Gradient</b> <b>Descent</b> for <b>Machine</b> <b>Learning</b> Elements of Artificial Intelligence G. Oltean BGD vs. <b>SGD</b> The summation part is important, especially with the concept of batch <b>gradient</b> <b>descent</b> (BGD) vs. <b>stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>). In Batch <b>Gradient</b> <b>Descent</b>, all the training data is taken into consideration to take a single step (one training epoch ...", "dateLastCrawled": "2022-02-03T10:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Adam, <b>Momentum and Stochastic Gradient Descent</b> - <b>Machine</b> <b>Learning</b> From ...", "url": "https://mlfromscratch.com/optimizers-explained/", "isFamilyFriendly": true, "displayUrl": "https://mlfromscratch.com/optimizers-explained", "snippet": "The basic difference between batch <b>gradient</b> <b>descent</b> (BGD) and <b>stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>), is that we only calculate the cost of one example for each step in <b>SGD</b>, but in BGD, we have to calculate the cost for all training examples in the dataset. Trivially, this speeds up neural networks greatly. Exactly this is the motivation behind <b>SGD</b>. The equation for <b>SGD</b> is used to update parameters in a neural network \u2013 we use the equation to update parameters in a backwards pass, using ...", "dateLastCrawled": "2022-02-02T18:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Stochastic</b> <b>Gradient</b> <b>Descent</b> using PyTorch | by Ashish Pandey | Geek ...", "url": "https://medium.com/geekculture/stochastic-gradient-descent-using-pytotch-bdd3ba5a3ae3", "isFamilyFriendly": true, "displayUrl": "https://medium.com/geekculture/<b>stochastic</b>-<b>gradient</b>-<b>descent</b>-using-pytotch-bdd3ba5a3ae3", "snippet": "Nearly all approaches start with the basic idea of multiplying the <b>gradient</b> by some small number, called the <b>learning</b> rate (LR). The <b>learning</b> rate is often a number between 0.001 and 0.1, although ...", "dateLastCrawled": "2022-01-29T14:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Gradient</b> <b>Descent</b>: One of <b>Machine</b> <b>Learning</b>\u2019s Most Popular Algorithms ...", "url": "https://urmiparekh.medium.com/gradient-descent-one-of-machine-learnings-most-popular-algorithms-c31963d1e67f", "isFamilyFriendly": true, "displayUrl": "https://urmiparekh.medium.com/<b>gradient</b>-<b>descent</b>-one-of-<b>machine</b>-<b>learning</b>s-most-popular...", "snippet": "<b>Gradient</b> <b>descent</b> is an optimization algorithm which is used to train a <b>machine</b> <b>learning</b> model. It is an optimization algorithm to find a local minimum of a differential function. It is used to find the values of a function\u2019s coefficients that minimize a cost function as much as possible. Source: Here. It i s a first-order iterative ...", "dateLastCrawled": "2022-01-17T15:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Batch, Mini Batch &amp; <b>Stochastic</b> <b>Gradient Descent</b> | by Sushant Patrikar ...", "url": "https://towardsdatascience.com/batch-mini-batch-stochastic-gradient-descent-7a62ecba642a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/batch-mini-batch-<b>stochastic</b>-<b>gradient-descent</b>-7a62ecba642a", "snippet": "In Batch <b>Gradient Descent</b> we were considering all the examples for every step of <b>Gradient Descent</b>. But what if our dataset is very huge. Deep <b>learning</b> models crave for data. The more the data the more chances of a model to be good. Suppose our dataset has 5 million examples, then just to take one step the model will have to calculate the gradients of all the 5 million examples. This does not seem an efficient way. To tackle this problem we have <b>Stochastic</b> <b>Gradient Descent</b>. In <b>Stochastic</b> ...", "dateLastCrawled": "2022-02-02T11:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Gradient Descent in Logistic Regression [Explained for Beginners</b> ...", "url": "https://www.upgrad.com/blog/gradient-descent-in-logistic-regression/", "isFamilyFriendly": true, "displayUrl": "https://www.upgrad.com/blog/<b>gradient-descent-in-logistic-regression</b>", "snippet": "It\u2019s massive, and hence there was a need for a slightly modified <b>Gradient</b> <b>Descent</b> Algorithm, namely \u2013 <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> Algorithm (<b>SGD</b>). The only difference <b>SGD</b> has with Normal <b>Gradient</b> <b>Descent</b> is that, in <b>SGD</b>, we don\u2019t deal with the entire training instance at a single time. In <b>SGD</b>, we compute the <b>gradient</b> of the cost function for just a single random example at each iteration. Now, doing so brings down the time taken for computations by a huge margin especially for large ...", "dateLastCrawled": "2022-01-28T16:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Gradient Descent With Momentum from Scratch</b> - <b>Machine</b> <b>Learning</b> Mastery", "url": "https://machinelearningmastery.com/gradient-descent-with-momentum-from-scratch/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>gradient-descent-with-momentum-from-scratch</b>", "snippet": "<b>Gradient</b> <b>descent</b> is an optimization algorithm that follows the negative <b>gradient</b> of an objective function in order to locate the minimum of the function. A problem with <b>gradient</b> <b>descent</b> is that it can bounce around the search space on optimization problems that have large amounts of curvature or noisy gradients, and it can get stuck in flat spots in the search", "dateLastCrawled": "2022-01-26T05:41:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Gossip <b>Learning</b> as a Decentralized Alternative to Federated <b>Learning</b>", "url": "http://publicatio.bibl.u-szeged.hu/15824/1/dais19a.pdf", "isFamilyFriendly": true, "displayUrl": "publicatio.bibl.u-szeged.hu/15824/1/dais19a.pdf", "snippet": "Federated <b>learning</b> is adistributed <b>machine</b> <b>learning</b> approach for computing models over data collected by edge devices. Most impor-tantly, the data itself is not collected centrally, but a master-worker ar-chitecture is applied where a master node performs aggregation and the edge devices are the workers, not unlike the parameter server approach. Gossip <b>learning</b> also assumes that the data remains at the edge devices, but it requires no aggregation server or any central component. In this ...", "dateLastCrawled": "2022-01-27T14:06:00.0000000Z", "language": "en", "isNavigational": false}], [], [], []], "all_bing_queries": ["+(stochastic gradient descent (sgd))  is like +(slowly walking downstairs)", "+(stochastic gradient descent (sgd)) is similar to +(slowly walking downstairs)", "+(stochastic gradient descent (sgd)) can be thought of as +(slowly walking downstairs)", "+(stochastic gradient descent (sgd)) can be compared to +(slowly walking downstairs)", "machine learning +(stochastic gradient descent (sgd) AND analogy)", "machine learning +(\"stochastic gradient descent (sgd) is like\")", "machine learning +(\"stochastic gradient descent (sgd) is similar\")", "machine learning +(\"just as stochastic gradient descent (sgd)\")", "machine learning +(\"stochastic gradient descent (sgd) can be thought of as\")", "machine learning +(\"stochastic gradient descent (sgd) can be compared to\")"]}