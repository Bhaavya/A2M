{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Fairness in Machine Learning</b> - <b>Science in the News</b>", "url": "https://sitn.hms.harvard.edu/uncategorized/2020/fairness-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://sitn.hms.harvard.edu/uncategorized/2020/<b>fairness</b>-<b>machine</b>-<b>learning</b>", "snippet": "<b>Machine</b> <b>learning</b> algorithms may seem <b>like</b> they should be objective, since decision-making is based entirely on the data. In a typical workflow, an <b>algorithm</b> is shown a large amount of representative data to learn from, and its decision-making process is refined by what it sees. However, any data we give the <b>algorithm</b> is describing, directly or not, the choices that have already been made in society. If black defendants are already falsely determined to be higher risk than white defendants ...", "dateLastCrawled": "2022-02-03T02:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A Tutorial on <b>Fairness</b> in <b>Machine</b> <b>Learning</b> | by Ziyuan Zhong | Towards ...", "url": "https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-tutorial-on-<b>fairness</b>-in-<b>machine</b>-<b>learning</b>-3ff8ba1040cb", "snippet": "1. Introduction. <b>F airness</b> is becomi n g one of the most popular topics in <b>machine</b> <b>learning</b> in recent years. Publications explode in this field (see Fig1). The research community has invested a large amount of effort in this field. At ICML 2018, two out of five best paper/runner-up award-winning papers are on <b>fairness</b>.", "dateLastCrawled": "2022-02-01T18:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "What is <b>Algorithm</b> <b>Fairness</b>?. An introduction to the field that aims ...", "url": "https://towardsdatascience.com/what-is-algorithm-fairness-3182e161cf9f", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/what-is-<b>algorithm</b>-<b>fairness</b>-3182e161cf9f", "snippet": "What is <b>algorithm</b> <b>fairness</b>? In <b>machine</b> <b>learning</b>, the terms <b>algorithm</b> and model are used interchangeably. To be precise, algorithms are mathematical functions <b>like</b> Linear Regression, Random Forests or Neural Networks. Models are algorithms that have been trained on data. Once trained, a model is used to make predictions which can help an automated computer system make decisions. These decisions can include anything from diagnosing a patient with cancer to accepting mortgage applications. No ...", "dateLastCrawled": "2022-02-02T19:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "SenSR: the <b>first practical algorithm for individual fairness</b> - MIT-IBM ...", "url": "https://mitibmwatsonailab.mit.edu/research/blog/training-individually-fair-ml-models-with-sensitive-subspace-robustness/", "isFamilyFriendly": true, "displayUrl": "https://mitibmwatsonailab.mit.edu/research/blog/training-<b>individual</b>ly-fair-ml-models...", "snippet": "Algorithmic <b>fairness</b> a sub-field of <b>Machine</b> <b>Learning</b> that studies the questions related to formalizing <b>fairness</b> in algorithms mathematically and developing techniques for training and auditing ML systems for bias and unfairness. In our paper, Training individually fair ML models with sensitive subspace robustness, published in ICLR 2020, w e consider training <b>machine</b> <b>learning</b> models that are fair in the sense that their performance is invariant under certain sensitive perturbations to the ...", "dateLastCrawled": "2022-01-15T07:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Understanding &#39;<b>fairness</b>&#39; in <b>machine</b> <b>learning</b> algorithms", "url": "https://www.cst.cam.ac.uk/understanding-fairness-machine-learning-algorithms", "isFamilyFriendly": true, "displayUrl": "https://www.cst.cam.ac.uk/understanding-<b>fairness</b>-<b>machine</b>-<b>learning</b>-<b>algorithms</b>", "snippet": "Her PhD research focuses on <b>fairness</b> in <b>machine</b> <b>learning</b> algorithms and their trade-offs on aggregate and <b>individual</b> levels. ... <b>Fairness</b> seems <b>like</b> an important notion as more and more decisions \u2013 from recommending new products to approving a loan to hiring a new employee \u2013 are informed by machines and models. &quot;There are some topics \u2013 such as data ethics \u2013 where it is really helpful having Cambridge input. This is such an important subject we need to have not just our own view but ...", "dateLastCrawled": "2021-12-11T13:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "What&#39;s Fair about <b>Individual</b> <b>Fairness</b>?", "url": "http://philsci-archive.pitt.edu/18889/1/Fleisher%20-%20Individual%20Fairness.pdf", "isFamilyFriendly": true, "displayUrl": "philsci-archive.pitt.edu/18889/1/Fleisher - <b>Individual</b> <b>Fairness</b>.pdf", "snippet": "<b>individual</b> <b>fairness</b> have argued that it gives the correct de\ufb01-nition of algorithmic <b>fairness</b>, and that it should therefore be preferred to other methods for determining <b>fairness</b>. I argue that <b>individual</b> <b>fairness</b> cannot serve as a de\ufb01nition of <b>fair-ness</b>. Moreover, IF methods should not be given priority over other <b>fairness</b> methods, nor used in isolation from them. To support these conclusions, I describe four in-principle prob-lems for <b>individual</b> <b>fairness</b> as a de\ufb01nition and as a method ...", "dateLastCrawled": "2022-01-21T10:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Algorithmic <b>Fairness</b> in <b>Machine</b> <b>Learning</b>", "url": "https://www2.cs.duke.edu/courses/spring19/compsci216/lectures/11-fairness.pdf", "isFamilyFriendly": true, "displayUrl": "https://www2.cs.duke.edu/courses/spring19/compsci216/lectures/11-<b>fairness</b>.pdf", "snippet": "Algorithmic <b>Fairness</b> in <b>Machine</b> <b>Learning</b> Thursday, April 4, 2019 CompSci216: Everything Data. Human Decision Making Data Jane likes Bollywood musicals. Decision Maker Bob Decision Bob: \u201cYou should watch Les Miserables, it\u2019s also a musical!\u201d Jane: \u201cNice try, Bob, but you clearly don\u2019t understand how to generalize from your prior experience.\u201d Suppose we want to recommend a movie. Human Decision Making Data Jane is a woman. Decision Maker Bob Decision Or even worse: Bob: \u201cI bet ...", "dateLastCrawled": "2022-01-21T01:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Individual</b> <b>Fairness</b> for k-Clustering - Proceedings of <b>Machine</b> <b>Learning</b> ...", "url": "http://proceedings.mlr.press/v119/mahabadi20a/mahabadi20a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v119/mahabadi20a/mahabadi20a.pdf", "snippet": "<b>Individual</b> <b>Fairness</b> for k-Clustering Sepideh Mahabadi* 1 Ali Vakilian* 2 Abstract We give a local search based <b>algorithm</b> for k- median and k-means (and more generally for any k-clustering with \u2018 pnorm cost function) from the perspective of <b>individual</b> <b>fairness</b>. More precisely, for a point xin a point set Pof size n, let r(x) be the minimum radius such that the ball of radius r(x) centered at xhas at least n=kpoints from P. Intuitively, if a set of krandom points are chosen from Pas centers ...", "dateLastCrawled": "2022-02-02T14:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Practical <b>Individual</b> <b>Fairness</b> Algorithms \u2013 Toronto <b>Machine</b> <b>Learning</b>", "url": "https://www.torontomachinelearning.com/events/practical-individual-fairness-algorithms/", "isFamilyFriendly": true, "displayUrl": "https://www.toronto<b>machinelearning</b>.com/events/practical-<b>individual</b>-<b>fairness</b>-<b>algorithms</b>", "snippet": "<b>Individual</b> <b>Fairness</b> (IF) is a very intuitive and desirable notion of <b>fairness</b>: we want ML models to treat similar individuals similarly, that is, to be fair for every person. For example, two resumes of individuals that only differ in their name and gender pronouns should be treated similarly by the model. Despite the intuition, training ML/AI models that abide by this rule in theory and in practice poses several challenges. In this talk, I will introduce a notion of Distributional ...", "dateLastCrawled": "2021-12-31T05:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Trade-Offs between <b>Fairness</b> and Privacy in <b>Machine</b> <b>Learning</b>", "url": "https://crcs.seas.harvard.edu/files/crcs/files/ai4sg-21_paper_23.pdf", "isFamilyFriendly": true, "displayUrl": "https://crcs.seas.harvard.edu/files/crcs/files/ai4sg-21_paper_23.pdf", "snippet": "The concerns of <b>fairness</b>, and privacy, in <b>machine</b> <b>learning</b> based systems have received a lot of atten-tion in the research community recently, but have primarily been studied in isolation. In this work, we look at cases where we want to satisfy both these properties simultaneously, and \ufb01nd that it may be necessary to make trade-offs between them. We prove a theoretical result to demonstrate this, which considers the issue of compatibility between <b>fair-ness</b> and differential privacy of ...", "dateLastCrawled": "2021-09-30T12:00:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Fairness in Machine Learning</b> - <b>Science in the News</b>", "url": "https://sitn.hms.harvard.edu/uncategorized/2020/fairness-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://sitn.hms.harvard.edu/uncategorized/2020/<b>fairness</b>-<b>machine</b>-<b>learning</b>", "snippet": "<b>Fairness</b> is difficult to pin down, and its exact definition is the subject of much contention among researchers. One simplistic way to think about it is that a fair <b>algorithm</b> will make <b>similar</b> decisions for <b>similar</b> individuals, or <b>similar</b> decisions regardless of what demographic an <b>individual</b> belongs to. This definition is vague, of course ...", "dateLastCrawled": "2022-02-03T02:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "What&#39;s Fair about <b>Individual</b> <b>Fairness</b>?", "url": "http://philsci-archive.pitt.edu/18889/1/Fleisher%20-%20Individual%20Fairness.pdf", "isFamilyFriendly": true, "displayUrl": "philsci-archive.pitt.edu/18889/1/Fleisher - <b>Individual</b> <b>Fairness</b>.pdf", "snippet": "gent topics as the use of AI and <b>machine</b> <b>learning</b> systems continues to spread through our society. As a result, research on algorithmic <b>fairness</b> has been expanding rapidly in the past few years. One paradigm in algorithmic <b>fairness</b> research is <b>individ-ual</b> <b>fairness</b> (IF). Proponents of IF suggest that the intuitive notion of <b>fairness</b> is expressed by the principle <b>similar</b> treat-ment: <b>similar</b> individuals should be treated similarly (Dwork et al. 2012). They attempt to capture this intuitive ...", "dateLastCrawled": "2022-01-21T10:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "SenSR: the <b>first practical algorithm for individual fairness</b> - MIT-IBM ...", "url": "https://mitibmwatsonailab.mit.edu/research/blog/training-individually-fair-ml-models-with-sensitive-subspace-robustness/", "isFamilyFriendly": true, "displayUrl": "https://mitibmwatsonailab.mit.edu/research/blog/training-<b>individual</b>ly-fair-ml-models...", "snippet": "Algorithmic <b>fairness</b> a sub-field of <b>Machine</b> <b>Learning</b> that studies the questions related to formalizing <b>fairness</b> in algorithms mathematically and developing techniques for training and auditing ML systems for bias and unfairness. In our paper, Training individually fair ML models with sensitive subspace robustness, published in ICLR 2020, w e consider training <b>machine</b> <b>learning</b> models that are fair in the sense that their performance is invariant under certain sensitive perturbations to the ...", "dateLastCrawled": "2022-01-15T07:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Verifying <b>Individual</b> <b>Fairness</b> in <b>Machine</b> <b>Learning</b> Models", "url": "http://proceedings.mlr.press/v124/george-john20a/george-john20a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v124/george-john20a/george-john20a.pdf", "snippet": "Verifying <b>Individual</b> <b>Fairness</b> in <b>Machine</b> <b>Learning</b> Models Philips George John, Deepak Vijaykeerthy, Diptikalyan Saha IBM Research AI Bengaluru 560 045, India Abstract We consider the problem of whether a given decision model, working with structured data, has <b>individual</b> <b>fairness</b>. Following the work of Dwork, a model is individually biased (or unfair) if there is a pair of valid inputs which are close to each other (according to an ap-propriate metric) but are treated differently by the model ...", "dateLastCrawled": "2022-02-03T15:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Practical <b>Individual</b> <b>Fairness</b> Algorithms \u2013 Toronto <b>Machine</b> <b>Learning</b>", "url": "https://www.torontomachinelearning.com/events/practical-individual-fairness-algorithms/", "isFamilyFriendly": true, "displayUrl": "https://www.toron<b>tomachinelearning</b>.com/events/practical-<b>individual</b>-<b>fairness</b>-<b>algorithms</b>", "snippet": "<b>Individual</b> <b>Fairness</b> (IF) is a very intuitive and desirable notion of <b>fairness</b>: we want ML models to treat <b>similar</b> individuals similarly, that is, to be fair for every person. For example, two resumes of individuals that only differ in their name and gender pronouns should be treated similarly by the model. Despite the intuition, training ML/AI models that abide by this rule in theory and in practice poses several challenges. In this talk, I will introduce a notion of Distributional ...", "dateLastCrawled": "2021-12-31T05:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A Tutorial on <b>Fairness</b> in <b>Machine</b> <b>Learning</b> | by Ziyuan Zhong | Towards ...", "url": "https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-tutorial-on-<b>fairness</b>-in-<b>machine</b>-<b>learning</b>-3ff8ba1040cb", "snippet": "The flaw <b>is similar</b> to the flaw of equality of opportunities: it may not help closing the gap between two groups. The reasoning <b>is similar</b> as before. 4.5 <b>Individual</b> <b>Fairness</b>. <b>Individual</b> <b>fairness</b> is a relatively different notion. The previous three criteria are all group-based while <b>individual</b> <b>fairness</b>, as its name suggests, is <b>individual</b>-based.", "dateLastCrawled": "2022-02-01T18:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Average <b>Individual</b> <b>Fairness</b>: Algorithms, Generalization and Experiments", "url": "https://proceedings.neurips.cc/paper/2019/file/0e1feae55e360ff05fef58199b3fa521-Paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/2019/file/0e1feae55e360ff05fef58199b3fa521-Paper.pdf", "snippet": "de\ufb01nitions of <b>fairness</b> with the strong <b>individual</b> level semantics of <b>individual</b> notions of <b>fairness</b>. One strand of this literature focuses on the \u201cmetric <b>fairness</b>\u201d de\ufb01nition \ufb01rst proposed by Dwork et al. (2012), and aims to ease the assumption that the <b>learning</b> <b>algorithm</b> has access to a task speci\ufb01c <b>fairness</b> metric. Kim et al ...", "dateLastCrawled": "2022-01-15T18:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "What is <b>Algorithm</b> <b>Fairness</b>?. An introduction to the field that aims ...", "url": "https://towardsdatascience.com/what-is-algorithm-fairness-3182e161cf9f", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/what-is-<b>algorithm</b>-<b>fairness</b>-3182e161cf9f", "snippet": "What is <b>algorithm</b> <b>fairness</b>? In <b>machine</b> <b>learning</b>, the terms <b>algorithm</b> and model are used interchangeably. To be precise, algorithms are mathematical functions like Linear Regression, Random Forests or Neural Networks. Models are algorithms that have been trained on data. Once trained, a model is used to make predictions which can help an automated computer system make decisions. These decisions can include anything from diagnosing a patient with cancer to accepting mortgage applications. No ...", "dateLastCrawled": "2022-02-02T19:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Machine</b> <b>Fairness</b>. How to assess AI system\u2019s <b>fairness</b> and\u2026 | by ...", "url": "https://medium.com/microsoftazure/machine-fairness-607acb8c0db6", "isFamilyFriendly": true, "displayUrl": "https://medium.com/microsoftazure/<b>machine</b>-<b>fairness</b>-607acb8c0db6", "snippet": "<b>Individual</b> <b>Fairness</b> \u2014 Give <b>similar</b> predictions to <b>similar</b> individuals. Group <b>Fairness</b> \u2014 Treat different groups equally. Subgroup <b>Fairness</b> \u2014 Subgroup <b>fairness</b> intends to obtain the best ...", "dateLastCrawled": "2022-01-29T00:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Procedural Justice in Algorithmic <b>Fairness</b>: Leveraging Transparency and ...", "url": "https://ischool.utexas.edu/~ml48959/materials/Publication/2019-CSCW-Al_ProceduralFairness.pdf", "isFamilyFriendly": true, "displayUrl": "https://ischool.utexas.edu/.../materials/Publication/2019-CSCW-Al_Procedural<b>Fairness</b>.pdf", "snippet": "<b>Individual</b> <b>fairness</b> techniques allow <b>machine</b> <b>learning</b> algorithms to learn to classify <b>similar</b> individuals similarly, depending on one\u2019s definition of <b>similar</b> individuals [22]. Group <b>fairness</b> research offers statistical techniques to treat protected groups similarly to the population as a whole [15]. Another set of validation techniques has been proposed to allow third parties to audit the process and examine whether the <b>machine</b> <b>learning</b> algorithms make decisions following <b>individual</b> or ...", "dateLastCrawled": "2022-01-14T01:01:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Verifying Individual Fairness in Machine Learning Models</b> | DeepAI", "url": "https://deepai.org/publication/verifying-individual-fairness-in-machine-learning-models", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>verifying-individual-fairness-in-machine-learning-models</b>", "snippet": "<b>Verifying Individual Fairness in Machine Learning Models</b>. 06/21/2020 . \u2219 . by Philips George John, et al. \u2219. ibm \u2219. 0 \u2219. share We consider the problem of whether a given decision model, working with structured data, has <b>individual</b> <b>fairness</b>. Following the work of Dwork, a model is individually biased (or unfair) if there is a pair of valid inputs which are close to each other (according to an appropriate metric) but are treated differently by the model (different class label, or large ...", "dateLastCrawled": "2021-12-16T16:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Fairness in Machine Learning: A Survey</b> | DeepAI", "url": "https://deepai.org/publication/fairness-in-machine-learning-a-survey", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>fairness-in-machine-learning-a-survey</b>", "snippet": "This article seeks to provide an overview of the different schools of <b>thought</b> and approaches to mitigating (social) biases and increase <b>fairness</b> in the <b>Machine</b> <b>Learning</b> literature. It organises approaches into the widely accepted framework of pre-processing, in-processing, and post-processing methods, subcategorizing into a further 11 method areas. Although much of the literature emphasizes binary classification, a discussion of <b>fairness</b> in regression, recommender systems,", "dateLastCrawled": "2022-01-18T13:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Differentiating Bias and <b>Fairness</b> in <b>Machine</b> <b>Learning</b>", "url": "https://www.closedloop.ai/post/bias-v-fairness-in-ml", "isFamilyFriendly": true, "displayUrl": "https://www.closedloop.ai/post/bias-v-<b>fairness</b>-in-ml", "snippet": "When an <b>individual</b> being judged by an <b>algorithm</b> would prefer a \u201cpositive\u201d label over the \u201ccorrect\u201d label, we refer to the decision as being polar. Most canonical examples, such as parol, bank loan, and job application decisions, are polar decisions. In the context of healthcare AI, most population health models fit this bill as well. Population health models tend to focus on identifying small subpopulations to assist with finite care management assistance. To contrast these types of ...", "dateLastCrawled": "2021-12-28T14:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "What does it mean to be fair? Measuring and understanding <b>fairness</b> | by ...", "url": "https://towardsdatascience.com/what-does-it-mean-to-be-fair-measuring-and-understanding-fairness-4ab873245c4c", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/what-does-it-mean-to-be-fair-measuring-and...", "snippet": "In this case, the <b>algorithm</b>\u2019s <b>individual</b> <b>fairness</b> was being brought into question. In this case, we would want to measure the approval <b>algorithm</b>\u2019s <b>fairness</b> by comparing a variety of men and women with similar profiles, not to see if the model is discriminating at the group level (women vs. men), but if it was specifically harming individuals with similar financial profiles (in this case spouses). This style of analysis, coined <b>individual</b> <b>fairness</b> by", "dateLastCrawled": "2022-02-01T09:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Algorithmic <b>Fairness</b> in <b>Machine</b> <b>Learning</b>", "url": "https://www2.cs.duke.edu/courses/spring19/compsci216/lectures/11-fairness.pdf", "isFamilyFriendly": true, "displayUrl": "https://www2.cs.duke.edu/courses/spring19/compsci216/lectures/11-<b>fairness</b>.pdf", "snippet": "Algorithmic <b>Fairness</b> in <b>Machine</b> <b>Learning</b> Thursday, April 4, 2019 CompSci216: Everything Data. Human Decision Making Data Jane likes Bollywood musicals. Decision Maker Bob Decision Bob: \u201cYou should watch Les Miserables, it\u2019s also a musical!\u201d Jane: \u201cNice try, Bob, but you clearly don\u2019t understand how to generalize from your prior experience.\u201d Suppose we want to recommend a movie. Human Decision Making Data Jane is a woman. Decision Maker Bob Decision Or even worse: Bob: \u201cI bet ...", "dateLastCrawled": "2022-01-21T01:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Practical <b>Individual</b> <b>Fairness</b> Algorithms \u2013 Toronto <b>Machine</b> <b>Learning</b>", "url": "https://www.torontomachinelearning.com/events/practical-individual-fairness-algorithms/", "isFamilyFriendly": true, "displayUrl": "https://www.toronto<b>machinelearning</b>.com/events/practical-<b>individual</b>-<b>fairness</b>-<b>algorithms</b>", "snippet": "<b>Individual</b> <b>Fairness</b> (IF) is a very intuitive and desirable notion of <b>fairness</b>: we want ML models to treat similar individuals similarly, that is, to be fair for every person. For example, two resumes of individuals that only differ in their name and gender pronouns should be treated similarly by the model. Despite the intuition, training ML/AI models that abide by this rule in theory and in practice poses several challenges. In this talk, I will introduce a notion of Distributional ...", "dateLastCrawled": "2021-12-31T05:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Fairness in Machine Learning: How</b> <b>Can</b> a <b>Model Trained on Aerial Imagery</b> ...", "url": "https://opendri.org/wp-content/uploads/2020/05/CatherineInness_-Fairness_in_ML_for_DRM.pdf", "isFamilyFriendly": true, "displayUrl": "https://opendri.org/wp-content/uploads/2020/05/CatherineInness_-<b>Fairness</b>_in_ML_for_DRM.pdf", "snippet": "One common definition of <b>fairness</b> in <b>machine</b> <b>learning</b> is the concept of equalized odds (Hardt, Price, &amp; Srebro ... population. For our disaster damage use case, false-negatives <b>can</b> <b>be thought</b> of as times where disaster damage is missed by the model, i.e. &#39;invisible&#39; to the humans reviewing the output of the model and therefore ignored in decision-making on allocation of protective measures. If the model misses damage more often for one subgroup of the population than for others, it would ...", "dateLastCrawled": "2021-12-05T09:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A <b>comparative study of fairness-enhancing interventions in machine</b> <b>learning</b>", "url": "http://sorelle.friedler.net/papers/fairness_comparison_fat19.pdf", "isFamilyFriendly": true, "displayUrl": "sorelle.friedler.net/papers/<b>fairness</b>_comparison_fat19.pdf", "snippet": "of any <b>machine</b> <b>learning</b> <b>algorithm</b> applied to that data will be fair, those <b>algorithm</b> modificationtechniques that modify an existing <b>algorithm</b> or create a new one that will be fair under any inputs, and those postprocessing techniques that take the output of any model and modify that output to be fair [28]. Many associated met-rics for measuring <b>fairness</b> in algorithms have also been explored. These are detailed further in Section 6 and are also surveyed in [31]. This description of <b>fairness</b> ...", "dateLastCrawled": "2022-01-15T17:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) <b>Can</b> Everyday AI be <b>Ethical? Machine Learning Algorithm Fairness</b> ...", "url": "https://www.researchgate.net/publication/329277474_Can_Everyday_AI_be_Ethical_Machine_Learning_Algorithm_Fairness_english_version", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/329277474_<b>Can</b>_Everyday_AI_be_Ethical_<b>Machine</b>...", "snippet": "<b>Machine Learning Algorithm Fairness (english version</b>) ... a theory makes it possible to produce a m odel origina ting from human <b>thought</b>. Then, the model is. put into action and prese nted with ...", "dateLastCrawled": "2021-10-16T01:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Human <b>Judgment in algorithmic loops: Individual justice and</b> automated ...", "url": "https://onlinelibrary.wiley.com/doi/full/10.1111/rego.12358", "isFamilyFriendly": true, "displayUrl": "https://onlinelibrary.wiley.com/doi/full/10.1111/rego.12358", "snippet": "<b>Individual</b> justice will often conflict with <b>algorithm</b>-driven consistency and <b>fairness</b>, but conversely algorithmic systems are incapable of respecting <b>individual</b> justice. To complicate matters further, the screen-level bureaucrats dealing with algorithmic systems on the ground will exercise discretion according to their own commitments in ways that may be at odds with the goals of the organization deploying them. The potential for unequal application of <b>individual</b> justice raises additional ...", "dateLastCrawled": "2021-11-23T10:31:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Verifying <b>Individual</b> <b>Fairness</b> in <b>Machine</b> <b>Learning</b> Models", "url": "http://proceedings.mlr.press/v124/george-john20a/george-john20a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v124/george-john20a/george-john20a.pdf", "snippet": "The challenges of verifying <b>individual</b> <b>fairness</b>, when <b>compared</b> to the existing work on ver-ifying <b>machine</b> <b>learning</b> models, are two-fold: Firstly, the existing work on verifying bias/<b>fairness</b> in <b>machine</b> <b>learning</b> models considers notions of group <b>fairness</b>/bias (Albarghouthi et al. 2017; Bastani et al. 2019). An <b>individual</b> <b>fairness</b> property considers the worst case (<b>fairness</b> for all similar input pairs, biased if there exists a bad input pair), rather than the average case (with high ...", "dateLastCrawled": "2022-02-03T15:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Verifying Individual Fairness in Machine Learning Models</b> | DeepAI", "url": "https://deepai.org/publication/verifying-individual-fairness-in-machine-learning-models", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>verifying-individual-fairness-in-machine-learning-models</b>", "snippet": "The challenges of verifying <b>individual</b> <b>fairness</b>, when <b>compared</b> to the existing work on verifying <b>machine</b> <b>learning</b> models, are two-fold: ... We give a meta-<b>algorithm</b>/framework for solving the verification problem, as well as particular algorithms for linear classifiers, and kernelized classifiers with polynomial/rbf kernels. Our algorithms are sound but incomplete (see section 4.1), with the linear classifier case being an exception in that it is exact (both sound and complete) if we allow ...", "dateLastCrawled": "2021-12-16T16:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Bias and Fairness in Machine Learning</b> - Abhishek Tiwari", "url": "https://www.abhishek-tiwari.com/bias-and-fairness-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.abhishek-tiwari.com/<b>bias-and-fairness-in-machine-learning</b>", "snippet": "For instance, it <b>can</b> be fair to use criminal history of an <b>individual</b> as an input feature in the decision-making process, but unfair to use family criminal history of the <b>individual</b> in the question. Obviously, removing an undesirable feature improves process <b>fairness</b>, it may also lead to reduced accuracy or lower outcome <b>fairness</b> [4] - a little cost to pay when humanity is at the stake.", "dateLastCrawled": "2022-01-29T16:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Machine</b> <b>learning</b> and algorithmic <b>fairness</b> in public and population ...", "url": "https://www.nature.com/articles/s42256-021-00373-4", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s42256-021-00373-4", "snippet": "Algorithmic <b>fairness</b> has recently emerged as a field of <b>machine</b> <b>learning</b>, with the goal of mitigating differences in <b>machine</b> <b>learning</b> outcomes across social groups. Broadly, algorithmic <b>fairness</b> ...", "dateLastCrawled": "2022-01-30T16:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "What does it mean to be fair? Measuring and understanding <b>fairness</b> | by ...", "url": "https://towardsdatascience.com/what-does-it-mean-to-be-fair-measuring-and-understanding-fairness-4ab873245c4c", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/what-does-it-mean-to-be-fair-measuring-and...", "snippet": "In this case, the <b>algorithm</b>\u2019s <b>individual</b> <b>fairness</b> was being brought into question. In this case, we would want to measure the approval <b>algorithm</b>\u2019s <b>fairness</b> by comparing a variety of men and women with similar profiles, not to see if the model is discriminating at the group level (women vs. men), but if it was specifically harming individuals with similar financial profiles (in this case spouses). This style of analysis, coined <b>individual</b> <b>fairness</b> by", "dateLastCrawled": "2022-02-01T09:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Practical <b>Individual</b> <b>Fairness</b> Algorithms \u2013 Toronto <b>Machine</b> <b>Learning</b>", "url": "https://www.torontomachinelearning.com/events/practical-individual-fairness-algorithms/", "isFamilyFriendly": true, "displayUrl": "https://www.toron<b>tomachinelearning</b>.com/events/practical-<b>individual</b>-<b>fairness</b>-<b>algorithms</b>", "snippet": "<b>Individual</b> <b>Fairness</b> (IF) is a very intuitive and desirable notion of <b>fairness</b>: we want ML models to treat similar individuals similarly, that is, to be fair for every person. For example, two resumes of individuals that only differ in their name and gender pronouns should be treated similarly by the model. Despite the intuition, training ML/AI models that abide by this rule in theory and in practice poses several challenges. In this talk, I will introduce a notion of Distributional ...", "dateLastCrawled": "2021-12-31T05:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Programming \u2018<b>Fairness</b>\u2019 into Your <b>Machine</b> <b>Learning</b> Model | by Almira ...", "url": "https://medium.com/sogetiblogsnl/programming-fairness-into-your-machine-learning-model-a3a5479bfe41", "isFamilyFriendly": true, "displayUrl": "https://medium.com/sogetiblogsnl/programming-<b>fairness</b>-into-your-<b>machine</b>-<b>learning</b>-model...", "snippet": "Programming \u2018<b>Fairness</b>\u2019 into Your <b>Machine</b> <b>Learning</b> Model . Almira Pillay. Follow. Jul 30 \u00b7 6 min read \u201cThe amount of work we <b>can</b> automate with AI is vastly larger than before. As leaders, it ...", "dateLastCrawled": "2021-12-09T00:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Frontiers | Addressing <b>Fairness</b>, Bias, and Appropriate Use of ...", "url": "https://www.frontiersin.org/articles/10.3389/frai.2020.561802/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/frai.2020.561802", "snippet": "The following sections provide an overview of different ways <b>fairness</b> <b>can</b> be defined and measured, to enable the proper tuning of a <b>machine</b> <b>learning</b> <b>algorithm</b>. <b>Individual</b> vs. Group <b>Fairness</b>. Given the dictionary definition of <b>fairness</b> (impartial and just treatment), we <b>can</b> consider <b>fairness</b> at the level of an <b>individual</b> or a group. We <b>can</b> ask ...", "dateLastCrawled": "2022-01-29T07:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A Survey on <b>Bias and Fairness in Machine Learning</b>", "url": "https://www.cs.purdue.edu/homes/bb/2020-fall-cs590bb/docs/Survery_of_Bias_and_Fairness_in_ML.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.purdue.edu/homes/bb/2020-fall-cs590bb/docs/Survery_of_Bias_and_<b>Fairness</b>...", "snippet": "A Survey on <b>Bias and Fairness in Machine Learning</b> NINAREH MEHRABI, FRED MORSTATTER, NRIPSUTA SAXENA, KRISTINA LERMAN, and ARAM GALSTYAN, USC-ISI With the widespread use of AI systems and applications in our everyday lives, it is important to take <b>fairness</b> issues into consideration while designing and engineering these types of systems. Such systems <b>can</b> be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that the decisions do not ...", "dateLastCrawled": "2022-02-03T03:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "The <b>Frontiers of Fairness in Machine Learning</b> | DeepAI", "url": "https://deepai.org/publication/the-frontiers-of-fairness-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/the-<b>frontiers-of-fairness-in-machine-learning</b>", "snippet": "The last few years have seen an explosion of academic and popular interest in algorithmic <b>fairness</b>. Despite this interest and the volume and velocity of work that has been produced recently, the fundamental science <b>of fairness in machine learning</b> is still in a nascent state. In March 2018, we convened a group of experts as part of a CCC visioning workshop to assess the state of the field, and distill the most promising research directions going forward.", "dateLastCrawled": "2022-01-19T04:12:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Towards Analogy-Based Explanations in Machine Learning</b> | DeepAI", "url": "https://deepai.org/publication/towards-analogy-based-explanations-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>towards-analogy-based-explanations-in-machine-learning</b>", "snippet": "More specifically, we take the view that an <b>analogy</b>-based approach is a viable alternative to existing approaches in the realm of explainable AI and interpretable <b>machine</b> <b>learning</b>, and that <b>analogy</b>-based explanations of the predictions produced by a <b>machine</b> <b>learning</b> algorithm can complement similarity-based explanations in a meaningful way. To corroborate these claims, we outline the basic idea of an <b>analogy</b>-based explanation and illustrate its potential usefulness by means of some examples.", "dateLastCrawled": "2022-01-10T12:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Practical <b>Individual</b> <b>Fairness</b> Algorithms \u2013 Toronto <b>Machine</b> <b>Learning</b>", "url": "https://www.torontomachinelearning.com/events/practical-individual-fairness-algorithms/", "isFamilyFriendly": true, "displayUrl": "https://www.toronto<b>machinelearning</b>.com/events/practical-<b>individual</b>-<b>fairness</b>-algorithms", "snippet": "<b>Individual</b> <b>Fairness</b> (IF) is a very intuitive and desirable notion of <b>fairness</b>: we want ML models to treat similar individuals similarly, that is, to be fair for every person. For example, two resumes of individuals that only differ in their name and gender pronouns should be treated similarly by the model. Despite the intuition, training ML/AI models that abide by this rule in theory and in practice poses several challenges. In this talk, I will introduce a notion of Distributional ...", "dateLastCrawled": "2021-12-31T05:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Finding the <b>fairness</b> in ethical <b>machine</b> <b>learning</b> - Taylor Fry", "url": "https://taylorfry.nz/articles/finding-the-fairness-in-ethical-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://taylorfry.nz/articles/finding-the-<b>fairness</b>-in-ethical-<b>machine</b>-<b>learning</b>", "snippet": "Likewise, <b>machine</b> <b>learning</b> infrastructure is also missing, specifically, regulating the use of <b>machine</b> <b>learning</b> to ensure it\u2019s used in an ethical and beneficial way, rather than used by a small number for their advantage at the significant disadvantage of the majority. It\u2019s important we all continue to have this conversation together, and take action at <b>individual</b>, organisational, governmental and global levels to bring about a future where AI is used to help not hinder.", "dateLastCrawled": "2022-01-07T11:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Residual Unfairness in Fair <b>Machine</b> <b>Learning</b> from Prejudiced Data", "url": "http://proceedings.mlr.press/v80/kallus18a/kallus18a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v80/kallus18a/kallus18a.pdf", "snippet": "Recent work on <b>fairness</b> in <b>machine</b> <b>learning</b> proposes and analyzes competing criteria for assessing the <b>fairness</b> of <b>ma-chine</b> <b>learning</b> algorithms, where some adjustments attempt to equalize accuracy metrics across groups (Corbett-Davies etal.,2017;Kleinbergetal.,2017;Hardtetal.,2016). Other work studies how historical prejudices may be re\ufb02ected in training data such that algorithmic systems might replicate historical biases (Angwin et al., 2016; Lum &amp; Isaac, 2016; Kilbertus et al., 2017). We ...", "dateLastCrawled": "2022-01-31T07:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "RStudio AI Blog: Starting to think about AI <b>Fairness</b>", "url": "https://blogs.rstudio.com/ai/posts/2021-07-15-ai-fairness/", "isFamilyFriendly": true, "displayUrl": "https://blogs.rstudio.com/ai/posts/2021-07-15-ai-<b>fairness</b>", "snippet": "Papers on <b>fairness</b> in <b>machine</b> <b>learning</b>, as is common in fields like computer science, abound with formulae. Even the papers referenced here, though selected not for their theorems and proofs but for the ideas they harbor, are no exception. But to start thinking about <b>fairness</b> as it might apply to an ML process at hand, common language \u2013 and common sense \u2013 will do just fine. If, after analyzing your use case, you judge that the more technical results are relevant to the process in ...", "dateLastCrawled": "2022-01-31T09:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "How to <b>Create Unbiased ML Models | Deepchecks</b>", "url": "https://deepchecks.com/how-to-create-unbiased-ml-models/", "isFamilyFriendly": true, "displayUrl": "https://deepchecks.com/how-to-create-unbiased-ml-models", "snippet": "\u201cIn <b>machine</b> <b>learning</b>, a given algorithm is said to be fair, or to have <b>fairness</b>, if its results are independent of given variables, especially those considered sensitive, such as the traits of individuals which should not correlate with the outcome (i.e. gender, ethnicity, sexual orientation, disability, etc.).\u201d \u201c<b>Fairness</b>\u201d, Wikipedia", "dateLastCrawled": "2022-01-13T06:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine</b> <b>Learning</b> and Theological Traditions of <b>Analogy</b> - Davison - 2021 ...", "url": "https://onlinelibrary.wiley.com/doi/full/10.1111/moth.12682", "isFamilyFriendly": true, "displayUrl": "https://onlinelibrary.wiley.com/doi/full/10.1111/moth.12682", "snippet": "12 <b>Machine</b> <b>Learning</b>, <b>Analogy</b>, and God. The texts considered in this article come from theological sources. They have offered ways to think analogically about features of the world, in this case the similarities we are beginning to see between capacities in <b>machine</b> <b>learning</b> and those in human beings and other animals. Much of the mediaeval ...", "dateLastCrawled": "2021-04-16T10:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Fairness</b> Through Awareness", "url": "https://www.cs.toronto.edu/~zemel/documents/fairAwareItcs2012.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.toronto.edu/~zemel/documents/fairAwareItcs2012.pdf", "snippet": "this <b>individual</b>-based <b>fairness</b>, we assume a distance metric that de\ufb01nes the similarity between the individuals. This is the source of \u201cawareness\u201d in the title of this paper. We formalize this guiding principle as a Lipschitz condition on the classi\ufb01er. In our approach a classi\ufb01er is a randomized mapping from individuals to outcomes, or equivalently, a mapping from individuals to distributions over outcomes. The Lipschitz condition requires that any two individuals x;ythat are at ...", "dateLastCrawled": "2022-01-29T03:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "An evaluation of scanpath-comparison and <b>machine</b>-<b>learning</b> ...", "url": "https://link.springer.com/article/10.3758/s13428-016-0788-z", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.3758/s13428-016-0788-z", "snippet": "The bottom line is that scanpath-comparison algorithms and the <b>machine</b>-<b>learning</b> techniques that accompany them are powerful tools to study the dynamics of <b>analogy</b> making. In building models of <b>analogy</b> making, we want to know what the models predict and how they make those predictions. Although the tools presented in this article are more involved with prediction than with explanation, the two are hardly unrelated, especially when we know the bases of the predictions. Our overarching goal has ...", "dateLastCrawled": "2021-11-05T03:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "The UX of AI Part I: Ants in Your Pants or Ants in Your Brain? | by zac ...", "url": "https://medium.com/@ZacTaschdjian/the-ux-of-ai-part-i-ants-in-your-pants-or-ants-in-your-brain-3cfef7990e7a", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@ZacTaschdjian/the-ux-of-ai-part-i-ants-in-your-pants-or-ants-in...", "snippet": "The UX of AI Part II: Inscrutability in Deep Reinforcement <b>Learning</b> Neural Networks. Paper 1 is \u201cTransparency and Explanation in Deep Reinforcement <b>Learning</b> Neural Networks\u201d(Iyar, et. al. 2018 ...", "dateLastCrawled": "2022-01-22T22:46:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Making Fair ML Software using Trustworthy Explanation", "url": "https://www.researchgate.net/profile/Kewen-Peng-4/publication/342733939_Making_Fair_ML_Software_using_Trustworthy_Explanation/links/5fe0ddcea6fdccdcb8ef5a11/Making-Fair-ML-Software-using-Trustworthy-Explanation.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/profile/Kewen-Peng-4/publication/342733939_Making_Fair_ML...", "snippet": "<b>Machine</b> <b>learning</b> software is being used in many applications (fi-nance, hiring, admissions, criminal justice) having huge social im-pact. But sometimes the behavior of this software is biased and ...", "dateLastCrawled": "2021-09-29T17:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Making Fair <b>ML Software using Trustworthy Explanation</b> | DeepAI", "url": "https://deepai.org/publication/making-fair-ml-software-using-trustworthy-explanation", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/making-fair-<b>ml-software-using-trustworthy-explanation</b>", "snippet": "<b>Machine</b> <b>learning</b> software is being used in many applications (finance, hiring, admissions, criminal justice) having a huge social impact. But sometimes the behavior of this software is biased and it shows discrimination based on some sensitive attributes such as sex, race, etc. Prior works concentrated on finding and mitigating bias in ML models.", "dateLastCrawled": "2022-01-24T00:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) Making fair ML software using trustworthy explanation", "url": "https://www.researchgate.net/publication/348827111_Making_fair_ML_software_using_trustworthy_explanation", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/348827111_Making_fair_ML_software_using...", "snippet": "PDF | On Dec 21, 2020, Joymallya Chakraborty and others published Making fair ML software using trustworthy explanation | Find, read and cite all the research you need on ResearchGate", "dateLastCrawled": "2021-11-13T01:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Service-Oriented Computing: 18th International Conference, ICSOC 2020 ...", "url": "https://dokumen.pub/service-oriented-computing-18th-international-conference-icsoc-2020-dubai-united-arab-emirates-december-1417-2020-proceedings-1st-ed-9783030653095-9783030653101.html", "isFamilyFriendly": true, "displayUrl": "https://<b>dokumen.pub</b>/service-oriented-computing-18th-international-conference-icsoc...", "snippet": "In a Fog environment, a Kubernetes Node is a worker <b>machine</b>, and it may be a virtual <b>machine</b> or a physical <b>machine</b> that corresponds to a node, a.k.a., Fog node. A set of Kubernetes Nodes makes up a Kubernetes cluster. A Kubernetes cluster corresponds to a set of fog nodes. Each microservice can be containerized and, therefore, it belongs to a single Docker container. A Kubernetes Pod is a group of containers with shared network and storage, that are always coscheduled and co-located. Finally ...", "dateLastCrawled": "2021-12-24T17:49:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Counterfactual Fairness: Unidentification, Bound and Algorithm ...", "url": "https://www.researchgate.net/publication/334843895_Counterfactual_Fairness_Unidentification_Bound_and_Algorithm", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/334843895_Counterfactual_Fairness_Un...", "snippet": "Fairness in <b>machine</b> <b>learning</b> has been a research subject with rapid growth recently. Many different definitions of fairness have been designed to fit different settings, e.g., equality of ...", "dateLastCrawled": "2021-12-23T12:05:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Putting <b>Fairness Principles into Practice: Challenges, Metrics</b>, and ...", "url": "https://deepai.org/publication/putting-fairness-principles-into-practice-challenges-metrics-and-improvements", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/putting-<b>fairness-principles-into-practice-challenges</b>...", "snippet": "By almost every measure, there has been an explosion in attention and research on <b>machine</b> <b>learning</b> fairness: there is a quickly growing amount of research on how to define, measure, and address <b>machine</b> <b>learning</b> fairness, and products are evaluated with these concerns in mind. Despite this significant attention, there has been much less published work detailing how fairness concerns are measured and addressed by product teams in industry. In this paper, we hope to shed light on the challenges ...", "dateLastCrawled": "2022-01-23T04:12:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(individual fairness)  is like +(machine learning algorithm)", "+(individual fairness) is similar to +(machine learning algorithm)", "+(individual fairness) can be thought of as +(machine learning algorithm)", "+(individual fairness) can be compared to +(machine learning algorithm)", "machine learning +(individual fairness AND analogy)", "machine learning +(\"individual fairness is like\")", "machine learning +(\"individual fairness is similar\")", "machine learning +(\"just as individual fairness\")", "machine learning +(\"individual fairness can be thought of as\")", "machine learning +(\"individual fairness can be compared to\")"]}