{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "On Consequentialism and <b>Fairness</b>", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7861221/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7861221", "snippet": "Note that whereas demographic parity only <b>requires</b> the set of predictions (\u0176) made for <b>all</b> <b>individuals</b> <b>in a dataset</b>, equal opportunity and equal calibration also require that we know the true outcome (Y) for <b>all</b> such <b>individuals</b>, even those who are given a negative prediction. As a result, the latter two requirements can only be properly verified on a <b>dataset</b> for which we can independently observe the true outcome (e.g., based on assigning treatment randomly).", "dateLastCrawled": "2022-01-28T22:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Tuning <b>Fairness</b> by Balancing Target Labels", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7861271/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7861271", "snippet": "We have empirically observed that <b>all</b> <b>fairness</b> <b>algorithms</b> benefit from this balancing of the test set. The situation is different for equality of opportunity. A perfect classifier automatically satisfies equality of opportunity on any <b>dataset</b>. Thus, an algorithm aiming for this <b>fairness</b> <b>constraint</b> should not <b>treat</b> the <b>dataset</b> as defective. Consequently, for evaluating equality of opportunity we perform no balancing of the test set. 5.4. Method. We evaluate two versions of our target label ...", "dateLastCrawled": "2021-08-24T23:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Affirmative</b> <b>Algorithms</b>: The Legal Grounds for <b>Fairness</b> as Awareness ...", "url": "https://lawreviewblog.uchicago.edu/2020/10/30/aa-ho-xiang/", "isFamilyFriendly": true, "displayUrl": "https://lawreviewblog.uchicago.edu/2020/10/30/aa-ho-xiang", "snippet": "Authors are listed alphabetically and have <b>equally</b> contributed to this work. * * * A part of the series, <b>Affirmative</b> Action at a Crossroads. Abstract. While there has been a flurry of research in algorithmic <b>fairness</b>, what is less recognized is that modern antidiscrimination law may prohibit the adoption of such techniques. We make three contributions. First, we discuss how such approaches will likely be deemed \u201calgorithmic <b>affirmative</b> action,\u201d posing serious legal risks of violating ...", "dateLastCrawled": "2022-01-31T02:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Algorithmic Fairness: Choices, Assumptions, and Definitions</b> | Annual ...", "url": "https://www.annualreviews.org/doi/10.1146/annurev-statistics-042720-125902", "isFamilyFriendly": true, "displayUrl": "https://www.annualreviews.org/doi/10.1146/annurev-statistics-042720-125902", "snippet": "Mathematical definitions of <b>fairness</b> generally <b>treat</b> the statistical learning problem that is used to formulate a predictive model as external to the <b>fairness</b> evaluation. Here, too, there are a number of choices that can have larger social implications but go unmeasured by the <b>fairness</b> evaluation. We focus specifically on the choices of training data, model, and predictive evaluation. 2.2.1. The data. The foundation of a predictive model is the data on which it is trained. The data available ...", "dateLastCrawled": "2022-01-21T17:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Fairness in Machine Learning: A Survey</b> | DeepAI", "url": "https://deepai.org/publication/fairness-in-machine-learning-a-survey", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>fairness-in-machine-learning-a-survey</b>", "snippet": "Key challenges for regularization approaches are: 1) they are often non-convex in nature or achieve convexity at the cost of probabilistic interpretation [goel2018non]; 2) not <b>all</b> <b>fairness</b> measures are <b>equally</b> affected by the strength of regularization parameters [di2020counterfactual, berk2017]; and 3) different regularization terms and penalties have diverse results on different data sets, i.e. this choice can have qualitative effects on the trade-off between accuracy and <b>fairness</b> [berk2017].", "dateLastCrawled": "2022-01-18T13:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Classification - <b>Fairness</b> and machine learning", "url": "https://fairmlbook.org/classification.html", "isFamilyFriendly": true, "displayUrl": "https://fairmlbook.org/classification.html", "snippet": "The condition refers to the group of <b>all</b> <b>individuals</b> receiving a particular score value. It does not mean that at the level of a single individual a score of r corresponds to a probability r of a positive outcome. The latter is a much stronger property that is satisfied by the conditional expectation R=\\mathbb{E}[Y\\mid X].", "dateLastCrawled": "2022-02-03T07:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Empirical Risk Minimization Under <b>Fairness</b> Constraints | Request PDF", "url": "https://www.researchgate.net/publication/333044604_Empirical_Risk_Minimization_Under_Fairness_Constraints", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/333044604_Empirical_Risk_Minimization_Under...", "snippet": "The <b>fairness</b> constraints need to be predefined according to various statistical <b>fairness</b> criteria, such as equality opportunity [13], equalized odds [13] and demographic parity notion <b>like</b> p%-<b>rule</b> ...", "dateLastCrawled": "2021-12-20T07:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A Survey on Bias and <b>Fairness in Machine Learning</b> | DeepAI", "url": "https://deepai.org/publication/a-survey-on-bias-and-fairness-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/a-survey-on-bias-and-<b>fairness-in-machine-learning</b>", "snippet": "However, <b>like</b> people, <b>algorithms</b> are vulnerable to biases that render their decisions \u201cunfair ... Group <b>Fairness</b>. <b>Treat</b> different groups <b>equally</b> (NIPS2017_6995; Dwork:2012:FTA:2090236.2090255). Subgroup <b>Fairness</b>. Subgroup <b>fairness</b> intends to obtain the best properties of the group and individual notions of <b>fairness</b>. It is different than these notions but uses them in order to obtain better outcomes. It picks a group <b>fairness</b> <b>constraint</b> <b>like</b> equalizing false positive and asks that this ...", "dateLastCrawled": "2022-01-22T23:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Two-stage Algorithm for <b>Fairness</b>-aware Machine Learning", "url": "https://www.researchgate.net/publication/320413534_Two-stage_Algorithm_for_Fairness-aware_Machine_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/320413534_Two-stage_Algorithm_for_<b>Fairness</b>...", "snippet": "PDF | Algorithmic decision making process now affects many aspects of our lives. Standard tools for machine learning, such as classification and... | Find, read and cite <b>all</b> the research you need ...", "dateLastCrawled": "2021-12-12T20:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Fair <b>Algorithms</b> for Learning in Allocation Problems - ResearchGate", "url": "https://www.researchgate.net/publication/330264942_Fair_Algorithms_for_Learning_in_Allocation_Problems", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/330264942_Fair_<b>Algorithms</b>_for_Learning_in...", "snippet": "Lulu Kang. Nazanin Nezami. We aim to design a <b>fairness</b>-aware allocation approach to maximize the geographical diversity and avoid unfairness in the sense of demographic disparity. During the ...", "dateLastCrawled": "2022-01-27T04:14:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "On Consequentialism and <b>Fairness</b>", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7861221/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7861221", "snippet": "As the name suggests, <b>rule</b> consequentialism <b>is similar</b> to act consequentialism, ... Note that whereas demographic parity only <b>requires</b> the set of predictions (\u0176) made for <b>all</b> <b>individuals</b> <b>in a dataset</b>, equal opportunity and equal calibration also require that we know the true outcome (Y) for <b>all</b> such <b>individuals</b>, even those who are given a negative prediction. As a result, the latter two requirements can only be properly verified on a <b>dataset</b> for which we can independently observe the true ...", "dateLastCrawled": "2022-01-28T22:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Learning Optimal Fair Classification Trees | DeepAI", "url": "https://deepai.org/publication/learning-optimal-fair-classification-trees", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/learning-optimal-fair-classification-trees", "snippet": "This is in contrast to \u201cindividual\u201d <b>fairness</b>, which <b>requires</b> that <b>individuals</b> with <b>similar</b> characteristics be classified similarly (Dwork et al., 2012). Both perspectives have their advantages and drawbacks, but in this work we will focus on group <b>fairness</b> primarily because of its simplicity and interpreability. In the following sections, we will discuss the use of many notions of group <b>fairness</b> that can sufficiently satisfy more nuanced and fine-grained <b>fairness</b> considerations. 1.1 ...", "dateLastCrawled": "2022-01-26T22:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Tuning <b>Fairness</b> by Balancing Target Labels", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7861271/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7861271", "snippet": "This prior knowledge can encode the semantics that \u201c<b>similar</b> <b>individuals</b> should be treated similarly\u201d (Dwork et al., ... We have empirically observed that <b>all</b> <b>fairness</b> <b>algorithms</b> benefit from this balancing of the test set. The situation is different for equality of opportunity. A perfect classifier automatically satisfies equality of opportunity on any <b>dataset</b>. Thus, an algorithm aiming for this <b>fairness</b> <b>constraint</b> should not <b>treat</b> the <b>dataset</b> as defective. Consequently, for evaluating ...", "dateLastCrawled": "2021-08-24T23:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Classification - <b>Fairness</b> and machine learning", "url": "https://fairmlbook.org/classification.html", "isFamilyFriendly": true, "displayUrl": "https://fairmlbook.org/classification.html", "snippet": "The condition refers to the group of <b>all</b> <b>individuals</b> receiving a particular score value. It does not mean that at the level of a single individual a score of r corresponds to a probability r of a positive outcome. The latter is a much stronger property that is satisfied by the conditional expectation R=\\mathbb{E}[Y\\mid X].", "dateLastCrawled": "2022-02-03T07:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Empirical Risk Minimization Under <b>Fairness</b> Constraints | Request PDF", "url": "https://www.researchgate.net/publication/333044604_Empirical_Risk_Minimization_Under_Fairness_Constraints", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/333044604_Empirical_Risk_Minimization_Under...", "snippet": "<b>Similar</b> to other machine learning <b>algorithms</b>, VFL suffers from <b>fairness</b> issues, i.e., the learned model may be unfairly discriminatory over the group with sensitive attributes. To tackle this ...", "dateLastCrawled": "2021-12-20T07:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Affirmative</b> <b>Algorithms</b>: The Legal Grounds for <b>Fairness</b> as Awareness ...", "url": "https://lawreviewblog.uchicago.edu/2020/10/30/aa-ho-xiang/", "isFamilyFriendly": true, "displayUrl": "https://lawreviewblog.uchicago.edu/2020/10/30/aa-ho-xiang", "snippet": "Authors are listed alphabetically and have <b>equally</b> contributed to this work. * * * A part of the series, <b>Affirmative</b> Action at a Crossroads. Abstract. While there has been a flurry of research in algorithmic <b>fairness</b>, what is less recognized is that modern antidiscrimination law may prohibit the adoption of such techniques. We make three contributions. First, we discuss how such approaches will likely be deemed \u201calgorithmic <b>affirmative</b> action,\u201d posing serious legal risks of violating ...", "dateLastCrawled": "2022-01-31T02:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Algorithmic Fairness: Choices, Assumptions, and Definitions</b> | Annual ...", "url": "https://www.annualreviews.org/doi/10.1146/annurev-statistics-042720-125902", "isFamilyFriendly": true, "displayUrl": "https://www.annualreviews.org/doi/10.1146/annurev-statistics-042720-125902", "snippet": "Mathematical definitions of <b>fairness</b> generally <b>treat</b> the statistical learning problem that is used to formulate a predictive model as external to the <b>fairness</b> evaluation. Here, too, there are a number of choices that can have larger social implications but go unmeasured by the <b>fairness</b> evaluation. We focus specifically on the choices of training data, model, and predictive evaluation. 2.2.1. The data. The foundation of a predictive model is the data on which it is trained. The data available ...", "dateLastCrawled": "2022-01-21T17:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Crtical Essay V3.docx - The duty to uphold the <b>rule</b> of law and ...", "url": "https://www.coursehero.com/file/66531744/Crtical-Essay-V3docx/", "isFamilyFriendly": true, "displayUrl": "https://www.coursehero.com/file/66531744/Crtical-Essay-V3docx", "snippet": "Student Name: Ryan Jeffrey / Student Number: N9992936 / Word Count: 2499 C. <b>Rule</b> of Law and procedural <b>fairness</b> Deontological ethics <b>requires</b> that rules are action-guiding which may recommend or limit acts. 29 The <b>rule</b> of law and procedural <b>fairness</b> contain limits on the outcome and procedures of administrative decision-making. 30 In particular, the <b>rule</b> of law can serve as an institutional <b>constraint</b> on arbitrariness in the exercise of power. 31 Arbitrariness occurs when power is exercised ...", "dateLastCrawled": "2021-12-23T07:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Fairer machine learning in the real world: Mitigating discrimination ...", "url": "https://journals.sagepub.com/doi/full/10.1177/2053951717743530", "isFamilyFriendly": true, "displayUrl": "https://journals.sagepub.com/doi/full/10.1177/2053951717743530", "snippet": "Understanding <b>fairness</b> by demographic will also be hard to grasp when those demographics are latent \u2013 such as treating <b>individuals</b> holding particular political views similarly in regards to moderating content online (Binns et al., 2017). More importantly, even though the three approaches we outline deal with different levels of formality and different ways of understanding or conceiving <b>fairness</b>, they <b>all</b> remain broadly centred on the software artefacts themselves. We do not suggest that ...", "dateLastCrawled": "2022-01-31T03:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Algorithms</b> and the Individual in Criminal Law | Canadian Journal of ...", "url": "https://www.cambridge.org/core/journals/canadian-journal-of-philosophy/article/abs/algorithms-and-the-individual-in-criminal-law/3EA248379E4082CFFECEAB74750999EF", "isFamilyFriendly": true, "displayUrl": "https://www.cambridge.org/core/journals/canadian-journal-of-philosophy/article/abs/...", "snippet": "If the right <b>requires</b> nothing more than that we <b>treat</b> agents as law-abiding until we have adequate particularized evidence that they aren\u2019t, then there is no conflict at <b>all</b> between it and the use of algorithmic risk scores in making sentencing determinations. While one could simply accept this conclusion, it seems to me that the \u201cpresumption of law-abidingness\u201d does not exhaust the obligations grounded in civic respect.", "dateLastCrawled": "2022-01-15T22:20:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Affirmative</b> <b>Algorithms</b>: The Legal Grounds for <b>Fairness</b> as Awareness ...", "url": "https://lawreviewblog.uchicago.edu/2020/10/30/aa-ho-xiang/", "isFamilyFriendly": true, "displayUrl": "https://lawreviewblog.uchicago.edu/2020/10/30/aa-ho-xiang", "snippet": "Authors are listed alphabetically and have <b>equally</b> contributed to this work. * * * A part of the series, <b>Affirmative</b> Action at a Crossroads. Abstract. While there has been a flurry of research in algorithmic <b>fairness</b>, what is less recognized is that modern antidiscrimination law may prohibit the adoption of such techniques. We make three contributions. First, we discuss how such approaches will likely be deemed \u201calgorithmic <b>affirmative</b> action,\u201d posing serious legal risks of violating ...", "dateLastCrawled": "2022-01-31T02:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "On Consequentialism and <b>Fairness</b>", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7861221/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7861221", "snippet": "Note that whereas demographic parity only <b>requires</b> the set of predictions (\u0176) made for <b>all</b> <b>individuals</b> <b>in a dataset</b>, equal opportunity and equal calibration also require that we know the true outcome (Y) for <b>all</b> such <b>individuals</b>, even those who are given a negative prediction. As a result, the latter two requirements <b>can</b> only be properly verified on a <b>dataset</b> for which we <b>can</b> independently observe the true outcome (e.g., based on assigning treatment randomly).", "dateLastCrawled": "2022-01-28T22:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Empirical Risk Minimization Under <b>Fairness</b> Constraints | Request PDF", "url": "https://www.researchgate.net/publication/333044604_Empirical_Risk_Minimization_Under_Fairness_Constraints", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/333044604_Empirical_Risk_Minimization_Under...", "snippet": "In general, algorithmic <b>fairness</b> methods <b>can</b> be divided into three families: (i) post-processing to modify a pre-trained model to increase the <b>fairness</b> of its outcomes [18,22,46]; (ii) in ...", "dateLastCrawled": "2021-12-20T07:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Algorithmic Fairness: Choices, Assumptions, and Definitions</b> | Annual ...", "url": "https://www.annualreviews.org/doi/10.1146/annurev-statistics-042720-125902", "isFamilyFriendly": true, "displayUrl": "https://www.annualreviews.org/doi/10.1146/annurev-statistics-042720-125902", "snippet": "For example, this definition <b>requires</b> that <b>all</b> <b>individuals</b> who were denied a loan be <b>equally</b> likely to have defaulted had the loan been granted. The other pair of definitions that appear on this margin are equality of positive predictive value and equality of false discovery rate. These are defined by the conditional independence relationship Y\u22a5A|D = 1. This definition of <b>fairness</b> is also sometimes called predictive parity (Chouldechova 2017), and it is assessed by an outcome test (Simoiu ...", "dateLastCrawled": "2022-01-21T17:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Fairness in Machine Learning: A Survey</b> | DeepAI", "url": "https://deepai.org/publication/fairness-in-machine-learning-a-survey", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>fairness-in-machine-learning-a-survey</b>", "snippet": "These methods <b>can</b> also <b>treat</b> previously trained ml models as a black-box ... [goel2018non]; 2) not <b>all</b> <b>fairness</b> measures are <b>equally</b> affected by the strength of regularization parameters [di2020counterfactual, berk2017]; and 3) different regularization terms and penalties have diverse results on different data sets, i.e. this choice <b>can</b> have qualitative effects on the trade-off between accuracy and <b>fairness</b> [berk2017]. For <b>constraint</b> optimization it <b>can</b> be difficult to balance conflicting ...", "dateLastCrawled": "2022-01-18T13:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Frontiers | On Consequentialism and <b>Fairness</b> | Artificial Intelligence", "url": "https://www.frontiersin.org/articles/10.3389/frai.2020.00034/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/frai.2020.00034", "snippet": "Note that whereas demographic parity only <b>requires</b> the set of predictions (\u0176) made for <b>all</b> <b>individuals</b> <b>in a dataset</b>, equal opportunity and equal calibration also require that we know the true outcome (Y) for <b>all</b> such <b>individuals</b>, even those who are given a negative prediction. As a result, the latter two requirements <b>can</b> only be properly verified on a <b>dataset</b> for which we <b>can</b> independently observe the true outcome (e.g., based on assigning treatment randomly).", "dateLastCrawled": "2022-02-03T13:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Review into bias in algorithmic decision-making</b> - <b>GOV.UK</b>", "url": "https://www.gov.uk/government/publications/cdei-publishes-review-into-bias-in-algorithmic-decision-making/main-report-cdei-review-into-bias-in-algorithmic-decision-making", "isFamilyFriendly": true, "displayUrl": "https://<b>www.gov.uk</b>/government/publications/cdei-publishes-review-into-bias-in...", "snippet": "Preface. <b>Fairness</b> is a highly prized human value. Societies in which <b>individuals</b> <b>can</b> flourish need to be held together by practices and institutions that are regarded as fair.", "dateLastCrawled": "2022-02-03T02:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Algorithmic <b>Fairness</b> - DeepAI", "url": "https://deepai.org/publication/algorithmic-fairness", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/algorithmic-<b>fairness</b>", "snippet": "Algorithmic <b>Fairness</b>. 01/21/2020 \u2219 by Dana Pessach, et al. \u2219 Tel Aviv University \u2219 43 \u2219 share. An increasing number of decisions regarding the daily lives of human beings are being controlled by artificial intelligence (AI) <b>algorithms</b> in spheres ranging from healthcare, transportation, and education to college admissions, recruitment ...", "dateLastCrawled": "2021-12-15T08:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Fairer machine learning in the real world: Mitigating discrimination ...", "url": "https://journals.sagepub.com/doi/full/10.1177/2053951717743530", "isFamilyFriendly": true, "displayUrl": "https://journals.sagepub.com/doi/full/10.1177/2053951717743530", "snippet": "Many of the <b>fairness</b> issues in machine learning are primarily <b>thought</b> to arise from data. Some think, falling for what could be called the \u2018neutrality fallacy\u2019, that machine learning will provide a more even and objective treatment of <b>individuals</b> (Sandvig, 2015).As Latour indicates, we are often more than happy to declare value-laden issues as matters of fact, and let machines settle them for us (1999).", "dateLastCrawled": "2022-01-31T03:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>ARTIFICIAL INTELLIGENCE (AI</b>) \u2013 Dr Rajiv Desai", "url": "https://drrajivdesaimd.com/2017/03/23/artificial-intelligence-ai/", "isFamilyFriendly": true, "displayUrl": "https://drrajivdesaimd.com/2017/03/23/<b>artificial-intelligence-ai</b>", "snippet": "<b>Algorithms</b> are solutions. Computer science <b>can</b> <b>be thought</b> of as the study of <b>algorithms</b>. However, we must be careful to include the fact that some problems may not have a solution. Although proving this statement is beyond the scope of this text, the fact that some problems cannot be solved is important for those who study computer science. We <b>can</b> fully define computer science, then, by including both types of problems and stating that computer science is the study of solutions to problems ...", "dateLastCrawled": "2022-02-02T00:12:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Learning Optimal Fair Classification Trees | DeepAI", "url": "https://deepai.org/publication/learning-optimal-fair-classification-trees", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/learning-optimal-fair-classification-trees", "snippet": "This is in contrast to \u201cindividual\u201d <b>fairness</b>, which <b>requires</b> that <b>individuals</b> with similar characteristics be classified similarly (Dwork et al., 2012). Both perspectives have their advantages and drawbacks, but in this work we will focus on group <b>fairness</b> primarily because of its simplicity and interpreability. In the following sections, we will discuss the use of many notions of group <b>fairness</b> that <b>can</b> sufficiently satisfy more nuanced and fine-grained <b>fairness</b> considerations.", "dateLastCrawled": "2022-01-26T22:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Classification - <b>Fairness</b> and machine learning", "url": "https://fairmlbook.org/classification.html", "isFamilyFriendly": true, "displayUrl": "https://fairmlbook.org/classification.html", "snippet": "The condition refers to the group of <b>all</b> <b>individuals</b> receiving a particular score value. It does not mean that at the level of a single individual a score of r corresponds to a probability r of a positive outcome. The latter is a much stronger property that is satisfied by the conditional expectation R=\\mathbb{E}[Y\\mid X].", "dateLastCrawled": "2022-02-03T07:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Fairness in Machine Learning: A Survey</b> | DeepAI", "url": "https://deepai.org/publication/fairness-in-machine-learning-a-survey", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>fairness-in-machine-learning-a-survey</b>", "snippet": "These methods <b>can</b> also <b>treat</b> previously trained ml models as a black-box ... \u2019s notion of individual <b>fairness</b>, i.e. that <b>all</b> similar <b>individuals</b> should be treated similarly. Thus, bandit approaches frame the <b>fairness</b> problem as a stochastic multi-armed bandit framework, assigning either <b>individuals</b> to arms, or groups of \u201csimilar\u201d <b>individuals</b> to arms, and <b>fairness</b> quality as a reward represented as regret [joseph2016fair, liu2017calibrated]. The two main notions of <b>fairness</b> that have ...", "dateLastCrawled": "2022-01-18T13:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Affirmative</b> <b>Algorithms</b>: The Legal Grounds for <b>Fairness</b> as Awareness ...", "url": "https://lawreviewblog.uchicago.edu/2020/10/30/aa-ho-xiang/", "isFamilyFriendly": true, "displayUrl": "https://lawreviewblog.uchicago.edu/2020/10/30/aa-ho-xiang", "snippet": "Authors are listed alphabetically and have <b>equally</b> contributed to this work. * * * A part of the series, <b>Affirmative</b> Action at a Crossroads. Abstract. While there has been a flurry of research in algorithmic <b>fairness</b>, what is less recognized is that modern antidiscrimination law may prohibit the adoption of such techniques. We make three contributions. First, we discuss how such approaches will likely be deemed \u201calgorithmic <b>affirmative</b> action,\u201d posing serious legal risks of violating ...", "dateLastCrawled": "2022-01-31T02:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "The Measure and Mismeasure of <b>Fairness</b>: A Critical Review of Fair ...", "url": "https://www.arxiv-vanity.com/papers/1808.00023/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1808.00023", "snippet": "The nascent field of fair machine learning aims to ensure that decisions guided by <b>algorithms</b> are equitable. Over the last several years, three formal definitions of <b>fairness</b> have gained prominence: (1) anti-classification, meaning that protected attributes\u2014like race, gender, and their proxies\u2014are not explicitly used to make decisions; (2) classification parity, meaning that common measures of predictive performance (e.g., false positive and false negative rates) are equal across groups ...", "dateLastCrawled": "2021-09-22T08:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) Algorithmic decision making and the cost of <b>fairness</b>", "url": "https://www.researchgate.net/publication/313107433_Algorithmic_decision_making_and_the_cost_of_fairness", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/313107433_Algorithmic_decision_making_and_the...", "snippet": "4. In their context, they consider human decisions, rather than algorithmic ones, but. the same de-biasing procedure <b>can</b> be applied to any <b>rule</b>. Algorithmic decision making and the cost of ...", "dateLastCrawled": "2022-01-27T02:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "On the Basis of Sex: A Review of Gender Bias in Machine Learning ...", "url": "https://deepai.org/publication/on-the-basis-of-sex-a-review-of-gender-bias-in-machine-learning-applications", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/on-the-basis-of-sex-a-review-of-gender-bias-in-machine...", "snippet": "Thus, these <b>algorithms</b> <b>can</b> sometimes have inadvertent but detrimental effects [balancing_competing_objectives]. Examples of such gender-based unfairness in real applications are abundant. For instance, Gender Shades, a black-box algorithmic audit of three commercial facial analysis models using their Application Programming Interfaces (APIs), found that the classifiers performed better on male faces than female faces and lighter faces than darker faces. <b>All</b> of the classifiers in the study ...", "dateLastCrawled": "2022-02-02T00:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Towards a Fair Marketplace: Counterfactual Evaluation of the trade-off ...", "url": "https://www.researchgate.net/publication/328439354_Towards_a_Fair_Marketplace_Counterfactual_Evaluation_of_the_trade-off_between_Relevance_Fairness_Satisfaction_in_Recommendation_Systems", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/328439354_Towards_a_Fair_Marketplace...", "snippet": "Addressing these biased and unfair recommendations is a challenging task and <b>requires</b> careful design of <b>algorithms</b> as improving the <b>fairness</b> of recommendations leads to loss in recommendation ...", "dateLastCrawled": "2022-01-14T16:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Fairer machine learning in the real world: Mitigating discrimination ...", "url": "https://journals.sagepub.com/doi/full/10.1177/2053951717743530", "isFamilyFriendly": true, "displayUrl": "https://journals.sagepub.com/doi/full/10.1177/2053951717743530", "snippet": "Understanding <b>fairness</b> by demographic will also be hard to grasp when those demographics are latent \u2013 such as treating <b>individuals</b> holding particular political views similarly in regards to moderating content online (Binns et al., 2017). More importantly, even though the three approaches we outline deal with different levels of formality and different ways of understanding or conceiving <b>fairness</b>, they <b>all</b> remain broadly centred on the software artefacts themselves. We do not suggest that ...", "dateLastCrawled": "2022-01-31T03:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Algorithms</b> and the Individual in Criminal Law | Canadian Journal of ...", "url": "https://www.cambridge.org/core/journals/canadian-journal-of-philosophy/article/abs/algorithms-and-the-individual-in-criminal-law/3EA248379E4082CFFECEAB74750999EF", "isFamilyFriendly": true, "displayUrl": "https://www.cambridge.org/core/journals/<b>can</b>adian-journal-of-philosophy/article/abs/...", "snippet": "If the right <b>requires</b> nothing more than that we <b>treat</b> agents as law-abiding until we have adequate particularized evidence that they aren\u2019t, then there is no conflict at <b>all</b> between it and the use of algorithmic risk scores in making sentencing determinations. While one could simply accept this conclusion, it seems to me that the \u201cpresumption of law-abidingness\u201d does not exhaust the obligations grounded in civic respect.", "dateLastCrawled": "2022-01-15T22:20:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Classification - <b>Fairness</b> and <b>machine</b> <b>learning</b>", "url": "https://fairmlbook.org/classification.html", "isFamilyFriendly": true, "displayUrl": "https://fairmlbook.org/classification.html", "snippet": "Dwork et al. Dwork et al., \u201c<b>Fairness</b> Through Awareness.\u201d argued that the independence criterion was inadequate as a <b>fairness</b> <b>constraint</b>. The separation criterion appeared under the name equalized odds , Hardt, Price, and Srebro, \u201cEquality of Opportunity in Supervised <b>Learning</b>.\u201d alongside the relaxation to equal false negative rates, called equality of opportunity.", "dateLastCrawled": "2022-02-03T07:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A Survey on Bias and <b>Fairness</b> in <b>Machine</b> <b>Learning</b> \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/1908.09635/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1908.09635", "snippet": "With the widespread use of AI systems and applications in our everyday lives, it is important to take <b>fairness</b> issues into consideration while designing and engineering these types of systems. Such systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that the decisions do not reflect discriminatory behavior toward certain groups or populations. We have recently seen work in <b>machine</b> <b>learning</b>, natural language ...", "dateLastCrawled": "2021-11-15T17:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A Human-centric Approach to <b>Fairness</b> in AI", "url": "https://www.timlrx.com/blog/a-human-centric-approach-to-fairness-in-ai", "isFamilyFriendly": true, "displayUrl": "https://www.timlrx.com/blog/a-human-centric-approach-to-<b>fairness</b>-in-ai", "snippet": "A lot of what is discussed in the <b>machine</b> <b>learning</b> literature touches on <b>fairness</b> (or rather equivalence in certain outcomes) between groups, yet this narrowly constricts <b>fairness</b> to the notion of equality. Of course, we should think about <b>fairness</b> in the context of prejudiced groups, but we should also ask whether it is fair to an individual. Adding constraints in models might lead to worse outcomes for other individuals. If the decision making processs has serious consequences e.g. a fraud ...", "dateLastCrawled": "2022-02-03T02:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "dimensions of <b>machine</b> Causality and the normative", "url": "https://maxkasy.github.io/home/files/other/ML_inequality_conference/slides_loftus.pdf", "isFamilyFriendly": true, "displayUrl": "https://maxkasy.github.io/home/files/other/ML_inequality_conference/slides_loftus.pdf", "snippet": "dimensions of <b>machine</b> <b>learning</b> Joshua Loftus (LSE Statistics) High level intro Causality, what is it good for? Causal <b>fairness</b> In prediction and ranking tasks, and with intersectionality Designing interventions Optimal fair policies, causal interference Concluding thoughts 2 / 27. Tech solutionism, using ML/AI in every situation 3 / 27. Imagination Albert Einstein: Imagination is more important than knowledge. For knowledge is limited, whereas imagination [...] stimulat[es] progress, giving ...", "dateLastCrawled": "2022-01-11T12:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) Matching Code and Law: Achieving Algorithmic <b>Fairness</b> with ...", "url": "https://www.academia.edu/40615359/Matching_Code_and_Law_Achieving_Algorithmic_Fairness_with_Optimal_Transport", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/40615359/Matching_Code_and_Law_Achieving_Algorithmic_<b>Fairness</b>...", "snippet": "Increasingly, discrimination by algorithms is perceived as a societal and legal problem. As a response, a number of criteria for implementing algorithmic <b>fairness</b> in <b>machine</b> <b>learning</b> have been developed in the literature. This paper proposes the", "dateLastCrawled": "2021-12-22T00:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Using <b>Constraint</b> Programming to Improve Grocery Picking Efficiency at ...", "url": "https://www.torontomachinelearning.com/events/using-constraint-programming-to-improve-grocery-picking-efficiency-at-loblaws/", "isFamilyFriendly": true, "displayUrl": "https://www.toronto<b>machinelearning</b>.com/events/using-<b>constraint</b>-programming-to-improve...", "snippet": "Jaya Kawale is the Director of <b>Machine</b> <b>Learning</b> at Tubi leading all of the <b>machine</b> <b>learning</b> efforts at Tubi encompassing homepage recommendations, content understanding and ads. Prior to Tubi, she has worked on different aspects of recommender systems at Netflix and Adobe research labs. She did her PhD from the University of Minnesota, Twin cities and her thesis won several awards including the Explorations in Science using computation award. She has published many top tier conference and ...", "dateLastCrawled": "2021-12-29T16:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Fairness</b> through awareness", "url": "https://www.cs.toronto.edu/~toni/Papers/awareness.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.toronto.edu/~toni/Papers/awareness.pdf", "snippet": "ing utility subject to the <b>fairness</b> <b>constraint</b>, that similar individuals are treated similarly. We also present an adapta-tion of our approach to achieve the complementary goal of \\fair a rmative action,&quot;which guarantees statistical parity (i.e., the demographics of the set of individuals receiving any classi cation are the same as the demographics of the underlying population), while treating similar individuals as similarly as possible. Finally, we discuss the relationship of <b>fairness</b> to ...", "dateLastCrawled": "2021-12-06T13:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Fairness</b> Through Awareness", "url": "https://www.cs.toronto.edu/~zemel/documents/fairAwareItcs2012.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.toronto.edu/~zemel/documents/fairAwareItcs2012.pdf", "snippet": "<b>Fairness</b> Through Awareness Cynthia Dwork Moritz Hardty Toniann Pitassiz Omer Reingoldx Richard Zemel{November 29, 2011 Abstract We study <b>fairness</b> in classi\ufb01cation, where individuals are classi\ufb01ed, e.g., admitted to a uni- versity, and the goal is to prevent discrimination against individuals based on their membership in some group, while maintaining utility for the classi\ufb01er (the university). The main conceptual contribution of this paper is a framework for fair classi\ufb01cation ...", "dateLastCrawled": "2022-01-29T03:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Machine Learning</b> 2020 summary: 84 interesting papers/articles | by ...", "url": "https://towardsdatascience.com/machine-learning-2020-summary-84-interesting-papers-articles-45bd45c0d35b", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>machine-learning</b>-2020-summary-84-interesting-papers...", "snippet": "8.<b>Machine learning</b> with natural sciences \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 A lot of research combining natural science and <b>machine learning</b> has been published. There is a lot of research on speeding up and improving the accuracy of numerical simulations using <b>machine learning</b> ...", "dateLastCrawled": "2022-01-29T20:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "On word analogies and negative results in NLP - Hacking semantics", "url": "https://hackingsemantics.xyz/2019/analogies/", "isFamilyFriendly": true, "displayUrl": "https://hackingsemantics.xyz/2019/analogies", "snippet": "Share of BATS <b>analogy</b> questions in which the vector the closest to the predicted vector is one of the source vectors (a,a&#39;, b), the target vector b&#39;, or some other vector. In most cases the result is simply the vector b (&quot;woman&quot;). If in most cases the predicted vector is the closest to the source vector, it means that the vector offset is simply too small to induce a meaning shift on its own. And that means that adding it will not get you somewhere significantly different. Which means you ...", "dateLastCrawled": "2021-06-02T05:25:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(fairness constraint)  is like +(a rule that requires algorithms to treat all individuals in a dataset equally)", "+(fairness constraint) is similar to +(a rule that requires algorithms to treat all individuals in a dataset equally)", "+(fairness constraint) can be thought of as +(a rule that requires algorithms to treat all individuals in a dataset equally)", "+(fairness constraint) can be compared to +(a rule that requires algorithms to treat all individuals in a dataset equally)", "machine learning +(fairness constraint AND analogy)", "machine learning +(\"fairness constraint is like\")", "machine learning +(\"fairness constraint is similar\")", "machine learning +(\"just as fairness constraint\")", "machine learning +(\"fairness constraint can be thought of as\")", "machine learning +(\"fairness constraint can be compared to\")"]}