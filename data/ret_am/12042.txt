{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A Gentle <b>Introduction to the BFGS Optimization Algorithm</b>", "url": "https://machinelearningmastery.com/bfgs-optimization-in-python/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/bfgs-optimization-in-python", "snippet": "When an objective function has more than <b>one</b> <b>input</b> variable, the <b>input</b> variables together may be thought of as a vector, which may be familiar from linear algebra. The gradient is the generalization of the <b>derivative</b> to multivariate functions. It captures the local slope of the function, allowing us to predict <b>the effect</b> of taking a <b>small</b> step from a point in any direction. \u2014 Page 21, Algorithms for Optimization, 2019. Similarly, the first <b>derivative</b> of multiple <b>input</b> variables may also be ...", "dateLastCrawled": "2022-01-29T18:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Minimizing the <b>cost function</b>: Gradient descent | by XuanKhanh Nguyen ...", "url": "https://towardsdatascience.com/minimizing-the-cost-function-gradient-descent-a5dd6b5350e1", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/minimizing-the-<b>cost-function</b>-gradient-descent-a5dd6b5350e1", "snippet": "It speci\ufb01es how to scale a <b>small</b> <b>change</b> in the <b>input</b> to obtain the corresponding <b>change</b> in the <b>output</b>. Let\u2019s say, f(x) = 1/2 x\u00b2 . We can reduce f(x) by moving in <b>small</b> steps with the opposite sign of the <b>derivative</b>. When f\u2019(x) = 0,the <b>derivative</b> provides no information about which direction to move. Points where f\u2019(x) = 0 are known as critical points. The concept of convergence is a well defined mathematical term. It means that \u201ceventually\u201d a sequence of elements gets closer and ...", "dateLastCrawled": "2022-02-03T01:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Partial</b> dependence through stratification - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S2666827021000736", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2666827021000736", "snippet": "The <b>partial</b> <b>derivative</b> of y with respect to x h e i g h t is 10 (holding all other variables constant), so the optimal <b>partial</b> dependence curve is a line with slope 10. Fig. 7 illustrates the curves for the techniques under consideration, with ALE and StratPD giving the sharpest representation of the linear relationship. (StratPD \u2019s curve is drawn on top of the SHAP plots using the righthand scale.)The FPD and both SHAP plots also suggest a linear relationship, albeit with a little less ...", "dateLastCrawled": "2021-11-20T22:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Kernel methods and their derivatives: Concept and perspectives for the ...", "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0235885", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/plos<b>one</b>/article?id=10.1371/journal.p<b>one</b>.0235885", "snippet": "Kernel methods are powerful <b>machine</b> <b>learning</b> techniques which use generic non-linear functions to solve complex tasks. They have a solid mathematical foundation and exhibit excellent performance in practice. However, kernel machines are still considered black-box models as the kernel feature mapping cannot be accessed directly thus making the kernels difficult to interpret. The aim of this work is to show that it is indeed possible to interpret the functions learned by various kernel methods ...", "dateLastCrawled": "2021-05-02T19:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Gradient Descent</b> Problems and Solutions in Neural Networks | by Shachi ...", "url": "https://medium.com/analytics-vidhya/gradient-descent-problems-and-solutions-in-deep-learning-8002bbac09d5", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/<b>gradient-descent</b>-problems-and-solutions-in-deep...", "snippet": "<b>Input</b> range from (-infinite,infinite) yields an <b>output</b> of range (0,<b>input</b>) respectively. <b>Derivative</b> of ReLU function for <b>input</b> less than 0 is 0 while equals or greater than 1 as 1.", "dateLastCrawled": "2022-02-02T23:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "JPEG steganalysis based on ResNeXt with Gauss <b>partial</b> <b>derivative</b> ...", "url": "https://link.springer.com/article/10.1007/s11042-020-09350-2", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11042-020-09350-2", "snippet": "During the training, the <b>learning</b> rate is initialized to 0.001 and decreases by 10% per 5,000 training iterations with step policy. The maximum number of iterations is set to 10 5. Parameters in filters. As described in Section 2.2, each m-order Gauss <b>partial</b> <b>derivative</b> produces m + 1 filters. Due to the limitation of memory, we use up to 5 ...", "dateLastCrawled": "2022-01-10T03:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Regression Multiple Choice Questions and Answers</b> | Regression Quiz", "url": "https://www.gkseries.com/mcq-on-regression/multiple-choice-questions-and-answers-on-regression", "isFamilyFriendly": true, "displayUrl": "https://www.gkseries.com/mcq-on-<b>regression/multiple-choice-questions-and-answers</b>-on...", "snippet": "Questions. Free download in PDF <b>Regression Multiple Choice Questions and Answers</b> for competitive exams. These short objective type questions with answers are very important for Board exams as well as competitive exams. These short solved questions or quizzes are provided by Gkseries. 1 In a linear regression problem, we are using \u201cR-squared ...", "dateLastCrawled": "2022-01-31T16:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Back propagation Algorithm</b> - Back Propagation in Neural Networks", "url": "https://intellipaat.com/blog/tutorial/artificial-intelligence-tutorial/back-propagation-algorithm/", "isFamilyFriendly": true, "displayUrl": "https://intellipaat.com/.../artificial-intelligence-tutorial/<b>back-propagation-algorithm</b>", "snippet": "The <b>output</b> for h1: The <b>output</b> for h1 is calculated by applying a sigmoid function to the net <b>input</b> Of h1. Learn more about Artificial Intelligence in this Artificial Intelligence training in Toronto to get ahead in your career!. The sigmoid function pumps the values for which it is used in the range, 0 to 1.", "dateLastCrawled": "2022-02-02T14:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>What Are Boundary Conditions</b>? Numerics Background | SimScale", "url": "https://www.simscale.com/docs/simwiki/numerics-background/what-are-boundary-conditions/", "isFamilyFriendly": true, "displayUrl": "https://www.simscale.com/docs/simwiki/numerics-background/<b>what-are-boundary-conditions</b>", "snippet": "Constraints on the <b>derivative</b> of velocity or pressure fields are mainly used in two cases. The first case is the application of a symmetry plane, thus: $$ \\cfrac{\\<b>partial</b> u}{\\<b>partial</b> n} = 0 \\tag{10}$$ Since this condition is always applied in addition to a Dirichlet b.c. it is naturally satisfied. The second application is the modeling of wall ...", "dateLastCrawled": "2022-01-30T21:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Activation Functions Explained</b> - <b>Machine</b> <b>Learning</b> From Scratch", "url": "https://mlfromscratch.com/activation-functions-explained/", "isFamilyFriendly": true, "displayUrl": "https://mlfromscratch.com/<b>activation-functions-explained</b>", "snippet": "This is my <b>Machine</b> <b>Learning</b> journey &#39;From Scratch&#39;. Conveying what I learned, in an easy-to-understand fashion is my priority. More posts by Casper Hansen. Casper Hansen. 22 Aug 2019 \u2022 27 min read. During the calculations of the values for activations in each layer, we use an activation function right before deciding what exactly the activation value should be. From the previous activations, weights and biases in each layer, we calculate a value for every activation in the next layer. But ...", "dateLastCrawled": "2022-02-03T00:42:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "An Introduction to <b>Partial Least Squares</b> Regression", "url": "https://stats.oarc.ucla.edu/wp-content/uploads/2016/02/pls.pdf", "isFamilyFriendly": true, "displayUrl": "https://stats.oarc.ucla.edu/wp-content/uploads/2016/02/pls.pdf", "snippet": "prediction is a function of all of the <b>input</b> factors. In this case, the PLS predictions can be interpreted as contrasts between broad bands of frequencies. Discussion As discussed in the introductory section, soft science applications involve so many variables that it is not practical to seek a \u2018\u2018hard\u2019\u2019 model explicitly relating them all. <b>Partial least squares</b> is <b>one</b> solution for such problems, but there are others, including other factor extraction techniques, like principal ...", "dateLastCrawled": "2022-02-03T06:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Partial</b> dependence through stratification - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S2666827021000736", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2666827021000736", "snippet": "The <b>partial</b> <b>derivative</b> of y with respect to x h e i g h t is 10 (holding all other variables constant), so the optimal <b>partial</b> dependence curve is a line with slope 10. Fig. 7 illustrates the curves for the techniques under consideration, with ALE and StratPD giving the sharpest representation of the linear relationship. (StratPD \u2019s curve is drawn on top of the SHAP plots using the righthand scale.)The FPD and both SHAP plots also suggest a linear relationship, albeit with a little less ...", "dateLastCrawled": "2021-11-20T22:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Loss Function</b> (Part II): <b>Logistic Regression</b> | by Shuyu Luo | Towards ...", "url": "https://towardsdatascience.com/optimization-loss-function-under-the-hood-part-ii-d20a239cde11", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/optimization-<b>loss-function</b>-under-the-hood-part-ii-d20a...", "snippet": "<b>Similar</b> to Gradient Descent, we firstly take the <b>partial</b> <b>derivative</b> of J(\u03b8) that is the slope of J(\u03b8), and note it as f(\u03b8). Instead of decreasing \u03b8 by a certain chosen <b>learning</b> rate \u03b1 multiplied with f(\u03b8) , Newton\u2019s Method gets an updated \u03b8 at the point of intersection of the tangent line of f(\u03b8) at previous \u03b8 and x axis. After amount of iterations, Newton\u2019s Method will converge at f(\u03b8) = 0.", "dateLastCrawled": "2022-02-02T17:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Gentle <b>Introduction to the BFGS Optimization Algorithm</b>", "url": "https://machinelearningmastery.com/bfgs-optimization-in-python/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/bfgs-optimization-in-python", "snippet": "When an objective function has more than <b>one</b> <b>input</b> variable, the <b>input</b> variables together may be thought of as a vector, which may be familiar from linear algebra. The gradient is the generalization of the <b>derivative</b> to multivariate functions. It captures the local slope of the function, allowing us to predict the <b>effect</b> of taking a <b>small</b> step from a point in any direction. \u2014 Page 21, Algorithms for Optimization, 2019. Similarly, the first <b>derivative</b> of multiple <b>input</b> variables may also be ...", "dateLastCrawled": "2022-01-29T18:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Kernel methods and their derivatives: Concept and perspectives for the ...", "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0235885", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/plos<b>one</b>/article?id=10.1371/journal.p<b>one</b>.0235885", "snippet": "Kernel methods are powerful <b>machine</b> <b>learning</b> techniques which use generic non-linear functions to solve complex tasks. They have a solid mathematical foundation and exhibit excellent performance in practice. However, kernel machines are still considered black-box models as the kernel feature mapping cannot be accessed directly thus making the kernels difficult to interpret. The aim of this work is to show that it is indeed possible to interpret the functions learned by various kernel methods ...", "dateLastCrawled": "2021-05-02T19:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Back propagation Algorithm</b> - Back Propagation in Neural Networks", "url": "https://intellipaat.com/blog/tutorial/artificial-intelligence-tutorial/back-propagation-algorithm/", "isFamilyFriendly": true, "displayUrl": "https://intellipaat.com/.../artificial-intelligence-tutorial/<b>back-propagation-algorithm</b>", "snippet": "The <b>output</b> for h1: The <b>output</b> for h1 is calculated by applying a sigmoid function to the net <b>input</b> Of h1. Learn more about Artificial Intelligence in this Artificial Intelligence training in Toronto to get ahead in your career!. The sigmoid function pumps the values for which it is used in the range, 0 to 1.", "dateLastCrawled": "2022-02-02T14:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "10.5 Influential Instances | Interpretable <b>Machine</b> <b>Learning</b>", "url": "https://christophm.github.io/interpretable-ml-book/influential.html", "isFamilyFriendly": true, "displayUrl": "https://christophm.github.io/interpretable-ml-book/influential.html", "snippet": "A big <b>change</b> means that an instance was very influential. The second approach upweights <b>one</b> of the persons by an infinitesimally <b>small</b> weight, which corresponds to the calculation of the first <b>derivative</b> of a statistic or model. This approach is also known as \u201cinfinitesimal approach\u201d or \u201c<b>influence</b> function\u201d. The answer is, by the way, that your mean estimate can be very strongly influenced by a single answer, since the mean scales linearly with single values. A more robust choice is ...", "dateLastCrawled": "2022-01-31T17:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Activation Functions Explained</b> - <b>Machine</b> <b>Learning</b> From Scratch", "url": "https://mlfromscratch.com/activation-functions-explained/", "isFamilyFriendly": true, "displayUrl": "https://mlfromscratch.com/<b>activation-functions-explained</b>", "snippet": "This is my <b>Machine</b> <b>Learning</b> journey &#39;From Scratch&#39;. Conveying what I learned, in an easy-to-understand fashion is my priority. More posts by Casper Hansen. Casper Hansen. 22 Aug 2019 \u2022 27 min read. During the calculations of the values for activations in each layer, we use an activation function right before deciding what exactly the activation value should be. From the previous activations, weights and biases in each layer, we calculate a value for every activation in the next layer. But ...", "dateLastCrawled": "2022-02-03T00:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "GIS Basics: Digital Image Processing | Mingke Erin Li", "url": "https://erin-1919.github.io/GIS-Basics-Digital-Image-Processing/", "isFamilyFriendly": true, "displayUrl": "https://erin-1919.github.io/GIS-Basics-Digital-Image-Processing", "snippet": "Otsu\u2019s <b>algorithm</b> is <b>one</b> approach to determine the best global threshold which is to maximize the between-class variance. The generalization form of the Otsu\u2019s <b>algorithm</b> is to find (k-1) global thresholds to separate k classes. There are occasions when Otsu fails, such as there are no strong peaks in the histogram, or the object is too <b>small</b> compared to the background. The possible solutions include doing a low pass filter before the Otsu\u2019s <b>algorithm</b> or only consider the pixels near ...", "dateLastCrawled": "2022-01-29T03:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Implementing Gradient Descent to Solve a Linear Regression Problem</b> in ...", "url": "https://www.codeproject.com/articles/879043/implementing-gradient-descent-to-solve-a-linear-re", "isFamilyFriendly": true, "displayUrl": "https://<b>www.codeproject.com</b>/articles/879043/<b>implementing-gradient-descent-to-solve</b>-a...", "snippet": "<b>Machine</b> <b>learning</b> is definitely amongst the most broadly used forms of A.I. in today&#39;s technology, it is used for speech recognition, handwriting recognition, separating spam emails, showing you the most relevant websites when you search something on a search engine, and many other things. Things You Need To Know. Knowing about matrices and vectors is necessary because matrix multiplication will be used in the code, but if you&#39;re not familiar with the concept, don&#39;t worry. I will explain each ...", "dateLastCrawled": "2022-02-02T02:34:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A Gentle <b>Introduction to the BFGS Optimization Algorithm</b>", "url": "https://machinelearningmastery.com/bfgs-optimization-in-python/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/bfgs-optimization-in-python", "snippet": "It captures the local slope of the function, allowing us to predict the <b>effect</b> of taking a <b>small</b> step from a point in any direction. \u2014 Page 21, Algorithms for Optimization, 2019. Similarly, the first <b>derivative</b> of multiple <b>input</b> variables may also be a vector, where each element is called a <b>partial</b> <b>derivative</b>. This vector of <b>partial</b> ...", "dateLastCrawled": "2022-01-29T18:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Kernel methods and their derivatives: Concept and perspectives for the ...", "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0235885", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/plos<b>one</b>/article?id=10.1371/journal.p<b>one</b>.0235885", "snippet": "Kernel methods are powerful <b>machine</b> <b>learning</b> techniques which use generic non-linear functions to solve complex tasks. They have a solid mathematical foundation and exhibit excellent performance in practice. However, kernel machines are still considered black-box models as the kernel feature mapping cannot be accessed directly thus making the kernels difficult to interpret. The aim of this work is to show that it is indeed possible to interpret the functions learned by various kernel methods ...", "dateLastCrawled": "2021-05-02T19:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Back propagation Algorithm</b> - Back Propagation in Neural Networks", "url": "https://intellipaat.com/blog/tutorial/artificial-intelligence-tutorial/back-propagation-algorithm/", "isFamilyFriendly": true, "displayUrl": "https://intellipaat.com/.../artificial-intelligence-tutorial/<b>back-propagation-algorithm</b>", "snippet": "The <b>output</b> for h1: The <b>output</b> for h1 is calculated by applying a sigmoid function to the net <b>input</b> Of h1. Learn more about Artificial Intelligence in this Artificial Intelligence training in Toronto to get ahead in your career!. The sigmoid function pumps the values for which it is used in the range, 0 to 1.", "dateLastCrawled": "2022-02-02T14:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Comparison of Activation Functions for Deep Neural Networks | by Ayy\u00fcce ...", "url": "https://towardsdatascience.com/comparison-of-activation-functions-for-deep-neural-networks-706ac4284c8a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/comparison-of-activation-functions-for-deep-neural...", "snippet": "Sigmoid Function and <b>Derivative</b>. Then we <b>can</b> sort the layers \ud83d\ude03 So let\u2019s think of non-binary functions. It is also derivated because it is different from the step function. This means that <b>learning</b> <b>can</b> happen. If we examine the graph x is between -2 and +2, y values <b>change</b> quickly. <b>Small</b> changes in x will be large in y. This means that it ...", "dateLastCrawled": "2022-02-01T23:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Multiple Linear Regression (MLR) Definition", "url": "https://www.investopedia.com/terms/m/mlr.asp", "isFamilyFriendly": true, "displayUrl": "https://<b>www.investopedia.com</b>/terms/m/mlr.asp", "snippet": "The <b>output</b> from a multiple regression <b>can</b> be displayed horizontally as an equation, or vertically in table form. Example of How to Use Multiple Linear Regression . As an example, an analyst may ...", "dateLastCrawled": "2022-02-03T00:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Loss Functions and Optimization Algorithms. Demystified. | by Apoorva ...", "url": "https://medium.com/data-science-group-iitr/loss-functions-and-optimization-algorithms-demystified-bb92daff331c", "isFamilyFriendly": true, "displayUrl": "https://medium.com/data-science-group-iitr/loss-functions-and-optimization-<b>algorithms</b>...", "snippet": "The choice of Optimisation Algorithms and Loss Functions for a deep <b>learning</b> model <b>can</b> play a big role in producing optimum and faster results. Before we begin, let us see how different components ...", "dateLastCrawled": "2022-01-31T23:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "The <b>vanishing gradient problem</b> and ReLUs - Adventures in <b>Machine</b> <b>Learning</b>", "url": "https://adventuresinmachinelearning.com/vanishing-gradient-problem-tensorflow/", "isFamilyFriendly": true, "displayUrl": "https://adventuresin<b>machinelearning</b>.com/<b>vanishing-gradient-problem</b>-tensorflow", "snippet": "Deep <b>learning</b> is huge in <b>machine</b> <b>learning</b> at the moment, ... the <b>derivative</b> (orange line) becomes very <b>small</b> i.e. &lt;&lt; 1. This causes vanishing gradients and poor <b>learning</b> for deep networks. This <b>can</b> occur when the weights of our networks are initialized poorly \u2013 with too-large negative and positive values. These too-large values saturate the <b>input</b> to the sigmoid and pushes the derivatives into the <b>small</b> valued regions. However, even if the weights are initialized nicely, and the derivatives ...", "dateLastCrawled": "2022-01-31T21:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "10.5 Influential Instances | Interpretable <b>Machine</b> <b>Learning</b>", "url": "https://christophm.github.io/interpretable-ml-book/influential.html", "isFamilyFriendly": true, "displayUrl": "https://christophm.github.io/interpretable-ml-book/influential.html", "snippet": "The second approach upweights <b>one</b> of the persons by an infinitesimally <b>small</b> weight, which corresponds to the calculation of the first <b>derivative</b> of a statistic or model. This approach is also known as \u201cinfinitesimal approach\u201d or \u201c<b>influence</b> function\u201d. The answer is, by the way, that your mean estimate <b>can</b> be very strongly influenced by a single answer, since the mean scales linearly with single values. A more robust choice is the median (the value at which <b>one</b> half of people earn ...", "dateLastCrawled": "2022-01-31T17:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Implementing Gradient Descent to Solve a Linear Regression Problem</b> in ...", "url": "https://www.codeproject.com/articles/879043/implementing-gradient-descent-to-solve-a-linear-re", "isFamilyFriendly": true, "displayUrl": "https://<b>www.codeproject.com</b>/articles/879043/<b>implementing-gradient-descent-to-solve</b>-a...", "snippet": "<b>Machine</b> <b>learning</b> is definitely amongst the most broadly used forms of A.I. in today&#39;s technology, it is used for speech recognition, handwriting recognition, separating spam emails, showing you the most relevant websites when you search something on a search engine, and many other things. Things You Need To Know. Knowing about matrices and vectors is necessary because matrix multiplication will be used in the code, but if you&#39;re not familiar with the concept, don&#39;t worry. I will explain each ...", "dateLastCrawled": "2022-02-02T02:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Maziar Raissi | <b>Hidden Fluid Mechanics</b>", "url": "https://maziarraissi.github.io/research/09_hidden_fluid_mechanics/", "isFamilyFriendly": true, "displayUrl": "https://maziarraissi.github.io/research/09_<b>hidden_fluid_mechanics</b>", "snippet": "This in fact is <b>one</b> of the advantages of the <b>algorithm</b> in which other unknown parameters such as the Reynolds and P\u00e9clet numbers <b>can</b> be inferred in addition to the velocity and pressure fields. When the fluid is non-Newtonian (e.g., blood flow in <b>small</b> vessels), <b>one</b> <b>can</b> encode the momentum equations, where the \u201cconstitutive\u201d law for the fluid\u2019s stress-strain relationship is unknown, into a physics-informed deep <b>learning</b> <b>algorithm</b>. Having data on the velocity (or even the passive ...", "dateLastCrawled": "2022-01-30T20:19:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "MLPR w7d - <b>Machine</b> <b>Learning</b> and Pattern Recognition", "url": "https://mlpr.inf.ed.ac.uk/2021/notes/w7d_backprop.html", "isFamilyFriendly": true, "displayUrl": "https://mlpr.inf.ed.ac.uk/2021/notes/w7d_backprop.html", "snippet": "If a function has many inputs, we <b>can</b> obtain all of its <b>partial</b> derivatives <b>in one</b> reverse sweep, which only costs a constant times the number of operations of <b>one</b> function evaluation. However, the forwards-mode procedure, like finite differences, would require multiple sweeps through the graph, <b>one</b> sweep for each <b>partial</b> <b>derivative</b>. For a neural network with millions of parameters, reverse-mode differentiation (or backpropagation", "dateLastCrawled": "2021-12-28T00:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Combination of fractional order <b>derivative</b> and memory-based <b>learning</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0341816218304867", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0341816218304867", "snippet": "RF is a nonparametric data mining <b>algorithm</b> that <b>can</b> reflect nonlinear regression relationships between SOM and spectral data, and is <b>one</b> of the most important techniques in ensemble <b>learning</b> (Breiman, 2001; Chen et al., 2018; Viscarra Rossel and Behrens, 2010). RF is composed of various classification or regression trees produced by a combination of bootstrap aggregation and random feature selection used at each split of the trees. When RF is applied for regression purpose, the final ...", "dateLastCrawled": "2022-01-16T15:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "An Introduction to <b>Partial Least Squares</b> Regression", "url": "https://stats.oarc.ucla.edu/wp-content/uploads/2016/02/pls.pdf", "isFamilyFriendly": true, "displayUrl": "https://stats.oarc.ucla.edu/wp-content/uploads/2016/02/pls.pdf", "snippet": "prediction is a function of all of the <b>input</b> factors. In this case, the PLS predictions <b>can</b> be interpreted as contrasts between broad bands of frequencies. Discussion As discussed in the introductory section, soft science applications involve so many variables that it is not practical to seek a \u2018\u2018hard\u2019\u2019 model explicitly relating them all. <b>Partial least squares</b> is <b>one</b> solution for such problems, but there are others, including other factor extraction techniques, like principal ...", "dateLastCrawled": "2022-02-03T06:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>machine</b> <b>learning</b> - What are the advantages of <b>ReLU</b> over sigmoid ...", "url": "https://stats.stackexchange.com/questions/126238/what-are-the-advantages-of-relu-over-sigmoid-function-in-deep-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://stats.stackex<b>change</b>.com/questions/126238", "snippet": "The <b>effect</b> of multiplying the gradient n times makes the gradient to be even smaller for lower layers, leading to a very <b>small</b> <b>change</b> or even no <b>change</b> in the weights of lower layers. Therefore, the deeper the network, the more the <b>effect</b> of vanishing gradients. This makes <b>learning</b> per iteration slower when activation functions that suffer from vanishing gradients is used e.g Sigmoid and tanh functions. Kindly refer", "dateLastCrawled": "2022-02-03T06:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Kernel methods and their derivatives: Concept and perspectives for the ...", "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0235885", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/plos<b>one</b>/article?id=10.1371/journal.p<b>one</b>.0235885", "snippet": "Kernel methods are powerful <b>machine</b> <b>learning</b> techniques which use generic non-linear functions to solve complex tasks. They have a solid mathematical foundation and exhibit excellent performance in practice. However, kernel machines are still considered black-box models as the kernel feature mapping cannot be accessed directly thus making the kernels difficult to interpret. The aim of this work is to show that it is indeed possible to interpret the functions learned by various kernel methods ...", "dateLastCrawled": "2021-05-02T19:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A Comprehensive Guide to the <b>Backpropagation</b> <b>Algorithm</b> in Neural ...", "url": "https://neptune.ai/blog/backpropagation-algorithm-in-neural-networks-guide", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/<b>backpropagation</b>-<b>algorithm</b>-in-neural-networks-guide", "snippet": "The <b>output</b> layer is the last layer which returns the network\u2019s predicted <b>output</b>. Like the <b>input</b> layer, there <b>can</b> only be a single <b>output</b> layer. If the objective of the network is to predict student scores in the next semester, then the <b>output</b> layer should return a score. The architecture in the next figure has a single neuron that returns the next semester\u2019s predicted score. Between the <b>input</b> and <b>output</b> layers, there might be 0 or more hidden layers. In this example, there are 2 hidden ...", "dateLastCrawled": "2022-02-02T19:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "The <b>vanishing gradient problem</b> and ReLUs - Adventures in <b>Machine</b> <b>Learning</b>", "url": "https://adventuresinmachinelearning.com/vanishing-gradient-problem-tensorflow/", "isFamilyFriendly": true, "displayUrl": "https://adventuresin<b>machinelearning</b>.com/<b>vanishing-gradient-problem</b>-tensorflow", "snippet": "Deep <b>learning</b> is huge in <b>machine</b> <b>learning</b> at the moment, ... the <b>derivative</b> (orange line) becomes very <b>small</b> i.e. &lt;&lt; 1. This causes vanishing gradients and poor <b>learning</b> for deep networks. This <b>can</b> occur when the weights of our networks are initialized poorly \u2013 with too-large negative and positive values. These too-large values saturate the <b>input</b> to the sigmoid and pushes the derivatives into the <b>small</b> valued regions. However, even if the weights are initialized nicely, and the derivatives ...", "dateLastCrawled": "2022-01-31T21:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Mathematics Hidden Behind Linear Regression | by Rahul Ravi | Towards ...", "url": "https://towardsdatascience.com/mathematics-hidden-behind-linear-regression-431fe4d11969", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/mathematics-hidden-behind-linear-regression-431fe4d11969", "snippet": "The global minimum of a function refers to the minimum point of a function in a specifically defined domain for <b>input</b>(s), i.e, for a 2D plot, there is <b>one</b> <b>input</b> for every <b>output</b>. However, in a 3D plot, there are 2 inputs for every <b>output</b>. Therefore, the objective is to identify 2 <b>input</b> values that <b>can</b> result in a minimum of the plane. To continue, The cost function as described above is computed by taking the differences between the actual value and the predicted value for a given <b>input</b> and ...", "dateLastCrawled": "2022-01-31T10:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Activation Functions Explained</b> - <b>Machine</b> <b>Learning</b> From Scratch", "url": "https://mlfromscratch.com/activation-functions-explained/", "isFamilyFriendly": true, "displayUrl": "https://mlfromscratch.com/<b>activation-functions-explained</b>", "snippet": "This is my <b>Machine</b> <b>Learning</b> journey &#39;From Scratch&#39;. Conveying what I learned, in an easy-to-understand fashion is my priority. More posts by Casper Hansen. Casper Hansen. 22 Aug 2019 \u2022 27 min read. During the calculations of the values for activations in each layer, we use an activation function right before deciding what exactly the activation value should be. From the previous activations, weights and biases in each layer, we calculate a value for every activation in the next layer. But ...", "dateLastCrawled": "2022-02-03T00:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Ridge Regression</b> Explained, Step by Step - <b>Machine</b> <b>Learning</b> Compass", "url": "https://machinelearningcompass.com/machine_learning_models/ridge_regression/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>compass.com/<b>machine</b>_<b>learning</b>_models/<b>ridge_regression</b>", "snippet": "<b>Ridge Regression</b> is an adaptation of the popular and widely used linear regression <b>algorithm</b>. It enhances regular linear regression by slightly changing its cost function, which results in less overfit models. In this article, you will learn everything you need to know about <b>Ridge Regression</b>, and how you <b>can</b> start using it in your own <b>machine</b> <b>learning</b> projects.", "dateLastCrawled": "2022-02-02T15:51:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine learning derivatives</b> - Geoffrey Huck", "url": "https://geoffreyhuck.com/articles/machine-learning-derivatives", "isFamilyFriendly": true, "displayUrl": "https://geoffreyhuck.com/articles/<b>machine-learning-derivatives</b>", "snippet": "An <b>analogy</b> would be finding which direction you should take to reach the highest mountain but with the restriction of only being able to see one meter away. In this article, we will first recall the rules of derivatives and <b>partial</b> derivatives. Then we will feature a few derivatives of functions that are commonly used in <b>machine</b> <b>learning</b>. The derivatives rules. The basics. Let\u2019s start by the derivatives of common functions. In the following : is a constant. is the variable by which we ...", "dateLastCrawled": "2021-12-29T05:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Machine</b> <b>Learning</b> 101: An Intuitive Introduction to <b>Gradient</b> Descent ...", "url": "https://towardsdatascience.com/machine-learning-101-an-intuitive-introduction-to-gradient-descent-366b77b52645", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>machine</b>-<b>learning</b>-101-an-intuitive-introduction-to...", "snippet": "In other words, the <b>gradient</b> is a vector, and each of its components is a <b>partial</b> <b>derivative</b> with respect to one specific variable. Take the function, f(x, y) = 2x\u00b2 + y \u00b2 as another example. Here, f(x, y) is a multi-variable function. Its <b>gradient</b> is a vector, containing the <b>partial</b> derivatives of f(x, y). The first with respect to x, and the second with respect to y. If we calculate the partials of f(x,y) we get. So the <b>gradient</b> is the following vector: Note that each component indicates ...", "dateLastCrawled": "2022-01-30T05:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Learning</b> in <b>Machine</b> <b>Learning</b>", "url": "https://cambum.net/CCE/LearningParameter.pdf", "isFamilyFriendly": true, "displayUrl": "https://cambum.net/CCE/<b>Learning</b>Parameter.pdf", "snippet": "<b>Learning</b> Rate \u2013 Basic Algorithm 21 The delta-bar-delta algorithm is an early heuristic approach to adapting individual <b>learning</b> rates for model parameters during training. If the <b>partial</b> <b>derivative</b> of the loss, with respect to a given model parameter, remains the same sign, then the <b>learning</b> rate should increase.", "dateLastCrawled": "2022-01-29T04:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Machine</b> <b>Learning</b> with Spreadsheets! Part 1: <b>Gradient</b> Descent and ...", "url": "https://medium.com/excel-with-ml/machine-learning-with-spreadsheets-part-1-gradient-descent-f9316676db9b", "isFamilyFriendly": true, "displayUrl": "https://medium.com/excel-with-ml/<b>machine</b>-<b>learning</b>-with-spreadsheets-part-1-<b>gradient</b>...", "snippet": "<b>Gradient</b> descent: Step-by-step spreadsheets show you how machines learn without the code. Go under the hood with backprop, <b>partial</b> derivatives, and <b>gradient</b> descent.", "dateLastCrawled": "2022-01-29T01:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Analogues in thermodynamics: the <b>Partial</b> <b>Derivative</b> <b>Machine</b> and ...", "url": "https://www.compadre.org/per/items/4833.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.compadre.org/per/items/4833.pdf", "snippet": "foster student <b>learning</b> in thermodynamics, the Oregon State University Physics Education Research Group de-veloped the <b>Partial</b> <b>Derivative</b> <b>Machine</b> (PDM) as a me-chanical analogue to a thermodynamic system [2]. The PDM (Fig. 1) allows students to explore challenging aspects of thermodynamics, including inaccessible vari-ables and thermodynamic potentials, on a mechanical system that students can understand without having to rst learn new thermodynamic concepts [3]. We con-ducted post ...", "dateLastCrawled": "2021-10-31T12:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Mathematics <b>for Machine Learning: Multivariate Calculus</b> | by Isaac Ng ...", "url": "https://medium.com/@isaacng/mathematics-for-machine-learning-multivariate-calculus-7102c7a586c6", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@isaacng/mathematics-<b>for-machine-learning-multivariate-calculus</b>...", "snippet": "Graph with no <b>derivative</b> at 0. Finding the gradient. The gradient of a variable in a function is the rise over run against the other variables. In the classic x, y coordinate plot, the rise over ...", "dateLastCrawled": "2022-01-25T18:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>machine</b> <b>learning</b> - <b>Neural Network Regularization and derivation</b> - Cross ...", "url": "https://stats.stackexchange.com/questions/502047/neural-network-regularization-and-derivation", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/502047/neural-network-regularization-and...", "snippet": "The p index is there for us to carry out matrix multiplication. Recall that to perform matrix-vector multiplication A x where A \u2208 R m \u00d7 n and x \u2208 R n, the i -th entry is equal to \u2211 p = 1 n A i p x p . Notice that we are using <b>partial</b> <b>derivative</b> and hence \u2202 W j p [ l] \u2202 W j k [ l] = { 1, p = k 0, p \u2260 k.", "dateLastCrawled": "2022-01-22T09:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Notes on <b>Deep Learning and Differential Equations</b>. \u2013 Cloud Computing ...", "url": "https://cloud4scieng.org/2020/06/10/notes-on-deep-learning-and-differential-equations/", "isFamilyFriendly": true, "displayUrl": "https://cloud4scieng.org/2020/06/10/notes-on-<b>deep-learning-and-differential-equations</b>", "snippet": "We now turn to the work on using neural networks to solve <b>partial</b> differential equations. The examples we will study are from four papers. Raissi, Perdikaris, and Karniadakis, \u201cPhysics Informed Deep <b>Learning</b> (Part II): Data-driven Discovery of Nonlinear <b>Partial</b> Di\ufb00erential Equations\u201d, Nov 2017, is the paper that introduces PINNS and demonstrates the concept by showing how to solve several \u201cclassical\u201d PDEs. Yang, Zhang, and Karniadakis, \u201cPhysics-Informed Generative Adversarial ...", "dateLastCrawled": "2022-02-03T00:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "neural network - Gradient descent and <b>partial derivatives</b> - Data ...", "url": "https://datascience.stackexchange.com/questions/28691/gradient-descent-and-partial-derivatives", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/28691", "snippet": "Data Science Stack Exchange is a question and answer site for Data science professionals, <b>Machine</b> <b>Learning</b> specialists, and those interested in <b>learning</b> more about the field.", "dateLastCrawled": "2022-02-01T18:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "18.4. Multivariable Calculus \u2014 Dive into Deep <b>Learning</b> 0.17.2 documentation", "url": "http://www.d2l.ai/chapter_appendix-mathematics-for-deep-learning/multivariable-calculus.html", "isFamilyFriendly": true, "displayUrl": "www.d2l.ai/chapter_appendix-mathematics-for-deep-<b>learning</b>/multivariable-calculus.html", "snippet": "Note that we have taken our direction to have length one for convenience, and used \\(\\theta\\) for the angle between \\(\\mathbf{v}\\) and \\(\\nabla_{\\mathbf{w}} L(\\mathbf{w})\\).If we want to find the direction that decreases \\(L\\) as rapidly as possible, we want to make this expression as negative as possible. The only way the direction we pick enters into this equation is through \\(\\cos(\\theta)\\), and thus we wish to make this cosine as negative as possible.Now, recalling the shape of cosine ...", "dateLastCrawled": "2022-01-29T17:33:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "[1\uc8fc\ucc28] Coursera <b>Machine</b> <b>Learning</b> : <b>Regression</b>", "url": "https://enthan.tistory.com/41", "isFamilyFriendly": true, "displayUrl": "https://enthan.tistory.com/41", "snippet": "<b>partial derivative is like</b> a derivative with respect to w1, treating all variables as constants . Convex =&gt; solution is unique + gradient descent algorithm will converge to minimum . \ubc18\uc751\ud615 . \uacf5\uc720\ud558\uae30. \uae00 \uc694\uc18c. \uad6c\ub3c5\ud558\uae30 \uc5d4\uc9c0\ub2c8\uc5b4\uc758 \ub534\uc0dd\uac01 &#39;Data Science &gt; <b>Machine</b> <b>Learning</b>&#39; \uce74\ud14c\uace0\ub9ac\uc758 \ub2e4\ub978 \uae00 (0) 2021.05.27 [2\uc8fc\ucc28] Multiple <b>Regression</b> (0) 2021.05.27 [1\uc8fc\ucc28] Coursera <b>Machine</b> <b>Learning</b> : <b>Regression</b> (0) 2021.05.27: Scikit-Learn\uc744 \uc774\uc6a9\ud55c \uba38\uc2e0\ub7ec\ub2dd (0) 2020.10 ...", "dateLastCrawled": "2021-12-25T20:07:00.0000000Z", "language": "ko", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Machine</b> <b>Learning</b>", "url": "https://feisky.xyz/machine-learning/tensorflow/getting-started.html", "isFamilyFriendly": true, "displayUrl": "https://feisky.xyz/<b>machine</b>-<b>learning</b>/tensorflow/getting-started.html", "snippet": "Too large # jumps risk inaccuracy, too small slow the <b>learning</b>. <b>learning</b>_rate = 0.002 # In TensorFlow, we need to run everything in the context of a session. with tf. Session() as sess: # Set up all the tensors. # Our input layer is the x value and the bias node. input = tf. constant(x_with_bias) # Our target is the y values.", "dateLastCrawled": "2021-05-17T21:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Gradient Descent | Programming <b>Machine</b> <b>Learning</b> by Paolo Perrotta | The ...", "url": "https://medium.com/pragmatic-programmers/gradient-descent-1a227b6ddba5", "isFamilyFriendly": true, "displayUrl": "https://medium.com/pragmatic-programmers/gradient-descent-1a227b6ddba5", "snippet": "Programming <b>Machine</b> <b>Learning</b> \u2014 by Paolo Perrotta (20 / 133) Let\u2019s look for a better train algorithm. The job of train is to find the parameters that minimize the loss, so let\u2019s start by ...", "dateLastCrawled": "2021-08-23T13:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "October | 2012 | <b>SLog of the most illogical mind</b>", "url": "https://slogofillogicalmind.wordpress.com/2012/10/", "isFamilyFriendly": true, "displayUrl": "https://slogofillogicalmind.wordpress.com/2012/10", "snippet": "Abstract data type is like a imaginary <b>machine</b> that have some skills that we need to use. Programmer is the engineer to build this robot. But if you just simply want to this robot, you just need to know what operation you can use with this. For example there is a stack in ADT. Just think of one robot called \u201cstack\u201d and what it does is stores all the things that you put in it and take it back to you the item but in reverse order that you put in. So the item that you put in at the last ...", "dateLastCrawled": "2022-01-26T01:55:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Predicting <b>cell phone adoption metrics</b> using <b>machine</b> <b>learning</b> and ...", "url": "https://www.sciencedirect.com/science/article/pii/S0736585321000617", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0736585321000617", "snippet": "In this paper we present a <b>machine</b> <b>learning</b> method that uses publicly available satellite imagery to predict telecoms demand metrics, including cell phone adoption and spending on mobile services, and apply the method to Malawi and Ethiopia. Our predictive <b>machine</b> <b>learning</b> approach consistently outperforms baseline models which use population density or nightlight luminosity, with an improvement in data variance prediction of at least 40%. The method is a starting point for developing more ...", "dateLastCrawled": "2021-10-21T14:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Predicting cell phone adoption metrics using machine learning</b> and ...", "url": "https://www.researchgate.net/publication/350756678_Predicting_cell_phone_adoption_metrics_using_machine_learning_and_satellite_imagery", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/350756678_Predicting_cell_phone_adoption...", "snippet": "<b>machine</b> <b>learning</b> with call records and satellite image ry, to address a similar set of questions relating to poverty estimation (Ayush et al., 2020; Jean et al., 2016; Perez et al., 2017; Steele ...", "dateLastCrawled": "2021-12-25T00:20:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(partial derivative)  is like +(the effect of a small change in one input on the output of a machine learning algorithm)", "+(partial derivative) is similar to +(the effect of a small change in one input on the output of a machine learning algorithm)", "+(partial derivative) can be thought of as +(the effect of a small change in one input on the output of a machine learning algorithm)", "+(partial derivative) can be compared to +(the effect of a small change in one input on the output of a machine learning algorithm)", "machine learning +(partial derivative AND analogy)", "machine learning +(\"partial derivative is like\")", "machine learning +(\"partial derivative is similar\")", "machine learning +(\"just as partial derivative\")", "machine learning +(\"partial derivative can be thought of as\")", "machine learning +(\"partial derivative can be compared to\")"]}