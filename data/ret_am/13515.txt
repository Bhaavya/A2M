{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Hypotheses and analysis | Project-4", "url": "https://elianatroper.github.io/Project-4/pages/hypotheses.html", "isFamilyFriendly": true, "displayUrl": "https://elianatroper.github.io/Project-4/pages/hypotheses.html", "snippet": "For this model we thought about how we could implement a <b>fairness</b> <b>metric</b> <b>like</b> p% score or equal opportunity. We are predicting covid severity at the national level. The features we are using, trips which are averaged across counties so they can be observed at the national level, and national headlines are all categories we did not see to include protected classes. We did however, try to implement some explainability and interpretability metrics. We used the SHAP library which implements ...", "dateLastCrawled": "2022-02-01T09:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Evaluating <b>Fairness</b> Using Permutation Tests | DeepAI", "url": "https://deepai.org/publication/evaluating-fairness-using-permutation-tests", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/evaluating-<b>fairness</b>-using-permutation-tests", "snippet": "5.1. Empirical Analyses. The first empirical analysis compares the output of the permutation test with conventional <b>fairness</b> metrics. Specifically, we focus on performing a permutation test for the Recall (TPR) and the False Positive Rate (FPR) and compare this with the Equalized Odds <b>fairness</b> <b>metric</b>.", "dateLastCrawled": "2022-01-21T19:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Interpreting A/B test results: false positives and statistical ...", "url": "https://netflixtechblog.com/interpreting-a-b-test-results-false-positives-and-statistical-significance-c1522d0db27a", "isFamilyFriendly": true, "displayUrl": "https://netflixtechblog.com/interpreting-a-b-test-results-false-positives-and...", "snippet": "The false positive rate is closely associated with the \u201cstatistical significance\u201d of the observed difference in <b>metric</b> values between the treatment and control groups, which we measure using the <b>p-value</b>. The <b>p-value</b> is the probability of seeing an outcome at least as extreme as our A/B test result, had there truly been no difference between the treatment and control experiences. An intuitive way to understand statistical significance and p-values, which have been confusing students of ...", "dateLastCrawled": "2022-02-02T18:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) Validating Generic Metrics of <b>Fairness</b> in Game-based Resource ...", "url": "https://www.academia.edu/6212668/Validating_Generic_Metrics_of_Fairness_in_Game_based_Resource_Allocation_Scenarios_with_Crowdsourced_Annotations", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/6212668/Validating_Generic_<b>Metrics</b>_of_<b>Fairness</b>_in_Game_based...", "snippet": "The fifth <b>metric</b>, called temporal group-based <b>fairness</b> (T GB), is a new <b>metric</b> proposed by the authors, is ad-hoc designed for the one-to-many interaction scenario, and takes into consideration context- based aspects of the distribution task, such as the sequence of distribution and the presence of group structures within the receiver agents. Finally, the sixth <b>metric</b> is machine learned on the preference data (via ranking Support Vector Machines, SVM). The results obtained show that all ...", "dateLastCrawled": "2022-01-11T04:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Fair Top-k Ranking with multiple protected groups - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S0306457321001916", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0306457321001916", "snippet": "Our test setup sets the <b>p-value</b> to F \u03c4 ... (fair vs. unfair) of the ranked group <b>fairness</b> condition into a ranked group <b>fairness</b> <b>metric</b>. This <b>metric</b> would be the maximum \u03b1 \u2208 [0, 1] for which a ranking \u03c4 satisfies the ranked group <b>fairness</b> condition, given minimum proportions p G. In this <b>metric</b> space, larger values imply a stronger compliance with the required number of protected elements from the top-positions onward. 3.3. Utility. Notions of utility aim to measure desiderata of ...", "dateLastCrawled": "2022-01-26T12:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) Validating Generic Metrics of <b>Fairness</b> in Game-Based Resource ...", "url": "https://www.researchgate.net/publication/259757452_Validating_Generic_Metrics_of_Fairness_in_Game-Based_Resource_Allocation_Scenarios_with_Crowdsourced_Annotations", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/259757452_Validating_Generic_<b>Metrics</b>_of...", "snippet": "The SVM model can automatically model <b>fairness</b> more accurately than any ad-hoc <b>metric</b> examined (with an accuracy of 81.86%) but it is lim- ited by its expressivity and generalisability.", "dateLastCrawled": "2021-09-01T20:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Data Balance Analysis on Spark | SynapseML", "url": "https://microsoft.github.io/SynapseML/docs/features/responsible_ai/Data%20Balance%20Analysis/", "isFamilyFriendly": true, "displayUrl": "https://microsoft.github.io/SynapseML/docs/features/responsible_ai/Data Balance Analysis", "snippet": "\u2b50\ufe0f If you <b>like</b> SynapseML, ... specially in terms of <b>fairness</b>. It is unfortunately all too easy to build an ML model that produces biased results for subsets of an overall population, by training or testing the model on biased ground truth data. There are multiple case studies of biased models assisting in granting loans, healthcare, recruitment opportunities and many other decision making tasks. In most of these examples, the data from which these models are trained was the common issue ...", "dateLastCrawled": "2022-01-31T02:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "The LinkedIn <b>Fairness</b> Toolkit (LiFT) is a Scala/Spark library that ...", "url": "https://reposhub.com/java/general-utility-functions/linkedin-LiFT.html", "isFamilyFriendly": true, "displayUrl": "https://reposhub.com/java/general-utility-functions/linkedin-LiFT.html", "snippet": "More trials yield results with lower variance in the computed <b>p-value</b>, but takes more time 18. seed The random value seed 19. distanceMetrics Distance and divergence metrics that are to be computed. These are metrics such as Demographic Parity and Equalized Odds. 20. permutationMetrics The metrics to use for permutation testing 21. distanceBenefitMetrics The model metrics that are to be used for computing benefit vectors, one for each distance <b>metric</b> specified. 22. performanceBenefitMetrics ...", "dateLastCrawled": "2021-12-30T14:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Chapter 6 People-based ML models</b> | Public Policy Analytics: Code ...", "url": "https://urbanspatial.github.io/PublicPolicyAnalytics/people-based-ml-models.html", "isFamilyFriendly": true, "displayUrl": "https://urbanspatial.github.io/PublicPolicyAnalytics/<b>people-based-ml-models</b>.html", "snippet": "<b>Like</b> OLS, Logistic regression also provides <b>p-value</b> estimates of statistical significance. The missing coefficients may reflect colinearity or small variability across the 3 levels of avgBounceDistance. Perhaps the only signal in this feature that really matters is the 1-4 ft. indicator.", "dateLastCrawled": "2021-11-25T03:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>GitHub</b> - <b>linkedin</b>/LiFT: The <b>LinkedIn Fairness Toolkit (LiFT</b>) is a Scala ...", "url": "https://github.com/linkedin/LiFT", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/<b>linkedin</b>/LiFT", "snippet": "More trials yield results with lower variance in the computed <b>p-value</b>, but takes more time 18. seed The random value seed 19. distanceMetrics Distance and divergence metrics that are to be computed. These are metrics such as Demographic Parity and Equalized Odds. 20. permutationMetrics The metrics to use for permutation testing 21. distanceBenefitMetrics The model metrics that are to be used for computing benefit vectors, one for each distance <b>metric</b> specified. 22. performanceBenefitMetrics ...", "dateLastCrawled": "2022-02-01T17:57:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "DataBalanceAnalysis - Adult Census Income | SynapseML", "url": "https://microsoft.github.io/SynapseML/docs/features/responsible_ai/DataBalanceAnalysis%20-%20Adult%20Census%20Income/", "isFamilyFriendly": true, "displayUrl": "https://microsoft.github.io/SynapseML/docs/features/responsible_ai/DataBalanceAnalysis...", "snippet": "Association <b>Metric</b> Family Description Interpretation/Formula Reference; Demographic Parity: <b>Fairness</b> : Proportion of each segment of a protected class (e.g. gender) should receive the positive outcome at equal rates. As close to 0 means better parity. D P = P (Y \u2223 A = &quot; M a l e &quot;) \u2212 P (Y \u2223 A = &quot; F e m a l e &quot;) DP = P(Y \\vert A = &quot;Male&quot;) - P(Y \\vert A = &quot;Female&quot;) D P = P (Y \u2223 A = &quot; M a l e &quot;) \u2212 P (Y \u2223 A = &quot; F e m a l e &quot;). Link: Pointwise Mutual Information (PMI), normalized PMI ...", "dateLastCrawled": "2022-02-01T18:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Balancing Between Accuracy and <b>Fairness</b> for Interactive Recommendation ...", "url": "https://link.springer.com/chapter/10.1007%2F978-3-030-47426-3_13", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/chapter/10.1007/978-3-030-47426-3_13", "snippet": "Since the <b>fairness</b> <b>metric</b> (Eq. and Eq. ) is ... between FairRec and the strongest baseline DRR, where * means the <b>p-value</b> is smaller than 0.05. Moreover, we propose a Unit <b>Fairness</b> Gain (UFG) to jointly consider accuracy and <b>fairness</b>, $$\\begin{aligned} \\text {UFG}=\\frac{\\text {PropFair}}{\\text {CVR}_{\\text {max}}-\\text {CVR}}=\\frac{\\text {PropFair}}{1-\\text {CVR}}. \\end{aligned}$$ (12) UFG indicates the <b>fairness</b> of the system under unit accuracy budget. For any recommendation system, the ...", "dateLastCrawled": "2022-02-02T00:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) Validating Generic Metrics of <b>Fairness</b> in Game-based Resource ...", "url": "https://www.academia.edu/6212668/Validating_Generic_Metrics_of_Fairness_in_Game_based_Resource_Allocation_Scenarios_with_Crowdsourced_Annotations", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/6212668/Validating_Generic_<b>Metrics</b>_of_<b>Fairness</b>_in_Game_based...", "snippet": "The fifth <b>metric</b>, called temporal group-based <b>fairness</b> (T GB), is a new <b>metric</b> proposed by the authors, is ad-hoc designed for the one-to-many interaction scenario, and takes into consideration context- based aspects of the distribution task, such as the sequence of distribution and the presence of group structures within the receiver agents. Finally, the sixth <b>metric</b> is machine learned on the preference data (via ranking Support Vector Machines, SVM). The results obtained show that all ...", "dateLastCrawled": "2022-01-11T04:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Algorithmic Decision Making with Conditional <b>Fairness</b>", "url": "https://kunkuang.github.io/papers//KDD20-ConditionalFairness.pdf", "isFamilyFriendly": true, "displayUrl": "https://kunkuang.github.io/papers//KDD20-Conditional<b>Fairness</b>.pdf", "snippet": "Individual <b>fairness</b> requires <b>similar</b> individuals should have <b>similar</b> outcomes. However, it is difficult to define the similarity between individuals. Group <b>fairness</b> requires equity among differ- ent groups but they only use sensitive attributes and outcomes as measuring features. As a result, these notions may fail to distin-guish between fair and unfair parts in the problem. For example, Pearl [26] studied the case of Berkeley\u2019s alleged sex bias in graduate admission[4] and found that ...", "dateLastCrawled": "2021-10-08T02:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) Validating Generic Metrics of <b>Fairness</b> in Game-Based Resource ...", "url": "https://www.researchgate.net/publication/259757452_Validating_Generic_Metrics_of_Fairness_in_Game-Based_Resource_Allocation_Scenarios_with_Crowdsourced_Annotations", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/259757452_Validating_Generic_<b>Metrics</b>_of...", "snippet": "The SVM model can automatically model <b>fairness</b> more accurately than any ad-hoc <b>metric</b> examined (with an accuracy of 81.86%) but it is lim- ited by its expressivity and generalisability.", "dateLastCrawled": "2021-09-01T20:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Lecture 10: <b>Fairness</b>, Ethics, &amp; Law in ML", "url": "https://www.papernot.fr/teaching/f19/ECE1784H_10.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.papernot.fr/teaching/f19/ECE1784H_10.pdf", "snippet": "Individual <b>Fairness</b> Treat similarindividuals similarly <b>Similar</b> for the purpose of the classification task <b>Similar</b> distribution over outcomes \u2022Assume task-specific similarity <b>metric</b> \u2022Extent to which two individuals are <b>similar</b> w.r.t. the classification task at hand \u2022Ideally captures ground truth \u2022Or, society\u2019s best approximation Examples: \u2022Financial/insurance risk metrics \u2022Already widely used (though secret) \u2022AALIM health care <b>metric</b> \u2022health <b>metric</b> for treating <b>similar</b> ...", "dateLastCrawled": "2021-11-04T00:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "The LinkedIn <b>Fairness</b> Toolkit (LiFT) is a Scala/Spark library that ...", "url": "https://reposhub.com/java/general-utility-functions/linkedin-LiFT.html", "isFamilyFriendly": true, "displayUrl": "https://reposhub.com/java/general-utility-functions/linkedin-LiFT.html", "snippet": "More trials yield results with lower variance in the computed <b>p-value</b>, but takes more time 18. seed The random value seed 19. distanceMetrics Distance and divergence metrics that are to be computed. These are metrics such as Demographic Parity and Equalized Odds. 20. permutationMetrics The metrics to use for permutation testing 21. distanceBenefitMetrics The model metrics that are to be used for computing benefit vectors, one for each distance <b>metric</b> specified. 22. performanceBenefitMetrics ...", "dateLastCrawled": "2021-12-30T14:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>GitHub</b> - dccuchile/<b>wefe</b>: <b>WEFE</b>: The Word Embeddings <b>Fairness</b> Evaluation ...", "url": "https://github.com/dccuchile/wefe", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/dccuchile/<b>wefe</b>", "snippet": "Computing a <b>fairness</b> <b>metric</b> on a given pre-trained word embedding model using user-given queries. <b>WEFE</b> also standardizes the process of mitigating bias through an interface <b>similar</b> to the scikit-learn fit-transform. This standardization separates the mitigation process into two stages: The logic of calculating the transformation to be performed on the model (fit). The execution of the mitigation transformation on the model (transform). The official documentation can be found at this link ...", "dateLastCrawled": "2021-12-14T20:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Hypotheses and analysis | Project-4", "url": "https://elianatroper.github.io/Project-4/pages/hypotheses.html", "isFamilyFriendly": true, "displayUrl": "https://elianatroper.github.io/Project-4/pages/hypotheses.html", "snippet": "The <b>p-value</b> is also small which indicates that our observation is not likely to have occurred by chance. We also applied t-test to look at another trips feature, percent_stay_home, and how the means of covid cases for low and high values in the percent_stay_home variable compare. We did the same thing as before, we found the mean value for percent_stay_home, which is 0.20. We then split the data into low_stay, below the mean, and high_stay, everything else. We got the results below. Results ...", "dateLastCrawled": "2022-02-01T09:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Testing for Measurement <b>Invariance</b> in R | by Okan Bulut | Towards Data ...", "url": "https://towardsdatascience.com/testing-for-measurement-invariance-in-r-b44cace10148", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/testing-for-measurement-<b>invariance</b>-in-r-b44cace10148", "snippet": "To assess scalar <b>invariance</b>, we need to follow a <b>similar</b> approach by comparing the scalar model against the <b>metric</b> model. A statistically insignificant result for the test would indicate scalar <b>invariance</b> of the factorial model. Van de Schoot, Lugtig, and Hox\u00b9 suggest that scalar <b>invariance</b> must hold to be able to interpret latent means and correlations across groups. If scalar <b>invariance</b> is not fully satisfied, then partial MI could be established by adjusting factor loadings and/or ...", "dateLastCrawled": "2022-01-31T19:29:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Interpreting A/B test results: false positives and statistical ...", "url": "https://netflixtechblog.com/interpreting-a-b-test-results-false-positives-and-statistical-significance-c1522d0db27a", "isFamilyFriendly": true, "displayUrl": "https://netflixtechblog.com/interpreting-a-b-test-results-false-positives-and...", "snippet": "The false positive rate is closely associated with the \u201cstatistical significance\u201d of the observed difference in <b>metric</b> values between the treatment and control groups, which we measure using the <b>p-value</b>. The <b>p-value</b> is the probability of seeing an outcome at least as extreme as our A/B test result, had there truly been no difference between the treatment and control experiences. An intuitive way to understand statistical significance and p-values, which have been confusing students of ...", "dateLastCrawled": "2022-02-02T18:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Hypotheses and analysis | Project-4", "url": "https://elianatroper.github.io/Project-4/pages/hypotheses.html", "isFamilyFriendly": true, "displayUrl": "https://elianatroper.github.io/Project-4/pages/hypotheses.html", "snippet": "For this model we <b>thought</b> about how we could implement a <b>fairness</b> <b>metric</b> like p% score or equal opportunity. We are predicting covid severity at the national level. The features we are using, trips which are averaged across counties so they <b>can</b> be observed at the national level, and national headlines are all categories we did not see to include protected classes. We did however, try to implement some explainability and interpretability metrics. We used the SHAP library which implements ...", "dateLastCrawled": "2022-02-01T09:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Reflection of <b>Political Bias within YouTube Search and Recommendation</b> ...", "url": "https://ysjournal.com/reflection-of-political-bias-within-youtube-search-and-recommendation-algorithms/", "isFamilyFriendly": true, "displayUrl": "https://ysjournal.com/reflection-of-<b>political-bias-within-youtube-search</b>-and...", "snippet": "Under the <b>fairness</b> <b>metric</b> of demographic parity, ... All of them have a <b>p value</b> smaller than 0.01 (figure 14-16). Ultimately, the videos until the 10th cycle seem to have a decreasing bias score which contradicts the echo-chamber theory. The expectation would be that if YouTube were to be using the echo chamber theory, the more videos we would click on, the more biased the videos would become. However, there does seem to be a difference in the minimising effect between categories. At the ...", "dateLastCrawled": "2021-12-30T05:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Inequality Indices</b> as Tests of <b>Fairness</b> | The Economic Journal | Oxford ...", "url": "https://academic.oup.com/ej/article/129/621/2216/5280840", "isFamilyFriendly": true, "displayUrl": "https://academic.oup.com/ej/article/129/621/2216/5280840", "snippet": "Section 1 presents two canonical processes within which we <b>can</b> identify <b>fairness</b>, ... Hence the <b>p-value</b> may mask this quantitative closeness. An additional problem is that comparisons of results are difficult across different datasets if they have different numbers of datapoints. We may, of course, solve this problem by simply bootstrapping the data and computing estimated p-values over a range of n. 14 However this procedure is computationally inconvenient. A further\u2014admittedly minor ...", "dateLastCrawled": "2021-12-20T14:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "The 20% Statistician: <b>Communicating uncertainty with p-values</b>", "url": "https://daniellakens.blogspot.com/2014/05/communicating-uncertainty-with-p-values.html", "isFamilyFriendly": true, "displayUrl": "https://daniellakens.blogspot.com/2014/05/<b>communicating-uncertainty-with-p-values</b>.html", "snippet": "In approximately 83.4% of close replication studies we <b>can</b> expect a Cohen\u2019s d between 0.02 and .081, and thus a <b>p-value</b> (given our sample size of 50 per condition) between .0001 and .92. To me, this sounds pretty bad. It tells me that only 83.4 (or five out of six) studies will observe an effect that, with a sample size of 50 in each condition, will provide a", "dateLastCrawled": "2021-12-09T11:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Stereotype-Free Classification of Fictitious Faces</b> | DeepAI", "url": "https://deepai.org/publication/stereotype-free-classification-of-fictitious-faces", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>stereotype-free-classification-of-fictitious-faces</b>", "snippet": "Thus, <b>fairness</b> definition in predictive models is still controversial with absence of consensus among researchers, and a new school of <b>thought</b> in <b>fairness</b> function is published quite often via research papers to dampen the discrimination effect. The variety of approaches leads to a difficulty for evaluation of the progress in the field, and no strengths and weaknesses <b>can</b> be assessed for further recommendations accordingly.", "dateLastCrawled": "2021-12-09T17:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Extreme Value Theory with High Frequency Financial Data", "url": "http://public.econ.duke.edu/~get/browse/courses/201/spr12/ECON201_FINAL_REPORTS/Sawant_Abhinay_Econ201FS.doc", "isFamilyFriendly": true, "displayUrl": "public.econ.duke.edu/~get/browse/courses/201/spr12/ECON201_FINAL_REPORTS/Sawant...", "snippet": "Therefore, the number of breaks <b>can</b> <b>be thought</b> of as a binomial random variable with probability p = (1 \u2013 X)/100 over the number of out-of-sample trials n. Using a two-sided test with the null hypothesis that the number of breaks equals the expected value n(1 \u2013 X), a <b>p-value</b> was determined. With the same general idea, Kupiec proposed a powerful two-sided test that for a valid VAR model, the statistic", "dateLastCrawled": "2021-12-08T00:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Chapter 3 Test Fairness</b> | 2018-19 Summative Technical Report", "url": "https://technicalreports.smarterbalanced.org/2018-19_summative-report/_book/test-fairness.html", "isFamilyFriendly": true, "displayUrl": "https://technicalreports.smarterbalanced.org/.../_book/test-<b>fairness</b>.html", "snippet": "Ensuring test <b>fairness</b> is a fundamental part of validity, starting with test design. It is an important feature built into each step of the test development process, such as item writing, test administration, and scoring. The 2014 Standards for Educational and Psychological Testing (AERA, APA, &amp; NCME, 2014, p. 49) state, \u201cThe term <b>fairness</b> has no single technical meaning, and is used in many ways in public discourse.\u201d It also suggests that <b>fairness</b> to all individuals in the intended ...", "dateLastCrawled": "2022-02-01T07:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "System to Integrate <b>Fairness</b> Transparently: An Industry Approach", "url": "https://www.arxiv-vanity.com/papers/2006.06082/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2006.06082", "snippet": "Numerous Machine Learning (ML) bias-related problems have generated significant press in recent years. This has led to scrutiny of corporate failure regarding how to incorporate human oversight in thorough evaluation and prevention of bias. Companies have a responsibility to monitor ML processes for bias and mitigate any bias detected, ensure business product integrity, preserve customer loyalty, and protect brand image. In this paper, we propose SIFT (System to Integrate <b>Fairness</b> ...", "dateLastCrawled": "2021-09-16T01:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>SCI100</b> Flashcards - <b>Quizlet</b>", "url": "https://quizlet.com/564091523/sci100-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/564091523/<b>sci100</b>-flash-cards", "snippet": "A <b>p-value</b> is a statistical measure of how likely it is that a relationship is due to random chance. For example, a <b>p-value</b> of 0.05 means that there is a probability of 0.05 (5%) that the results could be due to random chance. If there is a greater than 5% chance the results are due to chance, you cannot trust that a relationship between the variables really exists. In other words, even if you see a dramatic slope, if the <b>p-value</b> is greater than 0.05, you must treat the result as non ...", "dateLastCrawled": "2022-01-29T12:29:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) Validating Generic Metrics of <b>Fairness</b> in Game-based Resource ...", "url": "https://www.academia.edu/6212668/Validating_Generic_Metrics_of_Fairness_in_Game_based_Resource_Allocation_Scenarios_with_Crowdsourced_Annotations", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/6212668/Validating_Generic_<b>Metrics</b>_of_<b>Fairness</b>_in_Game_based...", "snippet": "The SVM model <b>can</b> automatically model <b>fairness</b> more accurately than any ad-hoc <b>metric</b> examined (with an accuracy of 81.86%) but it is lim- ited by its expressivity and generalisability. Keywords: <b>Fairness</b>, Social Preference, Resource Allocation, Dictator Game, Crowdsourcing, Preference Learning, Support Vector Machines, Feature Selec- tion, Genetic Algorithms. 1 Introduction The control and influence of virtual or artificial societies is a highly complex task, in part, due to the difficulty ...", "dateLastCrawled": "2022-01-11T04:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Evaluating <b>Fairness</b> Using Permutation Tests | DeepAI", "url": "https://deepai.org/publication/evaluating-fairness-using-permutation-tests", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/evaluating-<b>fairness</b>-using-permutation-tests", "snippet": "When certain assumptions about the underlying distribution or the <b>metric</b> to be measured <b>can</b> be made, we <b>can</b> resort to parametric tests suited for these purposes. However, when we wish to have a pluggable interface for any <b>metric</b> (with respect to which we wish to measure discrepancies in <b>fairness</b>), we need to make the testing framework as generic as possible. There are numerous definitions of <b>fairness</b> including equalized odds, equality of opportunity, individual or group <b>fairness</b>, and ...", "dateLastCrawled": "2022-01-21T19:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) Validating Generic Metrics of <b>Fairness</b> in Game-Based Resource ...", "url": "https://www.researchgate.net/publication/259757452_Validating_Generic_Metrics_of_Fairness_in_Game-Based_Resource_Allocation_Scenarios_with_Crowdsourced_Annotations", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/259757452_Validating_Generic_<b>Metrics</b>_of...", "snippet": "The SVM model <b>can</b> automatically model <b>fairness</b> more accurately than any ad-hoc <b>metric</b> examined (with an accuracy of 81.86%) but it is lim- ited by its expressivity and generalisability.", "dateLastCrawled": "2021-09-01T20:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Statistical Power in <b>Hypothesis Testing</b> | by Acusio Bivona | Medium", "url": "https://acusio-bivona.medium.com/statistical-power-in-hypothesis-testing-2cab6589c5de", "isFamilyFriendly": true, "displayUrl": "https://acusio-bivona.medium.com/statistical-power-in-<b>hypothesis-testing</b>-2cab6589c5de", "snippet": "For this post, I will use testing the <b>fairness</b> of a coin as the example, and my null <b>hypothesis</b> will be \u201cthis coin is fair\u201d. In this case, ... including the <b>p-value</b> for rejecting the null <b>hypothesis</b>, the size of our samples, and the unfairness level of the coin (aka the effect size). To reiterate, the power of a statistical test is characterized as the likelihood that the null <b>hypothesis</b> will be rejected, provided that it is indeed false. The power of a statistical test varies from 0 to ...", "dateLastCrawled": "2022-02-03T02:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "About \u2014 WEFE 0.3.2 documentation", "url": "https://wefe.readthedocs.io/en/latest/about.html", "isFamilyFriendly": true, "displayUrl": "https://wefe.readthedocs.io/en/latest/about.html", "snippet": "Computing a <b>fairness</b> <b>metric</b> on a given pre-trained word embedding model using user-given queries. ... The one-sided <b>p-value</b> of the permutation test is: \\[\\text{Pr}_{i}[s(T_{1_i}, T_{2_i}, A_1, A_2) &gt; s(T_1, T_2, A_1, A_2)]\\] RND\u00b6 Relative Norm Distance (RND), presented in the paper \u201cWord embeddings quantify 100 years of gender and ethnic stereotypes\u201d. RND averages the embeddings of each target set, then for each of the attribute words, calculates the norm of the difference between the ...", "dateLastCrawled": "2022-01-30T22:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Algorithmic Decision Making with Conditional <b>Fairness</b>", "url": "https://kunkuang.github.io/papers/KDD20-ConditionalFairness.pdf", "isFamilyFriendly": true, "displayUrl": "https://kunkuang.github.io/papers/KDD20-Conditional<b>Fairness</b>.pdf", "snippet": "<b>fairness</b> as a more sound <b>fairness</b> <b>metric</b> by conditioning on the <b>fairness</b> variables. Given different prior knowledge of fair vari-ables, we demonstrate that traditional <b>fairness</b> notations, such as demographic parity and equalized odds, are special cases of our conditional <b>fairness</b> notations. Moreover, we propose a Derivable Conditional <b>Fairness</b> Regularizer (DCFR), which <b>can</b> be integrated into any decision-making model, to track the trade-off between precision and <b>fairness</b> of algorithmic ...", "dateLastCrawled": "2021-11-28T00:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Neural Segregation of Objective and Contextual Aspects of <b>Fairness</b>", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3109551/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC3109551", "snippet": "Introduction. <b>Fairness</b> is of interest to sociologists (Homans, 1961), economists (Akerlof, 1979; Kahneman et al., 1986), and neuroscientists (Sanfey et al., 2003).<b>Fairness</b> reflects objective features of how people share resources, classically elicited in the \u201cultimatum game\u201d (UG) in which one player (the proposer) is given an endowment (e.g., \u00a310) and proposes a division (e.g., keep \u00a36/offer \u00a34) to a second player (the responder), who <b>can</b> accept (both get the proposed split) or reject ...", "dateLastCrawled": "2022-01-08T15:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Reflection of <b>Political Bias within YouTube Search and Recommendation</b> ...", "url": "https://ysjournal.com/reflection-of-political-bias-within-youtube-search-and-recommendation-algorithms/", "isFamilyFriendly": true, "displayUrl": "https://ysjournal.com/reflection-of-<b>political-bias-within-youtube-search</b>-and...", "snippet": "Under the <b>fairness</b> <b>metric</b> of demographic parity, ... All of them have a <b>p value</b> smaller than 0.01 (figure 14-16). Ultimately, the videos until the 10th cycle seem to have a decreasing bias score which contradicts the echo-chamber theory. The expectation would be that if YouTube were to be using the echo chamber theory, the more videos we would click on, the more biased the videos would become. However, there does seem to be a difference in the minimising effect between categories. At the ...", "dateLastCrawled": "2021-12-30T05:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Testing for Measurement <b>Invariance</b> in R | by Okan Bulut | Towards Data ...", "url": "https://towardsdatascience.com/testing-for-measurement-invariance-in-r-b44cace10148", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/testing-for-measurement-<b>invariance</b>-in-r-b44cace10148", "snippet": "By reviewing the <b>p.value</b> column, we <b>can</b> identify the parameters that are expected to have a significant impact on model fit (i.e., those with p&lt; .05). For example, these are some influential parameters:.p24. == .p59..p26. == .p61..p31. == .p66. With the list of potential parameters to adjust, we <b>can</b> make changes in the scalar model. However, we ...", "dateLastCrawled": "2022-01-31T19:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Chapter 6 Studying relationships between a categorical and a ...", "url": "https://jjmedinaariza.github.io/modelling_book/studying-relationships-between-a-categorical-and-a-quantitative-variable.html", "isFamilyFriendly": true, "displayUrl": "https://jjmedinaariza.github.io/modelling_book/studying-relationships-between-a...", "snippet": "\u201cWhen the <b>p value</b> is high, then we <b>can</b> conclude that we have not seeing anything unusual. Events that have a high probability of happening happen often. The data are thus consistent with the model from the null hypothesis, and we have no reason to reject the null hypothesis. But we realize many other similar hypotheses could also account for the data we\u2019ve seen, so we haven\u2019t proven that the null hypothesis is true. The most we <b>can</b> say is that it doesn\u2019t appear to be false. Formally ...", "dateLastCrawled": "2022-01-31T13:53:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Fairness</b> Metrics - Data Science, <b>Machine</b> <b>Learning</b>, AI", "url": "https://vitalflux.com/fairness-metrics-ml-model-sensitivity-bias-detection/", "isFamilyFriendly": true, "displayUrl": "https://vitalflux.com/<b>fairness-metrics-ml-model-sensitivity</b>-bias-detection", "snippet": "There are many different ways in which <b>machine</b> <b>learning</b> (ML) models\u2019 <b>fairness</b> could be determined. Some of them are statistical parity, the relative significance of features, model sensitivity etc. In this post, you would learn about how model sensitivity could be used to determine model <b>fairness</b> or bias of model towards the privileged or unprivileged group.The following are some of the topics covered in this post:", "dateLastCrawled": "2022-01-20T21:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A New <b>Metric</b> for Quantifying <b>Machine</b> <b>Learning</b> <b>Fairness</b> in Healthcare ...", "url": "https://towardsdatascience.com/a-new-metric-for-quantifying-machine-learning-fairness-in-healthcare-closedloop-ai-fc07b9c83487", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-new-<b>metric</b>-for-quantifying-<b>machine</b>-<b>learning</b>-<b>fairness</b>...", "snippet": "The <b>analogy</b> would be the difference between a Pearson correlation or a residual sum of squared errors in regression. While both quantify the models performance, the former is significantly easier to understand and explain; unsurprisingly, it is the <b>metric</b> most individuals use to describe a regression model. In order to achieve this effect, many <b>fairness</b> metrics are presented as the quotient of a protected subgroup to a base subgroup[2]. As the goal of healthcare is to deliver interventions ...", "dateLastCrawled": "2022-01-17T14:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A Human-centric Approach to <b>Fairness</b> in AI", "url": "https://www.timlrx.com/blog/a-human-centric-approach-to-fairness-in-ai", "isFamilyFriendly": true, "displayUrl": "https://www.timlrx.com/blog/a-human-centric-approach-to-<b>fairness</b>-in-ai", "snippet": "The case for a single <b>fairness</b> <b>metric</b>. Having a single <b>metric</b> to quantify what is fair or not seems to be a goal for many AI scholars in the field, as well as many of industry practitioners. After all, while methods might vary, the foundation of <b>machine</b> <b>learning</b> hinges on maximising or minimising clear objectives 3. A business problem can then be translated to a mathematical / statistical / computational problem and different methods can be compared against each other based on how far the ...", "dateLastCrawled": "2022-02-03T02:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A New <b>Metric</b> for Quantifying <b>Machine</b> <b>Learning</b> <b>Fairness</b> in Healthcare", "url": "https://www.closedloop.ai/post/a-new-metric-for-quantifying-machine-learning-fairness-in-healthcare", "isFamilyFriendly": true, "displayUrl": "https://www.closedloop.ai/post/a-new-<b>metric</b>-for-quantifying-<b>machine</b>-<b>learning</b>-<b>fairness</b>...", "snippet": "A New <b>Metric</b> for Quantifying <b>Machine</b> <b>Learning</b> <b>Fairness</b> in Healthcare. Joseph Gartner. March 2, 2020. Background . Several recent, high profile cases of unfair AI algorithms have highlighted the vital need to address bias early in the development of any AI system. For the most part, bias does not come into algorithms due to malicious intent by the individual creating the algorithm. Bias comes from a lack of diligence in ensuring that the AI system is fair for everyone. In order to combat bias ...", "dateLastCrawled": "2022-02-01T00:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "RStudio AI Blog: Starting to think about AI <b>Fairness</b>", "url": "https://blogs.rstudio.com/ai/posts/2021-07-15-ai-fairness/", "isFamilyFriendly": true, "displayUrl": "https://blogs.rstudio.com/ai/posts/2021-07-15-ai-<b>fairness</b>", "snippet": "Papers on <b>fairness</b> in <b>machine</b> <b>learning</b>, as is common in fields like computer science, abound with formulae. Even the papers referenced here, though selected not for their theorems and proofs but for the ideas they harbor, are no exception. But to start thinking about <b>fairness</b> as it might apply to an ML process at hand, common language \u2013 and common sense \u2013 will do just fine. If, after analyzing your use case, you judge that the more technical results are relevant to the process in ...", "dateLastCrawled": "2022-01-31T09:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Classification - <b>Fairness</b> and <b>machine</b> <b>learning</b>", "url": "https://fairmlbook.org/classification.html", "isFamilyFriendly": true, "displayUrl": "https://fairmlbook.org/classification.html", "snippet": "Simply put, the goal of classification is to determine a plausible value for an unknown variable Y given an observed variable X.For example, we might try to predict whether a loan applicant will pay back her loan by looking at various characteristics such as credit history, income, and net worth. Classification also applies in situations where the variable Y does not refer to an event that lies in the future. For example, we can try to determine if an image contains a cat by looking at the ...", "dateLastCrawled": "2022-02-03T07:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "The <b>measure and mismeasure of fairness: a critical review</b> of fair ...", "url": "https://blog.acolyer.org/2020/02/03/measure-mismeasure-fairness/", "isFamilyFriendly": true, "displayUrl": "https://blog.acolyer.org/2020/02/03/measure-mismeasure-<b>fairness</b>", "snippet": "One premise of many models of <b>fairness</b> in <b>machine</b> <b>learning</b> is that you can measure (\u2018prove\u2019) <b>fairness</b> of a <b>machine</b> <b>learning</b> model from within the system \u2013 i.e. from properties of the model itself and perhaps the data it is trained on. Beyond the questions of whether any one model of <b>fairness</b> is better or worse than another, I\u2019m coming to the realisation that this doesn\u2019t hold. To show that a <b>machine</b> <b>learning</b> model is fair, you need information from", "dateLastCrawled": "2022-01-30T11:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Fairness</b> through awareness", "url": "https://www.cs.toronto.edu/~toni/Papers/awareness.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.toronto.edu/~toni/Papers/awareness.pdf", "snippet": "based <b>fairness</b>, we assume a distance <b>metric</b> that de nes the similarity between the individuals. This is the source of \\awareness&quot; in the title of this paper. We formalize this guiding principle as a Lipschitz condition on the classi er. In our approach a classi er is a randomized mapping from individuals to outcomes, or equivalently, a mapping from individuals to distributions over outcomes. The Lipschitz condition requires that any two individuals x;ythat are at distance d(x;y) 2[0;1] map ...", "dateLastCrawled": "2021-12-06T13:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "An evaluation of scanpath-comparison and <b>machine</b>-<b>learning</b> ...", "url": "https://link.springer.com/article/10.3758/s13428-016-0788-z", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.3758/s13428-016-0788-z", "snippet": "In recent years, eyetracking has begun to be used to study the dynamics of <b>analogy</b> making. Numerous scanpath-comparison algorithms and <b>machine</b>-<b>learning</b> techniques are available that can be applied to the raw eyetracking data. We show how scanpath-comparison algorithms, combined with multidimensional scaling and a classification algorithm, can be used to resolve an outstanding question in <b>analogy</b> making\u2014namely, whether or not children\u2019s and adults\u2019 strategies in solving <b>analogy</b> problems ...", "dateLastCrawled": "2021-11-05T03:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Fairness</b> Through Awareness", "url": "https://www.cs.toronto.edu/~zemel/documents/fairAwareItcs2012.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.toronto.edu/~zemel/documents/fairAwareItcs2012.pdf", "snippet": "<b>Fairness</b> Through Awareness Cynthia Dwork Moritz Hardty Toniann Pitassiz Omer Reingoldx Richard Zemel{November 29, 2011 Abstract We study <b>fairness</b> in classi\ufb01cation, where individuals are classi\ufb01ed, e.g., admitted to a uni-versity, and the goal is to prevent discrimination against individuals based on their membership in some group, while maintaining utility for the classi\ufb01er (the university). The main conceptual contribution of this paper is a framework for fair classi\ufb01cation ...", "dateLastCrawled": "2022-01-29T03:48:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "FairFed: Enabling Group Fairness in Federated <b>Learning</b> | DeepAI", "url": "https://deepai.org/publication/fairfed-enabling-group-fairness-in-federated-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/fairfed-enabling-group-fairness-in-federated-<b>learning</b>", "snippet": "As <b>machine</b> <b>learning</b> becomes increasingly incorporated in crucial decision-making scenarios such as healthcare, recruitment, and loan assessment, there have been increasing concerns about the privacy and fairness of such systems. Federated <b>learning</b> has been viewed as a promising solution for collaboratively <b>learning</b> <b>machine</b> <b>learning</b> models among multiple parties while maintaining the privacy of their local data.", "dateLastCrawled": "2022-01-12T21:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "\u7ffb\u8a33\u7d50\u679c: FairFed: Enabling Group Fairness in Federated <b>Learning</b>", "url": "https://fugumt.com/fugumt/paper/translated/2110.00857.pdf.html", "isFamilyFriendly": true, "displayUrl": "https://fugumt.com/fugumt/paper/translated/2110.00857.pdf.html", "snippet": "As <b>machine</b> <b>learning</b> becomes increasingly incorporated in crucial decision-making scenarios such as healthcare, recruitment, and loan assessment, there have been increasing concerns about the privacy and fairness of such systems. Federated <b>learning</b> has been viewed as a promising solution for collaboratively <b>learning</b> <b>machine</b> <b>learning</b> models among multiple parties while maintaining the privacy of their local data. However, federated <b>learning</b> also poses new challenges in mitigating the potential ...", "dateLastCrawled": "2021-10-06T05:45:00.0000000Z", "language": "ja", "isNavigational": false}], [], [], []], "all_bing_queries": ["+(fairness metric)  is like +(p-value)", "+(fairness metric) is similar to +(p-value)", "+(fairness metric) can be thought of as +(p-value)", "+(fairness metric) can be compared to +(p-value)", "machine learning +(fairness metric AND analogy)", "machine learning +(\"fairness metric is like\")", "machine learning +(\"fairness metric is similar\")", "machine learning +(\"just as fairness metric\")", "machine learning +(\"fairness metric can be thought of as\")", "machine learning +(\"fairness metric can be compared to\")"]}