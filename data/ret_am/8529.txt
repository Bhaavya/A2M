{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Bring your monorepo down to size with <b>sparse-checkout</b> | The GitHub Blog", "url": "https://github.blog/2020-01-17-bring-your-monorepo-down-to-size-with-sparse-checkout/", "isFamilyFriendly": true, "displayUrl": "https://github.blog/2020-01-17-bring-your-monorepo-down-to-size-with-<b>sparse-checkout</b>", "snippet": "Note that this <b>number</b> is very <b>small</b>, so imagine adding an extra three zeroes to the end of the numbers. $ ls bootstrap.sh* client/ LICENSE.md README.md service/ web/ $ find . -type f | wc -l 1590 . To get started with the <b>sparse-checkout</b> <b>feature</b>, you can run git <b>sparse-checkout</b> init --cone to restrict the working directory to only the files at root (and in the .git directory). $ git <b>sparse-checkout</b> init --cone $ ls bootstrap.sh* LICENSE.md README.md $ find . -type f | wc -l 37. A developer ...", "dateLastCrawled": "2022-01-30T12:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "What Is <b>Sparse</b> Linear Regression? \u2013 chetumenu.com", "url": "https://chetumenu.com/what-is-sparse-linear-regression/", "isFamilyFriendly": true, "displayUrl": "https://chetumenu.com/what-is-<b>sparse</b>-linear-regression", "snippet": "What is <b>sparse</b> linear regression? <b>Sparse</b> linear regression is the well-studied inference problem where one is given a design matrix \\mathbfA \\in \\mathbbR^M\\times N and a response vector \\mathbfb \\in \\mathbbR^M, and the goal is to find a solution \\mathbfx \\in \\mathbbR^N which is k-<b>sparse</b> (that is, it has at most k non-zero coordinates). Subsequently, What is a <b>sparse</b> regression model? A regression vector is <b>sparse</b> if only some of its components are nonzero while the rest is set equal to zero ...", "dateLastCrawled": "2022-01-16T08:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "UNSUPERVISED <b>FEATURE</b> LEARNING VIA <b>SPARSE</b> HIERARCHICAL REPRESENTATIONS A ...", "url": "https://web.eecs.umich.edu/~honglak/thesis_final.pdf", "isFamilyFriendly": true, "displayUrl": "https://web.eecs.umich.edu/~honglak/thesis_final.pdf", "snippet": "UNSUPERVISED <b>FEATURE</b> LEARNING VIA <b>SPARSE</b> HIERARCHICAL REPRESENTATIONS A DISSERTATION SUBMITTED TO THE DEPARTMENT OF COMPUTER SCIENCE AND THE COMMITTEE ON GRADUATE STUDIES OF STANFORD UNIVERSITY IN PARTIAL FULFILLMENT OF THE REQUIREMENTS FOR THE DEGREE OF DOCTOR OF PHILOSOPHY Honglak Lee August 2010. Abstract Machine learning has proved a powerful tool for arti\ufb01cial intelligence and data mining problems. However, its success has usually relied on <b>having</b> a good <b>feature</b> representa-tion of the ...", "dateLastCrawled": "2022-01-26T02:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Unsupervised <b>feature</b> learning based on <b>sparse</b> coding and spectral ...", "url": "https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/iet-cvi.2014.0295", "isFamilyFriendly": true, "displayUrl": "https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/iet-cvi.2014.0295", "snippet": "Unsupervised <b>feature</b> learning based on <b>sparse</b> coding and spectral clustering for segmentation of synthetic aperture radar images. Masoumeh Rahmani, Masoumeh Rahmani. Department of Electrical Engineering, Faculty of Engineering, Shahid Chamran University of Ahvaz (SCU), Ahvaz, 61357-83151 Iran. Search for more papers by this author. Gholamreza Akbarizadeh, Corresponding Author. Gholamreza Akbarizadeh. g.akbari@scu.ac.ir; Department of Electrical Engineering, Faculty of Engineering, Shahid ...", "dateLastCrawled": "2022-01-10T22:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Introduction to NLP - Part 1: <b>Preprocessing</b> text in Python | by Zolzaya ...", "url": "https://towardsdatascience.com/introduction-to-nlp-part-1-preprocessing-text-in-python-8f007d44ca96", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/introduction-to-nlp-part-1-<b>preprocessing</b>-text-in-python...", "snippet": "The output <b>feature</b> matrix will be in <b>sparse</b>_matrix form. Let\u2019s convert it to a dataframe with proper column names to make it more readible: # Convert <b>sparse</b> matrix to dataframe X_train = pd.DataFrame.<b>sparse</b>.from_spmatrix(X_train) # Save mapping on which index refers to which terms col_map = {v:k for k, v in vectoriser.vocabulary_.items()} # Rename each column using the mapping for col in X_train.columns: X_train.rename(columns={col: col_map[col]}, inplace=True) X_train. count matrix (not ...", "dateLastCrawled": "2022-01-27T23:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Rural</b> and urban areas: comparing lives using <b>rural</b>/urban ...", "url": "https://link.springer.com/article/10.1057/rt.2011.2", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1057/rt.2011.2", "snippet": "Differences between <b>sparse</b> and less <b>sparse</b> areas are very <b>small</b>, however, with no difference between the two groups in the rates of employed adults with the highest level of qualifications. Health One key indicator of health in an area is life expectancy, which is an estimate of the <b>number</b> of years that a person can expect to live, on average, in a given population.", "dateLastCrawled": "2022-02-02T17:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>What is &#39;sparse reinforcement learning</b>&#39;? - Quora", "url": "https://www.quora.com/What-is-sparse-reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>What-is-sparse-reinforcement-learning</b>", "snippet": "Answer (1 of 2): To fully define <b>sparse</b> reinforcement learning, some background is needed. So I\u2019ll build the answer progressively starting from the necessary background up to the definition. In reinforcement learning problems where the state space S is infinite or finite but enormously large, it...", "dateLastCrawled": "2022-01-22T10:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Find close pairs in very high dimensional space with <b>sparse</b> vectors", "url": "https://stats.stackexchange.com/questions/171947/find-close-pairs-in-very-high-dimensional-space-with-sparse-vectors", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/171947", "snippet": "It looks <b>like</b> the approach you&#39;re looking for is a combination of minhash signatures and Locality Sensitive Hashing (LSH); the (freely available) pdf of Mining Massive Datasets describes this approach (and other similarity measures) in some detail in Chapter 3, but briefly:. A minhash signature is a condensed representation of your original matrix that is constructed by applying some <b>number</b> n of hash functions to features, thereby reducing the <b>number</b> of features per observation. This reduces ...", "dateLastCrawled": "2022-01-09T01:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Using <b>ColumnTransformer</b> to combine data processing steps | by Allison ...", "url": "https://towardsdatascience.com/using-columntransformer-to-combine-data-processing-steps-af383f7d5260", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/using-<b>columntransformer</b>-to-combine-data-processing...", "snippet": "&lt;465x30 <b>sparse</b> matrix of type &#39;&lt;class &#39;numpy.float64&#39;&gt;&#39; with 2325 stored elements in Compressed <b>Sparse</b> Row format&gt; More likely, you\u2019ll add the <b>ColumnTransformer</b> as a step in your Pipeline: lr = LinearRegression() pipe = Pipeline([(&quot;preprocessing&quot;, col_transformer), (&quot;lr&quot;, lr)]) pipe.fit(X_train, y_train) And now your pipe is ready to make predictions! Or to be used in cross validation without leaking information across slices. Note that we need to indicate the column in the format expected ...", "dateLastCrawled": "2022-01-27T23:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Uninformative or sparse patch notes infuriate me</b> | ResetEra", "url": "https://www.resetera.com/threads/uninformative-or-sparse-patch-notes-infuriate-me.417021/", "isFamilyFriendly": true, "displayUrl": "https://www.resetera.com/threads/<b>uninformative-or-sparse-patch-notes-infuriate-me</b>.417021", "snippet": "I mean there is a balance between showing all changes and tagging <b>a small</b> <b>number</b> of notable efforts and bigger improvements for each release. Something <b>like</b> &quot;Revised assets to reduce possible collision errors&quot; can aggregate a ton of bug fixes. The OP is being a drunkard by asking for what is essentially direct access to internal process tools, but patch notes are always good i think. There are other arguments for not <b>having</b> patch notes, but implementing a kind of relatively low effort ...", "dateLastCrawled": "2021-11-06T20:16:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Probabilistic Sparse Matrix Factorization</b> - ResearchGate", "url": "https://www.researchgate.net/publication/240191894_Probabilistic_Sparse_Matrix_Factorization", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/240191894_Probabilistic_<b>Sparse</b>_Matrix...", "snippet": "Fisher local factorization (Wang et al., 2004;Li et al., 2001) may be used when dependency of a new <b>feature</b> is constrained to a given <b>small</b> <b>number</b> of original features. ... NIMFA: A Python Library ...", "dateLastCrawled": "2021-12-26T06:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "UNSUPERVISED <b>FEATURE</b> LEARNING VIA <b>SPARSE</b> HIERARCHICAL REPRESENTATIONS A ...", "url": "https://web.eecs.umich.edu/~honglak/thesis_final.pdf", "isFamilyFriendly": true, "displayUrl": "https://web.eecs.umich.edu/~honglak/thesis_final.pdf", "snippet": "UNSUPERVISED <b>FEATURE</b> LEARNING VIA <b>SPARSE</b> HIERARCHICAL REPRESENTATIONS A DISSERTATION SUBMITTED TO THE DEPARTMENT OF COMPUTER SCIENCE AND THE COMMITTEE ON GRADUATE STUDIES OF STANFORD UNIVERSITY IN PARTIAL FULFILLMENT OF THE REQUIREMENTS FOR THE DEGREE OF DOCTOR OF PHILOSOPHY Honglak Lee August 2010. Abstract Machine learning has proved a powerful tool for arti\ufb01cial intelligence and data mining problems. However, its success has usually relied on <b>having</b> a good <b>feature</b> representa-tion of the ...", "dateLastCrawled": "2022-01-26T02:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Robust graph regularised <b>sparse</b> matrix regression for two\u2010dimensional ...", "url": "https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/iet-ipr.2019.1404", "isFamilyFriendly": true, "displayUrl": "https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/iet-ipr.2019.1404", "snippet": "Since the aim of <b>feature</b> selection is to find a compact representation, the <b>number</b> of selected features is generally limited to <b>a small</b> range, instead of displaying all features. Each <b>feature</b> selection algorithm is first performed on the training data set to determine the selected <b>feature</b>. Then, a classifier is trained with training samples containing only the selected features. Moreover, the learned classifier is used to classify the testing samples with the selected features. Repeat this ...", "dateLastCrawled": "2021-12-06T05:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "LARGE SCALE AND <b>SPARSE</b> MINIMAL COMPLEXITY MACHINES", "url": "http://eprint.iitd.ac.in/bitstream/handle/2074/8340/TH-6106.pdf?sequence=1", "isFamilyFriendly": true, "displayUrl": "eprint.iitd.ac.in/bitstream/handle/2074/8340/TH-6106.pdf?sequence=1", "snippet": "Chandra for his guidance; <b>friends</b> and colleagues Sumit Soman, Himanshu Pant, Prashant Gupta, Aashi Jindal; department staff members Rakesh, Yatindra, Mukesh and Ritwick; my parents, brother and especially my fianc\u00e9e Ms. Divya Vatsa for supporting me throughout this journey. (Mayank Sharma) iii. Abstract The capacity of a learning machine is characterized by various measures such as the Vapnik-Chervonenkis (VC) dimension, Rademacher complexity, and covering numbers. All these measures define ...", "dateLastCrawled": "2022-01-05T12:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Find close pairs in very high dimensional space with <b>sparse</b> vectors", "url": "https://stats.stackexchange.com/questions/171947/find-close-pairs-in-very-high-dimensional-space-with-sparse-vectors", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/171947", "snippet": "The <b>number</b> of such pairs is of a <b>similar</b> magnitude to N (~a million). I think this could be approached as looking for close point pairs in a very high-dimensional space. The distance function could be such that it&#39;s based on how many features the two vectors have in common. But it would probably be useful with a more conventional distance ...", "dateLastCrawled": "2022-01-09T01:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Machine Learning</b> Glossary | <b>Google Developers</b>", "url": "https://developers.google.com/machine-learning/glossary/", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine-learning</b>/glossary", "snippet": "<b>sparse</b> <b>feature</b>. <b>Feature</b> vector whose values are predominately zero or empty. For example, a vector containing a single 1 value and a million 0 values is <b>sparse</b>. As another example, words in a search query could also be a <b>sparse</b> <b>feature</b>\u2014there are many possible words in a given language, but only a few of them occur in a given query.", "dateLastCrawled": "2022-02-03T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Semistatic and <b>sparse</b> variance\u2010optimal hedging - Di Tella - 2020 ...", "url": "https://onlinelibrary.wiley.com/doi/10.1111/mafi.12235", "isFamilyFriendly": true, "displayUrl": "https://onlinelibrary.wiley.com/doi/10.1111/mafi.12235", "snippet": "It turns out that this problem of finding a <b>sparse</b> semistatic hedging strategy is closely related to the well-known problem of variable selection in high-dimensional regression (cf. Hastie, Tibshirani, &amp; Friedman, 2013, section 3.3), and more generally to <b>sparse</b> modeling approaches in statistics and machine learning. 1 Indeed, to solve the problem of optimal selection we will draw from methods developed in statistics, such as the LASSO, greedy forward selection and the method of Leaps-and ...", "dateLastCrawled": "2022-02-02T10:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Rural</b> and urban areas: comparing lives using <b>rural</b>/urban ...", "url": "https://link.springer.com/article/10.1057/rt.2011.2", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1057/rt.2011.2", "snippet": "Differences between <b>sparse</b> and less <b>sparse</b> areas are very <b>small</b>, however, with no difference between the two groups in the rates of employed adults with the highest level of qualifications. Health One key indicator of health in an area is life expectancy, which is an estimate of the <b>number</b> of years that a person can expect to live, on average, in a given population.", "dateLastCrawled": "2022-02-02T17:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Uninformative or sparse patch notes infuriate me</b> | ResetEra", "url": "https://www.resetera.com/threads/uninformative-or-sparse-patch-notes-infuriate-me.417021/", "isFamilyFriendly": true, "displayUrl": "https://www.resetera.com/threads/<b>uninformative-or-sparse-patch-notes-infuriate-me</b>.417021", "snippet": "I mean there is a balance between showing all changes and tagging <b>a small</b> <b>number</b> of notable efforts and bigger improvements for each release. Something like &quot;Revised assets to reduce possible collision errors&quot; can aggregate a ton of bug fixes. The OP is being a drunkard by asking for what is essentially direct access to internal process tools, but patch notes are always good i think. There are other arguments for not <b>having</b> patch notes, but implementing a kind of relatively low effort ...", "dateLastCrawled": "2021-11-06T20:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Creating a Simple Recommender System in Python using Pandas", "url": "https://stackabuse.com/creating-a-simple-recommender-system-in-python-using-pandas/", "isFamilyFriendly": true, "displayUrl": "https://stackabuse.com/creating-a-simple-recommender-system-in-python-using-pandas", "snippet": "The dataset that we are going to use for this problem is the MovieLens Dataset. To download the dataset, go the home page of the dataset and download the &quot;ml-latest-<b>small</b>.zip&quot; file, which contains a subset of the actual movie dataset and contains 100000 ratings for 9000 movies by 700 users.. Once you unzip the downloaded file, you will see &quot;links.csv&quot;, &quot;movies.csv&quot;, &quot;ratings.csv&quot; and &quot;tags.csv&quot; files, along with the &quot;README&quot; document.", "dateLastCrawled": "2022-02-03T07:51:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Nicolaides-Baraitser syndrome | Genetic and Rare Diseases Information ...", "url": "https://rarediseases.info.nih.gov/diseases/270/intellectual-disability-sparse-hair-brachydactyly-syndrome", "isFamilyFriendly": true, "displayUrl": "https://<b>rarediseases.info.nih.gov</b>/diseases/270/intellectual-disability-<b>sparse</b>-hair-br...", "snippet": "<b>Sparse</b> scalp hair is a major <b>feature</b> of NCBRS and is present in almost all affected people. It often gradually worsens with age, but in some people it improves over time. Skin is usually wrinkled and more noticeable in the distal limbs. Teeth may be widely spaced, and eruption of teeth (baby or adult) may be delayed. While the hands and feet usually appear normal at birth, the interphalangeal joints become prominent in the majority of affected people. Bone age <b>can</b> vary, and osteoporosis is ...", "dateLastCrawled": "2022-01-27T18:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Lecture 14: Learning: Sparse Spaces, Phonology</b> | Lecture Videos ...", "url": "https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-034-artificial-intelligence-fall-2010/lecture-videos/lecture-14-learning-sparse-spaces-phonology/", "isFamilyFriendly": true, "displayUrl": "https://<b>ocw.mit.edu</b>/.../lecture-videos/<b>lecture-14-learning-sparse-spaces-phonology</b>", "snippet": "It was so <b>small</b>. It was of little worth. These ideas like neural nets, genetic algorithms, I classify them as pistareens because getting them to do something is rather like getting a dog to walk on its hind legs. You <b>can</b> make it happen, but they never do it very well. And you have to think it took a lot of trickery and training to make it happen. So not too personally high on those ideas. But we teach them to you anyway because, of course, we only editorialize part of time and part of time ...", "dateLastCrawled": "2022-02-02T16:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Smarter Ways to Encode Categorical Data for Machine Learning | by Jeff ...", "url": "https://towardsdatascience.com/smarter-ways-to-encode-categorical-data-for-machine-learning-part-1-of-3-6dca2f71b159", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/smarter-ways-to-encode-categorical-data-for-machine...", "snippet": "<b>Sparse</b> data is a matrix with lots of zeroes relative to other values. If your encoders transform your data so that it becomes <b>sparse</b>, some algorithms may not work well. Sparsity <b>can</b> often be managed by flagging it, but many algorithms don\u2019t work well unless the data is dense. <b>Sparse</b> Digging Into Category Encoders. Without further ado, let\u2019s encode! Ordinal. OrdinalEncoder converts each string value to a whole <b>number</b>. The first unique value in your column becomes 1, the second becomes 2 ...", "dateLastCrawled": "2022-01-30T19:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Using <b>ColumnTransformer</b> to combine data processing steps | by Allison ...", "url": "https://towardsdatascience.com/using-columntransformer-to-combine-data-processing-steps-af383f7d5260", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/using-<b>columntransformer</b>-to-combine-data-processing...", "snippet": "Full disclosure: we\u2019re just going to use <b>a small</b> portion of the data set today. ... &lt;465x30 <b>sparse</b> matrix of type &#39;&lt;class &#39;numpy.float64&#39;&gt;&#39; with 2325 stored elements in Compressed <b>Sparse</b> Row format&gt; More likely, you\u2019ll add the <b>ColumnTransformer</b> as a step in your Pipeline: lr = LinearRegression() pipe = Pipeline([(&quot;preprocessing&quot;, col_transformer), (&quot;lr&quot;, lr)]) pipe.fit(X_train, y_train) And now your pipe is ready to make predictions! Or to be used in cross validation without leaking ...", "dateLastCrawled": "2022-01-27T23:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Find close pairs in very high dimensional space with <b>sparse</b> vectors", "url": "https://stats.stackexchange.com/questions/171947/find-close-pairs-in-very-high-dimensional-space-with-sparse-vectors", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/171947", "snippet": "The <b>number</b> of such pairs is of a similar magnitude to N (~a million). I think this could be approached as looking for close point pairs in a very high-dimensional space. The distance function could be such that it&#39;s based on how many features the two vectors have in common. But it would probably be useful with a more conventional distance ...", "dateLastCrawled": "2022-01-09T01:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Nuit Blanche: <b>Sparse</b> Proteomics Analysis - A compressed sensing-based ...", "url": "https://nuit-blanche.blogspot.com/2015/06/sparse-proteomics-analysis-compressed.html", "isFamilyFriendly": true, "displayUrl": "https://nuit-blanche.blogspot.com/2015/06/<b>sparse</b>-proteomics-analysis-compressed.html", "snippet": "<b>Sparse</b> Proteomics Analysis - A compressed sensing-based approach for <b>feature</b> selection and classification of high-dimensional proteomics mass spectrometry data - implementation - The knowledge generated by the genome only makes sense if one <b>can</b> do a good job at figuring out what sort of proteins are producing by different elements of the DNA. Hence, while GWAS studies are great and the alignement issues in sequencing is becoming easier to handle, the big unknown is now to connect information ...", "dateLastCrawled": "2022-01-06T17:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>What is &#39;sparse reinforcement learning</b>&#39;? - Quora", "url": "https://www.quora.com/What-is-sparse-reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>What-is-sparse-reinforcement-learning</b>", "snippet": "Answer (1 of 2): To fully define <b>sparse</b> reinforcement learning, some background is needed. So I\u2019ll build the answer progressively starting from the necessary background up to the definition. In reinforcement learning problems where the state space S is infinite or finite but enormously large, it...", "dateLastCrawled": "2022-01-22T10:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>GPU Kernels for Block-Sparse Weights</b> - SlideShare", "url": "https://www.slideshare.net/WillyDevNET/gpu-kernels-for-blocksparse-weights", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/WillyDevNET/<b>gpu-kernels-for-blocksparse-weights</b>", "snippet": "Given the right block-<b>sparse</b> connectivity, such networks <b>can</b> fully mix within a relatively <b>small</b> <b>number</b> of layers when combined with <b>a small</b>-world connectivity, as we will explain in section4.2. The LSTM architecture builds on the multiplicative LSTM (mSLTM) architecture proposed in [Krause et al., 2016], and utilizes layer normalization [Ba et al., 2016] for improved ef\ufb01ciency. Adding internal depth is a good way to increase parameter ef\ufb01ciency, even in the dense case; see \ufb01gure 8 in ...", "dateLastCrawled": "2022-01-10T05:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Uninformative or sparse patch notes infuriate me</b> | ResetEra", "url": "https://www.resetera.com/threads/uninformative-or-sparse-patch-notes-infuriate-me.417021/", "isFamilyFriendly": true, "displayUrl": "https://www.resetera.com/threads/<b>uninformative-or-sparse-patch-notes-infuriate-me</b>.417021", "snippet": "I mean there is a balance between showing all changes and tagging <b>a small</b> <b>number</b> of notable efforts and bigger improvements for each release. Something like &quot;Revised assets to reduce possible collision errors&quot; <b>can</b> aggregate a ton of bug fixes. The OP is being a drunkard by asking for what is essentially direct access to internal process tools, but patch notes are always good i think. There are other arguments for not <b>having</b> patch notes, but implementing a kind of relatively low effort ...", "dateLastCrawled": "2021-11-06T20:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>How is ARCore better than ARKit</b>?. In some ways, but not others | by ...", "url": "https://medium.com/6d-ai/how-is-arcore-better-than-arkit-5223e6b3e79d", "isFamilyFriendly": true, "displayUrl": "https://medium.com/6d-ai/<b>how-is-arcore-better-than-arkit</b>-5223e6b3e79d", "snippet": "It\u2019s one of those problems like VIO, that only a very <b>small</b> <b>number</b> of people <b>can</b> possibly solve. I only know of one unpublished (in stealth) system that reportedly <b>can</b> support consumer grade ...", "dateLastCrawled": "2022-01-31T20:37:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Optimal Sparse Descriptor Selection for QSAR Using</b> Bayesian Methods ...", "url": "https://onlinelibrary.wiley.com/doi/abs/10.1002/qsar.200810173", "isFamilyFriendly": true, "displayUrl": "https://onlinelibrary.wiley.com/doi/abs/10.1002/qsar.200810173", "snippet": "<b>Having</b> chosen the optimum descriptors in a supervised manner, we used a Bayesian regularized neural network to carry out nonlinear regression and derive robust parsimonious QSAR models for five drug data sets. Models were validated using independent test sets, and results <b>compared</b> with other contemporary descriptor selection methods. Issues around validating <b>small</b> QSAR data sets were also discussed in detail. The <b>sparse</b> <b>feature</b> selection algorithm proved to be an excellent, robust method for ...", "dateLastCrawled": "2021-11-06T04:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "LARGE SCALE AND <b>SPARSE</b> MINIMAL COMPLEXITY MACHINES", "url": "http://eprint.iitd.ac.in/bitstream/handle/2074/8340/TH-6106.pdf?sequence=1", "isFamilyFriendly": true, "displayUrl": "eprint.iitd.ac.in/bitstream/handle/2074/8340/TH-6106.pdf?sequence=1", "snippet": "spanning <b>small</b> and large datasets both in terms of the <b>number</b> of samples and classes. We show that the proposed multiclass MCM variants <b>can</b> retain accuracies even using 3 bits as <b>compared</b> to 12 bits of used by the SVM and the LS-SVM. vi Chapter 0 Mayank Sharma", "dateLastCrawled": "2022-01-05T12:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Probabilistic Sparse Matrix Factorization</b> - ResearchGate", "url": "https://www.researchgate.net/publication/240191894_Probabilistic_Sparse_Matrix_Factorization", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/240191894_Probabilistic_<b>Sparse</b>_Matrix...", "snippet": "An approach related to our work is the multi-domain CF method of [34]. In that paper, the authors propose a probabilistic framework which uses probabilistic matrix factorization (PMF) [26] to ...", "dateLastCrawled": "2021-12-26T06:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Smarter Ways to Encode Categorical Data for Machine Learning | by Jeff ...", "url": "https://towardsdatascience.com/smarter-ways-to-encode-categorical-data-for-machine-learning-part-1-of-3-6dca2f71b159", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/smarter-ways-to-encode-categorical-data-for-machine...", "snippet": "<b>Sparse</b> data is a matrix with lots of zeroes relative to other values. If your encoders transform your data so that it becomes <b>sparse</b>, some algorithms may not work well. Sparsity <b>can</b> often be managed by flagging it, but many algorithms don\u2019t work well unless the data is dense. <b>Sparse</b> Digging Into Category Encoders. Without further ado, let\u2019s encode! Ordinal. OrdinalEncoder converts each string value to a whole <b>number</b>. The first unique value in your column becomes 1, the second becomes 2 ...", "dateLastCrawled": "2022-01-30T19:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Rural</b> and urban areas: comparing lives using <b>rural</b>/urban ...", "url": "https://link.springer.com/article/10.1057/rt.2011.2", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1057/rt.2011.2", "snippet": "Differences between <b>sparse</b> and less <b>sparse</b> areas are very <b>small</b>, however, with no difference between the two groups in the rates of employed adults with the highest level of qualifications. Health One key indicator of health in an area is life expectancy, which is an estimate of the <b>number</b> of years that a person <b>can</b> expect to live, on average, in a given population.", "dateLastCrawled": "2022-02-02T17:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Development and application of a deep learning\u2013based <b>sparse</b> autoencoder ...", "url": "https://journals.sagepub.com/doi/10.1177/1475921718800363", "isFamilyFriendly": true, "displayUrl": "https://journals.sagepub.com/doi/10.1177/1475921718800363", "snippet": "where m is the <b>number</b> of samples, x (i) is the ith input, f (\u00b7) and g (\u00b7) are the encoder and decoder mapping functions, respectively. Considering the nonlinearity of the activation function as shown in equation (3), the gradient descent algorithm is commonly employed to perform the optimization.A typical utilization of the above autoencoder model is to reconstruct the original input as the output; however, if a distinct <b>feature</b> rather than the same as the input is obtained as the output ...", "dateLastCrawled": "2022-02-01T23:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Multi-criteria optimization of wireless connectivity over <b>sparse</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S1389128616302389", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1389128616302389", "snippet": "The <b>number</b> <b>of friends</b> provides no maximum bound, and is, in general, <b>a small</b> <b>number</b>, typically below 10. Therefore we need to compare the scanned <b>number</b> <b>of friends</b> to the total <b>number</b> <b>of friends</b> that we <b>can</b> connect to, based on the AP selection: (12) n u m <b>f r i e n d s</b> % = s <b>c a n</b> n e d n u m <b>f r i e n d s</b> t o t a l n u m <b>f r i e n d s</b>", "dateLastCrawled": "2021-11-22T23:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Efficient Depth Estimation Using <b>Sparse</b> Stereo-Vision with Other ...", "url": "https://www.intechopen.com/chapters/67181", "isFamilyFriendly": true, "displayUrl": "https://www.intechopen.com/chapters/67181", "snippet": "In such cases, custom image descriptors, where closest <b>feature</b> points <b>can</b> define the pixel of interest, <b>can</b> be used to overcome the above flaw. Getting this right is a challenge because the disparity between the two stereo images makes it a nontrivial problem to select the correct features to describe the pixel of interest. Since the introduced disparity <b>can</b> lead to some difference in the background of the two input images, hence not all features <b>can</b> be used to describe the pixel of interest.", "dateLastCrawled": "2022-01-29T03:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A collocation study of atmospheric motion vectors (AMVs) <b>compared</b> to ...", "url": "https://rmets.onlinelibrary.wiley.com/doi/full/10.1002/qj.4207", "isFamilyFriendly": true, "displayUrl": "https://rmets.onlinelibrary.wiley.com/doi/full/10.1002/qj.4207", "snippet": "A factor may be a categorical variable or a continuous variable that has been divided into <b>a small</b> <b>number</b> of intervals. The different factors and levels are defined as: Factor Method describes the method used to generate the AMV and has levels IR, Vis, WV, and Clear, indicating that IR, visible, cloudy WV or clear WV imagery was used.", "dateLastCrawled": "2022-01-12T19:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "To which extent are kernel methods better theoretically tractable ...", "url": "https://www.quora.com/To-which-extent-are-kernel-methods-better-theoretically-tractable-compared-to-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/To-which-extent-are-kernel-methods-better-theoretically...", "snippet": "Answer: There are several differences between kernel methods and neural networks, as far as theoretical understanding of these methods is concerned: 1. No. of parameters: First, the amount of parameter tuning required in kernel methods is far less than that in neural networks. For instance, with...", "dateLastCrawled": "2022-01-12T13:08:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine Learning by Analogy</b> - SlideShare", "url": "https://www.slideshare.net/ColleenFarrelly/machine-learning-by-analogy-59094152", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ColleenFarrelly/<b>machine-learning-by-analogy</b>-59094152", "snippet": "<b>Machine Learning by Analogy</b> 1. Colleen M. Farrelly 2. Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics behind these ...", "dateLastCrawled": "2022-01-31T07:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "TensorFlow is an interface for expressing <b>machine</b> <b>learning</b> algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PPT) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> | Colleen Farrelly - Academia.edu", "url": "https://www.academia.edu/30404581/Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/30404581/<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "<b>Machine</b> <b>Learning</b> by <b>Analogy</b> Colleen M. Farrelly Overview of Problem Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics ...", "dateLastCrawled": "2022-01-02T06:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Machine</b> <b>Learning</b> by <b>Analogy</b> II - slideshare.net", "url": "https://www.slideshare.net/ColleenFarrelly/machine-learning-by-analogy-ii", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ColleenFarrelly/<b>machine</b>-<b>learning</b>-by-<b>analogy</b>-ii", "snippet": "Updated <b>Machine</b> <b>Learning</b> by <b>Analogy</b> presentation that builds to more advanced methods (TensorFlow, geometry/topology-based methods...) and adds a section on ti\u2026", "dateLastCrawled": "2021-12-31T12:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "An E\ufb03cient <b>Sparse</b> Metric <b>Learning</b> in High ... - <b>Machine</b> <b>Learning</b>", "url": "http://machinelearning.org/archive/icml2009/papers/46.pdf", "isFamilyFriendly": true, "displayUrl": "<b>machinelearning</b>.org/archive/icml2009/papers/46.pdf", "snippet": "An E\ufb03cient <b>Sparse</b> Metric <b>Learning</b> in High-Dimensional Space via!1-Penalized Log-Determinant Regularization Guo-Jun Qi qi4@illinois.edu Depart. ECE, University of Illinois at Urbana-Champaign, 405 North Mathews Avenue, Urbana, IL 61801 USA Jinhui Tang, Zheng-Jun Zha, Tat-Seng Chua {tangjh, zhazj, chuats}@comp.nus.edu.sg School of Computing, National University of Singapore, Computing 1, 13 Computing Drive, Singapore 117417 Hong-Jiang Zhang hjzhang@microsoft.com Microsoft Advanced Technology ...", "dateLastCrawled": "2021-11-19T11:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "9.5 <b>Shapley</b> Values | Interpretable <b>Machine</b> <b>Learning</b>", "url": "https://christophm.github.io/interpretable-ml-book/shapley.html", "isFamilyFriendly": true, "displayUrl": "https://christophm.github.io/interpretable-ml-book/<b>shapley</b>.html", "snippet": "Let us reuse the game <b>analogy</b>: We start with an empty team, add the <b>feature</b> value that would contribute the most to the prediction and iterate until all <b>feature</b> values are added. How much each <b>feature</b> value contributes depends on the respective <b>feature</b> values that are already in the \u201cteam\u201d, which is the big drawback of the breakDown method. It is faster than the <b>Shapley</b> value method, and for models without interactions, the results are the same.", "dateLastCrawled": "2022-02-02T05:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Introduction to Matrices and Matrix Arithmetic for <b>Machine Learning</b>", "url": "https://machinelearningmastery.com/introduction-matrices-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/introduction-matrices-<b>machine-learning</b>", "snippet": "The geometric <b>analogy</b> used to help understand vectors and some of their operations does not hold with matrices. Further, a vector itself may be considered a matrix with one column and multiple rows. Often the dimensions of the matrix are denoted as m and n for the number of rows and the number of columns. Now that we know what a matrix is, let\u2019s look at defining one in Python. Defining a Matrix. We can represent a matrix in Python using a two-dimensional NumPy array. A NumPy array can be ...", "dateLastCrawled": "2022-02-02T11:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Lecture 4: \\(k\\)-Nearest Neighbours and SVM RBFs \u2014 CPSC 330 Applied ...", "url": "https://ubc-cs.github.io/cpsc330/lectures/04_kNNs-SVM-RBF.html", "isFamilyFriendly": true, "displayUrl": "https://ubc-cs.github.io/cpsc330/lectures/04_kNNs-SVM-RBF.html", "snippet": "<b>Analogy</b>-based models ... It does not work well on datasets with many features or where most <b>feature</b> values are 0 most of the time (<b>sparse</b> datasets). Attention. For regular \\(k\\) -NN for supervised <b>learning</b> (not with <b>sparse</b> matrices), you should scale your features. We\u2019ll be looking into it soon. Parametric vs non parametric\u00b6 You might see a lot of definitions of these terms. A simple way to think about this is: do you need to store at least \\(O(n)\\) worth of stuff to make predictions? If ...", "dateLastCrawled": "2022-01-11T11:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Machine learning MCQs</b> | T4Tutorials.com", "url": "https://t4tutorials.com/machine-learning-mcqs/", "isFamilyFriendly": true, "displayUrl": "https://t4tutorials.com/<b>machine-learning-mcqs</b>", "snippet": "<b>Machine learning MCQs</b>. 1. The general concept and process of forming definitions from examples of concepts to be learned. E. All of these. F. None of these. 2. The computer is the best <b>learning</b> for.", "dateLastCrawled": "2022-01-30T16:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "9.6 <b>SHAP</b> (SHapley Additive exPlanations) | Interpretable <b>Machine</b> <b>Learning</b>", "url": "https://christophm.github.io/interpretable-ml-book/shap.html", "isFamilyFriendly": true, "displayUrl": "https://christophm.github.io/interpretable-ml-book/<b>shap</b>.html", "snippet": "9.6 <b>SHAP</b> (SHapley Additive exPlanations). This chapter is currently only available in this web version. ebook and print will follow. <b>SHAP</b> (SHapley Additive exPlanations) by Lundberg and Lee (2017) 69 is a method to explain individual predictions. <b>SHAP</b> is based on the game theoretically optimal Shapley values.. There are two reasons why <b>SHAP</b> got its own chapter and is not a subchapter of Shapley values.First, the <b>SHAP</b> authors proposed KernelSHAP, an alternative, kernel-based estimation ...", "dateLastCrawled": "2022-02-03T01:34:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(sparse feature)  is like +(having a small number of friends)", "+(sparse feature) is similar to +(having a small number of friends)", "+(sparse feature) can be thought of as +(having a small number of friends)", "+(sparse feature) can be compared to +(having a small number of friends)", "machine learning +(sparse feature AND analogy)", "machine learning +(\"sparse feature is like\")", "machine learning +(\"sparse feature is similar\")", "machine learning +(\"just as sparse feature\")", "machine learning +(\"sparse feature can be thought of as\")", "machine learning +(\"sparse feature can be compared to\")"]}