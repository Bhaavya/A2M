{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Python: <b>Dictionary</b> to Spare <b>Vector</b> - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/40923059/python-dictionary-to-spare-vector", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/40923059", "snippet": "I am new to Python and programming in general. I was working on Pyschool exercises Topic 8, Q 11 on converting <b>Dictionary</b> to Spare Vectore. I was asked to Write a function that converts a <b>dictionary</b> back to its sparese <b>vector</b> representation. Examples", "dateLastCrawled": "2022-01-28T16:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>sparse vector</b> in python? - Stack Overflow", "url": "https://stackoverflow.com/questions/31732632/sparse-vector-in-python", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/31732632", "snippet": "A <b>sparse vector</b> is a <b>vector</b> whose entries are almost all zero, <b>like</b> [1, 0, 0, 0, 0, 0, 0, 2, 0]. Storing all those zeros wastes memory and dictionaries are commonly used to keep track of just the nonzero entries. For example, the <b>vector</b> shown earlier can be represented as", "dateLastCrawled": "2022-01-25T12:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Sparse Vector To Dictionary</b> - Python | Dream.In.Code", "url": "https://www.dreamincode.net/forums/topic/130063-sparse-vector-to-dictionary/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.dreamincode.net</b>/forums/topic/130063-<b>sparse-vector-to-dictionary</b>", "snippet": "Re: <b>Sparse Vector to Dictionary</b>. Posted 05 October 2009 - 06:06 PM. You shouldnt try to turn {len: 10, 1:4, 8:1} into [0, 4, 0, 0, 0, 0, 0, 0, 1, 0, 0]. The whole idea of storing <b>sparse</b> vectors in a <b>dictionary</b> is to avoid the memory filling zeros. What you want to write is something <b>like</b>:", "dateLastCrawled": "2022-01-17T04:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Sparse</b> Matrix <b>in Python using Dictionary - GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/sparse-matrix-in-python-using-dictionary/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/<b>sparse</b>-matrix-in-python-using-<b>dictionary</b>", "snippet": "<b>Like</b> Article. <b>Sparse</b> Matrix in Python using <b>Dictionary</b>. Difficulty Level : Medium; Last Updated : 07 Jul, 2021. A <b>sparse</b> matrix is a matrix in which most of the elements have zero value and thus efficient ways of storing such matrices are required. <b>Sparse</b> matrices are generally utilized in applied machine learning such as in data containing data-encodings that map categories to count and also in entire subfields of machine learning such as natural language processing (NLP). Examples: 0 0 0 1 ...", "dateLastCrawled": "2022-02-03T02:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "python - <b>Dictionary To Sparse Vector help- sort dictionary</b>? [SOLVED ...", "url": "https://www.daniweb.com/programming/software-development/threads/409992/dictionary-to-sparse-vector-help-sort-dictionary", "isFamilyFriendly": true, "displayUrl": "https://<b>www.daniweb.com</b>/.../409992/<b>dictionary-to-sparse-vector-help-sort-dictionary</b>", "snippet": "A <b>sparse</b> <b>vector</b> is a <b>vector</b> whose entries are almost all zero, <b>like</b> [1, 0, 0, 0, 0, 0, 0, 2, 0]. Storing all those zeros wastes memory and dictionaries are commonly used to keep track of just the nonzero entries. For example, the <b>vector</b> shown earlier can be represented as {0:1, 7:2}, since the <b>vector</b> it is meant to represent has the value 1 at index 0 and the value 2 at index 7. Write a function that converts a <b>dictionary</b> back to its sparese <b>vector</b> representation.", "dateLastCrawled": "2022-01-12T02:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Dictionary</b> based representations \u2014 <b>sparse</b>-plex v2019.02", "url": "https://sparse-plex.readthedocs.io/en/latest/book/sparse_signal_models/dictionary_representations.html", "isFamilyFriendly": true, "displayUrl": "https://<b>sparse</b>-plex.readthedocs.io/en/latest/book/<b>sparse</b>_signal_models/<b>dictionary</b>...", "snippet": "Thus such a <b>dictionary</b> is able provide multiple representations to same <b>vector</b> \\(x\\). We call such dictionaries redundant dictionaries or over-complete dictionaries. In contrast a basis with \\(D=N\\) is called a complete <b>dictionary</b>. A special class of signals is those signals which have a <b>sparse</b> representation in a given <b>dictionary</b> \\(\\mathcal{D}\\).", "dateLastCrawled": "2022-02-02T21:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Implementing <b>Sparse</b> <b>Vector</b> in <b>Java</b> - GeeksforGeeks", "url": "https://www.geeksforgeeks.org/implementing-sparse-vector-in-java/", "isFamilyFriendly": true, "displayUrl": "https://www.geeksforgeeks.org/implementing-<b>sparse</b>-<b>vector</b>-in-<b>java</b>", "snippet": "<b>Like</b> Article. Implementing <b>Sparse</b> <b>Vector</b> in <b>Java</b>. Difficulty Level : Hard; Last Updated : 02 Dec, 2020. A <b>vector</b> or arraylist is a one-dimensional array of elements. The elements of a <b>Sparse</b> <b>Vector</b> have mostly zero values. It is inefficient to use a one-dimensional array to store a <b>sparse</b> <b>vector</b>. It is also inefficient to add elements whose values are zero in forming sums of <b>sparse</b> vectors. We convert the one-dimensional <b>vector</b> to a <b>vector</b> of (index, value) pairs. Examples Input: Enter size ...", "dateLastCrawled": "2022-01-30T04:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "What are the <b>Types of Sparse Dictionary Learning Algorithms</b>? | FinsliQ", "url": "https://www.finsliqblog.com/ai-and-machine-learning/what-are-the-types-of-sparse-dictionary-learning-algorithms/", "isFamilyFriendly": true, "displayUrl": "https://www.finsliqblog.com/ai-and-machine-learning/what-are-the-types-of-<b>sparse</b>...", "snippet": "<b>Sparse</b> Dictionaries: This method mainly focuses on not only giving a <b>sparse</b> representation but also constructing <b>dictionary</b> learning algorithms which is enforced by the equation . where . is some predefined analytical <b>dictionary</b> with desirable properties <b>like</b> fast computation and . is a <b>sparse</b> matrix. These formulation helps to directly combine the fast implementation of the analytical dictionaries with the flexibility of <b>sparse</b> techniques.", "dateLastCrawled": "2021-12-30T02:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>sparse</b> <b>vector</b> in python? | DaniWeb", "url": "https://www.daniweb.com/programming/software-development/threads/498357/sparse-vector-in-python", "isFamilyFriendly": true, "displayUrl": "https://<b>www.daniweb.com</b>/programming/software-development/threads/498357/<b>sparse</b>-<b>vector</b>...", "snippet": "A <b>sparse</b> <b>vector</b> is a <b>vector</b> whose entries are almost all zero, <b>like</b> [1, 0, 0, 0, 0, 0, 0, 2, 0]. Storing all those zeros wastes memory and dictionaries are commonly used to keep track of just the nonzero entries. For example, the <b>vector</b> shown earlier can be represented as {0:1, 7:2}, since the <b>vector</b> it is meant to represent has the value 1 at index 0 and the value 2 at index 7. Write a function that converts a <b>dictionary</b> back to its sparese <b>vector</b> representation.", "dateLastCrawled": "2022-01-23T20:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Dictionary learning</b> tutorial \u2014 dictlearn 0.0.0 documentation", "url": "https://dictlearn.readthedocs.io/en/latest/tutorial.html", "isFamilyFriendly": true, "displayUrl": "https://dictlearn.readthedocs.io/en/latest/tutorial.html", "snippet": "Almost always we\u2019ll have have thousands of training signals that each should be represented with a <b>sparse</b> <b>vector</b>. To make this easier we are going to organize our training data as columns in a big matrix X with the shape (signal_size, n_signals).Then we have the <b>dictionary</b> which is denoted by D.The shape of the <b>dictionary</b> is (signal_size, n_atoms).Finally we have the <b>sparse</b> representation which is the matrix A with shape (n_atoms, n_signals), where each column is the representation for the ...", "dateLastCrawled": "2022-01-27T03:11:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Spatial Locality-Aware <b>Sparse</b> Coding and <b>Dictionary</b> Learning", "url": "https://wangjiangb.github.io/pdfs/sparse_context.pdf", "isFamilyFriendly": true, "displayUrl": "https://wangjiangb.github.io/pdfs/<b>sparse</b>_context.pdf", "snippet": "the <b>similar</b> features bearing <b>similar</b> spatial contexts should be encoded as the same <b>sparse</b> <b>vector</b>. This implies that the spatial context of a feature should also be <b>sparse</b>. We call this the spatially local sparsity property. This property can be used for better coding and <b>dictionary</b> learning. This is different from traditional <b>sparse</b> coding methods. Exploiting this information in <b>sparse</b> coding, we are able to obtain more stable features and learn a better <b>dictionary</b>. The proposed <b>sparse</b> ...", "dateLastCrawled": "2022-02-02T13:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Class-<b>wise dictionary learning for hyperspectral image classification</b>", "url": "https://weiwangtrento.github.io/assets/pdf/2017-Neurocomputing.pdf", "isFamilyFriendly": true, "displayUrl": "https://weiwangtrento.github.io/assets/pdf/2017-Neurocomputing.pdf", "snippet": "In order to compute the <b>sparse</b> <b>vector</b> for any new input sample, lm-regularizer is used in the following optimization problem: ^ =\u2016\u2016 \u2016\u2212\u2016&lt;\u03b5 ss zDsarg min . . ,st m 1 s 2 2 when m\u00bc1, the obtained <b>sparse</b> <b>vector</b> is de\ufb01ned as the spare representation of the input sample. Similarly, it is de\ufb01ned as the collaborative representation when ...", "dateLastCrawled": "2022-01-25T01:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Sparse</b> Hyperspectral Classification - Johns Hopkins University", "url": "http://thanglong.ece.jhu.edu/Tran/Pub/Sparse_Hyperspectral_Classification.pdf", "isFamilyFriendly": true, "displayUrl": "thanglong.ece.jhu.edu/Tran/Pub/<b>Sparse</b>_Hyperspectral_Classification.pdf", "snippet": "of an unknown pixel is expressed as a <b>sparse</b> <b>vector</b> whose nonzero entries correspond to the weights of the selected training samples. The <b>sparse</b> <b>vector</b> is recovered by solving a sparsity-constrained optimization problem, and it can directly determine the class label of the test sample. Two different approaches are proposed to incorporate the contextual information into the <b>sparse</b> recovery optimization problem in order to improve the classi\ufb01cation per-formance. In the \ufb01rst approach, an ...", "dateLastCrawled": "2022-02-02T14:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>sparse_vector</b> \u00b7 PyPI", "url": "https://pypi.org/project/sparse_vector/", "isFamilyFriendly": true, "displayUrl": "https://pypi.org/project/<b>sparse_vector</b>", "snippet": "A <b>sparse vector</b> is a 1D numerical list where most (say, more than 95% of) values will be 0 (or some other default) and for reasons of memory efficiency you don\u2019t wish to store these. (cf. <b>Sparse</b> array) This implementation has a <b>similar</b> interface to Python\u2019s 1D numpy.ndarray but stores the values and indices in linked lists to preserve memory. <b>sparse_vector</b> is for numerical data only, if you want any type of data, have a look at <b>sparse</b>_list, the parent library, a <b>dictionary</b>-of-keys ...", "dateLastCrawled": "2022-01-27T01:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "K-<b>Means, Sparse Coding, Dictionary Learning and All</b> That \u2014 Data, ML ...", "url": "https://bugra.github.io/posts/2015/2/10/k-means-sparse-coding-dictionary-learning-and-all-that/", "isFamilyFriendly": true, "displayUrl": "https://bugra.github.io/posts/2015/2/10/k-<b>means-sparse-coding-dictionary-learning-and</b>...", "snippet": "<b>Dictionary</b> learning is very <b>similar</b> to <b>sparse</b> coding in terms of it tries to represent the data, it tries to find good atoms from a \u201c<b>dictionary</b>\u201d where the <b>dictionary</b> atoms are learned from the training set. Especially, for classification tasks, as long as the user has a good <b>dictionary</b>, one could build very efficient vectors using atoms from the <b>dictionary</b> for a variety of tasks including denoising and classification.", "dateLastCrawled": "2022-01-01T11:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Calculating Text Similarity With <b>Gensim</b> | by Riley Huang | Better ...", "url": "https://betterprogramming.pub/introduction-to-gensim-calculating-text-similarity-9e8b55de342d", "isFamilyFriendly": true, "displayUrl": "https://betterprogramming.pub/introduction-to-<b>gensim</b>-calculating-text-<b>similar</b>ity-9e8b...", "snippet": "The question is represented by its id (integer), and hence the representation of the text document becomes a series of pairs, such as (2, 4.0), (3, 6.0), (4, 5.0). This series can be thought of as a <b>vector</b>. If the vectors in the two documents are <b>similar</b>, the documents must be <b>similar</b> too. <b>Sparse</b> <b>Vector</b>", "dateLastCrawled": "2022-01-30T03:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "K-SVD: DESIGN OF DICTIONARIES FOR <b>SPARSE</b> REPRESENTATION", "url": "https://elad.cs.technion.ac.il/wp-content/uploads/2018/02/31_KSVD_SPARSE.pdf", "isFamilyFriendly": true, "displayUrl": "https://elad.cs.technion.ac.il/wp-content/uploads/2018/02/31_KSVD_<b>SPARSE</b>.pdf", "snippet": "plete <b>dictionary</b> matrix D 2 IRn K that contains K atoms, fdjgK j=1, as its columns, it is assumed that a signal y 2 IRn can be represented as a <b>sparse</b> linear combination of these atoms. The representation of y may either be exact y = Dx, or approximate, y \u02c7 Dx, satisfying ky Dxk2 . The <b>vector</b> x2 IRK displays the representation coefcients of ...", "dateLastCrawled": "2021-12-30T14:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) Hyperspectral Image Classification Using <b>Dictionary</b>-Based <b>Sparse</b> ...", "url": "https://www.researchgate.net/publication/224236796_Hyperspectral_Image_Classification_Using_Dictionary-Based_Sparse_Representation", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/224236796_Hyperspectral_Image_Classification...", "snippet": "The <b>sparse</b> representation of an unknown pixel is expressed as a <b>sparse</b> <b>vector</b> whose nonzero entries correspond to the weights of the selected training samples. The <b>sparse</b> <b>vector</b> is recovered by ...", "dateLastCrawled": "2022-01-30T17:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "python - <b>Dictionary To Sparse Vector help- sort dictionary</b>? [SOLVED ...", "url": "https://www.daniweb.com/programming/software-development/threads/409992/dictionary-to-sparse-vector-help-sort-dictionary", "isFamilyFriendly": true, "displayUrl": "https://<b>www.daniweb.com</b>/.../409992/<b>dictionary-to-sparse-vector-help-sort-dictionary</b>", "snippet": "A <b>sparse</b> <b>vector</b> is a <b>vector</b> whose entries are almost all zero, like [1, 0, 0, 0, 0, 0, 0, 2, 0]. Storing all those zeros wastes memory and dictionaries are commonly used to keep track of just the nonzero entries. For example, the <b>vector</b> shown earlier can be represented as {0:1, 7:2}, since the <b>vector</b> it is meant to represent has the value 1 at index 0 and the value 2 at index 7. Write a function that converts a <b>dictionary</b> back to its sparese <b>vector</b> representation.", "dateLastCrawled": "2022-01-12T02:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Dictionary</b> Learning with Uniform <b>Sparse</b> Representations for Anomaly ...", "url": "https://deepai.org/publication/dictionary-learning-with-uniform-sparse-representations-for-anomaly-detection", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>dictionary</b>-learning-with-uniform-<b>sparse</b>-representations...", "snippet": "The convolutional <b>sparse</b> coding model is exploited in [7280790] in order to learn a <b>dictionary</b> of filters used for the same task. Particular DL and SR formulations for AD in network traffic and telemetry are analyzed in [ 10.1007/978-3-030-48256-5_34 , Xing2020DetectingAI , pilastre:hal-02466360 ] .", "dateLastCrawled": "2022-02-03T18:37:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Sparse Vector To Dictionary</b> - Python | Dream.In.Code", "url": "https://www.dreamincode.net/forums/topic/130063-sparse-vector-to-dictionary/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.dreamincode.net</b>/forums/topic/130063-<b>sparse-vector-to-dictionary</b>", "snippet": "<b>Sparse Vector to Dictionary</b>. Posted 05 October 2009 - 12:24 PM. Sorry to start another thread but I misunderstood the original problem given to us so I am trying not to get it mixed up. I&#39;m having a really hard time understanding how this assignment is supposed to be completed. We are supposed to take, as arguments, two <b>sparse</b> vectors as dictionaries and add them together. For example: dicA = {len: 10, 2:5, 8:3} = 0, 0, 5, 0, 0, 0, 0, 0, 3, 0, 0 dicB = {len: 10, 1:4, 8:1} = 4, 0, 0, 0, 0, 0 ...", "dateLastCrawled": "2022-01-17T04:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Finding a sparse vector in a subspace</b> - sumofsquares.org", "url": "https://www.sumofsquares.org/public/lec-sphere-sparsevec.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.sumofsquares.org/public/lec-sphere-<b>sparse</b>vec.pdf", "snippet": "The problem itself is somewhat natural, and <b>can</b> <b>be thought</b> of as an average-case real (as opposed to \ufb01nite \ufb01eld) version of the \u201cshortest codeword\u201d or \u201clattice shortest <b>vector</b>\u201d problem. This also turns out to be related (at least in terms of techniques) to problems in unsupervised learning such as <b>dictionary</b> learning / <b>sparse</b> coding. There is a related problem, often called \u201ccompressed sensing\u201d or \u201c<b>sparse</b> recovery\u201d in which we are given an af\ufb01ne subspace A of the form ...", "dateLastCrawled": "2022-01-09T08:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "AF Classi\ufb01cation from ECG Recording using Feature Ensemble and <b>Sparse</b> ...", "url": "https://physionet.org/files/challenge-2017/1.0.0/papers/174-192.pdf?download", "isFamilyFriendly": true, "displayUrl": "https://physionet.org/files/challenge-2017/1.0.0/papers/174-192.pdf?download", "snippet": "<b>dictionary</b> matrix, D, <b>can</b> <b>be thought</b> of as a commonly-occurring feature learned from the training data. Because we sorted the RR-intervals prior to learning the <b>dictionary</b>, the \ufb01rst entry of each <b>dictionary</b> elements corresponds to the shortest RR-interval extracted from the ECG signal. Each column of X is a <b>sparse</b> <b>vector</b> that indicates which <b>dictionary</b> elements (features) are used to reconstruct the corresponding data <b>vector</b>. Mathematically, performing this matrix decomposition corresponds ...", "dateLastCrawled": "2021-12-16T18:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Decoding the Thought Vector</b> - GitHub Pages", "url": "http://gabgoh.github.io/ThoughtVectors/", "isFamilyFriendly": true, "displayUrl": "gabgoh.github.io/<b>ThoughtVectors</b>", "snippet": "The <b>thought</b> <b>vector</b> is a element <b>vector</b>, and the neural net does a noble job of capturing most of the dimensions of variation in the images. Using a <b>dictionary</b> of total atoms, with each element a sum of <b>sparse</b> atoms, I <b>can</b> produce a pretty respectable reconstruction of most of the faces.", "dateLastCrawled": "2022-01-20T19:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>The Sparse Manifold Transform</b> - NeurIPS", "url": "https://proceedings.neurips.cc/paper/8251-the-sparse-manifold-transform.pdf", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/8251-<b>the-sparse-manifold-transform</b>.pdf", "snippet": "<b>Sparse</b> coding attempts to approximate a data <b>vector</b>, x 2 IR n, as a <b>sparse</b> superposition of <b>dictionary</b> elements i: x =\u21b5 + (1) where 2 IR n\u21e5m is a matrix with columns i, \u21b5 2 IR m is a <b>sparse</b> <b>vector</b> of coef\ufb01cients and is a <b>vector</b> containing independent Gaussian noise samples, which are assumed to be small relative to x. Typically m&gt;nso that the representation is overcomplete. For a given <b>dictionary</b>, , the <b>sparse</b> code, \u21b5, of a data <b>vector</b>, x, <b>can</b> be computed in an online fashion by ...", "dateLastCrawled": "2021-11-18T20:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) <b>Sparse Decomposition over non-full-rank dictionaries</b> | Vincent ...", "url": "https://www.academia.edu/14162552/Sparse_Decomposition_over_non_full_rank_dictionaries", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/14162552/<b>Sparse_Decomposition_over_non_full_rank_dictionaries</b>", "snippet": "INTRODUCTION Let an -dimensional <b>vector</b> is to be decomposed as a lin- Direct solution of this problem needs a combinatorial search ear combination of the vectors \u261b \u261e \u261e \u261e . After [1], and is NP-hard. Consequently, many different algorithms the vectors \u261e \u261e are called atoms and they col- have been proposed in recent years for finding the <b>sparse</b> lectively form a <b>dictionary</b> over which the <b>vector</b> is to be solution of (1). Some examples are Basis Pursuit (BP) [11], decomposed. We may ...", "dateLastCrawled": "2021-05-21T14:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Dictionary</b> Learning and <b>Sparse</b> Coding on Grassmann Manifolds: An ...", "url": "https://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Harandi_Dictionary_Learning_and_2013_ICCV_paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Harandi_<b>Dictionary</b>...", "snippet": "In this paper we explore <b>sparse</b> <b>dictionary</b> learning over the space of linear subspaces, which form Riemannian structures known as Grassmann manifolds. To this end, we propose to embed Grassmann manifolds into the space of symmetric matrices by an isometric mapping, which en-ables us to devise a closed-form solution for updating a Grassmanndictionary, atombyatom. Furthermore, tohan-dle non-linearity in data, we propose a kernelised version of the <b>dictionary</b> learning algorithm. Experiments on ...", "dateLastCrawled": "2021-12-14T12:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Theoretical Guarantees for Graph <b>Sparse</b> Coding", "url": "https://elad.cs.technion.ac.il/wp-content/uploads/2019/03/GRSC_Guarantees_ACHA.pdf", "isFamilyFriendly": true, "displayUrl": "https://elad.cs.technion.ac.il/wp-content/uploads/2019/03/GRSC_Guarantees_ACHA.pdf", "snippet": "this framework, one assumes a signal y 2RN to be a <b>sparse</b> combination of a few columns (or atoms) from a collection D 2RN K, termed the <b>dictionary</b>. Put di erently, y = Dx where x 2RKis a <b>sparse</b> <b>vector</b>. Finding such a <b>vector</b> <b>can</b> be formulated as the following optimization problem: argmin x kxk 0 s.t. y = Dx: (1) This is known as the (P", "dateLastCrawled": "2022-01-02T08:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "tf idf - How to <b>get keys from pyspark SparseVector</b> - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/53997118/how-to-get-keys-from-pyspark-sparsevector", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/53997118", "snippet": "I conducted a tf-idf transform and now I want to get the keys and values from the result. I am using the following udf code to get values: So if the sparsevector looks like: features=SparseVector (123241, {20672: 4.4233, 37393: 0.0, 109847: 3.7096, 118474: 5.4042})) extracted_keys in my extract will look like: [4.4233, 0.0, 3.7096, 5.4042] My ...", "dateLastCrawled": "2022-01-24T22:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(PDF) <b>When can dictionary learning uniquely recover sparse</b> data from ...", "url": "https://www.researchgate.net/publication/281641401_When_can_dictionary_learning_uniquely_recover_sparse_data_from_subsamples", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/281641401_When_<b>can</b>_<b>dictionary</b>_learning...", "snippet": "the <b>sparse</b> <b>dictionary</b> for the data, the theory gives accurate recovery of a in equation (2) (and thus x ) from the compressed <b>vector</b> y as long as the dimension n of y satis\ufb01es:", "dateLastCrawled": "2022-01-13T23:27:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Comparison of <b>Sparse</b> Signal <b>Recovery Algorithms with Highly Coherent</b> ...", "url": "http://dsp.ucsd.edu/~zhilin/papers/comparison.pdf", "isFamilyFriendly": true, "displayUrl": "dsp.ucsd.edu/~zhilin/papers/comparison.pdf", "snippet": "<b>Sparse</b> Bayesian Learning (SBL), T-MSBL I. MODEL USED IN THE COMPARISON The basic model of <b>sparse</b> signal recovery is y = &#39;x+v; (1) where &#39; 2 RN\u00a3M(N \u00bf M) is a known <b>dictionary</b> matrix, y 2 RN\u00a31 is an available measurement <b>vector</b>, and v is an unknown noise <b>vector</b>. The task is to estimate the source <b>vector</b> x, which has only K nonzero elements (K ...", "dateLastCrawled": "2022-02-01T19:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Spatial Locality-Aware <b>Sparse</b> Coding and <b>Dictionary</b> Learning", "url": "https://wangjiangb.github.io/pdfs/sparse_context.pdf", "isFamilyFriendly": true, "displayUrl": "https://wangjiangb.github.io/pdfs/<b>sparse</b>_context.pdf", "snippet": "N] 2RD Nare the <b>sparse</b> vectors. The <b>sparse</b> vectors <b>can</b> be considered as soft <b>vector</b> quantization of the features. <b>Compared</b> to hard <b>vector</b> quantization, soft <b>vector</b> quantization is more stable, because hard <b>vector</b> quantization, which usually takes the class label of the nearest basis, may be very different for similar descriptors,", "dateLastCrawled": "2022-02-02T13:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Learning fast dictionaries for <b>sparse</b> representations using low-rank ...", "url": "https://hal.inria.fr/hal-01709343v2/document", "isFamilyFriendly": true, "displayUrl": "https://hal.inria.fr/hal-01709343v2/document", "snippet": "Kroneckers model <b>can</b> be computed by merging <b>dictionary</b> learning ap-proaches with the tensor Canonic Polyadic Decomposition. Experiments on image denoising illustrate the advantages of the proposed approach. Keywords: Kronecker product, tensor data, <b>dictionary</b> learning. 1 Introduction Multi-dimensional data arise in a large variety of applications such as telecom-munications, biomedical sciences, image and video processing to name a few [10]. Explicitly accounting for this tensorial structure ...", "dateLastCrawled": "2022-01-22T09:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Sparse</b> coding of i-<b>vector</b>/JFA latent <b>vector</b> over ensemble dictionaries ...", "url": "https://link.springer.com/article/10.1007/s10772-017-9476-3", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10772-017-9476-3", "snippet": "Computing <b>sparse</b> representation (SR) over an exemplar <b>dictionary</b> is time consuming and computationally expensive for large <b>dictionary</b> size. This also requires huge memory requirement for saving the <b>dictionary</b>. In order to reduce the latency and to achieve some diversity, ensemble of exemplar <b>dictionary</b> based language identification (LID) system is explored. The full diversity <b>can</b> be obtained if each of the exemplar <b>dictionary</b> contains only one feature <b>vector</b> from each of the language class ...", "dateLastCrawled": "2021-11-27T17:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "PERFORMANCE LIMITS OF <b>DICTIONARY</b> LEARNING FOR <b>SPARSE</b> CODING", "url": "https://www.wisdom.weizmann.ac.il/~yonina/YoninaEldar/conferences/218_Performance%20Limits%20of%20Dictionary%20Learning%20for%20Sparse%20Coding.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.wisdom.weizmann.ac.il/~yonina/YoninaEldar/conferences/218_Performance...", "snippet": "PERFORMANCE LIMITS OF <b>DICTIONARY</b> LEARNING FOR <b>SPARSE</b> CODING Alexander Junga,YoninaC.Eldarb,NorbertGortz\u00a8 a aInstitute of Telecommunications,Vienna University of Technology,Austria;{ajung,norbert.goertz}@nt.tuwien.ac.at bTechnion\u2014IsraelInstitute of Technology,Israel; e-mail: yonina@ee.technion.ac.il ABSTRACT We consider the problem of <b>dictionary</b> learning under the assumption that the observed signals <b>can</b> be represented as", "dateLastCrawled": "2021-12-17T14:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "2386 IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 60, NO. 5, MAY 2012 ...", "url": "https://www.weizmann.ac.il/math/yonina/sites/math.yonina/files/uploads/publications/Dictionary%20Optimization%20for.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.weizmann.ac.il/math/yonina/sites/math.yonina/files/uploads/publications...", "snippet": "The goal in <b>dictionary</b> learning is to \ufb01nd a <b>dictionary</b> and a representation matrix that best match a given set of vectors that are the columns of . In addition, we would like each <b>vector</b> of to be <b>sparse</b>. In this section we brie\ufb02yreview two popular sparsifying <b>dictionary</b> design algorithms, K-SVD [7]andMOD(MethodofOptimalDirections)[8].Wewillgen-", "dateLastCrawled": "2022-01-25T00:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Facial expression recognition via <b>sparse</b> representation using positive ...", "url": "https://ietresearch.onlinelibrary.wiley.com/doi/pdf/10.1049/iet-ipr.2015.0302", "isFamilyFriendly": true, "displayUrl": "https://ietresearch.onlinelibrary.wiley.com/doi/pdf/10.1049/iet-ipr.2015.0302", "snippet": "method. It <b>can</b> make constraints to the number m of pixels for the over-complete <b>dictionary</b>. Ideally, the new <b>sparse</b> coef\ufb01cient <b>vector</b> v 0 = x T 0, e T 0 T generated by (13) has only n 0 +\u03c1m non-zero elements (where n 0 is the number of non-zero elements in x 0; \u03c1 is the proportion of the number of non-zero elements in e 0). For an image ...", "dateLastCrawled": "2022-01-27T20:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "IMAGE COMPRESSION USING LEARNED DICTIONARIES BY RLS-DLA AND <b>COMPARED</b> ...", "url": "https://www.ux.uis.no/~karlsk/Skretting_icassp11.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.ux.uis.no/~karlsk/Skretting_icassp11.pdf", "snippet": "signal <b>vector</b> as a <b>sparse</b> representation over the over-complete (redundant) <b>dictionary</b>: x~ = XK k=1 w(k)dk= Dw; r= x x~ = x Dw; (1) with a sparseness constraint on the coe cient, or weight, <b>vector</b> w, kwk0 sor kwk1 s1. The l0 pseudo-norm kk0 is the number of non-zero elements. Finding the optimal <b>sparse</b> coe cient <b>vector</b> <b>can</b> be formulated as wopt ...", "dateLastCrawled": "2021-12-27T16:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Zhilin&#39;s Scientific Journey: A simple comparison of <b>sparse</b> signal ...", "url": "https://marchonscience.blogspot.com/2011/10/simple-comparison-of-sparse-signal.html", "isFamilyFriendly": true, "displayUrl": "https://marchonscience.blogspot.com/2011/10/simple-comparison-of-<b>sparse</b>-signal.html", "snippet": "<b>Sparse</b> signal recovery (or called compressed sensing in literature) has wide applications in source localization, radar detection, target tracking, and power spectrum estimation, etc. The basic model is: y = A x + v, where A is a know <b>dictionary</b> matrix, y is an available measurement <b>vector</b> (data <b>vector</b>), and v is the unknown measurement noise <b>vector</b>. The task is to estimate the source <b>vector</b> x, which has only K nonzero elements (K is a very small number). In the applications mentioned above ...", "dateLastCrawled": "2021-12-28T05:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "- Technical Report CS-2009-13 - 2009 Learning <b>Sparse</b> Dictionaries for ...", "url": "https://www.cs.technion.ac.il/~ronrubin/Publications/sparsedict-rep.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.technion.ac.il/~ronrubin/Publications/<b>sparse</b>dict-rep.pdf", "snippet": "this approach are the much \ufb02ner-tuned dictionaries they produce <b>compared</b> to analyti-cal approaches, and the signi\ufb02cantly better performance in applications. However, this comes at the expense of generating an unstructured <b>dictionary</b>, which is more costly to apply. Also, complexity constraints limit the size of the dictionaries that <b>can</b> be trained in this way, and the dimensions of the signals that <b>can</b> be processed. In this paper, we present a novel <b>dictionary</b> structure that bridges some ...", "dateLastCrawled": "2022-01-03T00:45:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A Gentle Introduction to Vectors for <b>Machine</b> <b>Learning</b>", "url": "https://machinelearningmastery.com/gentle-introduction-vectors-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/gentle-introduction-<b>vectors</b>-<b>machine</b>-<b>learning</b>", "snippet": "It is common to introduce vectors using a geometric <b>analogy</b>, where a <b>vector</b> represents a point or coordinate in an n-dimensional space, where n is the number of dimensions, such as 2. The <b>vector</b> can also be thought of as a line from the origin of the <b>vector</b> space with a direction and a magnitude. These analogies are good as a starting point, but should not be held too tightly as we often consider very high dimensional vectors in <b>machine</b> <b>learning</b>. I find the <b>vector</b>-as-coordinate the most ...", "dateLastCrawled": "2022-02-01T03:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Machine Learning by Analogy</b> - SlideShare", "url": "https://www.slideshare.net/ColleenFarrelly/machine-learning-by-analogy-59094152", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ColleenFarrelly/<b>machine-learning-by-analogy</b>-59094152", "snippet": "<b>Machine Learning by Analogy</b> 1. Colleen M. Farrelly 2. Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics behind these ...", "dateLastCrawled": "2022-01-31T07:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Word Embeddings, Analogies, and <b>Machine</b> <b>Learning</b>: Beyond king - man ...", "url": "https://aclanthology.org/C16-1332.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/C16-1332.pdf", "snippet": "Word Embeddings, Analogies, and <b>Machine</b> <b>Learning</b>: Beyond King - Man + Woman = Queen Aleksandr Drozd y, Anna Gladkova z, Satoshi Matsuoka y yTokyo Institute of Technology, Meguro-ku, Tokyo 152-8550, Japan alex@smg.is.titech.ac.jp, matsu@is.titech.ac.jp z The University of Tokyo, Meguro-ku, Tokyo 153-8902 Japan gladkova@phiz.c.u-tokyo.ac.jp Abstract Solving word analogies became one of the most popular benchmarks for word embeddings on the assumption that linear relations between word pairs ...", "dateLastCrawled": "2022-01-20T00:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "TensorFlow is an interface for expressing <b>machine</b> <b>learning</b> algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PPT) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> | Colleen Farrelly - Academia.edu", "url": "https://www.academia.edu/30404581/Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/30404581/<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "<b>Machine</b> <b>Learning</b> by <b>Analogy</b> Colleen M. Farrelly Overview of Problem Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics ...", "dateLastCrawled": "2022-01-02T06:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Introduction to Matrices and Matrix Arithmetic for <b>Machine Learning</b>", "url": "https://machinelearningmastery.com/introduction-matrices-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/introduction-matrices-<b>machine-learning</b>", "snippet": "The geometric <b>analogy</b> used to help understand vectors and some of their operations does not hold with matrices. Further, a <b>vector</b> itself may be considered a matrix with one column and multiple rows. Often the dimensions of the matrix are denoted as m and n for the number of rows and the number of columns. Now that we know what a matrix is, let\u2019s look at defining one in Python. Defining a Matrix. We can represent a matrix in Python using a two-dimensional NumPy array. A NumPy array can be ...", "dateLastCrawled": "2022-02-02T11:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine Learning :: Cosine Similarity for Vector</b> Space Models (Part III ...", "url": "https://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/", "isFamilyFriendly": true, "displayUrl": "https://blog.christianperone.com/2013/09/<b>machine</b>-<b>learning</b>-", "snippet": "<b>Machine Learning :: Cosine Similarity for Vector</b> Space Models (Part III) 12/09/2013 19/01/2020 Christian S. Perone <b>Machine</b> <b>Learning</b> , Programming , Python * It has been a long time since I wrote the TF-IDF tutorial ( Part I and Part II ) and as I promissed, here is the continuation of the tutorial.", "dateLastCrawled": "2022-01-29T21:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Word Embedding: Syntactics or Semantics</b> \u00b7 Shengbin&#39;s Studio", "url": "https://wushbin.github.io/2017/10/09/Word-Embedding-Syntactics-or-Semantics/", "isFamilyFriendly": true, "displayUrl": "https://wushbin.github.io/2017/10/09/<b>Word-Embedding-Syntactics-or-Semantics</b>", "snippet": "From all the result of the two method, we know that the dense <b>vector</b> method get a better result than the <b>sparse</b> PPMI method in <b>analogy</b> analysis and similar word search. In addition, the computational efficiency of the dense <b>vector</b> is also better than the PPMI. Short vectors may be easier to use as features in <b>machine</b> <b>learning</b>. Dense vectors may generalize better than storing explicit counts. In addition, dense vectors may perform better in capturing synonymy than <b>sparse</b> vectors.", "dateLastCrawled": "2022-01-09T18:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Word Embeddings in NLP - GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/word-embeddings-in-nlp/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/word-embeddings-in-nlp", "snippet": "Word Embeddings are a method of extracting features out of text so that we can input those features into a <b>machine</b> <b>learning</b> model to work with text data. They try to preserve syntactical and semantic information. The methods such as Bag of Words(BOW), CountVectorizer and TFIDF rely on the word count in a sentence but do not save any syntactical or semantic information. In these algorithms, the size of the <b>vector</b> is the number of elements in the vocabulary. We can get a <b>sparse</b> matrix if most ...", "dateLastCrawled": "2022-01-30T08:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What is the difference between a <b>Vector</b> and a Tensor in <b>Machine</b> <b>Learning</b>?", "url": "https://www.quora.com/What-is-the-difference-between-a-Vector-and-a-Tensor-in-Machine-Learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-the-difference-between-a-<b>Vector</b>-and-a-Tensor-in-<b>Machine</b>...", "snippet": "Answer (1 of 2): A <b>vector</b> is a tensor of rank 1, a matrix is a tensor of rank 2. For a tensor with more than 2 dimensions, we refer to it as a tensor. Note that, rank of a matrix [1] from linear algebra is not the same as tensor rank [2] 1. Rank (linear algebra) - Wikipedia 2. Tensor - Wikipedia", "dateLastCrawled": "2022-01-13T06:46:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(sparse vector)  is like +(dictionary)", "+(sparse vector) is similar to +(dictionary)", "+(sparse vector) can be thought of as +(dictionary)", "+(sparse vector) can be compared to +(dictionary)", "machine learning +(sparse vector AND analogy)", "machine learning +(\"sparse vector is like\")", "machine learning +(\"sparse vector is similar\")", "machine learning +(\"just as sparse vector\")", "machine learning +(\"sparse vector can be thought of as\")", "machine learning +(\"sparse vector can be compared to\")"]}