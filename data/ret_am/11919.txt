{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Markov</b> <b>Decision</b> Processes and Bellman Equations | by Steve Roberts ...", "url": "https://towardsdatascience.com/markov-decision-processes-and-bellman-equations-45234cce9d25", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>markov</b>-<b>decision</b>-<b>process</b>es-and-bellman-equations-45234...", "snippet": "For the level given above, and <b>its</b> accompanying <b>state</b> transition diagram and matrix, we\u2019ve described a <b>Markov</b> <b>Process</b> (also known as a <b>Markov</b> Chain). This is <b>a model</b> of a stochastic <b>process</b> in which a sequence of events occur, with the probability of an event occurring being dependent only on the <b>current</b> <b>state</b>.", "dateLastCrawled": "2022-02-03T17:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "The <b>Algorithm Behind the Curtain: Reinforcement Learning Concepts</b> (2 of ...", "url": "https://randomant.net/reinforcement-learning-concepts/", "isFamilyFriendly": true, "displayUrl": "https://randomant.net/reinforcement-learning-concepts", "snippet": "Classically, RL problems are represented by a <b>Markov Decision Process</b> (<b>MDP</b>). An <b>MDP</b> <b>is like</b> a flow chart with circles representing each <b>state</b>, and arrows jutting out from each circle that represent all the possible actions that can be taken from that <b>state</b>. For example, an <b>MDP</b> representing a Chess game would have states that represent where all the pieces on the Chess board are located, and actions representing the possible moves <b>based</b> on the Chess pieces on the board. A simple <b>Markov</b> ...", "dateLastCrawled": "2022-01-31T04:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A <b>Markov Decision Process Model for Optimal</b> Trade of Options Using ...", "url": "https://link.springer.com/article/10.1007%2Fs10614-020-10030-4", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10614-020-10030-4", "snippet": "This paper presents a <b>Markov decision process model for</b> calculating optimal <b>decision</b> policy regarding the trade of options assuming the American options trading <b>system</b>. The proposed <b>model</b> incorporates the conditional <b>probabilities</b> of option prices given various features (or factors) that affect those prices. The generation of such <b>probabilities</b> requires statistical data of the feature values as well as the option price values. Given the availability of statistical data, the paper explains ...", "dateLastCrawled": "2022-02-01T14:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Probabilistic modelling of deception-<b>based</b> security framework using ...", "url": "https://www.sciencedirect.com/science/article/pii/S0167404821004223", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0167404821004223", "snippet": "<b>Markov decision process</b>-<b>based</b> <b>system</b> modelling. We formalised a real case of attacks captured on simulated IoT devices using a honeypot to demonstrate the applicability of modelling known attacks for predicting attackers\u2019 actions. For this purpose, we used the data collected in our previous study Haseeb et al., 2020b) where a medium-interaction server honeypot, i.e., Cowrie 2, was configured and deployed to listen on the ports where IoT devices and services operate. The honeypot has the ...", "dateLastCrawled": "2022-02-03T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>A Markov decision process approach to vacant</b> taxi routing with e ...", "url": "https://www.sciencedirect.com/science/article/pii/S0191261518303837", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0191261518303837", "snippet": "A queueing theory-<b>based</b> <b>model</b> for matching taxis and passengers is proposed to account for competition from other taxis and use of e-hailing apps. \u2022 The problem is formulated as a <b>Markov decision process</b> (<b>MDP</b>), taking into account the impact of <b>current</b> decisions on future return over multiple pickups and drop-offs. \u2022 An efficient implementation of the value iteration algorithm for solving the <b>MDP</b> problem is proposed making use of efficient matrix operations. \u2022 The <b>MDP</b> formulation ...", "dateLastCrawled": "2022-01-07T15:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A <b>markov decision process</b> <b>model</b> for the optimal dispatch of military ...", "url": "https://link.springer.com/article/10.1007%2Fs10729-014-9297-8", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10729-014-9297-8", "snippet": "We formulate an infinite-horizon, undiscounted, average reward <b>Markov decision process</b> (<b>MDP</b>) <b>model</b> to determine how to optimally dispatch MEDEVAC helicopters to casualty events on the battlefield in order to maximize the steady-<b>state</b> expected <b>system</b> utility. A computational example is applied to a MEDEVAC <b>system</b> forward deployed in Afghanistan in support of combat operations. We apply a Hawkes <b>process</b> using Monte Carlo methods to generate data concerning casualties in our scenario of ...", "dateLastCrawled": "2022-01-26T17:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Markov</b> <b>Decision</b> Processes - MIT OpenCourseWare - Free Download PDF", "url": "https://zbook.org/read/4729c3_markov-decision-processes-mit-opencourseware.html", "isFamilyFriendly": true, "displayUrl": "https://zbook.org/read/4729c3_<b>markov</b>-<b>decision</b>-<b>process</b>es-mit-opencourseware.html", "snippet": "<b>MDP</b> Framework S : states A : actions Pr(st 1 st, at) : transition probabilitiesLecture 20 5The transition <b>probabilities</b> describe the dynamics of the world. They playthe role of the <b>next</b>-<b>state</b> function in a problem-solving search, except thatevery <b>state</b> is thought to be a possible consequence of taking an action in astate. So, we specify, for each <b>state</b> s t and action a t, the probability thatthe <b>next</b> <b>state</b> will be s t 1. You can think of this as being represented as aset of matrices, one for ...", "dateLastCrawled": "2021-12-24T10:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) <b>An MDP-based recommender system</b> - ResearchGate", "url": "https://www.researchgate.net/publication/234054512_An_MDP-based_recommender_system", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/234054512_<b>An_MDP-based_recommender_system</b>", "snippet": "<b>MDP-based recommender system</b> using this predictive <b>model</b>. 3.1 The Basic <b>Model</b>. A <b>Markov</b> chain is <b>a model</b> of <b>system</b> dynamics \u2013 in our case, user \u201cdynamics. \u201d To use it, we need. to formulate ...", "dateLastCrawled": "2022-01-22T08:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A Semi-<b>Markov Decision Model for Recognizing the Destination</b> of a ...", "url": "https://www.hindawi.com/journals/mpe/2016/1907971/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.hindawi.com</b>/journals/mpe/2016/1907971", "snippet": "To present <b>decision</b> <b>process</b> hierarchically, Bui et al. proposed an abstract hidden <b>Markov</b> <b>model</b> (AHMM) <b>based</b> on the notion of abstract <b>Markov</b> policies (AMPs), which can be described simply in terms of a <b>state</b> space and a <b>Markov</b> policy that selects among a set of other AMPs. When the AHMM is used in intention recognition, it only concerns the <b>probabilities</b> of selection of a policy or abstract policy and does need to build the reward functions as MDPs. A problem of the AHMM is that it does not ...", "dateLastCrawled": "2021-10-11T10:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Hidden Markov Model</b>. Elaborated with examples | Towards Data Science", "url": "https://towardsdatascience.com/markov-and-hidden-markov-model-3eec42298d75", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>markov</b>-and-<b>hidden-markov-model</b>-3eec42298d75", "snippet": "A stochastic <b>process</b> can be classified in many ways <b>based</b> on <b>state</b> space, index set, etc. When the stochastic <b>proce s s</b> is interpreted as time, if the <b>process</b> has a finite number of elements such as integers, numbers, and natural numbers then it is Discrete Time. Stochastic <b>Model</b>. It is a discrete-time <b>process</b> indexed at time 1,2,3,\u2026that takes values called states which are observed. For an example if the states (S) ={hot , cold } <b>State</b> series over time =&gt; z\u2208 S_T. Weather for 4 days can ...", "dateLastCrawled": "2022-01-30T23:28:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Markov Decision Process</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/computer-science/markov-decision-process", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/computer-science/<b>markov-decision-process</b>", "snippet": "11.3.3. Emotion <b>model</b> <b>based</b> on <b>Markov</b> <b>decision</b> processThe <b>Markov decision process</b> (<b>MDP</b>) is a mathematical <b>model</b> of sequential decisions and a dynamic optimization method. A <b>MDP</b> consists of the following five elements: {T, S, A, p, r} where. 1. T is all <b>decision</b> time sets. 2. S is a set of countable nonempty states, which is a set of all possible states of the <b>system</b>. 3. A is a set of all possible <b>decision</b>-making behaviors when the <b>system</b> is in a given <b>state</b>. 4. p indicates the probability of ...", "dateLastCrawled": "2022-02-03T06:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 1, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Markov decision process</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Markov_decision_process", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Markov_decision_process</b>", "snippet": "A <b>Markov decision process</b> is a 4-tuple (,,,), where: is a set of states called the <b>state</b> space,; is a set of actions called the action space (alternatively, is the set of actions available from <b>state</b> ), (, \u2032) = (+ = \u2032 =, =) is the probability that action in <b>state</b> at time will lead to <b>state</b> \u2032 at time +,(, \u2032) is the immediate reward (or expected immediate reward) received after transitioning from <b>state</b> to <b>state</b> \u2032, due to action The <b>state</b> and action spaces may be finite or infinite ...", "dateLastCrawled": "2022-02-07T02:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Probabilistic modelling of deception-<b>based</b> security framework using ...", "url": "https://www.sciencedirect.com/science/article/pii/S0167404821004223", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0167404821004223", "snippet": "<b>Markov Decision Process</b> (<b>MDP</b>) allows to <b>model</b> systems which are stochastic in nature because it helps to make decisions under uncertainty (Alsheikh et al., 2015). MDPs are an extension of Discrete-time <b>Markov</b> Chains (DTMCs), allowing to make non-deterministic choices as well. <b>Similar</b> to DTMCs, possible configurations of the <b>system</b> are modelled as a set of states and transitions between them occur in the form of discrete time steps. However, there may exist a non-deterministic choice between ...", "dateLastCrawled": "2022-02-03T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A <b>Markov Decision Process Model for Optimal</b> Trade of Options Using ...", "url": "https://link.springer.com/article/10.1007%2Fs10614-020-10030-4", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10614-020-10030-4", "snippet": "This paper presents a <b>Markov decision process model for</b> calculating optimal <b>decision</b> policy regarding the trade of options assuming the American options trading <b>system</b>. The proposed <b>model</b> incorporates the conditional <b>probabilities</b> of option prices given various features (or factors) that affect those prices. The generation of such <b>probabilities</b> requires statistical data of the feature values as well as the option price values. Given the availability of statistical data, the paper explains ...", "dateLastCrawled": "2022-02-01T14:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A <b>markov decision process</b> <b>model</b> for the optimal dispatch of military ...", "url": "https://link.springer.com/article/10.1007%2Fs10729-014-9297-8", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10729-014-9297-8", "snippet": "We formulate an infinite-horizon, undiscounted, average reward <b>Markov decision process</b> (<b>MDP</b>) <b>model</b> to determine how to optimally dispatch MEDEVAC helicopters to casualty events on the battlefield in order to maximize the steady-<b>state</b> expected <b>system</b> utility. A computational example is applied to a MEDEVAC <b>system</b> forward deployed in Afghanistan in support of combat operations. We apply a Hawkes <b>process</b> using Monte Carlo methods to generate data concerning casualties in our scenario of ...", "dateLastCrawled": "2022-01-26T17:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) <b>Reinforcement Learning and Markov Decision Processes</b>", "url": "https://www.researchgate.net/publication/235004620_Reinforcement_Learning_and_Markov_Decision_Processes", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/235004620_Reinforcement_Learning_and_<b>Markov</b>...", "snippet": "The <b>Markov Decision Process</b>. Putting all elements together results in the de\ufb01nition of a <b>Markov decision process</b> , which will be the base <b>model</b> for the large majority of methods described in this", "dateLastCrawled": "2022-01-24T17:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "The <b>Algorithm Behind the Curtain: Reinforcement Learning Concepts</b> (2 of ...", "url": "https://randomant.net/reinforcement-learning-concepts/", "isFamilyFriendly": true, "displayUrl": "https://randomant.net/reinforcement-learning-concepts", "snippet": "A simple <b>Markov Decision Process</b> (<b>MDP</b>) that represents a typical workday . A key characteristic of an <b>MDP</b> is that each <b>state</b> must contain all the information the agent needs in order to make an informed <b>decision</b>, a requirement called the \u201c<b>Markov</b> property.\u201d The <b>Markov</b> property basically says that the agent can\u2019t be expected to have any historical memory of <b>its</b> own, outside the <b>state</b> itself. For example, the <b>current</b> <b>state</b> of the Chess board tells me everything I need to know about which ...", "dateLastCrawled": "2022-01-31T04:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Hidden Markov Model</b>. Elaborated with examples | Towards Data Science", "url": "https://towardsdatascience.com/markov-and-hidden-markov-model-3eec42298d75", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>markov</b>-and-<b>hidden-markov-model</b>-3eec42298d75", "snippet": "A stochastic <b>process</b> can be classified in many ways <b>based</b> on <b>state</b> space, index set, etc. When the stochastic <b>proce s s</b> is interpreted as time, if the <b>process</b> has a finite number of elements such as integers, numbers, and natural numbers then it is Discrete Time. Stochastic <b>Model</b>. It is a discrete-time <b>process</b> indexed at time 1,2,3,\u2026that takes values called states which are observed. For an example if the states (S) ={hot , cold } <b>State</b> series over time =&gt; z\u2208 S_T. Weather for 4 days can ...", "dateLastCrawled": "2022-01-30T23:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A Semi-<b>Markov Decision Model for Recognizing the Destination</b> of a ...", "url": "https://www.hindawi.com/journals/mpe/2016/1907971/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.hindawi.com</b>/journals/mpe/2016/1907971", "snippet": "To present <b>decision</b> <b>process</b> hierarchically, Bui et al. proposed an abstract hidden <b>Markov</b> <b>model</b> (AHMM) <b>based</b> on the notion of abstract <b>Markov</b> policies (AMPs), which can be described simply in terms of a <b>state</b> space and a <b>Markov</b> policy that selects among a set of other AMPs. When the AHMM is used in intention recognition, it only concerns the <b>probabilities</b> of selection of a policy or abstract policy and does need to build the reward functions as MDPs. A problem of the AHMM is that it does not ...", "dateLastCrawled": "2021-10-11T10:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Score Following as <b>a Multi-Modal Reinforcement Learning Problem</b>", "url": "https://transactions.ismir.net/articles/10.5334/tismir.31/", "isFamilyFriendly": true, "displayUrl": "https://transactions.ismir.net/articles/10.5334/tismir.31", "snippet": "In Section 2, we start by defining the task of score following as a <b>Markov Decision Process</b> (<b>MDP</b>) and explaining <b>its</b> basic building blocks. Section 3 introduces the concept of Policy Gradient Methods and provides details on three learning algorithms we will use. Section 4 proceeds with a description of our experiments and presents results for the case of synthesized piano data. Section 5 then briefly looks at <b>model</b> interpretability, providing some glimpses into the learned representations ...", "dateLastCrawled": "2022-01-28T21:47:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) Building Relational World Models for Reinforcement Learning ...", "url": "https://www.academia.edu/67308762/Building_Relational_World_Models_for_Reinforcement_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/67308762/Building_Relational_World_<b>Models</b>_for_Reinforcement...", "snippet": "A reinforcement learning domain <b>can</b> <b>be thought</b> of as a <b>Markov Decision Process</b> (<b>MDP</b>). An <b>MDP</b> is a five-tuple \u2039S,A,P,R, \u203a where S is a finite set of states; A is a finite set of actions; P is a transition function denoting the <b>next</b>-<b>state</b> distribution after taking action a in <b>state</b> s; R is a bounded reward function denoting the expected immediate re- ward received for taking action a in <b>state</b> s; and \u2208[0,1) is a discount factor. In many cases, the underlying <b>MDP</b> for a domain is unknown ...", "dateLastCrawled": "2022-01-26T07:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Markov</b> <b>Decision</b> Processes - MIT OpenCourseWare - Free Download PDF", "url": "https://zbook.org/read/4729c3_markov-decision-processes-mit-opencourseware.html", "isFamilyFriendly": true, "displayUrl": "https://zbook.org/read/4729c3_<b>markov</b>-<b>decision</b>-<b>process</b>es-mit-opencourseware.html", "snippet": "<b>MDP</b> FrameworkLecture 20 2A <b>Markov decision process</b> (known as an <b>MDP</b>) is a discrete-time statetransition <b>system</b>. It <b>can</b> be described formally with 4 components.2 . <b>MDP</b> Framework S : statesLecture 20 3First, it has a set of states. These states will play the role of outcomes in thedecision theoretic approach we saw last time, as well as providing whateverinformation is necessary for choosing actions. For a robot navigatingthrough a building, the <b>state</b> might be the room it\u2019s in, or the x,y ...", "dateLastCrawled": "2021-12-24T10:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>An MDP-based recommender system</b> - ResearchGate", "url": "https://www.researchgate.net/publication/234054512_An_MDP-based_recommender_system", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/234054512_<b>An_MDP-based_recommender_system</b>", "snippet": "<b>MDP-based recommender system</b> using this predictive <b>model</b>. 3.1 The Basic <b>Model</b>. A <b>Markov</b> chain is a <b>model</b> of <b>system</b> dynamics \u2013 in our case, user \u201cdynamics. \u201d To use it, we need. to formulate ...", "dateLastCrawled": "2022-01-22T08:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A pilot study on logic proof tutoring using hints generated from ...", "url": "https://www.educationaldatamining.org/EDM2008/uploads/proc/22_Barnes_41a.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.educationaldatamining.org/EDM2008/uploads/proc/22_Barnes_41a.pdf", "snippet": "The <b>MDP</b> Generator <b>uses</b> historical student data to generate a <b>Markov Decision Process</b> (<b>MDP</b>) that represents a student <b>model</b>, containing all previously seen problem states and student actions. Each action is annotated with a transition probability P and each <b>state</b> is assigned a value <b>based</b> on the <b>MDP</b> reward function R. On executing action a in <b>state</b> s the probability of transitioning to <b>state</b> s\u2019 is P(s\u2019 | s, a) and the expected reward associated with that transition is R(s\u2019| s, a). Our ...", "dateLastCrawled": "2022-01-12T03:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "PowerPoint Presentation", "url": "https://www.cs.princeton.edu/courses/archive/spring17/cos598F/lectures/RL.pptx", "isFamilyFriendly": true, "displayUrl": "https://www.cs.princeton.edu/courses/archive/spring17/cos598F/lectures/RL.pptx", "snippet": "<b>Markov Decision Process</b> (<b>MDP</b>) Set of states S. Set of actions A. <b>State</b> transition <b>probabilities</b> p(s\u2019 | s, a). This is the probability distribution over the <b>state</b> space given we take action a in <b>state</b> s. Discount factor \u03b3 in [0, 1] Reward function R: S x A -&gt; set of real numbers. For simplicity, assume discrete rewards. Finite <b>MDP</b> if both S ...", "dateLastCrawled": "2022-01-30T05:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Chapter 51 <b>Structural estimation of markov decision</b> processes ...", "url": "https://www.sciencedirect.com/science/article/pii/S1573441205800200", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1573441205800200", "snippet": "Ch. 51: <b>Structural Estimation of Markov Decision</b> Processes 3097 [Puterman and Shin (1978)], and adaptive <b>state</b> aggregation algorithms [Bertsekas and Castafion (1989)]. Puterman and Brumelle (1978, 1979) have shown that policy iteration is identical to Newton&#39;s method for computing a zero to a nonlinear function.", "dateLastCrawled": "2022-01-13T18:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>In reinforcement learning, what is the</b> <b>difference between the one-step</b> ...", "url": "https://www.quora.com/In-reinforcement-learning-what-is-the-difference-between-the-one-step-dynamics-the-policy-and-the-Markov-decision-process", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>In-reinforcement-learning-what-is-the</b>-difference-between-the-one...", "snippet": "Answer: This is the standard RL setup: The agent sees the environment <b>state</b> S_{t} and takes an action A_{t}. This action causes a change in the environment, making it go to <b>state</b> S_{t+1}, and the agent receives the reward R_{t+1} for <b>its</b> action A_{t}. The policy determines how the agent chooses...", "dateLastCrawled": "2022-01-16T11:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "SimArch: A Multi-agent <b>System For Human Path Simulation In Architecture</b> ...", "url": "https://yenchiah.wordpress.com/2016/05/11/simarch-a-multi-agent-system-for-human-path-simulation-in-architecture-design/", "isFamilyFriendly": true, "displayUrl": "https://yenchiah.wordpress.com/2016/05/11/simarch-a-multi-agent-<b>system</b>-for-human-path...", "snippet": "SimArch <b>uses</b> a <b>Markov Decision Process</b> as the behavior <b>model</b>. The <b>model</b> involves human mental states, target range detection, and collision prediction. It also models different human characteristics by assigning different transition <b>probabilities</b>. A modified weighted A* search algorithm quickly computes the sub-optimal moving path. SimArch takes a series of preprocessed gallery floorplans as inputs, <b>uses</b> the behavior <b>model</b> for simulation, and outputs a density map for evaluation which helps ...", "dateLastCrawled": "2021-12-23T05:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Introduction to <b>Reinforcement Learning</b> (DDPG and TD3) for News ...", "url": "https://towardsdatascience.com/reinforcement-learning-ddpg-and-td3-for-news-recommendation-d3cddec26011", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>reinforcement-learning</b>-ddpg-and-td3-for-news...", "snippet": "So you decided to build a deep-learning-powered recommender <b>system</b>, already know about <b>Markov Decision process</b> and the dataset structure, overall very eager to jump straight into the action. Let\u2019s try a more fundamental approach, without <b>reinforcement learning</b> just yet. All you have is a simple linear perceptron and the dataset: a bunch of states, corresponding actions, and rewards to those actions. Translated to more human language: films watched, the movie chosen to see <b>next</b> and <b>its</b> ...", "dateLastCrawled": "2022-02-02T05:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What is <b>the difference between Markov Models and</b> Hidden <b>Markov</b> ... - Quora", "url": "https://www.quora.com/What-is-the-difference-between-Markov-Models-and-Hidden-Markov-Models", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>the-difference-between-Markov-Models-and</b>-Hidden-<b>Markov</b>...", "snippet": "Answer (1 of 5): A &quot;<b>Markov</b> <b>Model</b>&quot; <b>process</b> is basically one that does not have any memory -- the distribution of the <b>next</b> <b>state</b>/observation depends exclusively on the <b>current</b> <b>state</b>. A <b>Markov</b> <b>Model</b> may be autonomous or controlled -- an autonomous <b>Markov</b> <b>process</b> will evolve by itself, and in the cas...", "dateLastCrawled": "2022-01-16T06:32:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Markov Decision Process</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/computer-science/markov-decision-process", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/computer-science/<b>markov-decision-process</b>", "snippet": "<b>Markov decision process</b> <b>state</b> transitions assuming a 1-D mobility <b>model</b> for the edge cloud. The mobile device moves one step to the left or right with probability r1 and stays in the same location with probability 1 \u2212 2 r1 thus, p = q = r1 and p0 = 2 r1. Migration occurs in slot t if and only if d ( t )\u2265 N.", "dateLastCrawled": "2022-02-03T06:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A <b>Markov Decision Process Model for Optimal</b> Trade of Options Using ...", "url": "https://link.springer.com/article/10.1007%2Fs10614-020-10030-4", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10614-020-10030-4", "snippet": "This paper presents a <b>Markov decision process model for</b> calculating optimal <b>decision</b> policy regarding the trade of options assuming the American options trading <b>system</b>. The proposed <b>model</b> incorporates the conditional <b>probabilities</b> of option prices given various features (or factors) that affect those prices. The generation of such <b>probabilities</b> requires statistical data of the feature values as well as the option price values. Given the availability of statistical data, the paper explains ...", "dateLastCrawled": "2022-02-01T14:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>In reinforcement learning, what is the</b> <b>difference between the one-step</b> ...", "url": "https://www.quora.com/In-reinforcement-learning-what-is-the-difference-between-the-one-step-dynamics-the-policy-and-the-Markov-decision-process", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>In-reinforcement-learning-what-is-the</b>-difference-between-the-one...", "snippet": "Answer: This is the standard RL setup: The agent sees the environment <b>state</b> S_{t} and takes an action A_{t}. This action causes a change in the environment, making it go to <b>state</b> S_{t+1}, and the agent receives the reward R_{t+1} for <b>its</b> action A_{t}. The policy determines how the agent chooses...", "dateLastCrawled": "2022-01-16T11:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Semi-<b>Markov Decision Model for Recognizing the Destination</b> of a ...", "url": "https://www.hindawi.com/journals/mpe/2016/1907971/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.hindawi.com</b>/journals/mpe/2016/1907971", "snippet": "To present <b>decision</b> <b>process</b> hierarchically, Bui et al. proposed an abstract hidden <b>Markov</b> <b>model</b> (AHMM) <b>based</b> on the notion of abstract <b>Markov</b> policies (AMPs), which <b>can</b> be described simply in terms of a <b>state</b> space and a <b>Markov</b> policy that selects among a set of other AMPs. When the AHMM is used in intention recognition, it only concerns the <b>probabilities</b> of selection of a policy or abstract policy and does need to build the reward functions as MDPs. A problem of the AHMM is that it does not ...", "dateLastCrawled": "2021-10-11T10:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) A <b>Natural Language Argumentation Interface for Explanation</b> ...", "url": "https://www.researchgate.net/publication/221367493_A_Natural_Language_Argumentation_Interface_for_Explanation_Generation_in_Markov_Decision_Processes", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221367493_A_Natural_Language_Argumentation...", "snippet": "A <b>Markov Decision Process</b> (<b>MDP</b>) policy presents, for each <b>state</b>, an action, which preferably maximizes the expected reward accrual over time. In this paper, we present a novel <b>system</b> that ...", "dateLastCrawled": "2021-12-20T08:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "The <b>Algorithm Behind the Curtain: Reinforcement Learning Concepts</b> (2 of ...", "url": "https://randomant.net/reinforcement-learning-concepts/", "isFamilyFriendly": true, "displayUrl": "https://randomant.net/reinforcement-learning-concepts", "snippet": "A simple <b>Markov Decision Process</b> (<b>MDP</b>) that represents a typical workday . A key characteristic of an <b>MDP</b> is that each <b>state</b> must contain all the information the agent needs in order to make an informed <b>decision</b>, a requirement called the \u201c<b>Markov</b> property.\u201d The <b>Markov</b> property basically says that the agent <b>can</b>\u2019t be expected to have any historical memory of <b>its</b> own, outside the <b>state</b> itself. For example, the <b>current</b> <b>state</b> of the Chess board tells me everything I need to know about which ...", "dateLastCrawled": "2022-01-31T04:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Score Following as <b>a Multi-Modal Reinforcement Learning Problem</b>", "url": "https://transactions.ismir.net/articles/10.5334/tismir.31/", "isFamilyFriendly": true, "displayUrl": "https://transactions.ismir.net/articles/10.5334/tismir.31", "snippet": "In Section 2, we start by defining the task of score following as a <b>Markov Decision Process</b> (<b>MDP</b>) and explaining <b>its</b> basic building blocks. Section 3 introduces the concept of Policy Gradient Methods and provides details on three learning algorithms we will use. Section 4 proceeds with a description of our experiments and presents results for the case of synthesized piano data. Section 5 then briefly looks at <b>model</b> interpretability, providing some glimpses into the learned representations ...", "dateLastCrawled": "2022-01-28T21:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Hidden Markov Model</b>. Elaborated with examples | Towards Data Science", "url": "https://towardsdatascience.com/markov-and-hidden-markov-model-3eec42298d75", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>markov</b>-and-<b>hidden-markov-model</b>-3eec42298d75", "snippet": "A stochastic <b>process</b> <b>can</b> be classified in many ways <b>based</b> on <b>state</b> space, index set, etc. When the stochastic <b>proce s s</b> is interpreted as time, if the <b>process</b> has a finite number of elements such as integers, numbers, and natural numbers then it is Discrete Time. Stochastic <b>Model</b>. It is a discrete-time <b>process</b> indexed at time 1,2,3,\u2026that takes values called states which are observed. For an example if the states (S) ={hot , cold } <b>State</b> series over time =&gt; z\u2208 S_T. Weather for 4 days <b>can</b> ...", "dateLastCrawled": "2022-01-30T23:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Electronics | Free Full-Text | Towards Dynamic Reconfiguration of a ...", "url": "https://www.mdpi.com/2079-9292/10/13/1597/htm", "isFamilyFriendly": true, "displayUrl": "https://www.<b>mdp</b>i.com/2079-9292/10/13/1597/htm", "snippet": "Moustafa et al. proposed a proactive approach to web services composition (WSC), which <b>uses</b> the <b>Markov Decision Process</b> (<b>MDP</b>) to <b>model</b> WSC <b>process</b> and <b>uses</b> Q-learning for a Reinforcement Learning technique to adapt to dynamic change in the WSC environments proactively. This approach monitors the WSC to determine proactive adaptation by analyzing the web service execution log\u2019s historical data.", "dateLastCrawled": "2021-12-25T11:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What is <b>the difference between Markov Models and</b> Hidden <b>Markov</b> ... - Quora", "url": "https://www.quora.com/What-is-the-difference-between-Markov-Models-and-Hidden-Markov-Models", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>the-difference-between-Markov-Models-and</b>-Hidden-<b>Markov</b>...", "snippet": "Answer (1 of 5): A &quot;<b>Markov</b> <b>Model</b>&quot; <b>process</b> is basically one that does not have any memory -- the distribution of the <b>next</b> <b>state</b>/observation depends exclusively on the <b>current</b> <b>state</b>. A <b>Markov</b> <b>Model</b> may be autonomous or controlled -- an autonomous <b>Markov</b> <b>process</b> will evolve by itself, and in the cas...", "dateLastCrawled": "2022-01-16T06:32:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Why does <b>Markov Decision Process</b> matter in Reinforcement <b>Learning</b>? | by ...", "url": "https://towardsdatascience.com/why-does-malkov-decision-process-matter-in-reinforcement-learning-b111b46b41bd", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/why-does-malkov-<b>decision</b>-<b>process</b>-matter-in...", "snippet": "It is named by <b>analogy</b> to \u201cone-armed bandit\u201d(= a slot <b>machine</b>) although the framework has k levers instead of one. ... we introduce <b>Markov Decision Process</b>(<b>MDP</b>) to solve such a problem. An <b>MDP</b> consists of two elements; the agent and the environment. The agent is a learner or <b>decision</b>-maker. In the above example, the agent is the rabbit. The environment is everything surrounding the agent. In the example, the environment includes everything in the field where the rabbit is with food and ...", "dateLastCrawled": "2022-01-31T01:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Markov Decision Process</b>: How Does Value Iteration Work? | Baeldung on ...", "url": "https://www.baeldung.com/cs/mdp-value-iteration", "isFamilyFriendly": true, "displayUrl": "https://www.baeldung.com/cs/<b>mdp</b>-value-iteration", "snippet": "From this point, we can make an <b>analogy</b> with the <b>Markov</b> model since the solution for this problem is a sequence of actions. A <b>Markov Decision Process</b> is used to model the agent, considering that the agent itself generates a series of actions. In the real world, we can have observable, hidden, or partially observed states, depending on the ...", "dateLastCrawled": "2022-01-30T20:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Introduction to Machine Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/24/introduction-to-machine-learning-2/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/24/<b>introduction-to-machine-learning</b>-2", "snippet": "<b>Markov decision process</b> (<b>MDP</b>): In reinforcement <b>learning</b>, <b>MDP</b> is a mathematical framework for modeling <b>decision</b>-making of an agent in situations or environments where outcomes are partly random and partly under control. In this model, environment is modeled as a set of states and actions that can be performed by an agent to control the system\u2019s state. The objective is to control the system in such a way that the agent\u2019s total payoff is maximized.", "dateLastCrawled": "2022-01-17T05:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Reinforcement <b>Learning</b> | Ioannis Anifantakis | Analytics Vidhya", "url": "https://medium.com/analytics-vidhya/reinforcement-learning-basic-understanding-4fcb91ba4e4", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/reinforcement-<b>learning</b>-basic-understanding-4fcb91ba4e4", "snippet": "<b>Markov Decision Process</b> (<b>MDP</b>) The States, Actions, Rewards, their mechanics (known as One-Step Dynamics ), together with the discount rate (\u03b3) define a <b>Markov Decision Process</b> (<b>MDP</b>) .", "dateLastCrawled": "2021-08-02T19:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Markov decision process</b>: value iteration with code implementation | by ...", "url": "https://medium.com/@ngao7/markov-decision-process-value-iteration-2d161d50a6ff", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@ngao7/<b>markov-decision-process</b>-value-iteration-2d161d50a6ff", "snippet": "<b>Markov decision process</b>, <b>MDP</b>, value iteration, policy iteration, policy evaluation, policy improvement, sweep, iterative policy evaluation, policy, optimal policy ...", "dateLastCrawled": "2022-01-08T01:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Lecture <b>Reinforcement Learning</b> - MIT OpenCourseWare", "url": "https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-s897-machine-learning-for-healthcare-spring-2019/lecture-notes/MIT6_S897S19_lec16note.pdf", "isFamilyFriendly": true, "displayUrl": "https://<b>ocw.mit.edu</b>/courses/electrical-engineering-and-computer-science/6-s897-<b>machine</b>...", "snippet": "4.1 Examples of <b>decision</b> processes. A <b>Markov decision process</b> (<b>MDP</b>) is a well-known type of <b>decision</b> <b>process</b>, where the states follow the <b>Markov</b> assumption that the state transitions, rewards, and actions depend only on the most recent state-action pair. See Figure 3(a) for an illustration. Algebraically, this means the states, actions and reward", "dateLastCrawled": "2022-02-03T08:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "18.1. <b>Markov Decision Process</b> (<b>MDP</b>) \u2014 Dive into Deep <b>Learning</b> 0.17.0 ...", "url": "http://preview.d2l.ai.s3-website-us-west-2.amazonaws.com/d2l-en/master/chapter_reinforcement_learning/mdp.html", "isFamilyFriendly": true, "displayUrl": "preview.d2l.ai.s3-website-us-west-2.amazonaws.com/...reinforcement_<b>learning</b>/<b>mdp</b>.html", "snippet": "In this section, we will discuss how to formulate reinforcement <b>learning</b> problems using <b>Markov</b> <b>decision</b> processes (MDPs) and describe in detail various components of MDPs. Definition of an <b>MDP</b> \u00b6 A <b>Markov decision process</b> (<b>MDP</b>) is a model for how the state of a system evolves as different actions are applied to it.", "dateLastCrawled": "2022-01-27T05:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Meta-<b>Learning</b> as a <b>Markov Decision Process</b>", "url": "https://hal.archives-ouvertes.fr/tel-02422144v2/document", "isFamilyFriendly": true, "displayUrl": "https://hal.archives-ouvertes.fr/tel-02422144v2/document", "snippet": "Meta-<b>Learning</b> as a <b>Markov Decision Process</b>. <b>Machine</b> <b>Learning</b> [cs.LG]. Uni-versit\u00e9 Paris Saclay (COmUE), 2019. English. \uffffNNT: 2019SACLS588\uffff. \ufffftel-02422144v2\uffff I would like to dedicate this thesis to my loving parents doctorat CLS588 Meta-<b>Learning</b> as a <b>Markov Decision Process</b> Th\u00e8se de doctorat de l\u2019Universit\u00e9 Paris-Saclay pr\u00e9par\u00e9e \u00e0 l\u2019Universit\u00e9 Paris-Sud Ecole doctorale n 580 Sciences et Technologies de l\u2019Information et de la Communication (STIC) Sp\u00e9cialit\u00e9 de doctorat ...", "dateLastCrawled": "2022-01-13T07:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>machine</b> <b>learning</b> - Why is the <b>optimal policy</b> in <b>Markov Decision Process</b> ...", "url": "https://stats.stackexchange.com/questions/132890/why-is-the-optimal-policy-in-markov-decision-process-mdp-independent-of-the-i", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/132890", "snippet": "The intuition behind the argument saying that the <b>optimal policy</b> is independent of initial state is the following: The <b>optimal policy</b> is defined by a function that selects an action for every possible state and actions in different states are independent.. Formally speaking, for an unknown initial distribution, the value function to maximize would be the following (not conditioned on initial state)", "dateLastCrawled": "2022-01-25T23:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Real-life <b>examples</b> of <b>Markov</b> <b>Decision</b> Processes - Cross Validated", "url": "https://stats.stackexchange.com/questions/145122/real-life-examples-of-markov-decision-processes", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/145122", "snippet": "A Markovian <b>Decision</b> <b>Process</b> indeed has to do with going from one state to another and is mainly used for planning and <b>decision</b> making. The theory. Just repeating the theory quickly, an <b>MDP</b> is: $$\\text{<b>MDP</b>} = \\langle S,A,T,R,\\gamma \\rangle$$", "dateLastCrawled": "2022-01-24T10:32:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Overview: Representation Techniques", "url": "https://webcms3.cse.unsw.edu.au/static/uploads/course/COMP4418/17s2/9c0e19e0e02df66fffb5d0bd4c20697922f5ffbf9a602b66bec3f74ac83fb77c/DecisionMaking.pdf", "isFamilyFriendly": true, "displayUrl": "https://webcms3.cse.unsw.edu.au/static/uploads/course/COMP4418/17s2/9c0e19e0e02df66...", "snippet": "<b>Markov Decision Process MDP is like</b> a Markov process, except every round we make a decision Transition probabilities depend on actions taken P(St+1 = S&#39; | St = s, At = a) = P(S, a, S&#39;) Rewards for every state, action pair u(St = s, At = a) Discount factor \u03b4 Example. A <b>machine</b> can be in one of three states: good, deteriorating, broken Can take ...", "dateLastCrawled": "2022-01-21T05:20:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], []], "all_bing_queries": ["+(markov decision process (mdp))  is like +(a model that uses probabilities to predict the likely next state of a system, based on its current state)", "+(markov decision process (mdp)) is similar to +(a model that uses probabilities to predict the likely next state of a system, based on its current state)", "+(markov decision process (mdp)) can be thought of as +(a model that uses probabilities to predict the likely next state of a system, based on its current state)", "+(markov decision process (mdp)) can be compared to +(a model that uses probabilities to predict the likely next state of a system, based on its current state)", "machine learning +(markov decision process (mdp) AND analogy)", "machine learning +(\"markov decision process (mdp) is like\")", "machine learning +(\"markov decision process (mdp) is similar\")", "machine learning +(\"just as markov decision process (mdp)\")", "machine learning +(\"markov decision process (mdp) can be thought of as\")", "machine learning +(\"markov decision process (mdp) can be compared to\")"]}