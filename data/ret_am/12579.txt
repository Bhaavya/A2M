{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Overfitting and Underfitting With Machine Learning Algorithms", "url": "https://machinelearningmastery.com/overfitting-and-underfitting-with-machine-learning-algorithms/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/overfitting-and-", "snippet": "This allows us to make predictions in the future <b>on data</b> the model <b>has</b> <b>never</b> <b>seen</b>. There is a terminology used in machine learning when we talk about how well a machine learning model learns and generalizes to new <b>data</b>, namely overfitting and underfitting. Overfitting and underfitting are the two biggest causes for poor performance of machine learning algorithms. Statistical Fit. In statistics, a fit refers to how well you approximate a target function. This is good terminology to use in ...", "dateLastCrawled": "2022-02-02T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Challenges of <b>Generalization</b> in Machine Learning", "url": "https://blogs.oracle.com/site/ai-and-datascience/post/challenges-of-generalization-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://blogs.oracle.com/site/ai-and-<b>data</b>science/post/challenges-of-<b>generalization</b>-in...", "snippet": "It is well known that models such as XGBoost and random forest <b>perform</b> well on these <b>data</b>, and that some form of ensemble or bagging can further improve <b>prediction</b> results. I will be comparing two model types\u2014 random forest and fully-connected neural networks. Neural networks can be sensitive to the starting point (i.e. how all the weights are initialized to begin the training) and can produce different results for different initializations. Similar behavior is observed with random forest ...", "dateLastCrawled": "2022-01-08T15:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Machine learning questions and answers</b> \u00b7 GitHub", "url": "https://gist.github.com/byelipk/f4f3438ef994ec282f6e9d6581981418", "isFamilyFriendly": true, "displayUrl": "https://gist.github.com/byelipk/f4f3438ef994ec282f6e9d6581981418", "snippet": "The <b>algorithm</b> loads part of the <b>data</b>, runs a training step, then repeats the process until it <b>has</b> run on all the <b>data</b>. 11) What type of learning <b>algorithm</b> relies on a similarity measure to make predictions? Instance-based learning algorithms use a measure of similarity to generalize to new cases. In an instance-based learning system, the <b>algorithm</b> learns the examples by heart, then uses the similarity measure to generalize. 12) What is the difference between a model parameter and a learning ...", "dateLastCrawled": "2022-02-03T04:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "How to Identify Overfitting Machine Learning Models in Scikit-Learn", "url": "https://machinelearningmastery.com/overfitting-machine-learning-models/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/overfitting-machine-learning-models", "snippet": "Overfitting refers to an unwanted behavior of a machine learning <b>algorithm</b> used for predictive modeling. It is the case where model performance on the training dataset is improved at the cost of worse performance <b>on data</b> not <b>seen</b> during training, such as a holdout test dataset or new <b>data</b>. We can identify if a machine learning model <b>has</b> overfit ...", "dateLastCrawled": "2022-02-03T02:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Overfitting in Machine Learning - Javatpoint", "url": "https://www.javatpoint.com/overfitting-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.javatpoint.com/overfitting-in-machine-learning", "snippet": "The same happens with machine learning; if the <b>algorithm</b> learns from a small part of the <b>data</b>, it is unable to capture the required <b>data</b> points and hence under fitted. Suppose the model learns the training dataset, <b>like</b> the Y student. They <b>perform</b> very well on the <b>seen</b> dataset but <b>perform</b> badly on unseen <b>data</b> or unknown instances. In such cases ...", "dateLastCrawled": "2022-02-02T23:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Random Forests <b>Algorithm</b> explained with a real-life example and some ...", "url": "https://towardsdatascience.com/random-forests-algorithm-explained-with-a-real-life-example-and-some-python-code-affbfa5a942c", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/random-forests-<b>algorithm</b>-explained-with-a-real-life...", "snippet": "It\u2019s a <b>data</b> robust <b>algorithm</b>, being able to handle different types of <b>data</b>, and doesn\u2019t require any <b>data</b> preprocessing. The true potential of Random Forests comes from combining the results different Decision Trees. Each model is different. <b>Like</b> Decision Trees, the <b>algorithm</b> optimizes for the local split. But, instead of exploring all possible splits for each feature in the dataset, it randomly picks a subset of those features. This reduces the number of outcomes the <b>algorithm</b> needs to ...", "dateLastCrawled": "2022-02-02T19:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "How Does <b>k-Means Clustering</b> in Machine Learning ... - Towards <b>Data</b> Science", "url": "https://towardsdatascience.com/how-does-k-means-clustering-in-machine-learning-work-fdaaaf5acfa0", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/how-does-<b>k-means-clustering</b>-in-machine-learning-work...", "snippet": "This number of clusters is actually the number of centroids going around in the <b>data</b>. <b>Before</b> we go further, let\u2019s take a look at how everything so far fits within the big picture: We know that the core of Machine Learning lies on the idea of <b>generalization</b> \u2014 making a reliable <b>prediction</b> of outputs for inputs that the model <b>has</b> <b>never</b> <b>seen</b> <b>before</b>. Unsupervised Learning is all about grouping <b>data</b> samples together, regardless of their labels (if they have any). <b>Clustering</b> is an Unsupervised ...", "dateLastCrawled": "2022-02-02T04:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "1. <b>The Machine Learning Landscape</b> - Hands-On Machine Learning with ...", "url": "https://www.oreilly.com/library/view/hands-on-machine-learning/9781491962282/ch01.html", "isFamilyFriendly": true, "displayUrl": "https://www.oreilly.com/library/view/hands-on-machine-learning/9781491962282/ch01.html", "snippet": "It is often a good idea to try to reduce the dimension of your training <b>data</b> using a dimensionality reduction <b>algorithm</b> <b>before</b> you feed it to another Machine Learning <b>algorithm</b> (such as a supervised learning <b>algorithm</b>). It will run much faster, the <b>data</b> will take up less disk and memory space, and in some cases it may also <b>perform</b> better. Yet another important unsupervised task is anomaly detection\u2014for example, detecting unusual credit card transactions to prevent fraud, catching ...", "dateLastCrawled": "2022-02-01T16:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>How Gradient Boosting Algorithm Works</b> - Dataaspirant - <b>Data</b> Science ...", "url": "https://dataaspirant.com/gradient-boosting-algorithm/", "isFamilyFriendly": true, "displayUrl": "https://<b>data</b>aspirant.com/gradient-boosting-<b>algorithm</b>", "snippet": "The <b>algorithm</b> itself <b>has</b> a very straightforward visual understanding and intuition to describe weights. Let&#39;s take a look at the following toy classification problem where we&#39;re going to break the <b>data</b> between trees at depth 1 (also known as &#39;stumps&#39;) for each iteration of AdaBoost. We have the following image for the first three iterations: Increase weightage for the misclassified samples. For incorrect estimation, the size of the point is the weight assigned to it. We can see that weights ...", "dateLastCrawled": "2022-01-29T16:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Data Warehousing - Database Questions and Answers</b> | MCQ - Trenovision", "url": "https://trenovision.com/data-warehousing-database-questions-and-answers-mcq/", "isFamilyFriendly": true, "displayUrl": "https://trenovision.com/<b>data-warehousing-database-questions-and-answers</b>-mcq", "snippet": "C. <b>data</b> that are <b>never</b> altered or deleted once they have been added. D. <b>data</b> that are <b>never</b> deleted once they have been added. Show Answer. Feedback The correct answer is: A. 36. The extract process is _____. A. capturing all of the <b>data</b> contained in various operational systems. B. capturing a subset of the <b>data</b> contained in various operational systems. C. capturing all of the <b>data</b> contained in various decision support systems. D. capturing a subset of the <b>data</b> contained in various decision ...", "dateLastCrawled": "2022-02-02T19:56:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Challenges of <b>Generalization</b> in Machine Learning", "url": "https://blogs.oracle.com/site/ai-and-datascience/post/challenges-of-generalization-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://blogs.oracle.com/site/ai-and-<b>data</b>science/post/challenges-of-<b>generalization</b>-in...", "snippet": "It is well known that models such as XGBoost and random forest <b>perform</b> well on these <b>data</b>, and that some form of ensemble or bagging can further improve <b>prediction</b> results. I will be comparing two model types\u2014 random forest and fully-connected neural networks. Neural networks can be sensitive to the starting point (i.e. how all the weights are initialized to begin the training) and can produce different results for different initializations. <b>Similar</b> behavior is observed with random forest ...", "dateLastCrawled": "2022-01-08T15:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Overfitting and Underfitting With Machine Learning Algorithms", "url": "https://machinelearningmastery.com/overfitting-and-underfitting-with-machine-learning-algorithms/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/overfitting-and-", "snippet": "The cause of poor performance in machine learning is either overfitting or underfitting the <b>data</b>. In this post, you will discover the concept of <b>generalization</b> in machine learning and the problems of overfitting and underfitting that go along with it. Let&#39;s get started. Approximate a Target Function in Machine Learning Supervised machine learning is best understood as approximating a target function (f) that maps input variables", "dateLastCrawled": "2022-02-02T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "How Does <b>k-Means Clustering</b> in Machine Learning ... - Towards <b>Data</b> Science", "url": "https://towardsdatascience.com/how-does-k-means-clustering-in-machine-learning-work-fdaaaf5acfa0", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/how-does-<b>k-means-clustering</b>-in-machine-learning-work...", "snippet": "We know that the core of Machine Learning lies on the idea of <b>generalization</b> \u2014 making a reliable <b>prediction</b> of outputs for inputs that the model <b>has</b> <b>never</b> <b>seen</b> <b>before</b>. Unsupervised Learning is all about grouping <b>data</b> samples together, regardless of their labels (if they have any). <b>Clustering</b> is an Unsupervised Learning <b>algorithm</b> that groups <b>data</b> samples into k clusters. The <b>algorithm</b> yields the k clusters based on k averages of points (i.e. centroids) that roam around the <b>data</b> set trying ...", "dateLastCrawled": "2022-02-02T04:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Overfitting in Machine Learning - Javatpoint", "url": "https://www.javatpoint.com/overfitting-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.javatpoint.com/overfitting-in-machine-learning", "snippet": "The same happens with machine learning; if the <b>algorithm</b> learns from a small part of the <b>data</b>, it is unable to capture the required <b>data</b> points and hence under fitted. Suppose the model learns the training dataset, like the Y student. They <b>perform</b> very well on the <b>seen</b> dataset but <b>perform</b> badly on unseen <b>data</b> or unknown instances. In such cases ...", "dateLastCrawled": "2022-02-02T23:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Towards Preventing Overfitting</b> - DataCamp", "url": "https://www.datacamp.com/community/tutorials/towards-preventing-overfitting-regularization", "isFamilyFriendly": true, "displayUrl": "https://www.<b>data</b>camp.com/community/tutorials/<b>towards-preventing-overfitting</b>-regularization", "snippet": "Overfitting is a phenomenon where a machine learning model models the training <b>data</b> too well but fails to <b>perform</b> well on the testing <b>data</b>. Performing sufficiently good on testing <b>data</b> is considered as a kind of ultimatum in machine learning. There are quite a number of techniques which help to prevent overfitting. Regularization is one such technique. And this post is all dedicated to that. In this post, I will try to explain Regularization with less math but with more intuition. Unlike my ...", "dateLastCrawled": "2022-01-31T03:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Model <b>evaluation</b>, model selection, and <b>algorithm</b> selection in machine ...", "url": "https://sebastianraschka.com/blog/2018/model-evaluation-selection-part4.html", "isFamilyFriendly": true, "displayUrl": "https://sebastianraschka.com/blog/2018/model-<b>evaluation</b>-selection-part4.html", "snippet": "For instance, assume we develop a new learning <b>algorithm</b> or want to decide which learning <b>algorithm</b> is best to ship with our new software (a trivial example would be an email program with a learning <b>algorithm</b> that learns how to filter spam based on the user\u2019s decisions). In this case, we are want to find out how different algorithms <b>perform</b> on datasets from a <b>similar</b> problem domain.", "dateLastCrawled": "2022-02-02T16:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) <b>Generalization</b> Capabilities Enhancement of a Learning System by ...", "url": "https://www.researchgate.net/publication/42803149_Generalization_Capabilities_Enhancement_of_a_Learning_System_by_Fuzzy_Space_Clustering", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/42803149_<b>Generalization</b>_Capabilities...", "snippet": "A preprocessing of <b>data</b> is required and involves the use of a clustering <b>algorithm</b> that divides the whole learning space into subspaces to ensure better <b>generalization</b> capabilities. In this paper ...", "dateLastCrawled": "2021-12-16T03:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>k-Nearest</b> Neighbors. The <b>k-nearest</b> neighbors (KNN) <b>algorithm</b>\u2026 | by ...", "url": "https://medium.com/@rndayala/k-nearest-neighbors-a76d0831bab0", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@rndayala/<b>k-nearest</b>-neighbors-a76d0831bab0", "snippet": "The gist of the kNN <b>algorithm</b> is: 1. Compute a distance value between the item to be predicted and every item in the training <b>data</b>-set 2. Pick the k closest <b>data</b> points (the items with the k ...", "dateLastCrawled": "2022-01-30T13:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Model <b>evaluation, model selection, and algorithm selection</b> in machine ...", "url": "https://sebastianraschka.com/blog/2016/model-evaluation-selection-part3.html", "isFamilyFriendly": true, "displayUrl": "https://sebastianraschka.com/blog/2016/model-evaluation-selection-part3.html", "snippet": "Note that if we normalize <b>data</b> or select features, we typically <b>perform</b> these operations inside the k-fold cross-validation loop in contrast to applying these steps to the whole dataset upfront <b>before</b> splitting the <b>data</b> into folds. Feature selection inside the cross-validation loop reduces the bias through overfitting, since we avoid peaking at the test <b>data</b> information during the training stage. However, feature selection inside the cross-validation loop may lead to an overly pessimistic ...", "dateLastCrawled": "2022-01-27T04:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>How Gradient Boosting Algorithm Works</b> - Dataaspirant - <b>Data</b> Science ...", "url": "https://dataaspirant.com/gradient-boosting-algorithm/", "isFamilyFriendly": true, "displayUrl": "https://<b>data</b>aspirant.com/gradient-boosting-<b>algorithm</b>", "snippet": "AdaBoost works well but lacks clarification as to why the <b>algorithm</b> <b>has</b> successfully planted the seeds of doubt. Some thought it was a super ... While there is a fascinating interview with him about the development of CART and how they solved statistical problems <b>similar</b> to <b>data</b> analysis and <b>data</b> science today) more than 40 years ago. There is also an excellent lecture from Hastie, a retrospective <b>data</b> analysis by one of the pioneers of approaches that we use every day. The below image ...", "dateLastCrawled": "2022-01-29T16:37:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Chapter 10 Predictive modeling</b> | Modern <b>Data</b> Science with R", "url": "https://mdsr-book.github.io/mdsr2e/ch-modeling.html", "isFamilyFriendly": true, "displayUrl": "https://mdsr-book.github.io/mdsr2e/ch-modeling.html", "snippet": "10.1 Predictive modeling. The basic goal of predictive modeling is to find a function that accurately describes how different measured explanatory variables <b>can</b> be combined to make a <b>prediction</b> about a response variable.. A function represents a relationship between inputs and an output (see Appendix C).Outdoor temperature is a function of season: Season is the input; temperature is the output.", "dateLastCrawled": "2022-01-31T01:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Better Generalization with Forecasts</b>", "url": "https://www.researchgate.net/publication/236735712_Better_Generalization_with_Forecasts", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/236735712_<b>Better_Generalization_with_Forecasts</b>", "snippet": "The core of the <b>algorithm</b> <b>has</b> a time complexity of O(N + nm3), where N is the size of training <b>data</b>, n is the number of distinguishable outcomes of observations, and m is model state-space ...", "dateLastCrawled": "2021-11-07T09:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>How Gradient Boosting Algorithm Works</b> - Dataaspirant - <b>Data</b> Science ...", "url": "https://dataaspirant.com/gradient-boosting-algorithm/", "isFamilyFriendly": true, "displayUrl": "https://<b>data</b>aspirant.com/gradient-boosting-<b>algorithm</b>", "snippet": "The <b>algorithm</b> itself <b>has</b> a very straightforward visual understanding and intuition to describe weights. Let&#39;s take a look at the following toy classification problem where we&#39;re going to break the <b>data</b> between trees at depth 1 (also known as &#39;stumps&#39;) for each iteration of AdaBoost. We have the following image for the first three iterations: Increase weightage for the misclassified samples. For incorrect estimation, the size of the point is the weight assigned to it. We <b>can</b> see that weights ...", "dateLastCrawled": "2022-01-29T16:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Supervised</b> learning - mlstory.org", "url": "https://mlstory.org/supervised.html", "isFamilyFriendly": true, "displayUrl": "https://mlstory.org/<b>supervised</b>.html", "snippet": "The mistake bound gives a bound on the number of passes required <b>before</b> the <b>algorithm</b> terminates. From mistake bounds to <b>generalization</b> . The previous analysis shows that the perceptron finds a good predictor on the training <b>data</b>. What <b>can</b> we say about new <b>data</b> that we have not yet <b>seen</b>? To talk about <b>generalization</b>, we need to make a statistical assumption about the <b>data</b> generating process. Specifically we assume that the <b>data</b> points in the training set S=\\{(x_1,y_1)\\ldots, (x_n,y_n ...", "dateLastCrawled": "2022-02-03T15:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "How to Implement Stacked <b>Generalization</b> (Stacking) From Scratch With Python", "url": "https://machinelearningmastery.com/implementing-stacking-scratch-python/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/implementing-stacking-scratch-python", "snippet": "The thing is I <b>thought</b> <b>before</b> AdaBoostClassifiers where able to hold different machine learning algorithms, but now I realize that AdaBoostClassifiers <b>can</b> only have one type of machine learning algorithms that dosent mean that you could have 5 DecisionTreeClassifiers in an AdaBoostClassifier <b>algorithm</b> which is controlled by the hyperparameter \u201cn_estimators\u201d. I just want to know the main difference between these two ensembles, its a bit confusing. Thanks for the help.", "dateLastCrawled": "2022-01-31T10:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) Predicting Survival on Titanic by Applying Exploratory <b>Data</b> ...", "url": "https://www.researchgate.net/publication/325228831_Predicting_Survival_on_Titanic_by_Applying_Exploratory_Data_Analytics_and_Machine_Learning_Techniques", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/325228831_Predicting_Survival_on_Titanic_by...", "snippet": "The experimental results have shown the model <b>prediction</b> value around 86.29% through spot check <b>algorithm</b> which found most satisfactory over results found in the literature varied from 72 to 82% only.", "dateLastCrawled": "2022-02-03T03:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) <b>Stacked Generalization</b> - ResearchGate", "url": "https://www.researchgate.net/publication/222467943_Stacked_Generalization", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/222467943_<b>Stacked_Generalization</b>", "snippet": "set of <b>data</b>, by means of partitions of the <b>data</b> one <b>can</b> also construct a best theorist, and then use whichever theory it prefers. 1 There are many different ways to implement <b>stacked generalization</b>.", "dateLastCrawled": "2022-01-30T17:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "How to Create an ARIMA Model <b>for Time Series Forecasting</b> in Python", "url": "https://machinelearningmastery.com/arima-for-time-series-forecasting-with-python/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/arim", "snippet": "A popular and widely used statistical method <b>for time series forecasting</b> is the ARIMA model. ARIMA is an acronym that stands for AutoRegressive Integrated Moving Average. It is a class of model that captures a suite of different standard temporal structures in time series <b>data</b>. In this tutorial, you will discover how to develop an ARIMA model <b>for time series forecasting</b> in", "dateLastCrawled": "2022-02-02T04:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "5.1 MLBasics-Learning.ppt", "url": "http://clgiles.ist.psu.edu/IST597/materials/slides/lect2/ch5.pptx", "isFamilyFriendly": true, "displayUrl": "clgiles.ist.psu.edu/IST597/materials/slides/lect2/ch5.pptx", "snippet": "The <b>algorithm</b> provides a <b>prediction</b> of the values of the missingentries. 10.Denoising. The ML <b>algorithm</b> is given as an input corrupted examples . x ~ \u03b5 R . n . froman unknown corruption process. Learner must predict the clean example . x. from its corrupted version. x ~ Or more generally the conditional probability distribution. p (x|x ~) 11. Density Estimation or Probability Mass Function Estimation. Learn a function. p. model:Rn Rwhere. p. model (x) <b>can</b> be interpreted as a pdf if x is ...", "dateLastCrawled": "2022-01-31T17:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Data Warehousing - Database Questions and Answers</b> | MCQ - Trenovision", "url": "https://trenovision.com/data-warehousing-database-questions-and-answers-mcq/", "isFamilyFriendly": true, "displayUrl": "https://trenovision.com/<b>data-warehousing-database-questions-and-answers</b>-mcq", "snippet": "C. <b>data</b> that are <b>never</b> altered or deleted once they have been added. D. <b>data</b> that are <b>never</b> deleted once they have been added. Show Answer. Feedback The correct answer is: A. 36. The extract process is _____. A. capturing all of the <b>data</b> contained in various operational systems. B. capturing a subset of the <b>data</b> contained in various operational systems. C. capturing all of the <b>data</b> contained in various decision support systems. D. capturing a subset of the <b>data</b> contained in various decision ...", "dateLastCrawled": "2022-02-02T19:56:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Challenges of <b>Generalization</b> in Machine Learning", "url": "https://blogs.oracle.com/ai-and-datascience/post/challenges-of-generalization-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://blogs.oracle.com/ai-and-<b>data</b>science/post/challenges-of-<b>generalization</b>-in...", "snippet": "Good practice dictates that the models only \u201csee\u201d the training <b>data</b> during learning, and the results of the model are <b>compared</b> based on the predictions for the validation <b>data</b>. The model giving the best validation results is selected as the best choice. Then, the model is trained on a number (denoted as k) of different subsets of the <b>data</b>, and the expected performance in the future is given by mean and standard deviation of the k validation results on the validation <b>data</b>, and a final ...", "dateLastCrawled": "2022-01-29T09:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Overfitting and Underfitting With Machine Learning Algorithms", "url": "https://machinelearningmastery.com/overfitting-and-underfitting-with-machine-learning-algorithms/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/overfitting-and-", "snippet": "Now, I always see (on the <b>data</b> that I have) that an overfit model (Model that <b>has</b> very low MSE on the train test <b>compared</b> to the Mean MSE from cross validations ) performs very well on the test set <b>compared</b> to a properly fit model. This makes me lean towards a overfit model.I have shuffled my train set 5 times and trained the overfit and under-fit models but i still find the overfit model as a winner. Please advise how I <b>can</b> improve my selection process.", "dateLastCrawled": "2022-02-02T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Model <b>evaluation</b>, model selection, and <b>algorithm</b> selection in machine ...", "url": "https://sebastianraschka.com/blog/2018/model-evaluation-selection-part4.html", "isFamilyFriendly": true, "displayUrl": "https://sebastianraschka.com/blog/2018/model-<b>evaluation</b>-selection-part4.html", "snippet": "If the dataset is very small, it might not be feasible to set aside <b>data</b> for testing, and in such cases, we <b>can</b> use k-fold cross-validation with a large k or Leave-one-out cross-validation as a workaround for evaluating the <b>generalization</b> performance. However, using these procedures, we have to bear in mind that we then do not compare between models but different algorithms that produce different models on the training folds. Nonetheless, the average performance over the different test folds ...", "dateLastCrawled": "2022-02-02T16:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Random Forests <b>Algorithm</b> explained with a real-life example and some ...", "url": "https://towardsdatascience.com/random-forests-algorithm-explained-with-a-real-life-example-and-some-python-code-affbfa5a942c", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/random-forests-<b>algorithm</b>-explained-with-a-real-life...", "snippet": "In a Regression task you <b>can</b> calculate actual variance of the <b>prediction</b> <b>compared</b> to the true targets. If the tree produces results that are too far off from its true targets, it <b>has</b> high-variance and therefore, it is overfit. A highly overfit tree <b>has</b> high-variance. That means its predictions are extremely far off from the actual targets.", "dateLastCrawled": "2022-02-02T19:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Evaluation</b> Metrics for Machine Learning Models | by Bhajandeep Singh ...", "url": "https://heartbeat.comet.ml/evaluation-metrics-for-machine-learning-models-d42138496366", "isFamilyFriendly": true, "displayUrl": "https://heartbeat.comet.ml/<b>evaluation</b>-metrics-for-machine-learning-models-d42138496366", "snippet": "As we\u2019ve <b>seen</b>, the ROC curve isn\u2019t just a single number; it\u2019s a whole curve. It provides nuanced details about the behavior of the classifier, but it\u2019s also hard to quickly compare many ROC curves to each other. The AUC is one way to summarize the ROC curve into a single number so that it <b>can</b> <b>be compared</b> easily and automatically. A good ...", "dateLastCrawled": "2022-01-28T22:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A Federated Learning Aggregation <b>Algorithm</b> for Pervasive Computing ...", "url": "https://deepai.org/publication/a-federated-learning-aggregation-algorithm-for-pervasive-computing-evaluation-and-comparison", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/a-federated-learning-aggregation-<b>algorithm</b>-for...", "snippet": "This scheme might explain why the FedPer <b>algorithm</b> does not succeed in generalizing (<b>Generalization</b> accuracy = 53.01%), as it <b>can</b> be <b>seen</b> in Figure 7. The personalization layer stays too strong, and such the model is not able to react appropriately to new <b>data</b>. However, the results show that even the personalization feature of FedPer is not significantly better than any of the other FL algorithms.", "dateLastCrawled": "2021-12-08T07:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A survey on Image <b>Data Augmentation</b> for Deep Learning | Journal of Big ...", "url": "https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0197-0", "isFamilyFriendly": true, "displayUrl": "https://journalofbig<b>data</b>.springeropen.com/articles/10.1186/s40537-019-0197-0", "snippet": "Improving the <b>generalization</b> ability of these models is one of the most difficult challenges. Generalizability refers to the performance difference of a model when evaluated on previously <b>seen</b> <b>data</b> (training <b>data</b>) versus <b>data</b> it <b>has</b> <b>never</b> <b>seen</b> <b>before</b> (testing <b>data</b>). Models with poor generalizability have overfitted the training <b>data</b>. One way to ...", "dateLastCrawled": "2022-02-02T08:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Model <b>evaluation, model selection, and algorithm selection</b> in machine ...", "url": "https://sebastianraschka.com/blog/2016/model-evaluation-selection-part3.html", "isFamilyFriendly": true, "displayUrl": "https://sebastianraschka.com/blog/2016/model-evaluation-selection-part3.html", "snippet": "Note that if we normalize <b>data</b> or select features, we typically <b>perform</b> these operations inside the k-fold cross-validation loop in contrast to applying these steps to the whole dataset upfront <b>before</b> splitting the <b>data</b> into folds. Feature selection inside the cross-validation loop reduces the bias through overfitting, since we avoid peaking at the test <b>data</b> information during the training stage. However, feature selection inside the cross-validation loop may lead to an overly pessimistic ...", "dateLastCrawled": "2022-01-27T04:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Supervised</b> learning - mlstory.org", "url": "https://mlstory.org/supervised.html", "isFamilyFriendly": true, "displayUrl": "https://mlstory.org/<b>supervised</b>.html", "snippet": "The mistake bound gives a bound on the number of passes required <b>before</b> the <b>algorithm</b> terminates. From mistake bounds to <b>generalization</b> . The previous analysis shows that the perceptron finds a good predictor on the training <b>data</b>. What <b>can</b> we say about new <b>data</b> that we have not yet <b>seen</b>? To talk about <b>generalization</b>, we need to make a statistical assumption about the <b>data</b> generating process. Specifically we assume that the <b>data</b> points in the training set S=\\{(x_1,y_1)\\ldots, (x_n,y_n ...", "dateLastCrawled": "2022-02-03T15:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Understanding the Bias-<b>Variance</b> Tradeoff | by ... - Towards <b>Data</b> Science", "url": "https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/understanding-the-bias-<b>variance</b>-tradeoff-165e6942b229", "snippet": "Understanding the Bias-<b>Variance</b> Tradeoff. Whenever we discuss model <b>prediction</b>, it\u2019s important to understand <b>prediction</b> errors (bias and <b>variance</b>). There is a tradeoff between a model\u2019s ability to minimize bias and <b>variance</b>. Gaining a proper understanding of these errors would help us not only to build accurate models but also to avoid the ...", "dateLastCrawled": "2022-01-31T01:31:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>MACHINE</b> <b>LEARNING</b> OF MORPHOLOGICAL RULES BY <b>GENERALIZATION</b> AND <b>ANALOGY</b>", "url": "https://aclanthology.org/C86-1069.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/C86-1069.pdf", "snippet": "<b>MACHINE</b> <b>LEARNING</b> OF MORPHOLOGICAL RULES BY <b>GENERALIZATION</b> AND <b>ANALOGY</b> Klaus Wothke ArbeiLssLe]le LinguisLische DaLenverarbeiLung INSTI[UI FOR DEUTSCHE SPRAI;HE Mannheim, West. Germany ABSTRAI:T: 1his paper describes an experi- menLal procedure For Lhe inducLive auLomaLed <b>learning</b> of morphological rules From exam- ples. At First an ouL].irle of Lhe problem is given. Then a Formalism for Lhe represen- t. arian of morphological rules is defined. This Formalism is used by Lhe auLomaLed procedure ...", "dateLastCrawled": "2021-09-17T12:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Machine</b> <b>Learning</b> of Morphological Rules by <b>Generalization</b> and <b>Analogy</b> ...", "url": "https://aclanthology.org/C86-1069/", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/C86-1069", "snippet": "Klaus Wothke. Coling 1986 Volume 1: The 11th International Conference on Computational Linguistics. 1986.", "dateLastCrawled": "2021-12-30T08:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "About <b>generalization</b>, abstraction and analogies | by Tudor Surdoiu ...", "url": "https://dacus-augustus.medium.com/about-generalization-abstraction-and-analogies-e59aa16e7871", "isFamilyFriendly": true, "displayUrl": "https://dacus-augustus.medium.com/about-<b>generalization</b>-abstraction-and-analogies-e59aa...", "snippet": "The pursuit of better <b>generalization</b> is probably the underlining\u2026 Get started. Open in app. Tudor Surdoiu. Sign in. Get started. Follow. 78 Followers. About. Get started. Open in app. About <b>generalization</b>, abstraction and analogies. Tudor Surdoiu. Just now \u00b7 4 min read. A presentation of essential cognition concepts inspired by the book Deep <b>Learning</b> with Python, second edition by Fran\u00e7ois Chollet. Photo by Eli\u0161ka Motisov\u00e1 on Unsplash Introduction. The pursuit of better <b>generalization</b> ...", "dateLastCrawled": "2022-01-30T15:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Real Artificial Intelligence: Understanding <b>Extrapolation</b> vs <b>Generalization</b>", "url": "https://towardsdatascience.com/real-artificial-intelligence-understanding-extrapolation-vs-generalization-b8e8dcf5fd4b", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/real-artificial-intelligence-understanding...", "snippet": "<b>Generalization</b> is the entire point of <b>machine</b> <b>learning</b>. Trained to solve one problem, the model attempts to utilize the patterns learned from that task to solve the same task, with slight variations. In <b>analogy</b>, consider a child being taught how to perform single-digit addition. <b>Generalization</b> is the act of performing tasks of the same difficulty and nature. This may also be referred to as interpolation, although <b>generalization</b> is a more commonly used and understood term.", "dateLastCrawled": "2022-02-01T00:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Statistical Relational Learning through Structural Analogy</b> and ...", "url": "https://www.qrg.northwestern.edu/papers/Files/QRG_Dist_Files/QRG_2011/Halstead-2011-Dissertation.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.qrg.northwestern.edu/papers/Files/QRG_Dist_Files/QRG_2011/Halstead-2011...", "snippet": "<b>Structural Analogy and Probabilistic Generalization</b> Daniel T. Halstead My primary research motivation is the development of a truly generic <b>Machine</b> <b>Learning</b> engine. Towards this, I am exploring the interplay between feature-based representations of data, for which there are powerful statistical <b>machine</b> <b>learning</b> algorithms, and structured representations, which are useful for reasoning and are capable of representing a broader spectrum of information. This places my work in the emergent field ...", "dateLastCrawled": "2021-08-29T05:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Conceptualization as a Basis for Cognition \u2014 Human and <b>Machine</b> | by ...", "url": "https://towardsdatascience.com/conceptualization-as-a-basis-for-cognition-human-and-machine-345d9e687e3c", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/conceptualization-as-a-basis-for-cognition-human-and...", "snippet": "Abstraction and <b>analogy</b> allow concepts to be re-applied in new domains. There are many, often conflicting, ... An overview of <b>generalization</b>: In <b>machine</b> <b>learning</b>, <b>generalization</b> refers to the capability of a trained model to classify or forecast unseen data. A generalized model will normally work for all subsets of unseen data. Goodfellow, Bengio, and Courville discuss the concepts of overfitting and underfitting. They point out how challenging it is for <b>machine</b>-<b>learning</b> algorithms to ...", "dateLastCrawled": "2022-01-20T17:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine Learning</b> - SlideShare", "url": "https://www.slideshare.net/darshanharry/machine-learning-46440299", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/darshanharry/<b>machine-learning</b>-46440299", "snippet": "<b>Generalization</b> 6. <b>Machine learning</b> and data mining 7. Algorithms 8. Decision tree <b>learning</b> 9. Other <b>learning</b> techniques 10.Examples 11.Applications 12.Few quotes 13.Question and answers 3. Introduction <b>Machine learning</b>, a branch of artificial intelligence, concerns the construction and study of systems that can learn from data. 4. \u2022In 1959, Arthur Samuel defined <b>machine learning</b> as a &quot;Field of study that gives computers the ability to learn without being explicitly programmed&quot;. \u2022\u201cThe ...", "dateLastCrawled": "2022-02-02T23:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>LEARNING</b> BY <b>ANALOGY: FORMULATING AND GENERALIZING PLANS FROM</b> PAST ...", "url": "https://www.sciencedirect.com/science/article/pii/B9780080510545500091", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/B9780080510545500091", "snippet": "Most work in <b>machine</b> <b>learning</b> has not addressed the issue of integrating <b>learning</b> and problem-solving into a unified process. (However, Chapter 6 of this book and Lenat [1977] are partial counterexamples.) Past and present investigations of analogical reasoning have focused on disjoint aspects of the problem. For instance, Winston [1980] investigated <b>analogy</b> as a powerful mechanism for classifying and structuring episodic descriptions. Kling [1971] studied <b>analogy</b> as a means of reducing the ...", "dateLastCrawled": "2021-12-05T21:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Basic Concepts in Machine Learning</b>", "url": "https://machinelearningmastery.com/basic-concepts-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>basic-concepts-in-machine-learning</b>", "snippet": "What are the <b>basic concepts in machine learning</b>? I found that the best way to discover and get a handle on the <b>basic concepts in machine learning</b> is to review the introduction chapters to <b>machine learning</b> textbooks and to watch the videos from the first model in online courses. Pedro Domingos is a lecturer and professor on <b>machine learning</b> at the University of Washing and", "dateLastCrawled": "2022-02-02T15:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "[2001.06668] <b>Learning</b> to See Analogies: A Connectionist Exploration", "url": "https://arxiv.org/abs/2001.06668", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/abs/2001.06668", "snippet": "This dissertation explores the integration of <b>learning</b> and <b>analogy</b>-making through the development of a computer program, called Analogator, that learns to make analogies by example. By &quot;seeing&quot; many different <b>analogy</b> problems, along with possible solutions, Analogator gradually develops an ability to make new analogies. That is, it learns to make analogies by <b>analogy</b>. This approach stands in contrast to most existing research on <b>analogy</b>-making, in which typically the a priori existence of ...", "dateLastCrawled": "2022-01-31T14:23:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>DBMS Generalization - javatpoint</b>", "url": "https://www.javatpoint.com/dbms-generalization", "isFamilyFriendly": true, "displayUrl": "https://www.javatpoint.com/dbms-<b>generalization</b>", "snippet": "<b>Generalization is like</b> a bottom-up approach in which two or more entities of lower level combine to form a higher level entity if they have some attributes in common. In <b>generalization</b>, an entity of a higher level can also combine with the entities of the lower level to form a further higher level entity. <b>Generalization</b> is more like subclass and superclass system, but the only difference is the approach. <b>Generalization</b> uses the bottom-up approach. In <b>generalization</b>, entities are combined to ...", "dateLastCrawled": "2022-02-02T14:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Model Compression with <b>TensorFlow</b> Lite: A Look into Reducing Model Size ...", "url": "https://towardsdatascience.com/model-compression-a-look-into-reducing-model-size-8251683c338e", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/model-compression-a-look-into-reducing-model-size...", "snippet": "To overly simplify for the gist of understanding <b>machine</b> <b>learning</b> models, a neural network is a set of nodes with weights(W) that connect between nodes. You can think of this as a set of instructions that we optimize to increase our likelihood of generating our desired class. The more specific this set of instructions are, the greater our model size, which is dependent on the size of our parameters (our configuration variables such as weight). Artificial Neural Network (Image by Govind ...", "dateLastCrawled": "2022-02-01T17:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Generalization, Specialization and Aggregation</b> in ER Model | Studytonight", "url": "https://www.studytonight.com/dbms/generalization-and-specialization.php", "isFamilyFriendly": true, "displayUrl": "https://www.studytonight.com/dbms/generalization-and-specialization.php", "snippet": "Generalization is a bottom-up approach in which two lower level entities combine to form a higher level entity. In generalization, the higher level entity can also combine with other lower level entities to make further higher level entity. It&#39;s more like Superclass and Subclass system, but the only difference is the approach, which is bottom-up.", "dateLastCrawled": "2022-02-03T00:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Consciousness Revisited - EETimes", "url": "https://www.eetimes.com/podcasts/wb-ep166/", "isFamilyFriendly": true, "displayUrl": "https://www.eetimes.com/podcasts/wb-ep166", "snippet": "And a <b>machine</b> doesn\u2019t have any of those. A <b>machine</b> has no freewill, has no comprehension. A simply algorithmic understanding, but algorithmic in understanding is not real understanding. The real understanding of conscience is non-algorithmic. It is a feeling that we have that we understand. And that feeling is closer to reality, let\u2019s say, but it\u2019s not algorithmic and allows us to make decisions which are much better than a computer when you have new situations.", "dateLastCrawled": "2022-01-31T22:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Future Internet | Free Full-Text | Misconfiguration in Firewalls and ...", "url": "https://www.mdpi.com/1999-5903/13/11/283/htm", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/1999-5903/13/11/283/htm", "snippet": "Firewalls and network access controls play important roles in security control and protection. Those firewalls may create an incorrect sense or state of protection if they are improperly configured. One of the major configuration problems in firewalls is related to misconfiguration in the access control roles added to the firewall that will control network traffic. In this paper, we evaluated recent research trends and open challenges related to firewalls and access controls in general and ...", "dateLastCrawled": "2022-01-30T23:01:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A deep <b>learning</b> approach to identifying immunogold particles in ...", "url": "https://www.nature.com/articles/s41598-021-87015-2", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41598-021-87015-2", "snippet": "<b>Generalization is similar</b> to transfer <b>learning</b> obtained through training, as it allows the learned evaluative features of a convolutional neural network to accomplish a similar level of ...", "dateLastCrawled": "2022-02-02T19:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Deep convolutional learning for general early design stage prediction</b> ...", "url": "https://www.sciencedirect.com/science/article/abs/pii/S1474034619305555", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/abs/pii/S1474034619305555", "snippet": "Deep <b>learning</b> models extract features from data, which aid in model generalization. In this study, we (1) evaluate the deep <b>learning</b> model\u2019s capability to predict the heating and cooling demand on unseen design cases and (2) obtain an understanding of extracted features. Results indicate that deep <b>learning</b> model <b>generalization is similar</b> to or better than that of a simple neural network with appropriate features. The reason for the satisfactory generalization using the deep <b>learning</b> model ...", "dateLastCrawled": "2021-10-18T07:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Generalization-Based k-Anonymization | SpringerLink", "url": "https://link.springer.com/chapter/10.1007%2F978-3-319-23240-9_17", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/chapter/10.1007/978-3-319-23240-9_17", "snippet": "The way to construct that <b>generalization is similar</b> that the one used in growing decision trees. Records that cannot be generalized satisfactorily are discarded, therefore some information is lost. In the experiments we performed we prove that the new approach gives good results. Keywords k-anonymity Generalization This is a preview of subscription content, log in to check access. Notes. Acknowledgments. This research is partially funded by the Spanish MICINN projects COGNITIO (TIN-2012 ...", "dateLastCrawled": "2022-01-27T22:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Mosaicking to Distill: Knowledge Distillation from Out-of-Domain Data", "url": "https://proceedings.neurips.cc/paper/2021/file/63dc7ed1010d3c3b8269faf0ba7491d4-Paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/2021/file/63dc7ed1010d3c3b8269faf0ba7491d4-Paper.pdf", "snippet": "KD has demonstrated encouraging results over various <b>machine</b> <b>learning</b> applications, including but not limited to computer vision [6, 29], data mining [2], and natural language processing [46, 20] Nevertheless, the conventional setup for KD has largely relied on the premise that, data from at least the same domain, if not the original training data, is available to train the student. This seemingly-mind assumption, paradoxically, imposes a major constraint for conventional KD approaches: in ...", "dateLastCrawled": "2022-01-30T20:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Analysis on weighted AUC for <b>imbalanced data learning through isometrics</b>", "url": "https://www.researchgate.net/publication/289018143_Analysis_on_weighted_AUC_for_imbalanced_data_learning_through_isometrics", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/289018143_Analysis_on_weighted_AUC_for...", "snippet": "This <b>generalization is similar</b> to some data related work, where the weighted AUC (see, e.g., ... <b>Machine</b> <b>learning</b> has been readily applied to high-speed network traffic classification. Evaluating ...", "dateLastCrawled": "2022-01-29T01:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) <b>MACHINE</b> <b>LEARNING</b> FOR ENERGY PERFORMANCE PREDICTION IN EARLY ...", "url": "https://www.researchgate.net/publication/339473161_MACHINE_LEARNING_FOR_ENERGY_PERFORMANCE_PREDICTION_IN_EARLY_DESIGN_STAGE_OF_BUILDINGS", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/339473161_<b>MACHINE</b>_<b>LEARNING</b>_FOR_ENERGY...", "snippet": "<b>Machine</b> <b>learning</b> (ML) models exhibit the potential for rapid and accurate predictions. Developing conventional ML models that can be generalized well in unseen design cases requires an effective ...", "dateLastCrawled": "2021-11-16T18:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "11 Projected Newton-type Methods in <b>Machine</b> <b>Learning</b> - PDF Free Download", "url": "https://docplayer.net/267612-11-projected-newton-type-methods-in-machine-learning.html", "isFamilyFriendly": true, "displayUrl": "https://docplayer.net/267612-11-projected-newton-type-methods-in-<b>machine</b>-<b>learning</b>.html", "snippet": "1 11 Projected Newton-type Methods in <b>Machine</b> <b>Learning</b> Mark Schmidt University of British Columbia Vancouver, BC, V6T 1Z4 Dongmin Kim University of Texas at Austin Austin, Texas Suvrit Sra Max Planck Insitute for Biological Cybernetics 72076, T\u00fcbingen, Germany We consider projected Newton-type methods for solving large-scale optimization problems arising in <b>machine</b> <b>learning</b> and related fields. We first introduce an algorithmic framework for projected Newton-type methods by reviewing a ...", "dateLastCrawled": "2021-07-21T16:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Component-based <b>machine</b> <b>learning</b> for performance prediction in building ...", "url": "https://www.semanticscholar.org/paper/Component-based-machine-learning-for-performance-in-Geyer-Singaravel/97e249874fa833713630e050291761a10c40bbd4", "isFamilyFriendly": true, "displayUrl": "https://www.semanticscholar.org/paper/Component-based-<b>machine</b>-<b>learning</b>-for-performance...", "snippet": "A component-based approach that develops <b>machine</b> <b>learning</b> models not only for a parameterized whole building design, but for parameterized components of the design as well is presented. <b>Machine</b> <b>learning</b> is increasingly being used to predict building performance. It replaces building performance simulation, and is used for data analytics. Major benefits include the simplification of prediction models and a dramatic reduction in computation times. However, the monolithic whole-building models ...", "dateLastCrawled": "2022-01-03T15:36:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Characterizing a Brain-Based Value-Function Approximator</b>", "url": "https://www.researchgate.net/publication/221441918_Characterizing_a_Brain-Based_Value-Function_Approximator", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221441918_<b>Characterizing_a_Brain-Based_Value</b>...", "snippet": "yet translated into <b>machine</b> <b>learning</b> RL. <b>Just as generalization</b> improves <b>learning</b>. e\ufb03ciency by spreading learned v alue to nea rby states, the clas sical conditioning. phenomena of &quot;latent ...", "dateLastCrawled": "2021-10-23T12:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "How is a motor <b>skill learned? Change and invariance</b> at the levels of ...", "url": "https://journals.physiology.org/doi/full/10.1152/jn.00856.2011", "isFamilyFriendly": true, "displayUrl": "https://journals.physiology.org/doi/full/10.1152/jn.00856.2011", "snippet": "Generalization in skill <b>learning</b> can suggest features of how skill is controlled and neurally represented, <b>just as generalization</b> in adaptation can provide insight into functional and neural bases of sensorimotor mappings (Shadmehr 2004; Tanaka et al. 2009). Crucially, we tested for generalization across levels of difficulty (speed), which, for a task characterized by an SAF, serves as a window into the functional organization of motor skill. We found that training caused the SAF to shift as ...", "dateLastCrawled": "2022-01-10T15:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>The American Scene</b>, by Henry James - New Paltz", "url": "https://www2.newpaltz.edu/~hathawar/americanscene2.html", "isFamilyFriendly": true, "displayUrl": "https://www2.newpaltz.edu/~hathawar/americanscene2.html", "snippet": "That was better, somehow, than the adventure of a little later--my <b>learning</b>, too definitely, that another stream, ample, admirable, in every way distinguished, a stream worthy of Ruysdael or Salvator Rosa, was known but as the Farmington River. This I could in no manner put up with--this taking by the greater of the comparatively common little names of the less. Farmington, as I was presently to learn, is a delightful, a model village; but villages, fords, bridges are not the godparents of ...", "dateLastCrawled": "2022-01-21T21:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Language Log: January 2004 Archives", "url": "http://itre.cis.upenn.edu/~myl/languagelog/archives/2004_01.html", "isFamilyFriendly": true, "displayUrl": "itre.cis.upenn.edu/~myl/languagelog/archives/2004_01.html", "snippet": "Then he pressed a button and the <b>machine</b> began listing all the phrases in my works in which the word grease appears in one form or another. There they were, streaming across the screen in front of me, faster than I could read them, with page references and line numbers. The greasy floor, the roads greasy with rain, the grease-stained cuff, the greasy jam butty, his greasy smile, the grease-smeared table, the greasy small change of their conversation, even, would you believe it, his body ...", "dateLastCrawled": "2022-02-02T09:26:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Shape Recognition, the magnitude of the challenge a <b>machine</b> <b>learning</b> ...", "url": "https://www.ics.uci.edu/~majumder/vispercep/paper08/shapeRecognition.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.ics.uci.edu/~majumder/vispercep/paper08/shapeRecognition.pdf", "snippet": "Using <b>machine</b> <b>learning</b> one can put the problem of shape generalization in the context of generative models. Generative models attempt to understand the relations of the variables (Chairs: how many legs, what angles are they relative to each-other, is there a long \ufb02at surface etc) within instances of a type of objects. Object <b>generalization can be thought of as</b> the following problem: Given a set of input vectors or objects generate and recognize members of that set (datum have high that ...", "dateLastCrawled": "2022-01-05T00:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Fermat&#39;s Library</b> | Deep <b>Learning</b>: A Critical Appraisal annotated ...", "url": "https://fermatslibrary.com/s/deep-learning-a-critical-appraisal", "isFamilyFriendly": true, "displayUrl": "https://<b>fermatslibrary</b>.com/s/deep-<b>learning</b>-a-critical-appraisal", "snippet": "As discussed later in this article, <b>generalization can be thought of as</b> coming in two . flavors, interpolation between known examples, and extrapolation, which requires going . beyond a space of known training examples (Marcus, 1998a). For neural networks to generalize well, there generally must be a lar ge amount of data, and the test data must be similar to the training data, allowing new answers to be . interpolated in between old ones. In Krizhevsky et al\u2019 s paper (Krizhevsky ...", "dateLastCrawled": "2021-12-06T09:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "terminology - <b>Extrapolation</b> v. Interpolation - Cross Validated", "url": "https://stats.stackexchange.com/questions/418803/extrapolation-v-interpolation", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/418803", "snippet": "<b>generalization can be thought of as</b> coming in two flavors, interpolation between known examples, and <b>extrapolation</b>, which requires going beyond a space of known training examples. The author wrote that <b>extrapolation</b> is a wall stopping us reaching artificial general intelligence. Let&#39;s suppose that we train a translation model to translate English to German very well with tons of data, we can be sure that it can fail a test with randomly permutated English words because it has never seen such ...", "dateLastCrawled": "2022-02-03T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) <b>Sparse Representations for Fast, One-Shot</b> <b>Learning</b>", "url": "https://www.researchgate.net/publication/2553612_Sparse_Representations_for_Fast_One-Shot_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/.../2553612_<b>Sparse_Representations_for_Fast_One-Shot</b>_<b>Learning</b>", "snippet": "<b>Generalization can be thought of as</b> nding a collection of mcubes (0 m n) covering the positive ones without overlapping the negative ones. A 0-cube is a point, 1-cube is a line, and so on. There ...", "dateLastCrawled": "2022-01-13T20:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Expressivity,<b>Trainability,and</b> Generalization in <b>Machine Learning</b> - \u4e91+\u793e\u533a ...", "url": "https://cloud.tencent.com/developer/article/1091188", "isFamilyFriendly": true, "displayUrl": "https://cloud.tencent.com/developer/article/1091188", "snippet": "A <b>Machine Learning</b> model is any computer program that has some of its functionality learned from data. During \u201c<b>learning</b>\u201d, we search for a reasonably good model that utilizes knowledge from the data to make decisions, out of a (potentially huge) space of models. This search process is usually formulated as solving an optimization problem over the space of models. Several Varieties of Optimization. A common approach, especially in deep <b>learning</b>, is to define some scalar metric that ...", "dateLastCrawled": "2022-01-28T07:40:00.0000000Z", "language": "ja", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "deep <b>learning</b> a critical appraisal.formatted", "url": "https://arxiv.org/pdf/1801.00631.pdf", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/pdf/1801.00631.pdf", "snippet": "As discussed later in this article, <b>generalization can be thought of as</b> coming in two flavors, interpolation between known examples, and extrapolation, which requires going beyond a space of known training examples (Marcus, 1998a). For neural networks to generalize well, there generally must be a large amount of data, and the test data must be similar to the training data, allowing new answers to be interpolated in between old ones. In Krizhevsky et al\u2019s paper (Krizhevsky, Sutskever ...", "dateLastCrawled": "2021-07-14T03:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Categorization = decision making + generalization - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S0149763413000754", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0149763413000754", "snippet": "Reinforcement <b>learning</b> models generally assume that <b>learning</b> takes place as a Markov decision process, meaning they assume the model can be represented as a series of discrete states where the next state is a function only of the current state and the current stimuli representing the environment. This simplification has an immediate consequence regarding category <b>learning</b>. The value of available options is conditionalized on the current state; similar yet not identical states do not inherit ...", "dateLastCrawled": "2022-01-05T03:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Eric Jang: 2017", "url": "https://blog.evjang.com/2017/", "isFamilyFriendly": true, "displayUrl": "https://blog.evjang.com/2017", "snippet": "Contrast this with supervised <b>learning</b> and unsupervised <b>learning</b>, we can obtain <b>learning</b> signals cheaply, no matter where we are in the model search space. The proposal distribution for minibatch gradients has nonzero overlap with the distribution of gradients. If we are using SGD with minibatch size=1, then the probability of sampling the transition with a useful <b>learning</b> signal is at worst 1/N where N is the size of the dataset (so <b>learning</b> is guaranteed after each epoch). We can brute ...", "dateLastCrawled": "2021-12-17T09:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Fast clustering-based anonymization approaches with time constraints ...", "url": "https://www.sciencedirect.com/science/article/pii/S0950705113000877", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0950705113000877", "snippet": "1. Introduction. With the advance of the data mining techniques and people\u2019s increasing concerns about the personal privacy, how to share the information without disclosing the personal privacy has become an important research topic in recent years .Extensive research work has been done on the protection of static data , , , , , , , , , , , , . k-anonymity , , \u2113-diversity , t-closeness , \u03b5-differential privacy and other principles are widely applied in designing the privacy preserving ...", "dateLastCrawled": "2021-12-11T00:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Methods in Behavioral Research [13th</b>&amp;nbsp;ed.] 9781259676987 - DOKUMEN.PUB", "url": "https://dokumen.pub/methods-in-behavioral-research-13thnbsped-9781259676987.html", "isFamilyFriendly": true, "displayUrl": "https://dokumen.pub/methods-in-behavioral-research-13thnbsped-9781259676987.html", "snippet": "<b>LEARNING</b> OBJECTIVES Summarize Milgram\u2019s obedience experiment. Discuss the three ethical principles outlined in the Belmont Report: beneficence, autonomy, and justice. Define deception and discuss the ethical issues surrounding its use in research. List the information contained in an informed consent form. Discuss potential problems in obtaining informed consent. Describe the purpose of debriefing research participants. Describe the function of an Institutional Review Board. Contrast the ...", "dateLastCrawled": "2022-01-30T14:39:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(generalization)  is like +(a prediction of how the algorithm would perform on data that it has never seen before)", "+(generalization) is similar to +(a prediction of how the algorithm would perform on data that it has never seen before)", "+(generalization) can be thought of as +(a prediction of how the algorithm would perform on data that it has never seen before)", "+(generalization) can be compared to +(a prediction of how the algorithm would perform on data that it has never seen before)", "machine learning +(generalization AND analogy)", "machine learning +(\"generalization is like\")", "machine learning +(\"generalization is similar\")", "machine learning +(\"just as generalization\")", "machine learning +(\"generalization can be thought of as\")", "machine learning +(\"generalization can be compared to\")"]}