{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Sequence Modeling with Neural Networks (Part</b> 2): Attention Models ...", "url": "https://indicodata.ai/blog/sequence-modeling-neural-networks-part2-attention-models/", "isFamilyFriendly": true, "displayUrl": "https://<b>indico</b>data.ai/blog/<b>sequence</b>-<b>model</b>ing-<b>neural</b>-<b>networks</b>-part2-attention-<b>models</b>", "snippet": "Our decoder <b>network</b> can now use different portions of the encoder <b>sequence</b> as context while it\u2019s processing the decoder <b>sequence</b>, instead of using a single fixed representation of the input <b>sequence</b>. This allows the <b>network</b> to focus on the most important parts of the input <b>sequence</b> instead of the whole input <b>sequence</b>, therefore producing smarter predictions for the next word in the decoder <b>sequence</b>. Below is a diagram showing a <b>sequence</b> to <b>sequence</b> <b>model</b> that uses attention during training ...", "dateLastCrawled": "2022-02-01T01:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Sequence Modeling with Neural Networks \u2013 Part</b> I - KDnuggets", "url": "https://www.kdnuggets.com/2018/10/sequence-modeling-neural-networks-part-1.html", "isFamilyFriendly": true, "displayUrl": "https://www.kdnuggets.com/2018/10/<b>sequence</b>-<b>model</b>ing-<b>neural</b>-<b>networks</b>-part-1.html", "snippet": "For example, if the <b>sequence</b> we care about is a sentence of 5 words, the <b>network</b> would be unrolled into a 5-layer <b>neural</b> <b>network</b>, one layer for each word. NB: Sn , the cell state at time n , can contain information from all of the past timesteps: each cell state is a function of the previous self state which in turn is a function of the previous cell state.", "dateLastCrawled": "2022-02-02T09:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Intuition on <b>Neural</b> <b>Network</b> <b>sequence</b> models | by Harsha Vardhan | Medium", "url": "https://harshav.medium.com/intuition-on-neural-network-sequence-models-7314a61c4081", "isFamilyFriendly": true, "displayUrl": "https://harshav.medium.com/intuition-on-<b>neural</b>-<b>network</b>-<b>sequence</b>-<b>models</b>-7314a61c4081", "snippet": "<b>Neural</b> Machine Translation, the task for which Bahdanau et al., first introduced attention in <b>sequence</b> modeling is the task of learning a <b>neural</b> <b>network</b> <b>model</b> to perform human language translation. A seq2seq <b>model</b> aims to transform an input <b>sequence</b> (source) to an output <b>sequence</b> (target). Here source is a <b>sequence</b> in one human language, target in another desired human language (for eg, English to Spanish).", "dateLastCrawled": "2022-01-18T02:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Neural</b> Models for <b>Sequence</b> Prediction --- Recurrent <b>Neural</b> Networks", "url": "https://www.cse.iitb.ac.in/~sunita/lectures/RNN.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cse.iitb.ac.in/~sunita/lectures/RNN.pdf", "snippet": "RNN: Recurrent <b>Neural</b> <b>Network</b> A <b>model</b> to process variable length 1-D input In CNN, each hidden output is a function of corresponding input and some immediate neighbors. In RNN, each output is a function of a &#39;state&#39; summarizing all previous inputs and current input. State summary computed recursively. RNN allows deeper, longer range interaction among parameters than CNNs for the same cost. RNNs: Basic type Notation: ht to denote state instead of zt Input to RNN is xt, instead of yt. RNN ...", "dateLastCrawled": "2022-01-23T05:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Sequential Data</b> \u2014 and the <b>Neural</b> <b>Network</b> Conundrum! | by Aashish ...", "url": "https://medium.com/analytics-vidhya/sequential-data-and-the-neural-network-conundrum-b2c005f8f865", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/<b>sequential-data</b>-and-the-<b>neural</b>-<b>network</b>-conundrum-b...", "snippet": "A traditional <b>neural</b> <b>network</b> <b>model</b> does make use of this information, however, so we\u2019d have to turn to a different type of a <b>model</b> <b>like</b> a Recurrent <b>Neural</b> <b>Network</b> <b>model</b>. A Recurrent <b>Neural</b> ...", "dateLastCrawled": "2022-02-02T19:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Neural</b> Machine Translation Using <b>Sequence</b> to <b>Sequence</b> <b>Model</b> | by Aditya ...", "url": "https://medium.com/geekculture/neural-machine-translation-using-sequence-to-sequence-model-164a5905bcd7", "isFamilyFriendly": true, "displayUrl": "https://medium.com/geekculture/<b>neural</b>-machine-translation-using-<b>sequence</b>-to-<b>sequence</b>...", "snippet": "In <b>sequence</b> to <b>sequence</b> <b>model</b> main parts are Encoder and Decoder. Encoder:-Encoders can be any <b>network</b> <b>like</b> Recurrent <b>Neural</b> <b>Network</b>, LSTM, GRU or Convolutional <b>neural</b> net but we are using this ...", "dateLastCrawled": "2022-02-03T04:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A Tutorial on Sequential Machine Learning", "url": "https://analyticsindiamag.com/a-tutorial-on-sequential-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/a-tutorial-on-sequential-machine-learning", "snippet": "Deep Recurrent <b>Neural</b> <b>Network</b> for Speech Recognition Deep Recurrent <b>Neural</b> <b>Network</b> for Speech Recognition; Recurrent <b>neural</b> networks are being used to create classical music. Recurrent <b>Neural</b> <b>Network</b> for Predicting Transcription Factor Binding Sites based on DNA <b>Sequence</b> Analysis; In order to efficiently <b>model</b> with this data or to get as much information, it contains a traditional machine algorithm that will not help as much. To deal with such data there are some sequential models available ...", "dateLastCrawled": "2022-02-02T16:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Gentle <b>Introduction to Models for Sequence Prediction with</b> RNNs", "url": "https://machinelearningmastery.com/models-sequence-prediction-recurrent-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>model</b>s-<b>sequence</b>-prediction", "snippet": "<b>Sequence</b> Prediction with Recurrent <b>Neural</b> Networks. Recurrent <b>Neural</b> Networks, <b>like</b> Long Short-Term Memory (LSTM) networks, are designed for <b>sequence</b> prediction problems. In fact, at the time of writing, LSTMs achieve state-of-the-art results in challenging <b>sequence</b> prediction problems <b>like</b> <b>neural</b> machine translation (translating English to ...", "dateLastCrawled": "2022-02-03T04:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Deep Sequential Neural Network</b> \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/1410.0510/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1410.0510", "snippet": "<b>Neural</b> Networks sequentially build high-level features through their successive layers. We propose here a new <b>neural</b> <b>network</b> <b>model</b> where each layer is associated with a set of candidate mappings. When an input is processed, at each layer, one mapping among these candidates is selected according to a sequential decision process. The resulting <b>model</b> is structured according to a DAG <b>like</b> architecture, so that a path from the root to a leaf node defines a <b>sequence</b> of transformations. Instead of ...", "dateLastCrawled": "2022-01-30T14:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "CS224n: Natural Language Processing with Deep Learning Lecture Notes ...", "url": "https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes06-NMT_seq2seq_attention.pdf", "isFamilyFriendly": true, "displayUrl": "https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes06-NMT_seq2seq...", "snippet": "to <b>Sequence</b> Learning with <b>Neural</b> Networks&quot; <b>model</b> made up of two recurrent <b>neural</b> networks: ... for the <b>sequence</b>. To do so, the encoder will use a recurrent <b>neural</b> <b>network</b> cell \u2013 usually an LSTM \u2013 to read the input tokens one at a time. The \ufb01nal hidden state of the cell will then become C. However, because it\u2019s so dif\ufb01cult to compress an arbitrary-length <b>sequence</b> into a single \ufb01xed-size vector (especially for dif\ufb01cult tasks <b>like</b> transla-tion), the encoder will usually consist of ...", "dateLastCrawled": "2022-01-28T14:32:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Sequence to Sequence Models with Attention</b>", "url": "https://www.cse.iitd.ac.in/~mausam/papers/tut19.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cse.iitd.ac.in/~mausam/papers/tut19.pdf", "snippet": "(<b>Neural</b> Language <b>Model</b>) Aside \u2022Training: <b>similar</b> to an RNN Transducer. \u2022Generation: the output of step i is input to step i+1. RNN Language Models. RNN Language <b>Model</b> for generation \u2022Define the probability distribution over the next item in a <b>sequence</b> (and hence the probability of a <b>sequence</b>). RNN Language Models. <b>Sequence</b> 2 <b>Sequence</b> Part I: No attention. Back to Original question Generating sentences is nice, but what if we want to add some additional conditioning contexts? How about ...", "dateLastCrawled": "2022-01-30T17:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Neural</b> Models for <b>Sequence</b> Prediction --- Recurrent <b>Neural</b> Networks", "url": "https://www.cse.iitb.ac.in/~sunita/lectures/RNN.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cse.iitb.ac.in/~sunita/lectures/RNN.pdf", "snippet": "<b>Neural</b> <b>network</b> x y = y 1,y 2,..,y n. Motivation Applicable in diverse domains spanning language, image, and speech processing. Before deep learning each community solved the task in their own silos \u2192 lot of domain expertise The promise of deep learning: as long as you have lots of labeled data, domain-specific representations learnable This has brought together these communities like never before! Translation Context: x Predicted <b>sequence</b>: y Pre-DL translation systems were driven by ...", "dateLastCrawled": "2022-01-23T05:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Sequence to Sequence Learning</b> with <b>Neural</b> Networks", "url": "http://cs224d.stanford.edu/papers/seq2seq.pdf", "isFamilyFriendly": true, "displayUrl": "cs224d.stanford.edu/papers/seq2seq.pdf", "snippet": "There have been a number of related attempts to address the general <b>sequence to sequence learning</b> problem with <b>neural</b> networks. Our approach is closely related to Kalchbrenner and Blunsom [18] who were the \ufb01rst to map the entire input sentence to vector, a nd is related to Cho et al. [5] although the latter was used only for rescoring hypotheses produced by a phrase-based system. Graves [10] introduced a novel differentiable attention mechanism that allows <b>neural</b> networks to focus on dif ...", "dateLastCrawled": "2022-02-03T06:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A deep recurrent <b>neural</b> <b>network</b> approach to learn <b>sequence</b> similarities ...", "url": "https://www.sciencedirect.com/science/article/pii/S0167923621002281", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0167923621002281", "snippet": "We proposed a novel deep recurrent <b>neural</b> <b>network</b> <b>model</b> with a triplet loss cost function for \u201clearning\u201d and generalizing similarity structures in sequential data. The specific <b>model</b> architecture of the TL-RNN approach allows us to fully benefit from its automatic feature engineering capability for extracting those individual <b>sequence</b>-level characteristics that define a similarity space for a given domain of sequential data streams. This property turns our approach into a unique data ...", "dateLastCrawled": "2022-01-11T00:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Sequence Modeling with Neural Networks \u2013 Part</b> I - KDnuggets", "url": "https://www.kdnuggets.com/2018/10/sequence-modeling-neural-networks-part-1.html", "isFamilyFriendly": true, "displayUrl": "https://www.kdnuggets.com/2018/10/<b>sequence</b>-<b>model</b>ing-<b>neural</b>-<b>networks</b>-part-1.html", "snippet": "Many operations needed to train the <b>model</b> (<b>network</b>) can be expressed through algebraic operations on the matrix of input feature values and the matrix of weights (think about a n-by-p design matrix, where n is the number of samples observed, and p is the number of variables measured in all samples). Perhaps the most common fixed-length vector representation for texts is the bag-of-words or bag-of-n-grams due to its simplicity, efficiency and often surprising accuracy. However, the bag-of ...", "dateLastCrawled": "2022-02-02T09:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Sequence-to-Sequence</b> Models: Encoder-Decoder using Tensorflow 2 | by ...", "url": "https://towardsdatascience.com/sequence-to-sequence-models-from-rnn-to-transformers-e24097069639", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>sequence-to-sequence</b>-<b>models</b>-from-rnn-to-transformers-e...", "snippet": "<b>Sequence-to-sequence</b> models are fundamental Deep Learning techniques that operate on <b>sequence</b> data. It converts <b>sequence</b> from one domain to <b>sequence</b> in another domain [1]. These models can be RNN-based simple encoder-decoder <b>network</b> or the advanced attention-based encoder-decoder RNN or the state-of-the-art transformer models. There are many applications of <b>sequence-to-sequence</b> models such as \u2014 machine translation, speech recognition, text summarization, question answering, demand ...", "dateLastCrawled": "2022-02-03T04:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Neural</b> Machine Translation Using <b>Sequence</b> to <b>Sequence</b> <b>Model</b> | by Aditya ...", "url": "https://medium.com/geekculture/neural-machine-translation-using-sequence-to-sequence-model-164a5905bcd7", "isFamilyFriendly": true, "displayUrl": "https://medium.com/geekculture/<b>neural</b>-machine-translation-using-<b>sequence</b>-to-<b>sequence</b>...", "snippet": "In <b>sequence</b> to <b>sequence</b> <b>model</b> main parts are Encoder and Decoder. Encoder:-Encoders can be any <b>network</b> like Recurrent <b>Neural</b> <b>Network</b>, LSTM, GRU or Convolutional <b>neural</b> net but we are using this ...", "dateLastCrawled": "2022-02-03T04:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "ASRNN: A recurrent <b>neural</b> <b>network</b> with an attention <b>model</b> for <b>sequence</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0950705120306778", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0950705120306778", "snippet": "The weak semi-CRF <b>model</b> yields performance <b>similar</b> to conventional semi-CRFs but runs significantly faster. 2.4. Deep learning-based <b>model</b>. Deep learning methods also have advantages in the task of <b>sequence labeling</b>. Some theoretical analysis methods are commonly used in this area , , , , . Desmar surveyed the current works for the statistical analytics and then theoretically and empirically examined several suitable tests . Garcia and Herrera focused on the statistical procedures for ...", "dateLastCrawled": "2021-12-30T07:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Recurrent <b>Neural</b> <b>Network</b>", "url": "https://www.cs.toronto.edu/~tingwuwang/rnn_tutorial.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.toronto.edu/~tingwuwang/rnn_tutorial.pdf", "snippet": "1. More than Language <b>Model</b> 1. RNN in sports 1. Sport is a <b>sequence</b> of event (<b>sequence</b> of images, voices) 2. Detecting events and key actors in multi-person videos [12] 1. &quot;In particular, we track people in videos and use a recurrent <b>neural</b> <b>network</b> (RNN) to represent the track features. We learn time-varying attention weights to combine these", "dateLastCrawled": "2022-02-02T17:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How Transformers Work. Transformers are a type of <b>neural</b>\u2026 | by Giuliano ...", "url": "https://towardsdatascience.com/transformers-141e32e69591", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>transformer</b>s-141e32e69591", "snippet": "The following picture shows how usually a <b>sequence</b> to <b>sequence</b> <b>model</b> works using Recurrent <b>Neural</b> Networks. Each word is processed separately, and the resulting sentence is generated by passing a hidden state to the decoding stage that, then, generates the output. GIF from 3 The problem of long-term dependencies. Consider a language <b>model</b> that is trying to predict the next word based on the previous ones. If we are trying to predict the next word of the sentence \u201cthe clouds in the sky ...", "dateLastCrawled": "2022-02-02T11:28:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Gentle <b>Introduction to Models for Sequence Prediction with</b> RNNs", "url": "https://machinelearningmastery.com/models-sequence-prediction-recurrent-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>model</b>s-<b>sequence</b>-prediction", "snippet": "The learned mapping function is static and may <b>be thought</b> of as a program that takes input variables and uses internal variables. Internal variables are represented by an internal state maintained by the <b>network</b> and built up or accumulated over each value in the input <b>sequence</b>. \u2026 RNNs combine the input vector with their state vector with a fixed (but learned) function to produce a new state vector. This <b>can</b> in programming terms be interpreted as running a fixed program with certain inputs ...", "dateLastCrawled": "2022-02-03T04:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Sequence</b> Models by Andrew Ng \u2014 11 Lessons Learned | by Ryan Shrott ...", "url": "https://towardsdatascience.com/sequence-models-by-andrew-ng-11-lessons-learned-c62fb1d3485b", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>sequence</b>-<b>models</b>-by-andrew-ng-11-lessons-learned-c62fb1d...", "snippet": "Ng does an excellent job describing the various modelling complexities involved in creating your own recurrent <b>neural</b> <b>network</b>. My favourite aspect of the course was the programming exercises. In particular, the final programming exercise has you implement a trigger word detection system. These systems <b>can</b> be used to predict when a person says \u201cAlexa\u201d or predict the timing of financial trigger events. <b>Sequence</b> models, in s upervised learning, <b>can</b> be used to address a variety of ...", "dateLastCrawled": "2022-01-29T09:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Sequence</b> to <b>Sequence</b> Learning with <b>Neural</b> Networks", "url": "https://proceedings.neurips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf", "snippet": "<b>Sequence</b> to <b>Sequence</b> Learning with <b>Neural</b> Networks Ilya Sutskever Google ilyasu@google.com Oriol Vinyals Google vinyals@google.com Quoc V. Le Google qvl@google.com Abstract Deep <b>Neural</b> Networks (DNNs) are powerful models that have achieved excel-lent performance on dif\ufb01cult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to <b>sequence</b> learning ...", "dateLastCrawled": "2022-01-30T10:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "How Transformers Work. Transformers are a type of <b>neural</b>\u2026 | by Giuliano ...", "url": "https://towardsdatascience.com/transformers-141e32e69591", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>transformer</b>s-141e32e69591", "snippet": "A Recurrent <b>Neural</b> <b>Network</b> <b>can</b> <b>be thought</b> of as multiple copies of the same <b>network</b>, A, each <b>network</b> passing a message to a successor. Consider what happens if we unroll the loop: An unrolled recurrent <b>neural</b> <b>network</b>. This chain-like nature shows that recurrent <b>neural</b> networks are clearly related to sequences and lists. In that way, if we want to translate some text, we <b>can</b> set each input as the word in that text. The Recurrent <b>Neural</b> <b>Network</b> passes the information of the previous words to ...", "dateLastCrawled": "2022-02-02T11:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>A Neural Conversational Model</b> - Stanford University", "url": "http://cs224d.stanford.edu/papers/ancm.pdf", "isFamilyFriendly": true, "displayUrl": "cs224d.stanford.edu/papers/ancm.pdf", "snippet": "The <b>model</b> is based on a recurrent <b>neural</b> <b>network</b> which reads the input <b>sequence</b> one token at a time, and predicts the output <b>sequence</b>, also one token at a time. During train- ing, thetrueoutputsequenceis givento themodel, solearn-ing <b>can</b> be done by backpropagation. The <b>model</b> is trained to maximizethecross entropyof the correctsequencegiven its context. During inference, given that the true output se-quenceis notobserved,we simplyfeedthe predictedoutput token as input to predict the next ...", "dateLastCrawled": "2022-01-24T23:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Neural Module Networks</b> - CVF Open Access", "url": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Andreas_Neural_Module_Networks_CVPR_2016_paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Andreas_<b>Neural_Module_Networks</b>...", "snippet": "<b>Neural</b> <b>network</b> architectures The idea of selecting a different <b>network</b> graph for each input datum is fundamen- tal to both recurrent networks (where the <b>network</b> grows in the length of the input) [8] and recursive <b>neural</b> networks (where the <b>network</b> is built, e.g., according to the syntactic structure of the input) [36]. But both of these approaches ultimately involve repeated application of a single com-putational module (e.g. an LSTM [13] or GRU [5] cell). From another direction, some kinds ...", "dateLastCrawled": "2022-01-30T15:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "How to implement <b>CNN</b> for NLP tasks like Sentence Classification | by ...", "url": "https://medium.com/saarthi-ai/sentence-classification-using-convolutional-neural-networks-ddad72c7048c", "isFamilyFriendly": true, "displayUrl": "https://medium.com/saarthi-ai/sentence-classification-using-convolutional-<b>neural</b>...", "snippet": "A convolutional layer <b>can</b> <b>be thought</b> of as composed of a series ... we <b>can</b> use pad_<b>sequence</b>() which simply pads the <b>sequence</b> of words with zeros. Additionally, we <b>can</b> add a maxlen parameter to ...", "dateLastCrawled": "2022-01-23T13:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Convolutional <b>neural</b> <b>network</b> architectures for predicting DNA\u2013protein ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4908339/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC4908339", "snippet": "The first layer of our <b>network</b> is a convolutional layer, which <b>can</b> <b>be thought</b> of as a motif scanner. For example, DeepBind uses 16 convolutional layers, each scanning the input <b>sequence</b> with step size 1 and window size of 24. The output of each neuron on a convolutional layer is the convolution of the kernel matrix and the part of the input within the neuron\u2019s window size.", "dateLastCrawled": "2021-12-22T17:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>machine learning</b> - <b>Can</b> a <b>neural</b> <b>network</b> be used to predict the next ...", "url": "https://ai.stackexchange.com/questions/3850/can-a-neural-network-be-used-to-predict-the-next-pseudo-random-number", "isFamilyFriendly": true, "displayUrl": "https://ai.stackexchange.com/questions/3850", "snippet": "Old question, but I <b>thought</b> it&#39;s worth one practical answer. I ... ( if you plot the occurrences of the random numbers and of the <b>neural</b> <b>network</b> predictions, you <b>can</b> see that that the two have the same trend, even if in the predicytions curve there are many spikes. So maybe the <b>neural</b> <b>network</b> is able to learn about pseudo-random number distributions ? If I reduce the size of the training set under a certain limit, I see that the classifier starts to predict always the same few numbers, which ...", "dateLastCrawled": "2022-02-02T23:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Neural</b> <b>Network</b>: How <b>can RNN learn logical rules in time sequence</b>?", "url": "https://www.researchgate.net/post/Neural-Network-How-can-RNN-learn-logical-rules-in-time-sequence", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/post/<b>Neural</b>-<b>Network</b>-How-<b>can-RNN-learn-logical-rules</b>-in...", "snippet": "<b>Neural</b> <b>Network</b>: How <b>can RNN learn logical rules in time sequence</b>? when X (t) == X (t-2), output should be 1, otherwise output should be 0. I build a LSTM with Keras try to learn this rule, with 16 ...", "dateLastCrawled": "2022-01-19T03:41:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Sequence</b> Models &amp; Recurrent <b>Neural</b> Networks (RNNs) | by Santhoopa ...", "url": "https://towardsdatascience.com/sequence-models-and-recurrent-neural-networks-rnns-62cadeb4f1e1", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>sequence</b>-<b>models</b>-and-recurrent-<b>neural</b>-<b>networks</b>-rnns-62...", "snippet": "Recurrent <b>neural</b> <b>network</b> (RNN) is a popular <b>sequence</b> <b>model</b> that has shown efficient performance for sequential data. Recurrent <b>Neural</b> Networks (RNNs) Recurrent <b>Neural</b> <b>Network</b> (RNN) is a Deep learning algorithm and it is a type of Artificial <b>Neural</b> <b>Network</b> architecture that is specialized for processing sequential data. RNNs are mostly used in the field of Natural Language Processing (NLP). RNN maintains internal memory, due to this they are very efficient for machine learning problems that ...", "dateLastCrawled": "2022-02-02T10:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "ASRNN: A recurrent <b>neural</b> <b>network</b> with an attention <b>model</b> for <b>sequence</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0950705120306778", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0950705120306778", "snippet": "This <b>model</b> shows higher performance <b>compared</b> to models that do not explicitly represent segments. Liu et al. ... This permits tokens to be sensitive to the contexts where they occur and is typical of <b>neural</b> <b>network</b> <b>sequence</b> prediction models . After computing the representations of each token (i.e., character) using the bottom Bi-LSTM, the top Bi-LSTM is used to compute representations for each span (i.e., word). Rather than the approach proposed by Kong et al. in 2016, which directly ...", "dateLastCrawled": "2021-12-30T07:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>DeepLearning</b> series: <b>Sequence</b> Models | by Michele Cavaioni | Machine ...", "url": "https://medium.com/machine-learning-bites/deeplearning-series-sequence-models-7855babeb586", "isFamilyFriendly": true, "displayUrl": "https://medium.com/machine-learning-bites/<b>deeplearning</b>-series-<b>sequence</b>-<b>models</b>-7855babeb586", "snippet": "Language <b>model</b> and <b>sequence</b> generation: ... It\u2019s like what we have seen in a deep <b>neural</b> <b>network</b>, where the <b>network</b> has a difficult time propagating back to affect the weights of earlier layers ...", "dateLastCrawled": "2022-01-30T19:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "ARG-SHINE: improve antibiotic resistance class prediction by ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8341004/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8341004", "snippet": "They then developed a transfer learning based deep <b>neural</b> <b>network</b> <b>model</b>, TRAC ... ARG-CNN and ARG-InterPro <b>can</b> achieve better performance <b>compared</b> with other available methods when the query <b>sequence</b> is not highly similar to the known ARG sequences. Our results show that ARG-KNN and BLAST best hit achieve better performance <b>compared</b> with DIAMOND best hit and DeepARG for sequences with high similarity with known ARG sequences. <b>Compared</b> with other methods, ARG-SHINE achieves the best ...", "dateLastCrawled": "2021-12-30T11:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Seq2Seq model using Convolutional Neural Network</b> | by Gautam Karmakar ...", "url": "https://medium.com/@gautam.karmakar/summary-seq2seq-model-using-convolutional-neural-network-b1eb100fb4c4", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@gautam.karmakar/summary-<b>seq2seq-model-using-convolutional-neural</b>...", "snippet": "Gautam Karmakar. Aug 24, 2018 \u00b7 8 min read. <b>Seq2seq model using Convolutional Neural Network</b>. Introduction: Seq2seq <b>model</b> maps variable input <b>sequence</b> to variable length output <b>sequence</b> using ...", "dateLastCrawled": "2021-11-24T13:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Sequence</b> Prediction Using <b>Neural</b> <b>Network</b> Classi ers", "url": "http://proceedings.mlr.press/v57/zhao16.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v57/zhao16.pdf", "snippet": "<b>Sequence</b> Prediction Using <b>Neural</b> <b>Network</b> Classi ers Yanpeng Zhao zhaoyp1@shanghaitech.edu.cn Shanbo Chu chushb@shanghaitech.edu.cn Yang Zhou zhouyang1@shanghaitech.edu.cn Kewei Tu tukw@shanghaitech.edu.cn School of Information Science and Technology, ShanghaiTech University, Shanghai, China Abstract Being able to guess the next element of a <b>sequence</b> is an important question in many elds. In this paper we present our approaches used in the <b>Sequence</b> Prediction ChallengE (SPiCe), whose goal is ...", "dateLastCrawled": "2022-01-29T01:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Temporal-Sequential Learning With a Brain-Inspired Spiking <b>Neural</b> ...", "url": "https://pubmed.ncbi.nlm.nih.gov/32714173/", "isFamilyFriendly": true, "displayUrl": "https://<b>pubmed</b>.ncbi.nlm.nih.gov/32714173", "snippet": "To overcome this deficiency, this paper introduces a spiking <b>neural</b> <b>network</b> based on psychological and neurobiological findings at multiple scales. <b>Compared</b> with existing methods, our <b>model</b> has four novel features: (1) It contains several collaborative subnetworks similar to those in brain regions with different cognitive functions. The individual building blocks of the simulated areas are <b>neural</b> functional minicolumns composed of biologically plausible neurons. Both excitatory and ...", "dateLastCrawled": "2021-12-28T11:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Transformer <b>Neural</b> <b>Network</b> In Deep Learning - Overview - GeeksforGeeks", "url": "https://www.geeksforgeeks.org/transformer-neural-network-in-deep-learning-overview/", "isFamilyFriendly": true, "displayUrl": "https://www.geeksforgeeks.org/transformer-<b>neural</b>-<b>network</b>-in-deep-learning-overview", "snippet": "Like LSTMs Transformers is an architecture for transforming one <b>sequence</b> into an antidote while helping other two parts that is encoders and decoders, but it differs from the previously described <b>sequence</b> your <b>sequence</b> <b>model</b>, because it does not work like GRUs. So it does not implement recurring <b>neural</b> networks. Recurrent <b>neural</b> <b>network</b> until now was one of the best ways to capture the tiny dependence on a <b>sequence</b>. However, the team presenting this paper that is \u2018Attention Is All You Need ...", "dateLastCrawled": "2022-02-02T11:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A <b>neural</b> <b>network</b> <b>based model effectively predicts enhancers</b> from ...", "url": "https://www.nature.com/articles/s41598-018-34420-9", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41598-018-34420-9", "snippet": "We studied the impact of integrating diverse data features for enhancer predictions by building and evaluating <b>neural</b> <b>network</b> models using (i) DNA <b>sequence</b> features, (ii) TF motif occurrence ...", "dateLastCrawled": "2022-01-06T10:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What is <b>Transformer</b> <b>Network</b> | Towards Data Science", "url": "https://towardsdatascience.com/transformer-neural-network-step-by-step-breakdown-of-the-beast-b3e096dc857f", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>transformer</b>-<b>neural</b>-<b>network</b>-step-by-step-breakdown-of...", "snippet": "The <b>Transformer</b> <b>Neural</b> <b>Network</b> is a novel architecture that aims to solve <b>sequence</b>-to-<b>sequence</b> tasks while handling long-range dependencies with ease. It was proposed in the paper \u201cAttention Is All You Need\u201d 2017 [1]. It is the current state-of-the-art technique in the field of NLP. Before directly jumping to <b>Transformer</b>, I will take some time to explain the reason why we use it and from where it comes into the picture. (If you want to skip this part then directly go to the <b>Transformer</b> ...", "dateLastCrawled": "2022-01-30T02:05:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine</b> <b>Learning</b>: Generative and Discriminative Models", "url": "https://cedar.buffalo.edu/~srihari/CSE574/Discriminative-Generative.pdf", "isFamilyFriendly": true, "displayUrl": "https://cedar.buffalo.edu/~srihari/CSE574/Discriminative-Generative.pdf", "snippet": "Models: An <b>analogy</b> \u2022 The task is to determine the language that someone is speaking \u2022 Generative approach: ... Hidden Markov <b>Model</b>. <b>SEQUENCE</b>. Conditional Random Field. CONDITION. G E N E R A T I V E. D I S C R I M I N A T I V E. y. x. x. 1. x. M. X. x. 1. x. N. Y. y. 1. y. N. p(y, x) p(y/ x) p(Y, X) p(Y / X) CONDITION. <b>SEQUENCE</b>. <b>Machine</b> <b>Learning</b> Srihari 18. Generative Classifier: Bayes \u2022 Given variables x =(x. 1 ,..,x. M ) and class variable . y \u2022 Joint pdf is . p(x,y) \u2013 Called ...", "dateLastCrawled": "2022-02-03T06:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Deep Learning: Models for Sequence Data</b> (RNN and LSTM)", "url": "https://cse.iitk.ac.in/users/piyush/courses/ml_autumn16/771A_lec24_slides.pdf", "isFamilyFriendly": true, "displayUrl": "https://cse.iitk.ac.in/users/piyush/courses/ml_autumn16/771A_lec24_slides.pdf", "snippet": "Filters are like basis/dictionary (PCA <b>analogy</b>) Each lter is convolved over entire input to produce a feature map Nonlinearity and pooling and applied after each convolution layer Last layer (one that connects to outputs) is fully connected <b>Machine</b> <b>Learning</b> (CS771A) <b>Deep Learning: Models for Sequence Data</b> (RNN and LSTM) 3. Recap: Convolutional Neural Network Special type of feedforward neural nets (local connectivity + weight sharing) Each layer uses a set of \\ lters&quot; (basically, weights to ...", "dateLastCrawled": "2022-01-17T20:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "How does <b>machine learning</b> work? Like a brain! | by David Rajnoch ...", "url": "https://towardsdatascience.com/how-does-machine-learning-work-a3bf1e102b11", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/how-does-<b>machine-learning</b>-work-a3bf1e102b11", "snippet": "Human <b>analogy</b> to describe <b>machine learning</b> in image classification. David Rajnoch . Jul 23, 2017 \u00b7 4 min read. I could point to dozens of articles about <b>machine learning</b> and convolutional neural networks. Every article describes different details. Sometimes too many details are mentioned and so I decided to write my own post using the parallel of <b>machine learning</b> and the human brain. I will not touch any mathematics or deep <b>learning</b> details. The goal is to stay simple and help people ...", "dateLastCrawled": "2022-01-29T20:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "time series - <b>Machine</b> <b>learning</b> models that combine sequences and static ...", "url": "https://stats.stackexchange.com/questions/288655/machine-learning-models-that-combine-sequences-and-static-features", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/288655/<b>machine</b>-<b>learning</b>-<b>models</b>-that-combine...", "snippet": "1 Answer1. Show activity on this post. Just a suggestion, if your classifying a <b>sequence</b> with an RNN you could add a final fully-connected layer that combines the output of the RNN with your static features (by concatenation) before going to the softmax and outputting the predicted class probabilities. Since this final layer is fully-connect ...", "dateLastCrawled": "2022-01-21T16:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "9.7. <b>Sequence</b> to <b>Sequence</b> <b>Learning</b> \u2014 Dive into Deep <b>Learning</b> 0.17.2 ...", "url": "https://d2l.ai/chapter_recurrent-modern/seq2seq.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_recurrent-modern/seq2seq.html", "snippet": "As we have seen in Section 9.5, in <b>machine</b> translation both the input and output are a variable-length <b>sequence</b>.To address this type of problem, we have designed a general encoder-decoder architecture in Section 9.6.In this section, we will use two RNNs to design the encoder and the decoder of this architecture and apply it to <b>sequence</b> to <b>sequence</b> <b>learning</b> for <b>machine</b> translation [Sutskever et al., 2014] [Cho et al., 2014b].. Following the design principle of the encoder-decoder architecture ...", "dateLastCrawled": "2022-01-26T00:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>DNA Sequencing Classifier using Machine Learning</b> :: InBlog", "url": "https://inblog.in/DNA-Sequencing-Classifier-using-Machine-Learning-98md9C4V7k", "isFamilyFriendly": true, "displayUrl": "https://inblog.in/<b>DNA-Sequencing-Classifier-using-Machine-Learning</b>-98md9C4V7k", "snippet": "DNA Sequencing With <b>Machine</b> <b>Learning</b>. In this notebook, I will apply a classification <b>model</b> that can predict a gene&#39;s function based on the DNA <b>sequence</b> of the coding <b>sequence</b> alone. In [ 1 ]: import numpy as np import pandas as pd import matplotlib.pyplot as plt %matplotlib inline. In [ 2 ]:", "dateLastCrawled": "2022-01-22T16:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Sequence</b> Classification with LSTM Recurrent Neural Networks in Python ...", "url": "https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>sequence</b>-classification-", "snippet": "The problem that we will use to demonstrate <b>sequence</b> <b>learning</b> in this tutorial is the IMDB movie review sentiment classification problem. Each movie review is a variable <b>sequence</b> of words and the sentiment of each movie review must be classified. The Large Movie Review Dataset (often referred to as the IMDB dataset) contains 25,000 highly-polar movie reviews (good or bad) for training and the same amount again for testing. The problem is to determine whether a given movie review has a ...", "dateLastCrawled": "2022-02-02T22:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Coursera: Neural Networks and Deep Learning</b> (Week 1) Quiz [MCQ Answers ...", "url": "https://www.apdaga.com/2019/03/coursera-neural-networks-and-deep-learning-week-1-quiz.html", "isFamilyFriendly": true, "displayUrl": "https://www.apdaga.com/2019/03/<b>coursera-neural-networks-and-deep-learning</b>-week-1-quiz.html", "snippet": "Recommended <b>Machine</b> <b>Learning</b> Courses: ... What does the <b>analogy</b> \u201cAI is the new electricity\u201d refer to? AI runs on computers and is thus powered by electricity, but it is letting computers do things not possible before. Similar to electricity starting about 100 years ago, AI is transforming multiple industries. Correct. Yes. AI is transforming many fields from the car industry to agriculture to supply-chain... Through the \u201csmart grid\u201d, AI is delivering a new wave of electricity. AI is ...", "dateLastCrawled": "2022-01-30T01:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "What is Instance-Based and <b>Model</b>-Based <b>Learning</b>? | by Sanidhya Agrawal ...", "url": "https://medium.com/@sanidhyaagrawal08/what-is-instance-based-and-model-based-learning-s1e10-8e68364ae084", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@sanidhyaagrawal08/what-is-instance-based-and-<b>model</b>-based-<b>learning</b>...", "snippet": "1. Instance-based <b>learning</b>: (s o metimes called memory-based <b>learning</b>) is a family of <b>learning</b> algorithms that, instead of performing explicit generalization, compares new problem instances with ...", "dateLastCrawled": "2022-01-29T00:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "A Gentle Introduction to <b>Long Short-Term Memory</b> Networks by the Experts", "url": "https://machinelearningmastery.com/gentle-introduction-long-short-term-memory-networks-experts/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/gentle-introduction-<b>long-short-term-memory</b>-networks...", "snippet": "<b>Long Short-Term Memory</b> (LSTM) networks are a type of recurrent neural network capable of <b>learning</b> order dependence in <b>sequence</b> prediction problems. This is a behavior required in complex problem domains like <b>machine</b> translation, speech recognition, and more. LSTMs are a complex area of deep <b>learning</b>. It can be hard to get your hands around what LSTMs are, and how terms like bidirectional", "dateLastCrawled": "2022-01-31T03:31:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>LEARNING</b> TO REPRESENT EDITS", "url": "https://openreview.net/pdf?id=BJl6AjC5F7", "isFamilyFriendly": true, "displayUrl": "https://openreview.net/pdf?id=BJl6AjC5F7", "snippet": "We introduce the problem of <b>learning</b> distributed representations of edits. By com-bining a \u201cneural editor\u201d with an \u201cedit encoder\u201d, our models learn to represent the salient information of an edit and can be used to apply edits to new inputs. We experiment on natural language and source code edit data. Our evaluation yields promising results that suggest that our neural network models learn to capture the structure and semantics of edits. We hope that this interesting task and data ...", "dateLastCrawled": "2022-01-14T03:18:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Self-<b>Directed Learning and Its Relation</b> to the VC-Dimension and to ...", "url": "https://www.researchgate.net/publication/220343451_Self-Directed_Learning_and_Its_Relation_to_the_VC-Dimension_and_to_Teacher-Directed_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/220343451_Self-<b>Directed_Learning</b>_and_Its...", "snippet": "<b>Machine</b> <b>Learning</b> KL641-04-ben-david September 8, 1998 16:48 100 S. BEN-DAVID AND N. EIRON the \u201cwrong\u201d value to it, or Algorithm 1 would have tried z before).", "dateLastCrawled": "2021-08-08T13:16:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(sequence model)  is like +(neural network)", "+(sequence model) is similar to +(neural network)", "+(sequence model) can be thought of as +(neural network)", "+(sequence model) can be compared to +(neural network)", "machine learning +(sequence model AND analogy)", "machine learning +(\"sequence model is like\")", "machine learning +(\"sequence model is similar\")", "machine learning +(\"just as sequence model\")", "machine learning +(\"sequence model can be thought of as\")", "machine learning +(\"sequence model can be compared to\")"]}