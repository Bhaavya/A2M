{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Concept Summary: Clustering</b> Algorithms \u2014 Dataiku Knowledge Base", "url": "https://knowledge.dataiku.com/latest/courses/intro-to-ml/clustering/clustering-summary.html", "isFamilyFriendly": true, "displayUrl": "https://knowledge.dataiku.com/latest/courses/intro-to-ml/clustering/clustering-summary...", "snippet": "<b>Concept Summary: Clustering</b> Algorithms\u00b6. This lesson contains the same information as the video, Concept: Clustering Algorithms in the course, Intro to Machine Learning. In prediction, or supervised learning, we are attempting to predict an outcome, such as whether or not a <b>student</b> will succeed on an <b>exam</b>, or the <b>student</b>\u2019s <b>exam</b> score.", "dateLastCrawled": "2022-02-02T14:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Concept Summary: Regression</b> Algorithms \u2014 Dataiku Knowledge Base", "url": "https://knowledge.dataiku.com/latest/courses/intro-to-ml/regression/regression-summary.html", "isFamilyFriendly": true, "displayUrl": "https://knowledge.dataiku.com/latest/courses/intro-to-ml/regression/regression-summary...", "snippet": "In our example, students with zero study hours, will still achieve a minimum score of 35, taking into account credit for things <b>like</b> attendance and mid-year assignments. In our example, the difference between four and seven hours of <b>studying</b> increased the <b>exam</b> score by five. The equation of the <b>exam</b> score is equal to b0 + b1* (Number of Study ...", "dateLastCrawled": "2022-02-03T01:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Solution of Final Exam : 10</b>-<b>701/15-781 Machine Learning</b>", "url": "https://www.cs.cmu.edu/~epxing/Class/10701/exams/final2004-solution.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.cmu.edu/~epxing/Class/10701/<b>exams</b>/final2004-solution.pdf", "snippet": "<b>Solution of Final Exam : 10</b>-<b>701/15-781 Machine Learning</b> Fall 2004 Dec. 12th 2004 Your Andrew ID in capital letters: Your full name: There are 9 questions. Some of them are easy and some are more di cult. So, if you get stuck on any one of the questions, proceed with the rest of the questions and return back at the end if you have time remaining. The maximum score of the <b>exam</b> is 100 points If you need more room to work out your answer to a question, use the back of the page and clearly mark ...", "dateLastCrawled": "2022-02-03T05:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Stanford Reinforcement Learning - XpCourse", "url": "https://www.xpcourse.com/stanford-reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://www.xpcourse.com/stanford-reinforcement-learning", "snippet": "### <b>Tabular</b> Temporal Difference Learning Both SARSA and <b>Q-Learning</b> are included. The agent still maintains <b>tabular</b> value functions but does not require an environment model and learns from \u2026 496 People Learned More Courses \u203a\u203a View Course Home | Learning for a Lifetime | Stanford Online Hot online.stanford.edu. Stanford Online offers learning opportunities via free online courses, online degrees, grad and professional certificates, e-learning, and open courses. 187 People Learned More ...", "dateLastCrawled": "2022-01-07T02:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Reinforcement Learning: <b>Bellman</b> Equation and Optimality (Part 2) | by ...", "url": "https://towardsdatascience.com/reinforcement-learning-markov-decision-process-part-2-96837c936ec3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/reinforcement-learning-markov-decision-process-part-2...", "snippet": "<b>Bellman</b> Equation for Value Function (State-Value Function) From the above equation, we can see that the value of a s tate can be decomposed into immediate reward(R[t+1]) plus the value of successor state(v[S (t+1)]) with a discount factor(\ud835\udefe).This still stands for <b>Bellman</b> Expectation Equation. But now what we are doing is we are finding the value of a particular state subjected to some policy(\u03c0).This is the difference between the <b>Bellman</b> Equation and the <b>Bellman</b> Expectation Equation.", "dateLastCrawled": "2022-02-02T12:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Omscs Class Review - XpCourse</b>", "url": "https://www.xpcourse.com/omscs-class-review", "isFamilyFriendly": true, "displayUrl": "https://www.xpcourse.com/<b>omscs-class-review</b>", "snippet": "The class teaches the fundamentals of reinforcement learning, starting with model-based methods such as value iteration and policy iteration. It also covered model-free methods such as <b>Q-learning</b> (i.e., the classic, <b>tabular</b> approach) and deep <b>Q-learning</b> (personally, I applied a Double DQN in the second project).", "dateLastCrawled": "2021-12-25T23:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "20 Best + Free <b>NYU Classes and Courses</b> - Take This Course", "url": "https://www.takethiscourse.net/nyu-classes/", "isFamilyFriendly": true, "displayUrl": "https://www.takethiscourse.net/nyu-classes", "snippet": "The NYU online classes &amp; courses have a lot to offer that helps a <b>student</b> to experience the best learning environment ever. Computer Science NYU Classes &amp; Courses: Basics of Computing and Programming. NYU via edX; 9 Weeks Duration; 6-8 Hours Weekly Study; This course is part of the Computer Science Fundamentals MicroBachelors Program; To know all about programming, this Nyu courses is what you need to enroll in. The course is not only interesting but also a self-paced one that will give you ...", "dateLastCrawled": "2022-01-26T05:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-Of ...", "url": "https://news.ycombinator.com/item?id=13518039", "isFamilyFriendly": true, "displayUrl": "https://news.ycombinator.com/item?id=13518039", "snippet": "After AlphaGo, I tasked myself with creating a neural network that would use <b>Q-Learning</b> to play Reversi (aka Othello). At that point, I had already utilized <b>Q-Learning</b> (the <b>tabular</b> version, not using a neural network) for some very simple and mostly proof-of-concept projects, so I understood how it worked. I read up only perceptrons, relu, the ...", "dateLastCrawled": "2021-07-30T11:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>GitHub</b> - <b>sourabh-joshi</b>/awesome-quincy-larson-emails: This repository is ...", "url": "https://github.com/sourabh-joshi/awesome-quincy-larson-emails", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/<b>sourabh-joshi</b>/awesome-quincy-larson-emails", "snippet": "Awesome Quincy Larson Email Archive. This repository is an archive of emails that are sent by the awesome Quincy Larson every week. If you find these learning resources to be worth your time, consider supporting the nonprofit with a tax-deductible donation: https://donate.freecodecamp.org January 28, 2022", "dateLastCrawled": "2022-02-02T23:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How can one create <b>their own reinforcement learning environment in</b> ...", "url": "https://www.quora.com/How-can-one-create-their-own-reinforcement-learning-environment-in-Python", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/How-can-one-create-<b>their-own-reinforcement-learning-environment</b>...", "snippet": "Answer (1 of 3): OpenAI Gym is your starting point. Reinforcement algorithms implementation libraries <b>like</b> stable-baselines or keras-rl work with OpenAI Gym out of the box. But even for implementing an algorithm by hand it can be used. You must implement different functions from the base gym.Env...", "dateLastCrawled": "2022-01-29T15:31:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Top <b>Machine Learning Interview Questions</b> and Answers for 2022 - Edureka", "url": "https://www.edureka.co/blog/interview-questions/machine-learning-interview-questions/", "isFamilyFriendly": true, "displayUrl": "https://www.edureka.co/blog/interview-questions/<b>machine-learning-interview-questions</b>", "snippet": "<b>Q-Learning</b>, SARSA, etc . Types of Machine Learning \u2013 <b>Machine Learning Interview Questions</b> \u2013 Edureka. There are three ways in which machines learn: Supervised Learning ; Unsupervised Learning; Reinforcement Learning; Supervised Learning: Supervised learning is a method in which the machine learns using labeled data. It is like learning under the guidance of a teacher; Training dataset is like a teacher which is used to train the machine; Model is trained on a pre-defined dataset before it ...", "dateLastCrawled": "2022-02-02T10:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "cp18_Reinforcement Learning for Markov Decision Making in Env_Bellman_Q ...", "url": "https://blog.csdn.net/Linli522362242/article/details/117889535", "isFamilyFriendly": true, "displayUrl": "https://blog.csdn.net/Linli522362242/article/details/117889535", "snippet": "Overall, the main approach is very <b>similar</b> to the <b>tabular</b> <b>Q-learning</b> method. The main difference is that we now have a multilayer NN that computes the action values. Training a DQN model according to the <b>Q-learning</b> algorithm In this section, we describe the procedure for training a DQN model using the <b>Q-learning</b> algorithm.", "dateLastCrawled": "2021-11-05T14:34:00.0000000Z", "language": "zh_chs", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "The Who in Explainable AI: How AI Background Shapes Perceptions of AI ...", "url": "https://deepai.org/publication/the-who-in-explainable-ai-how-ai-background-shapes-perceptions-of-ai-explanations", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/the-who-in-explainable-ai-how-ai-background-shapes...", "snippet": "<b>Tabular</b> <b>Q-learning</b> agents attempt to learn the utility (called a Q-value for \u201cquality\u201d of the action) of different actions in different situations. ... Our generation approach <b>is similar</b> to prior work in XAI and HRI (Das and Chernova, 2020; Ehsan et al., 2018) where they use a neural machine translation (NMT) (Luong et al., 2015) approach to produce satisfactory and plausible rationales to explain sequential behavior. We build on this technique and adapt it to fit our sequential ...", "dateLastCrawled": "2021-12-28T17:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) Strategies for simulating pedestrian navigation with multiple ...", "url": "https://www.researchgate.net/publication/270789039_Strategies_for_simulating_pedestrian_navigation_with_multiple_reinforcement_learning_agents", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/270789039_Strategies_for_simulating...", "snippet": "V ector Quantization for <b>Q-Learning</b> (VQQL) [9, 14] is a learn ing schema that uses VQ as the generalization metho d for the state space and the <b>tabular</b> version of <b>Q-Learning</b> for the learning process.", "dateLastCrawled": "2021-10-31T01:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine Learning</b> - GeeksforGeeks", "url": "https://www.geeksforgeeks.org/machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.geeksforgeeks.org/<b>machine-learning</b>", "snippet": "<b>Machine Learning</b> is the field of study that gives computers the capability to learn without being explicitly programmed. ML is one of the most exciting technologies that one would have ever come across. As it is evident from the name, it gives the computer that makes it more <b>similar</b> to humans: The ability to learn.<b>Machine learning</b> is actively being used today, perhaps in many more places than one would expect.", "dateLastCrawled": "2022-02-02T05:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Seita&#39;s Place", "url": "https://danieltakeshi.github.io/page13/", "isFamilyFriendly": true, "displayUrl": "https://danieltakeshi.github.io/page13", "snippet": "<b>Q-Learning</b> will choose the best action \\(a&#39;\\), but in SARSA, it is fixed, and then we can update \\(Q(s,a)\\). SARSA and <b>Q-Learning</b> (the first version here, not the second version with \\(f\\)) are the same for a greedy agent always picking the best action. When exploration happens, <b>Q-Learning</b> will attempt to pick the best action, but SARSA picks whatever is actually going to happen. This means <b>Q-Learning</b> does not pay attention to the policy, i.e., it is", "dateLastCrawled": "2021-12-28T10:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Reinforcement Learning: <b>Bellman</b> Equation and Optimality (Part 2) | by ...", "url": "https://towardsdatascience.com/reinforcement-learning-markov-decision-process-part-2-96837c936ec3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/reinforcement-learning-markov-decision-process-part-2...", "snippet": "<b>Bellman</b> Equation for Value Function (State-Value Function) From the above equation, we can see that the value of a s tate can be decomposed into immediate reward(R[t+1]) plus the value of successor state(v[S (t+1)]) with a discount factor(\ud835\udefe).This still stands for <b>Bellman</b> Expectation Equation. But now what we are doing is we are finding the value of a particular state subjected to some policy(\u03c0).This is the difference between the <b>Bellman</b> Equation and the <b>Bellman</b> Expectation Equation.", "dateLastCrawled": "2022-02-02T12:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-Of ...", "url": "https://news.ycombinator.com/item?id=13518039", "isFamilyFriendly": true, "displayUrl": "https://news.ycombinator.com/item?id=13518039", "snippet": "After AlphaGo, I tasked myself with creating a neural network that would use <b>Q-Learning</b> to play Reversi (aka Othello). At that point, I had already utilized <b>Q-Learning</b> (the <b>tabular</b> version, not using a neural network) for some very simple and mostly proof-of-concept projects, so I understood how it worked. I read up only perceptrons, relu, the ...", "dateLastCrawled": "2021-07-30T11:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "The <b>Transportation Problem</b>: Features, Types, &amp; Solutions - Video ...", "url": "https://study.com/academy/lesson/the-transportation-problem-features-types-solutions.html", "isFamilyFriendly": true, "displayUrl": "https://study.com/academy/lesson/the-<b>transportation-problem</b>-features-types-solutions.html", "snippet": "The <b>transportation problem</b> is a distribution-type linear programming problem, concerned with transferring goods between various origins and destinations. In case its main goal is to minimize the ...", "dateLastCrawled": "2022-02-02T17:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>GitHub</b> - <b>sourabh-joshi</b>/awesome-quincy-larson-emails: This repository is ...", "url": "https://github.com/sourabh-joshi/awesome-quincy-larson-emails", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/<b>sourabh-joshi</b>/awesome-quincy-larson-emails", "snippet": "Awesome Quincy Larson Email Archive. This repository is an archive of emails that are sent by the awesome Quincy Larson every week. If you find these learning resources to be worth your time, consider supporting the nonprofit with a tax-deductible donation: https://donate.freecodecamp.org January 28, 2022", "dateLastCrawled": "2022-02-02T23:36:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "The Who in Explainable AI: How AI Background Shapes Perceptions of AI ...", "url": "https://deepai.org/publication/the-who-in-explainable-ai-how-ai-background-shapes-perceptions-of-ai-explanations", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/the-who-in-explainable-ai-how-ai-background-shapes...", "snippet": "<b>Tabular</b> <b>Q-learning</b> agents attempt to learn the utility (called a Q-value for \u201cquality\u201d of the action) ... Granted not every AI <b>student</b> will go on to build AI systems, with the proliferation of AI systems in the workplace, a majority of these students could become stakeholders residing on the creation or development end of the technology spectrum\u2014as potential developers, designers, and managers of AI-based systems. As potential creators of AI systems, their perceptions matter in ...", "dateLastCrawled": "2021-12-28T17:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Seita&#39;s Place", "url": "https://danieltakeshi.github.io/page10/", "isFamilyFriendly": true, "displayUrl": "https://danieltakeshi.github.io/page10", "snippet": "Think back to my <b>tabular</b> <b>Q-Learning</b> example in this post. The target was parameterized using \\(Q(s&#39;,a&#39;; \\theta)\\). When I perform an update using SGD, I updated \\(\\theta_{s,a}\\). If this turns out to be the same component as \\(\\theta_{s&#39;,a&#39;}\\), then this will automatically update the target. Think of the successor state as being equal to the current state. And, again, during the gradient update, the target was assumed fixed, which is why I did not re-write \\(Q(s&#39;,a&#39;;\\theta)\\) into a ...", "dateLastCrawled": "2022-01-16T03:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Seita&#39;s Place", "url": "https://danieltakeshi.github.io/page13/", "isFamilyFriendly": true, "displayUrl": "https://danieltakeshi.github.io/page13", "snippet": "<b>Q-Learning</b> will choose the best action \\(a&#39;\\), but in SARSA, it is fixed, and then we <b>can</b> update \\(Q(s,a)\\). SARSA and <b>Q-Learning</b> (the first version here, not the second version with \\(f\\)) are the same for a greedy agent always picking the best action. When exploration happens, <b>Q-Learning</b> will attempt to pick the best action, but SARSA picks whatever is actually going to happen. This means <b>Q-Learning</b> does not pay attention to the policy, i.e., it is", "dateLastCrawled": "2021-12-28T10:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "The <b>Transportation Problem</b>: Features, Types, &amp; Solutions - Video ...", "url": "https://study.com/academy/lesson/the-transportation-problem-features-types-solutions.html", "isFamilyFriendly": true, "displayUrl": "https://study.com/academy/lesson/the-<b>transportation-problem</b>-features-types-solutions.html", "snippet": "The <b>transportation problem</b> is a distribution-type linear programming problem, concerned with transferring goods between various origins and destinations. In case its main goal is to minimize the ...", "dateLastCrawled": "2022-02-02T17:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) Using model-based reflection to guide reinforcement learning ...", "url": "https://www.academia.edu/1021508/Using_model_based_reflection_to_guide_reinforcement_learning", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/1021508", "snippet": "Using model-based reflection to guide reinforcement learning. Reasoning, Representation, and \u2026, 2005. Ashok Goel", "dateLastCrawled": "2021-01-12T00:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) <b>Reinforcement Learning: An Introduction second edition</b> | BB DK ...", "url": "https://www.academia.edu/39631493/Reinforcement_Learning_An_Introduction_second_edition", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/39631493/<b>Reinforcement_Learning_An_Introduction_second_edition</b>", "snippet": "Academia.edu is a platform for academics to share research papers.", "dateLastCrawled": "2022-01-28T21:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "How <b>can</b> one create <b>their own reinforcement learning environment in</b> ...", "url": "https://www.quora.com/How-can-one-create-their-own-reinforcement-learning-environment-in-Python", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/How-<b>can</b>-one-create-<b>their-own-reinforcement-learning-environment</b>...", "snippet": "Answer (1 of 3): OpenAI Gym is your starting point. Reinforcement algorithms implementation libraries like stable-baselines or keras-rl work with OpenAI Gym out of the box. But even for implementing an algorithm by hand it <b>can</b> be used. You must implement different functions from the base gym.Env...", "dateLastCrawled": "2022-01-29T15:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>GitHub</b> - <b>sourabh-joshi</b>/awesome-quincy-larson-emails: This repository is ...", "url": "https://github.com/sourabh-joshi/awesome-quincy-larson-emails", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/<b>sourabh-joshi</b>/awesome-quincy-larson-emails", "snippet": "Awesome Quincy Larson Email Archive. This repository is an archive of emails that are sent by the awesome Quincy Larson every week. If you find these learning resources to be worth your time, consider supporting the nonprofit with a tax-deductible donation: https://donate.freecodecamp.org January 28, 2022", "dateLastCrawled": "2022-02-02T23:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Reinforcement Learning with a Corrupted Reward Channel - researchgate.net", "url": "https://www.researchgate.net/publication/318830044_Reinforcement_Learning_with_a_Corrupted_Reward_Channel", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/318830044_Reinforcement_Learning_with_a...", "snippet": "PDF | No real-world reward function is perfect. Sensory errors and software bugs may result in agents getting higher (or lower) rewards than they... | Find, read and cite all the research you need ...", "dateLastCrawled": "2022-01-16T10:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Omscs Class Review - XpCourse</b>", "url": "https://www.xpcourse.com/omscs-class-review", "isFamilyFriendly": true, "displayUrl": "https://www.xpcourse.com/<b>omscs-class-review</b>", "snippet": "After all, taking an online course from a big brand business school doesn\u2019t require weeks or months of <b>studying</b> for a standardized test. You <b>can</b> do it without having to quit your job or make long sacrifices of time from your family. And it costs just a fraction of what you would pay in a full- or part-time MBA program, or for that matter, an online MBA or Executive MBA program.", "dateLastCrawled": "2021-12-25T23:12:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "The Who in Explainable AI: How AI Background Shapes Perceptions of AI ...", "url": "https://deepai.org/publication/the-who-in-explainable-ai-how-ai-background-shapes-perceptions-of-ai-explanations", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/the-who-in-explainable-ai-how-ai-background-shapes...", "snippet": "<b>Tabular</b> <b>Q-learning</b> agents attempt to learn the utility (called a Q-value for \u201cquality\u201d of the ... Granted not every AI <b>student</b> will go on to build AI systems, with the proliferation of AI systems in the workplace, a majority of these students could become stakeholders residing on the creation or development end of the technology spectrum\u2014as potential developers, designers, and managers of AI-based systems. As potential creators of AI systems, their perceptions matter in bridging the ...", "dateLastCrawled": "2021-12-28T17:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Top <b>Machine Learning Interview Questions</b> and Answers for 2022 - Edureka", "url": "https://www.edureka.co/blog/interview-questions/machine-learning-interview-questions/", "isFamilyFriendly": true, "displayUrl": "https://www.edureka.co/blog/interview-questions/<b>machine-learning-interview-questions</b>", "snippet": "The world has changed since Artificial Intelligence, Machine Learning, and Deep learning were introduced and will continue to do so in the years to come. In this <b>Machine Learning Interview Questions</b> in 2021 blog, I have collected the most frequently asked questions by interviewers. These questions are collected after consulting with Python Machine Learning certification training experts.. In case you have attended any <b>Machine Learning interview</b> in the recent past, do paste those interview ...", "dateLastCrawled": "2022-02-02T10:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Seita&#39;s Place", "url": "https://danieltakeshi.github.io/page10/", "isFamilyFriendly": true, "displayUrl": "https://danieltakeshi.github.io/page10", "snippet": "Think back to my <b>tabular</b> <b>Q-Learning</b> example in this post. The target was parameterized using \\(Q(s&#39;,a&#39;; \\theta)\\). When I perform an update using SGD, I updated \\(\\theta_{s,a}\\). If this turns out to be the same component as \\(\\theta_{s&#39;,a&#39;}\\), then this will automatically update the target. Think of the successor state as being equal to the current state. And, again, during the gradient update, the target was assumed fixed, which is why I did not re-write \\(Q(s&#39;,a&#39;;\\theta)\\) into a ...", "dateLastCrawled": "2022-01-16T03:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Machine Learning</b> - GeeksforGeeks", "url": "https://www.geeksforgeeks.org/machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.geeksforgeeks.org/<b>machine-learning</b>", "snippet": "<b>Machine Learning</b> is the field of study that gives computers the capability to learn without being explicitly programmed. ML is one of the most exciting technologies that one would have ever come across. As it is evident from the name, it gives the computer that makes it more similar to humans: The ability to learn.<b>Machine learning</b> is actively being used today, perhaps in many more places than one would expect.", "dateLastCrawled": "2022-02-02T05:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A Complete Guide On <b>KNN Algorithm In R</b> With Examples | Edureka", "url": "https://www.edureka.co/blog/knn-algorithm-in-r/", "isFamilyFriendly": true, "displayUrl": "https://www.edureka.co/blog/<b>knn-algorithm-in-r</b>", "snippet": "After <b>studying</b> the dataset during the training phase, when a new image is given to the model, the KNN algorithm will classify it into either cats or dogs depending on the similarity in their features. So if the new image has pointy ears, it will classify that image as a cat because it is similar to the cat images. In this manner, the KNN algorithm classifies data points based on how similar they are to their neighboring data points.", "dateLastCrawled": "2022-02-03T01:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) <b>Reinforcement Learning in Robotics: A</b> Survey", "url": "https://www.researchgate.net/publication/258140920_Reinforcement_Learning_in_Robotics_A_Survey", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/258140920_<b>Reinforcement_Learning_in_Robotics</b>...", "snippet": "Abstract and Figures. Reinforcement learning offers to robotics a framework and set of tools for the design of sophisticated and hard-to-engineer behaviors. Conversely, the challenges of robotic ...", "dateLastCrawled": "2022-01-24T19:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Reinforcement Learning: <b>Bellman</b> Equation and Optimality (Part 2) | by ...", "url": "https://towardsdatascience.com/reinforcement-learning-markov-decision-process-part-2-96837c936ec3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/reinforcement-learning-markov-decision-process-part-2...", "snippet": "<b>Bellman</b> Equation for Value Function (State-Value Function) From the above equation, we <b>can</b> see that the value of a s tate <b>can</b> be decomposed into immediate reward(R[t+1]) plus the value of successor state(v[S (t+1)]) with a discount factor(\ud835\udefe).This still stands for <b>Bellman</b> Expectation Equation. But now what we are doing is we are finding the value of a particular state subjected to some policy(\u03c0).This is the difference between the <b>Bellman</b> Equation and the <b>Bellman</b> Expectation Equation.", "dateLastCrawled": "2022-02-02T12:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) <b>Reinforcement Learning: An Introduction second edition</b> | BB DK ...", "url": "https://www.academia.edu/39631493/Reinforcement_Learning_An_Introduction_second_edition", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/39631493/<b>Reinforcement_Learning_An_Introduction_second_edition</b>", "snippet": "Academia.edu is a platform for academics to share research papers.", "dateLastCrawled": "2022-01-28T21:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Proceedings of the 59th Annual Meeting of the Association for ...", "url": "https://aclanthology.org/volumes/2021.acl-long/", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/volumes/2021.acl-long", "snippet": "While most related work tackles formal languages (e.g., <b>exam</b> papers), we generate poll questions for short and colloquial social media messages exhibiting severe data sparsity. To deal with that, we propose to encode user comments and discover latent topics therein as contexts. They are then incorporated into a sequence-to-sequence (S2S) architecture for question generation and its extension with dual decoders to additionally yield poll choices (answers). For experiments, we collect a large ...", "dateLastCrawled": "2022-01-29T06:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-Of ...", "url": "https://news.ycombinator.com/item?id=13518039", "isFamilyFriendly": true, "displayUrl": "https://news.ycombinator.com/item?id=13518039", "snippet": "Optimal behaviour (of any kind) <b>can</b> be framed as &quot;At each step, pick the action that maximizes your expected utility.&quot; So, for instance, you might study hard tonight because it&#39;ll lead you to pass your <b>exam</b> tomorrow and get a high-paying job later. In that scenario, <b>studying</b>&#39;s utility is higher than going out for a beer.", "dateLastCrawled": "2021-07-30T11:35:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "An <b>introduction to Q-Learning: Reinforcement Learning</b>", "url": "https://blog.floydhub.com/an-introduction-to-q-learning-reinforcement-learning/", "isFamilyFriendly": true, "displayUrl": "https://blog.floydhub.com/an-<b>introduction-to-q-learning-reinforcement-learning</b>", "snippet": "Reinforcement <b>learning</b> solves a particular kind of problem where decision making is sequential, and the goal is long-term, such as game playing, robotics, resource management, or logistics. For a robot, an environment is a place where it has been put to use. Remember this robot is itself the agent.", "dateLastCrawled": "2022-01-31T09:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Reinforcement <b>Q-Learning</b> from Scratch in Python with OpenAI Gym ...", "url": "https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/", "isFamilyFriendly": true, "displayUrl": "https://www.learndatasci.com/tutorials/reinforcement-<b>q-learning</b>-scratch-python-openai-gym", "snippet": "<b>Q-learning</b> is one of the easiest Reinforcement <b>Learning</b> algorithms. The problem with <b>Q-learning</b> however is, once the number of states in the environment are very high, it becomes difficult to implement them with Q table as the size would become very, very large. State of the art techniques uses Deep neural networks instead of the Q-table (Deep Reinforcement <b>Learning</b>). The neural network takes in state information and actions to the input layer and learns to output the right action over the ...", "dateLastCrawled": "2022-02-03T03:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) A comparison of <b>Q-learning</b> and Classifier Systems", "url": "https://www.researchgate.net/publication/2712709_A_comparison_of_Q-learning_and_Classifier_Systems", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/2712709_A_comparison_of_<b>Q-learning</b>_and...", "snippet": "Watkin&#39;s <b>tabular</b> <b>Q-learning</b> or other more efficient kinds of discrete partition of the state space like Chapman and Kaelbling (1991) or Munos et al. (1994)), to continuous", "dateLastCrawled": "2022-01-29T21:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>GAN Q-learning</b> | DeepAI", "url": "https://deepai.org/publication/gan-q-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>gan-q-learning</b>", "snippet": "Distributional reinforcement <b>learning</b> (distributional RL) has seen empirical success in complex Markov Decision Processes (MDPs) in the setting of nonlinear function approximation. However, there are many different ways in which one can leverage the distributional approach to reinforcement <b>learning</b>. In this paper, we propose <b>GAN Q-learning</b>, a novel distributional RL method based on generative adversarial networks (GANs) and analyze its performance in simple <b>tabular</b> environments, as well as ...", "dateLastCrawled": "2022-01-09T03:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Branch Prediction as a Reinforcement <b>Learning</b> Problem: Why, How and ...", "url": "https://ease-lab.github.io/ease_website/pubs/RL-BP_MLArchSys21.pdf", "isFamilyFriendly": true, "displayUrl": "https://ease-lab.github.io/ease_website/pubs/RL-BP_MLArchSys21.pdf", "snippet": "A. <b>Tabular</b> Methods: <b>Q-Learning</b> A number of <b>tabular</b> RL methods exist; most popular ones include TD-<b>learning</b> [15], SARSA [14], <b>Q-Learning</b> [17] and double <b>Q-Learning</b> [6]. Here we focus on the <b>Q-Learning</b> algorithm that provides speci\ufb01c convergence guarantees [17]3. <b>Q-Learning</b> stores the Q-values Q(s;a) for every state and action pair in a \ufb01xed-sized table. Given a state sfrom the environment, <b>Q-Learning</b> predicts the action greedily using the policy \u02c7 greedy (s). The <b>Q-Learning</b> update rule ...", "dateLastCrawled": "2021-11-20T06:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Reinforcement <b>learning</b>: <b>Temporal-Difference</b>, SARSA, <b>Q-Learning</b> ...", "url": "https://towardsdatascience.com/reinforcement-learning-temporal-difference-sarsa-q-learning-expected-sarsa-on-python-9fecfda7467e", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/reinforcement-<b>learning</b>-<b>temporal-difference</b>-sarsa-q...", "snippet": "Source: Introduction to Reinforcement <b>learning</b> by Sutton and Barto \u2014Chapter 6. The action A\u2019 in the above algorithm is given by following the same policy (\u03b5-greedy over the Q values) because SARSA is an on-policy method.. \u03b5-greedy policy. Epsilon-greedy policy is this: Generate a random number r \u2208[0,1]; If r&lt;\u03b5 choose an action derived from the Q values (which yields the maximum utility); Else choose a random action", "dateLastCrawled": "2022-02-03T02:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Q-learning</b> with Logarithmic Regret | DeepAI", "url": "https://deepai.org/publication/q-learning-with-logarithmic-regret", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>q-learning</b>-with-logarithmic-regret", "snippet": "<b>Q-learning</b> (Watkins and Dayan, 1992) is one of the most popular classes of methods for solving reinforcement <b>learning</b> (RL) problems. <b>Q-learning</b> tries to estimate the optimal state-action value function (. Q-function).With a Q-function, at every state, one can greedily choose the action with the largest Q value to interact with the RL environment while achieving near optimal expected cumulative rewards in the long run. Compared to another popular classes of methods, e.g., model-based RL, Q ...", "dateLastCrawled": "2022-01-27T08:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>PyTorch Tabular \u2013 A Framework for Deep Learning for Tabular Data</b> \u2013 Deep ...", "url": "https://deep-and-shallow.com/2021/01/27/pytorch-tabular-a-framework-for-deep-learning-for-tabular-data/", "isFamilyFriendly": true, "displayUrl": "https://deep-and-shallow.com/2021/01/27/<b>pytorch-tabular-a-framework-for</b>-deep-<b>learning</b>...", "snippet": "It is common knowledge that Gradient Boosting models, more often than not, kick the asses of every other <b>machine</b> <b>learning</b> models when it comes to <b>Tabular</b> Data.I have written extensively about Gradient Boosting, the theory behind and covered the different implementations like XGBoost, LightGBM, CatBoost, NGBoost etc. in detail. The unreasonable effectiveness of Deep <b>Learning</b> that was displayed in many other modalities \u2013 like text and image- haven not been demonstrated in <b>tabular</b> data.", "dateLastCrawled": "2022-01-29T08:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "On using Huber loss in (Deep) <b>Q-learning</b> | \u30e4\u30ed\u30df\u30eb", "url": "https://jaromiru.wordpress.com/2017/05/27/on-using-huber-loss-in-deep-q-learning/", "isFamilyFriendly": true, "displayUrl": "https://jaromiru.wordpress.com/2017/05/27/on-using-huber-loss-in-deep-<b>q-learning</b>", "snippet": "<b>MACHINE</b> <b>LEARNING</b> &amp; AI. Menu. Let\u2019s make a DQN. Theory; Implementation; Debugging; Full DQN; Double <b>Learning</b> and Prioritized Experience Replay; Let\u2019s make an A3C. Theory; Implementation; About me; On using Huber loss in (Deep) <b>Q-learning</b>. Posted on May 27, 2017 May 30, 2017 by \u30e4\u30ed\u30df\u30eb. I\u2019ve been recently working on a problem where I put a plain DQN to use. The problem is very simple, deterministic, partially observable and states are quite low-dimensional. The agent however can\u2019t ...", "dateLastCrawled": "2021-12-26T09:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Why doesn&#39;t <b>Q-learning</b> converge when using function approximation ...", "url": "https://ai.stackexchange.com/questions/11679/why-doesnt-q-learning-converge-when-using-function-approximation", "isFamilyFriendly": true, "displayUrl": "https://ai.stackexchange.com/questions/11679/why-doesnt-<b>q-learning</b>-converge-when-using...", "snippet": "In <b>tabular</b> <b>Q-learning</b>, when we update a Q-value, other Q-values in the table don&#39;t get affected by this. But in neural networks, one update to the weights aiming to alter one Q-value ends up affecting other Q-values whose states look similar (since neural networks learn a continuous function that is smooth)", "dateLastCrawled": "2022-01-28T08:14:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(tabular q-learning)  is like +(student studying for an exam)", "+(tabular q-learning) is similar to +(student studying for an exam)", "+(tabular q-learning) can be thought of as +(student studying for an exam)", "+(tabular q-learning) can be compared to +(student studying for an exam)", "machine learning +(tabular q-learning AND analogy)", "machine learning +(\"tabular q-learning is like\")", "machine learning +(\"tabular q-learning is similar\")", "machine learning +(\"just as tabular q-learning\")", "machine learning +(\"tabular q-learning can be thought of as\")", "machine learning +(\"tabular q-learning can be compared to\")"]}