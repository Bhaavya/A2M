{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Learning with a Wasserstein Loss</b> - Home | Poggio Lab", "url": "http://cbcl.mit.edu/wasserstein/wass_NIPS2015.pdf", "isFamilyFriendly": true, "displayUrl": "cbcl.mit.edu/<b>wasserstein</b>/wass_NIPS2015.pdf", "snippet": "<b>probability</b> <b>distributions</b> over metric spaces. In [3, 9], the optimal transport is used to formulate the <b>Wasserstein</b> barycenter as a <b>probability</b> distribution with minimum total <b>Wasserstein</b> distance to a set of given points on the <b>probability</b> simplex. [4] propagates histogram values on a graph by minimizing a Dirichlet energy induced by optimal transport. The <b>Wasserstein</b> distance is also used to formulate a metric for <b>comparing</b> clusters in [5], and is applied to image retrieval [10], contour 2 ...", "dateLastCrawled": "2022-02-01T18:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Learning with a <b>Wasserstein</b> <b>Loss</b> - People | MIT CSAIL", "url": "https://people.csail.mit.edu/hmobahi/pubs/wasserstein.pdf", "isFamilyFriendly": true, "displayUrl": "https://people.csail.mit.edu/hmobahi/pubs/<b>wasserstein</b>.pdf", "snippet": "<b>probability</b> <b>distributions</b> over metric spaces. In [3, 9], the optimal transport is used to formulate the <b>Wasserstein</b> barycenter as a <b>probability</b> distribution with minimum total <b>Wasserstein</b> distance to a set of given points on the <b>probability</b> simplex. [4] propagates histogram values on a graph by minimizing a Dirichlet energy induced by optimal transport. The <b>Wasserstein</b> distance is also used to formulate a metric for <b>comparing</b> clusters in [5], and is applied to image retrieval [10], contour 2 ...", "dateLastCrawled": "2022-01-31T07:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Learning with a Wasserstein Loss</b> - NIPS", "url": "https://proceedings.neurips.cc/paper/2015/file/a9eb812238f753132652ae09963a05e9-Paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/2015/file/a9eb812238f753132652ae09963a05e9-Paper.pdf", "snippet": "<b>probability</b> <b>distributions</b> over metric spaces. In [3, 9], the optimal transport is used to formulate the <b>Wasserstein</b> barycenter as a <b>probability</b> distribution with minimum total <b>Wasserstein</b> distance to a set of given points on the <b>probability</b> simplex. [4] propagates histogram values on a graph by minimizing a Dirichlet energy induced by optimal transport. The <b>Wasserstein</b> distance is also used to formulate a metric for <b>comparing</b> clusters in [5], and is applied to image retrieval [10], contour 2 ...", "dateLastCrawled": "2022-01-30T13:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "How to Implement <b>Wasserstein</b> <b>Loss</b> for Generative Adversarial Networks", "url": "https://machinelearningmastery.com/how-to-implement-wasserstein-loss-for-generative-adversarial-networks/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/how-to-implement-<b>wasserstein</b>-<b>loss</b>-for-generative...", "snippet": "The <b>Wasserstein</b> Generative Adversarial Network, or <b>Wasserstein</b> GAN, is an extension to the generative adversarial network that both improves the stability when training the model and provides a <b>loss</b> function that correlates with the quality of generated images. It is an important extension to the GAN model and requires a conceptual shift away from a discriminator that predicts the <b>probability</b> of", "dateLastCrawled": "2022-02-02T22:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Wasserstein Distance</b> and Textual Similarity - neptune.ai", "url": "https://neptune.ai/blog/wasserstein-distance-and-textual-similarity", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/<b>wasserstein-distance</b>-and-textual-similarity", "snippet": "<b>Wasserstein Distance</b> and Textual Similarity. In many machine learning (ML) projects, there comes a point when we have to decide the level of similarity between different objects of interest. We might be trying to understand the similarity between different images, weather patterns, or <b>probability</b> <b>distributions</b>.", "dateLastCrawled": "2022-01-19T11:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "HETEROGENEOUS <b>WASSERSTEIN</b> DISCREPANCY FOR INCOMPARABLE <b>DISTRIBUTIONS</b>", "url": "https://openreview.net/pdf?id=UORhn0DGIT", "isFamilyFriendly": true, "displayUrl": "https://openreview.net/pdf?id=UORhn0DGIT", "snippet": "<b>probability</b> measures. <b>Wasserstein</b> distance is for longer the celebrated OT-distance frequently-used in the literature, which seeks <b>probability</b> <b>distributions</b> to be sup- ported on the same metric space. Because of its high computational complexity, several approximate <b>Wasserstein</b> distances have been proposed based on entropy regularization or on slicing, and one-dimensional Wassserstein computation. In this paper, we propose a novel extension of <b>Wasserstein</b> distance to compare two incomparable ...", "dateLastCrawled": "2021-12-03T08:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Wasserstein GAN</b> \u00b7 Depth First Learning", "url": "https://www.depthfirstlearning.com/2019/WassersteinGAN", "isFamilyFriendly": true, "displayUrl": "https://www.depthfirstlearning.com/2019/<b>WassersteinGAN</b>", "snippet": "The paper builds upon an intuitive idea: the family of <b>Wasserstein</b> distances is a nice distance between <b>probability</b> <b>distributions</b>, that is well grounded in theory. The authors propose to use the 1-<b>Wasserstein</b> distance to estimate generative models. More specifically, they propose to use the 1-<b>Wasserstein</b> distance in place of the JSD in a standard GAN \u2014 that is to measure the difference between the true distribution and the model distribution of the data. They show that the 1-<b>Wasserstein</b> ...", "dateLastCrawled": "2022-01-30T22:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>distributions</b> - What is the advantages of <b>Wasserstein</b> metric compared ...", "url": "https://stats.stackexchange.com/questions/295617/what-is-the-advantages-of-wasserstein-metric-compared-to-kullback-leibler-diverg", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/295617", "snippet": "Here the measures between red and blue <b>distributions</b> are the same for KL divergence whereas <b>Wasserstein distance</b> measures the work required to transport the <b>probability</b> mass from the red state to the blue state using x-axis as a \u201croad\u201d. This measure is obviously the larger the further away the <b>probability</b> mass is (hence the alias earth mover&#39;s distance). So which one you want to use depends on your application area and what you want to measure. As a note, instead of KL divergence there ...", "dateLastCrawled": "2022-02-03T04:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>distributions</b> - What are the advantages of <b>Wasserstein distance</b> ...", "url": "https://stats.stackexchange.com/questions/490065/what-are-the-advantages-of-wasserstein-distance-compared-to-jensen-shannon-diver", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/490065/what-are-the-advantages-of...", "snippet": "<b>Wasserstein</b> metric is a distance function defined between <b>probability</b> <b>distributions</b> on a given metric space M. Intuitively, if each distribution is viewed as a unit amount of earth (soil) piled on M, the metric is the minimum &quot;cost&quot; of turning one pile into the other, which is assumed to be the amount of earth that needs to be moved times the mean distance it has to be moved.", "dateLastCrawled": "2022-01-25T17:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>probability</b> distribution - Why is <b>KL divergence</b> used so often in ...", "url": "https://ai.stackexchange.com/questions/25205/why-is-kl-divergence-used-so-often-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://ai.stackexchange.com/questions/25205/why-is-<b>kl-divergence</b>-used-so-often-in...", "snippet": "This post says &quot;KL is used to measure difference in 2 <b>probability</b> <b>distributions</b>, and is the most natural way to do so&quot;, but there are many other ways to measure the difference of 2 <b>distributions</b> and some are more natural/intuitive than KL (examples: <b>wasserstein</b> distance, cosine similarity, mean squared residuals, etc).", "dateLastCrawled": "2022-01-25T06:44:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Learning with a <b>Wasserstein</b> <b>Loss</b> - People | MIT CSAIL", "url": "https://people.csail.mit.edu/hmobahi/pubs/wasserstein.pdf", "isFamilyFriendly": true, "displayUrl": "https://people.csail.mit.edu/hmobahi/pubs/<b>wasserstein</b>.pdf", "snippet": "<b>probability</b> <b>distributions</b> over metric spaces. In [3, 9], the optimal transport is used to formulate the <b>Wasserstein</b> barycenter as a <b>probability</b> distribution with minimum total <b>Wasserstein</b> distance to a set of given points on the <b>probability</b> simplex. [4] propagates histogram values on a graph by minimizing a Dirichlet energy induced by optimal transport. The <b>Wasserstein</b> distance is also used to formulate a metric for <b>comparing</b> clusters in [5], and is applied to image retrieval [10], contour 2 ...", "dateLastCrawled": "2022-01-31T07:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Learning with a Wasserstein Loss</b> - Home | Poggio Lab", "url": "http://cbcl.mit.edu/wasserstein/wass_NIPS2015.pdf", "isFamilyFriendly": true, "displayUrl": "cbcl.mit.edu/<b>wasserstein</b>/wass_NIPS2015.pdf", "snippet": "<b>probability</b> <b>distributions</b> over metric spaces. In [3, 9], the optimal transport is used to formulate the <b>Wasserstein</b> barycenter as a <b>probability</b> distribution with minimum total <b>Wasserstein</b> distance to a set of given points on the <b>probability</b> simplex. [4] propagates histogram values on a graph by minimizing a Dirichlet energy induced by optimal transport. The <b>Wasserstein</b> distance is also used to formulate a metric for <b>comparing</b> clusters in [5], and is applied to image retrieval [10], contour 2 ...", "dateLastCrawled": "2022-02-01T18:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Wasserstein of Wasserstein Loss for Learning Generative Models</b>", "url": "http://proceedings.mlr.press/v97/dukler19a/dukler19a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v97/dukler19a/dukler19a.pdf", "snippet": "<b>Wasserstein of Wasserstein Loss for Learning Generative Models</b> ... successful in <b>comparing</b> images, has remained unnoticed in the context of WGANs, namely the <b>Wasserstein</b> distance on images (also named Earth Mover\u2019s distance or Monge- Kantorvich distance). In particular, the <b>Wasserstein</b> distance has been successful in image retrieval problems (Rubner et al.,2000;Zhang et al.,2007). It is known to correlate well with human perception for natural images, e.g., being robust to translations and ...", "dateLastCrawled": "2022-01-24T10:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>WASSERSTEIN</b> OF <b>WASSERSTEIN</b> <b>LOSS</b> FOR LEARNING GENERATIVE MODELS", "url": "https://www.researchgate.net/profile/Wuchen-Li-2/publication/330699954_Wasserstein_of_Wasserstein_Loss_for_Learning_Generative_Models/links/5c4faa6592851c22a3985217/Wasserstein-of-Wasserstein-Loss-for-Learning-Generative-Models.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/profile/Wuchen-Li-2/publication/330699954_<b>Wasserstein</b>_of...", "snippet": "<b>distributions</b> incorporates the distance between samples, via a ground metric of choice. In In this way, it provides a continuous <b>loss</b> function for learning <b>probability</b> models supported", "dateLastCrawled": "2022-01-02T04:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Learning with a Wasserstein Loss</b> - NIPS", "url": "https://proceedings.neurips.cc/paper/2015/file/a9eb812238f753132652ae09963a05e9-Paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/2015/file/a9eb812238f753132652ae09963a05e9-Paper.pdf", "snippet": "<b>probability</b> <b>distributions</b> over metric spaces. In [3, 9], the optimal transport is used to formulate the <b>Wasserstein</b> barycenter as a <b>probability</b> distribution with minimum total <b>Wasserstein</b> distance to a set of given points on the <b>probability</b> simplex. [4] propagates histogram values on a graph by minimizing a Dirichlet energy induced by optimal transport. The <b>Wasserstein</b> distance is also used to formulate a metric for <b>comparing</b> clusters in [5], and is applied to image retrieval [10], contour 2 ...", "dateLastCrawled": "2022-01-30T13:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Wasserstein Distance</b> and Textual Similarity - neptune.ai", "url": "https://neptune.ai/blog/wasserstein-distance-and-textual-similarity", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/<b>wasserstein-distance</b>-and-textual-<b>similar</b>ity", "snippet": "<b>Wasserstein Distance</b> and Textual Similarity. In many machine learning (ML) projects, there comes a point when we have to decide the level of similarity between different objects of interest. We might be trying to understand the similarity between different images, weather patterns, or <b>probability</b> <b>distributions</b>.", "dateLastCrawled": "2022-01-19T11:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(q,p)-<b>Wasserstein</b> GANs: <b>Comparing</b> Ground Metrics for <b>Wasserstein</b> GANs ...", "url": "https://deepai.org/publication/q-p-wasserstein-gans-comparing-ground-metrics-for-wasserstein-gans", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/q-p-<b>wasserstein</b>-gans-<b>comparing</b>-ground-metrics-for...", "snippet": "Recently, a popular family of metrics has been provided by the theory of Optimal Transport (OT), which studies <b>probability</b> <b>distributions</b> through a geometric framework. At its heart lie the <b>Wasserstein</b> metrics, which extend the underlying metric between sample points to entire <b>distributions</b>.Consequently, the metrics can be used to e.g. derive statistics between populations of <b>probability</b> <b>distributions</b>, allowing the inclusion of stochastic data objects in statistical pipelines (Mallasto ...", "dateLastCrawled": "2022-01-21T04:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "An <b>intuitive guide to optimal transport, part II: the Wasserstein</b> GAN ...", "url": "http://modelai.gettysburg.edu/2020/wgan/Resources/Lesson4/IntuitiveGuideOT.htm", "isFamilyFriendly": true, "displayUrl": "modelai.gettysburg.edu/2020/wgan/Resources/Lesson4/IntuitiveGuideOT.htm", "snippet": "While the discussion was rather technical, the take home message is simple: the <b>Wasserstein</b> metric can be used for <b>comparing</b> <b>probability</b> <b>distributions</b> that are radically different. What do I mean by different? The most common example is when two <b>distributions</b> have different support, meaning that they assign zero <b>probability</b> to different families of sets. For example, assume that", "dateLastCrawled": "2022-01-24T17:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "DISTANCES BETWEEN <b>PROBABILITY</b> <b>DISTRIBUTIONS</b> OF DIFFERENT DIMENSIONS", "url": "http://www.stat.uchicago.edu/~lekheng/work/probdist.pdf", "isFamilyFriendly": true, "displayUrl": "www.stat.uchicago.edu/~lekheng/work/probdist.pdf", "snippet": "<b>Comparing</b> <b>probability</b> <b>distributions</b> is an indispensable and ubiquitous task in machine learning and statistics. The most common way to compare a pair of Borel <b>probability</b> measures is to compute a metric between them, and by far the most widely used notions of metric are the <b>Wasserstein</b> metric and the total variation metric. The next most common way is to compute a divergence between them, and in this case almost every known divergences such as those of Kullback{Leibler, Jensen{Shannon, R ...", "dateLastCrawled": "2022-01-13T13:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Wasserstein GAN</b> \u00b7 Depth First Learning", "url": "https://www.depthfirstlearning.com/2019/WassersteinGAN", "isFamilyFriendly": true, "displayUrl": "https://www.depthfirstlearning.com/2019/<b>WassersteinGAN</b>", "snippet": "The paper builds upon an intuitive idea: the family of <b>Wasserstein</b> distances is a nice distance between <b>probability</b> <b>distributions</b>, that is well grounded in theory. The authors propose to use the 1-<b>Wasserstein</b> distance to estimate generative models. More specifically, they propose to use the 1-<b>Wasserstein</b> distance in place of the JSD in a standard GAN \u2014 that is to measure the difference between the true distribution and the model distribution of the data. They show that the 1-<b>Wasserstein</b> ...", "dateLastCrawled": "2022-01-30T22:27:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "How to Implement <b>Wasserstein</b> <b>Loss</b> for Generative Adversarial Networks", "url": "https://machinelearningmastery.com/how-to-implement-wasserstein-loss-for-generative-adversarial-networks/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/how-to-implement-<b>wasserstein</b>-<b>loss</b>-for-generative...", "snippet": "The most fundamental difference between such distances is their impact on the convergence of sequences of <b>probability</b> <b>distributions</b>. \u2014 <b>Wasserstein</b> GAN, 2017. They demonstrate that a critic neural network <b>can</b> be trained to approximate the <b>Wasserstein</b> distance, and, in turn, used to effectively train a generator model. \u2026 we define a form of GAN called <b>Wasserstein</b>-GAN that minimizes a reasonable and efficient approximation of the EM distance, and we theoretically show that the corresponding ...", "dateLastCrawled": "2022-02-02T22:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>distributions</b> - What is the advantages of <b>Wasserstein</b> metric compared ...", "url": "https://stats.stackexchange.com/questions/295617/what-is-the-advantages-of-wasserstein-metric-compared-to-kullback-leibler-diverg", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/295617", "snippet": "<b>Wasserstein</b> (or Vaserstein) metric is a distance function defined between <b>probability</b> <b>distributions</b> on a given metric space M. and. Kullback\u2013Leibler divergence is a measure of how one <b>probability</b> distribution diverges from a second expected <b>probability</b> distribution. I&#39;ve seen KL been used in machine learning implementations, but I recently came across the <b>Wasserstein</b> metric. Is there a good guideline on when to use one or the other? (I have insufficient reputation to create a new tag with ...", "dateLastCrawled": "2022-02-03T04:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Wasserstein</b> gradients for the temporal evolution of <b>probability</b> ...", "url": "https://www.researchgate.net/publication/354257387_Wasserstein_gradients_for_the_temporal_evolution_of_probability_distributions", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/354257387_<b>Wasserstein</b>_gradients_for_the...", "snippet": "This is an expository paper on the theory of gradient flows, and in particular of those PDEs which <b>can</b> be interpreted as gradient flows for the <b>Wasserstein</b> metric on the space of <b>probability</b> ...", "dateLastCrawled": "2021-10-22T16:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "On the equivalence between Fourier-based and <b>Wasserstein</b> metrics", "url": "https://mate.unipv.it/toscani/publi/Immagini.pdf", "isFamilyFriendly": true, "displayUrl": "https://mate.unipv.it/tos<b>can</b>i/publi/Immagini.pdf", "snippet": "de\ufb01ned also for <b>probability</b> <b>distributions</b> with di\ufb00erent centers of mass, and for discrete <b>probability</b> measures supported over a regular grid. Among other properties, it is shown that, in the discrete setting, these new Fourier-based met- rics are equivalent either to the Euclidean-<b>Wasserstein</b> distance W 2, or to the Kantorovich-<b>Wasserstein</b> distance W 1, with explicit constants of equivalence. Keywords Fourier-based Metrics \u00b7<b>Wasserstein</b> Distance \u00b7Fourier Transform Mathematics Subject ...", "dateLastCrawled": "2021-08-29T00:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Semantic Image Inpainting Through Improved <b>Wasserstein</b> Generative ...", "url": "https://deepai.org/publication/semantic-image-inpainting-through-improved-wasserstein-generative-adversarial-networks", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/semantic-image-inpainting-through-improved-<b>wasserstein</b>...", "snippet": "Moreover, the <b>Wasserstein</b> or EM distance is known to be a powerful tool to compare <b>probability</b> <b>distributions</b> with non-overlapping supports, in contrast to other distances such as the Kullback-Leibler divergence and the Jensen-Shannon divergence (used in the DCGAN and other GAN approaches) which produce the vanishing gradients problem, as mentioned above. Using the Kantorovich-Rubinstein duality, the <b>Wasserstein</b> distance between two <b>distributions</b>, say a", "dateLastCrawled": "2022-01-21T22:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Comparing</b> <b>Distributions</b> by Measuring Dif- ferences that Affect Decision ...", "url": "https://openreview.net/pdf?id=KB5onONJIAU", "isFamilyFriendly": true, "displayUrl": "https://openreview.net/pdf?id=KB5onONJIAU", "snippet": "<b>Comparing</b> <b>Distributions</b> by Measuring Dif-ferences that Affect Decision Making Anonymous authors Paper under double-blind review Abstract Measuring the discrepancy between two <b>probability</b> <b>distributions</b> is a fun-damental problem in machine learning and statistics. We propose a new class of discrepancies based on the optimal <b>loss</b> for a decision task { two <b>distributions</b> are di erent if the optimal decision <b>loss</b> is higher on their mixture than on each individual distribution. By suitably choosing ...", "dateLastCrawled": "2022-01-11T14:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Sample generation based on a supervised <b>Wasserstein</b> Generative ...", "url": "https://www.sciencedirect.com/science/article/pii/S002002552030606X", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S002002552030606X", "snippet": "The <b>Wasserstein</b> distance (also called earth-mover\u2019s distance) <b>can</b> be interpreted as the minimum energy cost of moving and transforming a pile of dirt in the shape of a <b>probability</b> distribution to the shape of the other distribution. It <b>can</b> always produce a meaningful gradient during the training process; even the generator distribution and the real data distribution are disjointed. Therefore, it is used to solve the problem of gradient vanishing in GAN and to implement a <b>Wasserstein</b> ...", "dateLastCrawled": "2021-12-22T03:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Newest &#39;wasserstein&#39; Questions</b> - Cross Validated", "url": "https://stats.stackexchange.com/questions/tagged/wasserstein", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/tagged/<b>wasserstein</b>", "snippet": "Reference for <b>Wasserstein</b> distance for generalized exponential family and generalized extreme value <b>probability</b> <b>distributions</b> The Wikipedia page gives the analytical results for computing the <b>Wasserstein</b> metric between two multivariate normal <b>distributions</b>, and I <b>thought</b> it used to give results for several other commonly ...", "dateLastCrawled": "2022-01-12T15:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Conditional <b>Wasserstein</b> GAN-based Oversampling of Tabular Data for ...", "url": "https://www.arxiv-vanity.com/papers/2008.09202/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2008.09202", "snippet": "Credit scoring is concerned with estimating the <b>probability</b> that a borrower will not pay back their loan or become otherwise delinquent ... The <b>Wasserstein</b>-1 distance is also called Earth Mover distance as the <b>Wasserstein</b>-1 distance between two <b>distributions</b> <b>can</b> be interpreted as the \u201ccost&quot; of the optimal transport plan to move <b>probability</b> mass of one distribution until it matches the other. Thus, even if p g and p data have disjoint supports, the <b>Wasserstein</b>-1 distance is a meaningful ...", "dateLastCrawled": "2022-01-25T03:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Sizing energy storage to reduce renewable power ... - Wiley Online Library", "url": "https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/iet-rpg.2020.0354", "isFamilyFriendly": true, "displayUrl": "https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/iet-rpg.2020.0354", "snippet": "The uncertainty of renewable generation is described via inexact <b>probability</b> <b>distributions</b> encapsulated in a data-driven <b>Wasserstein</b>-metric based ambiguity set, based on which the renewable energy curtailment rate is formulated as a distributionally robust chance constraint. The objective is to minimise the total investment cost, and the optimal sizing problem gives rise to a distributionally robust chance-constrained program, and is reformulated as a tractable linear program via ...", "dateLastCrawled": "2022-02-03T07:26:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>distributions</b> - What are the advantages of <b>Wasserstein distance</b> ...", "url": "https://stats.stackexchange.com/questions/490065/what-are-the-advantages-of-wasserstein-distance-compared-to-jensen-shannon-diver", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/490065/what-are-the-advantages-of...", "snippet": "<b>Wasserstein</b> metric is a distance function defined between <b>probability</b> <b>distributions</b> on a given metric space M. Intuitively, if each distribution is viewed as a unit amount of earth (soil) piled on M, the metric is the minimum &quot;cost&quot; of turning one pile into the other, which is assumed to be the amount of earth that needs to be moved times the mean distance it has to be moved. and. Jensen-Shannon divergence is a method of measuring the similarity between two <b>probability</b> <b>distributions</b>. It is ...", "dateLastCrawled": "2022-01-25T17:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(q,p)-<b>Wasserstein</b> GANs: <b>Comparing</b> Ground Metrics for <b>Wasserstein</b> GANs ...", "url": "https://deepai.org/publication/q-p-wasserstein-gans-comparing-ground-metrics-for-wasserstein-gans", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/q-p-<b>wasserstein</b>-gans-<b>comparing</b>-ground-metrics-for...", "snippet": "Recently, a popular family of metrics has been provided by the theory of Optimal Transport (OT), which studies <b>probability</b> <b>distributions</b> through a geometric framework. At its heart lie the <b>Wasserstein</b> metrics, which extend the underlying metric between sample points to entire <b>distributions</b>.Consequently, the metrics <b>can</b> be used to e.g. derive statistics between populations of <b>probability</b> <b>distributions</b>, allowing the inclusion of stochastic data objects in statistical pipelines (Mallasto ...", "dateLastCrawled": "2022-01-21T04:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) Intrinsic Sliced <b>Wasserstein</b> Distances for <b>Comparing</b> Collections ...", "url": "https://www.researchgate.net/publication/344971884_Intrinsic_Sliced_Wasserstein_Distances_for_Comparing_Collections_of_Probability_Distributions_on_Manifolds_and_Graphs", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/344971884_Intrinsic_Sliced_<b>Wasserstein</b>...", "snippet": "PDF | Collections of <b>probability</b> <b>distributions</b> arise in a variety of statistical applications ranging from user activity pattern analysis to brain... | Find, read and cite all the research you ...", "dateLastCrawled": "2021-08-16T15:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>distributions</b> - What is the advantages of <b>Wasserstein</b> metric <b>compared</b> ...", "url": "https://stats.stackexchange.com/questions/295617/what-is-the-advantages-of-wasserstein-metric-compared-to-kullback-leibler-diverg", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/295617", "snippet": "Here the measures between red and blue <b>distributions</b> are the same for KL divergence whereas <b>Wasserstein distance</b> measures the work required to transport the <b>probability</b> mass from the red state to the blue state using x-axis as a \u201croad\u201d. This measure is obviously the larger the further away the <b>probability</b> mass is (hence the alias earth mover&#39;s distance). So which one you want to use depends on your application area and what you want to measure. As a note, instead of KL divergence there ...", "dateLastCrawled": "2022-02-03T04:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(q,p)-<b>Wasserstein</b> GANs: <b>Comparing</b> Ground Metrics for <b>Wasserstein</b> GANs", "url": "https://www.arxiv-vanity.com/papers/1902.03642/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1902.03642", "snippet": "Generative Adversial Networks (GANs) have made a major impact in computer vision and machine learning as generative models. <b>Wasserstein</b> GANs (WGANs) brought Optimal Transport (OT) theory into GANs, by minimizing the 1-<b>Wasserstein</b> distance between model and data <b>distributions</b> as their objective function. Since then, WGANs have gained considerable interest due to their stability and theoretical framework. We contribute to the WGAN literature by introducing the family of (q,p)-<b>Wasserstein</b> GANs ...", "dateLastCrawled": "2021-12-07T19:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Wasserstein Distance</b> and Textual Similarity - neptune.ai", "url": "https://neptune.ai/blog/wasserstein-distance-and-textual-similarity", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/<b>wasserstein-distance</b>-and-textual-similarity", "snippet": "<b>Wasserstein Distance</b> and Textual Similarity. In many machine learning (ML) projects, there comes a point when we have to decide the level of similarity between different objects of interest. We might be trying to understand the similarity between different images, weather patterns, or <b>probability</b> <b>distributions</b>.", "dateLastCrawled": "2022-01-19T11:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Quantum <b>Wasserstein</b> GANs - NeurIPS", "url": "https://proceedings.neurips.cc/paper/8903-quantum-wasserstein-generative-adversarial-networks.pdf", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/8903-quantum-<b>wasserstein</b>-generative-adversarial...", "snippet": "of classical <b>distributions</b>, where the <b>loss</b> function measuring the difference between the real and the fake <b>distributions</b> <b>can</b> be borrowed directly from the classical GANs, the design of the <b>loss</b> function between real and fake quantum data as well as the ef\ufb01cient training of the corresponding GAN is much more challenging. The only existing results on quantum data either have a unique design speci\ufb01c to the 1-qubit case [13, 23], or suffer from robust training issues discussed below [4 ...", "dateLastCrawled": "2022-01-25T16:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Normalized Wasserstein Distance for Mixture Distributions</b> with ...", "url": "https://deepai.org/publication/normalized-wasserstein-distance-for-mixture-distributions-with-applications-in-adversarial-learning-and-domain-adaptation", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>normalized-wasserstein-distance-for-mixture</b>...", "snippet": "Quantifying distances between <b>probability</b> <b>distributions</b> is a fundamental problem in machine learning and statistics with several applications in generative models, domain adaptation, hypothesis testing, etc. Popular <b>probability</b> distance measures include . optimal transport measures such as the <b>Wasserstein</b> distance (Villani, 2008) and divergence measures such as the Kullback-Leibler (KL) divergence (Cover &amp; Thomas, 2012). Classical distance measures, however, <b>can</b> lead to some issues for ...", "dateLastCrawled": "2022-02-03T20:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A Simulated Annealing Based Inexact Oracle for <b>Wasserstein</b> <b>Loss</b> ...", "url": "http://proceedings.mlr.press/v70/ye17b/ye17b.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v70/ye17b/ye17b.pdf", "snippet": "<b>probability</b> measures, originated from the literature of op-timal transport (OT) (Monge, 1781). It takes into account the cross-term similarity between different support points of the <b>distributions</b>, a level of complexity beyond the usual vector data treatment, i.e., to convert the distribution into a vector of frequencies. It has been promoted for <b>comparing</b> sets of vectors (e.g. bag-of-words models) by researchers in computer vision, multimedia and more recently natural language processing ...", "dateLastCrawled": "2022-01-29T17:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Comparing</b> <b>Distributions</b> by Measuring Dif- ferences that Affect Decision ...", "url": "https://openreview.net/pdf?id=KB5onONJIAU", "isFamilyFriendly": true, "displayUrl": "https://openreview.net/pdf?id=KB5onONJIAU", "snippet": "<b>Comparing</b> <b>Distributions</b> by Measuring Dif-ferences that Affect Decision Making Anonymous authors Paper under double-blind review Abstract Measuring the discrepancy between two <b>probability</b> <b>distributions</b> is a fun-damental problem in machine learning and statistics. We propose a new class of discrepancies based on the optimal <b>loss</b> for a decision task { two <b>distributions</b> are di erent if the optimal decision <b>loss</b> is higher on their mixture than on each individual distribution. By suitably choosing ...", "dateLastCrawled": "2022-01-11T14:02:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "How to stabilize GAN training. Understand <b>Wasserstein</b> distance and ...", "url": "https://towardsdatascience.com/wasserstein-distance-gan-began-and-progressively-growing-gan-7e099f38da96", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>wasserstein</b>-distance-gan-began-and-progressively...", "snippet": "<b>Wasserstein</b> <b>loss</b> leads to a higher quality of the gradients to train G. ... Finally, one intuitive way to understand this paper is to make an <b>analogy</b> with the gradients on the history of in-layer activation functions. Specifically, the gradients of sigmoid and tanh activations that disappeared in favor of ReLUs, because of the improved gradients in the whole range of values. BEGAN (Boundary Equilibrium Generative Adversarial Networks 2017) We often see that the discriminator progresses too ...", "dateLastCrawled": "2022-01-25T18:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Learning Wasserstein Embeddings</b> | DeepAI", "url": "https://deepai.org/publication/learning-wasserstein-embeddings", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>learning-wasserstein-embeddings</b>", "snippet": "The <b>Wasserstein</b> distance received a lot of attention recently in the community of <b>machine</b> <b>learning</b>, especially for its principled way of comparing distributions. It has found numerous applications in several hard problems, such as domain adaptation, dimensionality reduction or generative models. However, its use is still limited by a heavy ...", "dateLastCrawled": "2022-01-05T07:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>Learning</b> <b>Wasserstein</b> Embeddings - ResearchGate", "url": "https://www.researchgate.net/publication/320564581_Learning_Wasserstein_Embeddings", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/320564581_<b>Learning</b>_<b>Wasserstein</b>_Embeddings", "snippet": "Designed through an <b>analogy</b> with ... Fast dictionary <b>learning</b> with a smoothed <b>wasserstein</b> <b>loss</b>. In AISTA TS, pages 630\u2013638, 2016. [32] F. Santambrogio. Introduction to optimal transport theory ...", "dateLastCrawled": "2021-12-13T04:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Wasserstein</b> GANs \u2013 Emma Benjaminson \u2013 Mechanical Engineering Graduate ...", "url": "https://sassafras13.github.io/Wasserstein/", "isFamilyFriendly": true, "displayUrl": "https://sassafras13.github.io/<b>Wasserstein</b>", "snippet": "Welcome back to the blog. Today we are (still) talking about MolGAN, this time with a focus on the <b>loss</b> function used to train the entire architecture. De Cao and Kipf use a <b>Wasserstein</b> GAN (WGAN) to operate on graphs, and today we are going to understand what that means [1]. The WGAN was developed by another team of researchers, Arjovsky et al., in 2017, and it uses the <b>Wasserstein</b> distance to compute the <b>loss</b> function for training the GAN [2]. In this post, we will provide some motivation ...", "dateLastCrawled": "2022-01-29T02:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Wasserstein</b> Distributionally Robust Optimization: Theory and ...", "url": "https://www.researchgate.net/publication/335395361_Wasserstein_Distributionally_Robust_Optimization_Theory_and_Applications_in_Machine_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/335395361_<b>Wasserstein</b>_Distributionally_Robust...", "snippet": "The <b>Wasserstein</b> distance between two probability measures on a metric space is a measure of closeness with applications in statistics, probability, and <b>machine</b> <b>learning</b>. In this work, we consider ...", "dateLastCrawled": "2022-01-26T23:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "[D] Is the <b>Wasserstein</b> distance really what we optimize in WGAN ...", "url": "https://www.reddit.com/r/MachineLearning/comments/ew2lzs/d_is_the_wasserstein_distance_really_what_we/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/<b>MachineLearning</b>/comments/ew2lzs/d_is_the_<b>wasserstein</b>_distance...", "snippet": "The &quot;genuine&quot; <b>Wasserstein</b> <b>loss</b> relies on optimal transport, a generalization of sorting to high-dimensional feature spaces. In a nutshell: OT relies on the matrix of distances between samples to define a &quot;least action&quot; matching between any two distributions. Now, unfortunately, in spaces of images, the L2 distance is (essentially) meaningless: natural images should not be compared with each other pixel-wise. As a consequence, the baseline <b>Wasserstein</b> distance between two batches of images is ...", "dateLastCrawled": "2021-09-30T11:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "deep <b>learning</b> - How can both generator and discriminator losses ...", "url": "https://datascience.stackexchange.com/questions/32699/how-can-both-generator-and-discriminator-losses-decrease", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/32699", "snippet": "In the widely used <b>analogy</b>: ... despite the WGAN having a different <b>loss</b> function, namely the <b>Wasserstein</b> distance, one should still not expect that the discriminator and generator simultaneously monotonically increase -- generally one of them &quot;wins&quot; the round and receives a lower portion of the <b>loss</b>. $\\endgroup$ \u2013 PSub. Mar 13 &#39;21 at 6:07 $\\begingroup$ @PSub You are completely misunderstanding the question. It&#39;s not a question about the small scale changes of the <b>loss</b> values. OP is asking ...", "dateLastCrawled": "2022-01-28T20:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Advanced <b>Machine</b> <b>Learning</b> - Computer Vision", "url": "https://www.vision.rwth-aachen.de/media/course/SS/2019/advanced-machine-learning/ml2_19-part17-gans-6on1.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.vision.rwth-aachen.de/media/course/SS/2019/advanced-<b>machine</b>-<b>learning</b>/ml2...", "snippet": "<b>Analogy</b>: police investigator \u2022Both generator and discriminator are deep networks We can train them with backprop. Image sources: www.bundesbank.de, weclipart.com, Kevin McGuiness 15 Advanced <b>Machine</b> <b>Learning</b> Part 17 \u2013Generative Adversarial Networks Training the Discriminator \u2022Procedure Fix generator weights Train discriminator to distinguish between real and generated images Image credit: Kevin McGuiness 16 Visual Computing Institute | Prof. Dr . Bastian Leibe Advanced <b>Machine</b> <b>Learning</b> ...", "dateLastCrawled": "2021-10-25T15:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A <b>Tour of Generative Adversarial Network Models</b> - <b>Machine</b> <b>Learning</b> Mastery", "url": "https://machinelearningmastery.com/tour-of-generative-adversarial-network-models/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>tour-of-generative-adversarial-network-models</b>", "snippet": "By <b>analogy</b> with auto-encoders, we propose Context Encoders \u2013 a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. \u2014 Context Encoders: Feature <b>Learning</b> by Inpainting, 2016. Example of the Context Encoders Encoder-Decoder Model Architecture. Taken from: Context Encoders: Feature <b>Learning</b> by Inpainting. The model is trained with a joint-<b>loss</b> that combines both the adversarial <b>loss</b> of generator and discriminator models ...", "dateLastCrawled": "2022-02-03T03:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "A <b>Gentle Introduction to Pix2Pix Generative</b> Adversarial Network", "url": "https://machinelearningmastery.com/a-gentle-introduction-to-pix2pix-generative-adversarial-network/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/a-<b>gentle-introduction-to-pix2pix-generative</b>...", "snippet": "Image-to-image translation is the controlled conversion of a given source image to a target image. An example might be the conversion of black and white photographs to color photographs. Image-to-image translation is a challenging problem and often requires specialized models and <b>loss</b> functions for a given translation task or dataset. The Pix2Pix GAN is a general approach for image-to-image translation. It is based", "dateLastCrawled": "2022-02-02T13:26:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(wasserstein loss)  is like +(comparing probability distributions)", "+(wasserstein loss) is similar to +(comparing probability distributions)", "+(wasserstein loss) can be thought of as +(comparing probability distributions)", "+(wasserstein loss) can be compared to +(comparing probability distributions)", "machine learning +(wasserstein loss AND analogy)", "machine learning +(\"wasserstein loss is like\")", "machine learning +(\"wasserstein loss is similar\")", "machine learning +(\"just as wasserstein loss\")", "machine learning +(\"wasserstein loss can be thought of as\")", "machine learning +(\"wasserstein loss can be compared to\")"]}