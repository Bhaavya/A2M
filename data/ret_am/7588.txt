{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A Gentle Introduction to <b>Cross-Entropy for Machine Learning</b>", "url": "https://machinelearningmastery.com/cross-entropy-for-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>cross-entropy-for-machine-learning</b>", "snippet": "Last Updated on December 22, 2020. <b>Cross-entropy</b> is commonly used in machine learning as a loss function. <b>Cross-entropy</b> is a measure from the field of information theory, building upon entropy and generally calculating the difference between two probability distributions. It is closely related to but is different from KL divergence that calculates the relative entropy between two probability distributions, whereas <b>cross-entropy</b> can be thought to calculate the total entropy between the ...", "dateLastCrawled": "2022-02-03T03:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "What Is <b>Cross-Entropy</b> Loss? | 365 Data Science", "url": "https://365datascience.com/tutorials/machine-learning-tutorials/cross-entropy-loss/", "isFamilyFriendly": true, "displayUrl": "https://365datascience.com/tutorials/machine-learning-tutorials/<b>cross-entropy</b>-loss", "snippet": "It\u2019s no <b>surprise</b> that <b>cross-entropy</b> loss is the most popular function used in machine learning or deep learning classification. After all, it helps determine the accuracy of our model in numerical values \u2013 0s and 1s, which we can later extract the probability percentage from. There are, of course, other loss functions that can help us resolve a problem. We must emphasize that any function that holds the basic property of being higher for worse results and lower for better results can be ...", "dateLastCrawled": "2022-02-02T15:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Cross-Entropy</b> Demystified. What is it? Is there any relation to\u2026 | by ...", "url": "https://naokishibuya.medium.com/demystifying-cross-entropy-e80e3ad54a8", "isFamilyFriendly": true, "displayUrl": "https://naokishibuya.medium.com/demystifying-<b>cross-entropy</b>-e80e3ad54a8", "snippet": "The <b>cross-entropy</b> compares the model\u2019s prediction with the label which is the true probability distribution. The <b>cross-entropy</b> goes down as the prediction gets more and more accurate. It becomes zero if the prediction is perfect. As such, the <b>cross-entropy</b> can be a loss function to train a classification model. Notes on Nats vs. Bits", "dateLastCrawled": "2022-01-26T13:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "This is equivalent to the <b>cross entropy</b> for a random variable with a ...", "url": "https://www.coursehero.com/file/pd6ptjm/This-is-equivalent-to-the-cross-entropy-for-a-random-variable-with-a-Gaussian/", "isFamilyFriendly": true, "displayUrl": "https://www.coursehero.com/file/pd6ptjm/This-is-equivalent-to-the-<b>cross-entropy</b>-for-a...", "snippet": "<b>Cross-entropy</b> is different from KL divergence but can be calculated using KL divergence, and is different from log loss but calculates the same quantity when used as a loss function. 23.8. Summary 223 23.8.1 Next In the next tutorial, you will discover information gain and mutual information between two probability distributions. Chapter 24 Information Gain and Mutual Information Information gain calculates the reduction in entropy or <b>surprise</b> from transforming a dataset in some way. It is ...", "dateLastCrawled": "2021-10-13T17:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "MIROSTAT: A NEURAL TEXT DECODING ALGORITHM THAT DIRECTLY CONTROLS ...", "url": "https://basusourya.medium.com/mirostat-a-perplexity-controlled-neural-text-decoding-algorithm-249f74921ea6", "isFamilyFriendly": true, "displayUrl": "https://basusourya.<b>medium</b>.com/mirostat-a-perplexity-controlled-neural-text-decoding...", "snippet": "In this paper, we work with an information-theoretic notion of <b>surprise</b> (defined in the paper, and is closely related to <b>cross-entropy</b> and perplexity) and analyze the <b>surprise</b> content in texts generated using popular decoding algorithms, find their relation to repetitions and incoherence in generated texts, and propose a text decoding algorithm that provides control over the <b>surprise</b> content in the texts. Hence, allowing the user to generate texts with a desirable amount of <b>surprise</b> in them.", "dateLastCrawled": "2022-01-22T06:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Entropy</b> Demystified. Is it a disorder, uncertainty or\u2026 | by Naoki | Medium", "url": "https://naokishibuya.medium.com/demystifying-entropy-f2c3221e2550", "isFamilyFriendly": true, "displayUrl": "https://naokishibuya.medium.com/demystifying-<b>entropy</b>-f2c3221e2550", "snippet": "Low <b>entropy</b> means that most of the times we are receiving the more predictable information which means less disorder, less uncertainty, less <b>surprise</b>, more predictability and less (specific) information. I hope these analogies are no longer confusing for you. Related Articles. Demystifying <b>Cross-Entropy</b>", "dateLastCrawled": "2022-01-29T16:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "KL divergence or relative entropy - Stanford University", "url": "https://nlp.stanford.edu/fsnlp/mathfound/fsnlp-slides-kl.pdf", "isFamilyFriendly": true, "displayUrl": "https://nlp.stanford.edu/fsnlp/mathfound/fsnlp-slides-kl.pdf", "snippet": "But we know roughly what it <b>is like</b> from a corpus <b>Cross entropy</b>: H\u2014X;q\u2013 \u2026H\u2014X\u2013\u2021D\u2014pkq\u2013 (8) \u2026\u2212 X x p\u2014x\u2013logq\u2014x\u2013 \u2026 E p\u2014log 1 q\u2014x\u2013 \u2013 (9) 16 <b>Cross entropy</b> of a language L\u2026 \u2014Xi\u2013\u02d8 p\u2014x\u2013according to a model m: H\u2014L;m\u2013\u2026\u2212lim n!1 1 n X x1n p\u2014x1n\u2013logm\u2014x1n\u2013 If the language is \u2018nice\u2019: H\u2014L;m\u2013\u2026\u2212lim n!1 1 n logm\u2014x1n\u2013 (10) I.e., it\u2019s just our average <b>surprise</b> for large n: H\u2014L;m\u2013\u02c7\u2212 1 n logm\u2014x1n\u2013 (11) Since H\u2014L\u2013is ...", "dateLastCrawled": "2022-02-02T14:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "machine learning - The <b>cross-entropy</b> error function in neural networks ...", "url": "https://datascience.stackexchange.com/questions/9302/the-cross-entropy-error-function-in-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/9302", "snippet": "Question 2. I&#39;ve learned that <b>cross-entropy</b> is defined as H y \u2032 ( y) := \u2212 \u2211 i ( y i \u2032 log. \u2061. ( y i) + ( 1 \u2212 y i \u2032) log. \u2061. ( 1 \u2212 y i)) This formulation is often used for a network with one output predicting two classes (usually positive class membership for 1 and negative for 0 output). In that case i may only have one value ...", "dateLastCrawled": "2022-01-30T02:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Is the <b>cross-entropy</b> cost function better than mean squared error cost ...", "url": "https://www.quora.com/Is-the-cross-entropy-cost-function-better-than-mean-squared-error-cost-function-Why", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Is-the-<b>cross-entropy</b>-cost-function-better-than-mean-squared...", "snippet": "Answer: TL;DR: I do not think you can put a strict preferance on either without specifying the intended use. Some background: * For a \u201csoft-max\u201d output, the cross ...", "dateLastCrawled": "2022-01-05T04:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Cross-Entropy</b> vs. Mean square error : MachineLearning", "url": "https://www.reddit.com/r/MachineLearning/comments/3ne2p7/crossentropy_vs_mean_square_error/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/MachineLearning/comments/3ne2p7/<b>crossentropy</b>_vs_mean_square_error", "snippet": "I feel <b>like</b> this question comes up a lot. Both loss functions have explicit probabilistic interpretations. Square loss corresponds to estimating the mean of (any!) distribution. <b>Cross-entropy</b> with softmax corresponds to maximizing the likelihood of a multinomial distribution. Intuitively, square loss is bad for classification because the model needs the targets to hit specific values (0/1) rather than having larger values correspond to higher probabilities. This makes it really hard for the ...", "dateLastCrawled": "2022-01-24T00:53:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "What Is <b>Cross-Entropy</b> Loss? | 365 Data Science", "url": "https://365datascience.com/tutorials/machine-learning-tutorials/cross-entropy-loss/", "isFamilyFriendly": true, "displayUrl": "https://365datascience.com/tutorials/machine-learning-tutorials/<b>cross-entropy</b>-loss", "snippet": "It\u2019s no <b>surprise</b> that <b>cross-entropy</b> loss is the most popular function used in machine learning or deep learning classification. After all, it helps determine the accuracy of our model in numerical values \u2013 0s and 1s, which we can later extract the probability percentage from. There are, of course, other loss functions that can help us resolve a problem. We must emphasize that any function that holds the basic property of being higher for worse results and lower for better results can be ...", "dateLastCrawled": "2022-02-02T15:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A Gentle Introduction to <b>Cross-Entropy for Machine Learning</b>", "url": "https://machinelearningmastery.com/cross-entropy-for-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>cross-entropy-for-machine-learning</b>", "snippet": "Last Updated on December 22, 2020. <b>Cross-entropy</b> is commonly used in machine learning as a loss function. <b>Cross-entropy</b> is a measure from the field of information theory, building upon entropy and generally calculating the difference between two probability distributions. It is closely related to but is different from KL divergence that calculates the relative entropy between two probability distributions, whereas <b>cross-entropy</b> can be thought to calculate the total entropy between the ...", "dateLastCrawled": "2022-02-03T03:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Comparing <b>Cross Entropy</b> and KL Divergence Loss \u2013 Tertiary Infotech", "url": "https://www.tertiaryinfotech.com/comparing-cross-entropy-and-kl-divergence-loss/", "isFamilyFriendly": true, "displayUrl": "https://www.tertiaryinfotech.com/comparing-<b>cross-entropy</b>-and-kl-divergence-loss", "snippet": "A skewed probability distribution has less \u201c<b>surprise</b>\u201d and in turn a low entropy because likely events dominate. Balanced distribution are more surprising and turn have higher entropy because events are equally likely. Entropy H(x) can be calculated for a random variable with a set of x in X discrete states discrete states and their probability P(x) as follows: $$ H(x) = -\\sum_x p(x)log(p(x))$$ <b>Cross-entropy</b> builds upon the idea of entropy from information theory and calculates the number ...", "dateLastCrawled": "2022-01-29T14:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Cross Entropy</b> Help", "url": "https://assignu.com/programming/cross-entropy-help/", "isFamilyFriendly": true, "displayUrl": "https://assignu.com/programming/<b>cross-entropy</b>-help", "snippet": "<b>Cross entropy</b> for machine learning is one of the most important subjects in recent times. It is a part of information technology and is mainly used to calculate the difference between two probability distributions. KL divergence has several similarities with <b>cross entropy</b>, still, both are not the same.", "dateLastCrawled": "2021-12-08T12:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "What is the entity of <b>cross entropy</b> (loss) - Data Science Stack Exchange", "url": "https://datascience.stackexchange.com/questions/96948/what-is-the-entity-of-cross-entropy-loss", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/.../96948/what-is-the-entity-of-<b>cross-entropy</b>-loss", "snippet": "<b>Cross-entropy</b> (loss), $-\\sum y_i\\;\\log(\\hat ... It describes the level of &quot;interest&quot; or &quot;<b>surprise</b>&quot; of information and it is also based on log 2. But I don&#39;t know if it is applicable to <b>cross entropy</b>. Share. Improve this answer. Follow answered Jun 22 &#39;21 at 10:26. Nicolas Martin Nicolas Martin. 915 1 1 silver badge 10 10 bronze badges $\\endgroup$ 1. 1 $\\begingroup$ So they are natural units or nats. Thanks. $\\endgroup$ \u2013 Herbert. Jun 22 &#39;21 at 14:09. Add a comment | Your Answer Thanks for ...", "dateLastCrawled": "2022-01-22T07:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Demystifying Entropy (And More</b>) - Jake Tae", "url": "https://jaketae.github.io/study/information-entropy/", "isFamilyFriendly": true, "displayUrl": "https://jaketae.github.io/study/information-entropy", "snippet": "This is no <b>surprise</b>, ... The two distributions are quite <b>similar</b>, meaning that our neural network did a good job of classifying given data. However, we can get a bit more scientific by calculating the <b>cross entropy</b> to see exactly how well our model performed with the given data. To achieve this, let\u2019s quickly write some functions to calculate KL divergence and <b>cross entropy</b>. We will be reusing the entropy function we defined above. On a trivial note, we prevent Python from running into ...", "dateLastCrawled": "2022-01-02T05:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "What is <b>information</b> theory? What does entropy measure? Mutual <b>information</b>?", "url": "https://charlesfrye.github.io/stats/2016/03/29/info-theory-surprise-entropy.html", "isFamilyFriendly": true, "displayUrl": "https://charlesfrye.github.io/stats/2016/03/29/info-theory-<b>surprise</b>-entropy.html", "snippet": "This average <b>surprise</b> does have a name in more traditional approaches: it is the <b>cross-entropy</b> of \\(Q\\) on \\(P\\). You might see it written: $$ H(P,Q) = -\\sum_x p(x) \\log(q(x)) $$ From the traditional, Shannon perspective, the interpretation of this quantity is that it is the length of encoded messages using a code optimized for a distribution \\(Q\\) on messages drawn from a distribution \\(P\\) (can you see why I prefer my approach?). It is used as a cost function to train parametrized models ...", "dateLastCrawled": "2021-12-12T02:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Visual Information Theory</b> -- colah&#39;s blog", "url": "http://colah.github.io/posts/2015-09-Visual-Information/", "isFamilyFriendly": true, "displayUrl": "colah.github.io/posts/2015-09-Visual-Information", "snippet": "<b>Cross-Entropy</b>. Shortly before his move to Australia, Bob married Alice, another figment of my imagination. To the <b>surprise</b> of myself, and also the other characters in my head, Alice was not a dog lover. She was a cat lover. Despite this, the two of them were able to find common ground in their shared obsession with animals and very limited vocabulary size. The two of them say the same words, just at different frequencies. Bob talks about dogs all the time, Alice talks about cats all the time", "dateLastCrawled": "2022-02-01T11:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Is the <b>cross-entropy</b> cost function better than mean squared error cost ...", "url": "https://www.quora.com/Is-the-cross-entropy-cost-function-better-than-mean-squared-error-cost-function-Why", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Is-the-<b>cross-entropy</b>-cost-function-better-than-mean-squared...", "snippet": "Answer: TL;DR: I do not think you can put a strict preferance on either without specifying the intended use. Some background: * For a \u201csoft-max\u201d output, the cross ...", "dateLastCrawled": "2022-01-05T04:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>KL Divergence</b> | Shangeth", "url": "https://shangeth.com/post/kl-divergence/", "isFamilyFriendly": true, "displayUrl": "https://shangeth.com/post/<b>kl-divergence</b>", "snippet": "\u201cIf an event is very probable, it is no <b>surprise</b> (and generally uninteresting) when that event happens as expected. However, if an event is unlikely to occur, it has much more information to learn that the event happened or will happen. For instance, the knowledge that some particular number will not be the winning number of a lottery provides very little information, because any particular chosen number will almost certainly not win. However, the knowledge that a particular number will ...", "dateLastCrawled": "2021-12-28T11:52:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A Gentle Introduction to <b>Cross-Entropy for Machine Learning</b>", "url": "https://machinelearningmastery.com/cross-entropy-for-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>cross-entropy-for-machine-learning</b>", "snippet": "<b>Cross-entropy</b> is commonly used in machine learning as a loss function. <b>Cross-entropy</b> is a measure from the field of information theory, building upon entropy and generally calculating the difference between two probability distributions. It is closely related to but is different from KL divergence that calculates the relative entropy between two probability distributions, whereas <b>cross-entropy</b> <b>can</b> <b>be thought</b> to calculate the", "dateLastCrawled": "2022-02-03T03:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Joint Entropy and Conditional Entropy Mutual Information", "url": "https://www.site.uottawa.ca/~diana/csi5180/L5.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.site.uottawa.ca/~diana/csi5180/L5.pdf", "snippet": "<b>Cross-Entropy</b> \u2022 Entropy <b>can</b> <b>be thought</b> of as a matter of how surprised we will be to see the next word given previous words we already saw. \u2022 The <b>cross entropy</b> between a random variable X with true probability distribution p(x) and another pmf q (normally a model of p) is given by: H(X,q)=H(X)+D(p||q). \u2022 <b>Cross-entropy</b> <b>can</b> help us find out what our average <b>surprise</b> for the next word is. 8 The Entropy of English \u2022 We <b>can</b> model English using n-gram models (also known a Markov chains ...", "dateLastCrawled": "2021-08-27T08:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Test Validate, Train,", "url": "http://comp6248.ecs.soton.ac.uk/handouts/mlreview-handouts.pdf", "isFamilyFriendly": true, "displayUrl": "comp6248.ecs.soton.ac.uk/handouts/mlreview-handouts.pdf", "snippet": "The <b>cross-entropy</b> <b>can</b> <b>be thought</b> of as a measure of <b>surprise</b>. Given some input x i, we <b>can</b> think of ^y i as the estimated probability that x i belongs to class 1, and 1 y^ i is the estimated probability that it belongs to class 0. Note the extreme case of in nite <b>cross-entropy</b>, if your model believes that a class has 0 probability of occurrence, and yet the class appears in the data, the \u2018<b>surprise</b>\u2019 of your model will be in nitely great. Jonathon Hare COMP6248 Deep Learning 20 / 28 ...", "dateLastCrawled": "2022-01-11T00:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Introducing <b>Cross-Entropy for Machine Learning</b> - BLOCKGENI", "url": "https://blockgeni.com/introducing-cross-entropy-for-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://blockgeni.com/introducing-<b>cross-entropy-for-machine-learning</b>", "snippet": "<b>Cross-entropy</b> is commonly used in machine learning as a loss function. <b>Cross-entropy</b> is a measure from the field of information theory, building upon entropy and generally calculating the difference between two probability distributions. It is closely related to but is different from KL divergence that calculates the relative entropy between two probability distributions, whereas <b>cross-entropy</b> <b>can</b> <b>be thought</b> to calculate the total entropy between the distributions. <b>Cross-entropy</b> is also ...", "dateLastCrawled": "2022-01-27T13:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "How to interpret increase in both loss and <b>accuracy</b> - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/40910857/how-to-interpret-increase-in-both-loss-and-accuracy", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/40910857", "snippet": "I will assume you are talking about <b>cross-entropy</b> loss, which <b>can</b> <b>be thought</b> of as a measure of &#39;<b>surprise</b>&#39;. Loss and <b>accuracy</b> increasing/decreasing simultaneously on the training data tells you nothing about whether your model is overfitting. This <b>can</b> only be determined by comparing loss/<b>accuracy</b> on the validation vs. training data. If loss and <b>accuracy</b> are both decreasing, it means your model is becoming more confident on its correct predictions, or less confident on its incorrect ...", "dateLastCrawled": "2022-01-28T22:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Cross-Entropy Loss</b> and Its Applications in Deep Learning - neptune.ai", "url": "https://neptune.ai/blog/cross-entropy-loss-and-its-applications-in-deep-learning", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/<b>cross-entropy-loss</b>-and-its-applications-in-deep-learning", "snippet": "<b>Cross-entropy loss</b> is the sum of the negative logarithm of predicted probabilities of each student. Model A\u2019s <b>cross-entropy loss</b> is 2.073; model B\u2019s is 0.505. <b>Cross-Entropy</b> gives a good measure of how effective each model is. Binary <b>cross-entropy</b> (BCE) formula. In our four student prediction \u2013 model B:", "dateLastCrawled": "2022-02-02T02:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A Primer on Information Theory", "url": "https://pkhungurn.github.io/notes/notes/math/info-theory-primer/info-theory-primer.pdf", "isFamilyFriendly": true, "displayUrl": "https://pkhungurn.github.io/notes/notes/math/info-theory-primer/info-theory-primer.pdf", "snippet": "Information <b>can</b> <b>be thought</b> of as the amount of \\<b>surprise</b>&quot; in the fact that E occcured. The expression log(1=P(E)) <b>can</b> be motivated by searching for a function that satis es a number of criteria [3]. Say, let S(p) be a function that measures the amount of <b>surprise</b> associated with observing an event that occurs with probability p. The following are reasonable criteria to impose on S: { S(1) = 0. (Observing a certain event is no <b>surprise</b>.) { If p &lt; q then S(p) &gt; S(q). (Rarer events are more ...", "dateLastCrawled": "2021-09-22T08:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "[D] A Short Introduction to Entropy, <b>Cross-Entropy</b> and KL-Divergence ...", "url": "https://www.reddit.com/r/MachineLearning/comments/7vhmp7/d_a_short_introduction_to_entropy_crossentropy/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/.../comments/7vhmp7/d_a_short_introduction_to_entropy_<b>crossentropy</b>", "snippet": "I <b>thought</b> this was a really nice video, but I&#39;m curious why people motivate entropy by talking about encoding messages. Personally, I think it&#39;s easier to think of entropy in terms of &quot;<b>surprise</b>&quot;: given some event E whose probability is p, one way to encode how surprising its realization would be is as log 1/p. (The intuition is that if p = 1 then the <b>surprise</b> is zero, and that the <b>surprise</b> of two independent events is the sum of their individual surprises.) Given some distribution p, its ...", "dateLastCrawled": "2021-08-20T08:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "What is <b>information</b> theory? What does entropy measure? Mutual <b>information</b>?", "url": "https://charlesfrye.github.io/stats/2016/03/29/info-theory-surprise-entropy.html", "isFamilyFriendly": true, "displayUrl": "https://charlesfrye.github.io/stats/2016/03/29/info-theory-<b>surprise</b>-entropy.html", "snippet": "This form of the average <b>surprise</b> (you might call it expected <b>surprise</b>, since we\u2019re working with a probability distribution rather than an empirical estimate, so the averaging operation is known as \u201cexpectation\u201d), has the advantage of providing exact answers, but the disadvantage of requiring an analytical form for, aka a model of, \\(p(x)\\), which <b>can</b> be hard to come by.", "dateLastCrawled": "2021-12-12T02:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Information theory - Yann Dubois", "url": "https://yanndubs.github.io/machine-learning-glossary/information", "isFamilyFriendly": true, "displayUrl": "https://yanndubs.github.io/machine-learning-glossary/information", "snippet": "A book with random letters will have more information content because each new letter would be a <b>surprise</b> to you. But it will definitely not have more meaning than a book with English words . Entropy Long Story Short \\[H(X) = H(p) \\equiv \\mathbb{E}\\left[\\operatorname{I} (p_i)\\right] = \\sum_{i=1}^K p_i \\ \\log(\\frac{1}{p_i}) = - \\sum_{i=1}^K p_i\\ log(p_i)\\] Intuition : The entropy of a random variable is the expected information-content. I.e. the expected amount of <b>surprise</b> you would have by ...", "dateLastCrawled": "2021-11-26T20:11:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Comparing <b>Cross Entropy</b> and KL Divergence Loss \u2013 Tertiary Infotech", "url": "https://www.tertiaryinfotech.com/comparing-cross-entropy-and-kl-divergence-loss/", "isFamilyFriendly": true, "displayUrl": "https://www.tertiaryinfotech.com/comparing-<b>cross-entropy</b>-and-kl-divergence-loss", "snippet": "A skewed probability distribution has less \u201c<b>surprise</b>\u201d and in turn a low entropy because likely events dominate. Balanced distribution are more surprising and turn have higher entropy because events are equally likely. Entropy H(x) <b>can</b> be calculated for a random variable with a set of x in X discrete states discrete states and their probability P(x) as follows: $$ H(x) = -\\sum_x p(x)log(p(x))$$ <b>Cross-entropy</b> builds upon the idea of entropy from information theory and calculates the number ...", "dateLastCrawled": "2022-01-29T14:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A Gentle Introduction to <b>Cross-Entropy for Machine Learning</b>", "url": "https://machinelearningmastery.com/cross-entropy-for-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>cross-entropy-for-machine-learning</b>", "snippet": "Last Updated on December 22, 2020. <b>Cross-entropy</b> is commonly used in machine learning as a loss function. <b>Cross-entropy</b> is a measure from the field of information theory, building upon entropy and generally calculating the difference between two probability distributions. It is closely related to but is different from KL divergence that calculates the relative entropy between two probability distributions, whereas <b>cross-entropy</b> <b>can</b> be thought to calculate the total entropy between the ...", "dateLastCrawled": "2022-02-03T03:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Cross Entropy</b> Help", "url": "https://assignu.com/programming/cross-entropy-help/", "isFamilyFriendly": true, "displayUrl": "https://assignu.com/programming/<b>cross-entropy</b>-help", "snippet": "As per the information theory, there is a concept of <b>surprise</b>. Any low probability event is termed as surprising which contains more information. On the other side, a higher probability event is an unsurprising one that contains less information. If you go by the formula, then it is h(x) = -log(P(x)) where information h(x) <b>can</b> be calculated for an event x and the probability of the event has been termed as P(x) Entropy is the number of bits required for transmitting a randomly selected event ...", "dateLastCrawled": "2021-12-08T12:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Cross Entropy</b> \u2013 Towards Data Science", "url": "https://towardsdatascience.com/tagged/cross-entropy", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/tagged/<b>cross-entropy</b>", "snippet": "all flows from -log(p), the <b>surprise</b>. Hi there! For any beginners in machine learning &amp; data science, it is essential that the notion of Entropy and <b>Cross Entropy</b> is clear to you. \u2026 Read more \u00b7 4 min read. 37. Do Lee \u00b7 Jun 15, 2021. Understand &amp; Implement Logistic Regression in Python. Sigmoid Function, Linear Regression, and Parameter Estimation (Log-Likelihood &amp; <b>Cross-Entropy</b> Loss) Photo by Meg Boulden on Unsplash Objective. The primary objective of this article is to understand how ...", "dateLastCrawled": "2022-01-18T09:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "2 From entropy to maximum likelihood | Statistical Methods: Likelihood ...", "url": "https://strimmerlab.github.io/publications/lecture-notes/MATH20802/from-entropy-to-maximum-likelihood.html", "isFamilyFriendly": true, "displayUrl": "https://strimmerlab.github.io/publications/lecture-notes/MATH20802/from-entropy-to...", "snippet": "2.1.2 <b>Surprise</b>, surprisal or Shannon information. The <b>surprise</b> to observe an event of probability \\(p\\) is defined as \\(-\\log(p)\\).This is also called surprisal or Shannon information.. Thus, the <b>surprise</b> to observe a certain event (with \\(p=1\\)) is zero, and conversely the <b>surprise</b> to observe an event that is certain not to happen (with \\(p=0\\)) is infinite.. The log-odds ratio <b>can</b> be viewed as the difference of the <b>surprise</b> of an event and the <b>surprise</b> of the complementary event: \\[ \\log ...", "dateLastCrawled": "2022-01-29T08:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "What is <b>information</b> theory? What does entropy measure? Mutual <b>information</b>?", "url": "https://charlesfrye.github.io/stats/2016/03/29/info-theory-surprise-entropy.html", "isFamilyFriendly": true, "displayUrl": "https://charlesfrye.github.io/stats/2016/03/29/info-theory-<b>surprise</b>-entropy.html", "snippet": "<b>Compared</b> with the Truth. Above we took an average over repeated experiments, implicitly assuming that there is a probability distribution over results \u2013 that is, after all, the whole motivation for considering probabilistic claims in the first place! Let\u2019s call that distribution \\(p(x)\\). This is, in a very real sense, \u201cNature\u2019s probability distribution\u201d. If that idea sounds strange or unbelievable to you, check out the Aside on Repetition. With this idea and its new symbol, we <b>can</b> ...", "dateLastCrawled": "2021-12-12T02:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "How meaningful is the connection between MLE and <b>cross entropy</b> in deep ...", "url": "https://stats.stackexchange.com/questions/297749/how-meaningful-is-the-connection-between-mle-and-cross-entropy-in-deep-learning", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/297749", "snippet": "<b>Cross entropy</b> loss <b>can</b> also be applied more generally. For example, in &#39;soft classification&#39; problems, we&#39;re given distributions over class labels rather than hard class labels (so we don&#39;t use the empirical distribution). I describe how to use <b>cross entropy</b> loss in that case here. To address some other specifics in your question: Different training and prediction probabilities. It looks like you&#39;re finding the output unit with maximum activation and comparing this to the class label. This ...", "dateLastCrawled": "2022-01-25T23:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Information theory - Yann Dubois", "url": "https://yanndubs.github.io/machine-learning-glossary/information", "isFamilyFriendly": true, "displayUrl": "https://yanndubs.github.io/machine-learning-glossary/information", "snippet": "A book with random letters will have more information content because each new letter would be a <b>surprise</b> to you. But it will definitely not have more meaning than a book with English words . Entropy Long Story Short \\[H(X) = H(p) \\equiv \\mathbb{E}\\left[\\operatorname{I} (p_i)\\right] = \\sum_{i=1}^K p_i \\ \\log(\\frac{1}{p_i}) = - \\sum_{i=1}^K p_i\\ log(p_i)\\] Intuition : The entropy of a random variable is the expected information-content. I.e. the expected amount of <b>surprise</b> you would have by ...", "dateLastCrawled": "2021-11-26T20:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Loss Function</b> - Pipline", "url": "https://nuoxu2016.gitbook.io/pipline/preprocessing-1/loss-function", "isFamilyFriendly": true, "displayUrl": "https://nuoxu2016.gitbook.io/pipline/preprocessing-1/<b>loss-function</b>", "snippet": "A skewed probability distribution has less \u201c<b>surprise</b>\u201d and in turn a low entropy because likely events dominate. Balanced distribution are more surprising and turn have higher entropy because events are equally likely. Skewed Probability Distribution (unsurprising): Low entropy. Balanced Probability Distribution (surprising): High entropy. Entropy H(x) <b>can</b> be calculated for a random variable with a set of x in X discrete states discrete states and their probability P(x) as follows: H(X ...", "dateLastCrawled": "2022-01-24T22:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>An Introduction to Objective Functions Used</b> in Machine Learning", "url": "https://mlbhanuyerra.github.io/2019-03-07-Objective-Functions/", "isFamilyFriendly": true, "displayUrl": "https://mlbhanuyerra.github.io/2019-03-07-Objective-Functions", "snippet": "Self-information <b>can</b> be viewed as the degree of <b>surprise</b> or amount of information we learn from observing a random event and is based on the probability of that event. We learn less from a highly probable event <b>compared</b> to what we learn from observing a highly improbable event. This means self-information is a monotonically decreasing over probability space. And we learn twice as much of information from observing two such independent events as <b>compared</b> to just one. These requirements leads ...", "dateLastCrawled": "2022-01-29T17:15:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>machine</b> <b>learning</b> - <b>Cross-entropy loss</b> explanation - Data Science Stack ...", "url": "https://datascience.stackexchange.com/questions/20296/cross-entropy-loss-explanation", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/20296", "snippet": "The answer from Neil is correct. However I think its important to point out that while the loss does not depend on the distribution between the incorrect classes (only the distribution between the correct class and the rest), the gradient of this loss function does effect the incorrect classes differently depending on how wrong they are. So when you use cross-ent in <b>machine</b> <b>learning</b> you will change weights differently for [0.1 0.5 0.1 0.1 0.2] and [0.1 0.6 0.1 0.1 0.1].", "dateLastCrawled": "2022-01-27T03:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Cross-Entropy</b> Demystified. What is it? Is there any relation to\u2026 | by ...", "url": "https://naokishibuya.medium.com/demystifying-cross-entropy-e80e3ad54a8", "isFamilyFriendly": true, "displayUrl": "https://naokishibuya.medium.com/demystifying-<b>cross-entropy</b>-e80e3ad54a8", "snippet": "However, the <b>machine</b> <b>learning</b> application uses the base e logarithm for implementation convenience. Binary <b>Cross-Entropy</b>. We can use the binary <b>cross-entropy</b> for binary classification where we have yes/no answer. For example, there are only dogs or cats in images. For the binary classifications, the <b>cross-entropy</b> formula contains only two ...", "dateLastCrawled": "2022-01-26T13:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Why and How to use <b>Cross Entropy</b>. The fundamental reasons for ...", "url": "https://towardsdatascience.com/why-and-how-to-use-cross-entropy-4e983cbdd873", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/why-and-how-to-use-<b>cross-entropy</b>-4e983cbdd873", "snippet": "The fundamental reasons for minimizing binary <b>cross entropy</b> (log loss) with probabilistic classification models . Will Arliss. Sep 26, 2020 \u00b7 7 min read. Introduction. This post discusses why logistic regression necessarily uses a different loss function than linear regression. First, the simple yet inefficient way to solve logistic regression will be presented, then the slightly less simple but much more efficient way will be explained and compared. The simple way. Linear regression is the ...", "dateLastCrawled": "2022-01-31T14:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Shannon <b>entropy</b> in the context of <b>machine</b> <b>learning</b> and AI | by Frank ...", "url": "https://medium.com/swlh/shannon-entropy-in-the-context-of-machine-learning-and-ai-24aee2709e32", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/shannon-<b>entropy</b>-in-the-context-of-<b>machine</b>-<b>learning</b>-and-ai-24...", "snippet": "Closely related to <b>cross entropy</b>, the KL divergence from q to p, written DKL(p||q), is another similarity measure often used in <b>machine</b> <b>learning</b>. In the language of Bayesian Inference, DKL(p||q ...", "dateLastCrawled": "2022-01-30T12:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine</b> <b>Learning</b> Concepts for Revision | by Raunak Sarada | Medium", "url": "https://raunaksarada-cse21.medium.com/machine-learning-concepts-for-revision-491384952d27", "isFamilyFriendly": true, "displayUrl": "https://raunaksarada-cse21.medium.com/<b>machine</b>-<b>learning</b>-concepts-for-revision-491384952d27", "snippet": "ML Concepts. A.I \u2014 Intelligence showed by machines which is common for humans <b>Machine</b> <b>Learning</b>- Recognize the pattern in data and automatically learn and improve through experience without explicitly being programmed Deep <b>Learning</b>- branch of <b>machine</b> <b>learning</b>.We have to deal with lots of data so in that case problems can\u2019t be solved with simple ML algorithms.", "dateLastCrawled": "2022-01-25T20:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Main concepts behind Machine Learning</b> | by Bruno Eidi Nishimoto ...", "url": "https://medium.com/neuronio/main-concepts-behind-machine-learning-22cd81d68a11", "isFamilyFriendly": true, "displayUrl": "https://medium.com/neuronio/<b>main-concepts-behind-machine-learning</b>-22cd81d68a11", "snippet": "<b>Machine</b> <b>Learning</b> is a concept that is currently trending. It is a subarea from Artificial Intelligence and it consists on the fact that the <b>machine</b> can learn by itself without being explicitly ...", "dateLastCrawled": "2022-01-19T01:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A Gentle Introduction to Information Entropy - <b>Machine</b> <b>Learning</b> Mastery", "url": "https://machinelearningmastery.com/what-is-information-entropy/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/what-is-information-entropy", "snippet": "Calculating information and entropy is a useful tool in <b>machine</b> <b>learning</b> and is used as the basis for techniques such as feature selection, building decision trees, and, more generally, fitting classification models. As such, a <b>machine</b> <b>learning</b> practitioner requires a strong understanding and intuition for information and entropy. In this post, you will discover a gentle introduction to information entropy. After reading this post, you will know: Information theory is concerned with data ...", "dateLastCrawled": "2022-02-02T13:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Deep Learning and Information Theory</b> \u2013 Deep &amp; Shallow", "url": "https://deep-and-shallow.com/2020/01/09/deep-learning-and-information-theory/", "isFamilyFriendly": true, "displayUrl": "https://deep-and-shallow.com/2020/01/09/<b>deep-learning-and-information-theory</b>", "snippet": "If you have tried to understand the maths behind <b>machine</b> <b>learning</b>, including deep <b>learning</b>, you would have come across topics from Information Theory \u2013 Entropy, <b>Cross Entropy</b>, KL Divergence, etc. The concepts from information theory is ever prevalent in the realm of <b>machine</b> <b>learning</b>, right from the splitting criteria of a Decision Tree to loss functions in Generative Adversarial Networks.", "dateLastCrawled": "2022-02-01T00:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "[D] A Short Introduction to Entropy, <b>Cross-Entropy</b> and KL-Divergence ...", "url": "https://www.reddit.com/r/MachineLearning/comments/7vhmp7/d_a_short_introduction_to_entropy_crossentropy/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/<b>MachineLearning</b>/comments/7vhmp7/d_a_short_introduction_to...", "snippet": "I am having trouble reconciling the concept with the <b>analogy</b>. At 2:35 even if a rainy day was 25% likely, there&#39;s still only two states, rainy and sunny, and therefor only 1 bit of information is needed to convey that, so only one bit of data needs to be sent, even though the 1 bit of data reduces the uncertainty of a rainy day by a factor of 4. I quite don&#39;t get what he means by this being 2 bits of information. I guess where I am stuck is how the uncertainty reduction factor translates to ...", "dateLastCrawled": "2021-08-20T08:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Lecture 4 Fundamentals of deep <b>learning</b> and neural networks", "url": "https://web.stanford.edu/class/biods388/downloads/BIODS388_Lecture_4.pdf", "isFamilyFriendly": true, "displayUrl": "https://web.stanford.edu/class/biods388/downloads/BIODS388_Lecture_4.pdf", "snippet": "Deep <b>learning</b>: <b>Machine</b> <b>learning</b> models based on \u201cdeep\u201d neural networks comprising millions (sometimes billions) of parameters organized into hierarchical layers. Features are multiplied and added together repeatedly, with the outputs from one layer of parameters being fed into the next layer -- before a prediction is made. Contrast with linear regression: Agenda for today - More on the structure of neural network models - <b>Machine</b> <b>learning</b> training loop and concept of loss, in the context ...", "dateLastCrawled": "2022-02-02T09:07:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Beat the Bookmakers With Tree-Based <b>Machine</b> <b>Learning</b> Algorithms | by ...", "url": "https://medium.com/analytics-vidhya/beat-the-bookmakers-with-tree-based-machine-learning-algorithms-1d349335b54", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/beat-the-bookmakers-with-tree-based-<b>machine</b>...", "snippet": "<b>Cross-entropy is similar</b> to Gini Impurity, but it involves using the concept of entropy from information theory. This article won\u2019t go in depth about it, but essentially, as the cross-entropy ...", "dateLastCrawled": "2022-01-26T04:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>A Traveler\u2019s Diary on the Road to Machine</b> <b>Learning</b> - Chapter 1 | by ...", "url": "https://medium.com/swlh/a-travelers-diary-on-the-road-to-machine-learning-chapter-1-8850ec5b4243", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/<b>a-travelers-diary-on-the-road-to-machine</b>-<b>learning</b>-chapter-1...", "snippet": "Types of <b>Machine</b> <b>Learning</b> algorithms: ... Sparse categorical <b>cross entropy is similar</b> to categorical cross entropy, only difference is it uses only one value as target. It saves memory as well as ...", "dateLastCrawled": "2021-05-21T04:27:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Deep Learning for Computer Architects</b> | Chen Jeff - Academia.edu", "url": "https://www.academia.edu/40860009/Deep_Learning_for_Computer_Architects", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/40860009/<b>Deep_Learning_for_Computer_Architects</b>", "snippet": "This text serves as a primer for computer architects in a new and rapidly evolving \ufb01eld. We review how <b>machine</b> <b>learning</b> has evolved since its inception in the 1960s and track the key developments leading up to the emergence of the powerful deep <b>learning</b> techniques that emerged in the last decade.", "dateLastCrawled": "2022-01-28T02:18:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(cross-entropy)  is like +(surprise)", "+(cross-entropy) is similar to +(surprise)", "+(cross-entropy) can be thought of as +(surprise)", "+(cross-entropy) can be compared to +(surprise)", "machine learning +(cross-entropy AND analogy)", "machine learning +(\"cross-entropy is like\")", "machine learning +(\"cross-entropy is similar\")", "machine learning +(\"just as cross-entropy\")", "machine learning +(\"cross-entropy can be thought of as\")", "machine learning +(\"cross-entropy can be compared to\")"]}