{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Deep Learning: An Introduction from the NLP Perspective", "url": "https://www.cs.jhu.edu/~kevinduh/notes/duh12deeplearn.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.jhu.edu/~kevinduh/notes/duh12deeplearn.pdf", "snippet": "Dictionary learning and <b>sparse</b> reconstruction methods 2 Is multiple-level of representations really necessary in NLP? For Vision problems, there is clear <b>analogy</b> to the brain\u2019s structure, but for language? Maybe: compositionally and recursion in natural language. 3 Black magic required for e ective training, e.g.", "dateLastCrawled": "2022-01-07T22:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Analogy</b> and Abstraction - Wiley Online <b>Library</b>", "url": "https://onlinelibrary.wiley.com/doi/10.1111/tops.12278", "isFamilyFriendly": true, "displayUrl": "https://online<b>library</b>.wiley.com/doi/10.1111/tops.12278", "snippet": "In this <b>analogy</b>, knowledge about a power supply is projected to the mitochondria. But <b>analogy</b> can also operate in mutual alignment1 1 Mutual alignment analogies are those that are processed by structural alignment with few or no projected candidate inferences, According to structure-mapping, all analogies involve structural alignment. Even when the base offers a clear projection (as in the goldmine example), one still must align that structure with the target&#39;s <b>representation</b> in order to ...", "dateLastCrawled": "2022-01-26T20:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Sparse</b> Additive Models (SPAM)", "url": "https://www.slideshare.net/ssuserd37bda/sparse-additive-models-spam", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ssuserd37bda/<b>sparse</b>-additive-models-spam", "snippet": "<b>Sparse</b> Backfitting [1] Lou, Yin, et al. &quot;<b>Sparse</b> partially linear additive models.&quot; Journal of Computational and Graphical Statistics 25.4 (2016): 1126-1140. GAMSEL:: 1 VS SpAM: vs Lasso: [2] Hastie, Trevor J. &quot;Generalized additive models.&quot; Statistical models in S. Routledge, 2017. 249-307. Discussion points: Now, we understand that SpAM is the functional version of group lasso. Then SpAM is alway better than lasso or group lasso? (Hint : lasso is linear model, and SpAM is \u2026? ) When there ...", "dateLastCrawled": "2022-01-20T23:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Analogical mapping and inference with</b> binary spatter codes and <b>sparse</b> ...", "url": "https://www.researchgate.net/publication/261148322_Analogical_mapping_and_inference_with_binary_spatter_codes_and_sparse_distributed_memory", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/261148322_Analogical_mapping_and_inference...", "snippet": "<b>Representation</b> <b>Analogy</b> MApper (Drama) [10] is very close . to being a distributed re-implementation of ACME. Ho wever, although the source and target are represented with VSAs, the network which ...", "dateLastCrawled": "2021-11-09T13:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Metaphor and Analogy in Science</b> Education - Google Books", "url": "https://books.google.com/books/about/Metaphor_and_Analogy_in_Science_Educatio.html?id=xZW7o2i_RHgC", "isFamilyFriendly": true, "displayUrl": "https://books.google.com/books/about/<b>Metaphor_and_Analogy_in_Science</b>_Educatio.html?id=...", "snippet": "Years ago a primary teacher told me about a great series of lessons she had just had. The class had visited rock pools on the seashore, and when she asked them about their observations they talked about: it was <b>like</b> a factory, it was <b>like</b> a church, it was <b>like</b> a garden, it was <b>like</b> our kitchen at breakfast time, etc. Each student\u2019s <b>analogy</b> could be elaborated, and these analogies provided her with strongly engaged students and a great platform from which to develop their learning about ...", "dateLastCrawled": "2022-01-15T00:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>CUSPARSE</b> \u00b7 Julia Packages", "url": "https://www.juliapackages.com/p/cusparse", "isFamilyFriendly": true, "displayUrl": "https://www.juliapackages.com/p/<b>cusparse</b>", "snippet": "A is transformed into CSC format moved to the GPU, then auto-converted to CSR format for you. Thus, d_A is not a transpose of A!Similarly, if you have a matrix in dense format on the GPU (in a CudaArray), you can simply call <b>sparse</b> to turn it into a <b>sparse</b> <b>representation</b>. Right now <b>sparse</b> by default turns the matrix it is given into CSR format. It takes an optional argument that lets you select CSC or HYB:", "dateLastCrawled": "2022-02-02T02:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Graph Algorithms in the Language of Linear Algebra</b>", "url": "https://sites.cs.ucsb.edu/~gilbert/cs240a/slides/old/cs240a-GALA.pdf", "isFamilyFriendly": true, "displayUrl": "https://sites.cs.ucsb.edu/~gilbert/cs240a/slides/old/cs240a-GALA.pdf", "snippet": "\u2022 By <b>analogy</b> to numerical scientific computing. . . \u2022 What should the combinatorial BLAS look <b>like</b>? The challenge of the software stack C = A*B y = A*x \u00b5 = xT y Basic Linear Algebra Subroutines (BLAS): Ops/Sec vs. Matrix Size . 4 Outline \u2022 Motivation \u2022 <b>Sparse</b> matrices for graph algorithms \u2022 CombBLAS: <b>sparse</b> arrays and graphs on parallel machines \u2022 KDT: attributed semantic graphs in a high-level language \u2022 Standards for graph algorithm primitives . 5 Multiple-source breadth ...", "dateLastCrawled": "2022-01-31T12:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Representing text in natural language processing</b> | by Michel Kana, Ph.D ...", "url": "https://towardsdatascience.com/representing-text-in-natural-language-processing-1eead30e57d8", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>representing-text-in-natural-language-processing</b>-1eead...", "snippet": "A 2-gram (or bigram) is a two-word sequence of words <b>like</b> \u201cplease eat\u201d, \u201ceat your\u201d, or \u201dyour food\u201d. A 3-gram (or trigram) will be a three-word sequence of words <b>like</b> \u201cplease eat your\u201d, or \u201ceat your food\u201d. N-gram language models estimate the probability of the last word given the previous words. For example, given the sequence of words \u201cplease eat your\u201d, the likelihood of the next word is higher for \u201cfood\u201d than for \u201cspoon\u201d. In the later case, our mom will be ...", "dateLastCrawled": "2022-02-02T15:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Code <b>Compass: boosting software reuse through machine learning</b>", "url": "https://www.code-compass.com/blog/intro/", "isFamilyFriendly": true, "displayUrl": "https://www.code-compass.com/blog/intro", "snippet": "The answer is yes: just <b>like</b> word embeddings learn to represent similar words by similar dense vector representations based on the words\u2019 similar context of use, we can learn a dense vector <b>representation</b> of libraries based on their context of use. By <b>analogy</b> with the term \u201cword vectors\u201d, we call our embeddings \u201c<b>library</b> vectors\u201d. The figure below illustrates the key idea:", "dateLastCrawled": "2021-12-25T18:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Representation</b> - Open Computing Facility", "url": "https://www.ocf.berkeley.edu/~jfkihlstrom/MemoryWeb/storage/representation.htm", "isFamilyFriendly": true, "displayUrl": "https://www.ocf.berkeley.edu/~jfkihlstrom/MemoryWeb/storage/<b>representation</b>.htm", "snippet": "According to Bartlett, remembering is not <b>like</b> taking a book off the shelf and reading it, as the traditional <b>library</b> metaphor would have it. Rather, remembering is more <b>like</b> writing the book anew, based on fragmentary notes. The process of remembering, of reconstructing a memory, is guided throughout by an organized framework of world-knowledge and attitudes, within which the memory is reconstructed. This organized framework is the schema.", "dateLastCrawled": "2022-01-30T20:03:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Analogical mapping and inference with</b> binary spatter codes and <b>sparse</b> ...", "url": "https://www.researchgate.net/publication/261148322_Analogical_mapping_and_inference_with_binary_spatter_codes_and_sparse_distributed_memory", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/261148322_Analogical_mapping_and_inference...", "snippet": "<b>Representation</b> <b>Analogy</b> MApper (Drama) [10] is very close . to being a distributed re-implementation of ACME. Ho wever, although the source and target are represented with VSAs, the network which ...", "dateLastCrawled": "2021-11-09T13:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Distributed Representation</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/computer-science/distributed-representation", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/computer-science/<b>distributed-representation</b>", "snippet": "It turns out that such a <b>distributed representation</b> is <b>sparse</b>, because only a few of the neurons are active each time. This is in line with what we believe happens in the human brain, where at each time less than 5% of the neurons, in each layer, fire, and the rest remain inactive. In the antipodal of such distributive representations would be to have a single neuron firing each time. In the case of neural networks with more general (compared to 0 and 1) activation functions, the features ...", "dateLastCrawled": "2022-01-05T14:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "4. Distributed word representations \u2014 Natural Language Processing 0.0.1 ...", "url": "https://hhexiy.github.io/nlp/2021/notes/distributed_representation.html", "isFamilyFriendly": true, "displayUrl": "https://hhexiy.github.io/nlp/2021/notes/distributed_<b>representation</b>.html", "snippet": "Document <b>representation</b>\u00b6 Let\u2019s start with a familiar setting: we have a set of documents (e.g. movie reviews), now instead of classifying them, we would like to find out which ones are closer. From the last lecture, we already have a (<b>sparse</b>) vector <b>representation</b> for documents. Let\u2019s load the movie reviews.", "dateLastCrawled": "2022-01-29T13:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Assignment 2 <b>CSEP 517: Natural Language Processing</b>", "url": "https://courses.cs.washington.edu/courses/csep517/17sp/assignments/assignment2.pdf", "isFamilyFriendly": true, "displayUrl": "https://courses.cs.washington.edu/courses/csep517/17sp/assignments/assignment2.pdf", "snippet": "1.Decide on a <b>representation</b> of the distribution of contexts each word v 2V occurs in, as a vector. A common choice is a jVj-length vector containing the counts of all words occuring just before or just after (say, within two word positions) of some token of the word v.1 2.Compress these long, <b>sparse</b> vectors into smaller (roughly 100-dimensional) vectors, usually \u201cdense\u201d (few or no 0s) vector. While we don\u2019t have time to get into more details of standalone methods for constructing word ...", "dateLastCrawled": "2021-11-02T15:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Representing text in natural language processing</b> | by Michel Kana, Ph.D ...", "url": "https://towardsdatascience.com/representing-text-in-natural-language-processing-1eead30e57d8", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>representing-text-in-natural-language-processing</b>-1eead...", "snippet": "The process of transforming text into numeric stuff, <b>similar</b> to what we did with the image above, is usually performed by building a language model. These models typically assign probabilities, frequencies or some obscure numbers to words, sequences of words, group of words, section of documents or whole documents. The most common techniques are: 1-hot encoding, N-grams, Bag-of-words, vector semantics (tf-idf), distributional semantics (Word2vec, GloVe). Let\u2019s see if we understand what all ...", "dateLastCrawled": "2022-02-02T15:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>What Are Word Embeddings</b> for Text?", "url": "https://machinelearningmastery.com/what-are-word-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>what-are-word-embeddings</b>", "snippet": "Word embeddings are a type of word <b>representation</b> that allows words with <b>similar</b> meaning to have a <b>similar</b> <b>representation</b>. They are a distributed <b>representation</b> for text that is perhaps one of the key breakthroughs for the impressive performance of deep learning methods on challenging natural language processing problems. In this post, you will discover the word embedding approach for representing text data. After", "dateLastCrawled": "2022-01-30T18:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Analogy</b> and Abstraction - Gentner - 2017 - Wiley Online <b>Library</b>", "url": "https://onlinelibrary.wiley.com/doi/10.1111/tops.12278", "isFamilyFriendly": true, "displayUrl": "https://online<b>library</b>.wiley.com/doi/10.1111/tops.12278", "snippet": "In this <b>analogy</b>, knowledge about a power supply is projected to the mitochondria. But <b>analogy</b> can also operate in mutual alignment1 1 Mutual alignment analogies are those that are processed by structural alignment with few or no projected candidate inferences, According to structure-mapping, all analogies involve structural alignment. Even when the base offers a clear projection (as in the goldmine example), one still must align that structure with the target&#39;s <b>representation</b> in order to ...", "dateLastCrawled": "2022-01-26T20:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Analogy</b>-Based Inference Patterns in Pharmacological ... - <b>library</b>.oapen.org", "url": "https://library.oapen.org/bitstream/handle/20.500.12657/50322/Poellinger%202017%20Analogy-Based%20Inference%20Patterns%20in%20Pharmacological%20Research%20FINAL.pdf?sequence=1", "isFamilyFriendly": true, "displayUrl": "https://<b>library</b>.oapen.org/bitstream/handle/20.500.12657/50322/Poellinger 2017 <b>Analogy</b>...", "snippet": "<b>Analogy</b>-Based Inference Patterns in Pharmacological Research Roland Poellinger Abstract Analogical arguments are ubiquitous vehicles of knowledge transfer in science and medicine. This paper outlines a Bayesian evidence-amalgamation framework for the purpose of formally exploring different <b>analogy</b>-based infer-ence patterns with respect to their justi\ufb01cation in pharmacological risk assessment. By relating formal explications of similarity, <b>analogy</b>, and analog simulation, three sources of ...", "dateLastCrawled": "2022-01-26T16:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Code <b>Compass: boosting software reuse through machine learning</b>", "url": "https://www.code-compass.com/blog/intro/", "isFamilyFriendly": true, "displayUrl": "https://www.code-compass.com/blog/intro", "snippet": "The answer is yes: just like word embeddings learn to represent <b>similar</b> words by <b>similar</b> dense vector representations based on the words\u2019 <b>similar</b> context of use, we can learn a dense vector <b>representation</b> of libraries based on their context of use. By <b>analogy</b> with the term \u201cword vectors\u201d, we call our embeddings \u201c<b>library</b> vectors\u201d. The figure below illustrates the key idea:", "dateLastCrawled": "2021-12-25T18:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>How does Word2vec solve sparse problem</b>? - Quora", "url": "https://www.quora.com/How-does-Word2vec-solve-sparse-problem", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>How-does-Word2vec-solve-sparse-problem</b>", "snippet": "Answer: Word2vec solves the sparsity problem of models that use word identities as a feature by projecting words to a low dimensional space. You are right that in a given textual collection, words might occur only with limited context. So the word vectors that you learn will be pretty weak - but ...", "dateLastCrawled": "2022-01-21T04:35:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Digital Signal Processing</b> - <b>library</b>.utia.cas.cz", "url": "http://library.utia.cas.cz/separaty/2016/ZOI/sorel-0459332.pdf", "isFamilyFriendly": true, "displayUrl": "<b>library</b>.utia.cas.cz/separaty/2016/ZOI/sorel-0459332.pdf", "snippet": "<b>can</b> <b>be thought</b> of as a tool for feature extraction. In signal recon-struction <b>sparse</b> coding <b>can</b> serve as a form of Bayesian prior for image denoising [3], inpainting [4], deblurring [5], super-resolution [6] and audio signal <b>representation</b> [7]. Although \ufb01nding the dic- tionary with which the training signals <b>can</b> be represented with optimal sparsity is strongly NP-hard [8], there is a number of ef-fective heuristic algorithms giving an approximate solution in poly-nomial time [9,10]. <b>Sparse</b> ...", "dateLastCrawled": "2022-01-24T22:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Sparse</b> <b>Representation</b> of Whole-brain FMRI Signals for Identification of ...", "url": "https://www.researchgate.net/publication/267983526_Sparse_Representation_of_Whole-brain_FMRI_Signals_for_Identification_of_Functional_Networks", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/267983526_<b>Sparse</b>_<b>Representation</b>_of_Whole...", "snippet": "These prior contributions inspired the exploration of whether/how <b>sparse</b> <b>representation</b> <b>can</b> be used to identify functional networks in a voxel-wise way and on the whole brain scale. This paper ...", "dateLastCrawled": "2021-12-28T17:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Homework 5 - Advanced Vector Space Models", "url": "https://computational-linguistics-class.org/homework/vector-semantics-2/vector-semantics-2.html", "isFamilyFriendly": true, "displayUrl": "https://computational-linguistics-class.org/homework/vector-semantics-2/vector...", "snippet": "Write a function cluster_with_<b>sparse</b>_<b>representation</b>(word_to_paraphrases_dict, word_to_k_dict). The input and output remains the same as in Task 1, however the clustering of paraphrases will no longer be random and is based on <b>sparse</b> vector <b>representation</b>. We will feature-based (not dense) vector space <b>representation</b>.", "dateLastCrawled": "2022-01-26T19:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "SPORCO: A Python package for standard and convolutional <b>sparse</b> ...", "url": "https://nuit-blanche.blogspot.com/2017/08/sporco-python-package-for-standard-and.html", "isFamilyFriendly": true, "displayUrl": "https://nuit-blanche.blogspot.com/2017/08/sporco-python-package-for-standard-and.html", "snippet": "<b>SParse</b> Optimization Research COde (SPORCO) is an open-source Python package for solving optimization problems with sparsity-inducing regularization, consisting primarily of <b>sparse</b> coding and dictionary learning, for both standard and convolutional forms of <b>sparse</b> <b>representation</b>. In the current version, all optimization problems are solved within the Alternating Direction Method of Multipliers (ADMM) framework. SPORCO was developed for applications in signal and image processing, but is also ...", "dateLastCrawled": "2022-01-30T05:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Analogy</b> and Abstraction - Gentner - 2017 - Wiley Online <b>Library</b>", "url": "https://onlinelibrary.wiley.com/doi/10.1111/tops.12278", "isFamilyFriendly": true, "displayUrl": "https://online<b>library</b>.wiley.com/doi/10.1111/tops.12278", "snippet": "<b>Analogy</b> is often <b>thought</b> of chiefly as a way to transfer knowledge from one situation to another, and indeed, it often serves that function. Projecting information from a well-understood domain <b>can</b> lend structure to an unfamiliar domain, as in: The mitochondria are the power supply for a cell. In this <b>analogy</b>, knowledge about a power supply is projected to the mitochondria. But <b>analogy</b> <b>can</b> also operate in mutual alignment1 1 Mutual alignment analogies are those that are processed by ...", "dateLastCrawled": "2022-01-26T20:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Representation and Computation in Cognitive</b> Models - Forbus - 2017 ...", "url": "https://onlinelibrary.wiley.com/doi/10.1111/tops.12277", "isFamilyFriendly": true, "displayUrl": "https://online<b>library</b>.wiley.com/doi/10.1111/tops.12277", "snippet": "We close with additional thoughts on open questions involving <b>representation</b> in cognition, and an <b>analogy</b>. 2 Setting the stage. There is a set of long\u2010standing misconceptions that need to be cleared out of the way first, so that we <b>can</b> focus on the matter at hand. 2.1 Misconception: Symbolic means serial, logical, and non\u2010numerical. Symbolic models have integrated structural and numerical information from the start of cognitive science. For example, semantic networks used relationships ...", "dateLastCrawled": "2019-12-29T03:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Why do we use word embeddings in NLP? | by Natasha Latysheva | Towards ...", "url": "https://towardsdatascience.com/why-do-we-use-embeddings-in-nlp-2f20e1b632d2", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/why-do-we-use-<b>embedding</b>s-in-nlp-2f20e1b632d2", "snippet": "Ideally, whatever numerical <b>representation</b> method we come up with would be semantically meaningful \u2014 the numerical values should capture as much of the linguistic meaning of a word as possible. A well-chosen, informative input <b>representation</b> <b>can</b> have a massive impact on overall model performance.", "dateLastCrawled": "2022-01-29T03:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Representing text in natural language processing</b> | by Michel Kana, Ph.D ...", "url": "https://towardsdatascience.com/representing-text-in-natural-language-processing-1eead30e57d8", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>representing-text-in-natural-language-processing</b>-1eead...", "snippet": "Therefore we <b>can</b> define a word by counting what other words occur in its environment, and we <b>can</b> represent the word by a vector, a list of numbers, a point in N-dimensional space. Such a <b>representation</b> is usually called embedding. Computer <b>can</b> use this cheating trick to understand the meaning of words in its context. Word-document <b>representation</b>", "dateLastCrawled": "2022-02-02T15:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "How does one measure if one has <b>a good word vector representation</b>? - Quora", "url": "https://www.quora.com/How-does-one-measure-if-one-has-a-good-word-vector-representation", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/How-does-one-measure-if-one-has-<b>a-good-word-vector-representation</b>", "snippet": "Answer (1 of 3): There are different tasks to measure how good your vectors are. The tasks are divided as intrinsic and extrinsic tasks. Intrinsic tasks * Word similarity evaluation on wordsim353 dataset, simlex999 * Word <b>analogy</b> test provided by Mikolov. Extrinsic tasks * Sentence classific...", "dateLastCrawled": "2022-01-23T01:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Building a RNN Recommendation Engine with <b>TensorFlow</b> | by Alfonso CARTA ...", "url": "https://medium.com/decathlontechnology/building-a-rnn-recommendation-engine-with-tensorflow-505644aa9ff3", "isFamilyFriendly": true, "displayUrl": "https://medium.com/decathlontechnology/building-a-rnn-recommendation-engine-with...", "snippet": "Such a wide choice <b>can</b> be an obstacle for users looking for new compelling new content. Of course customers <b>can</b> use search to access content. To this regard, a recommendation engine <b>can</b> come in ...", "dateLastCrawled": "2022-02-03T13:53:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Sparse and Redundant Representations_ From Theory to Applications</b> ...", "url": "https://www.academia.edu/3774175/Sparse_and_Redundant_Representations_From_Theory_to_Applications_in_Signal_and_Image_Processing", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/3774175/<b>Sparse_and_Redundant_Representations_From_Theory</b>_to...", "snippet": "<b>Sparse and Redundant Representations_ From Theory to Applications</b> in Signal and Image Processing. Elham Alaee. Download Download PDF. Full PDF Package Download Full PDF Package. This Paper. A short summary of this paper. 36 Full PDFs related to this paper. Read Paper. <b>Sparse and Redundant Representations_ From Theory to Applications</b> in Signal and Image Processing. Download ...", "dateLastCrawled": "2022-01-22T17:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Sparse methods for direction-of-arrival estimation</b>", "url": "https://www.researchgate.net/publication/322186937_Sparse_methods_for_direction-of-arrival_estimation", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/322186937_<b>Sparse_methods_for_direction-of</b>...", "snippet": "problem <b>can</b> be solved within the common framework of <b>sparse</b> <b>representation</b>. The main challenge then is The main challenge then is how to exploit the temporal redundancy of multiple snapshots.", "dateLastCrawled": "2021-12-28T22:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Distributed Representation</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/computer-science/distributed-representation", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/computer-science/<b>distributed-representation</b>", "snippet": "It turns out that such a <b>distributed representation</b> is <b>sparse</b>, because only a few of the neurons are active each time. This is in line with what we believe happens in the human brain, where at each time less than 5% of the neurons, in each layer, fire, and the rest remain inactive. In the antipodal of such distributive representations would be to have a single neuron firing each time. In the case of neural networks with more general (<b>compared</b> to 0 and 1) activation functions, the features ...", "dateLastCrawled": "2022-01-05T14:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Sparse</b> Matrices for High-Performance Graph Analytics", "url": "https://sites.cs.ucsb.edu/~gilbert/cs140/slides/GilbertORNL3Oct2014.pdf", "isFamilyFriendly": true, "displayUrl": "https://sites.cs.ucsb.edu/~gilbert/cs140/slides/GilbertORNL3Oct2014.pdf", "snippet": "\u2022 By <b>analogy</b> to numerical scientific computing. . . \u2022 What should the combinatorial BLAS look like? The middleware challenge for graph analysis C = A*B y = A*x \u00b5 = xT y Basic Linear Algebra Subroutines (BLAS): Ops/Sec vs. Matrix Size . 14 Identification of Primitives <b>Sparse</b> matrix-matrix multiplication (SpGEMM) Element-wise operations \u00d7 Matrices over various semirings: (+ . x), (min . +), (or . and), \u2026 <b>Sparse</b> matrix-dense vector multiplication <b>Sparse</b> matrix indexing \u00d7.* <b>Sparse</b> array ...", "dateLastCrawled": "2021-11-21T16:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "The deal.II <b>Library</b>: PETScWrappers::SparseMatrix Class Reference", "url": "https://www.dealii.org/current/doxygen/deal.II/classPETScWrappers_1_1SparseMatrix.html", "isFamilyFriendly": true, "displayUrl": "https://www.dealii.org/current/doxygen/deal.II/classPETScWrappers_1_1<b>Sparse</b>Matrix.html", "snippet": "Initialize a <b>sparse</b> matrix using the given sparsity pattern. Note that PETSc <b>can</b> be very slow if you do not provide it with a good estimate of the lengths of rows. Using the present function is a very efficient way to do this, as it uses the exact number of nonzero entries for each row of the matrix by using the given sparsity pattern argument.", "dateLastCrawled": "2021-11-02T12:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Analogical mapping and inference with</b> binary spatter codes and <b>sparse</b> ...", "url": "https://www.researchgate.net/publication/261148322_Analogical_mapping_and_inference_with_binary_spatter_codes_and_sparse_distributed_memory", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/261148322_Analogical_mapping_and_inference...", "snippet": "Vector symbolic architectures (VSAs) are a class of connectionist models for the <b>representation</b> and manipulation of compositional structures, which <b>can</b> be used to model <b>analogy</b>. We study a novel ...", "dateLastCrawled": "2021-11-09T13:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Representation</b> - Open Computing Facility", "url": "https://www.ocf.berkeley.edu/~jfkihlstrom/MemoryWeb/storage/representation.htm", "isFamilyFriendly": true, "displayUrl": "https://www.ocf.berkeley.edu/~jfkihlstrom/MemoryWeb/storage/<b>representation</b>.htm", "snippet": "<b>Sparse</b> distributed representations often are often used to solve the problem of catastrophic interference, because one <b>sparse</b> coding <b>can</b> be used for A-B, and another for A-C. They learn rapidly, too. But they don&#39;t show much by way of generalization -- which is as bad a problem, for a model of learning, as catastrophic interference.", "dateLastCrawled": "2022-01-30T20:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Code <b>Compass: boosting software reuse through machine learning</b>", "url": "https://www.code-compass.com/blog/intro/", "isFamilyFriendly": true, "displayUrl": "https://www.code-compass.com/blog/intro", "snippet": "<b>Compared</b> to a <b>sparse</b> encoding, a dense encoding typically has a far smaller number of dimensions (on the order of 100 or 200), and each entry in the vector is now no longer just a binary 0 or 1, but a scalar value. However, <b>compared</b> to the <b>sparse</b> encoding, the \u201cmeaning\u201d of each dimension is no longer obvious: it is learned by a machine learning model from data, rather than assigned by a human. To determine the similarity of two dense vector embeddings, one <b>can</b> compute their cosine ...", "dateLastCrawled": "2021-12-25T18:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>How does Word2vec solve sparse problem</b>? - Quora", "url": "https://www.quora.com/How-does-Word2vec-solve-sparse-problem", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>How-does-Word2vec-solve-sparse-problem</b>", "snippet": "Answer: Word2vec solves the sparsity problem of models that use word identities as a feature by projecting words to a low dimensional space. You are right that in a given textual collection, words might occur only with limited context. So the word vectors that you learn will be pretty weak - but ...", "dateLastCrawled": "2022-01-21T04:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Why aren&#39;t <b>hash tables used to store sparse matrices</b>? - Quora", "url": "https://www.quora.com/Why-arent-hash-tables-used-to-store-sparse-matrices", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Why-arent-<b>hash-tables-used-to-store-sparse-matrices</b>", "snippet": "Answer (1 of 3): They could be, and that might be a reasonable choice if the order of accesses to the matrix were completely random. However, we don\u2019t typically access the elements of a <b>sparse</b> matrix in random order. If elements in a row or column are more typically accessed in sequence, then it ...", "dateLastCrawled": "2022-01-15T03:15:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine Learning by Analogy</b> - SlideShare", "url": "https://www.slideshare.net/ColleenFarrelly/machine-learning-by-analogy-59094152", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ColleenFarrelly/<b>machine-learning-by-analogy</b>-59094152", "snippet": "<b>Machine Learning by Analogy</b> 1. Colleen M. Farrelly 2. Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics behind these ...", "dateLastCrawled": "2022-01-31T07:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "TensorFlow is an interface for expressing <b>machine</b> <b>learning</b> algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PPT) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> | Colleen Farrelly - Academia.edu", "url": "https://www.academia.edu/30404581/Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/30404581/<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "<b>Machine</b> <b>Learning</b> by <b>Analogy</b> Colleen M. Farrelly Overview of Problem Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics ...", "dateLastCrawled": "2022-01-02T06:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Machine</b> <b>Learning</b> by <b>Analogy</b> II - slideshare.net", "url": "https://www.slideshare.net/ColleenFarrelly/machine-learning-by-analogy-ii", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ColleenFarrelly/<b>machine</b>-<b>learning</b>-by-<b>analogy</b>-ii", "snippet": "Updated <b>Machine</b> <b>Learning</b> by <b>Analogy</b> presentation that builds to more advanced methods (TensorFlow, geometry/topology-based methods...) and adds a section on ti\u2026", "dateLastCrawled": "2021-12-31T12:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Neural Networks: Analogies. When our brains form analogies, they\u2026 | by ...", "url": "https://towardsdatascience.com/neural-networks-analogies-7ebeb3ac5d5e", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/neural-networks-analogies-7ebeb3ac5d5e", "snippet": "I\u2019ll outline a potential route to artificial neural networks which exhibit transfer <b>learning</b>: First, <b>Sparse</b> Distributed Representations. Numenta\u2019s Hierarchical Te m poral Memory, along with other techniques, relies upon a <b>sparse</b> distributed <b>representation</b>. An example of this is a very long string of ones and zeroes, where almost all the values are zero \u2014 there is a <b>sparse</b> distribution of the ones. If each digit represented a different thing, like \u2018pointy ears\u2019, \u2018tail ...", "dateLastCrawled": "2022-01-28T03:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "On <b>Machine</b> <b>Learning</b> \u2014 Data, ML &amp; Leadership", "url": "https://bugra.github.io/posts/2014/8/23/on-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://bugra.github.io/posts/2014/8/23/on-<b>machine</b>-<b>learning</b>", "snippet": "<b>Sparse</b> Colorful Filters. Recently, I wrote how we do classification at CB Insights.The post outlines some of the things that I have been thinking about how to apply <b>machine</b> <b>learning</b> for a given problem along with the process that we adopted for the classification problem at CB Insights, but also gave me a good opportunity to reflect even further about the <b>machine</b> <b>learning</b> process; shortcomings of papers, books and even traditional education system when it comes to teach the <b>machine</b> <b>learning</b>.", "dateLastCrawled": "2021-12-10T17:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Conceptualization as a Basis for Cognition \u2014 Human and <b>Machine</b> | by ...", "url": "https://towardsdatascience.com/conceptualization-as-a-basis-for-cognition-human-and-machine-345d9e687e3c", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/conceptualization-as-a-basis-for-cognition-human-and...", "snippet": "Abstraction and <b>analogy</b> allow concepts to be re-applied in new domains. There are many, often conflicting, ... <b>Machine</b>-<b>learning</b> systems must learn to conceptualize to reach the goal of creating machines with higher intelligence. To substantiate this claim, let\u2019s first examine what generalization in artificial intelligence means specifically in the context of artificial intelligence/<b>machine</b> <b>learning</b> (as opposed to the layman\u2019s use of the term), and then explore how that differs from ...", "dateLastCrawled": "2022-01-20T17:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Compressed Sensing Meets <b>Machine</b> <b>Learning</b>: Classification via <b>Sparse</b> ...", "url": "https://nuit-blanche.blogspot.com/2008/05/cs-mini-course-classification-via.html", "isFamilyFriendly": true, "displayUrl": "https://nuit-blanche.blogspot.com/2008/05/cs-mini-course-classification-via.html", "snippet": "Compressed Sensing Meets <b>Machine</b> <b>Learning</b>: Classification via <b>Sparse</b> <b>Representation</b> and Distributed Pattern Recognition This Spring, Allen Yang has given a mini course at Berkeley entitled Compressed Sensing Meets <b>Machine</b> <b>Learning</b>. The three lectures are listed here (it includes accompanying code): lecture 1: Classification via <b>Sparse</b> <b>Representation</b>; lecture 2: Classification of Mixture Subspace Models via <b>Sparse</b> <b>Representation</b>, lecture 3: Distributed Pattern Recognition; The third lecture ...", "dateLastCrawled": "2022-01-25T10:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Accelerating Innovation Through <b>Analogy</b> Mining", "url": "http://hyadatalab.com/papers/analogy-kdd17.pdf", "isFamilyFriendly": true, "displayUrl": "hyadatalab.com/papers/<b>analogy</b>-kdd17.pdf", "snippet": "<b>machine</b> <b>learning</b> models that develop similarity metrics suited for <b>analogy</b> mining. We demonstrate that <b>learning</b> purpose and mechanism representations allows us to \u2022nd analogies with higher precision and recall than traditional information-retrieval methods based on TF-IDF, LSA, LDA and GlOVe, in challenging noisy set-tings. Furthermore, we ...", "dateLastCrawled": "2022-01-29T02:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Regio-<b>selectivity</b> prediction with a <b>machine</b>-learned reaction ...", "url": "https://pubs.rsc.org/en/content/articlelanding/2021/sc/d0sc04823b#!", "isFamilyFriendly": true, "displayUrl": "https://pubs.rsc.org/en/content/articlelanding/2021/sc/d0sc04823b#!", "snippet": "A thorough benchmarking shows that <b>machine</b> learned <b>representation</b> and chemically meaningful descriptors complement each other in the fusion model, enhancing performance, and allow <b>learning</b> from a tiny experimental dataset. Second, we implement a multi-task neural network that is trained on DFT calculations of 136k organic molecules to enable on-the-fly calculations for six key atomic/bond descriptors. Finally, we demonstrate the fusion model using on-the-fly descriptors on three general ...", "dateLastCrawled": "2022-02-01T01:49:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Sparse Adaptive Local Machine Learning</b> Algorithms for Sensing and Analytics", "url": "https://pdxscholar.library.pdx.edu/cgi/viewcontent.cgi?article=1000&context=mcecs_mentoring", "isFamilyFriendly": true, "displayUrl": "https://pdxscholar.library.pdx.edu/cgi/viewcontent.cgi?article=1000&amp;context=mcecs...", "snippet": "Fig. 2: A <b>sparse representation can be thought of as</b> the dot product of a dictionary vector and a sparse code vector. Given a . dictionary . of general components, we can use a . sparse code. to select as few of them as possible to reconstruct an image of interest (Fig. 2). This reconstruction is called a . sparse representation. Sparse Coding. Image processing is expensive. Instead of working with the original image, we can identify its most relevant components and discard the rest. This ...", "dateLastCrawled": "2021-08-31T12:20:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(sparse representation)  is like +(library analogy)", "+(sparse representation) is similar to +(library analogy)", "+(sparse representation) can be thought of as +(library analogy)", "+(sparse representation) can be compared to +(library analogy)", "machine learning +(sparse representation AND analogy)", "machine learning +(\"sparse representation is like\")", "machine learning +(\"sparse representation is similar\")", "machine learning +(\"just as sparse representation\")", "machine learning +(\"sparse representation can be thought of as\")", "machine learning +(\"sparse representation can be compared to\")"]}