{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Is <b>Convex</b> Optimization Useful For <b>Machine</b> <b>Learning</b>? \u2013 charmestrength.com", "url": "https://charmestrength.com/is-convex-optimization-useful-for-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://charmestrength.com/is-<b>convex</b>-optimization-useful-for-<b>machine</b>-<b>learning</b>", "snippet": "<b>Function</b> optimization is the reason why we minimize error, cost, or loss when fitting a <b>machine</b> <b>learning</b> <b>algorithm</b>. Optimization is also performed during data preparation, hyperparameter tuning, and model selection in a predictive modeling project. Related guide for Is <b>Convex</b> Optimization Useful For <b>Machine</b> <b>Learning</b>? Which ML algorithms employ <b>convex</b> optimization techniques? <b>Convex</b> problems can be solved on a <b>convex</b> minimization or <b>convex</b> maximization problem. Most <b>machine</b> <b>learning</b> ...", "dateLastCrawled": "2022-01-20T04:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Optimization Algorithms for <b>Machine</b> <b>Learning</b> | by Aviejay Paul ...", "url": "https://towardsdatascience.com/optimization-algorithms-for-machine-learning-e794f2e7dfa7", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/optimization-<b>algorithms</b>-for-<b>machine</b>-<b>learning</b>-e794f2e7dfa7", "snippet": "Optimization Algorithms for <b>Machine</b> <b>Learning</b>. Chapter-5: Pre-requisites to Solve Optimization Problems. Aviejay Paul. Jul 4, 2021 \u00b7 10 min read. Photo by John Moeses Bauan on Unsplash. The link to Chapter-4: Important <b>Convex</b> Functions and <b>Convex</b> Properties is here. Chapter 5 is about some final topics that we will need to look at before diving into <b>Convex</b> Optimization. As the chapter name goes, you could consider these pre-requisites for <b>Convex</b> Optimization. Mind you, these concepts will be ...", "dateLastCrawled": "2022-01-26T02:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Report for CS229: <b>Convex</b> Optimization For <b>Machine</b> <b>Learning</b> (cvx4ml)", "url": "http://cs229.stanford.edu/proj2017/final-reports/5242031.pdf", "isFamilyFriendly": true, "displayUrl": "cs229.stanford.edu/proj2017/final-reports/5242031.pdf", "snippet": "Report for CS229: <b>Convex</b> Optimization For <b>Machine</b> <b>Learning</b> (cvx4ml) Abstract \u201cHumanity is a wandering fires in the fog. The appearance of breakthroughs through the fog from one flame to another can be called a miracle - A.N. Kolmogorov\u201d. <b>Machine</b> <b>Learning</b> connects engineering fields with usual people life. But I believe that <b>Machine</b> <b>Learning</b> can be improved by mathematical optimization, which has already become an important tool in many areas. Very important that there are effective ...", "dateLastCrawled": "2022-01-11T23:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Why study <b>convex optimization</b> for theoretical <b>machine</b> <b>learning</b>? - Cross ...", "url": "https://stats.stackexchange.com/questions/324981/why-study-convex-optimization-for-theoretical-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/324981", "snippet": "$\\begingroup$ <b>Machine</b> <b>Learning</b> is about building <b>function</b> approximation <b>like</b> couning methods, ... Gradient descent is the &quot;hello world&quot; optimization <b>algorithm</b> covered on probably any <b>machine</b> <b>learning</b> course. It is obvious in the case of regression, or classification models, but even with tasks such as clustering we are looking for a solution that optimally fits our data (e.g. k-means minimizes the within-cluster sum of squares). So if you want to understand how the <b>machine</b> <b>learning</b> ...", "dateLastCrawled": "2022-01-25T18:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Why is Convex Optimization such a big</b> deal in <b>Machine</b> <b>Learning</b>? - Quora", "url": "https://www.quora.com/Why-is-Convex-Optimization-such-a-big-deal-in-Machine-Learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>Why-is-Convex-Optimization-such-a-big</b>-deal-in-<b>Machine</b>-<b>Learning</b>", "snippet": "Answer (1 of 10): <b>Convex</b> optimization is the core of most <b>machine</b> <b>learning</b> methods Some key concepts here: - <b>Convex</b> functions are those for which it&#39;s possible to draw a line segment from any two points on the graph and this line will always be inside the graph (except at the endpoints). This m...", "dateLastCrawled": "2022-01-17T17:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Why are most of the <b>machine learning algorithms a convex optimization</b> ...", "url": "https://www.quora.com/Why-are-most-of-the-machine-learning-algorithms-a-convex-optimization-problem", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Why-are-most-of-the-<b>machine-learning-algorithms-a-convex</b>...", "snippet": "Answer: Thanks for the A2A , I really <b>like</b> Avinash\u2019s answer - My answer too is that they Are Not ! but we need them to be :) The next question is do we always or can we make do sometimes ? The most important thing is to get a model which is generalized (works well on unseen data). A <b>Convex</b> Funct...", "dateLastCrawled": "2022-01-24T22:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Gradient Descent for Non-convex Problems in Modern Machine Learning</b>", "url": "https://kilthub.cmu.edu/articles/thesis/Gradient_Descent_for_Non-convex_Problems_in_Modern_Machine_Learning/8336783", "isFamilyFriendly": true, "displayUrl": "https://kilthub.cmu.edu/articles/thesis/<b>Gradient_Descent_for_Non-convex</b>_Problems_in...", "snippet": "<b>Machine</b> <b>learning</b> has become an important tool set for artificial intelligence and data science across many fields. A modern <b>machine</b> <b>learning</b> method can be often reduced to a mathematical optimization problem. Among algorithms to solve the optimization problem, gradient descent and its variants <b>like</b> stochastic gradient descent and momentum methods are the most popular ones. The optimization problem induced from classical <b>machine</b> <b>learning</b> methods is often a <b>convex</b> and smooth one, for which ...", "dateLastCrawled": "2022-01-24T05:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Convex Multi-Task</b> Feature <b>Learning</b>", "url": "https://home.ttic.edu/~argyriou/papers/mtl_feat.pdf", "isFamilyFriendly": true, "displayUrl": "https://home.ttic.edu/~argyriou/papers/mtl_feat.pdf", "snippet": "color, price) considered a priori, much <b>like</b> features used for percep-tual maps, a technique for visualizing peoples\u2019 perception of products [1]. <b>Learning</b> common sparse representations across multiple tasks or datasets may also be of interest for example for data compression. While the problem of <b>learning</b> (or selecting) sparse representations has been extensively studied either for single-task supervised <b>learning</b> (e.g., using 1-norm regularization) or for unsupervised <b>learning</b> (e.g., using ...", "dateLastCrawled": "2022-01-15T03:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Non-Convex</b> Optimization in Deep <b>Learning</b> | by ER RAQABI El Mehdi | The ...", "url": "https://medium.com/swlh/non-convex-optimization-in-deep-learning-26fa30a2b2b3", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/<b>non-convex</b>-optimization-in-deep-<b>learning</b>-26fa30a2b2b3", "snippet": "<b>Convex</b> Optimization. CO is a subfield of mathematical optimization that deals with minimizing specific <b>convex</b> <b>function</b> over <b>convex</b> sets. It is interesting since in many cases, convergence time is ...", "dateLastCrawled": "2022-01-24T12:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Gradient Descent in <b>Machine</b> <b>Learning</b> - Javatpoint", "url": "https://www.javatpoint.com/gradient-descent-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.javatpoint.com/gradient-descent-in-<b>machine</b>-<b>learning</b>", "snippet": "Gradient descent was initially discovered by &quot;Augustin-Louis Cauchy&quot; in mid of 18th century. Gradient Descent is defined as one of the most commonly used iterative optimization algorithms of <b>machine</b> <b>learning</b> to train the <b>machine</b> <b>learning</b> and deep <b>learning</b> models. It helps in finding the local minimum of a <b>function</b>.", "dateLastCrawled": "2022-02-02T12:51:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Report for CS229: <b>Convex</b> Optimization For <b>Machine</b> <b>Learning</b> (cvx4ml)", "url": "http://cs229.stanford.edu/proj2017/final-reports/5242031.pdf", "isFamilyFriendly": true, "displayUrl": "cs229.stanford.edu/proj2017/final-reports/5242031.pdf", "snippet": "Report for CS229: <b>Convex</b> Optimization For <b>Machine</b> <b>Learning</b> (cvx4ml) Abstract \u201cHumanity is a wandering fires in the fog. The appearance of breakthroughs through the fog from one flame to another can be called a miracle - A.N. Kolmogorov\u201d. <b>Machine</b> <b>Learning</b> connects engineering fields with usual people life. But I believe that <b>Machine</b> <b>Learning</b> can be improved by mathematical optimization, which has already become an important tool in many areas. Very important that there are effective ...", "dateLastCrawled": "2022-01-11T23:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Machine Learning Note - Convex Optimization</b> | Xiaowen Ying", "url": "https://www.xiaowenying.com/machine-learning/2019/11/11/Convex-Optimization.html", "isFamilyFriendly": true, "displayUrl": "https://www.xiaowenying.com/<b>machine</b>-<b>learning</b>/2019/11/11/<b>Convex</b>-Optimization.html", "snippet": "<b>Machine Learning Note - Convex Optimization</b>. 11 November 2019. Introduction . I\u2019ve been taking an online <b>Machine</b> <b>Learning</b> class recently. This post is my note on <b>convex</b> optimization part. Contents. Optimization. 1. Overview; 2. Standard Form; 3. Categories; <b>Convex</b> Optimization. 1. <b>Convex</b> Set: 2. <b>Convex</b> <b>Function</b>. 2.1 Definition; 2.2 First Order Convexity Condition; 2.3 Second Order Convexity Condtion; 3. Proof of Convexity; Optimization 1. Overview. AI problem = Model + Optimization ...", "dateLastCrawled": "2022-01-01T03:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Theory <b>of Convex Optimization for Machine Learning</b>", "url": "https://www.researchgate.net/publication/262489426_Theory_of_Convex_Optimization_for_Machine_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/262489426_Theory_of_<b>Convex</b>_Optimization_for...", "snippet": "<b>Convex</b> optimization is used to dene the contribution of each <b>machine</b> to a global needed throughput. A Mirror Descent for Saddle Points method is proposed to cope with the assignment problem. The ...", "dateLastCrawled": "2021-11-07T19:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Convex Multi-Task</b> Feature <b>Learning</b>", "url": "https://home.ttic.edu/~argyriou/papers/mtl_feat.pdf", "isFamilyFriendly": true, "displayUrl": "https://home.ttic.edu/~argyriou/papers/mtl_feat.pdf", "snippet": "Our <b>algorithm</b> can also be used, as a special case, to simply select { not learn ... <b>Convex Multi-Task</b> Feature <b>Learning</b> 3 which <b>is similar</b> to the one developed in [22]. The <b>algorithm</b> simulta-neously learns both the features and the task functions through two alternating steps. The \ufb02rst step consists in independently <b>learning</b> the parameters of the tasks\u2019 regression or classi\ufb02cation functions. The sec-ondstepconsistsinlearning,inanunsupervisedway,alow-dimensional representation for these ...", "dateLastCrawled": "2022-01-15T03:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>machine</b> <b>learning</b> - <b>Non-Convex</b> Loss <b>Function</b> - Cross Validated", "url": "https://stats.stackexchange.com/questions/279292/non-convex-loss-function", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/279292/<b>non-convex</b>-loss-<b>function</b>", "snippet": "We know &quot;if a <b>function</b> is a <b>non-convex</b> loss <b>function</b> without plotting the graph&quot; by using Calculus.To quote Wikipedia&#39;s <b>convex</b> <b>function</b> article: &quot;If the <b>function</b> is twice differentiable, and the second derivative is always greater than or equal to zero for its entire domain, then the <b>function</b> is <b>convex</b>.&quot; If the second derivative is always greater than zero then it is strictly <b>convex</b>. Therefore if we can prove that the second derivatives of our selected cost <b>function</b> are always positive the ...", "dateLastCrawled": "2022-01-24T08:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A cheatsheet to Clustering algorithms | by Sairam Penjarla | Analytics ...", "url": "https://medium.com/analytics-vidhya/a-cheatsheet-to-clustering-algorithms-a2d49fa2cc69", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/a-cheatsheet-to-clustering-<b>algorithms</b>-a2d49fa2cc69", "snippet": "A cheatsheet to Clustering algorithms. Sairam Penjarla. Jul 15, 2021 \u00b7 3 min read. Clustering algorithms are one of the most popular <b>algorithm</b> used by <b>machine</b> <b>learning</b> practitioners across the ...", "dateLastCrawled": "2022-01-29T15:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Why is Convex Optimization such a big</b> deal in <b>Machine</b> <b>Learning</b>? - Quora", "url": "https://www.quora.com/Why-is-Convex-Optimization-such-a-big-deal-in-Machine-Learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>Why-is-Convex-Optimization-such-a-big</b>-deal-in-<b>Machine</b>-<b>Learning</b>", "snippet": "Answer (1 of 10): <b>Convex</b> optimization is the core of most <b>machine</b> <b>learning</b> methods Some key concepts here: - <b>Convex</b> functions are those for which it&#39;s possible to draw a line segment from any two points on the graph and this line will always be inside the graph (except at the endpoints). This m...", "dateLastCrawled": "2022-01-17T17:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "What are <b>convex</b> and non-<b>convex</b> functions, and how are they used in ...", "url": "https://www.quora.com/What-are-convex-and-non-convex-functions-and-how-are-they-used-in-Machine-Learning-optimization-problems-such-as-gradient-descent", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-are-<b>convex</b>-and-non-<b>convex</b>-<b>functions</b>-and-how-are-they-used...", "snippet": "Answer: For <b>convex</b> and non-<b>convex</b> functions, you can look on Wikipedia. The prototype of a <b>convex</b> <b>function</b> is a (piece-wise) linear <b>function</b> f(x)=ax+b or a parabola f(x)=x^2. For a non-<b>convex</b> <b>function</b>, you can look for example f(x)=x^3. The key word is that of semi-positivity of the Hessian of th...", "dateLastCrawled": "2022-01-21T09:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Gradient Descent in <b>Machine</b> <b>Learning</b> - Javatpoint", "url": "https://www.javatpoint.com/gradient-descent-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.javatpoint.com/gradient-descent-in-<b>machine</b>-<b>learning</b>", "snippet": "Gradient descent was initially discovered by &quot;Augustin-Louis Cauchy&quot; in mid of 18th century. Gradient Descent is defined as one of the most commonly used iterative optimization algorithms of <b>machine</b> <b>learning</b> to train the <b>machine</b> <b>learning</b> and deep <b>learning</b> models. It helps in finding the local minimum of a <b>function</b>.", "dateLastCrawled": "2022-02-02T12:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>machine</b> <b>learning</b> - Cost <b>function</b> of neural network is non-<b>convex</b> ...", "url": "https://stats.stackexchange.com/questions/106334/cost-function-of-neural-network-is-non-convex", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/106334", "snippet": "The cost <b>function</b> of a neural network is in general neither <b>convex</b> nor concave. This means that the matrix of all second partial derivatives (the Hessian) is neither positive semidefinite, nor negative semidefinite. Since the second derivative is a matrix, it&#39;s possible that it&#39;s neither one or the other. To make this analogous to one-variable ...", "dateLastCrawled": "2022-02-03T01:18:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Convex Optimization with Submodular Functions</b> \u2013 Optimization in <b>Machine</b> ...", "url": "https://wordpress.cs.vt.edu/optml/2018/03/20/convex-optimization-with-submodular-functions/", "isFamilyFriendly": true, "displayUrl": "https://wordpress.cs.vt.edu/optml/2018/03/20/<b>convex-optimization-with-submodular-functions</b>", "snippet": "The base polyhedra, B(F), <b>can</b> <b>be thought</b> of as the hollow outer shell of the set <b>function</b> formed by the intersecting constraint hyperplanes and the submodular polyhedra, P(F), <b>can</b> <b>be thought</b> of as the base polyhdra and the discrete, encapsulated, non-empty interior space encompassed by the base polyhedra. What we found confusing was whether Figure 2.1 below was representing just the domain of the set <b>function</b>, F, or if it was also representing the <b>function</b> values. We agreed that we <b>thought</b> ...", "dateLastCrawled": "2022-01-23T15:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Why is Convex Optimization such a big</b> deal in <b>Machine</b> <b>Learning</b>? - Quora", "url": "https://www.quora.com/Why-is-Convex-Optimization-such-a-big-deal-in-Machine-Learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>Why-is-Convex-Optimization-such-a-big</b>-deal-in-<b>Machine</b>-<b>Learning</b>", "snippet": "Answer (1 of 10): <b>Convex</b> optimization is the core of most <b>machine</b> <b>learning</b> methods Some key concepts here: - <b>Convex</b> functions are those for which it&#39;s possible to draw a line segment from any two points on the graph and this line will always be inside the graph (except at the endpoints). This m...", "dateLastCrawled": "2022-01-17T17:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Theory <b>of Convex Optimization for Machine Learning</b>", "url": "https://www.researchgate.net/publication/262489426_Theory_of_Convex_Optimization_for_Machine_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/262489426_Theory_of_<b>Convex</b>_Optimization_for...", "snippet": "First-order methods for <b>convex</b> optimization play a fundamental role in the solution of modern large-scale computational problems, encompassing applications in <b>machine</b> <b>learning</b> (Bubeck, 2014 ...", "dateLastCrawled": "2021-11-07T19:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Why are most of the <b>machine learning algorithms a convex optimization</b> ...", "url": "https://www.quora.com/Why-are-most-of-the-machine-learning-algorithms-a-convex-optimization-problem", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Why-are-most-of-the-<b>machine-learning-algorithms-a-convex</b>...", "snippet": "Answer: Thanks for the A2A , I really like Avinash\u2019s answer - My answer too is that they Are Not ! but we need them to be :) The next question is do we always or <b>can</b> we make do sometimes ? The most important thing is to get a model which is generalized (works well on unseen data). A <b>Convex</b> Funct...", "dateLastCrawled": "2022-01-24T22:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "On the Convergence of the Concave-<b>Convex</b> Procedure", "url": "https://proceedings.neurips.cc/paper/3646-on-the-convergence-of-the-concave-convex-procedure.pdf", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/3646-on-the-convergence-of-the-concave-<b>convex</b>...", "snippet": "The concave-<b>convex</b> procedure (CCCP) is a majorization-minimization <b>algorithm</b> that solves d.c. (difference of <b>convex</b> functions) programs as a sequence of <b>convex</b> programs. In <b>machine</b> <b>learning</b>, CCCP is extensively used in many <b>learning</b> algo-rithms like sparse support vector machines (SVMs), transductive SVMs, sparse principal component analysis, etc. Though widely used in many applications, the convergence behavior of CCCP has not gotten a lot of speci\ufb01c attention. Yuille and Rangarajan ...", "dateLastCrawled": "2022-01-17T20:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Non-Discriminatory <b>Machine</b> <b>Learning</b> through <b>Convex</b> Fairness Criteria ...", "url": "https://www.researchgate.net/publication/330302454_Non-Discriminatory_Machine_Learning_through_Convex_Fairness_Criteria", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/330302454_Non-Discriminatory_<b>Machine</b>_<b>Learning</b>...", "snippet": "In particular, our <b>algorithm</b> <b>can</b> handle non-<b>convex</b> &quot;linear fractional&quot; constraints (which includes fairness constraints such as predictive parity) for which no prior <b>algorithm</b> was known. Key to ...", "dateLastCrawled": "2022-01-29T21:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "5.2 <b>Least Squares Linear Regression</b> - GitHub Pages", "url": "https://jermwatt.github.io/machine_learning_refined/notes/5_Linear_regression/5_2_Least.html", "isFamilyFriendly": true, "displayUrl": "https://jermwatt.github.io/<b>machine</b>_<b>learning</b>_refined/notes/5_Linear_regression/5_2...", "snippet": "* The following is part of an early draft of the second edition of <b>Machine</b> <b>Learning</b> Refined. The published text (with revised material) ... determining the overall shape of a <b>function</b> - i.e., whether or not a <b>function</b> is <b>convex</b> - helps determine the appropriate optimization method(s) we <b>can</b> apply to efficiently determine the ideal parameters. In the case of the Least Squares cost <b>function</b> for linear regression it is easy to check that the cost <b>function</b> is always <b>convex</b> regardless of the ...", "dateLastCrawled": "2022-02-02T08:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "[<b>MCQ&#39;s] Machine Learning - Last Moment Tuitions</b>", "url": "https://lastmomenttuitions.com/mcqs-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://lastmomenttuitions.com/<b>mcqs-machine-learning</b>", "snippet": "Which of the following is a widely used and effective <b>machine</b> <b>learning</b> <b>algorithm</b> based on the idea of bagging? A. Decision Tree B. Regression C. Classification D. Random Forest Answer : D Explanation: The Radom Forest <b>algorithm</b> builds an ensemble of Decision Trees, mostly trained with the bagging method. 12. To find the minimum or the maximum of a <b>function</b>, we set the gradient to zero because: A. The value of the gradient at extrema of a <b>function</b> is always zero B. Depends on the type of ...", "dateLastCrawled": "2022-02-03T02:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "[D] <b>Can</b> Stochastic Gradient Descent Converge on Non-<b>Convex</b> Functions ...", "url": "https://www.reddit.com/r/MachineLearning/comments/slnvzw/d_can_stochastic_gradient_descent_converge_on/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/<b>MachineLearning</b>/comments/slnvzw/d_<b>can</b>_stochastic_gradient...", "snippet": "For instance, in <b>Machine</b> <b>Learning</b> applications with Neural Networks in the real world - Loss Functions almost always tend to be Non-<b>Convex</b>. Seeing as Non-<b>Convex</b> Functions usually have Saddle Points (i.e. point where the first derivatives of the Loss <b>Function</b> is 0), these usually &quot;trap&quot; and prevent the Gradient Descent from reaching the optimal point, since Gradient Descent <b>can</b> not move forward when the derivative is 0. I am aware of famous adaptions of Gradient Descent and Stochastic ...", "dateLastCrawled": "2022-02-07T14:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Gradient Descent For Linear Regression</b> \u2013 Wingshore", "url": "https://wingshore.wordpress.com/2014/11/20/gradient-descent-for-linear-regression/", "isFamilyFriendly": true, "displayUrl": "https://wingshore.wordpress.com/2014/11/20/<b>gradient-descent-for-linear-regression</b>", "snippet": "Hi guys, we believe you are thoroughly excited as we are. Finally, the time has arrived where we will discuss the first ever <b>Learning</b> <b>Algorithm</b> called \u2013 \u201c<b>Gradient Descent for Linear Regression</b>\u201c. So, far we have discussed the cost <b>function</b>, it\u2019s model, the basic foundation of <b>machine</b> <b>learning</b>, our goal to minimize cost <b>function</b> in order to have accuracy in predicting the ideal values and further, we have scooped our way through in understanding the Gradient Descent <b>Algorithm</b> in general.", "dateLastCrawled": "2022-01-28T04:48:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Why is Convex Optimization such a big</b> deal in <b>Machine</b> <b>Learning</b>? - Quora", "url": "https://www.quora.com/Why-is-Convex-Optimization-such-a-big-deal-in-Machine-Learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>Why-is-Convex-Optimization-such-a-big</b>-deal-in-<b>Machine</b>-<b>Learning</b>", "snippet": "Answer (1 of 10): <b>Convex</b> optimization is the core of most <b>machine</b> <b>learning</b> methods Some key concepts here: - <b>Convex</b> functions are those for which it&#39;s possible to draw a line segment from any two points on the graph and this line will always be inside the graph (except at the endpoints). This m...", "dateLastCrawled": "2022-01-17T17:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Machine</b> <b>learning</b> <b>algorithm</b> based on <b>convex</b> hull analysis - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S1877050921009911", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1877050921009911", "snippet": "In this paper <b>machine</b> <b>learning</b> methods for automatic classification problems using computational geometry are considered. Classes are defined with <b>convex</b> hulls of points sets in a multidimensional feature space. Classification algorithms based on the estimation of the proximity of the test point to <b>convex</b> class shells are considered. Several ways of such estimation are suggested when the test point is located both outside the <b>convex</b> hull and inside it. A new method for estimating proximity ...", "dateLastCrawled": "2022-02-02T09:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Convex Optimization in R</b> - <b>Machine</b> <b>Learning</b> Mastery", "url": "https://machinelearningmastery.com/convex-optimization-in-r/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>convex-optimization-in-r</b>", "snippet": "These methods might be useful in the core of your own implementation of a <b>machine</b> <b>learning</b> <b>algorithm</b>. You may want to implement your own <b>algorithm</b> tuning scheme to optimize the parameters of a model for some cost <b>function</b>. A good example may be the case where you want to optimize the hyper-parameters of a blend of predictions from an ensemble of multiple child models. Kick-start your project with my new book <b>Machine</b> <b>Learning</b> Mastery With R, including step-by-step tutorials and the R source ...", "dateLastCrawled": "2022-02-03T13:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "What are <b>the advantages of convex optimization compared to more general</b> ...", "url": "https://www.quora.com/What-are-the-advantages-of-convex-optimization-compared-to-more-general-optimization-problems", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-are-<b>the-advantages-of-convex-optimization-compared</b>-to-more...", "snippet": "Answer: Convexity confers two advantages. The first is that, in a constrained problem, a <b>convex</b> feasible region makes it easier to ensure that you do not generate infeasible solutions while searching for an optimum. If you have two feasible solutions, any solution within the line segment connecti...", "dateLastCrawled": "2022-01-14T12:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "[D] <b>Can</b> Stochastic Gradient Descent Converge on Non-<b>Convex</b> Functions ...", "url": "https://www.reddit.com/r/MachineLearning/comments/slnvzw/d_can_stochastic_gradient_descent_converge_on/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/<b>MachineLearning</b>/comments/slnvzw/d_<b>can</b>_stochastic_gradient...", "snippet": "For instance, in <b>Machine</b> <b>Learning</b> applications with Neural Networks in the real world - Loss Functions almost always tend to be Non-<b>Convex</b>. Seeing as Non-<b>Convex</b> Functions usually have Saddle Points (i.e. point where the first derivatives of the Loss <b>Function</b> is 0), these usually &quot;trap&quot; and prevent the Gradient Descent from reaching the optimal point, since Gradient Descent <b>can</b> not move forward when the derivative is 0. I am aware of famous adaptions of Gradient Descent and Stochastic ...", "dateLastCrawled": "2022-02-07T14:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Convex Factorization Machine for Regression</b> | DeepAI", "url": "https://deepai.org/publication/convex-factorization-machine-for-regression", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>convex-factorization-machine-for-regression</b>", "snippet": "We propose the <b>convex</b> factorization <b>machine</b> (CFM), which is a <b>convex</b> variant of the widely used Factorization Machines (FMs). Specifically, we employ a linear+quadratic model and regularize the linear term with the \u2113_2-regularizer and the quadratic term with the trace norm regularizer. Then, we formulate the CFM optimization as a semidefinite programming problem and propose an efficient optimization procedure with Hazan&#39;s <b>algorithm</b>. A key advantage of CFM over existing FMs is that it <b>can</b> ...", "dateLastCrawled": "2022-01-17T19:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Understanding Non-convex Optimization</b>", "url": "https://praneethnetrapalli.org/UnderstandingNonconvexOptimization-V5.pdf", "isFamilyFriendly": true, "displayUrl": "https://praneethnetrapalli.org/<b>UnderstandingNonconvexOptimization</b>-V5.pdf", "snippet": "\u2022<b>Convex</b> optimization ()is a <b>convex</b> <b>function</b>, \ud835\udc9eis <b>convex</b> set \u2022ut \u201ctoday\u2019s problems\u201d, and this tutorial, are non-<b>convex</b> \u2022Our focus: non-<b>convex</b> problems that arise in <b>machine</b> <b>learning</b> Variable, in \ud835\udc51 <b>function</b> feasible set. Outline of Tutorial Part I (algorithms &amp; background) \u2022<b>Convex</b> optimization (brief overview) \u2022Nonconvex optimization Part II \u2022Example applications of nonconvex optimization \u2022Open directions. <b>Convex</b> Functions onvex functions \u201clie below the line\u201d \ud835\udf40 ...", "dateLastCrawled": "2022-01-30T21:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Machine</b> <b>Learning</b>: Algorithms, Real-World Applications and Research ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7983091/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7983091", "snippet": "Supervised: Supervised <b>learning</b> is typically the task of <b>machine</b> <b>learning</b> to learn a <b>function</b> that maps an input to an output based on sample input-output pairs [].It uses labeled training data and a collection of training examples to infer a <b>function</b>. Supervised <b>learning</b> is carried out when certain goals are identified to be accomplished from a certain set of inputs [], i.e., a task-driven approach.The most common supervised tasks are \u201cclassification\u201d that separates the data, and ...", "dateLastCrawled": "2022-01-27T01:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Non-Convex</b> Optimization in Deep <b>Learning</b> | by ER RAQABI El Mehdi | The ...", "url": "https://medium.com/swlh/non-convex-optimization-in-deep-learning-26fa30a2b2b3", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/<b>non-convex</b>-optimization-in-deep-<b>learning</b>-26fa30a2b2b3", "snippet": "<b>Non-Convex</b> Optimization. A NCO is any problem where the objective or any of the constraints are <b>non-convex</b>. Even simple looking problems with as few as ten variables <b>can</b> be extremely challenging ...", "dateLastCrawled": "2022-01-24T12:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Selected Non-convex Optimization Problems in Machine Learning</b>", "url": "https://eprints.qut.edu.au/200748/1/Thanh_Nguyen_Thesis.pdf", "isFamilyFriendly": true, "displayUrl": "https://eprints.qut.edu.au/200748/1/Thanh_Nguyen_Thesis.pdf", "snippet": "Overall, our works extend the \ufb01eld of non-<b>convex</b> optimization for <b>machine</b> <b>learning</b> by con-tributing to its ongoing success with several important theoretical analyses and algorithms, including the empirical studies on their effectiveness. These works, together with other existing works in this area, demonstrate that the non-<b>convex</b> approach <b>can</b> be superior to the <b>convex</b> approach in many cases and should be further studied. ii. Statement of Original Authorship I hereby declare that this ...", "dateLastCrawled": "2022-01-23T20:40:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Introduction to Machine Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/24/introduction-to-machine-learning-2/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/24/<b>introduction-to-machine-learning</b>-2", "snippet": "<b>Machine</b> <b>learning</b> terminology for model building and validation There seems to be an <b>analogy</b> between statistical modeling and <b>machine</b> <b>learning</b> that we will cover in subsequent chapters in depth. However, a quick view has been provided as follows: in statistical modeling, linear regression with two independent variables is trying to fit the best plane with\u2026", "dateLastCrawled": "2022-01-17T05:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "11.2. <b>Convexity</b> \u2014 Dive into Deep <b>Learning</b> 0.17.2 documentation", "url": "https://d2l.ai/chapter_optimization/convexity.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_optimization/<b>convexity</b>.html", "snippet": "A twice-differentiable <b>function</b> is <b>convex</b> if and only if its Hessian (a matrix of second derivatives) is positive semidefinite. <b>Convex</b> constraints can be added via the Lagrangian. In practice we may simply add them with a penalty to the objective <b>function</b>. Projections map to points in the <b>convex</b> set closest to the original points.", "dateLastCrawled": "2022-01-30T21:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Introduction to <b>Machine</b> <b>Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/30/introduction-to-machine-learning-3/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/30/introduction-to-<b>machine</b>-<b>learning</b>-3", "snippet": "The loss <b>function</b> or cost <b>function</b> in <b>machine</b> <b>learning</b> is a <b>function</b> that maps the values of variables onto a real number intuitively representing some cost associated with the variable values. Optimization methods are applied to minimize the loss <b>function</b> by changing the parameter values, which is the central theme of <b>machine</b> <b>learning</b>.", "dateLastCrawled": "2022-01-28T18:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Basic Concepts in Machine Learning</b>", "url": "https://machinelearningmastery.com/basic-concepts-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>basic-concepts-in-machine-learning</b>", "snippet": "Probability Estimation: when the output of the <b>function</b> is a probability. <b>Machine Learning</b> in Practice. <b>Machine learning</b> algorithms are only a very small part of using <b>machine learning</b> in practice as a data analyst or data scientist. In practice, the process often looks like: Start Loop Understand the domain, prior knowledge and goals. Talk to ...", "dateLastCrawled": "2022-02-02T15:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Gradient</b> Descent: A Quick, Simple Introduction | Built In", "url": "https://builtin.com/data-science/gradient-descent", "isFamilyFriendly": true, "displayUrl": "https://builtin.com/data-science/<b>gradient</b>-descent", "snippet": "<b>Gradient</b> descent is an optimization algorithm that&#39;s used when training a <b>machine</b> <b>learning</b> model. It&#39;s based on a <b>convex</b> <b>function</b> and tweaks its parameters iteratively to minimize a given <b>function</b> to its local minimum.", "dateLastCrawled": "2022-02-02T07:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "4.1 <b>The Second-Order Optimality Condition</b>", "url": "https://jermwatt.github.io/machine_learning_refined/notes/4_Second_order_methods/4_3_Second.html", "isFamilyFriendly": true, "displayUrl": "https://jermwatt.github.io/<b>machine</b>_<b>learning</b>_refined/notes/4_Second_order_methods/4_3...", "snippet": "Furthermore, in <b>machine</b> <b>learning</b> we almost always find (points nearby) minima since we either deal with <b>convex</b> functions (as we often do in with applications linear <b>machine</b> <b>learning</b>) whose stationay points are minima, or design algorithms that do not tend to halt at saddle points so that once again the points we find via optimization are (nearby) minima.", "dateLastCrawled": "2022-01-14T00:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Solving Word <b>Analogies: A Machine Learning Perspective</b> | Request PDF", "url": "https://www.researchgate.net/publication/335597029_Solving_Word_Analogies_A_Machine_Learning_Perspective", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/335597029_Solving_Word_Analogies_A_<b>Machine</b>...", "snippet": "We introduce a supervised corpus-based <b>machine</b> <b>learning</b> algorithm for classifying analogous word pairs, and we show that it can solve multiple-choice SAT <b>analogy</b> questions, TOEFL synonym questions ...", "dateLastCrawled": "2021-10-16T21:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>machine</b> <b>learning</b> - How does Gradient Descent work? - Data Science Stack ...", "url": "https://datascience.stackexchange.com/questions/102509/how-does-gradient-descent-work", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/102509/how-does-gradient-descent-work", "snippet": "If the <b>function</b> we minimize was <b>convex</b>, it would not matter what we choose for initial values, as gradient descent would get us to the minimum no matter what. But as the dimensions of the model increase, it is extremely unlikely that we have a <b>convex</b> loss <b>function</b>. And in this case, initialization of the weight depends on the activation functions used in the model. As discussed in", "dateLastCrawled": "2022-01-16T12:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Regularization : What? Why? and How? (Part -1) | by Siddhant Rai ...", "url": "https://medium.com/mlearning-ai/regularization-what-why-and-how-part-1-ef6bdb6bafea", "isFamilyFriendly": true, "displayUrl": "https://medium.com/m<b>learning</b>-ai/regularization-what-why-and-how-part-1-ef6bdb6bafea", "snippet": "In general <b>machine</b> <b>learning</b> sense, it is solving an objective <b>function</b> to perform maximum or minimum evaluation. In reality, optimization is lot more profound in usage. Then we have two terms ...", "dateLastCrawled": "2022-01-28T00:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>machine</b> <b>learning</b> - Cost <b>function</b> of neural network is non-<b>convex</b> ...", "url": "https://stats.stackexchange.com/questions/106334/cost-function-of-neural-network-is-non-convex", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/106334", "snippet": "$\\begingroup$ I mean, this is how it should be interpreted, not just an <b>analogy</b>. $\\endgroup$ \u2013 avocado. May 23 &#39;16 at 12:27 . 5 $\\begingroup$ @loganecolss You are correct that this is not the only reason why cost functions are non-<b>convex</b>, but one of the most obvious reasons. Depdending on the network and the training set, there might be other reasons why there are multiple minima. But the bottom line is: The permuation alone creates non-convexity, regardless of other effects. $\\endgroup ...", "dateLastCrawled": "2022-02-03T01:18:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(convex function)  is like +(machine learning algorithm)", "+(convex function) is similar to +(machine learning algorithm)", "+(convex function) can be thought of as +(machine learning algorithm)", "+(convex function) can be compared to +(machine learning algorithm)", "machine learning +(convex function AND analogy)", "machine learning +(\"convex function is like\")", "machine learning +(\"convex function is similar\")", "machine learning +(\"just as convex function\")", "machine learning +(\"convex function can be thought of as\")", "machine learning +(\"convex function can be compared to\")"]}