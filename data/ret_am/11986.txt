{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Anatomy and connections related to relational reasoning. Areas of the ...", "url": "https://researchgate.net/figure/Anatomy-and-connections-related-to-relational-reasoning-Areas-of-the-prefrontal-cortex_fig1_227708787", "isFamilyFriendly": true, "displayUrl": "https://researchgate.net/figure/Anatomy-and-connections-related-to-relational...", "snippet": "To improve LSTM\u2019s memory, we encode a novel Relational Memory Core (RMC) as the cell state inside an LSTM cell using the standard <b>multi-head</b> <b>self attention</b> mechanism with variable length memory ...", "dateLastCrawled": "2021-07-23T18:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) Enhancing <b>brain</b> decoding using attention augmented deep neural ...", "url": "https://www.researchgate.net/publication/355251522_Enhancing_brain_decoding_using_attention_augmented_deep_neural_networks", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/355251522_Enhancing_<b>brain</b>_decoding_using...", "snippet": "2.1 Convolutional <b>Multi Head</b> ... <b>self-attention</b> and add global attention before the softmax classi\ufb01er to enhance . its performance. Contrary to [5], we use 16, 2 and 32 \ufb01lters for the three ...", "dateLastCrawled": "2022-01-31T10:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Week_13_Attention_and_its_links_to_memory.pdf - COMP596 <b>Brain</b>-inspired ...", "url": "https://www.coursehero.com/file/108384433/Week-13-Attention-and-its-links-to-memorypdf/", "isFamilyFriendly": true, "displayUrl": "https://www.coursehero.com/file/108384433/Week-13-Attention-and-its-links-to-memorypdf", "snippet": "You can ask !. Earn . Earn Free Access Learn More &gt; Upload Documents", "dateLastCrawled": "2022-01-19T06:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Appendix to 2020 CS379C Class Discussion Notes", "url": "https://web.stanford.edu/class/cs379c/archive/2020/class_messages_listing/appendix.html", "isFamilyFriendly": true, "displayUrl": "https://web.stanford.edu/class/cs379c/archive/2020/class_messages_listing/appendix.html", "snippet": "Suppose that the source (&quot;from&quot; register) and sink (&quot;to&quot; register) of the last register transfer is available in some form, perhaps in the first attentional subunit of a transformer decoder stack \u2013 consisting of a masked <b>multi-head</b> <b>self-attention</b> layer and a point-wise, fully connected layer \u2013 so that, for example, if the source is the third character of the first pattern and the sink is the register of the second operand of the comparator, the model should expect that the output ...", "dateLastCrawled": "2022-01-31T03:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Paper Digest: <b>ICASSP 2020 Highlights \u2013 Paper Digest</b>", "url": "https://www.paperdigest.org/2020/04/icassp-2020-highlights/", "isFamilyFriendly": true, "displayUrl": "https://www.paperdigest.org/2020/04/icassp-2020-highlights", "snippet": "Speech Enhancement Using Self-Adaptation and <b>Multi-Head</b> <b>Self-Attention</b>: Y. Koizumi, K. Yaiabe, M. Delcroix, Y. Maxuxama and D. Takeuchi: This paper investigates a self-adaptation method for speech enhancement using auxiliary speaker-aware features; we extract a speaker representation used for adaptation directly from the test utterance. 38: PEVD-Based Speech Enhancement in Reverberant Environments: V. W. Neo, C. Evers and P. A. Naylor: In this work, we focus on reverberant environments. It ...", "dateLastCrawled": "2022-01-30T12:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Expression of Concern: Abstracts</b> - 2019 - Basic &amp;amp; Clinical ...", "url": "https://onlinelibrary.wiley.com/doi/full/10.1111/bcpt.13363", "isFamilyFriendly": true, "displayUrl": "https://onlinelibrary.wiley.com/doi/full/10.1111/bcpt.13363", "snippet": "Conclusions: The approach proposed in this paper achieves better performance without any external feature, and can effectively solve the <b>multi-head</b> problem. The results demonstrate that the joint method based on the proposed tagging scheme can extract biomedical entities and relations effectively. Besides, the <b>self-attention</b> mechanism is utilized to learn long-term dependencies among tokens for capturing text information adequately. Our work is useful for biomedical text mining, and the ...", "dateLastCrawled": "2021-12-20T20:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Artificial Intelligence in Education: 21st International Conference ...", "url": "https://dokumen.pub/artificial-intelligence-in-education-21st-international-conference-aied-2020-ifrane-morocco-july-610-2020-proceedings-part-ii-1st-ed-9783030522391-9783030522407.html", "isFamilyFriendly": true, "displayUrl": "https://dokumen.pub/artificial-intelligence-in-education-21st-international-conference...", "snippet": "The resulting embeddings were processed by a <b>multi-head</b> attention layer that consists of a <b>selfattention</b> distributed across a number of heads. Attention computes the compatibility function of a query Q given a set of corresponding key-value pairs (K-V). These relationships modeled by <b>self-attention</b> do not necessarily correspond to those typically understood in natural language (e.g., syntactic structure, coreferences etc.), but are rather some latent dependencies that arise from the text. A ...", "dateLastCrawled": "2022-01-29T07:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Hiroshi Ishiguro</b> | PubFacts", "url": "https://www.pubfacts.com/author/Hiroshi+Ishiguro", "isFamilyFriendly": true, "displayUrl": "https://www.pubfacts.com/author/<b>Hiroshi+Ishiguro</b>", "snippet": "We propose a <b>self-attention</b> enhanced spatial temporal graph convolutional network for skeleton-based emotion recognition, in which the spatial convolutional part models the skeletal structure of the body as a static graph, and the <b>self-attention</b> part dynamically constructs more connections between the joints and provides supplementary information. Our experiment demonstrates that the proposed model significantly outperforms other models and that the features of the extracted skeleton data ...", "dateLastCrawled": "2021-10-01T10:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Graph-Based Deep Learning for Medical Diagnosis and Analysis: Past ...", "url": "https://www.arxiv-vanity.com/papers/2105.13137/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2105.13137", "snippet": "A. and B. <b>Brain</b> graph of f MRI and EEG data for <b>brain</b> responses and emotion analysis, respectively. C. DMRI sampling represented by a graph (DMRI <b>brain</b> reconstruction). D. Graph-<b>like</b> representation for organ segmentation (CT -pulmonary airway). Image adapted from [wu2020comprehensive, zhang2019functional, song2018eeg, hong2019multifold, selvan2020graph]. The popularity of the rapidly growing field of deep learning on GNNs is also reflected by the numerous recent surveys on graph ...", "dateLastCrawled": "2021-11-24T11:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Attention and Localization based on a Deep Convolutional Recurrent ...", "url": "https://cr.writing-reprint.net.ru/65", "isFamilyFriendly": true, "displayUrl": "https://cr.writing-reprint.net.ru/65", "snippet": "Attention and Localization based on a Deep Convolutional Recurrent Model for Weakly Supervised Audio Tagging", "dateLastCrawled": "2022-01-22T21:59:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Anatomy and connections related to relational reasoning. Areas of the ...", "url": "https://researchgate.net/figure/Anatomy-and-connections-related-to-relational-reasoning-Areas-of-the-prefrontal-cortex_fig1_227708787", "isFamilyFriendly": true, "displayUrl": "https://researchgate.net/figure/Anatomy-and-connections-related-to-relational...", "snippet": "To improve LSTM\u2019s memory, we encode a novel Relational Memory Core (RMC) as the cell state inside an LSTM cell using the standard <b>multi-head</b> <b>self attention</b> mechanism with variable length memory ...", "dateLastCrawled": "2021-07-23T18:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Application of Multilayer Network Models in Bioinformatics", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8044439/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8044439", "snippet": "Using medium time windows of 75\u2013100 s, we identified <b>brain</b> regions with low flexibility (considered core regions and observed in visual and attentional areas) and <b>brain</b> regions with high flexibility (considered peripheral regions and observed in subcortical and temporal <b>lobe</b> regions) by comparison with appropriate control dynamic network models. In general, this work demonstrates the effect of time window size on the network dynamics observed during task execution, providing practical ...", "dateLastCrawled": "2022-01-08T07:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Frontiers | Application of Multilayer Network Models in <b>Bioinformatics</b> ...", "url": "https://www.frontiersin.org/articles/10.3389/fgene.2021.664860/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/fgene.2021.664860", "snippet": "The <b>brain</b> is the control center of most animal activities, and it has been the goal of many researchers to unravel the mystery of the <b>brain</b> and simulate the human <b>brain</b> with external devices such as computers. Before that, the structure and mechanism of the <b>brain</b> needs to be clarified, and it is costly to study the human <b>brain</b> because of its complexity. The human <b>brain</b> is a complex system organized by the structural and functional relationships among its components", "dateLastCrawled": "2022-01-29T23:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Flowchart showing the network parameter-based determination of TLE ...", "url": "https://www.researchgate.net/figure/Flowchart-showing-the-network-parameter-based-determination-of-TLE-lateralization-First_fig2_323264395", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/figure/Flowchart-showing-the-network-parameter-based...", "snippet": "Download scientific diagram | Flowchart showing the network parameter-based determination of TLE lateralization. First, based on the connectivity matrix shown in Figure 1, node-related network ...", "dateLastCrawled": "2022-01-14T00:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Frontiers</b> | <b>Differential Entropy Feature Signal Extraction</b> Based on ...", "url": "https://www.frontiersin.org/articles/10.3389/fphy.2020.629620/full", "isFamilyFriendly": true, "displayUrl": "https://www.<b>frontiers</b>in.org/articles/10.3389/fphy.2020.629620", "snippet": "The ability of perceived robots for expressing <b>similar</b> human behaviors is considered to be more ... of <b>brain</b> areas were calculated by attention mechanism and the sum of weights was taken as the contribution value of <b>brain</b> areas, which showed that <b>frontal</b> <b>lobe</b> areas play an important role in feature signal recognition experiments . The feature signals of different activation areas were extracted by DE and PSD topographic distribution, which found that prefrontal and temporal lobes of the ...", "dateLastCrawled": "2022-02-03T00:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Paper Digest: <b>ICASSP 2020 Highlights \u2013 Paper Digest</b>", "url": "https://www.paperdigest.org/2020/04/icassp-2020-highlights/", "isFamilyFriendly": true, "displayUrl": "https://www.paperdigest.org/2020/04/icassp-2020-highlights", "snippet": "Speech Enhancement Using Self-Adaptation and <b>Multi-Head</b> <b>Self-Attention</b>: Y. Koizumi, K. Yaiabe, M. Delcroix, Y. Maxuxama and D. Takeuchi: This paper investigates a self-adaptation method for speech enhancement using auxiliary speaker-aware features; we extract a speaker representation used for adaptation directly from the test utterance. 38", "dateLastCrawled": "2022-01-30T12:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "The neuroanatomical basis of understanding sarcasm and its relationship ...", "url": "https://europepmc.org/article/MED/15910115", "isFamilyFriendly": true, "displayUrl": "https://europepmc.org/article/MED/15910115", "snippet": "<b>Similar</b> Articles The neuroanatomical basis of understanding sarcasm and its relationship to social cognition. ... these 2 abilities were related to the ability to understand sarcasm. This suggests that the right <b>frontal</b> <b>lobe</b> mediates understanding of sarcasm by integrating affective processing with perspective taking. Full text links . Read article at publisher&#39;s site (DOI): 10.1037/0894-4105.19.3.288. References . Articles referenced by this article (64) Title not supplied. AUTHOR UNKNOWN ...", "dateLastCrawled": "2021-06-25T13:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Expression of Concern: Abstracts</b> - 2019 - Basic &amp;amp; Clinical ...", "url": "https://onlinelibrary.wiley.com/doi/full/10.1111/bcpt.13363", "isFamilyFriendly": true, "displayUrl": "https://onlinelibrary.wiley.com/doi/full/10.1111/bcpt.13363", "snippet": "Conclusions: The approach proposed in this paper achieves better performance without any external feature, and can effectively solve the <b>multi-head</b> problem. The results demonstrate that the joint method based on the proposed tagging scheme can extract biomedical entities and relations effectively. Besides, the <b>self-attention</b> mechanism is utilized to learn long-term dependencies among tokens for capturing text information adequately. Our work is useful for biomedical text mining, and the ...", "dateLastCrawled": "2021-12-20T20:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Hiroshi Ishiguro</b> | PubFacts", "url": "https://www.pubfacts.com/author/Hiroshi+Ishiguro", "isFamilyFriendly": true, "displayUrl": "https://www.pubfacts.com/author/<b>Hiroshi+Ishiguro</b>", "snippet": "We propose a <b>self-attention</b> enhanced spatial temporal graph convolutional network for skeleton-based emotion recognition, in which the spatial convolutional part models the skeletal structure of the body as a static graph, and the <b>self-attention</b> part dynamically constructs more connections between the joints and provides supplementary information. Our experiment demonstrates that the proposed model significantly outperforms other models and that the features of the extracted skeleton data ...", "dateLastCrawled": "2021-10-01T10:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Attention and Localization based on a Deep Convolutional Recurrent ...", "url": "https://cr.writing-reprint.net.ru/65", "isFamilyFriendly": true, "displayUrl": "https://cr.writing-reprint.net.ru/65", "snippet": "Attention and Localization based on a Deep Convolutional Recurrent Model for Weakly Supervised Audio Tagging", "dateLastCrawled": "2022-01-22T21:59:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) Enhancing <b>brain</b> decoding using attention augmented deep neural ...", "url": "https://www.researchgate.net/publication/355251522_Enhancing_brain_decoding_using_attention_augmented_deep_neural_networks", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/355251522_Enhancing_<b>brain</b>_decoding_using...", "snippet": "<b>Brain</b> activity <b>can</b> be used as a control signal for <b>brain</b>-machine interfaces (BMIs). A powerful and widely acknowledged BMI approach, so far only applied in invasive recording techniques, uses ...", "dateLastCrawled": "2022-01-31T10:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Week_13_Attention_and_its_links_to_memory.pdf - COMP596 <b>Brain</b>-inspired ...", "url": "https://www.coursehero.com/file/108384433/Week-13-Attention-and-its-links-to-memorypdf/", "isFamilyFriendly": true, "displayUrl": "https://www.coursehero.com/file/108384433/Week-13-Attention-and-its-links-to-memorypdf", "snippet": "COMP596: <b>Brain</b>-inspired artificial intelligence Week 13: Attention and its links to memory Blake. Study Resources. Main Menu; by School; by Literature Title; by Subject; Textbook Solutions Expert Tutors Earn. Main Menu; Earn Free Access; Upload Documents; Refer Your Friends; Earn Money; Become a Tutor ; Scholarships; For Educators Log in Sign up Find Study Resources by School by Literature Title by Subject Browse Textbook Solutions Ask Expert Tutors You <b>can</b> ask ! Earn . Earn Free Access ...", "dateLastCrawled": "2022-01-19T06:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Anatomy and connections related to relational reasoning. Areas of the ...", "url": "https://researchgate.net/figure/Anatomy-and-connections-related-to-relational-reasoning-Areas-of-the-prefrontal-cortex_fig1_227708787", "isFamilyFriendly": true, "displayUrl": "https://researchgate.net/figure/Anatomy-and-connections-related-to-relational...", "snippet": "Greater ventromedial PFC (BA11) recruitment has previously been observed in sentences compared to word-lists more generally (Brennan &amp; Pylkk\u00e4nen, 2012) and, as discussed above, is <b>thought</b> to be ...", "dateLastCrawled": "2021-07-23T18:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Expression of Concern: Abstracts</b> - 2019 - Basic &amp;amp; Clinical ...", "url": "https://onlinelibrary.wiley.com/doi/full/10.1111/bcpt.13363", "isFamilyFriendly": true, "displayUrl": "https://onlinelibrary.wiley.com/doi/full/10.1111/bcpt.13363", "snippet": "Conclusions: The approach proposed in this paper achieves better performance without any external feature, and <b>can</b> effectively solve the <b>multi-head</b> problem. The results demonstrate that the joint method based on the proposed tagging scheme <b>can</b> extract biomedical entities and relations effectively. Besides, the <b>self-attention</b> mechanism is utilized to learn long-term dependencies among tokens for capturing text information adequately. Our work is useful for biomedical text mining, and the ...", "dateLastCrawled": "2021-12-20T20:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Appendix to 2020 CS379C Class Discussion Notes", "url": "https://web.stanford.edu/class/cs379c/archive/2020/class_messages_listing/appendix.html", "isFamilyFriendly": true, "displayUrl": "https://web.stanford.edu/class/cs379c/archive/2020/class_messages_listing/appendix.html", "snippet": "The conventional view of speech as primarily localized in Broca&#39;s and Wernicke&#39;s areas in the <b>frontal</b> and temporal lobes has yielded to a more nuanced distributed model that engages a diverse collection of cortical areas \u2013 see here, but if you want to understand how language and meaning are structurally and computationally related in the <b>brain</b>, you&#39;ll need to study the multi-modal sensory-motor association areas in the temporal and parietal lobes, the dual &quot;what-versus-where&quot; streams of ...", "dateLastCrawled": "2022-01-31T03:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Graph-Based Deep Learning for Medical Diagnosis and Analysis: Past ...", "url": "https://www.arxiv-vanity.com/papers/2105.13137/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2105.13137", "snippet": "<b>Brain</b> signals are an example of a graph signal, and the graph representation <b>can</b> encode the complex structure of the <b>brain</b> to represent either physical or functional connectivity across different <b>brain</b> regions. At the structural level, the network is defined by the anatomical connections between regions of <b>brain</b> tissue. At the functional level, the graph nodes represent <b>brain</b> regions of interest (ROI), while edges capture the correlation between their activities computed via an ...", "dateLastCrawled": "2021-11-24T11:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "researchopenworld.com", "url": "https://researchopenworld.com/2021/10/", "isFamilyFriendly": true, "displayUrl": "https://researchopenworld.com/2021/10", "snippet": "In a population of 44,991 people [9], throughout the entire pandemic there have been 2 COVID-19 deaths [10] and 918 confirmed infections as of September 24, 2021. This is a death rate of 2/44,991 = 0.00004 and an infection rate of 0.02. As percentages, these are an infection rate of 2% and a death rate of 0.004%.", "dateLastCrawled": "2021-12-23T06:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "4,786 <b>results</b> in SearchWorks catalog - Stanford University", "url": "https://searchworks.stanford.edu/catalog?per_page=100&q=Poster+presentations&sort=pub_date_sort+desc%2C+title_sort+asc&view=list", "isFamilyFriendly": true, "displayUrl": "https://<b>search</b>works.stanford.edu/catalog?per_page=100&amp;q=Poster+presentations&amp;sort=pub...", "snippet": "It is a unique forum where agile researchers, practitioners, <b>thought</b> leaders, coaches, and trainers get together to present and discuss their most recent innovations, research <b>results</b>, experiences, concerns, challenges, and trends. XP conferences provide an informal environment to learn and trigger discussions and welcome both people new to agile and seasoned agile practitioners. The 18 papers included in this volume were carefully reviewed and selected from overall 37 submissions. They stem ...", "dateLastCrawled": "2022-02-03T05:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "pub2.db.tokushima-u.ac.jp", "url": "http://pub2.db.tokushima-u.ac.jp/ERD/person/19966/researchmap/published_papers.csv", "isFamilyFriendly": true, "displayUrl": "pub2.db.tokushima-u.ac.jp/ERD/person/19966/researchmap/published_papers.csv", "snippet": "published_papers &quot;\u30bf\u30a4\u30c8\u30eb(\u65e5\u672c\u8a9e)&quot;,&quot;\u30bf\u30a4\u30c8\u30eb(\u82f1\u8a9e)&quot;,&quot;\u8457\u8005(\u65e5\u672c\u8a9e)&quot;,&quot;\u8457\u8005(\u82f1\u8a9e)&quot;,&quot;\u62c5\u5f53\u533a\u5206&quot;,&quot;\u6982\u8981(\u65e5\u672c\u8a9e)&quot;,&quot;\u6982\u8981(\u82f1\u8a9e)&quot;,&quot;\u51fa\u7248\u8005 ...", "dateLastCrawled": "2022-01-23T12:45:00.0000000Z", "language": "ja", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "pub2.db.tokushima-u.ac.jp", "url": "http://pub2.db.tokushima-u.ac.jp/ERD/person/19966/researchmap/published_papers.jsonl", "isFamilyFriendly": true, "displayUrl": "pub2.db.tokushima-u.ac.jp/ERD/person/19966/researchmap/published_papers.jsonl", "snippet": "{&quot;insert&quot;:{&quot;user_id&quot;:&quot;1000261631&quot;,&quot;type&quot;:&quot;published_papers&quot;},&quot;similar_merge&quot;:{&quot;see_also&quot;:[{&quot;@id&quot;:&quot;https://reader.elsevier.com/reader/sd/pii/S0950705121006018?token ...", "dateLastCrawled": "2022-01-14T15:47:00.0000000Z", "language": "ja", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Application of Multilayer Network Models in Bioinformatics", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8044439/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8044439", "snippet": "Using medium time windows of 75\u2013100 s, we identified <b>brain</b> regions with low flexibility (considered core regions and observed in visual and attentional areas) and <b>brain</b> regions with high flexibility (considered peripheral regions and observed in subcortical and temporal <b>lobe</b> regions) by comparison with appropriate control dynamic network models. In general, this work demonstrates the effect of time window size on the network dynamics observed during task execution, providing practical ...", "dateLastCrawled": "2022-01-08T07:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Flowchart showing the network parameter-based determination of TLE ...", "url": "https://www.researchgate.net/figure/Flowchart-showing-the-network-parameter-based-determination-of-TLE-lateralization-First_fig2_323264395", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/figure/Flowchart-showing-the-network-parameter-based...", "snippet": "Download scientific diagram | Flowchart showing the network parameter-based determination of TLE lateralization. First, based on the connectivity matrix shown in Figure 1, node-related network ...", "dateLastCrawled": "2022-01-14T00:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Paper Digest: <b>ICASSP 2020 Highlights \u2013 Paper Digest</b>", "url": "https://www.paperdigest.org/2020/04/icassp-2020-highlights/", "isFamilyFriendly": true, "displayUrl": "https://www.paperdigest.org/2020/04/icassp-2020-highlights", "snippet": "Speech Enhancement Using Self-Adaptation and <b>Multi-Head</b> <b>Self-Attention</b>: Y. Koizumi, K. Yaiabe, M. Delcroix, Y. Maxuxama and D. Takeuchi : This paper investigates a self-adaptation method for speech enhancement using auxiliary speaker-aware features; we extract a speaker representation used for adaptation directly from the test utterance. 38: PEVD-Based Speech Enhancement in Reverberant Environments: V. W. Neo, C. Evers and P. A. Naylor: In this work, we focus on reverberant environments. It ...", "dateLastCrawled": "2022-01-30T12:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Frontiers | Application of Multilayer Network Models in <b>Bioinformatics</b> ...", "url": "https://www.frontiersin.org/articles/10.3389/fgene.2021.664860/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/fgene.2021.664860", "snippet": "Using medium time windows of 75\u2013100 s, we identified <b>brain</b> regions with low flexibility (considered core regions and observed in visual and attentional areas) and <b>brain</b> regions with high flexibility (considered peripheral regions and observed in subcortical and temporal <b>lobe</b> regions) by comparison with appropriate control dynamic network models. In general, this work demonstrates the effect of time window size on the network dynamics observed during task execution, providing practical ...", "dateLastCrawled": "2022-01-29T23:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Frontiers</b> | <b>Differential Entropy Feature Signal Extraction</b> Based on ...", "url": "https://www.frontiersin.org/articles/10.3389/fphy.2020.629620/full", "isFamilyFriendly": true, "displayUrl": "https://www.<b>frontiers</b>in.org/articles/10.3389/fphy.2020.629620", "snippet": "<b>In brain</b>-computer-interface (BCI) devices, signal acquisition via reducing the electrode channels <b>can</b> reduce the computational complexity of models and filter out the irrelevant noise. Differential entropy (DE) plays an important role in emotional components of signals, which <b>can</b> reflect the area activity differences. Therefore, to extract distinctive feature signals and improve the recognition accuracy based on feature signals, a method of DE feature signal recognition based on a ...", "dateLastCrawled": "2022-02-03T00:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Expression of Concern: Abstracts</b> - 2019 - Basic &amp;amp; Clinical ...", "url": "https://onlinelibrary.wiley.com/doi/full/10.1111/bcpt.13363", "isFamilyFriendly": true, "displayUrl": "https://onlinelibrary.wiley.com/doi/full/10.1111/bcpt.13363", "snippet": "Conclusions: The approach proposed in this paper achieves better performance without any external feature, and <b>can</b> effectively solve the <b>multi-head</b> problem. The results demonstrate that the joint method based on the proposed tagging scheme <b>can</b> extract biomedical entities and relations effectively. Besides, the <b>self-attention</b> mechanism is utilized to learn long-term dependencies among tokens for capturing text information adequately. Our work is useful for biomedical text mining, and the ...", "dateLastCrawled": "2021-12-20T20:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Jiangfen WU</b> | Peking University, Beijing | PKU | Advanced Academy of ...", "url": "https://www.researchgate.net/profile/Jiangfen-Wu-2", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/profile/<b>Jiangfen-Wu</b>-2", "snippet": "Purpose: <b>Frontal</b> <b>lobe</b> epilepsy is a common epileptic disorder and is characterized by recurring seizures that arise in the <b>frontal</b> lobes. The purpose of this study is to identify the epileptogenic ...", "dateLastCrawled": "2021-11-11T12:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Appendix to 2020 CS379C Class Discussion Notes", "url": "https://web.stanford.edu/class/cs379c/archive/2020/class_messages_listing/appendix.html", "isFamilyFriendly": true, "displayUrl": "https://web.stanford.edu/class/cs379c/archive/2020/class_messages_listing/appendix.html", "snippet": "The conventional view of speech as primarily localized in Broca&#39;s and Wernicke&#39;s areas in the <b>frontal</b> and temporal lobes has yielded to a more nuanced distributed model that engages a diverse collection of cortical areas \u2013 see here, but if you want to understand how language and meaning are structurally and computationally related in the <b>brain</b>, you&#39;ll need to study the multi-modal sensory-motor association areas in the temporal and parietal lobes, the dual &quot;what-versus-where&quot; streams of ...", "dateLastCrawled": "2022-01-31T03:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Graph-Based Deep Learning for Medical Diagnosis and Analysis: Past ...", "url": "https://www.arxiv-vanity.com/papers/2105.13137/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2105.13137", "snippet": "<b>Compared</b> with CNNs, graph neural networks represent signals from <b>brain</b> regions as nodes in a topological graph and represent the relationships between them using the graph edges. This structure <b>can</b> preserve rich connection information <b>compared</b> to what is possible with the 2D and 3D matrices used by regular CNNs. Graph convolutional networks (GCNs) have extended the theory of signal processing on graphs [shuman2013emerging] to enable the representation learning power of CNNs to be applied to ...", "dateLastCrawled": "2021-11-24T11:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Hiroshi Ishiguro</b> | PubFacts", "url": "https://www.pubfacts.com/author/Hiroshi+Ishiguro", "isFamilyFriendly": true, "displayUrl": "https://www.pubfacts.com/author/<b>Hiroshi+Ishiguro</b>", "snippet": "This allowed us to look beyond the <b>brain</b> regional activation in isolation to investigate whether the <b>brain</b> regional interactivity <b>can</b> provide further insights for understanding the neural substrates of the affect. Our results indicated that the differential affect states emerged from subtle variation in information flow of the <b>brain</b> cortical regions that were in both hemispheres. They also showed that these regions that were rather common between affect states than distinct to a specific ...", "dateLastCrawled": "2021-10-01T10:24:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "10.5. <b>Multi-Head Attention</b> \u2014 Dive into Deep <b>Learning</b> 0.17.2 documentation", "url": "http://d2l.ai/chapter_attention-mechanisms/multihead-attention.html", "isFamilyFriendly": true, "displayUrl": "d2l.ai/chapter_attention-mechanisms/<b>multihead-attention</b>.html", "snippet": "This design is called <b>multi-head attention</b>, where each of the \\(h\\) attention pooling outputs is a head [Vaswani et al., 2017]. Using fully-connected layers to perform learnable linear transformations, Fig. 10.5.1 describes <b>multi-head attention</b>.", "dateLastCrawled": "2022-02-02T01:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Introduction to Transformers in Machine Learning</b>", "url": "https://www.machinecurve.com/index.php/2020/12/28/introduction-to-transformers-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/.../12/28/<b>introduction-to-transformers-in-machine-learning</b>", "snippet": "The masked <b>multi-head</b> attention segment, which performs <b>multi-head</b> <b>self-attention</b> on the outputs, but does so in a masked way, so that positions depend on the past only. The <b>multi-head</b> attention segment , which performs <b>multi-head</b> <b>self-attention</b> on a combination of the ( encoded ) inputs and the outputs, so that the model learns to correlate encoded inputs with desired outputs.", "dateLastCrawled": "2022-02-03T03:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Attending to Attention. A summary of a revolutionary paper\u2026 | by Akash ...", "url": "https://towardsdatascience.com/attending-to-attention-eba798f0e940", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/attending-to-attention-eba798f0e940", "snippet": "The first is a <b>multi-head</b> <b>self-attention</b> mechanism, and the second is a simple, position-wise fully connected feed-forward network. We employ a residual connection around each of the two sub-layers, followed by layer normalization. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of ...", "dateLastCrawled": "2022-01-25T10:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Machine</b> <b>Learning</b> Glossary: Language Evaluation | Google Developers", "url": "https://developers.google.com/machine-learning/glossary/language", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine</b>-<b>learning</b>/glossary/language", "snippet": "Refer also to <b>self-attention</b> and <b>multi-head</b> <b>self-attention</b>, which are the building blocks of Transformers. B. bag of words. #language. A representation of the words in a phrase or passage, irrespective of order. For example, bag of words represents the following three phrases identically: the dog jumps; jumps the dog; dog jumps the; Each word is mapped to an index in a sparse vector, where the vector has an index for every word in the vocabulary. For example, the phrase the dog jumps is ...", "dateLastCrawled": "2022-01-29T02:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Intuitive Introduction to BERT \u2013 MachineCurve</b>", "url": "https://www.machinecurve.com/index.php/2021/01/04/intuitive-introduction-to-bert/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/index.php/2021/01/04/<b>intuitive-introduction-to-bert</b>", "snippet": "From our article about GPT: \u201cThe input is then served to a masked <b>multi-head</b> attention segment, which computes <b>self-attention</b> in a unidirectional way.Here, the residual is added and the result is layer normalized.\u201d Indeed, GPT (which uses the Transformer decoder segment autoregressively during pretraining) and the original Transformer (which performs Seq2Seq), apply a mask in one of the attention modules \u2013 the masked <b>multi-head</b> <b>self-attention</b> subsegment in the decoder segment.. For any ...", "dateLastCrawled": "2022-01-30T22:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Capturing Multi-Resolution Context by Dilated <b>Self-Attention</b>", "url": "https://www.merl.com/publications/docs/TR2021-036.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.merl.com/publications/docs/TR2021-036.pdf", "snippet": "to <b>machine</b> translation or language modeling, where close-by words are more likely to have a dependent relationship, while only a few distant words or word groups are relevant to trace the semantic con-text and syntax of a sentence [15]. This hypothesis is investigated in this work by combining re-stricted (or time-restricted) <b>self-attention</b> with a dilation mechanism, whereby a high <b>self-attention</b> resolution for neighboring frames and a lower <b>self-attention</b> resolution for distant information ...", "dateLastCrawled": "2021-12-02T00:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "CoAtNet: how to perfectly combine CNNs and Transformers | by Leonardo ...", "url": "https://medium.com/codex/coatnet-how-to-perfectly-combine-cnns-and-transformer-9632e187ecbf", "isFamilyFriendly": true, "displayUrl": "https://medium.com/codex/coatnet-how-to-perfectly-combine-cnns-and-transformer-9632e...", "snippet": "The <b>multi-head</b> attention block computes <b>self-attention</b> several times with different weight matrices and then concatenates the results together, which are resized to the embedding dimension using ...", "dateLastCrawled": "2022-01-26T06:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Transformers in NLP: A beginner friendly explanation | Towards Data Science", "url": "https://towardsdatascience.com/transformers-89034557de14", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>transformer</b>s-89034557de14", "snippet": "If you are looking for an <b>analogy</b> between <b>self attention</b> and attention, think of z serving the purpose of context vectors and not global alignment weights. The <b>Transformer</b> . \u26a0\ufe0f A word of caution: the contents of this image may appear exponentially more complicated than they are. We will break this scary beast down into small baby beasts and it will all make sense. (I promise #2) (left) The <b>Transformer</b> architecture. Source: paper. (right) An abstracted version of the same for better ...", "dateLastCrawled": "2022-02-02T13:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "5.3. Underfitting and Overfitting \u2014 Dive into Deep <b>Learning</b> 0.17.0 ...", "url": "http://preview.d2l.ai/d2l-en/master/chapter_machine-learning-fundamentals/underfit-overfit.html", "isFamilyFriendly": true, "displayUrl": "preview.d2l.ai/d2l-en/master/chapter_<b>machine</b>-<b>learning</b>-fundamentals/underfit-overfit.html", "snippet": "The noise term \\(\\epsilon\\) obeys a normal distribution with a mean of 0 and a standard deviation of 0.1. For optimization, we typically want to avoid very large values of gradients or losses. This is why the features are rescaled from \\(x^i\\) to \\(\\frac{x^i}{i!}\\).It allows us to avoid very large values for large exponents \\(i\\).We will synthesize 100 samples each for the training set and test set.", "dateLastCrawled": "2021-10-08T21:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "9.5. <b>Machine Translation</b> and the Dataset \u2014 Dive into Deep <b>Learning</b> 0.17 ...", "url": "https://d2l.ai/chapter_recurrent-modern/machine-translation-and-dataset.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_recurrent-modern/<b>machine-translation</b>-and-dataset.html", "snippet": "<b>Machine Translation</b> and the Dataset \u2014 Dive into Deep <b>Learning</b> 0.17.2 documentation. 9.5. <b>Machine Translation</b> and the Dataset. We have used RNNs to design language models, which are key to natural language processing. Another flagship benchmark is <b>machine translation</b>, a central problem domain for sequence transduction models that transform ...", "dateLastCrawled": "2022-01-29T21:57:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(multi-head self-attention)  is like +(frontal lobe in brain)", "+(multi-head self-attention) is similar to +(frontal lobe in brain)", "+(multi-head self-attention) can be thought of as +(frontal lobe in brain)", "+(multi-head self-attention) can be compared to +(frontal lobe in brain)", "machine learning +(multi-head self-attention AND analogy)", "machine learning +(\"multi-head self-attention is like\")", "machine learning +(\"multi-head self-attention is similar\")", "machine learning +(\"just as multi-head self-attention\")", "machine learning +(\"multi-head self-attention can be thought of as\")", "machine learning +(\"multi-head self-attention can be compared to\")"]}