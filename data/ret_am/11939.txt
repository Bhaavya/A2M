{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Using Web images to train a deep neural network to detect sparsely ...", "url": "https://www.sciencedirect.com/science/article/pii/S1574954121003381", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1574954121003381", "snippet": "The <b>mini-batch</b> size is user-adjustable (typically in binary-number <b>increments</b>, e.g. 2, 4, 8, 16, 32, etc.) depending on the data set. Too <b>small</b> a <b>mini-batch</b> size may provide insufficient image data for the network to meaningfully learn from some or all of the classes within each iteration, while too large a <b>mini-batch</b> size may cause the network to overfit to the training images. Because of the high computational complexity of the deep learning process, the <b>mini-batch</b> size is ultimately ...", "dateLastCrawled": "2022-01-25T20:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Simulation analysis of semiconductor manufacturing with</b> <b>small</b> lot ...", "url": "https://www.researchgate.net/publication/221526038_Simulation_analysis_of_semiconductor_manufacturing_with_small_lot_size_and_batch_tool_replacements", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221526038_Simulation_analysis_of...", "snippet": "<b>Small</b> lot sizes and the conversion of batch processes to <b>mini-batch</b> or single-wafer processes are widely regarded as a promising means for a step-wise cycle time reduction. However, there is still ...", "dateLastCrawled": "2021-08-04T14:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Improved digital chest tomosynthesis image quality by use of a ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7774945/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7774945", "snippet": "With reference to these results, the highest values of epoch and <b>mini-batch</b> indicating a tendency of convergence were selected as the optimization values. The SDNR value was the highest for epochs of 70 and a <b>mini-batch</b> size of 128 with an initial learning rate of 0.001 (Fig 8A\u20138C). Therefore, learning was performed by setting the <b>mini-batch</b> ...", "dateLastCrawled": "2022-01-18T00:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Small Molecule Accurate Recognition Technology (SMART</b>) to Enhance ...", "url": "https://www.nature.com/articles/s41598-017-13923-x", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41598-017-13923-x", "snippet": "The initial learning rate was \\(\\alpha =0.001\\), and the <b>mini-batch</b> size was 256. We applied dropout regularization 69 on layers 5, 6, and 7 of the network, and batch normalization 49 .", "dateLastCrawled": "2022-02-03T16:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Simulation analysis of semiconductor manufacturing</b> Simulation analysis ...", "url": "https://www.deepdyve.com/lp/inderscience-publishers/simulation-analysis-of-semiconductor-manufacturing-simulation-analysis-LeuQ010qp4", "isFamilyFriendly": true, "displayUrl": "https://www.deepdyve.com/lp/inderscience-publishers/simulation-analysis-of...", "snippet": "Long cycle times in semiconductor manufacturing represent an increasing challenge for the industry and lead to a growing need for breakthrough approaches to reduce it. <b>Small</b> lot sizes and the conversion of batch processes to <b>mini-batch</b> or single-wafer processes are widely regarded as a promising means for a step-wise cycle time reduction. However, there is still a lack of comprehensive and meaningful studies. In this paper, we present results of our modelling and simulation assessment. Our ...", "dateLastCrawled": "2020-06-02T05:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "3.1. <b>Linear Regression</b> - Dive into Deep Learning \u2014 Dive into Deep ...", "url": "https://ja.d2l.ai/chapter_deep-learning-basics/linear-regression.html", "isFamilyFriendly": true, "displayUrl": "https://ja.d2l.ai/chapter_deep-learning-basics/<b>linear-regression</b>.html", "snippet": "The <b>mini-batch</b> stochastic gradient descent is widely used for deep learning to find numerical solutions. Its algorithm is simple: first, we initialize the values of the model parameters, typically at random; then we iterate over the data multiple times, so that each iteration may reduce the value of the loss function. In each iteration, we first randomly and uniformly sample a <b>mini-batch</b>", "dateLastCrawled": "2022-02-01T07:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Marshmallow Fondant <b>Small</b> Batch - cakeboxing.com", "url": "https://cakeboxing.com/marshmallow-fondant-small-batch/", "isFamilyFriendly": true, "displayUrl": "https://cakeboxing.com/marshmallow-fondant-<b>small</b>-batch", "snippet": "Marshmallow fondant <b>small</b> batch. Making homemade marshmallow fondant with butter MMF is quite easy to do. Why would I put fondant on a cookie. Microwave on high for another 30. I also found a <b>small</b> batch recipe in the Wilton online forum. Marshmallow fondant is easier to make than you might think with only two ingredients. A <b>small</b> round cutter about 1 14-inches. The mixture will expand. Knead until mixture loses stickiness adding more confectioners sugar if necessary. <b>Small</b> Marshmallow ...", "dateLastCrawled": "2022-01-21T23:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Fast <b>and accurate detection of kiwifruit in orchard using improved</b> ...", "url": "https://link.springer.com/article/10.1007/s11119-020-09754-y", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11119-020-09754-y", "snippet": "The stochastic gradient descent (SGD) was used to train the DY3TNet model with a <b>mini-batch</b> size of 64, and the momentum of the network was set to a fixed value of 0.9 and a weight decay of 0.0005. In this work, a learning rate of 0.001 was applied for all layers in the network. It took about 12 h to perform a total of 10,000 iterations over the training set. To provide well-differentiated weights for object and background, leading to faster and more accurate training results, the transfer ...", "dateLastCrawled": "2022-01-17T11:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Termite control system <b>with multi-fluid proportion metering and</b> batch ...", "url": "https://www.freepatentsonline.com/6568559.html", "isFamilyFriendly": true, "displayUrl": "https://www.freepatentsonline.com/6568559.html", "snippet": "This is accomplished by repeatedly pumping the correct volumes of each fluid together <b>in small</b> batches that are then mixed in the line 30 and in the hose 34. For example, using twenty pulses from the pump 28 as the basis for each proportioning cycle (i.e. <b>mini-batch</b>), and assuming a 24:1 mixing ratio between water and termiticide, provides:", "dateLastCrawled": "2021-10-18T06:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Page 4", "url": "https://www.seasonsandsuppers.ca/page/4/?m%3Fcuisine=dessert&m", "isFamilyFriendly": true, "displayUrl": "https://www.seasonsandsuppers.ca/page/4/?m?cuisine=dessert&amp;m", "snippet": "Start adding more flour, <b>in small</b> <b>increments</b>, mixing in well before adding more. Continue adding flour until you have a moist dough that wraps around the hook and cleans the bowl of the mixer. Remove dough to a lightly floured surface and knead briefly. Form into a ball, cover with a clean tea towel and let rest 10 minutes. Meanwhile, prepare the filling. Add the chopped walnuts to a food processor and pulse a few times to break them down a bit. Add the brown sugar and butter and pulse a few ...", "dateLastCrawled": "2022-01-29T18:40:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Improved digital chest tomosynthesis image quality by use of a ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7774945/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7774945", "snippet": "With reference to these results, the highest values of epoch and <b>mini-batch</b> indicating a tendency of convergence were selected as the optimization values. The SDNR value was the highest for epochs of 70 and a <b>mini-batch</b> size of 128 with an initial learning rate of 0.001 (Fig 8A\u20138C). Therefore, learning was performed by setting the <b>mini-batch</b> ...", "dateLastCrawled": "2022-01-18T00:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Simulation analysis of semiconductor manufacturing with</b> <b>small</b> lot ...", "url": "https://www.researchgate.net/publication/221526038_Simulation_analysis_of_semiconductor_manufacturing_with_small_lot_size_and_batch_tool_replacements", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221526038_Simulation_analysis_of...", "snippet": "<b>Small</b> lot sizes and the conversion of batch processes to <b>mini-batch</b> or single-wafer processes are widely regarded as a promising means for a step-wise cycle time reduction. However, there is still ...", "dateLastCrawled": "2021-08-04T14:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Deep Multimodal Learning From MRI and Clinical Data for Early ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8525883/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8525883", "snippet": "A challenge in the proposed supervised model training is the relatively <b>small</b> number of infants at high-risk compared to those at low-risk. Imbalanced datasets can severely affect the model&#39;s learning ability (Haixiang et al., 2017). In such cases, the deep learning models may become majority class classifiers, i.e., they fail to learn the concepts of the minority class. To overcome this challenge, we employed a data balancing and augmentation method (Kawahara et al., 2017), which uses ...", "dateLastCrawled": "2022-02-03T06:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Using Web images to train a deep neural network to detect sparsely ...", "url": "https://www.sciencedirect.com/science/article/pii/S1574954121003381", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1574954121003381", "snippet": "The <b>mini-batch</b> size is user-adjustable (typically in binary-number <b>increments</b>, e.g. 2, 4, 8, 16, 32, etc.) depending on the data set. Too <b>small</b> a <b>mini-batch</b> size may provide insufficient image data for the network to meaningfully learn from some or all of the classes within each iteration, while too large a <b>mini-batch</b> size may cause the network to overfit to the training images. Because of the high computational complexity of the deep learning process, the <b>mini-batch</b> size is ultimately ...", "dateLastCrawled": "2022-01-25T20:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Simulation analysis of semiconductor manufacturing</b> Simulation analysis ...", "url": "https://www.deepdyve.com/lp/inderscience-publishers/simulation-analysis-of-semiconductor-manufacturing-simulation-analysis-LeuQ010qp4", "isFamilyFriendly": true, "displayUrl": "https://www.deepdyve.com/lp/inderscience-publishers/simulation-analysis-of...", "snippet": "<b>Small</b> lot sizes and the conversion of batch processes to <b>mini-batch</b> or single-wafer processes are widely regarded as a promising means for a step-wise cycle time reduction. However, there is still a lack of comprehensive and meaningful studies. In this paper, we present results of our modelling and simulation assessment. Our simulation analysis shows that <b>small</b> lot size and the replacement of batch tools with <b>mini-batch</b> or single wafer tools lead to significant reductions in cycle time but ...", "dateLastCrawled": "2020-06-02T05:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>CROSSBOW: Scaling Deep Learning with Small Batch Sizes</b> on Multi-GPU ...", "url": "https://deepai.org/publication/crossbow-scaling-deep-learning-with-small-batch-sizes-on-multi-gpu-servers", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>crossbow-scaling-deep-learning-with-small-batch-sizes</b>...", "snippet": "The reasons are twofold: (1) with large and redundant training datasets (as it is often the case), <b>small</b> batches ensure faster training because only few batches are sufficient to capture the dimensionality of the problem space and converge quickly to good solutions [34, 5]; (2) a <b>small</b> batch size leads to \u201cnoisier\u201d gradient updates, which widen the exploration of the loss landscape, making it more likely to find better solutions with a higher test accuracy [21, 20, 30, 23]", "dateLastCrawled": "2021-12-24T01:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "3.1. <b>Linear Regression</b> - Dive into Deep Learning \u2014 Dive into Deep ...", "url": "https://ja.d2l.ai/chapter_deep-learning-basics/linear-regression.html", "isFamilyFriendly": true, "displayUrl": "https://ja.d2l.ai/chapter_deep-learning-basics/<b>linear-regression</b>.html", "snippet": "The <b>mini-batch</b> stochastic gradient descent is widely used for deep learning to find numerical solutions. Its algorithm is simple: first, we initialize the values of the model parameters, typically at random; then we iterate over the data multiple times, so that each iteration may reduce the value of the loss function. In each iteration, we first randomly and uniformly sample a <b>mini-batch</b>", "dateLastCrawled": "2022-02-01T07:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Small Molecule Accurate Recognition Technology (SMART</b>) to Enhance ...", "url": "https://www.nature.com/articles/s41598-017-13923-x", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41598-017-13923-x", "snippet": "Again, the aforementioned grid-cell approaches 28 are <b>similar</b> to ours in that the shifted grid positions can be thought of as corresponding to the first layer of convolutions, which have <b>small</b> ...", "dateLastCrawled": "2022-02-03T16:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Entropy | Free Full-Text | An Appraisal of <b>Incremental Learning</b> Methods ...", "url": "https://www.mdpi.com/1099-4300/22/11/1190/htm", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/1099-4300/22/11/1190/htm", "snippet": "As a special case of machine learning, <b>incremental learning</b> can acquire useful knowledge from incoming data continuously while it does not need to access the original data. It is expected to have the ability of memorization and it is regarded as one of the ultimate goals of artificial intelligence technology. However, <b>incremental learning</b> remains a long term challenge. Modern deep neural network models achieve outstanding performance on stationary data distributions with batch training. This ...", "dateLastCrawled": "2022-02-01T01:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Fast <b>and accurate detection of kiwifruit in orchard using improved</b> ...", "url": "https://link.springer.com/article/10.1007/s11119-020-09754-y", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11119-020-09754-y", "snippet": "The stochastic gradient descent (SGD) was used to train the DY3TNet model with a <b>mini-batch</b> size of 64, and the momentum of the network was set to a fixed value of 0.9 and a weight decay of 0.0005. In this work, a learning rate of 0.001 was applied for all layers in the network. It took about 12 h to perform a total of 10,000 iterations over the training set. To provide well-differentiated weights for object and background, leading to faster and more accurate training results, the transfer ...", "dateLastCrawled": "2022-01-17T11:04:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Improved digital chest tomosynthesis image quality by use of a ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7774945/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7774945", "snippet": "The VDSR training workflow <b>can</b> be understood through the <b>mini-batch</b> SGDM method ... After <b>measuring</b> the RMSE of each iteration, the optimal number of iterations (k, m) in IR for SART\u2013TV\u2013FISTA converged to 30 and SART converged to 24 (Fig 7A). Therefore, the number of iterations was set to 30 for SART\u2013TV\u2013FISTA and 24 for SART. The energy in DE\u2013VM processing was set to 60 keV because it resulted in the highest SDNR in VM energy optimization of DE\u2013VM\u2013SART\u2013TV\u2013FISTA (Fig 7B ...", "dateLastCrawled": "2022-01-18T00:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Improved digital chest tomosynthesis image quality by use of a ...", "url": "https://journals.plos.org/plosone/article?id=10.1371%2Fjournal.pone.0244745", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0244745", "snippet": "The VDSR training workflow <b>can</b> be understood through the <b>mini-batch</b> SGDM method ... After <b>measuring</b> the RMSE of each iteration, the optimal number of iterations (k, m) in IR for SART\u2013TV\u2013FISTA converged to 30 and SART converged to 24 . Therefore, the number of iterations was set to 30 for SART\u2013TV\u2013FISTA and 24 for SART. The energy in DE\u2013VM processing was set to 60 keV because it resulted in the highest SDNR in VM energy optimization of DE\u2013VM\u2013SART\u2013TV\u2013FISTA . Next, the SDNR was ...", "dateLastCrawled": "2021-10-27T08:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Small Molecule Accurate Recognition Technology (SMART</b>) to Enhance ...", "url": "https://www.nature.com/articles/s41598-017-13923-x", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41598-017-13923-x", "snippet": "The initial learning rate was \\(\\alpha =0.001\\), and the <b>mini-batch</b> size was 256. We applied dropout regularization 69 on layers 5, 6, and 7 of the network, and batch normalization 49 .", "dateLastCrawled": "2022-02-03T16:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Towards calibrated and scalable uncertainty representations for neural ...", "url": "http://bayesiandeeplearning.org/2019/papers/15.pdf", "isFamilyFriendly": true, "displayUrl": "bayesiandeeplearning.org/2019/papers/15.pdf", "snippet": "<b>mini batch</b> of size m. 2. Langevin dynamics phase: over N iterations evaluate the gradient steps with a decreasing step size ( t) and add Gaussian noise ( t). Equation 4 characterizes the process for N forward passes, where x ti is the ith <b>minibatch</b> of data. It illustrates that as the step size ( t) decays to zero, the noise term ( t) begins to dominate. The method then approximates Langevin Monte Carlo to sample from the posterior over parameters. t = t 2 (rlogp( t) + N n Xn 1 rlogp(x tij t ...", "dateLastCrawled": "2021-08-01T03:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Multivariate time-series modeling with generative neural networks ...", "url": "https://www.sciencedirect.com/science/article/pii/S245230622100126X", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S245230622100126X", "snippet": "Batch optimization <b>can</b> be obtained as a special case of this <b>mini-batch</b> optimization procedure for n bat = \u03c4; it <b>can</b> be used with relatively <b>small</b> datasets. To update the parameter vector \u03b8 , we utilize the Adam optimizer of Kingma and Ba (2014) which uses a \u201cmemory-sticking gradient\u201d procedure \u2014 a weighted combination of the current gradient and past gradients from earlier iterations.", "dateLastCrawled": "2022-01-12T11:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Small</b> Molecule Accurate Recognition Technology (SMART) to Enhance ...", "url": "https://europepmc.org/article/MED/29079836", "isFamilyFriendly": true, "displayUrl": "https://europepmc.org/article/MED/29079836", "snippet": "Again, the aforementioned grid-cell approaches 28 are similar to ours in that the shifted grid positions <b>can</b> <b>be thought</b> of as corresponding to the first layer of convolutions, which have <b>small</b> receptive fields (like grid cells), and they are shifted across the input space like shifted grids. Also, our approach uses layers of convolutions that <b>can</b> capture multi-scale similarities. The grid-cell approaches, however, use hand-designed features (i.e. counts of peaks within each grid cell), and ...", "dateLastCrawled": "2021-09-05T05:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Entropy | Free Full-Text | An Appraisal of <b>Incremental Learning</b> Methods ...", "url": "https://www.mdpi.com/1099-4300/22/11/1190/htm", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/1099-4300/22/11/1190/htm", "snippet": "Through <b>measuring</b> the importance of weights, the old knowledge <b>can</b> be protected by limiting the learning rate. The loss function is: L (\u03b8) = L n (\u03b8) + \u03bb R (\u03b8 i) (2) where L n is the loss function of new data, \u03bb is a hyperparameter, R is the regularization term and \u03b8 i is the important parameters to the old knowledge. Weights of a neural network model will be updated by back propagation (BP) and stochastic gradient descent (SGD). While in an <b>incremental learning</b> scenario, weights of an ...", "dateLastCrawled": "2022-02-01T01:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "no bake mini chocolate tarts", "url": "https://hotlinesoft.com/nrqr/no-bake-mini-chocolate-tarts.html", "isFamilyFriendly": true, "displayUrl": "https://hotlinesoft.com/nrqr/no-bake-mini-chocolate-tarts.html", "snippet": "Divide in half and press into two 10 cm/ 4&quot; mini tart pans. Add a <b>small</b> dollop of whipped cream on top of each then a sprinkle of lime zest. Easy to serve. Line a muffin tin with 12 cupcake liners. Press the mixture to the bottom and the sides of 4 mini tart tins (about 4-5 inch diameter). Easy to eat. For the filling, bring the coconut cream to a low simmer and add in the espresso powder. Mini Apple Tarts . <b>Mini Batch</b> Baker is a place I created for very <b>small</b>-batch baking. Chill until ready ...", "dateLastCrawled": "2022-01-16T23:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Page 4", "url": "https://www.seasonsandsuppers.ca/page/4/?m%3Fcourse=dessert&m", "isFamilyFriendly": true, "displayUrl": "https://www.seasonsandsuppers.ca/page/4/?m?course=dessert&amp;m", "snippet": "Begin adding the last 1 cup of flour <b>in small</b> <b>increments</b>, mixing well between each addition, until the dough wraps the kneading hook and cleans the bowl. The dough may be a bit sticky still at the bottom of the bowl, but resist the urge to add too much flour in the bowl. You <b>can</b> always add a bit more on the counter. Remove the dough to a floured work surface and knead the dough briefly, adding more flour only if the dough is sticking to the surface or your hands. Form into a ball and place ...", "dateLastCrawled": "2022-01-22T03:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Focus <b>on Ingredients -- Malt I Converting All</b>-Grain ... Pages 1-39 ...", "url": "https://fliphtml5.com/uaxh/frwh/basic", "isFamilyFriendly": true, "displayUrl": "https://fliphtml5.com/uaxh/frwh/basic", "snippet": "Remember that by using extract,you <b>can</b> create a concentrated wort, and so your boil volumes <b>can</b> remain <b>small</b> enough to continuebrewing on the stovetop as you may already be doing. Instead of bringing plain water to a boil andadding extracts, you\u2019ll create some all-grain wort instead, adding a whole new dimension to you rbeers.Let\u2019s look at the basic elements of the recipe conversion process in detail first, then we\u2019ll formalizethe procedure and work an example or two to demonstrate the ...", "dateLastCrawled": "2022-01-29T06:54:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Direct Gradient Calculation: Simple and Variation\u2010Tolerant On\u2010Chip ...", "url": "https://onlinelibrary.wiley.com/doi/10.1002/aisy.202100064", "isFamilyFriendly": true, "displayUrl": "https://onlinelibrary.wiley.com/doi/10.1002/aisy.202100064", "snippet": "In addition, DGC is suitable for <b>mini-batch</b> learning <b>compared</b> with that of BP. Moreover, DGC shows greater tolerance to variations in hardware for FCNN <b>compared</b> with BP. In the case of a normal variation with a standard deviation of 0.4, the accuracy of DGC decreases by 1.08%p, whereas the accuracy of BP decreases by 2.41%p. The main issue related to DGC is the relatively slow training speed. If this issue <b>can</b> be resolved, DGC is expected to be an efficient on-chip training method for ...", "dateLastCrawled": "2021-11-08T02:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Improved digital chest tomosynthesis image quality by use of a ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7774945/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7774945", "snippet": "Using the results of optimization verification, DE\u2013VM\u2013VDSR images were generated by setting the number of iterations (k, m) to 30, <b>mini-batch</b> size to 128, epochs to 70, and standard deviation of the domain filter (\u03c3 d) to 1; the simulated nodule contrast (SDNR), ripple artifact (Gumbel distribution), spatial resolution (radial MTF), and noise (NPS) were evaluated and <b>compared</b> with those of the images obtained using conventional algorithms.", "dateLastCrawled": "2022-01-18T00:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Deep Multimodal Learning From MRI and Clinical Data for Early ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8525883/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8525883", "snippet": "A <b>mini-batch</b> Adam algorithm (Kingma and Ba, 2014) was selected to minimize the loss function. The learning rate was selected from empirical values [0.001, 0.01, 0.1, and 0.5]. Batch size was chosen using (Hackman and Farah, 2009; Johnston, 2009; Nordhov et al., 2010; Blencowe et al., 2012). Total number of epochs was 50. These hyperparameters ...", "dateLastCrawled": "2022-02-03T06:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) <b>Simulation analysis of semiconductor manufacturing with</b> <b>small</b> lot ...", "url": "https://www.researchgate.net/publication/221526038_Simulation_analysis_of_semiconductor_manufacturing_with_small_lot_size_and_batch_tool_replacements", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221526038_Simulation_analysis_of...", "snippet": "<b>Small</b> lot sizes and the conversion of batch processes to <b>mini-batch</b> or single-wafer processes are widely regarded as a promising means for a step-wise cycle time reduction. However, there is still ...", "dateLastCrawled": "2021-08-04T14:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Using Web images to train a deep neural network to detect sparsely ...", "url": "https://www.sciencedirect.com/science/article/pii/S1574954121003381", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1574954121003381", "snippet": "The <b>mini-batch</b> size is user-adjustable (typically in binary-number <b>increments</b>, e.g. 2, 4, 8, 16, 32, etc.) depending on the data set. Too <b>small</b> a <b>mini-batch</b> size may provide insufficient image data for the network to meaningfully learn from some or all of the classes within each iteration, while too large a <b>mini-batch</b> size may cause the network to overfit to the training images. Because of the high computational complexity of the deep learning process, the <b>mini-batch</b> size is ultimately ...", "dateLastCrawled": "2022-01-25T20:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "SNR\u2010dependent drone classification using convolutional neural networks ...", "url": "https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/rsn2.12161", "isFamilyFriendly": true, "displayUrl": "https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/rsn2.12161", "snippet": "<b>Measuring</b> the fundamental frequency of the spectral lines gives an estimate of the rotation rate , whilst spectral ... decay rate, and <b>mini batch</b> size of CNNs trained on the MNIST and CIFAR-10 databases. The classifier is trained with a set of initial parameters, and the objective function is calculated. The acquisition function is used to determine the next set of hyperparameters to use for training, based on the estimated objective function. Model performance <b>can</b> be treated as a sample ...", "dateLastCrawled": "2022-01-16T03:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>CROSSBOW: Scaling Deep Learning with Small Batch Sizes</b> on Multi-GPU ...", "url": "https://deepai.org/publication/crossbow-scaling-deep-learning-with-small-batch-sizes-on-multi-gpu-servers", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>crossbow-scaling-deep-learning-with-small-batch-sizes</b>...", "snippet": "The results now highlight the higher performance of SMA <b>compared</b> to TensorFlow\u2019s S-SGD scheme, with up to a 72% reduction in T T A for VGG with 8 GPUs (and 7% for ResNet-32, respectively). A similar improvement <b>can</b> be observed for ResNet-50 in Figure 9(c), with Crossbow achieving a 18% reduction in T T A with 8 GPUs.", "dateLastCrawled": "2021-12-24T01:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Fast <b>and accurate detection of kiwifruit in orchard using improved</b> ...", "url": "https://link.springer.com/article/10.1007/s11119-020-09754-y", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11119-020-09754-y", "snippet": "Coefficients k of 0.7\u20130.9 and 1.1\u20131.3 in <b>increments</b> of 0.1 were selected based on the target edge which <b>can</b> be accurately identified during ... height of the kiwifruit bounding box. The stochastic gradient descent (SGD) was used to train the DY3TNet model with a <b>mini-batch</b> size of 64, and the momentum of the network was set to a fixed value of 0.9 and a weight decay of 0.0005. In this work, a learning rate of 0.001 was applied for all layers in the network. It took about 12 h to perform ...", "dateLastCrawled": "2022-01-17T11:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Small Molecule Accurate Recognition Technology (SMART</b>) to Enhance ...", "url": "https://www.nature.com/articles/s41598-017-13923-x", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41598-017-13923-x", "snippet": "Again, the aforementioned grid-cell approaches 28 are similar to ours in that the shifted grid positions <b>can</b> be thought of as corresponding to the first layer of convolutions, which have <b>small</b> ...", "dateLastCrawled": "2022-02-03T16:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Entropy | Free Full-Text | An Appraisal of <b>Incremental Learning</b> Methods ...", "url": "https://www.mdpi.com/1099-4300/22/11/1190/htm", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/1099-4300/22/11/1190/htm", "snippet": "<b>Compared</b> with other few-shot detection algorithms, the advantage of ONCE is that after training on the basic dataset, the new <b>small</b> sample dataset <b>can</b> be directly used for inference, and the contents of the basic dataset will not be forgotten in this process. iTAML is also an <b>incremental learning</b> algorithm designed based on meta-learning, but it focuses on solving classification tasks.", "dateLastCrawled": "2022-02-01T01:53:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Batch, <b>Mini Batch</b> &amp; Stochastic <b>Gradient Descent</b> | by Sushant Patrikar ...", "url": "https://towardsdatascience.com/batch-mini-batch-stochastic-gradient-descent-7a62ecba642a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/batch-<b>mini-batch</b>-stochastic-<b>gradient-descent</b>-7a62ecba642a", "snippet": "So, after creating the mini-batches of fixed size, we do the following steps in one epoch: Pick a <b>mini-batch</b>. Feed it to Neural Network. Calculate the mean gradient of the <b>mini-batch</b>. Use the mean gradient we calculated in step 3 to update the weights. Repeat steps 1\u20134 for the mini-batches we created.", "dateLastCrawled": "2022-02-02T11:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A.5 <b>Mini-Batch</b> Optimization", "url": "https://jermwatt.github.io/machine_learning_refined/notes/3_First_order_methods/3_11_Minibatch.html", "isFamilyFriendly": true, "displayUrl": "https://jermwatt.github.io/<b>machine</b>_<b>learning</b>_refined/notes/3_First_order_methods/3_11...", "snippet": "The size of the subset used is called the batch-size of the proces e.g., in our description of the <b>mini-batch</b> optimization scheme above we used batch-size = $1$ (<b>mini-batch</b> optimization using a batch-size of $1$ is also often referred to as stochastic optimization). What batch-size works best in practice - in terms of providing the greatest speed up in optimization - varies and is often problem dependent.", "dateLastCrawled": "2022-01-25T15:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Batch vs <b>Mini-batch</b> vs <b>Stochastic Gradient Descent</b> with Code Examples ...", "url": "https://medium.datadriveninvestor.com/batch-vs-mini-batch-vs-stochastic-gradient-descent-with-code-examples-cd8232174e14", "isFamilyFriendly": true, "displayUrl": "https://medium.datadriveninvestor.com/batch-vs-<b>mini-batch</b>-vs-stochastic-gradient...", "snippet": "Batch vs Stochastic vs <b>Mini-batch</b> <b>Gradient Descent</b>. Source: Stanford\u2019s Andrew Ng\u2019s MOOC Deep <b>Learning</b> Course. It is possible to use only the <b>Mini-batch</b> <b>Gradient Descent</b> code to implement all versions of <b>Gradient Descent</b>, you just need to set the <b>mini_batch</b>_size equals one to Stochastic GD or the number of training examples to Batch GD. Thus ...", "dateLastCrawled": "2022-01-27T13:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Gradient Descent: One of <b>Machine</b> <b>Learning</b>\u2019s Most Popular Algorithms ...", "url": "https://urmiparekh.medium.com/gradient-descent-one-of-machine-learnings-most-popular-algorithms-c31963d1e67f", "isFamilyFriendly": true, "displayUrl": "https://urmiparekh.medium.com/gradient-descent-one-of-<b>machine</b>-<b>learning</b>s-most-popular...", "snippet": "<b>Mini-batch</b> Gradient Descent: It computes the gradients on small random sets of instances called as mini-batches. It is most favorable and widely used algorithm which makes precise and faster results using a batch of \u2018m\u2019 training examples. The common <b>mini-batch</b> sizes range between 50 and 256 but it can be vary for different applications.", "dateLastCrawled": "2022-01-17T15:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Gradient</b> Descent: A Quick, Simple Introduction | Built In", "url": "https://builtin.com/data-science/gradient-descent", "isFamilyFriendly": true, "displayUrl": "https://builtin.com/data-science/<b>gradient</b>-descent", "snippet": "Common <b>mini-batch</b> sizes range between 50 and 256, but like any other <b>machine</b> <b>learning</b> technique, there is no clear rule because it varies for different applications. This is the go-to algorithm when training a neural network and it is the most common type of <b>gradient</b> descent within deep <b>learning</b>.", "dateLastCrawled": "2022-02-02T07:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Machine</b> <b>Learning</b> 101: An Intuitive Introduction to <b>Gradient</b> Descent ...", "url": "https://towardsdatascience.com/machine-learning-101-an-intuitive-introduction-to-gradient-descent-366b77b52645", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>machine</b>-<b>learning</b>-101-an-intuitive-introduction-to...", "snippet": "To build a <b>Machine</b> <b>Learning</b> model, we often need at least 3 things. A problem T, a performance measure P, and an experience E, ... In <b>analogy</b>, we can think of <b>Gradient</b> Descent as being a ball rolling down on a valley. The deepest valley is the optimal global minimum and that is the place we aim for. Depending on where the ball starts rolling, it may rest in the bottom of a valley. But not in the lowest one. This is called a local minimum and in the context of our model, the valley is the ...", "dateLastCrawled": "2022-01-30T05:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Introduction to Machine Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/24/introduction-to-machine-learning-2/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/24/<b>introduction-to-machine-learning</b>-2", "snippet": "<b>Machine</b> <b>learning</b> terminology for model building and validation There seems to be an <b>analogy</b> between statistical modeling and <b>machine</b> <b>learning</b> that we will cover in subsequent chapters in depth. However, a quick view has been provided as follows: in statistical modeling, linear regression with two independent variables is trying to fit the best plane with\u2026", "dateLastCrawled": "2022-01-17T05:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Variants of Gradient Descent Optimizer in Deep <b>Learning</b> with Simple <b>Analogy</b>", "url": "https://manasanoolumortha.medium.com/variants-of-gradient-descent-optimizer-in-deep-learning-with-simple-analogy-6f2f59bd2e26", "isFamilyFriendly": true, "displayUrl": "https://manasanoolumortha.medium.com/variants-of-gradient-descent-optimizer-in-deep...", "snippet": "Variants of Gradient Descent Optimizer in Deep <b>Learning</b> with Simple <b>Analogy</b>. Manasa Noolu(Mortha) Jan 9, 2021 \u00b7 5 min read. The role of optimizers is an essential phase in deep <b>learning</b>. It is important to understand the underlying math to decide on appropriate parameters to boost up the accuracy. There are different types of optimizers, however, I am going to explain the variants of the Gradient Descent optimizer with a simple <b>analogy</b>. Sometimes, it is difficult to interpret the ...", "dateLastCrawled": "2022-01-24T21:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>All Unit MCQ questions of ML</b> \u2013 TheCodingShef", "url": "https://thecodingshef.com/all-unit-mcq-questions-of-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://thecodingshef.com/<b>all-unit-mcq-questions-of</b>-<b>machine</b>-<b>learning</b>", "snippet": "<b>Analogy</b>; Deduction; Introduction Correct option is D. Types of <b>learning</b> used in <b>machine</b> Supervised; Unsupervised; Reinforcement; All of these Correct option is D. A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience Supervised <b>learning</b> problem; Un Supervised <b>learning</b> problem; Well posed <b>learning</b> problem; All of these Correct option is C. Which of the ...", "dateLastCrawled": "2022-01-30T22:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and-sample-codes ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/README.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "Model 1: accuracy = 92%. Model 2: accuracy = 95%. Model 2 has higher accuracy than model 1, but model 2 is useless. This is called accuracy paradox, which means the model with higher accuracy may not have better generalization power. In general, when TP &lt; FP, the accuracy will always increase when we change the classifier to always output &#39;negative&#39;.Conversely, when TN &lt; FN, the same will happen when we change the classifier to always output &#39;positive&#39; [1].. Recall@k", "dateLastCrawled": "2021-09-22T11:54:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>full batch vs online learning vs mini batch</b> - <b>Cross Validated</b>", "url": "https://stats.stackexchange.com/questions/110078/full-batch-vs-online-learning-vs-mini-batch", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/110078/<b>full-batch-vs-online-learning</b>-vs-mini...", "snippet": "a) full-batch <b>learning</b>. b) online-<b>learning</b> where for every iteration we randomly pick a training case. c) mini-batch <b>learning</b> where for every iteration we randomly pick 100 training cases. The answer is b. But I wonder why c is wrong. Isn&#39;t online-<b>learning</b> a special case of mini-batch where each iteration contains only a single training case?", "dateLastCrawled": "2022-01-24T13:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Initialisation, Normalisation, Dropout", "url": "https://www.inf.ed.ac.uk/teaching/courses/mlp/2019-20/lectures/mlp06-enc.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.inf.ed.ac.uk/teaching/courses/mlp/2019-20/lectures/mlp06-enc.pdf", "snippet": "<b>Machine</b> <b>Learning</b> Practical | MLP Lecture 6 22 October 2019 MLP Lecture 6 / 22 October 2019 Initialisation, Normalisation, Dropout1. Recap: Vanishing/exploding gradients z(1) = W(1)x, h(1) = f(z(1)) and y = h(L) Assuming f is identity mapping, y = W(L)W(L 1):::W(2)W(1)x W(l) = &quot; 2 0 0 2 #! y = W(L) &quot; 2 0 0 2 # L 1 x (Exploding gradients) W(l) = &quot;:5 0 0 :5 #! y = W(L) &quot;:5 0 0 :5 # L 1 x (Vanishing gradients) MLP Lecture 6 / 22 October 2019 Initialisation, Normalisation, Dropout2. Recap ...", "dateLastCrawled": "2022-01-31T14:01:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine Learning</b> | Ordinary Least Squares | Mathematical Optimization", "url": "https://www.scribd.com/document/429447261/Machine-Learning", "isFamilyFriendly": true, "displayUrl": "https://www.scribd.com/document/429447261/<b>Machine-Learning</b>", "snippet": "<b>Machine Learning</b>", "dateLastCrawled": "2021-11-04T20:26:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "sgd-bias-variance.pdf - S&amp;DS 355 555 Introductory <b>Machine</b> <b>Learning</b> ...", "url": "https://www.coursehero.com/file/80854564/sgd-bias-variancepdf/", "isFamilyFriendly": true, "displayUrl": "https://www.coursehero.com/file/80854564/sgd-bias-variancepdf", "snippet": "View sgd-bias-variance.pdf from S&amp;DS 355 at Yale University. S&amp;DS 355 / 555 Introductory <b>Machine</b> <b>Learning</b> Stochastic Gradient Descent and Bias-Variance Tradeoffs September 22 Goings on \u2022 Nothing", "dateLastCrawled": "2021-12-06T21:41:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(mini-batch)  is like +(measuring in small increments)", "+(mini-batch) is similar to +(measuring in small increments)", "+(mini-batch) can be thought of as +(measuring in small increments)", "+(mini-batch) can be compared to +(measuring in small increments)", "machine learning +(mini-batch AND analogy)", "machine learning +(\"mini-batch is like\")", "machine learning +(\"mini-batch is similar\")", "machine learning +(\"just as mini-batch\")", "machine learning +(\"mini-batch can be thought of as\")", "machine learning +(\"mini-batch can be compared to\")"]}