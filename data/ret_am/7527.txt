{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Optimization</b> - Control Theory", "url": "https://fab.cba.mit.edu/classes/865.21/topics/control/08_optimization.html", "isFamilyFriendly": true, "displayUrl": "https://fab.cba.mit.edu/classes/865.21/topics/control/08_<b>optimization</b>.html", "snippet": "<b>Convex</b> <b>Optimization</b> by Boyd and Vandenberghe is the gold standard by which all other (<b>convex</b>) <b>optimization</b> books are judged. Numerical Recipes in C is a great resource that focuses on how to implement these (and many other algorithms) efficiently, rather than on the theory behind them.", "dateLastCrawled": "2022-02-02T14:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Convex</b> <b>optimization</b> with an interpolation-based projection and ...", "url": "https://www.researchgate.net/publication/353345360_Convex_optimization_with_an_interpolation-based_projection_and_its_application_to_deep_learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/353345360_<b>Convex</b>_<b>optimization</b>_with_an...", "snippet": "<b>Convex</b> <b>optimization</b> with an in terpolation\u2011based projection . and its application to deep learning. Riad Akrour 1 \u00b7 Asma Atamna 2 \u00b7 Jan Peters 1. Received: 16 November 2020 / Revised: 2 May ...", "dateLastCrawled": "2021-08-11T01:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Linear and <b>Convex</b> <b>Optimization</b>: A Mathematical Approach [1&amp;nbsp;ed ...", "url": "https://ebin.pub/linear-and-convex-optimization-a-mathematical-approach-1nbsped-1119664047-9781119664048.html", "isFamilyFriendly": true, "displayUrl": "https://ebin.pub/linear-and-<b>convex</b>-<b>optimization</b>-a-mathematical-approach-1nbsped...", "snippet": "<b>Convex</b> <b>optimization</b> is not included in many books at this level. However, in the past three decades new algorithms and many new applications have increased interest in <b>convex</b> <b>optimization</b>. <b>Like</b> linear programming, large applied problems can now be solved efficiently and reliably. Conceptually, <b>convex</b> programming fits better with linear programming than with general nonlinear programming. These types of <b>optimization</b> are also appropriate for this book because they have a clear theory and ...", "dateLastCrawled": "2022-02-03T04:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "What jobs are related <b>to convex optimization, discrete optimization</b> ...", "url": "https://www.quora.com/What-jobs-are-related-to-convex-optimization-discrete-optimization-and-algorithms-What-additional-skills-do-I-need-for-these-jobs", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-jobs-are-related-to-<b>convex</b>-<b>optimization</b>-discrete...", "snippet": "Answer (1 of 4): This is an interesting question. I started my graduate study in operations research and slowly started taking <b>optimization</b> (or <b>convex</b> <b>optimization</b>) courses. I took about 10 courses just in <b>optimization</b> because well it was very interesting. Only later I slowly found out that optim...", "dateLastCrawled": "2022-01-22T11:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Minimization-Maximization Problems: Applications (in Communication ...", "url": "https://people.ece.umn.edu/~mhong/ctw2019.pdf", "isFamilyFriendly": true, "displayUrl": "https://people.ece.umn.edu/~mhong/ctw2019.pdf", "snippet": "Max-Min SNR <b>optimization</b> [Zander, 1992] [Foschini and Gans, 1998] ... sequence of) <b>convex</b> problems <b>like</b> SDP/SOCP For non-<b>convex</b> problems, two popular ways in literature 1 Approximate the mini-max objective 2 Translate to an \\envelop form&quot; Mingyi Hong (University of Minnesota) Minimization-Maximization Problems: Applications (in Communication), Challenges and AlgorithmsMay 31, 2019 8 / 39. Min-max problems and motivation Popular Approximation for Non-<b>Convex</b> Min-Max One way to simplify is to ...", "dateLastCrawled": "2021-12-08T18:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "What is the role of <b>convex</b> <b>optimization</b> in power allocation problems in ...", "url": "https://www.quora.com/What-is-the-role-of-convex-optimization-in-power-allocation-problems-in-wireless-communication-say-for-example-OFDM-based-systems", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-the-role-of-<b>convex</b>-<b>optimization</b>-in-power-allocation...", "snippet": "Answer (1 of 2): Allocation problem of any sort asks for <b>optimization</b>. In this particular question, we are dealing with power allocation, and since you took OFDM-based systems as an example, I&#39;ll stick to that. In a Cognitive Radio system, you&#39;d <b>like</b> to have users sending certain power to certai...", "dateLastCrawled": "2021-12-22T10:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Neural Network</b> <b>Training</b> <b>Is Like</b> Lock Picking - Cross Validated", "url": "https://stats.stackexchange.com/questions/352036/what-should-i-do-when-my-neural-network-doesnt-learn", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/352036", "snippet": "Non-<b>convex</b> <b>optimization</b> is hard. The objective function of a <b>neural network</b> is only <b>convex</b> when there are no hidden units, all activations are linear, and the design matrix is full-rank -- because this configuration is identically an ordinary regression problem. In all other cases, the <b>optimization</b> problem is non-<b>convex</b>, and non-<b>convex</b> <b>optimization</b> is hard. The challenges of <b>training</b> neural networks are well-known (see: Why is it hard to train deep neural networks?). Additionally, neural ...", "dateLastCrawled": "2022-01-30T03:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Courses - natcor.ac.uk", "url": "http://www.natcor.ac.uk/courses/", "isFamilyFriendly": true, "displayUrl": "www.natcor.ac.uk/courses", "snippet": "In day three, the course covers the automated design of algorithms in the continuous and discrete spaces including topics <b>like</b> <b>fitness</b> landscape analysis, generalised pattern search and frameworks for algorithm design. The day ends with a session on introduction to evolutionary algorithms. Day four of the course starts with sessions on hyper-heuristic techniques, their origins, classification and insights into different types of hyper-heuristics. The second part of this day is dedicated to a ...", "dateLastCrawled": "2022-01-31T05:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Optimization problems and algorithms</b> | <b>Udemy</b>", "url": "https://www.udemy.com/course/optimisation/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.udemy.com</b>/course/optimisation", "snippet": "<b>Fitness</b> General Health Sports Nutrition &amp; Diet Yoga Mental Health Martial Arts &amp; Self Defense Safety &amp; First Aid Dance Meditation Other Health &amp; <b>Fitness</b>. Music. Instruments Music Production Music Fundamentals Vocal Music Techniques Music Software Other Music. Teaching &amp; Academics. Engineering Humanities Math Science Online Education Social Science Language Learning Teacher <b>Training</b> Test Prep Other Teaching &amp; Academics. Web Development JavaScript React CSS Angular Node.Js HTML5 PHP Django ...", "dateLastCrawled": "2022-02-02T17:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Non-<b>Convex Loss Function in Deep Learning Is</b> a Big Deal? - Artificial ...", "url": "https://ai.stackexchange.com/questions/4271/non-convex-loss-function-in-deep-learning-is-a-big-deal", "isFamilyFriendly": true, "displayUrl": "https://ai.stackexchange.com/questions/4271/non-<b>convex-loss-function-in-deep-learning</b>...", "snippet": "Can I know if it is a big deal in deep learning? Is <b>training</b> a deep network, when loss function is <b>convex</b>, the same as optimizing a <b>convex</b> problem or not? I would be thankful if any paper has addressed this issue. deep-learning. Share. Improve this question. Follow asked Oct 14 &#39;17 at 16:25. Katatonia Katatonia. 441 2 2 silver badges 9 9 bronze badges $\\endgroup$ 0. Add a comment | 1 Answer Active Oldest Votes. 2 $\\begingroup$ <b>Optimization</b>. In <b>optimization</b>, the loss function (sometimes ...", "dateLastCrawled": "2022-01-20T01:41:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "What jobs are related <b>to convex optimization, discrete optimization</b> ...", "url": "https://www.quora.com/What-jobs-are-related-to-convex-optimization-discrete-optimization-and-algorithms-What-additional-skills-do-I-need-for-these-jobs", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-jobs-are-related-to-<b>convex</b>-<b>optimization</b>-discrete...", "snippet": "Answer (1 of 4): This is an interesting question. I started my graduate study in operations research and slowly started taking <b>optimization</b> (or <b>convex</b> <b>optimization</b>) courses. I took about 10 courses just in <b>optimization</b> because well it was very interesting. Only later I slowly found out that optim...", "dateLastCrawled": "2022-01-22T11:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Optimization</b> - Control Theory", "url": "https://fab.cba.mit.edu/classes/865.21/topics/control/08_optimization.html", "isFamilyFriendly": true, "displayUrl": "https://fab.cba.mit.edu/classes/865.21/topics/control/08_<b>optimization</b>.html", "snippet": "Particle swarm <b>optimization</b> <b>is similar</b> to CMA-ES, but instead of drawing samples from some distribution, each sample moves around somewhat independently. Samples are treated as particles with position and momentum. They move downhill based on local information (their recent evaluations), but also on some shared knowledge about the best solutions found so far by any other particles. Gradient Based Methods . When applicable, gradient based methods can converge much faster than gradient free ...", "dateLastCrawled": "2022-02-02T14:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "data_mining_experiments/CVXMOD \u2013 <b>Convex</b> <b>optimization</b> software in Python ...", "url": "https://github.com/bieli/data_mining_experiments/blob/master/theory/CVXMOD%20%E2%80%93%20Convex%20optimization%20software%20in%20Python.html", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/bieli/data_mining_experiments/blob/master/theory/CVXMOD \u2013 <b>Convex</b>...", "snippet": "data mining experiments - simple examples, materials and archives - data_mining_experiments/CVXMOD \u2013 <b>Convex</b> <b>optimization</b> software in Python.html at master \u00b7 bieli/data_mining_experiments", "dateLastCrawled": "2021-09-15T17:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Eduard Gorbunov", "url": "https://eduardgorbunov.github.io/index.html", "isFamilyFriendly": true, "displayUrl": "https://eduardgorbunov.github.io/index.html", "snippet": "Abstract: Motivated by recent increased interest in <b>optimization</b> algorithms for non-<b>convex</b> <b>optimization</b> in application to <b>training</b> deep neural networks and other <b>optimization</b> problems in data analysis, we give an overview of recent theoretical results on global performance guarantees of <b>optimization</b> algorithms for non-<b>convex</b> <b>optimization</b>. We start with classical arguments showing that general non-<b>convex</b> problems could not be solved efficiently in a reasonable time. Then we give a list of ...", "dateLastCrawled": "2022-01-29T08:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Convex ensemble learning with sparsity and diversity</b> - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S1566253513001413", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1566253513001413", "snippet": "We prove that the final <b>convex</b> <b>optimization</b> leads to a closed-form solution. The main contributions of this work are summarized as follows. First, we present a general mathematical framework for learning both sparsity and diversity in classifier ensemble. Unlike conventional methods with implicit notes of sparsity or/and diversity, our approach explicitly combines and optimizes both in a unified learning model. Second, derived from the error-ambiguity decomposition, the sparsity and ...", "dateLastCrawled": "2021-12-01T18:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A new <b>fitness</b> estimation strategy for <b>particle swarm optimization</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0020025512006238", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0020025512006238", "snippet": "FESPSO, a new <b>fitness</b> estimation strategy, is proposed for <b>particle swarm optimization</b> to reduce the number of <b>fitness</b> evaluations, thereby reducing the computational cost. Different from most existing approaches which either construct an approximate model using data or utilize the idea of <b>fitness</b> inheritance, FESPSO estimates the <b>fitness</b> of a particle based on its positional relationship with other particles. More precisely, Once the <b>fitness</b> of a particle is known, either estimated or ...", "dateLastCrawled": "2022-01-06T07:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Non-<b>Convex Loss Function in Deep Learning Is</b> a Big Deal? - Artificial ...", "url": "https://ai.stackexchange.com/questions/4271/non-convex-loss-function-in-deep-learning-is-a-big-deal", "isFamilyFriendly": true, "displayUrl": "https://ai.stackexchange.com/questions/4271/non-<b>convex-loss-function-in-deep-learning</b>...", "snippet": "Can I know if it is a big deal in deep learning? Is <b>training</b> a deep network, when loss function is <b>convex</b>, the same as optimizing a <b>convex</b> problem or not? I would be thankful if any paper has addressed this issue. deep-learning. Share. Improve this question. Follow asked Oct 14 &#39;17 at 16:25. Katatonia Katatonia. 441 2 2 silver badges 9 9 bronze badges $\\endgroup$ 0. Add a comment | 1 Answer Active Oldest Votes. 2 $\\begingroup$ <b>Optimization</b>. In <b>optimization</b>, the loss function (sometimes ...", "dateLastCrawled": "2022-01-20T01:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Convex</b> Hull-Based Multi-objective Genetic Programming for Maximizing ...", "url": "https://deepai.org/publication/convex-hull-based-multi-objective-genetic-programming-for-maximizing-roc-performance", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>convex</b>-hull-based-multi-objective-genetic-programming...", "snippet": "A selection procedure is described that can be efficiently implemented and follows <b>similar</b> design principles than classical hyper-volume based <b>optimization</b> algorithms. It is hypothesized that by using a tailored indicator-based selection scheme CH-MOGP gets more efficient for ROC <b>convex</b> hull approximation than algorithms which compute all Pareto optimal points. To test our hypothesis we compare the new CH-MOGP to MOGP with classical selection schemes, including NSGA-II, MOEA/D) and SMS-EMOA ...", "dateLastCrawled": "2021-12-06T20:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Neural Network</b> <b>Training</b> Is Like Lock Picking - Cross Validated", "url": "https://stats.stackexchange.com/questions/352036/what-should-i-do-when-my-neural-network-doesnt-learn", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/352036", "snippet": "A <b>similar</b> phenomenon also arises in another context, with a different solution. When <b>training</b> triplet networks, <b>training</b> with online hard negative mining immediately risks model collapse, so people train with semi-hard negative mining first as a kind of &quot;pre <b>training</b>.&quot; This is an easier task, so the model learns a good initialization before ...", "dateLastCrawled": "2022-01-30T03:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "When <b>should I use Metaheuristics optimization</b>? - Quora", "url": "https://www.quora.com/When-should-I-use-Metaheuristics-optimization", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/When-<b>should-I-use-Metaheuristics-optimization</b>", "snippet": "Answer (1 of 4): <b>Optimization</b> is very powerful in its formal strength, but you need to constrain the search space really well. Often, one has no idea how to do that nearly completely. As a result, we often use heuristics to save computer cycles in the war against combinatorial explosion, or simpl...", "dateLastCrawled": "2022-01-24T22:41:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Does gradient descent in deep learning assume a smooth <b>fitness</b> ...", "url": "https://ai.stackexchange.com/questions/27231/does-gradient-descent-in-deep-learning-assume-a-smooth-fitness-landscape", "isFamilyFriendly": true, "displayUrl": "https://ai.stackexchange.com/questions/27231/does-gradient-descent-in-deep-learning...", "snippet": "$\\begingroup$ You say &quot;I&#39;ve come across the concept of <b>fitness</b> landscape before&quot;. <b>Can</b> you please provide the link to the research papers or books that use this term &quot;<b>fitness</b> landscape&quot;. I think this term occurs more in the context of evolutionary algorithms and not deep learning. $\\endgroup$ \u2013 nbro \u2666. Apr 9 &#39;21 at 12:57 $\\begingroup$ @nbro: You are right. It does occur mainly in the evolutionary context. I just <b>thought</b> the concept might still be applicable. $\\endgroup$ \u2013 Joebevo. Apr 9 ...", "dateLastCrawled": "2022-01-26T09:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "[<b>MCQ] Soft Computing</b> - Last Moment Tuitions", "url": "https://lastmomenttuitions.com/mcq-soft-computing/", "isFamilyFriendly": true, "displayUrl": "https://lastmomenttuitions.com/<b>mcq-soft-computing</b>", "snippet": "26.Membership function <b>can</b> <b>be thought</b> of as a technique to solve empirical problems on the basis of a) knowledge b) example c) learning d) experience Ans: D. 27.Three main basic features involved in characterizing membership function are a)Intution, Inference, Rank Ordering b)Fuzzy Algorithm, Neural network, Genetic Algorithm c)Core, Support , Boundary d)Weighted Average, center of Sums, Median Ans : C. 28. A fuzzy set whose membership function has at least one element x in the universe ...", "dateLastCrawled": "2022-02-02T15:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Introduction and background to <b>optimization</b> theory - Book chapter ...", "url": "https://iopscience.iop.org/book/978-0-7503-2404-5/chapter/bk978-0-7503-2404-5ch1", "isFamilyFriendly": true, "displayUrl": "https://iopscience.iop.org/book/978-0-7503-2404-5/chapter/bk978-0-7503-2404-5ch1", "snippet": "Conic <b>optimization</b> and interior point method <b>optimization</b> are examples of <b>convex</b> theory based <b>optimization</b>. <b>Convex</b> itself means to achieve a suitable objective function that <b>can</b> help in obtaining a local optimal function in a problem. This <b>optimization</b> concept is used in obtaining both global and local optima. Several engineering applications employ <b>convex</b> <b>optimization</b> methods. Linear sub-space, second order cone and Affine sub-space are a few major examples of the <b>convex</b> functions used for ...", "dateLastCrawled": "2021-12-28T21:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Convex Analysis and Nonlinear Optimization: Theory and Examples</b> ...", "url": "https://www.researchgate.net/publication/236736832_Convex_Analysis_and_Nonlinear_Optimization_Theory_and_Examples", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/236736832_<b>Convex</b>_Analysis_and_Nonlinear...", "snippet": "Since the <b>convex</b> conjugate is an involution (\u03c8 * ) * = \u03c8 by the Moreau biconjugation theorem [20], we <b>can</b> obtain a similar <b>optimization</b> for \u03c8(\u03b2) = sup \u03b7 \u03b2 \u00b7 \u03b7 \u2212 \u03c8 * (\u03b7). This leads to ...", "dateLastCrawled": "2021-11-08T18:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Linear and <b>Convex</b> <b>Optimization</b>: A Mathematical Approach [1&amp;nbsp;ed ...", "url": "https://ebin.pub/linear-and-convex-optimization-a-mathematical-approach-1nbsped-1119664047-9781119664048.html", "isFamilyFriendly": true, "displayUrl": "https://ebin.pub/linear-and-<b>convex</b>-<b>optimization</b>-a-mathematical-approach-1nbsped...", "snippet": "Preface This book is about <b>optimization</b> problems that arise in the field of operations research, including linear <b>optimization</b> (continuous and discrete) and <b>convex</b> programming. Linear programming plays a central role because these problems <b>can</b> be solved very efficiently; it also has useful connections with discrete and <b>convex</b> <b>optimization</b>. <b>Convex</b> <b>optimization</b> is not included in many books at this level. However, in the past three decades new algorithms and many new applications have ...", "dateLastCrawled": "2022-02-03T04:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Additional Exercises For Convex Optimization Boyd Solutions</b> PDF ...", "url": "https://sites.google.com/a/mariumvit.tech/theocritushwan/additional-exercises-for-convex-optimization-boyd-solutions", "isFamilyFriendly": true, "displayUrl": "https://<b>sites.google.com</b>/.../<b>additional-exercises-for-convex-optimization-boyd-solutions</b>", "snippet": "<b>Additional Exercises For Convex Optimization Boyd Solutions PDF Download</b>. Is that <b>Additional Exercises For Convex Optimization Boyd Solutions PDF Download</b> readers influence the future? Of course yes. <b>Additional Exercises For Convex Optimization Boyd Solutions PDF Download</b> Gives the readers many references and knowledge that bring positive influence in the future.<b>Additional Exercises For Convex Optimization Boyd Solutions PDF Download</b> Gives the readers good spirit. Although the content of ...", "dateLastCrawled": "2021-12-22T22:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Learning with <b>Submodular Functions: A Convex Optimization Perspective</b>", "url": "https://www.researchgate.net/publication/339502686_Learning_with_Submodular_Functions_A_Convex_Optimization_Perspective", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/339502686_Learning_with_Submodular_Functions...", "snippet": "Such <b>optimization</b> algorithms have important applications in many areas of computer science and applied mathematics such as <b>training</b> deep neural networks [5], design of online experiments [6], or ...", "dateLastCrawled": "2021-12-23T01:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "What jobs are related <b>to convex optimization, discrete optimization</b> ...", "url": "https://www.quora.com/What-jobs-are-related-to-convex-optimization-discrete-optimization-and-algorithms-What-additional-skills-do-I-need-for-these-jobs", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-jobs-are-related-to-<b>convex</b>-<b>optimization</b>-discrete...", "snippet": "Answer (1 of 4): This is an interesting question. I started my graduate study in operations research and slowly started taking <b>optimization</b> (or <b>convex</b> <b>optimization</b>) courses. I took about 10 courses just in <b>optimization</b> because well it was very interesting. Only later I slowly found out that optim...", "dateLastCrawled": "2022-01-22T11:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Neural Network</b> <b>Training</b> Is Like Lock Picking - Cross Validated", "url": "https://stats.stackexchange.com/questions/352036/what-should-i-do-when-my-neural-network-doesnt-learn", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/352036", "snippet": "Non-<b>convex</b> <b>optimization</b> is hard. The objective function of a <b>neural network</b> is only <b>convex</b> when there are no hidden units, all activations are linear, and the design matrix is full-rank -- because this configuration is identically an ordinary regression problem. In all other cases, the <b>optimization</b> problem is non-<b>convex</b>, and non-<b>convex</b> <b>optimization</b> is hard. The challenges of <b>training</b> neural networks are well-known (see: Why is it hard to train deep neural networks?). Additionally, neural ...", "dateLastCrawled": "2022-01-30T03:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Explanation of why Neural Networks are non <b>convex</b> ~ Data Science ...", "url": "https://answerbun.com/data-science/explanation-of-why-neural-networks-are-non-convex/", "isFamilyFriendly": true, "displayUrl": "https://answerbun.com/data-science/explanation-of-why-neural-networks-are-non-<b>convex</b>", "snippet": "I don\u2019t remember where I heard this arguement but it was a pretty interesting one and I couldn\u2019t remember the entire reasoning and so as I began to think about it, I figured some of it out and I believe it is pretty reasonable.", "dateLastCrawled": "2022-01-27T18:18:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Convex</b> <b>optimization</b> with an interpolation-based projection and ...", "url": "https://www.researchgate.net/publication/353345360_Convex_optimization_with_an_interpolation-based_projection_and_its_application_to_deep_learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/353345360_<b>Convex</b>_<b>optimization</b>_with_an...", "snippet": "<b>convex</b> <b>optimization</b> algorithms <b>can</b> be recast as problems sol vable by Algorithm 1. To prov e converg ence of Algorithm 1, we need an additional assumption on the bound-edness of the distance to an ...", "dateLastCrawled": "2021-08-11T01:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Trade-off curve, <b>convex</b> hull, and <b>fitness</b> calculation | Download ...", "url": "https://www.researchgate.net/figure/Trade-off-curve-convex-hull-and-fitness-calculation_fig2_248880267", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/figure/Trade-off-curve-<b>convex</b>-hull-and-<b>fitness</b>...", "snippet": "Fig. 2 shows the <b>convex</b> hull and the Pareto front. The <b>convex</b> hull is derived from the Pareto front. Based on the study of Feng et al. 1997, <b>fitness</b> for each chromosome equals the shortest ...", "dateLastCrawled": "2022-02-03T11:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Optimization</b> - Control Theory", "url": "https://fab.cba.mit.edu/classes/865.21/topics/control/08_optimization.html", "isFamilyFriendly": true, "displayUrl": "https://fab.cba.mit.edu/classes/865.21/topics/control/08_<b>optimization</b>.html", "snippet": "Usually when <b>training</b> a neural net, you\u2019re trying to optimize performance over your whole <b>training</b> dataset. So technically speaking evaluating the gradient, or even just the objective function, would require testing every single sample. This would take far too long. So instead, the gradient and objective are evaluated on small batches of randomly selected samples (often a few dozen out of millions). This makes things faster, and <b>can</b> actually help convergence since the noise <b>can</b> allow the ...", "dateLastCrawled": "2022-02-02T14:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Reinforcement Learning with Particle Swarm <b>Optimization</b> Policy (PSO-P ...", "url": "https://www.gwern.net/docs/reinforcement-learning/2016-hein.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.gwern.net/docs/reinforcement-learning/2016-hein.pdf", "snippet": "with respect to the <b>fitness</b> function f s I T t: \u22c5 \u2192 with f s s t t ( )x x= ( , ). Figure 1 illustrates the process of computing f s t ( )x. THe PSO-POLICy FRAMewORK The PSO algorithm is a population-based, stochastic <b>optimization</b> heuristic for solving non-<b>convex</b> <b>optimization</b> problems (Kennedy &amp; Eberhart, 1995). Generally, PSO <b>can</b> operate on ...", "dateLastCrawled": "2022-01-30T12:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A novel hybrid soft computing <b>optimization</b> framework for dynamic ...", "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0261709", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0261709", "snippet": "Design <b>optimization</b> issues <b>can</b> be classified in a variety of ways. One way to categorize them is into two groups that are functional and combinatorial <b>optimization</b>. The objective function in functional <b>optimization</b> is typically expressed as a continuous or piece-wise continuous function of the design parameters . It is basically a search engine and a technique for optimizing designs. It was first used to model natural evolutionary development in a computing environment. GA is a set of ...", "dateLastCrawled": "2022-01-31T01:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "What is the role of <b>convex</b> <b>optimization</b> in power allocation problems in ...", "url": "https://www.quora.com/What-is-the-role-of-convex-optimization-in-power-allocation-problems-in-wireless-communication-say-for-example-OFDM-based-systems", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-the-role-of-<b>convex</b>-<b>optimization</b>-in-power-allocation...", "snippet": "Answer (1 of 2): Allocation problem of any sort asks for <b>optimization</b>. In this particular question, we are dealing with power allocation, and since you took OFDM-based systems as an example, I&#39;ll stick to that. In a Cognitive Radio system, you&#39;d like to have users sending certain power to certai...", "dateLastCrawled": "2021-12-22T10:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Online <b>convex</b> combination of ranking models | SpringerLink", "url": "https://link.springer.com/article/10.1007%2Fs11257-021-09306-7", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11257-021-09306-7", "snippet": "We argued in Sect. 1 that given the moderate dimensionality of the <b>convex</b> combination, black-box <b>optimization</b> algorithms <b>can</b> be good candidates to optimize combination weights of ranking models. There are certain properties of the environment that affect adversely the applicability of these algorithms, including the non-continuity of the ranking measures, the non-stationarity of the online scenario, and a fair amount noise in the evaluation. Stochastic approximation (SA) algorithms <b>can</b> deal ...", "dateLastCrawled": "2022-02-01T08:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Integrated <b>optimization</b> algorithm: A metaheuristic approach for ...", "url": "https://www.sciencedirect.com/science/article/pii/S0020025521011658", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0020025521011658", "snippet": "Algorithm 1: Integrated <b>Optimization</b> Algorithm; Input 1: lc = dimension, lsp = 1, v = 0.9, Input 2: Random solutions for search agents, Output: Solution of the best search agent: for each iteration do: : 1) Each leader compares itself to the central point and makes a movement if a better solution is found. : 2) Single round coordinate descent operations are carried out by the leaders. 3) for each follower or wanderer do using CPU multithreading: if search agent is follower: : ; Next solution ...", "dateLastCrawled": "2022-01-24T00:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Theory and practice of <b>hyperparameter tuning</b> with Apache Ignite ML | by ...", "url": "https://zaleslaw.medium.com/theory-and-practice-of-hyperparameter-tuning-with-apache-ignite-ml-2059d2f50329", "isFamilyFriendly": true, "displayUrl": "https://zaleslaw.medium.com/theory-and-practice-of-<b>hyperparameter-tuning</b>-with-apache...", "snippet": "Hyperparameter <b>optimization</b> <b>can</b> be formulated as a bilevel (two-level) <b>optimization</b> problem, where the optimal parameters on the <b>training</b> set depend on the hyperparameters. Let w denote parameters (e.g., weights and biases) and \u03bb denote hyperparameters (e.g., dropout probability). Let L_t and L_v be functions mapping parameters and hyperparameters to <b>training</b> and validation losses, respectively. We aim to solve. subjects to: where: From . A bilevel <b>optimization</b> problem consists of two sub ...", "dateLastCrawled": "2022-01-29T01:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "The Curious Case of <b>Convex</b> Neural Networks - ECML PKDD 2021", "url": "https://www.readkong.com/page/the-curious-case-of-convex-neural-networks-ecml-pkdd-2021-1894706", "isFamilyFriendly": true, "displayUrl": "https://www.readkong.com/page/the-curious-case-of-<b>convex</b>-neural-networks-ecml-pkdd...", "snippet": "B <b>Optimization</b> Algorithm for <b>Training</b> IOC-NNs The only architectural constraint in designing an IOC-NN is the choice of a <b>convex</b> and non-decreasing activation function. Furthermore, any feed-forward neural architecture <b>can</b> be trained as IOC-NN by adding two steps to the <b>optimization</b> algorithm. For ex- ample, the constrained version of the vanilla stochastic gradient descent algorithm is shown in Algorithm 1. is used to bring down the value of the updated weights post exponentiation for ...", "dateLastCrawled": "2022-01-17T18:19:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Optimization</b> for <b>Machine</b> <b>Learning</b>", "url": "http://optml.mit.edu/talks/pkuLectAlgo3.pdf", "isFamilyFriendly": true, "displayUrl": "optml.mit.edu/talks/pkuLectAlgo3.pdf", "snippet": "<b>Optimization</b> for <b>Machine</b> <b>Learning</b> \u2013 Sra, Nowozin, Wright Theory of <b>Convex</b> <b>Optimization</b> for <b>Machine</b> <b>Learning</b> \u2013 Bubeck NIPS 2016 <b>Optimization</b> Tutorial \u2013 Bach, Sra Some related courses: EE227A, Spring 2013, (Sra, UC Berkeley) 10-801, Spring 2014 (Sra, CMU) EE364a,b (Boyd, Stanford) EE236b,c (Vandenberghe, UCLA) Venues: NIPS, ICML, UAI, AISTATS, SIOPT, Math. Prog. Suvrit Sra(suvrit@mit.edu)<b>Optimization</b> for <b>Machine</b> <b>Learning</b> 2 / 29. Lecture Plan \u2013Introduction (3 lectures) \u2013Problems and ...", "dateLastCrawled": "2021-08-29T23:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Basic Concepts in Machine Learning</b>", "url": "https://machinelearningmastery.com/basic-concepts-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>basic-concepts-in-machine-learning</b>", "snippet": "For example combinatorial <b>optimization</b>, <b>convex</b> <b>optimization</b>, constrained <b>optimization</b>. All <b>machine learning</b> algorithms are combinations of these three components. A framework for understanding all algorithms. Types of <b>Learning</b> . There are four types of <b>machine learning</b>: Supervised <b>learning</b>: (also called inductive <b>learning</b>) Training data includes desired outputs. This is spam this is not, <b>learning</b> is supervised. Unsupervised <b>learning</b>: Training data does not include desired outputs. Example is ...", "dateLastCrawled": "2022-02-02T15:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Introduction to Machine Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/24/introduction-to-machine-learning-2/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/24/<b>introduction-to-machine-learning</b>-2", "snippet": "<b>Optimization</b> methods are applied to minimize the loss function by changing the parameter values, which is the central theme of <b>machine</b> <b>learning</b>.Zero-one loss is L0-1 = 1 (m &lt;= 0); in zero-one loss, value of loss is 0 for m &gt;= 0 whereas 1 for m &lt; 0. The difficult part with this loss is it is not differentiable, non-<b>convex</b>, and also NP-hard. Hence, in order to make <b>optimization</b> feasible and solvable, these losses are replaced by different surrogate losses for different problems.", "dateLastCrawled": "2022-01-17T05:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "11.2. <b>Convexity</b> \u2014 Dive into Deep <b>Learning</b> 0.17.2 documentation", "url": "https://d2l.ai/chapter_optimization/convexity.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_<b>optimization</b>/<b>convexity</b>.html", "snippet": "Furthermore, even though the <b>optimization</b> problems in deep <b>learning</b> are generally nonconvex, they often exhibit some properties of <b>convex</b> ones near local minima. This can lead to exciting new <b>optimization</b> variants such as [Izmailov et al., 2018].", "dateLastCrawled": "2022-01-30T21:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Optimization</b> for deep <b>learning</b>: an overview", "url": "https://www.ise.ncsu.edu/fuzzy-neural/wp-content/uploads/sites/9/2022/01/Optimization-for-deep-learning.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.ise.ncsu.edu/.../uploads/sites/9/2022/01/<b>Optimization</b>-for-deep-<b>learning</b>.pdf", "snippet": "timization problems beyond <b>convex</b> problems. A somewhat related <b>analogy</b> is the development of conic <b>optimization</b>: in 1990\u2019s, researchers realized that many seemingly non-<b>convex</b> problems can actually be reformulated as conic <b>optimization</b> problems (e.g. semi-de nite programming) which are <b>convex</b> problems, thus the boundary of tractability has advanced signi cantly. Neural network problems are surely not the worst non-<b>convex</b> <b>optimization</b> problems and their global optima could be found ...", "dateLastCrawled": "2022-01-19T03:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Regularization : What? Why? and How? (Part -1) | by Siddhant Rai ...", "url": "https://medium.com/mlearning-ai/regularization-what-why-and-how-part-1-ef6bdb6bafea", "isFamilyFriendly": true, "displayUrl": "https://medium.com/m<b>learning</b>-ai/regularization-what-why-and-how-part-1-ef6bdb6bafea", "snippet": "In general <b>machine</b> <b>learning</b> sense, it is solving an objective function to perform maximum or minimum evaluation. In reality, <b>optimization</b> is lot more profound in usage. Then we have two terms ...", "dateLastCrawled": "2022-01-28T00:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Gradient</b> Descent: A Quick, Simple Introduction | Built In", "url": "https://builtin.com/data-science/gradient-descent", "isFamilyFriendly": true, "displayUrl": "https://builtin.com/data-science/<b>gradient</b>-descent", "snippet": "<b>Gradient</b> descent is an <b>optimization</b> algorithm that&#39;s used when training a <b>machine</b> <b>learning</b> model. It&#39;s based on a <b>convex</b> function and tweaks its parameters iteratively to minimize a given function to its local minimum.", "dateLastCrawled": "2022-02-02T07:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Summary of Thesis: <b>Non-convex Optimization for Machine Learning</b>: Design ...", "url": "https://ai.stanford.edu/~tengyuma/slides/summary.pdf", "isFamilyFriendly": true, "displayUrl": "https://ai.stanford.edu/~tengyuma/slides/summary.pdf", "snippet": "Summary of Thesis: <b>Non-convex Optimization for Machine Learning</b>: Design, Analysis, and Understanding Tengyu Ma October 15, 2018 Non-<b>convex</b> <b>optimization</b> is ubiquitous in modern <b>machine</b> <b>learning</b>: re-cent breakthroughs in deep <b>learning</b> require optimizing non-<b>convex</b> training objective functions; problems that admit accurate <b>convex</b> relaxation can often be solved more e ciently with non-<b>convex</b> formulations. However, the theoretical understanding of non-<b>convex</b> <b>optimization</b> remained rather limited ...", "dateLastCrawled": "2021-09-02T16:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "7. <b>Optimization</b>, the central part of any <b>Machine</b> <b>Learning</b> algortithm ...", "url": "https://compphysics.github.io/MachineLearning/doc/LectureNotes/_build/html/chapteroptimization.html", "isFamilyFriendly": true, "displayUrl": "https://compphysics.github.io/<b>MachineLearning</b>/doc/LectureNotes/_build/html/chapter...", "snippet": "7. <b>Optimization</b>, the central part of any <b>Machine</b> <b>Learning</b> algortithm\u00b6. Almost every problem in <b>machine</b> <b>learning</b> and data science starts with a dataset \\(X\\), a model \\(g(\\beta)\\), which is a function of the parameters \\(\\beta\\) and a cost function \\(C(X, g(\\beta))\\) that allows us to judge how well the model \\(g(\\beta)\\) explains the observations \\(X\\).The model is fit by finding the values of \\(\\beta\\) that minimize the cost function. Ideally we would be able to solve for \\(\\beta ...", "dateLastCrawled": "2022-01-31T15:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "[2005.14605] CoolMomentum: A Method for Stochastic <b>Optimization</b> by ...", "url": "https://arxiv.org/abs/2005.14605", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/abs/2005.14605", "snippet": "This <b>analogy</b> provides useful insights for non-<b>convex</b> stochastic <b>optimization</b> in <b>machine</b> <b>learning</b>. Here we find that integration of the discretized Langevin equation gives a coordinate updating rule equivalent to the famous Momentum <b>optimization</b> algorithm. As a main result, we show that a gradual decrease of the momentum coefficient from the initial value close to unity until zero is equivalent to application of Simulated Annealing or slow cooling, in physical terms. Making use of this novel ...", "dateLastCrawled": "2021-10-23T08:45:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Best <b>Artificial Intelligence</b> Course (AIML) by UT Austin", "url": "https://www.mygreatlearning.com/pg-program-artificial-intelligence-course", "isFamilyFriendly": true, "displayUrl": "https://www.mygreat<b>learning</b>.com/pg-program-<b>artificial-intelligence</b>-course", "snippet": "<b>Machine</b> <b>Learning</b>: <b>Machine</b> <b>learning</b> is a sub-branch of AI that teaches machines to learn any task without the help of explicit directions. It teaches machines to learn by drawing inferences from past experience. <b>Machine</b> <b>learning</b> primarily focuses on developing computer programs that can access and analyze data to identify patterns and understand data behaviour to reach possible conclusions without any kind of human intervention.", "dateLastCrawled": "2022-02-01T17:58:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Which <b>machine</b> <b>learning</b> algorithms for classification support online ...", "url": "https://www.quora.com/Which-machine-learning-algorithms-for-classification-support-online-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Which-<b>machine</b>-<b>learning</b>-algorithms-for-classification-support...", "snippet": "Answer (1 of 5): Most algorithms can be adapted to make them online, even though the standard implementations may not support it. E.g. both decision trees and support ...", "dateLastCrawled": "2022-01-09T00:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "What is <b>the relationship between Online Machine Learning</b> and ...", "url": "https://www.quora.com/What-is-the-relationship-between-Online-Machine-Learning-and-Incremental-Machine-Learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>the-relationship-between-Online-Machine-Learning</b>-and...", "snippet": "Answer (1 of 4): Online <b>learning</b> usually refers to the case where each example is only used once (e.g. if you&#39;re updating an ad click prediction model online after each impression or click), while incremental methods usually pick one example at a time from a finite dataset and can process the sam...", "dateLastCrawled": "2022-01-14T06:00:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "SimplifiedMachineLearningWorkflows-book/Wolfram-Technology-Conference ...", "url": "https://github.com/antononcube/SimplifiedMachineLearningWorkflows-book/blob/master/Data/Wolfram-Technology-Conference-2016-to-2019-abstracts.csv", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/antononcube/Simplified<b>MachineLearning</b>Workflows-book/blob/master/...", "snippet": "Finally, I use <b>machine</b> <b>learning</b> algorithms to train a series of classifiers that can predict a text&#39;s authorship based on its MFW frequencies. Cross-validation indicates that Gallus and Monk are very likely one and the same author. The results also reveal the especially high and hitherto underexplored effectiveness of the Bray Curtis Distance measure and of logistic regression in shedding light on questions of authorship attribution. Data Analytics &amp; Information Science : 2016.Gunnar.Prei\u00df ...", "dateLastCrawled": "2021-12-28T12:42:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(convex optimization)  is like +(fitness training)", "+(convex optimization) is similar to +(fitness training)", "+(convex optimization) can be thought of as +(fitness training)", "+(convex optimization) can be compared to +(fitness training)", "machine learning +(convex optimization AND analogy)", "machine learning +(\"convex optimization is like\")", "machine learning +(\"convex optimization is similar\")", "machine learning +(\"just as convex optimization\")", "machine learning +(\"convex optimization can be thought of as\")", "machine learning +(\"convex optimization can be compared to\")"]}