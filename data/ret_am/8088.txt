{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "10 <b>Leading Language Models For NLP</b> In 2021 - TOPBOTS", "url": "https://www.topbots.com/leading-nlp-language-models-2020/", "isFamilyFriendly": true, "displayUrl": "https://www.topbots.com/leading-nlp-<b>language</b>-<b>models</b>-2020", "snippet": "<b>Masked</b> <b>language</b> modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK] and then train a <b>model</b> to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some ...", "dateLastCrawled": "2022-02-02T10:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Emerging Cross-lingual Structure in Pretrained <b>Language</b> Models", "url": "https://aclanthology.org/2020.acl-main.536.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/2020.acl-main.536.pdf", "snippet": "representation <b>learning</b> from a <b>language</b> <b>model</b>. The representations are used in a transfer <b>learning</b> setup to improve performance on a variety of down-stream NLP tasks. Follow-up work byHoward and Ruder(2018);Radford et al.(2018) further improves on this idea by \ufb01ne-tuning the entire <b>lan-guage</b> <b>model</b>. BERT (Devlin et al.,2019) signi\ufb01-cantly outperforms these methods by introducing a <b>masked</b>-<b>language</b> <b>model</b> and next-sentence pre-diction objectives combined with a bi-directional transformer ...", "dateLastCrawled": "2021-12-30T00:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Top 10 <b>Pre-Trained</b> NLP <b>Language</b> Models - Daffodil", "url": "https://insights.daffodilsw.com/blog/top-5-nlp-language-models", "isFamilyFriendly": true, "displayUrl": "https://insights.daffodilsw.com/blog/top-5-nlp-<b>language</b>-<b>models</b>", "snippet": "<b>Masked</b> <b>language</b> modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK] and then train a <b>model</b> to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of computing to be effective. As an alternative, experts propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, their approach corrupts it by ...", "dateLastCrawled": "2022-02-02T17:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Transformer-based <b>Language</b> Models | The Ezra Tech Blog", "url": "https://techblog.ezra.com/an-overview-of-different-transformer-based-language-models-c9d3adafead8", "isFamilyFriendly": true, "displayUrl": "https://techblog.ezra.com/an-<b>overview-of-different-transformer-based-language-models</b>-c...", "snippet": "<b>Masked</b> <b>Language</b> <b>Model</b>. This task is used to allow for the <b>model</b>\u2019s bidirectionality. The standard conditional <b>language</b> models where the target word is predicted from the previous or next word do not allow for both left-to-right and right-to-left training. This is because with bidirectional conditioning the word indirectly sees itself. In order to achieve the bidirectional nature of the BERT <b>language</b> <b>model</b>, 15% of the training tokens are <b>masked</b> using a special [MASK] token. The <b>model</b> is ...", "dateLastCrawled": "2022-01-27T18:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Top 50 NLP Interview Questions and Answers for 2022", "url": "https://www.projectpro.io/article/nlp-interview-questions-and-answers/439", "isFamilyFriendly": true, "displayUrl": "https://www.projectpro.io/article/nlp-interview-questions-and-answers/439", "snippet": "The <b>Masked</b> <b>Language</b> <b>Model</b> is a <b>model</b> that takes a sentence with a few hidden (<b>masked</b>) words as input and tries to complete the sentence by correctly guessing those hidden words. 10. What is the Bag-of-words <b>model</b> in NLP? Bag-of-words refers to an unorganized set of words. The Bag-of-words <b>model</b> is NLP is a <b>model</b> that assigns a vector to a ...", "dateLastCrawled": "2022-01-29T15:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Mixed-Lingual Pre-training for Cross-lingual Summarization", "url": "https://aclanthology.org/2020.aacl-main.53.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/2020.aacl-main.53.pdf", "snippet": "such as translation and monolingual tasks <b>like</b> <b>masked</b> <b>language</b> models. Thus, our <b>model</b> can leverage the massive monolingual data to enhance its modeling of <b>language</b>. Moreover, the architecture has no task-speci\ufb01c compo- nents, which saves memory and increases opti-mization ef\ufb01ciency. We show in experiments that this pre-training scheme can effectively boost the performance of cross-lingual summa-rization. In Neural Cross-Lingual Summariza-tion (NCLS) (Zhu et al.,2019b) dataset, our <b>model</b> ...", "dateLastCrawled": "2022-01-19T14:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "How to <b>Develop Word-Based Neural Language Models in Python</b> with Keras", "url": "https://machinelearningmastery.com/develop-word-based-neural-language-models-python-keras/", "isFamilyFriendly": true, "displayUrl": "https://machine<b>learning</b>mastery.com/<b>develop-word-based-neural-language-models</b>-python-keras", "snippet": "<b>Language</b> modeling involves predicting the next word in a sequence given the sequence of words already present. A <b>language</b> <b>model</b> is a key element in many natural <b>language</b> processing models such as machine translation and speech recognition. The choice of how the <b>language</b> <b>model</b> is framed must match how the <b>language</b> <b>model</b> is intended to be used. In this tutorial, you will", "dateLastCrawled": "2022-02-02T23:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "nlp - How is the GPT&#39;s <b>masked</b>-self-attention is utilized on fine-tuning ...", "url": "https://stackoverflow.com/questions/64799622/how-is-the-gpts-masked-self-attention-is-utilized-on-fine-tuning-inference", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/64799622", "snippet": "In the standard Transformer, the target sentence is provided to the decoder only once (you might confuse that with the <b>masked</b> <b>language</b>-<b>model</b> objective for BERT). The purpose of the masking is to make sure that the states do not attend to tokens that are &quot;in the future&quot; but only to those &quot;in the past&quot;. The mask looks <b>like</b> this (queries are on ...", "dateLastCrawled": "2022-01-18T19:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "How the brain acquires <b>foreign</b> <b>language</b> grammar \u2013 A Skill-theory ...", "url": "https://gianfrancoconti.com/2015/07/07/1974/", "isFamilyFriendly": true, "displayUrl": "https://gianfrancoconti.com/2015/07/07/1974", "snippet": "It was later developed as a <b>model</b> of L2-<b>learning</b> (Anderson, 1980, 1983, 2000). The fundamental epistemological premise of adopting a skill-development <b>model</b> as a framework for L2-acquisition is that <b>language</b> is considered as governed by the same principles that regulate any other cognitive skill. A number of scholars such as Mc Laughlin (1987), Levelt (1989), O\u2019Malley and Chamot (1990) and Johnson (1996), have produced a number of persuasive arguments in favour of this notion. Although ACT ...", "dateLastCrawled": "2022-02-03T08:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Word knowledge, <b>learning</b> and acquisition in a second <b>language</b>: Proposed ...", "url": "https://www.cambridge.org/core/journals/language-teaching/article/word-knowledge-learning-and-acquisition-in-a-second-language-proposed-replications-of-elgort-2011-and-qiao-and-forster-2017/BC4A26C7F6CF77137FC114E1BE8C39D9", "isFamilyFriendly": true, "displayUrl": "https://www.cambridge.org/core/journals/<b>language</b>-teaching/article/word-knowledge...", "snippet": "For example, the PLE may be observed for known L2 words with Chinese-English bilinguals who learned English in an English as a second <b>language</b> (ESL) but not English as <b>a foreign</b> <b>language</b> (EFL) context. In this case, with the right <b>learning</b> treatment, it should be possible to replicate the PLE for the recently learned words even if the PLE is not observed for \u2018known\u2019 L2 words. Such an outcome with Chinese speakers would provide even stronger support to the finding of the original study ...", "dateLastCrawled": "2022-01-28T04:24:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Emerging Cross-lingual Structure in Pretrained <b>Language</b> Models", "url": "https://aclanthology.org/2020.acl-main.536.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/2020.acl-main.536.pdf", "snippet": "representation <b>learning</b> from a <b>language</b> <b>model</b>. The representations are used in a transfer <b>learning</b> setup to improve performance on a variety of down- stream NLP tasks. Follow-up work byHoward and Ruder(2018);Radford et al.(2018) further improves on this idea by \ufb01ne-tuning the entire <b>lan-guage</b> <b>model</b>. BERT (Devlin et al.,2019) signi\ufb01-cantly outperforms these methods by introducing a <b>masked</b>-<b>language</b> <b>model</b> and next-sentence pre-diction objectives combined with a bi-directional transformer ...", "dateLastCrawled": "2021-12-30T00:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Transformer-based <b>Language</b> Models | The Ezra Tech Blog", "url": "https://techblog.ezra.com/an-overview-of-different-transformer-based-language-models-c9d3adafead8", "isFamilyFriendly": true, "displayUrl": "https://techblog.ezra.com/an-<b>overview-of-different-transformer-based-language-models</b>-c...", "snippet": "<b>Masked</b> <b>Language</b> <b>Model</b>. This task is used to allow for the <b>model</b>\u2019s bidirectionality. The standard conditional <b>language</b> models where the target word is predicted from the previous or next word do not allow for both left-to-right and right-to-left training. This is because with bidirectional conditioning the word indirectly sees itself. In order to achieve the bidirectional nature of the BERT <b>language</b> <b>model</b>, 15% of the training tokens are <b>masked</b> using a special [MASK] token. The <b>model</b> is ...", "dateLastCrawled": "2022-01-27T18:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Top 50 NLP Interview Questions and Answers for 2022", "url": "https://www.projectpro.io/article/nlp-interview-questions-and-answers/439", "isFamilyFriendly": true, "displayUrl": "https://www.projectpro.io/article/nlp-interview-questions-and-answers/439", "snippet": "The <b>Masked</b> <b>Language</b> <b>Model</b> is a <b>model</b> that takes a sentence with a few hidden (<b>masked</b>) words as input and tries to complete the sentence by correctly guessing those hidden words. 10. What is the Bag-of-words <b>model</b> in NLP? Bag-of-words refers to an unorganized set of words. The Bag-of-words <b>model</b> is NLP is a <b>model</b> that assigns a vector to a ...", "dateLastCrawled": "2022-01-29T15:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Mixed-Lingual Pre-training for Cross-lingual Summarization", "url": "https://aclanthology.org/2020.aacl-main.53.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/2020.aacl-main.53.pdf", "snippet": "lingual <b>masked</b> <b>language</b> <b>model</b> (CMLM) and ma-chine translation (MT). This mixed-lingual pre-training scheme can take advantage of massive un-labeled monolingual data to improve the <b>model</b>\u2019s <b>language</b> modeling capability, and leverage cross-lingual tasks to improve the <b>model</b>\u2019s cross-lingual representation. We then \ufb01netune the <b>model</b> on the", "dateLastCrawled": "2022-01-19T14:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Top 10 <b>Pre-Trained</b> NLP <b>Language</b> Models - Daffodil", "url": "https://insights.daffodilsw.com/blog/top-5-nlp-language-models", "isFamilyFriendly": true, "displayUrl": "https://insights.daffodilsw.com/blog/top-5-nlp-<b>language</b>-<b>models</b>", "snippet": "Transfer <b>learning</b>, where a <b>model</b> is first <b>pre-trained</b> on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural <b>language</b> processing (NLP). The effectiveness of transfer <b>learning</b> has given rise to a diversity of approaches, methodology, and practice. The Google research team suggests a unified approach to transfer <b>learning</b> in NLP to set a new state of the art in the field. To this end, they propose treating each NLP problem as a \u201ctext ...", "dateLastCrawled": "2022-02-02T17:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "10 <b>Leading Language Models For NLP</b> In 2021 - TOPBOTS", "url": "https://www.topbots.com/leading-nlp-language-models-2020/", "isFamilyFriendly": true, "displayUrl": "https://www.topbots.com/leading-nlp-<b>language</b>-<b>models</b>-2020", "snippet": "Transfer <b>learning</b>, where a <b>model</b> is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural <b>language</b> processing (NLP). The effectiveness of transfer <b>learning</b> has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer <b>learning</b> techniques for NLP by introducing a unified framework that converts every <b>language</b> problem into a text-to-text format. Our ...", "dateLastCrawled": "2022-02-02T10:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Zero Shot Cross-Lingual Transfer with <b>Multilingual</b> BERT | by Ceshine ...", "url": "https://medium.com/the-artificial-impostor/zero-shot-cross-lingual-transfer-with-multilingual-bert-9fe111e02bb", "isFamilyFriendly": true, "displayUrl": "https://medium.com/the-artificial-impostor/zero-shot-cross-lingual-transfer-with...", "snippet": "BERT[1] is a <b>language</b> representation <b>model</b> that uses two new pre-training objectives \u2014 <b>masked</b> <b>language</b> <b>model</b>(MLM) and next sentence prediction, that obtained SOTA results on many downstream ...", "dateLastCrawled": "2022-01-28T20:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "How to <b>Develop Word-Based Neural Language Models in Python</b> with Keras", "url": "https://machinelearningmastery.com/develop-word-based-neural-language-models-python-keras/", "isFamilyFriendly": true, "displayUrl": "https://machine<b>learning</b>mastery.com/<b>develop-word-based-neural-language-models</b>-python-keras", "snippet": "<b>Language</b> modeling involves predicting the next word in a sequence given the sequence of words already present. A <b>language</b> <b>model</b> is a key element in many natural <b>language</b> processing models such as machine translation and speech recognition. The choice of how the <b>language</b> <b>model</b> is framed must match how the <b>language</b> <b>model</b> is intended to be used. In this tutorial, you will", "dateLastCrawled": "2022-02-02T23:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Word knowledge, <b>learning</b> and acquisition in a second <b>language</b>: Proposed ...", "url": "https://www.cambridge.org/core/journals/language-teaching/article/word-knowledge-learning-and-acquisition-in-a-second-language-proposed-replications-of-elgort-2011-and-qiao-and-forster-2017/BC4A26C7F6CF77137FC114E1BE8C39D9", "isFamilyFriendly": true, "displayUrl": "https://www.cambridge.org/core/journals/<b>language</b>-teaching/article/word-knowledge...", "snippet": "For example, the PLE may be observed for known L2 words with Chinese-English bilinguals who learned English in an English as a second <b>language</b> (ESL) but not English as <b>a foreign</b> <b>language</b> (EFL) context. In this case, with the right <b>learning</b> treatment, it should be possible to replicate the PLE for the recently learned words even if the PLE is not observed for \u2018known\u2019 L2 words. Such an outcome with Chinese speakers would provide even stronger support to the finding of the original study ...", "dateLastCrawled": "2022-01-28T04:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Why is Fine tuning a text <b>model</b> so influential on the results ...", "url": "https://www.reddit.com/r/LanguageTechnology/comments/rzuyqo/why_is_fine_tuning_a_text_model_so_influential_on/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/<b>Language</b>Technology/comments/rzuyqo/why_is_fine_tuning_a_text...", "snippet": "Newbie to this field, but nonetheless BERT was trained on 3.3 billion+ words, when I do a <b>masked</b> <b>learning</b> task it is fairly successful on my healthcare dataset without fine tuning. However, when I fine tune the dataset, maybe adding only additional 1 million words (only ~0.02% more words), suddenly the same task is significantly more accurate.", "dateLastCrawled": "2022-01-11T13:46:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Emerging Cross-lingual Structure in Pretrained <b>Language</b> Models", "url": "https://aclanthology.org/2020.acl-main.536.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/2020.acl-main.536.pdf", "snippet": "representation <b>learning</b> from a <b>language</b> <b>model</b>. The representations are used in a transfer <b>learning</b> setup to improve performance on a variety of down- stream NLP tasks. Follow-up work byHoward and Ruder(2018);Radford et al.(2018) further improves on this idea by \ufb01ne-tuning the entire <b>lan-guage</b> <b>model</b>. BERT (Devlin et al.,2019) signi\ufb01-cantly outperforms these methods by introducing a <b>masked</b>-<b>language</b> <b>model</b> and next-sentence pre-diction objectives combined with a bi-directional transformer ...", "dateLastCrawled": "2021-12-30T00:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Transformer-based <b>Language</b> Models | The Ezra Tech Blog", "url": "https://techblog.ezra.com/an-overview-of-different-transformer-based-language-models-c9d3adafead8", "isFamilyFriendly": true, "displayUrl": "https://techblog.ezra.com/an-<b>overview-of-different-transformer-based-language-models</b>-c...", "snippet": "<b>Masked</b> <b>Language</b> <b>Model</b>. This task is used to allow for the <b>model</b>\u2019s bidirectionality. The standard conditional <b>language</b> models where the target word is predicted from the previous or next word do not allow for both left-to-right and right-to-left training. This is because with bidirectional conditioning the word indirectly sees itself. In order to achieve the bidirectional nature of the BERT <b>language</b> <b>model</b>, 15% of the training tokens are <b>masked</b> using a special [MASK] token. The <b>model</b> is ...", "dateLastCrawled": "2022-01-27T18:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "10 <b>Leading Language Models For NLP</b> In 2021 - TOPBOTS", "url": "https://www.topbots.com/leading-nlp-language-models-2020/", "isFamilyFriendly": true, "displayUrl": "https://www.topbots.com/leading-nlp-<b>language</b>-<b>models</b>-2020", "snippet": "<b>Masked</b> <b>language</b> modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK] and then train a <b>model</b> to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some ...", "dateLastCrawled": "2022-02-02T10:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Language Modeling</b> - GitHub Pages", "url": "https://lena-voita.github.io/nlp_course/language_modeling.html", "isFamilyFriendly": true, "displayUrl": "https://lena-voita.github.io/nlp_course/<b>language_modeling</b>.html", "snippet": "Since left-to-right neural <b>language</b> models <b>can</b> <b>be thought</b> of as classifiers, ... Later in the course, we will see more examples of <b>language</b> models <b>learning</b> lots of cool stuff when given huge training datasets. Use Interpretable Neurons to Control Generated Texts. Interpretable neurons are not only fun, but also <b>can</b> be used to control your <b>language</b> <b>model</b>. For example, we <b>can</b> fix the sentiment neuron to generate texts with a desired sentiment. Below are the examples of samples starting from ...", "dateLastCrawled": "2022-01-29T13:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "GPT-3 &amp; <b>Beyond: 10 NLP Research Papers You Should Read</b>", "url": "https://www.topbots.com/nlp-research-papers-2020/", "isFamilyFriendly": true, "displayUrl": "https://www.topbots.com/nlp-research-papers-2020", "snippet": "<b>Masked</b> <b>language</b> modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK] and then train a <b>model</b> to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some ...", "dateLastCrawled": "2022-01-29T19:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "How to <b>Develop Word-Based Neural Language Models in Python</b> with Keras", "url": "https://machinelearningmastery.com/develop-word-based-neural-language-models-python-keras/", "isFamilyFriendly": true, "displayUrl": "https://machine<b>learning</b>mastery.com/<b>develop-word-based-neural-language-models</b>-python-keras", "snippet": "<b>Language</b> modeling involves predicting the next word in a sequence given the sequence of words already present. A <b>language</b> <b>model</b> is a key element in many natural <b>language</b> processing models such as machine translation and speech recognition. The choice of how the <b>language</b> <b>model</b> is framed must match how the <b>language</b> <b>model</b> is intended to be used. In this tutorial, you will", "dateLastCrawled": "2022-02-02T23:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A critical analysis of Vygotsky and Piagets theory of <b>language</b> <b>learning</b> ...", "url": "https://www.grin.com/document/268010", "isFamilyFriendly": true, "displayUrl": "https://www.grin.com/document/268010", "snippet": "According to Vygostky, the most important part is to be played by the <b>language</b> in the cognitive development, Vygostky believes in the centrality of <b>language</b> as a tool for <b>thought</b> or a powerful means of mediation (Mitchell, R. &amp; Myles, F. 2004, Second <b>Language</b> <b>Learning</b> Theories, 2 nd Ed. London: Hodder Arnold, p. 194, 195) Vygostky further elaborated his socio-cultural theory by explaining the importance of Cultural Tools through mediation and <b>language</b>, Co-constructed process, self-regulation ...", "dateLastCrawled": "2021-12-27T22:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Using lexical <b>language</b> models to detect borrowings in monolingual wordlists", "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0242709", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0242709", "snippet": "Although <b>masked</b> with time, <b>language</b>-internal evidence for borrowing <b>can</b> be observed in many languages from different families. In many Hmong-Mien languages, for example, some Chinese words are borrowed with a very specific tone that only occurs in Chinese words . Similarly, it is easy for German speakers to identify job as a loan from English, since only in borrowed words the grapheme j is pronounced as [dZ] in German. In the same line, but in a radically different context, speakers of ...", "dateLastCrawled": "2020-12-10T17:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "On <b>language</b> and <b>thought</b>: Bilingual experience <b>influences semantic</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0911604420300920", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0911604420300920", "snippet": "Whether <b>language</b> <b>can</b> affect <b>thought</b> is a frequent topic of discussion. Although the strong Whorfian hypothesis that <b>language</b> determines <b>thought</b> is controversial ( Heider, 1972 ; Lakoff, 1987 ), recent research has shown that <b>language</b> influences mental representation of space ( Levinson &amp; Wilkins, 2006 ), time ( Casasanto &amp; Boroditsky, 2008 ), motion ( Slobin, 2003 ), and color ( Winawer et al., 2007 ).", "dateLastCrawled": "2021-11-06T18:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Why is Fine tuning a text <b>model</b> so influential on the results ...", "url": "https://www.reddit.com/r/LanguageTechnology/comments/rzuyqo/why_is_fine_tuning_a_text_model_so_influential_on/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/<b>Language</b>Technology/comments/rzuyqo/why_is_fine_tuning_a_text...", "snippet": "Newbie to this field, but nonetheless BERT was trained on 3.3 billion+ words, when I do a <b>masked</b> <b>learning</b> task it is fairly successful on my healthcare dataset without fine tuning. However, when I fine tune the dataset, maybe adding only additional 1 million words (only ~0.02% more words), suddenly the same task is significantly more accurate.", "dateLastCrawled": "2022-01-11T13:46:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "10 <b>Leading Language Models For NLP</b> In 2021 - TOPBOTS", "url": "https://www.topbots.com/leading-nlp-language-models-2020/", "isFamilyFriendly": true, "displayUrl": "https://www.topbots.com/leading-nlp-<b>language</b>-<b>models</b>-2020", "snippet": "<b>Masked</b> <b>language</b> modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK] and then train a <b>model</b> to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some ...", "dateLastCrawled": "2022-02-02T10:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Two Words, One Meaning: Evidence of Automatic Co-Activation of ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3155883/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC3155883", "snippet": "The use of a within-<b>language</b> full priming condition against which cross-<b>language</b> effects <b>can</b> <b>be compared</b> provides valuable insights into the ... Devlex-II is an unsupervised connectionist network that does bilingual lexicon <b>learning</b> based on Hebbian <b>Learning</b> principles. Although the <b>model</b> does predict the expansion of the L2 lexicon with extensive training (i.e., increased proficiency), it proposes that it is the L2 AoA what mainly defines the functional properties of the L2 lexicon. As ...", "dateLastCrawled": "2021-10-17T20:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Zero Shot Cross-Lingual Transfer with <b>Multilingual</b> BERT | by Ceshine ...", "url": "https://medium.com/the-artificial-impostor/zero-shot-cross-lingual-transfer-with-multilingual-bert-9fe111e02bb", "isFamilyFriendly": true, "displayUrl": "https://medium.com/the-artificial-impostor/zero-shot-cross-lingual-transfer-with...", "snippet": "BERT[1] is a <b>language</b> representation <b>model</b> that uses two new pre-training objectives \u2014 <b>masked</b> <b>language</b> <b>model</b>(MLM) and next sentence prediction, that obtained SOTA results on many downstream ...", "dateLastCrawled": "2022-01-28T20:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) SlovakBERT: Slovak <b>Masked</b> <b>Language</b> <b>Model</b>", "url": "https://www.researchgate.net/publication/354983058_SlovakBERT_Slovak_Masked_Language_Model", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/354983058_SlovakBERT_Slovak_<b>Masked</b>_<b>Language</b>_<b>Model</b>", "snippet": "PDF | We introduce a new Slovak <b>masked</b> <b>language</b> <b>model</b> called SlovakBERT in this paper. It is the first Slovak-only transformers-based <b>model</b> trained on a... | Find, read and cite all the research ...", "dateLastCrawled": "2021-10-21T05:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "GPT-3 &amp; <b>Beyond: 10 NLP Research Papers You Should Read</b>", "url": "https://www.topbots.com/nlp-research-papers-2020/", "isFamilyFriendly": true, "displayUrl": "https://www.topbots.com/nlp-research-papers-2020", "snippet": "<b>Masked</b> <b>language</b> modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK] and then train a <b>model</b> to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some ...", "dateLastCrawled": "2022-01-29T19:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Cross-lingual <b>Language</b> <b>Model</b> Pretraining for Retrieval", "url": "https://maroo.cs.umass.edu/getpdf.php?id=1440", "isFamilyFriendly": true, "displayUrl": "https://maroo.cs.umass.edu/getpdf.php?id=1440", "snippet": "Cross-lingual <b>Language</b> <b>Model</b> Pretraining for Retrieval Puxuan Yu, Hongliang Fei, Ping Li Cognitive Computing Lab Baidu Research Bellevue, WA, USA {pxyuwhu,feihongliang0,pingli98}@gmail.com ABSTRACT Existing research on cross-lingual retrieval cannot take good advan-tage of large-scale pretrained <b>language</b> models such as multilingual BERT and XLM. We hypothesize that the absence of cross-lingual passage-level relevance data for finetuning and the lack of query-document style pretraining are ...", "dateLastCrawled": "2022-01-23T02:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Bilingual Education: 5 Reasons it should be Required - The Edvocate", "url": "https://www.theedadvocate.org/bilingual-education-5-reasons-it-should-be-required/", "isFamilyFriendly": true, "displayUrl": "https://www.theedadvocate.org/bilingual-education-5-reasons-it-should-be-required", "snippet": "4. It Leads to Collaborative <b>Learning</b>. Dual <b>language</b> programs show students a broader world-view, whatever the native <b>language</b> of the student, and lead to greater opportunities for collaborative <b>learning</b>. We should not limit what children learn based on outdated principles <b>masked</b> in patriotism. 5. Early Bilingual Education Increases Fluency in ...", "dateLastCrawled": "2022-02-03T05:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A NEW <b>TYPE OF MASKED FORM PRIMING</b> | Studies in Second <b>Language</b> ...", "url": "https://www.cambridge.org/core/journals/studies-in-second-language-acquisition/article/new-type-of-masked-form-priming/046773B7CDE945439C77A6AF3390EBAC", "isFamilyFriendly": true, "displayUrl": "https://www.cambridge.org/core/journals/studies-in-second-<b>language</b>-acquisition/article/...", "snippet": "Indeed, the self-rated English proficiency of each bilingual participant corresponded to one of the two highest levels of China\u2019s Standards of English <b>Language</b> Ability (National <b>Language</b> Standard GF 0018\u20132018), namely, level 8 or 9. The mean age of the bilinguals was 25.6 (SD = 6.2), with their age of English acquisition being 9.8 (SD = 2.61). All bilingual participants received \uffe520 cash for their participation.", "dateLastCrawled": "2022-01-27T21:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Language Modeling</b> - GitHub Pages", "url": "https://lena-voita.github.io/nlp_course/language_modeling.html", "isFamilyFriendly": true, "displayUrl": "https://lena-voita.github.io/nlp_course/<b>language_modeling</b>.html", "snippet": "The key point of this paper is that a <b>language</b> <b>model</b> <b>can</b> be used to estimate the &quot;predictability&quot; of a word given context. Computational LM instead of a Human one - a very novel idea Previously, the predictability of a word given context was estimated in cloze-style tasks: humans were asked to guess the next word given context.", "dateLastCrawled": "2022-01-29T13:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Why is Fine tuning a text <b>model</b> so influential on the results ...", "url": "https://www.reddit.com/r/LanguageTechnology/comments/rzuyqo/why_is_fine_tuning_a_text_model_so_influential_on/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/<b>Language</b>Technology/comments/rzuyqo/why_is_fine_tuning_a_text...", "snippet": "The amount of information in the context is very small there <b>compared</b> to Wikipedia, but the relevance of it is key. Less generally, healthcare text has a lot of words that are very rare in normal writing, so BERT won&#39;t have useful embeddings for them if it&#39;s been trained on general <b>language</b>. Also 1M tokens is actually quite a lot.", "dateLastCrawled": "2022-01-11T13:46:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine</b> <b>Learning</b> Glossary: <b>Language</b> Evaluation | Google Developers", "url": "https://developers.google.com/machine-learning/glossary/language", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine</b>-<b>learning</b>/glossary/<b>language</b>", "snippet": "For instance, a <b>masked</b> <b>language</b> <b>model</b> can calculate probabilities for candidate word(s) to replace the underline in the following sentence: The ____ in the hat came back. The literature typically uses the string &quot;MASK&quot; instead of an underline. For example: The &quot;MASK&quot; in the hat came back. Most modern <b>masked</b> <b>language</b> models are bidirectional.", "dateLastCrawled": "2022-01-29T02:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "The Illustrated GPT-2 (Visualizing Transformer <b>Language</b> Models) \u2013 Jay ...", "url": "http://jalammar.github.io/illustrated-gpt2/", "isFamilyFriendly": true, "displayUrl": "jalammar.github.io/illustrated-gpt2", "snippet": "GPT-2 <b>Masked</b> Self-Attention; Beyond <b>Language</b> modeling; You\u2019ve Made it! Part 3: Beyond <b>Language</b> Modeling. <b>Machine</b> Translation; Summarization ; Transfer <b>Learning</b>; Music Generation; Part #1: GPT2 And <b>Language</b> Modeling # So what exactly is a <b>language</b> <b>model</b>? What is a <b>Language</b> <b>Model</b>. In The Illustrated Word2vec, we\u2019ve looked at what a <b>language</b> <b>model</b> is \u2013 basically a <b>machine</b> <b>learning</b> <b>model</b> that is able to look at part of a sentence and predict the next word. The most famous <b>language</b> models ...", "dateLastCrawled": "2022-02-01T04:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "The <b>language</b> of <b>proteins: NLP, machine learning &amp; protein sequences</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S2001037021000945", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2001037021000945", "snippet": "For example, in the <b>masked</b> <b>language</b> task, some fraction of the tokens in the original text are <b>masked</b> at random, and the <b>language</b> <b>model</b> attempts to predict the original text. (B) (Pre-)trained <b>language</b> models are commonly fine-tuned on downstream tasks over labeled text, through a standard supervised-<b>learning</b> approach. Fine-tuning is typically much faster and provides superior performance than training a <b>model</b> from scratch, especially when labeled data is scarce.", "dateLastCrawled": "2022-02-03T02:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Word Embeddings, WordPiece and Language-Agnostic BERT</b> (LaBSE) | by ...", "url": "https://medium.com/mlearning-ai/word-embeddings-wordpiece-and-language-agnostic-bert-labse-98c7626878c7", "isFamilyFriendly": true, "displayUrl": "https://medium.com/m<b>learning</b>-ai/<b>word-embeddings-wordpiece-and-language-agnostic-bert</b>...", "snippet": "LaBSE <b>model</b> combines <b>masked</b> <b>language</b> <b>model</b> (MLM) and translation <b>language</b> <b>model</b> (TLM) pretraining with a translation ranking task using bi-directional dual encoders.", "dateLastCrawled": "2022-02-03T17:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Natrual <b>language</b> processing basic concepts - <b>language</b> <b>model</b> - word ...", "url": "https://shuffleai.blog/blog/nlp_concepts_part_1.html", "isFamilyFriendly": true, "displayUrl": "https://shuffleai.blog/blog/nlp_concepts_part_1.html", "snippet": "Before deep <b>learning</b>&#39;s domination in natural <b>language</b> processing, a <b>language</b> <b>model</b> is basically a large lookup table, recording frequencies of different combinations of words&#39; occurrences in a large corpus. Now it&#39;s a neural network trained on a corpus or dataset. In addition, a causal <b>language</b> <b>model</b>(e.g., GPT) predicts the next word, and a <b>masked</b> <b>language</b> <b>model</b>(e.g., BERT) fills the blank given the rest of a sentence. If you input &quot;The man ____ to the store&quot; to BERT, it will predict the ...", "dateLastCrawled": "2021-12-24T07:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "An introduction to Deep <b>Learning</b> in Natural <b>Language</b> Processing: Models ...", "url": "https://www.sciencedirect.com/science/article/pii/S0925231221010997", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0925231221010997", "snippet": "The pre-training was driven by two <b>language</b> <b>model</b> objectives, i.e. <b>Masked</b> <b>Language</b> <b>Model</b> (MLM) and Next Sentence Prediction (NSP). In MLM, showed in Fig. 8 , the network masks a small number of words of the input sequence and it tries to predict them in output, whereas in NSP the network tries to understand the relations between sentences by means of a binary loss.", "dateLastCrawled": "2022-01-04T17:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>GitHub</b> - <b>rosinality/ml-papers</b>: My collection of <b>machine</b> <b>learning</b> papers", "url": "https://github.com/rosinality/ml-papers", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/rosinality/ml-papers", "snippet": "210413 <b>Masked</b> <b>Language</b> Modeling and the Distributional Hypothesis #<b>language</b>_<b>model</b> #mlm; 210417 mT6 #<b>language</b>_<b>model</b>; 210418 Data-Efficient <b>Language</b>-Supervised Zero-Shot <b>Learning</b> with #multimodal; 210422 ImageNet-21K Pretraining for the Masses #backbone; 210510 Are Pre-trained Convolutions Better than Pre-trained Transformers #nlp #convolution # ...", "dateLastCrawled": "2022-01-31T15:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Intuitive Introduction to BERT \u2013 MachineCurve</b>", "url": "https://www.machinecurve.com/index.php/2021/01/04/intuitive-introduction-to-bert/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/index.php/2021/01/04/<b>intuitive-introduction-to-bert</b>", "snippet": "We do however often want to create a <b>machine</b> <b>learning</b> <b>model</b> that can perform one task really well. This is where finetuning comes in: using a labeled corpus, which is often smaller, we can then train the pretrained <b>model</b> further, with an additional or replacing NLP task. The end result is a <b>model</b> that has been pretrained on the large unlabeled corpus and which is finetuned to a specific <b>language</b> task, such as summarization, text generation in a particular domain, or translation.", "dateLastCrawled": "2022-01-30T22:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "The 5 <b>Components Towards Building Production-Ready Machine Learning Systems</b>", "url": "https://www.topbots.com/building-production-ready-machine-learning-systems/", "isFamilyFriendly": true, "displayUrl": "https://www.topbots.com/<b>building-production-ready-machine-learning-systems</b>", "snippet": "A well-known recent case study of applying knowledge distillation in practice is Hugging Face\u2019s DistilBERT, which is a smaller <b>language</b> <b>model</b> derived from the supervision of the popular BERT <b>language</b> <b>model</b>. DistilBERT removed the toke-type embeddings and the pooler (used for the next sentence classification task) from BERT while keeping the rest of the architecture identical and reducing the number of layers by a factor of two. Overall, DistilBERT has about half the total number of ...", "dateLastCrawled": "2022-01-25T15:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "SpringerLink - International Journal of <b>Machine</b> <b>Learning</b> and Cybernetics", "url": "https://link.springer.com/article/10.1007/s13042-020-01069-8", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s13042-020-01069-8", "snippet": "The Neural Network <b>Language</b> <b>Model</b> (NNLM) is a pioneering work which introduces the idea of deep <b>learning</b> into <b>language</b> modeling and successfully mitigates the curse of dimensionality (i.e. Sequences in the test set is likely to have not been observed in the training data) by <b>learning</b> a distributed representation of words. The goal of <b>language</b> modeling is to learn a <b>model</b> that predicts the next word given previous ones. Practically, we assume the", "dateLastCrawled": "2022-01-29T10:14:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Improving Text Generation with Dynamic Masking and Recovering", "url": "https://www.ijcai.org/proceedings/2021/0534.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.ijcai.org/proceedings/2021/0534.pdf", "snippet": "tokens, <b>just as masked language model</b> does. Therefore, our approach jointly maximizes both the likelihoods of both sen-tence generation and prediction of masked tokens. We verify the effectiveness and generality of our ap-proach on three types of text generation tasks which use var-ious forms of input data including text, graph, and image. For sequence-to-sequence (seq2seq) generation task (specif-ically, <b>machine</b> translation), our model obtains signi\ufb01cant improvement of 1.01 and 0.90 BLEU ...", "dateLastCrawled": "2022-01-29T07:50:00.0000000Z", "language": "en", "isNavigational": false}], [], []], "all_bing_queries": ["+(masked language model)  is like +(learning a foreign language)", "+(masked language model) is similar to +(learning a foreign language)", "+(masked language model) can be thought of as +(learning a foreign language)", "+(masked language model) can be compared to +(learning a foreign language)", "machine learning +(masked language model AND analogy)", "machine learning +(\"masked language model is like\")", "machine learning +(\"masked language model is similar\")", "machine learning +(\"just as masked language model\")", "machine learning +(\"masked language model can be thought of as\")", "machine learning +(\"masked language model can be compared to\")"]}