{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Picking Loss Functions - A comparison between MSE, Cross Entropy</b>, and ...", "url": "https://rohanvarma.me/Loss-Functions/", "isFamilyFriendly": true, "displayUrl": "https://rohanvarma.me/<b>Loss</b>-Functions", "snippet": "There are several different common <b>loss</b> functions to choose from: the cross-entropy <b>loss</b>, the <b>mean</b>-<b>squared</b> <b>error</b>, the huber <b>loss</b>, and the <b>hinge</b> <b>loss</b> - just to name a few. Given a particular model, each <b>loss</b> function has particular properties that make it interesting - for example, the (L2-regularized) <b>hinge</b> <b>loss</b> comes with the maximum-margin ...", "dateLastCrawled": "2021-12-04T02:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A definitive explanation to the <b>Hinge Loss</b> for Support Vector Machines ...", "url": "https://towardsdatascience.com/a-definitive-explanation-to-hinge-loss-for-support-vector-machines-ab6d8d3178f1", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-definitive-explanation-to-<b>hinge-loss</b>-for-support...", "snippet": "Some examples of cost functions (other than the <b>hinge loss</b>) include: Root <b>Mean</b> <b>Squared</b> <b>Error</b>(Regression) Logarithmic <b>Loss</b>(Classification) <b>Mean</b> Absolute <b>Error</b>(Regression) Gini Impurity(Classification) As you might have deducted, <b>Hinge Loss</b> is also a type of cost function that is specifically tailored to Support Vector Machines. Why this <b>loss</b> exactly and not the other losses mentioned above? Well, why don\u2019t we find out with our first introduction to the <b>Hinge Loss</b>! <b>Hinge Loss</b>: An ...", "dateLastCrawled": "2022-02-02T07:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>About loss and loss functions</b> \u2013 MachineCurve", "url": "https://www.machinecurve.com/index.php/2019/10/04/about-loss-and-loss-functions/", "isFamilyFriendly": true, "displayUrl": "https://www.machinecurve.com/index.php/2019/10/04/<b>about-loss-and-loss-functions</b>", "snippet": "<b>Squared</b> <b>hinge</b>. The <b>squared</b> <b>hinge</b> <b>loss</b> <b>is like</b> the <b>hinge</b> formula displayed above, but then the \\(max()\\) function output is <b>squared</b>. This helps achieving two things: Firstly, it makes the <b>loss</b> value more sensitive to outliers, just as we saw with MSE vs MAE. Large errors will add to the <b>loss</b> more significantly than smaller errors. Note that simiarly, this may also <b>mean</b> that you\u2019ll need to inspect your dataset for the presence of such outliers first. Secondly, <b>squared</b> <b>hinge</b> <b>loss</b> is ...", "dateLastCrawled": "2022-01-25T13:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Keras Loss Functions - Types and Examples</b> - DataFlair", "url": "https://data-flair.training/blogs/keras-loss-functions/", "isFamilyFriendly": true, "displayUrl": "https://data-flair.training/blogs/keras-<b>loss</b>-functions", "snippet": "Categorical <b>Hinge</b>; <b>Squared</b> <b>Hinge</b>; 2. Regression <b>Loss</b> functions in Keras. These are useful to model the linear relationship between several independent and a dependent variable. Different types of Regression <b>Loss</b> function in Keras: <b>Mean</b> Square <b>Error</b>; <b>Mean</b> Absolute <b>Error</b>; Cosine Similarity; Huber <b>Loss</b>; <b>Mean</b> Absolute Percentage <b>Error</b>; <b>Mean</b> <b>Squared</b> Logarithmic <b>Error</b>; Log Cosh; 3. Binary and Multiclass <b>Loss</b> in Keras. These <b>loss</b> functions are useful in algorithms where we have to identify the ...", "dateLastCrawled": "2022-02-03T07:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 4, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Hinge loss</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Hinge_loss", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Hinge_loss</b>", "snippet": "In machine learning, the <b>hinge loss</b> is a <b>loss</b> function used for training classifiers.The <b>hinge loss</b> is used for &quot;maximum-margin&quot; classification, most notably for support vector machines (SVMs).. For an intended output t = \u00b11 and a classifier score y, the <b>hinge loss</b> of the prediction y is defined as = (,)Note that should be the &quot;raw&quot; output of the classifier&#39;s decision function, not the predicted class label. For instance, in linear SVMs, = +, where (,) are the parameters of the hyperplane ...", "dateLastCrawled": "2022-02-03T02:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Ultimate Guide To Loss functions</b> In Tensorflow Keras API With Python ...", "url": "https://analyticsindiamag.com/ultimate-guide-to-loss-functions-in-tensorflow-keras-api-with-python-implementation/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/ultimate-guide-to-lo", "snippet": "<b>Mean</b> <b>Squared</b> <b>error</b>; <b>Mean</b> Absolute <b>Error</b>; <b>Mean</b> Absolute percentage <b>error</b>; <b>Mean</b> <b>Squared</b> Logarithmic <b>Error</b>; Cosine Similarity; Huber; Log Cosh; <b>Hinge</b> losses for \u201cmaximum-margin\u201d classification. <b>Hinge</b>; <b>Squared</b> <b>Hinge</b>; Categorical <b>Hinge</b> ; Implementation. You can use the <b>loss</b> function by simply calling tf.keras.<b>loss</b> as shown in the below command, and we are also importing NumPy additionally for our upcoming sample usage of <b>loss</b> functions: import tensorflow as tf import numpy as np bce_<b>loss</b> = tf ...", "dateLastCrawled": "2022-02-03T09:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>sklearn.svm.LinearSVC</b> \u2014 scikit-learn 1.0.2 documentation", "url": "https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html", "isFamilyFriendly": true, "displayUrl": "https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html", "snippet": "<b>loss</b> {\u2018<b>hinge</b>\u2019, \u2018<b>squared</b>_<b>hinge</b>\u2019}, default=\u2019<b>squared</b>_<b>hinge</b> \u2019 Specifies the <b>loss</b> function. \u2018<b>hinge</b>\u2019 is the standard SVM <b>loss</b> (used e.g. by the SVC class) while \u2018<b>squared</b>_<b>hinge</b>\u2019 is the square of the <b>hinge</b> <b>loss</b>. The combination of penalty=&#39;l1&#39; and <b>loss</b>=&#39;<b>hinge</b>&#39; is not supported. dual bool, default=True. Select the algorithm to either solve the dual or primal optimization problem. Prefer dual=False when n_samples &gt; n_features. tol float, default=1e-4. Tolerance for stopping ...", "dateLastCrawled": "2022-02-02T20:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Loss</b> Functions. <b>Loss</b> functions explanations and\u2026 | by Tomer Kordonsky ...", "url": "https://medium.com/artificialis/loss-functions-361b2ad439a0", "isFamilyFriendly": true, "displayUrl": "https://medium.com/artificialis/<b>loss</b>-functions-361b2ad439a0", "snippet": "The <b>Hinge</b> <b>Loss</b> Equation def <b>Hinge</b>(yhat, y): return np.max(0,1 - yhat * y) Where y is the actual label (-1 or 1) and \u0177 is the prediction; The <b>loss</b> is 0 when the signs of the labels and prediction ...", "dateLastCrawled": "2022-02-03T02:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Loss</b> Functions in Neural Networks", "url": "https://www.theaidream.com/post/loss-functions-in-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://www.theaidream.com/post/<b>loss</b>-functions-in-neural-networks", "snippet": "<b>Loss</b> functions are mainly classified into two different categories Classification <b>loss</b> and Regression <b>Loss</b>. Classification <b>loss</b> is the case where the aim is to predict the output from the different categorical values for example, if we have a dataset of handwritten images and the digit is to be predicted that lies between (0\u20139), in these kinds of scenarios classification <b>loss</b> is used.", "dateLastCrawled": "2022-02-02T10:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How to Choose <b>Loss</b> Functions When Training Deep Learning Neural Networks", "url": "https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/how-to-choose-<b>loss</b>-functions-when-training-deep...", "snippet": "<b>Squared</b> <b>Hinge</b> <b>Loss</b>; Multi-Class Classification <b>Loss</b> Functions Multi-Class Cross-Entropy <b>Loss</b>; Sparse Multiclass Cross-Entropy <b>Loss</b>; Kullback Leibler Divergence <b>Loss</b>; We will focus on how to choose and implement different <b>loss</b> functions. For more theory on <b>loss</b> functions, see the post: <b>Loss</b> and <b>Loss</b> Functions for Training Deep Learning Neural Networks ; Regression <b>Loss</b> Functions. A regression predictive modeling problem involves predicting a real-valued quantity. In this section, we will ...", "dateLastCrawled": "2022-02-02T15:37:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Picking Loss Functions - A comparison between MSE, Cross Entropy</b>, and ...", "url": "https://rohanvarma.me/Loss-Functions/", "isFamilyFriendly": true, "displayUrl": "https://rohanvarma.me/<b>Loss</b>-Functions", "snippet": "There are several different common <b>loss</b> functions to choose from: the cross-entropy <b>loss</b>, the <b>mean</b>-<b>squared</b> <b>error</b>, the huber <b>loss</b>, and the <b>hinge</b> <b>loss</b> - just to name a few. Given a particular model, each <b>loss</b> function has particular properties that make it interesting - for example, the (L2-regularized) <b>hinge</b> <b>loss</b> comes with the maximum-margin ...", "dateLastCrawled": "2021-12-04T02:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Types of <b>Keras Loss Functions Explained for Beginners</b> - MLK - Machine ...", "url": "https://machinelearningknowledge.ai/types-of-keras-loss-functions-explained-for-beginners/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningknowledge.ai/types-of-<b>keras-loss-functions-explained-for-beginners</b>", "snippet": "The <b>squared</b> <b>hinge</b> <b>loss</b> is calculated using <b>squared</b>_<b>hinge</b>() function and <b>is similar</b> to <b>Hinge</b> <b>Loss</b> calculation discussed above except that the result is <b>squared</b>. Syntax of <b>Squared</b> <b>Hinge</b> <b>Loss</b> in Keras. In [22]: tf. keras. losses. <b>squared</b>_<b>hinge</b> (y_true, y_pred) Example of <b>Squared</b> <b>Hinge</b> <b>Loss</b> in Keras. In this example, at first, data is generated using numpy randon function, then Keras <b>squared</b> <b>hinge</b> <b>loss</b> function calculates the <b>loss</b>. In [23]: import numpy as np y_true = np. random. choice ([-1, 1 ...", "dateLastCrawled": "2022-01-31T03:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Squared</b> <b>hinge</b> <b>loss</b> function | Peltarion Platform", "url": "https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/loss-functions/squared-hinge", "isFamilyFriendly": true, "displayUrl": "https://peltarion.com/.../modeling-view/build-an-ai-model/<b>loss</b>-functions/<b>squared</b>-<b>hinge</b>", "snippet": "The <b>squared</b> <b>hinge</b> <b>loss</b> is a <b>loss</b> function used for \u201cmaximum margin\u201d binary classification problems. Mathematically it is defined as: where y\u0302 the predicted value and y is either 1 or -1. Thus, the <b>squared</b> <b>hinge</b> <b>loss</b> is: y\u0302 should be the actual numerical output of the classifier and not the predicted label. The <b>hinge</b> <b>loss</b> guarantees that ...", "dateLastCrawled": "2022-01-24T09:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Loss</b> Functions in Machine Learning and LTR | Yuan Du", "url": "https://yuan-du.com/post/2020-12-13-loss-functions/decision-theory/", "isFamilyFriendly": true, "displayUrl": "https://yuan-du.com/post/2020-12-13-<b>loss</b>-functions/decision-theory", "snippet": "<b>Hinge</b> <b>loss</b>. The <b>hinge</b> <b>loss</b> is a <b>loss</b> function used for \u201cmaximum-margin\u201d classification, most notably for support vector machine (SVM).It\u2019s equivalent to minimize the <b>loss</b> function \\(L(y,f) = [1-yf]_+\\).", "dateLastCrawled": "2022-01-31T06:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 4, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Hinge loss</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Hinge_loss", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Hinge_loss</b>", "snippet": "In machine learning, the <b>hinge loss</b> is a <b>loss</b> function used for training classifiers.The <b>hinge loss</b> is used for &quot;maximum-margin&quot; classification, most notably for support vector machines (SVMs).. For an intended output t = \u00b11 and a classifier score y, the <b>hinge loss</b> of the prediction y is defined as = (,)Note that should be the &quot;raw&quot; output of the classifier&#39;s decision function, not the predicted class label. For instance, in linear SVMs, = +, where (,) are the parameters of the hyperplane ...", "dateLastCrawled": "2022-02-03T02:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "neural networks - What is the definition of the <b>hinge loss</b> function ...", "url": "https://ai.stackexchange.com/questions/26330/what-is-the-definition-of-the-hinge-loss-function", "isFamilyFriendly": true, "displayUrl": "https://ai.stackexchange.com/questions/26330/what-is-the-definition-of-the-<b>hinge-loss</b>...", "snippet": "$\\begingroup$ The idea behind <b>hinge loss</b> (not obvious from its expression) is that the NN must predict with confidence i.e.its prediction score must exceed a certain threshold (a hyperparameter) for the <b>loss</b> to be 0. Hence while training the NN tries to predict with maximum confidence or exceed the threshold so that <b>loss</b> is 0. $\\endgroup$ \u2013 user9947", "dateLastCrawled": "2022-01-28T01:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "How to Choose <b>Loss</b> Functions When Training Deep Learning Neural Networks", "url": "https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/how-to-choose-<b>loss</b>-functions-when-training-deep...", "snippet": "<b>Squared</b> <b>Hinge</b> <b>Loss</b>; Multi-Class Classification <b>Loss</b> Functions Multi-Class Cross-Entropy <b>Loss</b>; Sparse Multiclass Cross-Entropy <b>Loss</b> ; Kullback Leibler Divergence <b>Loss</b>; We will focus on how to choose and implement different <b>loss</b> functions. For more theory on <b>loss</b> functions, see the post: <b>Loss</b> and <b>Loss</b> Functions for Training Deep Learning Neural Networks; Regression <b>Loss</b> Functions. A regression predictive modeling problem involves predicting a real-valued quantity. In this section, we will ...", "dateLastCrawled": "2022-02-02T15:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Keras - Model Compilation</b> - <b>Tutorialspoint</b>", "url": "https://www.tutorialspoint.com/keras/keras_model_compilation.htm", "isFamilyFriendly": true, "displayUrl": "https://<b>www.tutorialspoint.com</b>/keras/<b>keras_model_compilation</b>.htm", "snippet": "<b>mean</b>_<b>squared</b>_logarithmic_<b>error</b>; <b>squared</b>_<b>hinge</b>; <b>hinge</b>; categorical_<b>hinge</b>; logcosh; huber_<b>loss</b>; categorical_crossentropy; sparse_categorical_crossentropy; binary_crossentropy ; kullback_leibler_divergence; poisson; cosine_proximity; is_categorical_crossentropy; All above <b>loss</b> function accepts two arguments \u2212. y_true \u2212 true labels as tensors. y_pred \u2212 prediction with same shape as y_true. Import the losses module before using <b>loss</b> function as specified below \u2212. from keras import losses ...", "dateLastCrawled": "2022-02-03T02:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Loss</b> Functions. <b>Loss</b> functions explanations and\u2026 | by Tomer Kordonsky ...", "url": "https://medium.com/artificialis/loss-functions-361b2ad439a0", "isFamilyFriendly": true, "displayUrl": "https://medium.com/artificialis/<b>loss</b>-functions-361b2ad439a0", "snippet": "The <b>Hinge</b> <b>Loss</b> Equation def <b>Hinge</b>(yhat, y): return np.max(0,1 - yhat * y) Where y is the actual label (-1 or 1) and \u0177 is the prediction; The <b>loss</b> is 0 when the signs of the labels and prediction ...", "dateLastCrawled": "2022-02-03T02:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "machine learning - Why use <b>squared loss</b> on probabilities instead of ...", "url": "https://stats.stackexchange.com/questions/264625/why-use-squared-loss-on-probabilities-instead-of-logistic-loss", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/264625/why-use-<b>squared-loss</b>-on-probabilities...", "snippet": "In other words, logistic <b>loss</b> and <b>squared loss</b> have the same minimum. This paper compares the properties of the Brier score (&quot;square <b>loss</b>&quot;) to some other <b>loss</b> functions. They find that square <b>loss</b>/Brier score converges more slowly than logistic <b>loss</b>. Square <b>loss</b> has some advantages that might compensate in some cases:", "dateLastCrawled": "2022-01-18T18:10:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Picking Loss Functions - A comparison between MSE, Cross Entropy</b>, and ...", "url": "https://rohanvarma.me/Loss-Functions/", "isFamilyFriendly": true, "displayUrl": "https://rohanvarma.me/<b>Loss</b>-Functions", "snippet": "In this post, I\u2019ll discuss three common <b>loss</b> functions: the <b>mean</b>-<b>squared</b> (MSE) <b>loss</b>, cross-entropy <b>loss</b>, and the <b>hinge</b> <b>loss</b>. These are the most commonly used functions I\u2019ve seen used in traditional machine learning and deep learning models, so I <b>thought</b> it would be a good idea to figure out the underlying theory behind each one, and when to prefer one over the others.", "dateLastCrawled": "2021-12-04T02:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Loss</b> (<b>Error</b>) Functions in <b>Machine Learning</b> - 360DigiTMG", "url": "https://360digitmg.com/loss-functions-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://360digitmg.com/<b>loss</b>-functions-in-<b>machine-learning</b>", "snippet": "<b>Squared</b> <b>Hinge</b> <b>Loss</b>: This is an extension of the <b>hinge</b> <b>loss</b> and it is quite simply the square of the <b>hinge</b> <b>loss</b> function. Since this is a square of the original <b>loss</b>, it has some mathematical properties that make it easier to calculate the gradients. This is perfectly suitable for Yes or No type of questions where the deviation in probability is ...", "dateLastCrawled": "2022-01-30T16:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "How to use <b>hinge</b> &amp; <b>squared</b> <b>hinge</b> <b>loss</b> with TensorFlow 2 and Keras ...", "url": "https://www.machinecurve.com/index.php/2019/10/15/how-to-use-hinge-squared-hinge-loss-with-keras/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machinecurve</b>.com/index.php/2019/10/15/how-to-use-<b>hinge</b>-<b>squared</b>-<b>hinge</b>-<b>loss</b>...", "snippet": "<b>Squared</b> <b>hinge</b> <b>loss</b> may then be what you are looking for, especially when you already considered the <b>hinge</b> <b>loss</b> function for your machine learning problem. <b>Squared</b> <b>hinge</b> <b>loss</b> is nothing else but a square of the output of the <b>hinge</b>\u2019s \\(max(\u2026)\\) function. It generates a <b>loss</b> function as illustrated above, compared to regular <b>hinge</b> <b>loss</b>. As you <b>can</b> see, larger errors are punished more significantly than with traditional <b>hinge</b>, whereas smaller errors are punished slightly lightlier ...", "dateLastCrawled": "2022-01-31T18:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Bayes <b>Decision</b> Rule for prediction problems", "url": "https://stephens999.github.io/fiveMinuteStats/decision_theory_bayes_rule.html", "isFamilyFriendly": true, "displayUrl": "https://stephens999.github.io/fiveMinuteStats/<b>decision</b>_theory_bayes_rule.html", "snippet": "<b>squared</b> <b>loss</b>: \\(L(\\hat{Y},Y) = (Y-\\hat{Y})^2\\) ... Thus, the above result (\u201cOptimal <b>Decision</b> Rule\u201d section) <b>can</b> <b>be thought</b> of as characterizing the optimality of Bayesian inference in terms of a \u201cfrequentist\u201d measure (\\(r\\)) which measures long-run performance across many samples \\((X,Y)\\) from \\(p(X,Y)\\). For example, predicting \\(Y\\) by its posterior <b>mean</b>, \\(E(Y|X)\\), is optimal in terms of expected <b>squared</b> <b>loss</b> (with expectation taken across \\(p(X,Y)\\)). Because of this connection ...", "dateLastCrawled": "2022-02-03T07:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Loss</b> Functions. <b>Loss</b> functions explanations and\u2026 | by Tomer Kordonsky ...", "url": "https://medium.com/artificialis/loss-functions-361b2ad439a0", "isFamilyFriendly": true, "displayUrl": "https://medium.com/artificialis/<b>loss</b>-functions-361b2ad439a0", "snippet": "We <b>can</b> achieve this using the Huber <b>Loss</b> (Smooth L1 <b>Loss</b>), a combination of L1 (MAE) and L2 (MSE) losses. <b>Can</b> be called Huber <b>Loss</b> or Smooth MAE Less sensitive to outliers in data than the <b>squared</b> ...", "dateLastCrawled": "2022-02-03T02:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "java - correct implementation of <b>Hinge</b> <b>loss</b> minimization for gradient ...", "url": "https://stackoverflow.com/questions/28988732/correct-implementation-of-hinge-loss-minimization-for-gradient-descent", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/28988732", "snippet": "You <b>can</b> use gradient descent to train a linear SVM for sure, but your approach is a bit strange. First, lets try to fix the obvious: for an SVM (and for the <b>Hinge</b> <b>loss</b> function) your classes have to be -1 and 1, not 0 and 1. If you are encoding your classes as 0 and 1, the <b>Hinge</b> <b>loss</b> function will not work. \u2013", "dateLastCrawled": "2022-01-27T11:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Loss</b> Functions and Optimization Algorithms. Demystified. | by Apoorva ...", "url": "https://medium.com/data-science-group-iitr/loss-functions-and-optimization-algorithms-demystified-bb92daff331c", "isFamilyFriendly": true, "displayUrl": "https://medium.com/data-science-group-iitr/<b>loss</b>-functions-and-optimization-algorithms...", "snippet": "One of the most widely used <b>loss function</b> is <b>mean</b> square <b>error</b>, which calculates the square of difference between actual value and predicted value. Different <b>loss</b> functions are used to deal with ...", "dateLastCrawled": "2022-01-31T23:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Cost Function and <b>Loss</b> Function in Machine Learning - Shishir Kant Singh", "url": "http://shishirkant.com/cost-function-and-loss-function-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "shishirkant.com/cost-function-and-<b>loss</b>-function-in-machine-learning", "snippet": "With machine learning, features associated with it also have flourished. The driving force behind optimization in machine learning is the response from an internal function of the algorithm, called the cost function. Cost Function It\u2019s a function that determines how well a Machine Learning model performs for a given set of data. TheContinue Reading", "dateLastCrawled": "2022-01-29T23:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>correct implementation of Hinge loss minimization for gradient descent</b> ...", "url": "https://www.reddit.com/r/MachineLearning/comments/2yoi5v/correct_implementation_of_hinge_loss_minimization/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/.../2yoi5v/<b>correct_implementation_of_hinge_loss_minimization</b>", "snippet": "<b>correct implementation of Hinge loss minimization for gradient descent</b>. Close. 0. Posted by 5 years ago. Archived. <b>correct implementation of Hinge loss minimization for gradient descent</b>. I copied the <b>hinge</b> <b>loss</b> function from here (also LossC and LossFunc upon which it&#39;s based. Then I included it in my gradient descent algorithm like so: ...", "dateLastCrawled": "2021-02-16T06:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "scikit-learn \ud83d\ude80 - <b>MSE is negative when returned by cross</b>_val_score ...", "url": "https://bleepcoder.com/scikit-learn/19381477/mse-is-negative-when-returned-by-cross-val-score", "isFamilyFriendly": true, "displayUrl": "https://bleepcoder.com/scikit-learn/19381477/<b>mse-is-negative-when-returned-by-cross</b>...", "snippet": "At least I asked myself how a the <b>mean</b> of a square <b>can</b> possibly be negative and <b>thought</b> that cross_val_score was not working correctly or did not use the supplied metric. Only after digging in the sklearn source code I realized that the sign was flipped. This behavior is mentioned in make_scorer in scorer.py, however it&#39;s not mentioned in cross_val_score and I think it should be, because otherwise it makes people think that cross_val_score is not working correctly. API Bug Documentation ...", "dateLastCrawled": "2022-01-25T07:46:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Picking Loss Functions - A comparison between MSE, Cross Entropy</b>, and ...", "url": "https://rohanvarma.me/Loss-Functions/", "isFamilyFriendly": true, "displayUrl": "https://rohanvarma.me/<b>Loss</b>-Functions", "snippet": "There are several different common <b>loss</b> functions to choose from: the cross-entropy <b>loss</b>, the <b>mean</b>-<b>squared</b> <b>error</b>, the huber <b>loss</b>, and the <b>hinge</b> <b>loss</b> - just to name a few. Given a particular model, each <b>loss</b> function has particular properties that make it interesting - for example, the (L2-regularized) <b>hinge</b> <b>loss</b> comes with the maximum-margin ...", "dateLastCrawled": "2021-12-04T02:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>What Are Loss Functions in ML</b> <b>And Why Are They Important</b>", "url": "https://analyticsindiamag.com/what-are-loss-functions-in-ml-and-why-are-they-important/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/<b>what-are-loss-functions-in-ml</b>-<b>and-why-are-they-important</b>", "snippet": "<b>Mean</b>-<b>Squared</b> <b>error</b>; Cross-entropy <b>loss</b>; <b>Hinge</b> <b>loss</b>; Huber <b>Loss</b>; <b>Mean</b>-<b>Squared</b> <b>Loss</b>. <b>Mean</b> <b>Squared</b> <b>Error</b>(MSE) is used to measure the accuracy of an estimator. The lesser the value of MSE, the better are the predictions. This expression <b>can</b> be defined as the <b>mean</b> value of the <b>squared</b> deviations of the predicted values from that of true values. Here \u2018n\u2019 denotes the total number of samples in the data. Cross-Entropy. Cross entropy is a widely popular concept of information theory. It is the ...", "dateLastCrawled": "2022-02-02T06:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "How to use <b>hinge</b> &amp; <b>squared</b> <b>hinge</b> <b>loss</b> with TensorFlow 2 and Keras ...", "url": "https://www.machinecurve.com/index.php/2019/10/15/how-to-use-hinge-squared-hinge-loss-with-keras/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machinecurve</b>.com/index.php/2019/10/15/how-to-use-<b>hinge</b>-<b>squared</b>-<b>hinge</b>-<b>loss</b>...", "snippet": "Today, we\u2019ll cover two closely related <b>loss</b> functions that <b>can</b> be used in neural networks \u2013 and hence in TensorFlow 2 based Keras \u2013 that behave similar to how a Support Vector Machine generates a decision boundary for classification: the <b>hinge</b> <b>loss</b> and <b>squared</b> <b>hinge</b> <b>loss</b>. In this blog, you\u2019ll first find a brief introduction to the two ...", "dateLastCrawled": "2022-01-31T18:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Loss</b> Functions in Neural Networks", "url": "https://www.theaidream.com/post/loss-functions-in-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://www.theaidream.com/post/<b>loss</b>-functions-in-neural-networks", "snippet": "<b>Loss</b> functions are mainly classified into two different categories Classification <b>loss</b> and Regression <b>Loss</b>. Classification <b>loss</b> is the case where the aim is to predict the output from the different categorical values for example, if we have a dataset of handwritten images and the digit is to be predicted that lies between (0\u20139), in these kinds of scenarios classification <b>loss</b> is used.", "dateLastCrawled": "2022-02-02T10:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "What is <b>the advantage/disadvantage of Hinge-loss compared</b> to cross ...", "url": "https://www.quora.com/What-is-the-advantage-disadvantage-of-Hinge-loss-compared-to-cross-entropy", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>the-advantage-disadvantage-of-Hinge-loss-compared</b>-to...", "snippet": "Answer (1 of 2): Cross Entropy (or Log <b>Loss</b>), Hing <b>Loss</b> (SVM <b>Loss</b>), <b>Squared</b> <b>Loss</b> etc. are different forms of <b>Loss</b> functions. Log <b>Loss</b> in the classification context gives Logistic Regression, while the <b>Hinge</b> <b>Loss</b> is Support Vector Machines. Logistic Regression and SVMs both are linear classifiers,...", "dateLastCrawled": "2022-01-29T16:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "How to Choose <b>Loss</b> Functions When Training Deep Learning Neural Networks", "url": "https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/how-to-choose-<b>loss</b>-functions-when-training-deep...", "snippet": "<b>Squared</b> <b>Hinge</b> <b>Loss</b>; Multi-Class Classification <b>Loss</b> Functions Multi-Class Cross-Entropy <b>Loss</b>; Sparse Multiclass Cross-Entropy <b>Loss</b>; Kullback Leibler Divergence <b>Loss</b>; We will focus on how to choose and implement different <b>loss</b> functions. For more theory on <b>loss</b> functions, see the post: <b>Loss</b> and <b>Loss</b> Functions for Training Deep Learning Neural Networks ; Regression <b>Loss</b> Functions. A regression predictive modeling problem involves predicting a real-valued quantity. In this section, we will ...", "dateLastCrawled": "2022-02-02T15:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>About loss and loss functions</b> \u2013 MachineCurve", "url": "https://www.machinecurve.com/index.php/2019/10/04/about-loss-and-loss-functions/", "isFamilyFriendly": true, "displayUrl": "https://www.machinecurve.com/index.php/2019/10/04/<b>about-loss-and-loss-functions</b>", "snippet": "<b>Squared</b> <b>hinge</b> <b>loss</b>, on the other hand, is differentiable, simply because of the square and the mathematical benefits it introduces during differentiation. This makes it easier for us to use a <b>hinge</b>-like <b>loss</b> in gradient based optimization \u2013 we\u2019ll simply take <b>squared</b> <b>hinge</b>. Categorical / multiclass <b>hinge</b>. Both normal <b>hinge</b> and <b>squared</b> <b>hinge</b> <b>loss</b> work only for binary classification problems in which the actual target value is either +1 or -1. Although that\u2019s perfectly fine for when you ...", "dateLastCrawled": "2022-01-25T13:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Loss</b> Functions Part 2 | Akash\u2019s Research Blog", "url": "https://akashmehra.github.io/blog/loss_functions/2021/08/10/loss_functions_part2.html", "isFamilyFriendly": true, "displayUrl": "https://akashmehra.github.io/blog/<b>loss</b>_functions/2021/08/10/<b>loss</b>_functions_part2.html", "snippet": "<b>Loss</b> Functions Part 2. In this part of the multi-part series on the <b>loss</b> functions we&#39;ll be taking a look at MSE, MAE, Huber <b>Loss</b>, <b>Hinge</b> <b>Loss</b>, and Triplet <b>Loss</b>. We&#39;ll also look at the code for these <b>Loss</b> functions in PyTorch and some examples of how to use them. Aug 10, 2021 \u2022 Akash Mehra \u2022 10 min read. <b>loss</b>_functions.", "dateLastCrawled": "2022-01-26T22:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "machine learning - <b>hinge loss</b> vs logistic <b>loss</b> advantages and ...", "url": "https://stats.stackexchange.com/questions/146277/hinge-loss-vs-logistic-loss-advantages-and-disadvantages-limitations", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/146277/<b>hinge-loss</b>-vs-logistic-<b>loss</b>...", "snippet": "<b>Hinge loss</b> <b>can</b> be defined using $\\text{max}(0, 1-y_i\\mathbf{w}^T\\mathbf{x}_i)$ and the log <b>loss</b> <b>can</b> be defined as $\\text{log}(1 + \\exp(-y_i\\mathbf{w}^T\\mathbf{x}_i))$ I have the following question... Stack Exchange Network. Stack Exchange network consists of 178 Q&amp;A communities including Stack Overflow, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers. Visit Stack Exchange. Loading\u2026 0 +0; Tour Start here for a quick overview ...", "dateLastCrawled": "2022-01-26T09:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "A Brief Overview of <b>Loss</b> Functions in <b>Pytorch</b> | by Pratyaksha Jha ...", "url": "https://medium.com/udacity-pytorch-challengers/a-brief-overview-of-loss-functions-in-pytorch-c0ddb78068f7", "isFamilyFriendly": true, "displayUrl": "https://medium.com/udacity-<b>pytorch</b>-challengers/a-brief-overview-of-<b>loss</b>-functions-in-p...", "snippet": "<b>Hinge</b> Embedding <b>Loss</b>. torch.nn.HingeEmbeddingLoss. Measures the <b>loss</b> given an input tensor x and a labels tensor y containing values (1 or -1). It is used for measuring whether two inputs are ...", "dateLastCrawled": "2022-02-02T14:21:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Introduction to Machine Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/24/introduction-to-machine-learning-2/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/24/<b>introduction-to-machine-learning</b>-2", "snippet": "<b>Squared</b> <b>loss</b> (for regression) <b>Hinge</b> <b>loss</b> (SVM) Logistic/log <b>loss</b> (logistic regression) Some <b>loss</b> functions are as follows: When to stop tuning <b>machine</b> <b>learning</b> models. When to stop tuning the hyperparameters in a <b>machine</b> <b>learning</b> model is a million-dollar question. This problem can be mostly solved by keeping tabs on training and testing errors. While increasing the complexity of a model, the following stages occur: Stage 1: Underfitting stage \u2013 high train and high test errors (or low ...", "dateLastCrawled": "2022-01-17T05:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Data Science: <b>Support Vector Machines (SVM</b>)", "url": "https://www.datasciencesmachinelearning.com/2019/01/support-vector-machines-svm.html", "isFamilyFriendly": true, "displayUrl": "https://www.datasciences<b>machinelearning</b>.com/2019/01/<b>support-vector-machines-svm</b>.html", "snippet": "In this case, <b>squared</b> <b>hinge</b> <b>loss</b> function (as against <b>hinge</b> <b>loss</b> function) and l2 penalty are the major changes compared to the earlier three methods. This method is useful for when sample size is larger.", "dateLastCrawled": "2022-01-28T08:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Introduction to <b>Machine</b> <b>Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/30/introduction-to-machine-learning-3/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/30/introduction-to-<b>machine</b>-<b>learning</b>-3", "snippet": "However, in <b>machine</b> <b>learning</b> methodology, <b>squared</b> <b>loss</b> will be minimized with respect to ... <b>Squared</b> <b>loss</b> (for regression) <b>Hinge</b> <b>loss</b> (SVM) Logistic/log <b>loss</b> (logistic regression) Some <b>loss</b> functions are as follows: When to stop tuning <b>machine</b> <b>learning</b> models. When to stop tuning the hyperparameters in a <b>machine</b> <b>learning</b> model is a million-dollar question. This problem can be mostly solved by keeping tabs on training and testing errors. While increasing the complexity of a model, the ...", "dateLastCrawled": "2022-01-28T18:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A study on L2-<b>loss (Squared Hinge-Loss) multiclass SVM</b> | Request PDF", "url": "https://www.researchgate.net/publication/235884495_A_study_on_L2-loss_Squared_Hinge-Loss_multiclass_SVM", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/235884495_A_study_on_L2-<b>loss</b>_<b>Squared</b>_<b>Hinge</b>...", "snippet": "Taking the <b>analogy</b> to classification task, it has been previously studied [13] that using the <b>squared</b> <b>hinge</b> <b>loss</b> in SVM would yield better accuracy when \u03bb is large. In this case, underfitting ...", "dateLastCrawled": "2021-12-14T01:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Metrics to Evaluate Classification and Regression Algorithms | by ...", "url": "https://medium.com/@poojitha.penmethsa/metrics-to-evaluate-classification-and-regression-algorithms-1554f1e00a75", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@poojitha.penmethsa/metrics-to-evaluate-classification-and...", "snippet": "<b>Hinge</b> <b>loss</b>. cross-entropy <b>loss</b> / log <b>loss</b>. likelihood <b>loss</b>. MSE / Quadratic <b>loss</b> / L2 <b>loss</b>: Mean <b>Squared</b> Error, or MSE <b>loss</b> is the default <b>loss</b> to use for regression problems. Mathematically, it ...", "dateLastCrawled": "2022-01-17T01:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Understanding the 3 most common <b>loss</b> functions for <b>Machine</b> <b>Learning</b> ...", "url": "https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-the-3-most-common-<b>loss</b>-functions-for...", "snippet": "A <b>loss function</b> in <b>Machine</b> <b>Learning</b> is a measure of how accurately your ML model is able to predict the expected outcome i.e the ground truth. The <b>loss function</b> will take two items as input: the output value of our model and the ground truth expected value. The output of the <b>loss function</b> is called the <b>loss</b> which is a measure of how well our model did at predicting the outcome. A high value for the <b>loss</b> means our model performed very poorly. A low value for the <b>loss</b> means our model performed ...", "dateLastCrawled": "2022-02-02T13:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and-sample-codes ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/README.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "Models 1.1 Support vector <b>machine</b> 1.1.1 Principle 1.1.2 Kernel 1.1.3 Soft margin SVM 1.1.4 <b>Hinge</b> <b>loss</b> view 1.1.5 Multi-class SVM 1.1.6 Extensions 1.2 Tree-based models 1.2.1 Decision tree 1.2.2 Random forest 1.2.3 Gradient boosted decision trees 1.2.4 Tools 1.3 EM Principle 1.4 MaxEnt 1.4.1 Entropy 1.5 Model selection 1.5.1 Under-fitting / Over-fitting 1.5.2 Model ensemble, sklearn 2.", "dateLastCrawled": "2021-09-22T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "3.3. Metrics and scoring: quantifying the quality of predictions ...", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "isFamilyFriendly": true, "displayUrl": "https://scikit-learn.org/stable/modules/model_evaluation.html", "snippet": "<b>hinge</b>_<b>loss</b> (y_true, pred_decision, *[, ...]) Average <b>hinge</b> <b>loss</b> (non-regularized). ... \u201cThe Matthews correlation coefficient is used in <b>machine</b> <b>learning</b> as a measure of the quality of binary (two-class) classifications. It takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used even if the classes are of very different sizes. The MCC is in essence a correlation coefficient value between -1 and +1. A coefficient of +1 ...", "dateLastCrawled": "2022-02-03T06:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Fast generalization rates for distance metric</b> <b>learning</b> - <b>Machine</b> <b>Learning</b>", "url": "https://link.springer.com/article/10.1007/s10994-018-5734-0", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10994-018-5734-0", "snippet": "For example, the <b>squared</b> <b>hinge</b> <b>loss</b> \\(\\ell _s^1(x) = \\max (1-x, 0) ... <b>Analogy</b>-preserving semantic embedding for visual object categorization. In Proceedings of the 30th international conference on <b>machine</b> <b>learning</b>, Atlanta, GA (pp. 639\u2013647). Jin, R., Wang, S., &amp; Zhou, Y. (2010). Regularized distance metric <b>learning</b>: Theory and algorithm. Advances in neural information processing systems (Vol. 23, pp. 862\u2013870). Cambridge, MA: MIT Press. Google Scholar Kulis, B. (2012). Metric <b>learning</b>: A ...", "dateLastCrawled": "2021-12-28T14:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "CPSC 340: Data Mining <b>Machine</b> <b>Learning</b>", "url": "https://www.cs.ubc.ca/~schmidtm/Courses/340-F16/L35.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.ubc.ca/~schmidtm/Courses/340-F16/L35.pdf", "snippet": "<b>Machine</b> <b>Learning</b> and Data Mining Course Review/Preview Fall 2016 Some images from this lecture are taken from Google Image Search. Admin \u2022Assignment 6: \u20131 late day to hand in next Monday, 2 for Wednesday, 3 for Friday. \u2022Final: \u2013December 12 (8:30am \u2013HEBB 100) \u2013Covers Assignments 1-6. \u2013List of topics posted. \u2013Final from last year will be posted after class. \u2013Closed-book, cheat sheet: 4-pages each double-sided. Last Time: Semi-Supervised <b>Learning</b> \u2022In semi-supervised <b>learning</b> ...", "dateLastCrawled": "2021-11-22T01:45:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>About loss and loss functions</b> \u2013 MachineCurve", "url": "https://www.machinecurve.com/index.php/2019/10/04/about-loss-and-loss-functions/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/index.php/2019/10/04/<b>about-loss-and-loss-functions</b>", "snippet": "We\u2019re then using <b>machine</b> <b>learning</b> for ... The <b>squared hinge loss is like</b> the hinge formula displayed above, but then the \\(max()\\) function output is squared. This helps achieving two things: Firstly, it makes the loss value more sensitive to outliers, just as we saw with MSE vs MAE. Large errors will add to the loss more significantly than smaller errors. Note that simiarly, this may also mean that you\u2019ll need to inspect your dataset for the presence of such outliers first. Secondly ...", "dateLastCrawled": "2022-01-25T13:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "\u635f\u5931\u51fd\u6570 - \u7b97\u6cd5\u6742\u8d27\u94fa - bjmsong.github.io", "url": "https://bjmsong.github.io/2020/02/21/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/", "isFamilyFriendly": true, "displayUrl": "https://bjmsong.github.io/2020/02/21/\u635f\u5931\u51fd\u6570", "snippet": "the training data is fed into the <b>machine</b> <b>learning</b> model; Loss : compare between some actual targets and predicted targets; the lower the loss, the more the set of targets and the set of predictions resemble each other; the more they resemble each other, the better the <b>machine</b> <b>learning</b> model performs. Backward pass", "dateLastCrawled": "2021-12-27T11:43:00.0000000Z", "language": "zh_chs", "isNavigational": false}], [], [], [], []], "all_bing_queries": ["+(squared hinge loss)  is like +(mean squared error)", "+(squared hinge loss) is similar to +(mean squared error)", "+(squared hinge loss) can be thought of as +(mean squared error)", "+(squared hinge loss) can be compared to +(mean squared error)", "machine learning +(squared hinge loss AND analogy)", "machine learning +(\"squared hinge loss is like\")", "machine learning +(\"squared hinge loss is similar\")", "machine learning +(\"just as squared hinge loss\")", "machine learning +(\"squared hinge loss can be thought of as\")", "machine learning +(\"squared hinge loss can be compared to\")"]}