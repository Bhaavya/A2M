{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "On Consequentialism and <b>Fairness</b>", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7861221/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7861221", "snippet": "5.1.2. <b>Individual</b> <b>Fairness</b> . A more general application of <b>the same</b> idea argues that models must make similar predictions for similar individuals (in terms of their representations, X) (Dwork et al., 2012). This proposal was originally framed as being in the Rawlsian tradition, suggesting it should be a matter of public deliberation to ...", "dateLastCrawled": "2022-01-28T22:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "On the Apparent Conflict Between <b>Individual</b> and Group <b>Fairness</b> | DeepAI", "url": "https://deepai.org/publication/on-the-apparent-conflict-between-individual-and-group-fairness", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/.../on-the-apparent-conflict-between-<b>individual</b>-and-group-<b>fairness</b>", "snippet": "This is the idea that people deserve to be treated as unique individuals, and assessed on a case-by-case basis. <b>Like</b> <b>individual</b> <b>fairness</b>, this concept is grounded in the treatment of individuals. However, on this view, even supposedly individually-fair models can be seen as individually unjust because they still generalise between people who share <b>the same</b> features. In this sense, what passes for <b>individual</b> <b>fairness</b> actually amounts to a special case of group <b>fairness</b>. So again the apparent ...", "dateLastCrawled": "2022-01-15T05:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Building ML models for everyone: understanding <b>fairness</b> in machine ...", "url": "https://cloud.google.com/blog/products/ai-machine-learning/building-ml-models-for-everyone-understanding-fairness-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://cloud.google.com/blog/products/ai-machine-learning/building-ml-<b>model</b>s-for...", "snippet": "Rather than thinking of <b>fairness</b> as a separate initiative, it\u2019s important to apply <b>fairness</b> analysis throughout your entire ML process, making sure to continuously reevaluate your models from the perspective of <b>fairness</b> and inclusion. This is especially important when AI is deployed in critical business processes, <b>like</b> credit application reviews and medical diagnosis, that affect a wide range of end users.", "dateLastCrawled": "2022-02-01T03:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Human-centric Approach to <b>Fairness</b> in AI", "url": "https://www.timlrx.com/blog/a-human-centric-approach-to-fairness-in-ai", "isFamilyFriendly": true, "displayUrl": "https://www.timlrx.com/blog/a-human-centric-approach-to-<b>fairness</b>-in-ai", "snippet": "Of course, we should think about <b>fairness</b> in the context of prejudiced groups, but we should also ask whether it is fair to an <b>individual</b>. Adding constraints in models might lead to worse outcomes for other individuals. If the decision making processs has serious consequences e.g. a fraud model, it is hard to think that we are truly fair by forcing models to obtain an equal number of fraud cases across certain demographic attributes.", "dateLastCrawled": "2022-02-03T02:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Fairness</b> in rankings and recommendations: an overview | SpringerLink", "url": "https://link.springer.com/article/10.1007/s00778-021-00697-y", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00778-021-00697-y", "snippet": "<b>Individual</b> <b>fairness</b> definitions are based on the premise that similar entities should be treated similarly. Group <b>fairness</b> definitions group entities based on the value of one or more protected attributes and ask that all groups are treated similarly. To operationalize both approaches to <b>fairness</b>, we need to define similarity for the input and the output of an algorithm. For input similarity, we need a means of quantifying similarity of entities in the case of <b>individual</b> <b>fairness</b>, and, a way ...", "dateLastCrawled": "2022-02-03T01:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) On the Apparent Conflict Between <b>Individual</b> and Group <b>Fairness</b>", "url": "https://www.researchgate.net/publication/337967636_On_the_Apparent_Conflict_Between_Individual_and_Group_Fairness", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/337967636_On_the_Apparent_Conflict_Between...", "snippet": "posed in the fair machine learning literature. The conclusion is. that the apparent con\ufb02ict between <b>individual</b> and group <b>fairness</b>. is more of an artefact of the blunt application of <b>fairness</b> ...", "dateLastCrawled": "2021-11-01T06:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Chapter 11 Bias and Fairness</b> | Big Data and Social Science", "url": "https://textbook.coleridgeinitiative.org/chap-bias.html", "isFamilyFriendly": true, "displayUrl": "https://textbook.coleridgeinitiative.org/chap-bias.html", "snippet": "Unfortunately, just as there is no single machine learning algorithm that is best suited to <b>every</b> application, no one <b>fairness</b> metric will fit <b>every</b> situation. However, we hope this chapter will provide you with a grounding in the available ways of measuring algorithmic <b>fairness</b> that will help you navigate the trade-offs involved putting these into practice in your own applications. 11.2 Sources of Bias. Bias may be introduced into a machine learning project at any step along the way and it ...", "dateLastCrawled": "2022-01-30T01:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Building <b>prediction</b> models with grouped data: A case study on the ...", "url": "https://onlinelibrary.wiley.com/doi/full/10.1111/1748-8583.12396", "isFamilyFriendly": true, "displayUrl": "https://onlinelibrary.wiley.com/doi/full/10.1111/1748-8583.12396", "snippet": "First, the small number of respondents in each group impeded the most important feature of the CRE approach, namely the separation of <b>individual</b> effects (e.g., the effect of one&#39;s <b>fairness</b> perception) and the context effects (e.g., the effect of the <b>fairness</b> climate). Second, with the CRE approach, the high complexity of the models results in <b>prediction</b> models that overfit training sets but cannot be generalized well to test sets. Another interesting finding is that the SL approach, which ...", "dateLastCrawled": "2021-08-21T17:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Human <b>Judgment in algorithmic loops: Individual justice and</b> automated ...", "url": "https://onlinelibrary.wiley.com/doi/full/10.1111/rego.12358", "isFamilyFriendly": true, "displayUrl": "https://onlinelibrary.wiley.com/doi/full/10.1111/rego.12358", "snippet": "<b>Individual</b> <b>fairness</b> measures involve pre-specifying the subset of features that define <b>individual</b> similarity in the context of the task. By contrast, the kind of <b>individual</b> justice addressed above starts from the assumption that even if the facts at hand suggest two cases are exactly alike, a fresh consideration is necessary because there&#39;s a chance that different features, or alternative reasoning, might lead to a different decision.", "dateLastCrawled": "2021-11-23T10:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Mirror Mirror", "url": "https://shiraamitchell.github.io/fairness/", "isFamilyFriendly": true, "displayUrl": "https://shiraamitchell.github.io/<b>fairness</b>", "snippet": "In the hiring example, <b>individual</b> counterfactual <b>fairness</b> can be regarded as a negative answer to each <b>individual</b>\u2019s question \u201cwould the decision have been different if I were not black?\u201d Counterfactual parity provides a negative answer to the question \u201cwould the rates of diring be different if everyone were black?\u201d", "dateLastCrawled": "2022-01-06T19:19:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "What&#39;s Fair about <b>Individual</b> <b>Fairness</b>?", "url": "https://www.researchgate.net/publication/353603675_What's_Fair_about_Individual_Fairness", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/353603675_What", "snippet": "<b>Individual</b> <b>fairness</b> is motivated by an intuitive principle I call &quot;<b>similar</b> treatment,&quot; which requires that <b>similar</b> individuals be treated similarly. IF offers a precise account of this definition ...", "dateLastCrawled": "2021-10-22T09:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "The (Im)possibility of <b>Fairness: Different Value Systems Require</b> ...", "url": "https://cacm.acm.org/magazines/2021/4/251365-the-impossibility-of-fairness/fulltext", "isFamilyFriendly": true, "displayUrl": "https://cacm.acm.org/magazines/2021/4/251365-the-impossibility-of-<b>fairness</b>", "snippet": "One such definition is <b>individual</b> <b>fairness</b>: 10 individuals who are <b>similar</b> (with respect to some task) should be treated similarly (with respect to that task). Simultaneously, a different definition states that demographic groups should, on the whole, receive <b>similar</b> decisions. This group <b>fairness</b> definition is inspired by civil rights law in the U.S. 5,11 and U.K. 21 Other definitions state that fair systems should err evenly across demographic groups. 7,13,24 Many of these definitions have ...", "dateLastCrawled": "2022-02-03T00:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "On Consequentialism and <b>Fairness</b>", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7861221/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7861221", "snippet": "5.1.2. <b>Individual</b> <b>Fairness</b> . A more general application of <b>the same</b> idea argues that models must make <b>similar</b> predictions for <b>similar</b> individuals (in terms of their representations, X) (Dwork et al., 2012). This proposal was originally framed as being in the Rawlsian tradition, suggesting it should be a matter of public deliberation to ...", "dateLastCrawled": "2022-01-28T22:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "On the Apparent Conflict Between <b>Individual</b> and Group <b>Fairness</b> | DeepAI", "url": "https://deepai.org/publication/on-the-apparent-conflict-between-individual-and-group-fairness", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/.../on-the-apparent-conflict-between-<b>individual</b>-and-group-<b>fairness</b>", "snippet": "<b>Individual</b> <b>fairness</b> says that for any two individuals, if their distance in task-relevant similarity is sufficiently small, they should receive <b>the same</b> outcome (as defined in (Dwork et al., 2012)); or alternatively, that \u2018less qualified individuals should not be favored over more qualified individuals\u2019 (Joseph et al., 2016).", "dateLastCrawled": "2022-01-15T05:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) On the Apparent Conflict Between <b>Individual</b> and Group <b>Fairness</b>", "url": "https://www.researchgate.net/publication/337967636_On_the_Apparent_Conflict_Between_Individual_and_Group_Fairness", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/337967636_On_the_Apparent_Conflict_Between...", "snippet": "posed in the fair machine learning literature. The conclusion is. that the apparent con\ufb02ict between <b>individual</b> and group <b>fairness</b>. is more of an artefact of the blunt application of <b>fairness</b> ...", "dateLastCrawled": "2021-11-01T06:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Towards Reducing Biases in Combining Multiple Experts Online", "url": "https://www.ijcai.org/proceedings/2021/0416.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.ijcai.org/proceedings/2021/0416.pdf", "snippet": "<b>Individual</b> <b>fairness</b> builds upon \u201c<b>treating</b> <b>similar</b> individuals similarly\u201d [Dwork et al., 2012]. Proceedings of the Thirtieth International Joint Conference on Arti\ufb01cial Intelligence (IJCAI-21) 3024. On the other hand, group <b>fairness</b> is achieved by balanc-ing certain statistical metrics approximately across differ-ent demographic groups (such as groups divided by gender, race, etc). Equalized odds [Zafar et al., 2017] [Hardt et al., 2016], a.k.a. disparate mistreatment, requires that no ...", "dateLastCrawled": "2022-02-02T10:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Building ML models for everyone: understanding <b>fairness</b> in machine ...", "url": "https://cloud.google.com/blog/products/ai-machine-learning/building-ml-models-for-everyone-understanding-fairness-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://cloud.google.com/blog/products/ai-machine-learning/building-ml-<b>model</b>s-for...", "snippet": "If you select Partial dependence plots on the top left, you can see how <b>individual</b> features impact the <b>model</b>\u2019s <b>prediction</b> for an <b>individual</b> data point (if you have one selected), or globally across all data points. In the global dependence plots here, we can see that the overall quality rating of a house had a significant effect on the <b>model</b>\u2019s <b>prediction</b> (price increases as quality rating increases) but the number of bedrooms above ground did not:", "dateLastCrawled": "2022-02-01T03:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Interventions | <b>Machine Learning Bias Mitigation</b>", "url": "https://cdeiuk.github.io/bias-mitigation/interventions/", "isFamilyFriendly": true, "displayUrl": "https://cdeiuk.github.io/bias-mitigation/interventions", "snippet": "Since the intervention is <b>the same</b> independently of the <b>fairness</b> notion considered, the reported accuracy of the <b>Fairness</b> Through Unawareness model for different notions of <b>fairness</b> is <b>the same</b>. Experimental results. Finance. We applied the intervention to the adult dataset in order to impose different notions of <b>fairness</b> with respect to sex. Run our analysis yourself on Binder. Demographic parity. The intervention reduced demographic parity difference only slightly, from 0.193 to 0.174 ...", "dateLastCrawled": "2022-02-02T02:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Mirror Mirror", "url": "https://shiraamitchell.github.io/fairness/", "isFamilyFriendly": true, "displayUrl": "https://shiraamitchell.github.io/<b>fairness</b>", "snippet": "In the hiring example, <b>individual</b> counterfactual <b>fairness</b> can be regarded as a negative answer to each <b>individual</b>\u2019s question \u201cwould the decision have been different if I were not black?\u201d Counterfactual parity provides a negative answer to the question \u201cwould the rates of diring be different if everyone were black?\u201d", "dateLastCrawled": "2022-01-06T19:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Human <b>Judgment in algorithmic loops: Individual justice and</b> automated ...", "url": "https://onlinelibrary.wiley.com/doi/full/10.1111/rego.12358", "isFamilyFriendly": true, "displayUrl": "https://onlinelibrary.wiley.com/doi/full/10.1111/rego.12358", "snippet": "Note that the notion of <b>treating</b> people as individuals is not shared by all accounts of discrimination, ... in so far as it implies that <b>every</b> case deserves <b>individual</b> assessment, no matter how <b>similar</b> it may be to a pre-existing case. Another way to allocate human review is on the basis of contestation by those who are subject to an automated decision. This is the approach envisioned under the safeguard included in Article 22(3) for data subjects to contest and request human intervention in ...", "dateLastCrawled": "2021-11-23T10:31:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "On the Apparent Conflict Between <b>Individual</b> and Group <b>Fairness</b> | DeepAI", "url": "https://deepai.org/publication/on-the-apparent-conflict-between-individual-and-group-fairness", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/.../on-the-apparent-conflict-between-<b>individual</b>-and-group-<b>fairness</b>", "snippet": "Although they do not address the relationship between different worldviews and the <b>individual</b> / group <b>fairness</b> distinction, focusing on assumed worldviews in this way <b>can</b> help illustrate why apparent conflicts between <b>individual</b> and group <b>fairness</b> are misguided. The important difference is between worldviews, not whether we render our assumptions about them at the <b>individual</b> or group level. The reason for <b>treating</b> people differently on the basis of group membership is the assumption that it ...", "dateLastCrawled": "2022-01-15T05:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) On the Apparent Conflict Between <b>Individual</b> and Group <b>Fairness</b>", "url": "https://www.researchgate.net/publication/337967636_On_the_Apparent_Conflict_Between_Individual_and_Group_Fairness", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/337967636_On_the_Apparent_Conflict_Between...", "snippet": "posed in the fair machine learning literature. The conclusion is. that the apparent con\ufb02ict between <b>individual</b> and group <b>fairness</b>. is more of an artefact of the blunt application of <b>fairness</b> ...", "dateLastCrawled": "2021-11-01T06:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Fairness in machine learning with tractable models</b> - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S0950705120308443", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0950705120308443", "snippet": "The solution presented here <b>can</b> <b>be thought</b> of as \u201c<b>fairness</b> through percentile equivalence\u201d. The central premise of this adjustment is that individuals at <b>the same</b> percentile of their respective distributions should be treated identically. Therefore, the training variables are adjusted in such a way that individuals with attributes at <b>the same</b> percentile of their own distribution are allocated adjusted variables with <b>the same</b> value. This modification results in a \u201cfair world\u201d where ...", "dateLastCrawled": "2022-01-20T21:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Human-centric Approach to <b>Fairness</b> in AI", "url": "https://www.timlrx.com/blog/a-human-centric-approach-to-fairness-in-ai", "isFamilyFriendly": true, "displayUrl": "https://www.timlrx.com/blog/a-human-centric-approach-to-<b>fairness</b>-in-ai", "snippet": "Of course, we should think about <b>fairness</b> in the context of prejudiced groups, but we should also ask whether it is fair to an <b>individual</b>. Adding constraints in models might lead to worse outcomes for other individuals. If the decision making processs has serious consequences e.g. a fraud model, it is hard to think that we are truly fair by forcing models to obtain an equal number of fraud cases across certain demographic attributes.", "dateLastCrawled": "2022-02-03T02:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Bias Mitigation Post-processing for Individual and Group</b> <b>Fairness</b> ...", "url": "https://www.researchgate.net/publication/332790741_Bias_Mitigation_Post-processing_for_Individual_and_Group_Fairness", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/332790741_<b>Bias_Mitigation_Post-processing_for</b>...", "snippet": "Researchers have investigated both notions of <b>fairness</b>, (i) <b>individual</b> <b>fairness</b> that focuses on that similar people should be treated similarly [3,12], and (ii) group <b>fairness</b> which focuses on ...", "dateLastCrawled": "2022-01-27T12:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Algorithmic Fairness: Choices, Assumptions, and Definitions</b> | Annual ...", "url": "https://www.annualreviews.org/doi/full/10.1146/annurev-statistics-042720-125902", "isFamilyFriendly": true, "displayUrl": "https://www.annualreviews.org/doi/full/10.1146/annurev-statistics-042720-125902", "snippet": "The <b>individual</b>&#39;s final score or <b>prediction</b> is then the sum of those integer-valued points. These and other simple models <b>can</b> help humans understand the basis upon which a model&#39;s predictions are being made and facilitate normative debate about the legitimacy of that basis, e.g., revealing how a risk assessment model makes use of potentially contentious covariates like a defendant&#39;s financial, employment, or family situation. These techniques <b>can</b> also clarify how perturbations to the inputs ...", "dateLastCrawled": "2022-02-02T18:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Chapter 11 Bias and Fairness</b> | Big Data and Social Science", "url": "https://textbook.coleridgeinitiative.org/chap-bias.html", "isFamilyFriendly": true, "displayUrl": "https://textbook.coleridgeinitiative.org/chap-bias.html", "snippet": "The public debate around COMPAS also inspired a number of academic researchers to look closer at these definitions of <b>fairness</b>, showing that in the presence of different base rates, it would be impossible for a model to satisfy both definitions of <b>fairness</b> at <b>the same</b> time. The case of COMPAS demonstrates the potentially dramatic impact of decisions about how equity is defined and measured in real applications with considerable implications for <b>individual</b>\u2019s lives. It\u2019s incumbent upon the ...", "dateLastCrawled": "2022-01-30T01:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Human Judgment in algorithmic loops: <b>Individual</b> justice and automated ...", "url": "https://onlinelibrary.wiley.com/doi/10.1111/rego.12358", "isFamilyFriendly": true, "displayUrl": "https://onlinelibrary.wiley.com/doi/10.1111/rego.12358", "snippet": "<b>Individual</b> <b>fairness</b> measures involve pre-specifying the subset of features that define <b>individual</b> similarity in the context of the task. By contrast, the kind of <b>individual</b> justice addressed above starts from the assumption that even if the facts at hand suggest two cases are exactly alike, a fresh consideration is necessary because there&#39;s a chance that different features, or alternative reasoning, might lead to a different decision.", "dateLastCrawled": "2022-01-29T07:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Algorithms and Society Notes: <b>Fairness</b> - University at Buffalo", "url": "http://www-student.cse.buffalo.edu/~atri/algo-and-society/support/notes/fairness/index.html", "isFamilyFriendly": true, "displayUrl": "www-student.cse.buffalo.edu/~atri/algo-and-society/support/notes/<b>fairness</b>/index.html", "snippet": "The notion of <b>individual</b> <b>fairness</b> get over the shortcomings of group <b>fairness</b> that we talked about-- see Dwork et al. for more details. Let us recap where we are. First, group <b>fairness</b> is easy to define (and is actually used in the US legal system)-- however, it has some &quot;blind spots&quot; where it deems a scenario to be fair, which reasonable people will contend are not fair.", "dateLastCrawled": "2021-11-20T21:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Fairness-aware Machine Learning: Practical Challenges and</b> Lessons ...", "url": "https://www.slideshare.net/KrishnaramKenthapadi/fairnessaware-machine-learning-practical-challenges-and-lessons-learned-wsdm-2019-tutorial", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/KrishnaramKenthapadi/<b>fairness</b>aware-machine-learning...", "snippet": "<b>Fairness</b>-Aware Data Collection Techniques 1. Address population biases \u2022 Target under-represented (with respect to the user population) groups 2. Address representation issues \u2022 Oversample from minority groups \u2022 Sufficient data from each group may be required to avoid model <b>treating</b> them as &quot;outliers&quot; 3.", "dateLastCrawled": "2021-12-25T22:11:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "On the Apparent Conflict Between <b>Individual</b> and Group <b>Fairness</b> | DeepAI", "url": "https://deepai.org/publication/on-the-apparent-conflict-between-individual-and-group-fairness", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/.../on-the-apparent-conflict-between-<b>individual</b>-and-group-<b>fairness</b>", "snippet": "Although they do not address the relationship between different worldviews and the <b>individual</b> / group <b>fairness</b> distinction, focusing on assumed worldviews in this way <b>can</b> help illustrate why apparent conflicts between <b>individual</b> and group <b>fairness</b> are misguided. The important difference is between worldviews, not whether we render our assumptions about them at the <b>individual</b> or group level. The reason for <b>treating</b> people differently on the basis of group membership is the assumption that it ...", "dateLastCrawled": "2022-01-15T05:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Interventions | <b>Machine Learning Bias Mitigation</b>", "url": "https://cdeiuk.github.io/bias-mitigation/interventions/", "isFamilyFriendly": true, "displayUrl": "https://cdeiuk.github.io/bias-mitigation/interventions", "snippet": "The first is that it achieves <b>fairness</b> through some randomisation of decision thresholds, which means that the post-processed classifier <b>can</b> fail <b>individual</b> <b>fairness</b>. In fact two identical individuals could receive different outcomes. The second is that it fully mitigates bias, which <b>can</b> have a negative performance implications. It is not possible to balance <b>fairness</b> and accuracy requirements by reducing the bias partially but not fully.", "dateLastCrawled": "2022-02-02T02:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "The (Im)possibility of <b>Fairness: Different Value Systems Require</b> ...", "url": "https://cacm.acm.org/magazines/2021/4/251365-the-impossibility-of-fairness/fulltext", "isFamilyFriendly": true, "displayUrl": "https://cacm.acm.org/magazines/2021/4/251365-the-impossibility-of-<b>fairness</b>", "snippet": "Unfortunately, WYSIWYG appears to be crucial to ensuring <b>individual</b> <b>fairness</b>: if there is structural bias in the decision pipeline, no mechanism <b>can</b> guarantee <b>individual</b> <b>fairness</b>. <b>Fairness</b> <b>can</b> only be achieved under the WYSIWYG worldview using an <b>individual</b> <b>fairness</b> mechanism and using a group <b>fairness</b> mechanism will be unfair within this worldview.", "dateLastCrawled": "2022-02-03T00:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Human-centric Approach to <b>Fairness</b> in AI", "url": "https://www.timlrx.com/blog/a-human-centric-approach-to-fairness-in-ai", "isFamilyFriendly": true, "displayUrl": "https://www.timlrx.com/blog/a-human-centric-approach-to-<b>fairness</b>-in-ai", "snippet": "A business problem <b>can</b> then be translated to a mathematical / statistical / computational problem and different methods <b>can</b> <b>be compared</b> against each other based on how far the predicted output differs from the actual output as measured by the objective function. If <b>fairness</b> could be distilled down to a single metric, a data scientist <b>can</b> include it as part of the objective function or as a constraint and find an ideal point that maximises overall business needs while satisfying <b>fairness</b> ...", "dateLastCrawled": "2022-02-03T02:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Chapter 11 Bias and Fairness</b> | Big Data and Social Science", "url": "https://textbook.coleridgeinitiative.org/chap-bias.html", "isFamilyFriendly": true, "displayUrl": "https://textbook.coleridgeinitiative.org/chap-bias.html", "snippet": "The public debate around COMPAS also inspired a number of academic researchers to look closer at these definitions of <b>fairness</b>, showing that in the presence of different base rates, it would be impossible for a model to satisfy both definitions of <b>fairness</b> at <b>the same</b> time. The case of COMPAS demonstrates the potentially dramatic impact of decisions about how equity is defined and measured in real applications with considerable implications for <b>individual</b>\u2019s lives. It\u2019s incumbent upon the ...", "dateLastCrawled": "2022-01-30T01:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Algorithmic <b>Fairness</b>: Tackling Bias in City Algorithms | Data-Smart ...", "url": "https://datasmart.ash.harvard.edu/news/article/algorithmic-fairness-tackling-bias-city-algorithms", "isFamilyFriendly": true, "displayUrl": "https://datasmart.ash.harvard.edu/news/article/algorithmic-<b>fairness</b>-tackling-bias-city...", "snippet": "The foundation for ensuring <b>fairness</b> in the algorithms created by an organization is a set of structural conditions that promote a culture committed to reducing bias. Without this, the policy and technical strategies that follow are irrelevant, because they will never become a priority for analysts. Create diverse teams. Building a culture of <b>fairness</b> starts with hiring a diverse group of engineers. Black, Latinx, and Native American people are underrepresented in tech by 16 to 18 percentage ...", "dateLastCrawled": "2022-02-03T10:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Counterfactual Fairness</b> | DeepAI", "url": "https://deepai.org/publication/counterfactual-fairness", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>counterfactual-fairness</b>", "snippet": "Our definition of <b>counterfactual fairness</b> captures the intuition that a decision is fair towards an <b>individual</b> if it <b>the same</b> in (a) the actual world and (b) a counterfactual world where the <b>individual</b> belonged to a different demographic group. We demonstrate our framework on a real-world problem of fair <b>prediction</b> of success in law school. READ FULL TEXT VIEW PDF. POST COMMENT Comments. There are no comments yet. POST REPLY \u00d7. Authors. Matt J. Kusner 25 publications . Joshua R. Loftus 5 ...", "dateLastCrawled": "2022-01-26T15:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Mirror Mirror", "url": "https://shiraamitchell.github.io/fairness/", "isFamilyFriendly": true, "displayUrl": "https://shiraamitchell.github.io/<b>fairness</b>", "snippet": "Conversations <b>can</b> easily fall into the mire of assuming or imposing one or another condition as though it were an obviously correct formalization of <b>fairness</b>, <b>treating</b> violations of that principle as moral or conceptual gotchas. The frustrating and repetitive stalemate of these conversations likely reflects both confusion and fundamentally different ideas of <b>fairness</b>.", "dateLastCrawled": "2022-01-06T19:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Fourth Amendment <b>Fairness</b> - University of Michigan", "url": "https://repository.law.umich.edu/cgi/viewcontent.cgi?article=1778&context=mlr", "isFamilyFriendly": true, "displayUrl": "https://repository.law.umich.edu/cgi/viewcontent.cgi?article=1778&amp;context=mlr", "snippet": "scales, and <b>compared</b>, with the goal of promoting overall social welfare. But interest aggregation is disconnected from many settled doctrinal rules and leads to results that are unfair for individuals. The main alternative is originalism; but historical sources themselves suggest that the Fourth Amend-ment calls for new moral reasoning. This Article argues that the Fourth Amendment\u2019s prohibition on \u201cunreasona-ble searches and seizures\u201d is best understood, at least in large part, as a ...", "dateLastCrawled": "2021-12-29T17:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Building <b>prediction</b> models with grouped data: A case study on the ...", "url": "https://onlinelibrary.wiley.com/doi/full/10.1111/1748-8583.12396", "isFamilyFriendly": true, "displayUrl": "https://onlinelibrary.wiley.com/doi/full/10.1111/1748-8583.12396", "snippet": "The <b>prediction</b> question <b>can</b> be expressed in Y = f (X h, X l) + U g, where U g is the group-specific intercept that pertains to Group g. <b>Compared</b> with the SL approach, the addition of RE largely complicates model estimation, as RE have to be estimated with an iterative procedure. To our knowledge, very few RE <b>prediction</b> models have been proposed.", "dateLastCrawled": "2021-08-21T17:57:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Towards Analogy-Based Explanations in Machine Learning</b> | DeepAI", "url": "https://deepai.org/publication/towards-analogy-based-explanations-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>towards-analogy-based-explanations-in-machine-learning</b>", "snippet": "More specifically, we take the view that an <b>analogy</b>-based approach is a viable alternative to existing approaches in the realm of explainable AI and interpretable <b>machine</b> <b>learning</b>, and that <b>analogy</b>-based explanations of the predictions produced by a <b>machine</b> <b>learning</b> algorithm can complement similarity-based explanations in a meaningful way. To corroborate these claims, we outline the basic idea of an <b>analogy</b>-based explanation and illustrate its potential usefulness by means of some examples.", "dateLastCrawled": "2022-01-10T12:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Practical <b>Individual</b> <b>Fairness</b> Algorithms \u2013 Toronto <b>Machine</b> <b>Learning</b>", "url": "https://www.torontomachinelearning.com/events/practical-individual-fairness-algorithms/", "isFamilyFriendly": true, "displayUrl": "https://www.toronto<b>machinelearning</b>.com/events/practical-<b>individual</b>-<b>fairness</b>-algorithms", "snippet": "<b>Individual</b> <b>Fairness</b> (IF) is a very intuitive and desirable notion of <b>fairness</b>: we want ML models to treat similar individuals similarly, that is, to be fair for every person. For example, two resumes of individuals that only differ in their name and gender pronouns should be treated similarly by the model. Despite the intuition, training ML/AI models that abide by this rule in theory and in practice poses several challenges. In this talk, I will introduce a notion of Distributional ...", "dateLastCrawled": "2021-12-31T05:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "RStudio AI Blog: Starting to think about AI <b>Fairness</b>", "url": "https://blogs.rstudio.com/ai/posts/2021-07-15-ai-fairness/", "isFamilyFriendly": true, "displayUrl": "https://blogs.rstudio.com/ai/posts/2021-07-15-ai-<b>fairness</b>", "snippet": "Papers on <b>fairness</b> in <b>machine</b> <b>learning</b>, as is common in fields like computer science, abound with formulae. Even the papers referenced here, though selected not for their theorems and proofs but for the ideas they harbor, are no exception. But to start thinking about <b>fairness</b> as it might apply to an ML process at hand, common language \u2013 and common sense \u2013 will do just fine. If, after analyzing your use case, you judge that the more technical results are relevant to the process in ...", "dateLastCrawled": "2022-01-31T09:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Finding the <b>fairness</b> in ethical <b>machine</b> <b>learning</b> - Taylor Fry", "url": "https://taylorfry.nz/articles/finding-the-fairness-in-ethical-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://taylorfry.nz/articles/finding-the-<b>fairness</b>-in-ethical-<b>machine</b>-<b>learning</b>", "snippet": "Likewise, <b>machine</b> <b>learning</b> infrastructure is also missing, specifically, regulating the use of <b>machine</b> <b>learning</b> to ensure it\u2019s used in an ethical and beneficial way, rather than used by a small number for their advantage at the significant disadvantage of the majority. It\u2019s important we all continue to have this conversation together, and take action at <b>individual</b>, organisational, governmental and global levels to bring about a future where AI is used to help not hinder.", "dateLastCrawled": "2022-01-07T11:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Residual Unfairness in Fair <b>Machine</b> <b>Learning</b> from Prejudiced Data", "url": "http://proceedings.mlr.press/v80/kallus18a/kallus18a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v80/kallus18a/kallus18a.pdf", "snippet": "Recent work on <b>fairness</b> in <b>machine</b> <b>learning</b> proposes and analyzes competing criteria for assessing the <b>fairness</b> of <b>ma-chine</b> <b>learning</b> algorithms, where some adjustments attempt to equalize accuracy metrics across groups (Corbett-Davies etal.,2017;Kleinbergetal.,2017;Hardtetal.,2016). Other work studies how historical prejudices may be re\ufb02ected in training data such that algorithmic systems might replicate historical biases (Angwin et al., 2016; Lum &amp; Isaac, 2016; Kilbertus et al., 2017). We ...", "dateLastCrawled": "2022-01-31T07:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Machine</b> <b>Learning</b> and Theological Traditions of <b>Analogy</b> - Davison - 2021 ...", "url": "https://onlinelibrary.wiley.com/doi/full/10.1111/moth.12682", "isFamilyFriendly": true, "displayUrl": "https://onlinelibrary.wiley.com/doi/full/10.1111/moth.12682", "snippet": "12 <b>Machine</b> <b>Learning</b>, <b>Analogy</b>, and God. The texts considered in this article come from theological sources. They have offered ways to think analogically about features of the world, in this case the similarities we are beginning to see between capacities in <b>machine</b> <b>learning</b> and those in human beings and other animals. Much of the mediaeval ...", "dateLastCrawled": "2021-04-16T10:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "How to <b>Create Unbiased ML Models | Deepchecks</b>", "url": "https://deepchecks.com/how-to-create-unbiased-ml-models/", "isFamilyFriendly": true, "displayUrl": "https://deepchecks.com/how-to-create-unbiased-ml-models", "snippet": "\u201cIn <b>machine</b> <b>learning</b>, a given algorithm is said to be fair, or to have <b>fairness</b>, if its results are independent of given variables, especially those considered sensitive, such as the traits of individuals which should not correlate with the outcome (i.e. gender, ethnicity, sexual orientation, disability, etc.).\u201d \u201c<b>Fairness</b>\u201d, Wikipedia", "dateLastCrawled": "2022-01-13T06:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Fairness</b> Through Awareness", "url": "https://www.cs.toronto.edu/~zemel/documents/fairAwareItcs2012.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.toronto.edu/~zemel/documents/fairAwareItcs2012.pdf", "snippet": "this <b>individual</b>-based <b>fairness</b>, we assume a distance metric that de\ufb01nes the similarity between the individuals. This is the source of \u201cawareness\u201d in the title of this paper. We formalize this guiding principle as a Lipschitz condition on the classi\ufb01er. In our approach a classi\ufb01er is a randomized mapping from individuals to outcomes, or equivalently, a mapping from individuals to distributions over outcomes. The Lipschitz condition requires that any two individuals x;ythat are at ...", "dateLastCrawled": "2022-01-29T03:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "An evaluation of scanpath-comparison and <b>machine</b>-<b>learning</b> ...", "url": "https://link.springer.com/article/10.3758/s13428-016-0788-z", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.3758/s13428-016-0788-z", "snippet": "The bottom line is that scanpath-comparison algorithms and the <b>machine</b>-<b>learning</b> techniques that accompany them are powerful tools to study the dynamics of <b>analogy</b> making. In building models of <b>analogy</b> making, we want to know what the models predict and how they make those predictions. Although the tools presented in this article are more involved with prediction than with explanation, the two are hardly unrelated, especially when we know the bases of the predictions. Our overarching goal has ...", "dateLastCrawled": "2021-11-05T03:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "The UX of AI Part I: Ants in Your Pants or Ants in Your Brain? | by zac ...", "url": "https://medium.com/@ZacTaschdjian/the-ux-of-ai-part-i-ants-in-your-pants-or-ants-in-your-brain-3cfef7990e7a", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@ZacTaschdjian/the-ux-of-ai-part-i-ants-in-your-pants-or-ants-in...", "snippet": "The UX of AI Part II: Inscrutability in Deep Reinforcement <b>Learning</b> Neural Networks. Paper 1 is \u201cTransparency and Explanation in Deep Reinforcement <b>Learning</b> Neural Networks\u201d(Iyar, et. al. 2018 ...", "dateLastCrawled": "2022-01-22T22:46:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Making Fair ML Software using Trustworthy Explanation", "url": "https://www.researchgate.net/profile/Kewen-Peng-4/publication/342733939_Making_Fair_ML_Software_using_Trustworthy_Explanation/links/5fe0ddcea6fdccdcb8ef5a11/Making-Fair-ML-Software-using-Trustworthy-Explanation.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/profile/Kewen-Peng-4/publication/342733939_Making_Fair_ML...", "snippet": "<b>Machine</b> <b>learning</b> software is being used in many applications (fi-nance, hiring, admissions, criminal justice) having huge social im-pact. But sometimes the behavior of this software is biased and ...", "dateLastCrawled": "2021-09-29T17:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Making Fair <b>ML Software using Trustworthy Explanation</b> | DeepAI", "url": "https://deepai.org/publication/making-fair-ml-software-using-trustworthy-explanation", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/making-fair-<b>ml-software-using-trustworthy-explanation</b>", "snippet": "<b>Machine</b> <b>learning</b> software is being used in many applications (finance, hiring, admissions, criminal justice) having a huge social impact. But sometimes the behavior of this software is biased and it shows discrimination based on some sensitive attributes such as sex, race, etc. Prior works concentrated on finding and mitigating bias in ML models.", "dateLastCrawled": "2022-01-24T00:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) Making fair ML software using trustworthy explanation", "url": "https://www.researchgate.net/publication/348827111_Making_fair_ML_software_using_trustworthy_explanation", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/348827111_Making_fair_ML_software_using...", "snippet": "PDF | On Dec 21, 2020, Joymallya Chakraborty and others published Making fair ML software using trustworthy explanation | Find, read and cite all the research you need on ResearchGate", "dateLastCrawled": "2021-11-13T01:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Service-Oriented Computing: 18th International Conference, ICSOC 2020 ...", "url": "https://dokumen.pub/service-oriented-computing-18th-international-conference-icsoc-2020-dubai-united-arab-emirates-december-1417-2020-proceedings-1st-ed-9783030653095-9783030653101.html", "isFamilyFriendly": true, "displayUrl": "https://<b>dokumen.pub</b>/service-oriented-computing-18th-international-conference-icsoc...", "snippet": "In a Fog environment, a Kubernetes Node is a worker <b>machine</b>, and it may be a virtual <b>machine</b> or a physical <b>machine</b> that corresponds to a node, a.k.a., Fog node. A set of Kubernetes Nodes makes up a Kubernetes cluster. A Kubernetes cluster corresponds to a set of fog nodes. Each microservice can be containerized and, therefore, it belongs to a single Docker container. A Kubernetes Pod is a group of containers with shared network and storage, that are always coscheduled and co-located. Finally ...", "dateLastCrawled": "2021-12-24T17:49:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Counterfactual Fairness: Unidentification, Bound and Algorithm ...", "url": "https://www.researchgate.net/publication/334843895_Counterfactual_Fairness_Unidentification_Bound_and_Algorithm", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/334843895_Counterfactual_Fairness_Un...", "snippet": "Fairness in <b>machine</b> <b>learning</b> has been a research subject with rapid growth recently. Many different definitions of fairness have been designed to fit different settings, e.g., equality of ...", "dateLastCrawled": "2021-12-23T12:05:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Putting <b>Fairness Principles into Practice: Challenges, Metrics</b>, and ...", "url": "https://deepai.org/publication/putting-fairness-principles-into-practice-challenges-metrics-and-improvements", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/putting-<b>fairness-principles-into-practice-challenges</b>...", "snippet": "By almost every measure, there has been an explosion in attention and research on <b>machine</b> <b>learning</b> fairness: there is a quickly growing amount of research on how to define, measure, and address <b>machine</b> <b>learning</b> fairness, and products are evaluated with these concerns in mind. Despite this significant attention, there has been much less published work detailing how fairness concerns are measured and addressed by product teams in industry. In this paper, we hope to shed light on the challenges ...", "dateLastCrawled": "2022-01-23T04:12:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(individual fairness)  is like +(treating every prediction the same)", "+(individual fairness) is similar to +(treating every prediction the same)", "+(individual fairness) can be thought of as +(treating every prediction the same)", "+(individual fairness) can be compared to +(treating every prediction the same)", "machine learning +(individual fairness AND analogy)", "machine learning +(\"individual fairness is like\")", "machine learning +(\"individual fairness is similar\")", "machine learning +(\"just as individual fairness\")", "machine learning +(\"individual fairness can be thought of as\")", "machine learning +(\"individual fairness can be compared to\")"]}