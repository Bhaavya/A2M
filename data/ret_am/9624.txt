{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Why is Convex Optimization such a big</b> deal in Machine <b>Learning</b>? - Quora", "url": "https://www.quora.com/Why-is-Convex-Optimization-such-a-big-deal-in-Machine-Learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>Why-is-Convex-Optimization-such-a-big</b>-deal-in-Machine-<b>Learning</b>", "snippet": "Answer (1 of 10): <b>Convex</b> <b>optimization</b> is the core of most machine <b>learning</b> methods Some key concepts here: - <b>Convex</b> functions are those for which it&#39;s possible to draw a line segment from any two points on the graph and this line will always be inside the graph (except at the endpoints). This m...", "dateLastCrawled": "2022-01-17T17:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Theory <b>of Convex Optimization for Machine Learning</b>", "url": "https://www.researchgate.net/publication/262489426_Theory_of_Convex_Optimization_for_Machine_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/262489426_Theory_of_<b>Convex</b>_<b>Optimization</b>_for...", "snippet": "<b>Convex</b> <b>optimization</b> is used to define at each time the contribution of each fuel cell to the global output so as to satisfy a power demand as long as possible. An algorithm based on the Mirror ...", "dateLastCrawled": "2021-11-07T19:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Convex Optimization</b> - slideshare.net", "url": "https://www.slideshare.net/madilraja/convex-optimization", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/madilraja/<b>convex-optimization</b>", "snippet": "<b>Convex Optimization</b> Muhammad Adil Raja Introduction <b>Convex</b> Sets <b>Convex</b> Functions <b>Convex Optimization</b> Problems References INTRODUCTION I I Many situations arise in machine <b>learning</b> where we would <b>like</b> to optimize the value of some function. I That is, given a function f : Rn ! R, we want to find x 2 Rn that minimizes (or maximizes) f (x). I In ...", "dateLastCrawled": "2022-02-02T17:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Contextual Learning with Online Convex Optimization</b>: Theory and ...", "url": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3501316", "isFamilyFriendly": true, "displayUrl": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3501316", "snippet": "We develop a joint contextual <b>learning</b> and <b>optimization</b> algorithm for this model, which sequentially selects for a patient (i) the best treatment based on their contextual information and (ii) the corresponding dosage that is optimized over the prior history of all patients seen who received the same treatment. This algorithm synergizes the strength of the contextual bandit with online <b>convex</b> <b>optimization</b> techniques in a seamless fashion. We prove that it admits a sub-linear regret, which is ...", "dateLastCrawled": "2021-12-11T08:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Regularization : What? Why? and How? (Part -1) | by Siddhant Rai ...", "url": "https://medium.com/mlearning-ai/regularization-what-why-and-how-part-1-ef6bdb6bafea", "isFamilyFriendly": true, "displayUrl": "https://medium.com/m<b>learning</b>-ai/regularization-what-why-and-how-part-1-ef6bdb6bafea", "snippet": "<b>Convex</b>/non-<b>convex</b> <b>optimization</b> and linear/non-linear <b>optimization</b>. For linear and nonlinear optimizations, the name is quite evident of our task at hand. It is literally solving a system of ...", "dateLastCrawled": "2022-01-28T00:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Mathematical <b>Optimization</b> and Machine <b>Learning</b> | by Opex Analytics ...", "url": "https://medium.com/opex-analytics/mathematical-optimization-and-machine-learning-970cb6cdb697", "isFamilyFriendly": true, "displayUrl": "https://medium.com/opex-analytics/mathematical-<b>optimization</b>-and-machine-<b>learning</b>-970cb...", "snippet": "Well again, this is a <b>convex</b> unconstrained <b>optimization</b> model. (Note that the objective function is a concave maximization objective, which is considered a <b>convex</b> <b>optimization</b> model.) Basically ...", "dateLastCrawled": "2022-01-11T01:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Convex</b> <b>Optimization</b> for Machine <b>Learning</b> : learnmachinelearning", "url": "https://www.reddit.com/r/learnmachinelearning/comments/oisty1/convex_optimization_for_machine_learning/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/learnmachine<b>learning</b>/comments/oisty1/<b>convex</b>_<b>optimization</b>_for...", "snippet": "1. level 1. <b>optimization</b>_ml. \u00b7 1m. 600 page book to learn <b>convex</b> <b>optimization</b> is too much for your situation. It have lots of details which I think you won\u2019t need. I would suggest to look for 2-3 lectures of \u2018Stephen Wright, UW Madison\u2019 about <b>optimization</b> for machine <b>learning</b>. The slides can be found online in his website.", "dateLastCrawled": "2021-08-14T18:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Optimization</b> for Deep <b>Learning</b>: An Overview | SpringerLink", "url": "https://link.springer.com/article/10.1007/s40305-020-00309-6", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s40305-020-00309-6", "snippet": "<b>Optimization</b> is a critical component in deep <b>learning</b>. We think <b>optimization</b> for neural networks is an interesting topic for theoretical research due to various reasons. First, its tractability despite non-convexity is an intriguing question and may greatly expand our understanding of tractable problems. Second, classical <b>optimization</b> theory is far from enough to explain many phenomena. Therefore, we would <b>like</b> to understand the challenges and opportunities from a theoretical perspective and ...", "dateLastCrawled": "2022-01-22T19:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Convex</b> <b>Optimization</b> (Course) : learnmachinelearning", "url": "https://www.reddit.com/r/learnmachinelearning/comments/5gz6cq/convex_optimization_course/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/learnmachine<b>learning</b>/comments/5gz6cq/<b>convex</b>_<b>optimization</b>_course", "snippet": "A subreddit dedicated to <b>learning</b> machine <b>learning</b>. Press J to jump to the feed. Press question mark to learn the rest of the keyboard shortcuts . Search within r/learnmachinelearning. r/learnmachinelearning. Log In Sign Up. User account menu. 8. <b>Convex</b> <b>Optimization</b> (Course) Close. 8. Posted by 5 years ago. Archived. <b>Convex</b> <b>Optimization</b> (Course) stat.cmu.edu/~ryant... 0 comments. share. save. hide. report. 91% Upvoted. This thread is archived. New comments cannot be posted and votes cannot ...", "dateLastCrawled": "2021-08-14T18:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Notes from MIT Deep <b>Learning</b> Theory and Non-<b>Convex</b> <b>Optimization</b> ...", "url": "https://kiranvodrahalli.github.io/notes/liveTeX/MITdeep2019/", "isFamilyFriendly": true, "displayUrl": "https://kiranvodrahalli.github.io/notes/liveTeX/MITdeep2019", "snippet": "In machine <b>learning</b> it seems <b>like</b> the path is important, and it\u2019s all mixed in with generalization and everything. It\u2019s not a standalone <b>optimization</b> problem. Most of the practical advances in recent years in deep <b>learning</b> have happened due to clever new architectures and designs and concepts, rather than new preconditioner.", "dateLastCrawled": "2022-01-20T20:57:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A <b>convex</b> <b>optimization</b> approach for identification of <b>human</b> tissue ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4908329/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC4908329", "snippet": "<b>Similar</b> analysis of other <b>human</b> tissues can potentially contribute to identification of new therapeutic targets for other <b>human</b> disorders. 4 Conclusion. In this article, we present a novel method for computing tissue-specific interactomes from organism-specific interactomes and expression profiles of genes in various tissues. Our method casts the problem as a <b>convex</b> <b>optimization</b> problem that diffuses functional activity of genes over the organism-specific interactome, while simultaneously ...", "dateLastCrawled": "2017-01-11T22:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Learning</b> for <b>Convex</b> <b>Optimization</b>", "url": "https://www.researchgate.net/publication/323444129_Learning_for_Convex_Optimization", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/323444129_<b>Learning</b>_for_<b>Convex</b>_<b>Optimization</b>", "snippet": "PDF | In many engineered systems, <b>optimization</b> is used for decision making at time-scales ranging from real-time operation to long-term planning. This... | Find, read and cite all the research you ...", "dateLastCrawled": "2021-10-25T22:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Theory <b>of Convex Optimization for Machine Learning</b>", "url": "https://www.researchgate.net/publication/262489426_Theory_of_Convex_Optimization_for_Machine_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/262489426_Theory_of_<b>Convex</b>_<b>Optimization</b>_for...", "snippet": "<b>Convex</b> <b>optimization</b> is used to define at each time the contribution of each fuel cell to the global output so as to satisfy a power demand as long as possible. An algorithm based on the Mirror ...", "dateLastCrawled": "2021-11-07T19:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Why is Convex Optimization such a big</b> deal in Machine <b>Learning</b>? - Quora", "url": "https://www.quora.com/Why-is-Convex-Optimization-such-a-big-deal-in-Machine-Learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>Why-is-Convex-Optimization-such-a-big</b>-deal-in-Machine-<b>Learning</b>", "snippet": "Answer (1 of 10): <b>Convex</b> <b>optimization</b> is the core of most machine <b>learning</b> methods Some key concepts here: - <b>Convex</b> functions are those for which it&#39;s possible to draw a line segment from any two points on the graph and this line will always be inside the graph (except at the endpoints). This m...", "dateLastCrawled": "2022-01-17T17:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Learning</b> to Optimize with Reinforcement <b>Learning</b> \u2013 The Berkeley ...", "url": "https://bair.berkeley.edu/blog/2017/09/12/learning-to-optimize-with-rl/", "isFamilyFriendly": true, "displayUrl": "https://bair.berkeley.edu/blog/2017/09/12/<b>learning</b>-to-optimize-with-rl", "snippet": "There are two reasons: first, many <b>optimization</b> algorithms are devised under the assumption of convexity and applied to non-<b>convex</b> objective functions; by <b>learning</b> the <b>optimization</b> algorithm under the same setting as it will actually be used in practice, the learned <b>optimization</b> algorithm could hopefully achieve better performance. Second, devising new <b>optimization</b> algorithms manually is usually laborious and can take months or years; <b>learning</b> the <b>optimization</b> algorithm could reduce the ...", "dateLastCrawled": "2022-02-03T06:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "What jobs are related <b>to convex optimization, discrete optimization</b> ...", "url": "https://www.quora.com/What-jobs-are-related-to-convex-optimization-discrete-optimization-and-algorithms-What-additional-skills-do-I-need-for-these-jobs", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-jobs-are-related-to-<b>convex</b>-<b>optimization</b>-discrete...", "snippet": "Answer (1 of 4): This is an interesting question. I started my graduate study in operations research and slowly started taking <b>optimization</b> (or <b>convex</b> <b>optimization</b>) courses. I took about 10 courses just in <b>optimization</b> because well it was very interesting. Only later I slowly found out that optim...", "dateLastCrawled": "2022-01-22T11:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "2 <b>Convex</b> Variational Formulations for <b>Learning</b> ... - <b>Optimization</b> Online", "url": "http://www.optimization-online.org/DB_FILE/2016/08/5586.pdf", "isFamilyFriendly": true, "displayUrl": "www.<b>optimization</b>-online.org/DB_FILE/2016/08/5586.pdf", "snippet": "<b>Convex</b> Variational Formulations for <b>Learning</b> Problems Pedro Borges Abstract\u2014In this article, we introduce new techniques to solve the nonlinear regression problem and the nonlinear classi\ufb01cation problem. Our benchmarks suggest that our method for regression is signi\ufb01cantly more effective when compared to classical methods and our method for classi\ufb01cation is competitive. Our list of classical methods includes least squares, random forests, decision trees, boosted trees, nearest ...", "dateLastCrawled": "2022-01-31T21:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "6 <b>Learning</b> and VC-dimension", "url": "https://www.cs.cornell.edu/courses/cs4850/2010sp/Course%20Notes/Chap%206%20Learning-march_9_2010.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.cornell.edu/courses/cs4850/2010sp/Course Notes/Chap 6 <b>Learning</b>-march_9...", "snippet": "One first has a <b>human</b> judge or ``teacher\u2019\u2019 label many examples as either ``car\u2019\u2019 or ``not car\u2019\u2019. The teacher provides no other information. The labeled examples are fed to a <b>learning</b> algorithm or ``learner\u2019\u2019 whose task is to output a ``classifier\u2019\u2019 consistent with the labeled examples. Note that the learner is not responsible for classifying all examples correctly, only for the given examples, called the ``training set\u2019\u2019, since that is all it is given. Intuitively, if ...", "dateLastCrawled": "2022-02-02T17:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Notes from MIT Deep <b>Learning</b> Theory and Non-<b>Convex</b> <b>Optimization</b> ...", "url": "https://kiranvodrahalli.github.io/notes/liveTeX/MITdeep2019/", "isFamilyFriendly": true, "displayUrl": "https://kiranvodrahalli.github.io/notes/liveTeX/MITdeep2019", "snippet": "Instead of finding classifiers by <b>convex</b> <b>optimization</b>, you could do something <b>similar</b>: Just use the classifiers derived by computing the mean of samples for each class. We will use those. The first result is that if the unsupervised loss is low, then average loss on classification tasks is low. This is already useful \u2013 you can just try to minimize unsupervised loss. You can prove this; the key step is just Jensen\u2019s inequality applied to contrastive loss. This is already good in many ...", "dateLastCrawled": "2022-01-20T20:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>optimization</b> - Neural Networks: what&#39;s the point of <b>learning</b> features ...", "url": "https://cstheory.stackexchange.com/questions/20917/neural-networks-whats-the-point-of-learning-features-that-dont-linearly-separ", "isFamilyFriendly": true, "displayUrl": "https://cstheory.stackexchange.com/questions/20917/neural-networks-whats-the-point-of...", "snippet": "Unless I&#39;m mistaken, deep neural networks are good for <b>learning</b> functions that are nonlinear in the input. In such cases, the input set is linearly inseparable, so the optimisation problem that results from the approximation problem is not <b>convex</b>, so it cannot be globally optimised with local <b>optimization</b>. Support Vector Machines (try to) get ...", "dateLastCrawled": "2022-01-13T21:40:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "1 Automatic code generation for real-time <b>convex</b> <b>optimization</b>", "url": "https://assets.cambridge.org/97805217/62229/excerpt/9780521762229_excerpt.pdf", "isFamilyFriendly": true, "displayUrl": "https://assets.cambridge.org/97805217/62229/excerpt/9780521762229_excerpt.pdf", "snippet": "Mathematical <b>optimization</b> is traditionally <b>thought</b> of as an aid to <b>human</b> decision mak-ing. For example, a ... control, \ufb01nance, statistics and machine <b>learning</b>, and network operation <b>can</b> be cast (exactly, or with reasonable approximations) as <b>convex</b> problems. In many other prob-lems, <b>convex</b> <b>optimization</b> <b>can</b> provide a good heuristic for approximate solution of the problem [13,14]. In any case, much of what we say in this chapter carries over to local <b>optimization</b> methodsfornonconvexproblems ...", "dateLastCrawled": "2022-01-06T18:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "1 Automatic Code Generation for Real-Time <b>Convex</b> <b>Optimization</b>", "url": "https://stanford.edu/~boyd/papers/pdf/rt_cvx_opt.pdf", "isFamilyFriendly": true, "displayUrl": "https://stanford.edu/~boyd/papers/pdf/rt_cvx_opt.pdf", "snippet": "Mathematical <b>optimization</b> is traditionally <b>thought</b> of as an aid to <b>human</b> deci-sion making. For example, a tool for portfolio <b>optimization</b> suggests a portfolio to a <b>human</b> decision maker, who possibly carries out the proposed trades. <b>Opti-mization</b> is also used in many aspects of engineering design; in most cases, an engineer is in the decision loop, continually reviewing the proposed designs and changing parameters in the problem speci\ufb01cation if needed. When <b>optimization</b> is used in an ...", "dateLastCrawled": "2022-01-30T08:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Meta-<b>Learning</b> With Differentiable <b>Convex</b> <b>Optimization</b>", "url": "https://openaccess.thecvf.com/content_CVPR_2019/papers/Lee_Meta-Learning_With_Differentiable_Convex_Optimization_CVPR_2019_paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://openaccess.thecvf.com/content_CVPR_2019/papers/Lee_Meta-<b>Learning</b>_With...", "snippet": "Meta-<b>Learning</b> with Differentiable <b>Convex</b> <b>Optimization</b> Kwonjoon Lee2 Subhransu Maji1,3 Avinash Ravichandran1 Stefano Soatto1,4 1Amazon Web Services 2UC San Diego 3UMass Amherst 4UCLA kwl042@ucsd.edu {smmaji,ravinash,soattos}@amazon.com Abstract Many meta-<b>learning</b> approaches for few-shot <b>learning</b> rely on simple base learners such as nearest-neighbor clas-si\ufb01ers. However, even in the few-shot regime, discrimina-tively trained linear predictors <b>can</b> offer better generaliza-tion. We propose to ...", "dateLastCrawled": "2022-02-02T00:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Summary - Convex optimization overview</b> - CS 229 - Machine <b>Learning</b> ...", "url": "https://www.studocu.com/en-us/document/stanford-university/machine-learning/summary-convex-optimization-overview/746800", "isFamilyFriendly": true, "displayUrl": "https://www.studocu.com/.../machine-<b>learning</b>/<b>summary-convex-optimization-overview</b>/746800", "snippet": "<b>Convex</b> <b>Optimization</b> Overview <b>convex</b> <b>optimization</b> overview chuong do november 29, 2009 during last section, we began our study of <b>convex</b> <b>optimization</b>, the study", "dateLastCrawled": "2022-01-30T10:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Why is Convex Optimization such a big</b> deal in Machine <b>Learning</b>? - Quora", "url": "https://www.quora.com/Why-is-Convex-Optimization-such-a-big-deal-in-Machine-Learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>Why-is-Convex-Optimization-such-a-big</b>-deal-in-Machine-<b>Learning</b>", "snippet": "Answer (1 of 10): <b>Convex</b> <b>optimization</b> is the core of most machine <b>learning</b> methods Some key concepts here: - <b>Convex</b> functions are those for which it&#39;s possible to draw a line segment from any two points on the graph and this line will always be inside the graph (except at the endpoints). This m...", "dateLastCrawled": "2022-01-17T17:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Theory <b>of Convex Optimization for Machine Learning</b>", "url": "https://www.researchgate.net/publication/262489426_Theory_of_Convex_Optimization_for_Machine_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/262489426_Theory_of_<b>Convex</b>_<b>Optimization</b>_for...", "snippet": "First-order methods for <b>convex</b> <b>optimization</b> play a fundamental role in the solution of modern large-scale computational problems, encompassing applications in machine <b>learning</b> (Bubeck, 2014 ...", "dateLastCrawled": "2021-11-07T19:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "What jobs are related <b>to convex optimization, discrete optimization</b> ...", "url": "https://www.quora.com/What-jobs-are-related-to-convex-optimization-discrete-optimization-and-algorithms-What-additional-skills-do-I-need-for-these-jobs", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-jobs-are-related-to-<b>convex</b>-<b>optimization</b>-discrete...", "snippet": "Answer (1 of 4): This is an interesting question. I started my graduate study in operations research and slowly started taking <b>optimization</b> (or <b>convex</b> <b>optimization</b>) courses. I took about 10 courses just in <b>optimization</b> because well it was very interesting. Only later I slowly found out that optim...", "dateLastCrawled": "2022-01-22T11:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Learning</b> to Optimize with Reinforcement <b>Learning</b> \u2013 The Berkeley ...", "url": "https://bair.berkeley.edu/blog/2017/09/12/learning-to-optimize-with-rl/", "isFamilyFriendly": true, "displayUrl": "https://bair.berkeley.edu/blog/2017/09/12/<b>learning</b>-to-optimize-with-rl", "snippet": "There are two reasons: first, many <b>optimization</b> algorithms are devised under the assumption of convexity and applied to non-<b>convex</b> objective functions; by <b>learning</b> the <b>optimization</b> algorithm under the same setting as it will actually be used in practice, the learned <b>optimization</b> algorithm could hopefully achieve better performance. Second, devising new <b>optimization</b> algorithms manually is usually laborious and <b>can</b> take months or years; <b>learning</b> the <b>optimization</b> algorithm could reduce the ...", "dateLastCrawled": "2022-02-03T06:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Notes from MIT Deep <b>Learning</b> Theory and Non-<b>Convex</b> <b>Optimization</b> ...", "url": "https://kiranvodrahalli.github.io/notes/liveTeX/MITdeep2019/", "isFamilyFriendly": true, "displayUrl": "https://kiranvodrahalli.github.io/notes/liveTeX/MITdeep2019", "snippet": "I\u2019ll say yeah, machine <b>learning</b> is <b>optimization</b> \u2014 I <b>can</b>\u2019t separate them. The more I work on them, the more I cannot separate between the two. Machine learnign really should be understood as an <b>optimization</b> problem. Deep <b>learning</b> is equal to nonconvex <b>learning</b> in my mind. The only <b>convex</b> <b>learning</b> is linear <b>learning</b> (shallow, one layer), once you go to deep, you\u2019re nonconvex and vice versa. So in my mind they\u2019re almost analagous to each other.", "dateLastCrawled": "2022-01-20T20:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Optimization</b> for Deep <b>Learning</b>: An Overview | SpringerLink", "url": "https://link.springer.com/article/10.1007/s40305-020-00309-6", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s40305-020-00309-6", "snippet": "<b>Optimization</b> is a critical component in deep <b>learning</b>. We think <b>optimization</b> for neural networks is an interesting topic for theoretical research due to various reasons. First, its tractability despite non-convexity is an intriguing question and may greatly expand our understanding of tractable problems. Second, classical <b>optimization</b> theory is far from enough to explain many phenomena. Therefore, we would like to understand the challenges and opportunities from a theoretical perspective and ...", "dateLastCrawled": "2022-01-22T19:26:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A <b>convex</b> <b>optimization</b> approach for identification of <b>human</b> tissue ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4908329/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC4908329", "snippet": "These scores are properly normalized and <b>can</b> <b>be compared</b> across different genes and platforms. Leveraging the UPC method, we propose a novel approach that uses the topological context of an interaction to infer its specificity score. Our approach formulates the inference problem as a suitably regularized <b>convex</b> <b>optimization</b> problem. The objective function of the <b>optimization</b> problem has two terms\u2014the first term corresponds to a diffusion kernel that propagates activity of genes through ...", "dateLastCrawled": "2017-01-11T22:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "What are <b>the advantages of convex optimization compared to more general</b> ...", "url": "https://www.quora.com/What-are-the-advantages-of-convex-optimization-compared-to-more-general-optimization-problems", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-are-<b>the-advantages-of-convex-optimization-compared</b>-to-more...", "snippet": "Answer: Convexity confers two advantages. The first is that, in a constrained problem, a <b>convex</b> feasible region makes it easier to ensure that you do not generate infeasible solutions while searching for an optimum. If you have two feasible solutions, any solution within the line segment connecti...", "dateLastCrawled": "2022-01-14T12:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Why is Convex Optimization such a big</b> deal in Machine <b>Learning</b>? - Quora", "url": "https://www.quora.com/Why-is-Convex-Optimization-such-a-big-deal-in-Machine-Learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>Why-is-Convex-Optimization-such-a-big</b>-deal-in-Machine-<b>Learning</b>", "snippet": "Answer (1 of 10): <b>Convex</b> <b>optimization</b> is the core of most machine <b>learning</b> methods Some key concepts here: - <b>Convex</b> functions are those for which it&#39;s possible to draw a line segment from any two points on the graph and this line will always be inside the graph (except at the endpoints). This m...", "dateLastCrawled": "2022-01-17T17:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "2 <b>Convex</b> Variational Formulations for <b>Learning</b> ... - <b>Optimization</b> Online", "url": "http://www.optimization-online.org/DB_FILE/2016/08/5586.pdf", "isFamilyFriendly": true, "displayUrl": "www.<b>optimization</b>-online.org/DB_FILE/2016/08/5586.pdf", "snippet": "<b>Convex</b> Variational Formulations for <b>Learning</b> Problems Pedro Borges Abstract\u2014In this article, we introduce new techniques to solve the nonlinear regression problem and the nonlinear classi\ufb01cation problem. Our benchmarks suggest that our method for regression is signi\ufb01cantly more effective when <b>compared</b> to classical methods and our method for classi\ufb01cation is competitive. Our list of classical methods includes least squares, random forests, decision trees, boosted trees, nearest ...", "dateLastCrawled": "2022-01-31T21:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Learning</b> to Optimize with Reinforcement <b>Learning</b> \u2013 The Berkeley ...", "url": "https://bair.berkeley.edu/blog/2017/09/12/learning-to-optimize-with-rl/", "isFamilyFriendly": true, "displayUrl": "https://bair.berkeley.edu/blog/2017/09/12/<b>learning</b>-to-optimize-with-rl", "snippet": "There are two reasons: first, many <b>optimization</b> algorithms are devised under the assumption of convexity and applied to non-<b>convex</b> objective functions; by <b>learning</b> the <b>optimization</b> algorithm under the same setting as it will actually be used in practice, the learned <b>optimization</b> algorithm could hopefully achieve better performance. Second, devising new <b>optimization</b> algorithms manually is usually laborious and <b>can</b> take months or years; <b>learning</b> the <b>optimization</b> algorithm could reduce the ...", "dateLastCrawled": "2022-02-03T06:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Machine <b>Learning</b> and <b>Optimization</b>", "url": "https://www-users.cse.umn.edu/~boley/ML-Optimization/Abstract.html", "isFamilyFriendly": true, "displayUrl": "https://www-users.cse.umn.edu/~boley/ML-<b>Optimization</b>/Abstract.html", "snippet": "Splitting methods (or more precisely alternating direction methods) are based on the idea that a general <b>convex</b> <b>optimization</b> problem <b>can</b> be split into two or more parts, each of which <b>can</b> be solved much more easily <b>compared</b> to the problem as a whole. The methods cycle through all the variables in turn, optimizing over each subset of variables leaving the rest fixed. The proposed work builds on a preliminary analysis of a simple model problem using the eigen-structure of certain matrix ...", "dateLastCrawled": "2021-11-23T08:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "15.095 Machine <b>Learning</b> under a Modern <b>Optimization Lens</b>", "url": "https://ryancorywright.github.io/pdf/15-095-syllabus-fall_2019.pdf", "isFamilyFriendly": true, "displayUrl": "https://ryancorywright.github.io/pdf/15-095-syllabus-fall_2019.pdf", "snippet": "using three modern <b>optimization</b> lenses: <b>convex</b>, robust and mixed integer <b>optimization</b>. We de-emphasize the use of probability models, start with data and then revisit the central problems of ML using formal <b>optimization</b> methods and demonstrate that they <b>can</b> greatly bene t from a modern <b>optimization</b> treatment. We take a rigorous, non-heuristic ...", "dateLastCrawled": "2022-01-26T16:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Notes from MIT Deep <b>Learning</b> Theory and Non-<b>Convex</b> <b>Optimization</b> ...", "url": "https://kiranvodrahalli.github.io/notes/liveTeX/MITdeep2019/", "isFamilyFriendly": true, "displayUrl": "https://kiranvodrahalli.github.io/notes/liveTeX/MITdeep2019", "snippet": "I\u2019ll say yeah, machine <b>learning</b> is <b>optimization</b> \u2014 I <b>can</b>\u2019t separate them. The more I work on them, the more I cannot separate between the two. Machine learnign really should be understood as an <b>optimization</b> problem. Deep <b>learning</b> is equal to nonconvex <b>learning</b> in my mind. The only <b>convex</b> <b>learning</b> is linear <b>learning</b> (shallow, one layer), once you go to deep, you\u2019re nonconvex and vice versa. So in my mind they\u2019re almost analagous to each other.", "dateLastCrawled": "2022-01-20T20:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Deep Learning as a Mixed Convex-Combinatorial Optimization Problem</b> ...", "url": "https://www.arxiv-vanity.com/papers/1710.11573/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1710.11573", "snippet": "<b>Learning</b> a deep hard-threshold network thus reduces to finding a feasible setting of its targets and then optimizing its weights given these targets, i.e., mixed <b>convex</b>-combinatorial <b>optimization</b>. The simplest method for this is to perform exhaustive search on the targets. Exhaustive search iterates through all possible settings of the hidden-layer targets, updating the weights of each perceptron whose inputs or targets changed, and returns the weights and feasible targets that result in the ...", "dateLastCrawled": "2022-01-21T11:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>optimization</b> - Neural Networks: what&#39;s the point of <b>learning</b> features ...", "url": "https://cstheory.stackexchange.com/questions/20917/neural-networks-whats-the-point-of-learning-features-that-dont-linearly-separ", "isFamilyFriendly": true, "displayUrl": "https://cstheory.stackexchange.com/questions/20917/neural-networks-whats-the-point-of...", "snippet": "$\\begingroup$ <b>can</b> you rephrase that last question &quot;whats the point in it <b>learning</b> features?&quot; it seems not a question or to not be what you are intending to ask. plausibly the whole point/raison d&#39;etre of ML is to learn features. also, ANNs have been <b>learning</b> nonlinear functions ever since early days (1980s), that in particular is not a twist of deep <b>learning</b>. maybe what this question is getting at is the following: deep <b>learning</b> also seems to learn corresponding deep features that cannot be ...", "dateLastCrawled": "2022-01-13T21:40:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Optimization</b> for <b>Machine</b> <b>Learning</b>", "url": "http://optml.mit.edu/talks/pkuLectAlgo3.pdf", "isFamilyFriendly": true, "displayUrl": "optml.mit.edu/talks/pkuLectAlgo3.pdf", "snippet": "<b>Optimization</b> for <b>Machine</b> <b>Learning</b> \u2013 Sra, Nowozin, Wright Theory of <b>Convex</b> <b>Optimization</b> for <b>Machine</b> <b>Learning</b> \u2013 Bubeck NIPS 2016 <b>Optimization</b> Tutorial \u2013 Bach, Sra Some related courses: EE227A, Spring 2013, (Sra, UC Berkeley) 10-801, Spring 2014 (Sra, CMU) EE364a,b (Boyd, Stanford) EE236b,c (Vandenberghe, UCLA) Venues: NIPS, ICML, UAI, AISTATS, SIOPT, Math. Prog. Suvrit Sra(suvrit@mit.edu)<b>Optimization</b> for <b>Machine</b> <b>Learning</b> 2 / 29. Lecture Plan \u2013Introduction (3 lectures) \u2013Problems and ...", "dateLastCrawled": "2021-08-29T23:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Basic Concepts in Machine Learning</b>", "url": "https://machinelearningmastery.com/basic-concepts-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>basic-concepts-in-machine-learning</b>", "snippet": "For example combinatorial <b>optimization</b>, <b>convex</b> <b>optimization</b>, constrained <b>optimization</b>. All <b>machine learning</b> algorithms are combinations of these three components. A framework for understanding all algorithms. Types of <b>Learning</b> . There are four types of <b>machine learning</b>: Supervised <b>learning</b>: (also called inductive <b>learning</b>) Training data includes desired outputs. This is spam this is not, <b>learning</b> is supervised. Unsupervised <b>learning</b>: Training data does not include desired outputs. Example is ...", "dateLastCrawled": "2022-02-02T15:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Introduction to Machine Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/24/introduction-to-machine-learning-2/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/24/<b>introduction-to-machine-learning</b>-2", "snippet": "<b>Optimization</b> methods are applied to minimize the loss function by changing the parameter values, which is the central theme of <b>machine</b> <b>learning</b>.Zero-one loss is L0-1 = 1 (m &lt;= 0); in zero-one loss, value of loss is 0 for m &gt;= 0 whereas 1 for m &lt; 0. The difficult part with this loss is it is not differentiable, non-<b>convex</b>, and also NP-hard. Hence, in order to make <b>optimization</b> feasible and solvable, these losses are replaced by different surrogate losses for different problems.", "dateLastCrawled": "2022-01-17T05:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "11.2. <b>Convexity</b> \u2014 Dive into Deep <b>Learning</b> 0.17.2 documentation", "url": "https://d2l.ai/chapter_optimization/convexity.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_<b>optimization</b>/<b>convexity</b>.html", "snippet": "Furthermore, even though the <b>optimization</b> problems in deep <b>learning</b> are generally nonconvex, they often exhibit some properties of <b>convex</b> ones near local minima. This can lead to exciting new <b>optimization</b> variants such as [Izmailov et al., 2018].", "dateLastCrawled": "2022-01-30T21:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Optimization</b> for deep <b>learning</b>: an overview", "url": "https://www.ise.ncsu.edu/fuzzy-neural/wp-content/uploads/sites/9/2022/01/Optimization-for-deep-learning.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.ise.ncsu.edu/.../uploads/sites/9/2022/01/<b>Optimization</b>-for-deep-<b>learning</b>.pdf", "snippet": "timization problems beyond <b>convex</b> problems. A somewhat related <b>analogy</b> is the development of conic <b>optimization</b>: in 1990\u2019s, researchers realized that many seemingly non-<b>convex</b> problems can actually be reformulated as conic <b>optimization</b> problems (e.g. semi-de nite programming) which are <b>convex</b> problems, thus the boundary of tractability has advanced signi cantly. Neural network problems are surely not the worst non-<b>convex</b> <b>optimization</b> problems and their global optima could be found ...", "dateLastCrawled": "2022-01-19T03:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Regularization : What? Why? and How? (Part -1) | by Siddhant Rai ...", "url": "https://medium.com/mlearning-ai/regularization-what-why-and-how-part-1-ef6bdb6bafea", "isFamilyFriendly": true, "displayUrl": "https://medium.com/m<b>learning</b>-ai/regularization-what-why-and-how-part-1-ef6bdb6bafea", "snippet": "In general <b>machine</b> <b>learning</b> sense, it is solving an objective function to perform maximum or minimum evaluation. In reality, <b>optimization</b> is lot more profound in usage. Then we have two terms ...", "dateLastCrawled": "2022-01-28T00:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Gradient</b> Descent: A Quick, Simple Introduction | Built In", "url": "https://builtin.com/data-science/gradient-descent", "isFamilyFriendly": true, "displayUrl": "https://builtin.com/data-science/<b>gradient</b>-descent", "snippet": "<b>Gradient</b> descent is an <b>optimization</b> algorithm that&#39;s used when training a <b>machine</b> <b>learning</b> model. It&#39;s based on a <b>convex</b> function and tweaks its parameters iteratively to minimize a given function to its local minimum.", "dateLastCrawled": "2022-02-02T07:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Summary of Thesis: <b>Non-convex Optimization for Machine Learning</b>: Design ...", "url": "https://ai.stanford.edu/~tengyuma/slides/summary.pdf", "isFamilyFriendly": true, "displayUrl": "https://ai.stanford.edu/~tengyuma/slides/summary.pdf", "snippet": "Summary of Thesis: <b>Non-convex Optimization for Machine Learning</b>: Design, Analysis, and Understanding Tengyu Ma October 15, 2018 Non-<b>convex</b> <b>optimization</b> is ubiquitous in modern <b>machine</b> <b>learning</b>: re-cent breakthroughs in deep <b>learning</b> require optimizing non-<b>convex</b> training objective functions; problems that admit accurate <b>convex</b> relaxation can often be solved more e ciently with non-<b>convex</b> formulations. However, the theoretical understanding of non-<b>convex</b> <b>optimization</b> remained rather limited ...", "dateLastCrawled": "2021-09-02T16:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "7. <b>Optimization</b>, the central part of any <b>Machine</b> <b>Learning</b> algortithm ...", "url": "https://compphysics.github.io/MachineLearning/doc/LectureNotes/_build/html/chapteroptimization.html", "isFamilyFriendly": true, "displayUrl": "https://compphysics.github.io/<b>MachineLearning</b>/doc/LectureNotes/_build/html/chapter...", "snippet": "7. <b>Optimization</b>, the central part of any <b>Machine</b> <b>Learning</b> algortithm\u00b6. Almost every problem in <b>machine</b> <b>learning</b> and data science starts with a dataset \\(X\\), a model \\(g(\\beta)\\), which is a function of the parameters \\(\\beta\\) and a cost function \\(C(X, g(\\beta))\\) that allows us to judge how well the model \\(g(\\beta)\\) explains the observations \\(X\\).The model is fit by finding the values of \\(\\beta\\) that minimize the cost function. Ideally we would be able to solve for \\(\\beta ...", "dateLastCrawled": "2022-01-31T15:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "[2005.14605] CoolMomentum: A Method for Stochastic <b>Optimization</b> by ...", "url": "https://arxiv.org/abs/2005.14605", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/abs/2005.14605", "snippet": "This <b>analogy</b> provides useful insights for non-<b>convex</b> stochastic <b>optimization</b> in <b>machine</b> <b>learning</b>. Here we find that integration of the discretized Langevin equation gives a coordinate updating rule equivalent to the famous Momentum <b>optimization</b> algorithm. As a main result, we show that a gradual decrease of the momentum coefficient from the initial value close to unity until zero is equivalent to application of Simulated Annealing or slow cooling, in physical terms. Making use of this novel ...", "dateLastCrawled": "2021-10-23T08:45:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Best <b>Artificial Intelligence</b> Course (AIML) by UT Austin", "url": "https://www.mygreatlearning.com/pg-program-artificial-intelligence-course", "isFamilyFriendly": true, "displayUrl": "https://www.mygreat<b>learning</b>.com/pg-program-<b>artificial-intelligence</b>-course", "snippet": "<b>Machine</b> <b>Learning</b>: <b>Machine</b> <b>learning</b> is a sub-branch of AI that teaches machines to learn any task without the help of explicit directions. It teaches machines to learn by drawing inferences from past experience. <b>Machine</b> <b>learning</b> primarily focuses on developing computer programs that can access and analyze data to identify patterns and understand data behaviour to reach possible conclusions without any kind of human intervention.", "dateLastCrawled": "2022-02-01T17:58:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "What is <b>the relationship between Online Machine Learning</b> and ...", "url": "https://www.quora.com/What-is-the-relationship-between-Online-Machine-Learning-and-Incremental-Machine-Learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>the-relationship-between-Online-Machine-Learning</b>-and...", "snippet": "Answer (1 of 4): Online <b>learning</b> usually refers to the case where each example is only used once (e.g. if you&#39;re updating an ad click prediction model online after each impression or click), while incremental methods usually pick one example at a time from a finite dataset and can process the sam...", "dateLastCrawled": "2022-01-14T06:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Which <b>machine</b> <b>learning</b> algorithms for classification support online ...", "url": "https://www.quora.com/Which-machine-learning-algorithms-for-classification-support-online-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Which-<b>machine</b>-<b>learning</b>-algorithms-for-classification-support...", "snippet": "Answer (1 of 5): Most algorithms can be adapted to make them online, even though the standard implementations may not support it. E.g. both decision trees and support ...", "dateLastCrawled": "2022-01-09T00:45:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "SimplifiedMachineLearningWorkflows-book/Wolfram-Technology-Conference ...", "url": "https://github.com/antononcube/SimplifiedMachineLearningWorkflows-book/blob/master/Data/Wolfram-Technology-Conference-2016-to-2019-abstracts.csv", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/antononcube/Simplified<b>MachineLearning</b>Workflows-book/blob/master/...", "snippet": "Finally, I use <b>machine</b> <b>learning</b> algorithms to train a series of classifiers that can predict a text&#39;s authorship based on its MFW frequencies. Cross-validation indicates that Gallus and Monk are very likely one and the same author. The results also reveal the especially high and hitherto underexplored effectiveness of the Bray Curtis Distance measure and of logistic regression in shedding light on questions of authorship attribution. Data Analytics &amp; Information Science : 2016.Gunnar.Prei\u00df ...", "dateLastCrawled": "2021-12-28T12:42:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(convex optimization)  is like +(human learning)", "+(convex optimization) is similar to +(human learning)", "+(convex optimization) can be thought of as +(human learning)", "+(convex optimization) can be compared to +(human learning)", "machine learning +(convex optimization AND analogy)", "machine learning +(\"convex optimization is like\")", "machine learning +(\"convex optimization is similar\")", "machine learning +(\"just as convex optimization\")", "machine learning +(\"convex optimization can be thought of as\")", "machine learning +(\"convex optimization can be compared to\")"]}