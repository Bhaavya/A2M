{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Word</b> <b>Embedding</b> and Sentiment Analysis (IMDB)", "url": "https://thedatafrog.com/articles/word-embedding-sentiment-analysis", "isFamilyFriendly": true, "displayUrl": "https://thedatafrog.com/articles/<b>word</b>-<b>embedding</b>-sentiment-analysis", "snippet": "In the <b>embedding</b> process, <b>each</b> <b>word</b> (or more precisely, <b>each</b> integer corresponding to a <b>word</b>) is translated to a vector in N-dimensional <b>space</b>. That does sound complicated! but it&#39;s not. To understand better, we&#39;re going to perform the <b>embedding</b> in two dimensions only. <b>Each</b> <b>word</b> is then going to be converted to a vector with two coordinates $(x,y)$, and these coordinates can be <b>represented</b> as a <b>point</b> in a plane. As you will see below, the text of a review will appear as an ensemble of points ...", "dateLastCrawled": "2021-12-25T03:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Word embeddings</b>, what are they really? | by Kristin H. Huseby | Towards ...", "url": "https://towardsdatascience.com/word-embeddings-what-are-they-really-f106e1ff0874", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>word-embeddings</b>-what-are-they-really-f106e1ff0874", "snippet": "Now this is almost <b>like</b> a magic trick: The <b>point</b> of this model was never to predict the context words, what we are after is the hidden layer, which is the <b>word</b> <b>embedding</b>. If you are familiar with autoencoders, this is a similar concept. The neural network has to utilize the vector <b>space</b> in the size of the hidden layer to capture the information about <b>each</b> <b>word</b> that enables it to predict the context words. If two words often occur with the same context words, the model has to produce similar ...", "dateLastCrawled": "2022-01-26T04:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Word</b> Embeddings - Contemporary Text Analysis | <b>Coursera</b>", "url": "https://www.coursera.org/lecture/business-analytics-diversity-of-practical-applications/word-embeddings-YXBjh", "isFamilyFriendly": true, "displayUrl": "https://<b>www.coursera.org</b>/.../<b>word</b>-<b>embeddings</b>-YXBjh", "snippet": "For instance will have, a small vector of cat and it will be located nearby the <b>word</b> feline in our <b>multidimensional</b> <b>space</b>, <b>like</b> shown in the picture. Here, we see that similar words are close to <b>each</b> other, and words with different sensors are located more distantly. The next great idea inside NLP is the distributional hypothesis, formulated by Firth almost 50 years ago. And the idea is, that the words meaning can be acquired from the worst context. For example, if you have several sentences ...", "dateLastCrawled": "2022-01-16T15:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Graph-based exploration and clustering analysis of semantic spaces ...", "url": "https://appliednetsci.springeropen.com/articles/10.1007/s41109-019-0228-y", "isFamilyFriendly": true, "displayUrl": "https://appliednetsci.springeropen.com/articles/10.1007/s41109-019-0228-y", "snippet": "<b>Word</b> <b>embedding</b> models map words (or <b>word</b> phrases) in large corpora of text into <b>a multidimensional</b> vector <b>space</b>, <b>where each</b> <b>word</b> <b>is represented</b> by a vector in this <b>space</b>, and semantically similar words are located closer to <b>each</b> other. <b>Word</b> embeddings are created using self-supervised machine learning algorithms. The benefit of semantic spaces generated by <b>word</b> <b>embedding</b> algorithms is that they can be trained on very large text corpora (e.g., texts with 100 billion words from Google News ...", "dateLastCrawled": "2022-02-02T21:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Sentiment Classification using <b>Word</b> Embeddings (<b>Word2Vec</b>) | by Dipika ...", "url": "https://medium.com/swlh/sentiment-classification-using-word-embeddings-word2vec-aedf28fbb8ca", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/sentiment-classification-using-<b>word</b>-<b>embeddings</b>-<b>word2vec</b>-aedf28...", "snippet": "<b>Word</b> embeddings use some models to map a <b>word</b> into vectors such that similar words will be closer to <b>each</b> other. As shown in the below figure, for example some of the positive words which are ...", "dateLastCrawled": "2022-01-30T23:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "50+ <b>NLP Interview Questions and Answers</b> in 2022", "url": "https://www.mygreatlearning.com/blog/nlp-interview-questions/", "isFamilyFriendly": true, "displayUrl": "https://www.mygreatlearning.com/blog/nlp-interview-questions", "snippet": "35. <b>Word</b> embeddings capture multiple dimensions of data and are <b>represented</b> as vectors a. True b. False Ans: a) 36. In NLP, <b>Word</b> <b>embedding</b> vectors help establish distance between two tokens a. True b. False Ans: a) One can use Cosine similarity to establish distance between two vectors <b>represented</b> through <b>Word</b> Embeddings 37. Language Biases are ...", "dateLastCrawled": "2022-02-02T21:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Decoding the language of <b>microbiomes using word-embedding techniques</b> ...", "url": "https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007859", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007859", "snippet": "Two words, \u201capple\u201d and \u201cbanana\u201d, are close to <b>each</b> other in <b>embedding</b> <b>space</b> because they tend to occur with similar sets of words <b>like</b> \u201ceat\u201d, \u201cfruit\u201d, \u201ctasty\u201d, and \u201csmoothie\u201d. Likewise, the words \u201cking\u201d and \u201cmarshmallow\u201d tend to occur in different contexts; \u201cking\u201d is most often found in the company of words <b>like</b> \u201cpolitics\u201d, \u201cthrone\u201d, and \u201cempire\u201d while \u201cmarshmallow\u201d is found with words <b>like</b> \u201ctoddler\u201d, \u201cfluffy\u201d, and \u201cscrumptious ...", "dateLastCrawled": "2020-05-04T19:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Plotting text and <b>image</b> vectors using t-SNE | by ... - Towards Data Science", "url": "https://towardsdatascience.com/plotting-text-and-image-vectors-using-t-sne-d0e43e55d89", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/plotting-text-and-<b>image</b>-vectors-using-t-sne-d0e43e55d89", "snippet": "Text vectors (either <b>word</b> vectors or sentence vectors )are created by converting textual data into the numerical form using <b>embedding</b> techniques <b>like</b> word2vec, fasttext, Infersent, Google universal sentence encoder etc. The dimension of your vector may depend upon your technique of preparing it. For example, Infesent creates a vector of 4096 dimensions for <b>each</b> sentence, whereas GUSE(Google Universal Sentence Encoder) creates a vector of 512 dimensions for <b>each</b> sentence.", "dateLastCrawled": "2022-01-27T16:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Supplementto: Jones,JasonJ.,MohammadRuhulAmin,JessicaKim ...", "url": "https://www.sociologicalscience.com/download/vol-7/january/supplemental_materials/SocSci_v7_1to35_supp.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.sociologicalscience.com/download/vol-7/january/supplemental_materials/...", "snippet": "In a <b>word</b> <b>embedding</b>, every <b>word</b> is individually <b>represented</b> by a vector in one <b>multidimensional</b> <b>space</b>. The natural metric to measure distances in such a <b>space</b> is the cosine distance. One way to think of the value of the cosine distance is as the rotation necessary to bring two vectors into alignment. If two vectors have a cosine distance of 0 ...", "dateLastCrawled": "2021-09-15T23:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Sequence Classification with LSTM Recurrent Neural Networks in Python ...", "url": "https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/sequence-classification-", "snippet": "<b>Word</b> <b>Embedding</b>. We will map <b>each</b> movie review into a real vector domain, a popular technique when working with text called <b>word</b> <b>embedding</b>. This is a technique where words are encoded as real-valued vectors in a high dimensional <b>space</b>, where the similarity between words in terms of meaning translates to closeness in the vector <b>space</b>. Keras provides a convenient way to convert positive integer representations of words into a <b>word</b> <b>embedding</b> by an <b>Embedding</b> layer. We will map <b>each</b> <b>word</b> onto a 32 ...", "dateLastCrawled": "2022-02-02T22:19:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Word</b> <b>Embedding</b> and Sentiment Analysis (IMDB)", "url": "https://thedatafrog.com/articles/word-embedding-sentiment-analysis", "isFamilyFriendly": true, "displayUrl": "https://thedatafrog.com/articles/<b>word</b>-<b>embedding</b>-sentiment-analysis", "snippet": "In the <b>embedding</b> process, <b>each</b> <b>word</b> (or more precisely, <b>each</b> integer corresponding to a <b>word</b>) is translated to a vector in N-dimensional <b>space</b>. That does sound complicated! but it&#39;s not. To understand better, we&#39;re going to perform the <b>embedding</b> in two dimensions only. <b>Each</b> <b>word</b> is then going to be converted to a vector with two coordinates $(x,y)$, and these coordinates can be <b>represented</b> as a <b>point</b> in a plane. As you will see below, the text of a review will appear as an ensemble of points ...", "dateLastCrawled": "2021-12-25T03:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Graph-based exploration and clustering analysis of semantic spaces ...", "url": "https://appliednetsci.springeropen.com/articles/10.1007/s41109-019-0228-y", "isFamilyFriendly": true, "displayUrl": "https://appliednetsci.springeropen.com/articles/10.1007/s41109-019-0228-y", "snippet": "<b>Word</b> <b>embedding</b> models map words (or <b>word</b> phrases) in large corpora of text into <b>a multidimensional</b> vector <b>space</b>, <b>where each</b> <b>word</b> <b>is represented</b> by a vector in this <b>space</b>, and semantically <b>similar</b> words are located closer to <b>each</b> other. <b>Word</b> embeddings are created using self-supervised machine learning algorithms. The benefit of semantic spaces generated by <b>word</b> <b>embedding</b> algorithms is that they can be trained on very large text corpora (e.g., texts with 100 billion words from Google News ...", "dateLastCrawled": "2022-02-02T21:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Vector <b>Space</b> Models for the Digital Humanities", "url": "http://bookworm.benschmidt.org/posts/2015-10-25-Word-Embeddings.html", "isFamilyFriendly": true, "displayUrl": "bookworm.benschmidt.org/posts/2015-10-25-<b>Word</b>-<b>Embeddings</b>.html", "snippet": "<b>Word</b> <b>embedding</b> models try to reflect <b>similar</b> relationships between words with <b>similar</b> paths in <b>space</b>. We train a model that \u2018learns\u2019 scores for <b>each</b> <b>word</b> in the text for some arbitrary number of characteristics. The number of characteristics that we choose are called the dimensions: <b>each</b> <b>word</b> occupies a distinct <b>point</b> in that broader \u201c<b>space</b>.\u201d Optimally positioning words in <b>space</b>. Here is a simple example: a plot of words starting with \u2018g\u2019 in a two-dimensional <b>space</b>. On the ...", "dateLastCrawled": "2021-12-16T02:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "SOIL - <b>Word</b> embeddings <b>for application in geosciences: development</b> ...", "url": "https://soil.copernicus.org/articles/5/177/2019/", "isFamilyFriendly": true, "displayUrl": "https://soil.copernicus.org/articles/5/177/2019", "snippet": "In one-hot encoding, <b>each</b> <b>word</b> <b>is represented</b> by a vector of length equal to the number of classes or words, <b>where each</b> dimension represents a feature. The problem with this representation is that the resulting array is sparse (mostly zeros) and very large when using large corpora; in addition, it also presents the problem of poor estimation of the parameters of the less-common words ( Turian et al. , 2010 ) .", "dateLastCrawled": "2022-01-23T14:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Sentiment Classification using <b>Word</b> Embeddings (<b>Word2Vec</b>) | by Dipika ...", "url": "https://medium.com/swlh/sentiment-classification-using-word-embeddings-word2vec-aedf28fbb8ca", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/sentiment-classification-using-<b>word</b>-<b>embeddings</b>-<b>word2vec</b>-aedf28...", "snippet": "<b>Word</b> embeddings use some models to map a <b>word</b> into vectors such that <b>similar</b> words will be closer to <b>each</b> other. As shown in the below figure, for example some of the positive words which are ...", "dateLastCrawled": "2022-01-30T23:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Decoding the language of <b>microbiomes using word-embedding techniques</b> ...", "url": "https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007859", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007859", "snippet": "Using this algorithm, two taxa that occur with <b>similar</b> sets of other taxa at <b>similar</b> frequencies should be close in <b>embedding</b> <b>space</b>, and two taxa that are found in the presence of different neighbor sets should be far from <b>each</b> other. To visualize this, we return to the analogy of <b>word</b> analysis. Two words, \u201capple\u201d and \u201cbanana\u201d, are close to <b>each</b> other in <b>embedding</b> <b>space</b> because they tend to occur with <b>similar</b> sets of words like \u201ceat\u201d, \u201cfruit\u201d, \u201ctasty\u201d, and \u201csmoothie ...", "dateLastCrawled": "2020-05-04T19:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Plotting text and <b>image</b> vectors using t-SNE | by ... - Towards Data Science", "url": "https://towardsdatascience.com/plotting-text-and-image-vectors-using-t-sne-d0e43e55d89", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/plotting-text-and-<b>image</b>-vectors-using-t-sne-d0e43e55d89", "snippet": "Text vectors (either <b>word</b> vectors or sentence vectors )are created by converting textual data into the numerical form using <b>embedding</b> techniques like word2vec, fasttext, Infersent, Google universal sentence encoder etc. The dimension of your vector may depend upon your technique of preparing it. For example, Infesent creates a vector of 4096 dimensions for <b>each</b> sentence, whereas GUSE(Google Universal Sentence Encoder) creates a vector of 512 dimensions for <b>each</b> sentence.", "dateLastCrawled": "2022-01-27T16:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Topic Modeling (LDA/Word2Vec) with Spacy</b> \u00b7 GitHub", "url": "https://gist.github.com/narulkargunjan/5319ed32d092d1fa7b52fec3a774e0e5", "isFamilyFriendly": true, "displayUrl": "https://gist.github.com/narulkargunjan/5319ed32d092d1fa7b52fec3a774e0e5", "snippet": "# The goal of *<b>word</b> vector <b>embedding</b> models*, or *<b>word</b> vector models* for short, is to learn dense, numerical vector representations for <b>each</b> term in a corpus vocabulary. If the model is successful, the vectors it learns about <b>each</b> term should encode some information about the *meaning* or *concept* the term represents, and the relationship between it and other terms in the vocabulary. <b>Word</b> vector models are also fully unsupervised &amp;mdash; they learn all of these meanings and relationships ...", "dateLastCrawled": "2022-01-18T22:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "50+ <b>NLP Interview Questions and Answers</b> in 2022", "url": "https://www.mygreatlearning.com/blog/nlp-interview-questions/", "isFamilyFriendly": true, "displayUrl": "https://www.mygreatlearning.com/blog/nlp-interview-questions", "snippet": "35. <b>Word</b> embeddings capture multiple dimensions of data and are <b>represented</b> as vectors a. True b. False Ans: a) 36. In NLP, <b>Word</b> <b>embedding</b> vectors help establish distance between two tokens a. True b. False Ans: a) One can use Cosine similarity to establish distance between two vectors <b>represented</b> through <b>Word</b> Embeddings 37. Language Biases are ...", "dateLastCrawled": "2022-02-02T21:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Sequence Classification with LSTM Recurrent Neural Networks in Python ...", "url": "https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/sequence-classification-", "snippet": "We will map <b>each</b> movie review into a real vector domain, a popular technique when working with text called <b>word</b> <b>embedding</b>. This is a technique where words are encoded as real-valued vectors in a high dimensional <b>space</b>, where the similarity between words in terms of meaning translates to closeness in the vector <b>space</b>.", "dateLastCrawled": "2022-02-02T22:19:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Word</b> <b>Embedding</b> and Sentiment Analysis (IMDB)", "url": "https://thedatafrog.com/articles/word-embedding-sentiment-analysis", "isFamilyFriendly": true, "displayUrl": "https://thedatafrog.com/articles/<b>word</b>-<b>embedding</b>-sentiment-analysis", "snippet": "In the <b>embedding</b> process, <b>each</b> <b>word</b> (or more precisely, <b>each</b> integer corresponding to a <b>word</b>) is translated to a vector in N-dimensional <b>space</b>. That does sound complicated! but it&#39;s not. To understand better, we&#39;re going to perform the <b>embedding</b> in two dimensions only. <b>Each</b> <b>word</b> is then going to be converted to a vector with two coordinates $(x,y)$, and these coordinates <b>can</b> be <b>represented</b> as a <b>point</b> in a plane. As you will see below, the text of a review will appear as an ensemble of points ...", "dateLastCrawled": "2021-12-25T03:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Vector <b>Space</b> Models for the Digital Humanities", "url": "http://bookworm.benschmidt.org/posts/2015-10-25-Word-Embeddings.html", "isFamilyFriendly": true, "displayUrl": "bookworm.benschmidt.org/posts/2015-10-25-<b>Word</b>-<b>Embeddings</b>.html", "snippet": "<b>Word</b> <b>embedding</b> models try to reflect similar relationships between words with similar paths in <b>space</b>. We train a model that \u2018learns\u2019 scores for <b>each</b> <b>word</b> in the text for some arbitrary number of characteristics. The number of characteristics that we choose are called the dimensions: <b>each</b> <b>word</b> occupies a distinct <b>point</b> in that broader \u201c<b>space</b>.\u201d Optimally positioning words in <b>space</b>. Here is a simple example: a plot of words starting with \u2018g\u2019 in a two-dimensional <b>space</b>. On the ...", "dateLastCrawled": "2021-12-16T02:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Extending Multi-Sense <b>Word</b> <b>Embedding</b> to Phrases and Sentences for ...", "url": "https://deepai.org/publication/extending-multi-sense-word-embedding-to-phrases-and-sentences-for-unsupervised-semantic-applications", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/extending-multi-sense-<b>word</b>-<b>embedding</b>-to-phrases-and...", "snippet": "Most unsupervised NLP models represent <b>each</b> <b>word</b> with a single <b>point</b> or single region in semantic <b>space</b>, while the existing multi-sense <b>word</b> embeddings cannot represent longer <b>word</b> sequences like phrases or sentences. We propose a novel <b>embedding</b> method for a text sequence (a phrase or a sentence) <b>where each</b> sequence <b>is represented</b> by a distinct set of multi-mode codebook embeddings to capture different semantic facets of its meaning. The codebook embeddings <b>can</b> be viewed as the cluster ...", "dateLastCrawled": "2022-01-25T08:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) MSc <b>Thesis: Word Vector-Space Embeddings of Natural Language Data</b> ...", "url": "https://www.academia.edu/11719738/MSc_Thesis_Word_Vector_Space_Embeddings_of_Natural_Language_Data_over_Time", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/11719738/MSc_<b>Thesis_Word_Vector_Space_Embeddings</b>_of_Natural...", "snippet": "Dynamic embeddings <b>can</b> be viewed from two perspectives - (1) From <b>word</b> perspec- tive as trajectories of words moving in <b>space</b> with changing times and (2) From time perspective as a stack of static <b>word</b> embeddings (one for <b>each</b> time slice). The <b>word</b> perspective inspired us to create an application of the dynamic <b>embedding</b>: the vi- sualization tool (see chapter 4). This tool helps visualize semantic change and other diachronic phenomena. This tool posed a challenge of reducing the dimension of ...", "dateLastCrawled": "2022-01-14T04:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) Extending Multi-Sense <b>Word</b> <b>Embedding</b> to Phrases and Sentences for ...", "url": "https://www.researchgate.net/publication/350483831_Extending_Multi-Sense_Word_Embedding_to_Phrases_and_Sentences_for_Unsupervised_Semantic_Applications", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/350483831_Extending_Multi-Sense_<b>Word</b>...", "snippet": "The codebook embeddings <b>can</b> be viewed as the cluster centers which summarize the distribution of possibly co-occurring words in a pre-trained <b>word</b> <b>embedding</b> <b>space</b>. We introduce an end-to-end ...", "dateLastCrawled": "2021-11-14T06:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Cultural cartography with word embeddings</b> - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S0304422X21000504", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0304422X21000504", "snippet": "This notion forms the basis of \u201c<b>embedding</b>\u201d methods in computer science and is a mathematical feature common to many matrix factorization tools familiar to social scientists\u2014e.g., factor analysis and correspondence analysis. 11 In comparison to other kinds of dimensional analysis, <b>word</b> embeddings have a relatively high number of dimensions because, in part, relational meanings of words are intransitive (<b>word</b> A <b>can</b> be similar to <b>word</b> B, and <b>word</b> C <b>can</b> be similar to <b>word</b> B, but this does ...", "dateLastCrawled": "2022-01-17T06:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) From <b>Word</b> To Sense Embeddings: A Survey on Vector Representations ...", "url": "https://www.researchgate.net/publication/329507891_From_Word_To_Sense_Embeddings_A_Survey_on_Vector_Representations_of_Meaning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/329507891_From_<b>Word</b>_To_Sense_<b>Embeddings</b>_A...", "snippet": "An illustration of the meaning conflation deficiency in a 2D semantic <b>space</b> around the ambiguous <b>word</b> mouse. Having the <b>word</b>, with its different meanings, <b>represented</b> as a single <b>point</b> (vector ...", "dateLastCrawled": "2022-01-04T16:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Sequence Classification with LSTM Recurrent Neural Networks in Python ...", "url": "https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/sequence-classification-", "snippet": "<b>Word</b> <b>Embedding</b>. We will map <b>each</b> movie review into a real vector domain, a popular technique when working with text called <b>word</b> <b>embedding</b>. This is a technique where words are encoded as real-valued vectors in a high dimensional <b>space</b>, where the similarity between words in terms of meaning translates to closeness in the vector <b>space</b>. Keras provides a convenient way to convert positive integer representations of words into a <b>word</b> <b>embedding</b> by an <b>Embedding</b> layer. We will map <b>each</b> <b>word</b> onto a 32 ...", "dateLastCrawled": "2022-02-02T22:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "AI-Assisted Design Concept Exploration Through Character <b>Space</b> Construction", "url": "https://www.frontiersin.org/articles/10.3389/fpsyg.2021.819237/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/fpsyg.2021.819237/full", "snippet": "A CS visually resembles a dimensionality reduction on 2D semantic <b>space</b> employing <b>multidimensional</b> scaling (MDS); however, the way a CS is formed in practice fundamentally differs from how an MDS is formed. A CS is informally utilized as a synthesis tool employing a quadrant system, on which a user identifies <b>each</b> end of the axis through creative exploration and speculation. The primary focus is the upper-right quadrant, which represents the target concept expressed by a compound adjective ...", "dateLastCrawled": "2022-02-02T09:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Frontiers | A Framework for the Computational Linguistic Analysis of ...", "url": "https://www.frontiersin.org/articles/10.3389/frai.2020.00055/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/frai.2020.00055", "snippet": "We address this tension by training vector <b>space</b> models to represent the data, in which <b>each</b> unique <b>word</b> in a large corpus <b>is represented</b> by a vector (<b>embedding</b>) in high-dimensional <b>space</b>. The geometry of the resulting vector <b>space</b> captures many semantic relations between words. Furthermore, prior work has shown that vector <b>space</b> models trained on corpora from different time periods <b>can</b> capture semantic change", "dateLastCrawled": "2022-01-25T19:45:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Word embeddings</b>, what are they really? | by Kristin H. Huseby | Towards ...", "url": "https://towardsdatascience.com/word-embeddings-what-are-they-really-f106e1ff0874", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>word-embeddings</b>-what-are-they-really-f106e1ff0874", "snippet": "Mathematically, we <b>can</b> see the representation of words in the bag of words type vectors as <b>each</b> <b>word</b> being <b>represented</b> by a direction in <b>a multidimensional</b> <b>space</b> where the dimensionality is determined by the number of words we have included in the vocabulary. The order of the words is random, and all words contribute in a direction orthogonal to all other words. This means that as far as we <b>can</b> tell from the representations of words, they are all equally different from <b>each</b> other. If we have ...", "dateLastCrawled": "2022-01-26T04:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Sentiment Classification using <b>Word</b> Embeddings (<b>Word2Vec</b>) | by Dipika ...", "url": "https://medium.com/swlh/sentiment-classification-using-word-embeddings-word2vec-aedf28fbb8ca", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/sentiment-classification-using-<b>word</b>-<b>embeddings</b>-<b>word2vec</b>-aedf28...", "snippet": "<b>Word</b> embeddings use some models to map a <b>word</b> into vectors such that similar words will be closer to <b>each</b> other. As shown in the below figure, for example some of the positive words which are ...", "dateLastCrawled": "2022-01-30T23:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "SOIL - <b>Word</b> embeddings <b>for application in geosciences: development</b> ...", "url": "https://soil.copernicus.org/articles/5/177/2019/", "isFamilyFriendly": true, "displayUrl": "https://soil.copernicus.org/articles/5/177/2019", "snippet": "In one-hot encoding, <b>each</b> <b>word</b> <b>is represented</b> by a vector of length equal to the number of classes or words, <b>where each</b> dimension represents a feature. The problem with this representation is that the resulting array is sparse (mostly zeros) and very large when using large corpora; in addition, it also presents the problem of poor estimation of the parameters of the less-common words ( Turian et al. , 2010 ) .", "dateLastCrawled": "2022-01-23T14:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Vector <b>Space</b> Models for the Digital Humanities", "url": "http://bookworm.benschmidt.org/posts/2015-10-25-Word-Embeddings.html", "isFamilyFriendly": true, "displayUrl": "bookworm.benschmidt.org/posts/2015-10-25-<b>Word</b>-<b>Embeddings</b>.html", "snippet": "<b>Word</b> <b>embedding</b> models try to reflect similar relationships between words with similar paths in <b>space</b>. We train a model that \u2018learns\u2019 scores for <b>each</b> <b>word</b> in the text for some arbitrary number of characteristics. The number of characteristics that we choose are called the dimensions: <b>each</b> <b>word</b> occupies a distinct <b>point</b> in that broader \u201c<b>space</b>.\u201d Optimally positioning words in <b>space</b>. Here is a simple example: a plot of words starting with \u2018g\u2019 in a two-dimensional <b>space</b>. On the ...", "dateLastCrawled": "2021-12-16T02:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "PNAS Plus: Hippocampal theta codes for distances in semantic and ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6883851/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC6883851", "snippet": "<b>Embedding</b> of <b>word</b> items in representational semantic spaces. (A) A total of 189 subjects performed a verbal free-recall task, in which they were asked to remember 12-item lists of simple nouns. After a distractor, subjects freely recall as many words as possible from the prior list, which <b>can</b> be conceptualized as a cognitive transition from one <b>word</b> to another. Successful recalls are indicated in green for an example list. B) <b>Each</b> <b>word</b> occupies a position in the pretrained 300-dimensional ...", "dateLastCrawled": "2021-11-09T09:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "50+ <b>NLP Interview Questions and Answers</b> in 2022", "url": "https://www.mygreatlearning.com/blog/nlp-interview-questions/", "isFamilyFriendly": true, "displayUrl": "https://www.mygreatlearning.com/blog/nlp-interview-questions", "snippet": "35. <b>Word</b> embeddings capture multiple dimensions of data and are <b>represented</b> as vectors a. True b. False Ans: a) 36. In NLP, <b>Word</b> <b>embedding</b> vectors help establish distance between two tokens a. True b. False Ans: a) One <b>can</b> use Cosine similarity to establish distance between two vectors <b>represented</b> through <b>Word</b> Embeddings 37. Language Biases are ...", "dateLastCrawled": "2022-02-02T21:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Sequence Classification with LSTM Recurrent Neural Networks in Python ...", "url": "https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/sequence-classification-", "snippet": "<b>Word</b> <b>Embedding</b>. We will map <b>each</b> movie review into a real vector domain, a popular technique when working with text called <b>word</b> <b>embedding</b>. This is a technique where words are encoded as real-valued vectors in a high dimensional <b>space</b>, where the similarity between words in terms of meaning translates to closeness in the vector <b>space</b>. Keras provides a convenient way to convert positive integer representations of words into a <b>word</b> <b>embedding</b> by an <b>Embedding</b> layer. We will map <b>each</b> <b>word</b> onto a 32 ...", "dateLastCrawled": "2022-02-02T22:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "The theory you need <b>to know before you start an NLP project</b> | by Arne ...", "url": "https://towardsdatascience.com/the-theory-you-need-to-know-before-you-start-an-nlp-project-1890f5bbb793", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/the-theory-you-need-<b>to-know-before-you-start-an</b>-nlp...", "snippet": "<b>Word</b> <b>embedding</b> techniques are used to overcome this problem. Using <b>word</b> embeddings, vocabulary is transformed into vectors in such a way that words with similar context are close by. Word2Vec is a framework from Google that uses shallow neural networks to train <b>word</b> <b>embedding</b> models [3]. There are two types of Word2Vec algorithms: Skip-Gram ...", "dateLastCrawled": "2022-01-23T06:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>word2vec Explained: deriving Mikolov et</b> al.&#39;s negative-sampling <b>word</b> ...", "url": "https://www.researchgate.net/publication/260231515_word2vec_Explained_deriving_Mikolov_et_al's_negative-sampling_word-embedding_method", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/260231515_<b>word2vec_Explained_deriving_Mikolov</b>...", "snippet": "Given a text, <b>each</b> <b>word</b> is embedded in some n-dimensional <b>space</b> based on its contextual similarity to other words (e.g., &quot;prince&quot; and &quot;princess&quot; would be found in a similar region of language ...", "dateLastCrawled": "2022-01-30T21:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Review and classification of variability analysis techniques with ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3224455/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC3224455", "snippet": "The Phase <b>space</b> representation is a transformation mapping a time series into <b>a multidimensional</b> <b>space</b>, <b>where each</b> dimension represents an independent variable describing the system under study. There are several variants of this transformation , however the most famous is extracted from Takens&#39; <b>embedding</b> theorem . Taken&#39;s theorem justifies the transformation of a time series into a m-dimensional multivariate time series. This is done by associating to <b>each</b> m successive samples distant a ...", "dateLastCrawled": "2022-01-26T23:15:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Word</b> Embeddings, Analogies, and <b>Machine</b> <b>Learning</b>: Beyond king - man ...", "url": "https://aclanthology.org/C16-1332.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/C16-1332.pdf", "snippet": "<b>Word</b> Embeddings, Analogies, and <b>Machine</b> <b>Learning</b>: Beyond King - Man + Woman = Queen Aleksandr Drozd y, Anna Gladkova z, Satoshi Matsuoka y yTokyo Institute of Technology, Meguro-ku, Tokyo 152-8550, Japan alex@smg.is.titech.ac.jp, matsu@is.titech.ac.jp z The University of Tokyo, Meguro-ku, Tokyo 153-8902 Japan gladkova@phiz.c.u-tokyo.ac.jp Abstract Solving <b>word</b> analogies became one of the most popular benchmarks for <b>word</b> embeddings on the assumption that linear relations between <b>word</b> pairs ...", "dateLastCrawled": "2022-01-20T00:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Word</b> Embeddings, Analogies, and <b>Machine</b> <b>Learning</b>: Beyond king - man ...", "url": "https://aclanthology.org/C16-1332/", "isFamilyFriendly": true, "displayUrl": "https://acl<b>anthology</b>.org/C16-1332", "snippet": "\ufeff%0 Conference Proceedings %T <b>Word</b> Embeddings, Analogies, and <b>Machine</b> <b>Learning</b>: Beyond king - man + woman = queen %A Drozd, Aleksandr %A Gladkova, Anna %A Matsuoka, Satoshi %S Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers %D 2016 %8 dec %I The COLING 2016 Organizing Committee %C Osaka, Japan %F drozd-etal-2016-<b>word</b> %X Solving <b>word</b> analogies became one of the most popular benchmarks for <b>word</b> embeddings on the assumption that ...", "dateLastCrawled": "2022-01-17T09:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>Word</b> Embeddings, Analogies, and <b>Machine</b> <b>Learning</b>: Beyond King ...", "url": "https://www.researchgate.net/publication/311843169_Word_Embeddings_Analogies_and_Machine_Learning_Beyond_King_-_Man_Woman_Queen", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/311843169_<b>Word</b>_<b>Embeddings</b>_Analogies_and...", "snippet": "<b>Word</b> Embeddings, Analogies, and <b>Machine</b> <b>Learning</b>: Beyond King - Man+ Woman= Queen December 2016 Conference: Proceedings of COLING 2016, the 26th International Conference on Computational ...", "dateLastCrawled": "2021-11-25T14:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Finding the <b>Word</b> <b>Analogy</b> from given words using Word2Vec embeddings ...", "url": "https://www.geeksforgeeks.org/finding-the-word-analogy-from-given-words-using-word2vec-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/finding-the-<b>word</b>-<b>analogy</b>-from-given-<b>words</b>-using-<b>word</b>2vec...", "snippet": "What if we can use a <b>Machine</b> <b>Learning</b> algorithm to automate this task of finding the <b>word</b> <b>analogy</b>. In ... Overview of <b>Word</b> <b>Embedding</b> using Embeddings from Language Models (ELMo) 16, Mar 21. <b>Word</b> Embeddings in NLP. 11, Oct 20. Implement your own word2vec(skip-gram) model in Python. 18, Jan 19. Scraping And Finding Ordered Words In A Dictionary using Python. 23, Jul 17 . Python - Replace all words except the given <b>word</b>. 25, Sep 20. Python | Finding &#39;n&#39; Character Words in a Text File. 15, Oct ...", "dateLastCrawled": "2022-01-26T14:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>What Are Word Embeddings</b> for Text? - <b>Machine</b> <b>Learning</b> Mastery", "url": "https://machinelearningmastery.com/what-are-word-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>what-are-word-embeddings</b>", "snippet": "<b>Word</b> embeddings are a type of <b>word</b> representation that allows words with similar meaning to have a similar representation. They are a distributed representation for text that is perhaps one of the key breakthroughs for the impressive performance of deep <b>learning</b> methods on challenging natural language processing problems. In this post, you will discover the <b>word</b> <b>embedding</b> approach for representing text data. After", "dateLastCrawled": "2022-01-30T18:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "GitHub - jungsoh/<b>word</b>-embeddings-<b>word</b>-<b>analogy</b>-by-document-similarity ...", "url": "https://github.com/jungsoh/word-embeddings-word-analogy-by-document-similarity", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/jungsoh/<b>word</b>-<b>embeddings</b>-<b>word</b>-<b>analogy</b>-by-document-similarity", "snippet": "An example of a <b>word</b> <b>analogy</b> problem is to fill in the blank: Man is to Woman as King is to _____`. Because <b>word</b> embeddings are very computationally expensive to train, most <b>machine</b> <b>learning</b> practitioners will load a pre-trained set of embeddings. We will load a collection of pre-trained embeddings and measure similarity between <b>word</b> embeddings ...", "dateLastCrawled": "2022-01-25T02:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Neural Networks &amp; <b>Word</b> Embeddings | by Nwamaka Imasogie | Nwamaka ...", "url": "https://medium.com/nwamaka-imasogie/neural-networks-word-embeddings-8ec8b3845b2e", "isFamilyFriendly": true, "displayUrl": "https://medium.com/nwamaka-imasogie/neural-networks-<b>word</b>-<b>embeddings</b>-8ec8b3845b2e", "snippet": "All modern NLP techniques use neural networks as a statistical architecture. <b>Word</b> embeddings are mathematical representations of words, sentences and (sometimes) whole documents. Embeddings allow ...", "dateLastCrawled": "2022-01-20T20:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "How to Solve <b>Analogies</b> with Word2Vec | by Khuyen Tran | Towards Data ...", "url": "https://towardsdatascience.com/how-to-solve-analogies-with-word2vec-6ebaf2354009", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/how-to-solve-<b>analogies</b>-with-<b>word</b>2vec-6ebaf2354009", "snippet": "To make words understood by <b>machine</b> <b>learning</b> algorithms, <b>word</b> <b>embedding</b> is used to map words into vectors of real numbers. There are various <b>word</b> <b>embedding</b> models and word2vec is one of them. In simple words, word2vec is a group of related models that are used to produce <b>word</b> embeddings. These models are trained to construct the linguistic contexts of words. Word2vec takes a large corpus of text and produces a vector space, with each unique <b>word</b> in the corpus being assigned to a ...", "dateLastCrawled": "2022-01-30T16:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Word2Vec in Gensim Explained for Creating <b>Word</b> <b>Embedding</b> Models ...", "url": "https://machinelearningknowledge.ai/word2vec-in-gensim-explained-for-creating-word-embedding-models-pretrained-and-custom/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>knowledge.ai/<b>word</b>2vec-in-gensim-explained-for-creating-<b>word</b>...", "snippet": "What is <b>Word</b> Embeddings? <b>Machine</b> <b>learning</b> and ... This is another way putting that word2vec can draw the <b>analogy</b> that if Man is to Woman then Kind is to Queen! The publicly released model of word2vec by Google consists of 300 features and the model is trained in the Google news dataset. The vocabulary size of the model is around 1.6 billion words. However, this might have taken a huge time for the model to be trained on but they have applied a method of simple subsampling approach to ...", "dateLastCrawled": "2022-02-02T15:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Introduction to <b>Word Embeddings</b>. What is a <b>word</b> <b>embedding</b>? | by Hunter ...", "url": "https://towardsdatascience.com/introduction-to-word-embeddings-4cf857b12edc", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/introduction-to-<b>word-embeddings</b>-4cf857b12edc", "snippet": "A very basic definition of a <b>word</b> <b>embedding</b> is a real number, vector representation of a <b>word</b>. Typically, these days, words with similar meaning will have vector representations that are close together in the <b>embedding</b> space (though this hasn\u2019t always been the case). When constructing a <b>word</b> <b>embedding</b> space, typically the goal is to capture some sort of relationship in that space, be it meaning, morphology, context, or some other kind of relationship. By encoding <b>word embeddings</b> in a ...", "dateLastCrawled": "2022-01-30T04:18:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Word Embeddings Explained. What is <b>Word Embedding</b> ? | by Ashwin Prasad ...", "url": "https://medium.com/analytics-vidhya/word-embeddings-explained-62c046f7c79e", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/word-<b>embedding</b>s-explained-62c046f7c79e", "snippet": "<b>Word Embedding</b> is a technique in Natural Language Processing which is used to represent words in a Deep <b>Learning</b> environment. The main advantage of using <b>word embedding</b> is that it allows words of\u2026", "dateLastCrawled": "2022-01-24T11:15:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Survey and challenges of story generation models - A multimodal ...", "url": "https://www.sciencedirect.com/science/article/pii/S156625352030378X", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S156625352030378X", "snippet": "Pang et al. used the deep Bolzmann <b>machine</b> (DBM), which is a joint density model for the visual, auditory, and textual modalities, for <b>learning</b> highly non-linear relations between low-level features across different modalities for emotional prediction. It is trained using joint representation over multimodal inputs; thus, it can handle training samples, which is absent from certain modality. It can be used for emotional prediction and retrieval on any combination of modalities.", "dateLastCrawled": "2022-01-24T04:42:00.0000000Z", "language": "en", "isNavigational": false}], [], []], "all_bing_queries": ["+(word embedding)  is like +(creating a multidimensional space where each word is represented by a point)", "+(word embedding) is similar to +(creating a multidimensional space where each word is represented by a point)", "+(word embedding) can be thought of as +(creating a multidimensional space where each word is represented by a point)", "+(word embedding) can be compared to +(creating a multidimensional space where each word is represented by a point)", "machine learning +(word embedding AND analogy)", "machine learning +(\"word embedding is like\")", "machine learning +(\"word embedding is similar\")", "machine learning +(\"just as word embedding\")", "machine learning +(\"word embedding can be thought of as\")", "machine learning +(\"word embedding can be compared to\")"]}