{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "python - Scikit-learn: How to obtain <b>True Positive</b>, <b>True</b> Negative ...", "url": "https://stackoverflow.com/questions/31324218/scikit-learn-how-to-obtain-true-positive-true-negative-false-positive-and-fal", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/31324218", "snippet": "After this, I would <b>like</b> to obtain the <b>True Positive</b>(<b>TP</b>), <b>True</b> Negative(TN), False <b>Positive</b>(FP) and False Negative(FN) values. I&#39;ll use these parameters to obtain the Sensitivity and Specificity. Finally, I would use this to put in HTML in order to show a chart with the TPs of each label. Code: The variables I have for the moment: trainList #It is a list with all the data of my dataset in JSON form labelList #It is a list with all the labels of my data Most part of the method: #I transform ...", "dateLastCrawled": "2022-01-26T12:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "The <b>Confusion Matrix</b>: <b>Getting</b> the TPR, TNR, FPR ... - Towards Data Science", "url": "https://towardsdatascience.com/multi-class-classification-extracting-performance-metrics-from-the-confusion-matrix-b379b427a872", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/multi-class-classification-extracting-performance...", "snippet": "So the output of the <b>test</b> can be either <b>Positive</b> (affected) or Negative (not affected ). So, in this hypothetical case we have a binary classification case. Handmade sketch made by the author. An example of 2 populations, one affected by covid-19 and the other not affected, assuming that we really know the ground truth. Additionally, based on the output of the <b>test</b>, we can denote a person as affected (blue population) or not affected (red population). <b>True</b> Positives (<b>TP</b>, blue distribution ...", "dateLastCrawled": "2022-02-02T22:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Understanding and using sensitivity, specificity and predictive values", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2636062/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC2636062", "snippet": "= a (<b>true</b> <b>positive</b>) / a+b (<b>true</b> <b>positive</b> + false <b>positive</b>) = Probability (patient having disease when <b>test</b> is <b>positive</b>) Example: We will use sensitivity and specificity provided in Table 3 to calculate <b>positive</b> predictive value. PPV = a (<b>true</b> <b>positive</b>) / a+b (<b>true</b> <b>positive</b> + false <b>positive</b>) = 75 / 75 + 15 = 75 / 90 = 83.3%", "dateLastCrawled": "2022-01-30T18:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "What is a <b>confusion matrix</b>?. Everything you Should Know about\u2026 | by ...", "url": "https://medium.com/analytics-vidhya/what-is-a-confusion-matrix-d1c0f8feda5", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/what-is-a-<b>confusion-matrix</b>-d1c0f8feda5", "snippet": "<b>True</b> <b>Positive</b> (<b>TP</b>) = 6. You predicted <b>positive</b> and it\u2019s <b>true</b>. You predicted that an animal is a cat and it actually is. <b>True</b> Negative (TN) = 11. You predicted negative and it\u2019s <b>true</b>. You ...", "dateLastCrawled": "2022-02-03T01:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Confusion Matrix</b> - Get Items FP/FN/<b>TP</b>/TN - Python - Data Science Stack ...", "url": "https://datascience.stackexchange.com/questions/28493/confusion-matrix-get-items-fp-fn-tp-tn-python", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/28493", "snippet": "And use it <b>like</b> this. print_<b>confusion_matrix</b>(x_<b>test</b>, x_pred) Alternatively, if you want the values return and not only printed you can do it <b>like</b> this: def get_<b>confusion_matrix</b>_values(y_<b>true</b>, y_pred): cm = <b>confusion_matrix</b>(y_<b>true</b>, y_pred) return(cm[0][0], cm[0][1], cm[1][0], cm[1][1]) <b>TP</b>, FP, FN, TN = get_<b>confusion_matrix</b>_values(x_<b>test</b>, x_pred) Share. Improve this answer. Follow answered Mar 2 &#39;18 at 2:16. Shaido Shaido. 632 5 5 silver badges 12 12 bronze badges $\\endgroup$ Add a comment | 2 ...", "dateLastCrawled": "2022-01-27T02:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "ROC <b>Curve &amp; AUC Explained with Python Examples</b> - Data ... - Data Analytics", "url": "https://vitalflux.com/roc-curve-auc-python-false-positive-true-positive-rate/", "isFamilyFriendly": true, "displayUrl": "https://vitalflux.com/roc-curve-auc-python-false-<b>positive</b>-<b>true</b>-<b>positive</b>-rate", "snippet": "<b>True</b> <b>Positive</b> Rate (TPR) = <b>True</b> <b>Positive</b> (<b>TP</b>) / (<b>TP</b> + FN) = <b>TP</b> / Positives. False <b>Positive</b> Rate (FPR) = False <b>Positive</b> (FP) / (FP + TN) = FP / Negatives. Higher value of TPR would mean that the value of false negative is very low which would mean almost all positives are predicted correctly. Lower value of FPR would mean that the value of false <b>positive</b> is very low which means almost all negatives are predicted correctly. Going by the above, the decision threshold near top left of ROC curve ...", "dateLastCrawled": "2022-02-02T06:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Model Evaluation Metrics in Machine Learning - KDnuggets", "url": "https://www.kdnuggets.com/2020/05/model-evaluation-metrics-machine-learning.html", "isFamilyFriendly": true, "displayUrl": "https://www.kdnuggets.com/2020/05/model-evaluation-metrics-machine-learning.html", "snippet": "<b>True</b> <b>Positive</b> Rate = <b>TP</b>/actual yes Recall gives us the <b>true</b> <b>positive</b> rate (TPR), which is the ratio of <b>true</b> positives to everything <b>positive</b>. In the case of the 99/1 split between classes A and B, the model that classifies everything as A would have a recall of 0% for the <b>positive</b> class, B (precision would be undefined \u2014 0/0). Precision and ...", "dateLastCrawled": "2022-01-29T20:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Confusion Matrix in Machine Learning</b> - <b>GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/confusion-matrix-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/confusion-matrix-machine-learning", "snippet": "precision = (<b>TP</b>) / (<b>TP</b>+FP) <b>TP</b> is the number of <b>true</b> positives, and FP is the number of false positives. A trivial way to have <b>perfect</b> precision is to make one single <b>positive</b> prediction and ensure it is correct (precision = 1/1 = 100%). This would not be very useful since the classifier would ignore all but one <b>positive</b> instance. Recall recall = (<b>TP</b>) / (<b>TP</b>+FN) Python3 # Finding precision and recall. from sklearn.metrics import precision_<b>score</b>, recall_<b>score</b>. precision_<b>score</b>(y_train_5, y_train ...", "dateLastCrawled": "2022-02-02T19:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Confusion Matrix</b> in <b>Machine Learning</b> with EXAMPLE", "url": "https://www.guru99.com/confusion-matrix-machine-learning-example.html", "isFamilyFriendly": true, "displayUrl": "https://www.guru99.com/<b>confusion-matrix</b>-<b>machine-learning</b>-example.html", "snippet": "F <b>Score</b>: F1 <b>score</b> is a weighted average <b>score</b> of the <b>true</b> <b>positive</b> (recall) and precision. Roc Curve: Roc curve shows the <b>true</b> <b>positive</b> rates against the false <b>positive</b> rate at various cut points. It also demonstrates a trade-off between sensitivity (recall and specificity or the <b>true</b> negative rate).", "dateLastCrawled": "2022-02-02T20:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "analytics - How do I calculate <b>true positive</b>, <b>true</b> negative, false ...", "url": "https://stackoverflow.com/questions/33492346/how-do-i-calculate-true-positive-true-negative-false-positive-and-false-negat", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/33492346", "snippet": "The terms &quot;<b>positive</b>&quot; and &quot;negative&quot; only make sense with a binomial classifier -- a <b>true positive</b> is when you get a correct &quot;yes, this belongs here&quot; and a <b>true</b> negative when you correctly get &quot;no, this doesn&#39;t belong to the category&quot;. So there can only be two categories, or actually just one, and its complement. Everything which doesn&#39;t belong in the category is a negative. Then the confusion matrix looks <b>like</b> this:", "dateLastCrawled": "2022-01-27T12:46:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Understanding and using sensitivity, specificity and predictive values", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2636062/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC2636062", "snippet": "= a (<b>true</b> <b>positive</b>) / a+c (<b>true</b> <b>positive</b> + false negative) ... Since our new <b>test</b> is 90% sensitive, the <b>test</b> will detect 9,000 (<b>TP</b>) people who are actually affected with PACG and miss 1,000 (FN). Looking at those numbers, we would think that our <b>test</b> is very good because we have detected 9,000 out of 10,000 PACG-affected people. However, of the original 1 million, 990,000 are not affected. If we look at the <b>test</b> results on the normal population (remember, the specificity of the <b>test</b> is 95% ...", "dateLastCrawled": "2022-01-30T18:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Confusion Matrix in Machine Learning</b> - <b>GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/confusion-matrix-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/confusion-matrix-machine-learning", "snippet": "precision = (<b>TP</b>) / (<b>TP</b>+FP) <b>TP</b> is the number of <b>true</b> positives, and FP is the number of false positives. A trivial way to have <b>perfect</b> precision is to make one single <b>positive</b> prediction and ensure it is correct (precision = 1/1 = 100%). This would not be very useful since the classifier would ignore all but one <b>positive</b> instance. Recall recall = (<b>TP</b>) / (<b>TP</b>+FN) Python3 # Finding precision and recall. from sklearn.metrics import precision_<b>score</b>, recall_<b>score</b>. precision_<b>score</b>(y_train_5, y_train ...", "dateLastCrawled": "2022-02-02T19:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "python - Scikit-learn: How to obtain <b>True Positive</b>, <b>True</b> Negative ...", "url": "https://stackoverflow.com/questions/31324218/scikit-learn-how-to-obtain-true-positive-true-negative-false-positive-and-fal", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/31324218", "snippet": "After this, I would like to obtain the <b>True Positive</b>(<b>TP</b>), <b>True</b> Negative(TN), False <b>Positive</b>(FP) and False Negative(FN) values. I&#39;ll use these parameters to obtain the Sensitivity and Specificity. Finally, I would use this to put in HTML in order to show a chart with the TPs of each label. Code: The variables I have for the moment: trainList #It is a list with all the data of my dataset in JSON form labelList #It is a list with all the labels of my data Most part of the method: #I transform ...", "dateLastCrawled": "2022-01-26T12:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Accuracy, Precision, Recall &amp; F1-Score - Python</b> Examples - <b>Data Analytics</b>", "url": "https://vitalflux.com/accuracy-precision-recall-f1-score-python-example/", "isFamilyFriendly": true, "displayUrl": "https://vitalflux.com/<b>accuracy-precision-recall-f1-score-python</b>-example", "snippet": "The following confusion matrix is printed:. Fig 1. Confusion Matrix representing predictions vs Actuals on <b>Test</b> Data. The predicted data results in the above diagram could be read in the following manner given 1 represents malignant cancer (<b>positive</b>).. <b>True</b> <b>Positive</b> (<b>TP</b>): <b>True</b> <b>positive</b> represents the value of correct predictions of positives out of actual <b>positive</b> cases.Out of 107 actual <b>positive</b>, 104 is correctly predicted <b>positive</b>.", "dateLastCrawled": "2022-02-02T22:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Confusion matrix</b> and other metrics in machine learning | by Hugo ...", "url": "https://medium.com/hugo-ferreiras-blog/confusion-matrix-and-other-metrics-in-machine-learning-894688cb1c0a", "isFamilyFriendly": true, "displayUrl": "https://medium.com/hugo-ferreiras-blog/<b>confusion-matrix</b>-and-other-metrics-in-machine...", "snippet": "<b>True</b> positives (<b>TP</b>): ... from sklearn.metrics import roc_auc_<b>score</b> roc_auc_<b>score</b>(y_<b>test</b>, y_pred_prob) The output for our classifier is: 0.9977033760372253. The AUC is indeed quite close to 1, and ...", "dateLastCrawled": "2022-01-14T03:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "How to Best Evaluate a <b>Classification</b> Model | by ... - Towards Data Science", "url": "https://towardsdatascience.com/how-to-best-evaluate-a-classification-model-2edb12bcc587", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/how-to-best-evaluate-a-<b>classification</b>-model-2edb12bcc587", "snippet": "The best value for f1 <b>score</b> is 1 and the worst is 0. Sensitivity and Specificity. Sensitivity, also known as the <b>true</b> <b>positive</b> rate (TPR), is the same as recall. Hence, it measures the proportion of <b>positive</b> class that is correctly predicted as <b>positive</b>. Specificity <b>is similar</b> to sensitivity but focused on negative class. It measures the ...", "dateLastCrawled": "2022-02-03T02:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Assessing the performance of prediction models: a framework for some ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3575184/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC3575184", "snippet": "This scaled Brier <b>score</b> happens to be very <b>similar</b> to Pearson\u2019s R 2 statistic 35. Calculation of the Brier <b>score</b> for survival outcomes is possible with a weight function, which considers the conditional probability of being uncensored during time 36,37,3. We can then calculate the Brier <b>score</b> at fixed time points, and create a time-dependent curve. It is useful to use a benchmark curve, based on the Brier <b>score</b> for the overall Kaplan-Meier estimator, which does not consider any predictive ...", "dateLastCrawled": "2022-02-03T05:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Confusion Matrix</b>, Accuracy, Precision, Recall, F1 <b>Score</b> | by ...", "url": "https://medium.com/analytics-vidhya/confusion-matrix-accuracy-precision-recall-f1-score-ade299cf63cd", "isFamilyFriendly": true, "displayUrl": "https://medium.com/.../<b>confusion-matrix</b>-accuracy-precision-recall-f1-<b>score</b>-ade299cf63cd", "snippet": "Let us consider a task to classify whether a person is pregnant or not pregnant. If the <b>test</b> for pregnancy is <b>positive</b> (+ve ), then the person is pregnant. On the other hand, if the <b>test</b> for\u2026", "dateLastCrawled": "2022-02-02T15:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "How to Calculate Precision, Recall, and F-Measure for Imbalanced ...", "url": "https://machinelearningmastery.com/precision-recall-and-f-measure-for-imbalanced-classification/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/precision-recall-and-f-measure-for-", "snippet": "We can also use the recall_<b>score</b>() for imbalanced multiclass classification problems. In this case, the dataset has a 1:1:100 imbalance, with 100 in each minority class and 10,000 in the majority class. A model predicts 77 <b>true</b> positives and 23 false negatives for class 1 and 95 <b>true</b> positives and five false negatives for class 2.", "dateLastCrawled": "2022-02-03T02:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "3.3. Metrics and scoring: quantifying the quality of predictions ...", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "isFamilyFriendly": true, "displayUrl": "https://scikit-learn.org/stable/modules/model_evaluation.html", "snippet": "The second use case is to build a completely custom scorer object from a simple python function using make_scorer, which can take several parameters:. the python function you want to use (my_custom_loss_func in the example below)whether the python function returns a <b>score</b> (greater_is_better=<b>True</b>, the default) or a loss (greater_is_better=False).If a loss, the output of the python function is negated by the scorer object, conforming to the cross validation convention that scorers return ...", "dateLastCrawled": "2022-02-03T06:06:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>ROC Curve</b>, a Complete Introduction | by Reza Bagheri | Towards Data Science", "url": "https://towardsdatascience.com/roc-curve-a-complete-introduction-2f2da2e0434c", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>roc-curve</b>-a-complete-introduction-2f2da2e0434c", "snippet": "<b>True</b> <b>Positive</b> (<b>TP</b>): Here the classifier predicts or labels a <b>positive</b> item as <b>positive</b> which is a correct prediction ... and it <b>can</b> <b>be thought</b> of as a random classifier with p=0. Point C is a classifier that predicts everything as <b>positive</b>, and it is a random classifier with p=1. Both TPR and FPR range from 0 to 1 and all these points lie on the diagonal line. By changing the selection probability, you <b>can</b> change the position of the random classifier along the diagonal line. We <b>can</b> now ...", "dateLastCrawled": "2022-01-30T19:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Classification Accuracy is Not Enough: More Performance Measures You ...", "url": "https://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/classification-accuracy-is-not-enough-", "snippet": "It is also called Sensitivity or the <b>True</b> <b>Positive</b> Rate. Recall <b>can</b> <b>be thought</b> of as a measure of a classifiers completeness. A low recall indicates many False Negatives. The recall of the All No Recurrence model is 0/(0+85) or 0. The recall of the All Recurrence model is 85/(85+0) or 1. The recall of CART is 10/(10+75) or 0.12. As you would expect, the All Recurrence model has <b>a perfect</b> recall because it predicts \u201crecurrence\u201d for all instances. The recall for CART is lower than that of ...", "dateLastCrawled": "2022-02-03T05:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "python - Scikit-learn: How to obtain <b>True Positive</b>, <b>True</b> Negative ...", "url": "https://stackoverflow.com/questions/31324218/scikit-learn-how-to-obtain-true-positive-true-negative-false-positive-and-fal", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/31324218", "snippet": "After this, I would like to obtain the <b>True Positive</b>(<b>TP</b>), <b>True</b> Negative(TN), False <b>Positive</b>(FP) and False Negative(FN) values. I&#39;ll use these parameters to obtain the Sensitivity and Specificity. Finally, I would use this to put in HTML in order to show a chart with the TPs of each label. Code: The variables I have for the moment: trainList #It is a list with all the data of my dataset in JSON form labelList #It is a list with all the labels of my data Most part of the method: #I transform ...", "dateLastCrawled": "2022-01-26T12:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Recall And Precision</b>: A Comprehensive Guide For 2021", "url": "https://www.jigsawacademy.com/blogs/ai-ml/recall-and-precision/", "isFamilyFriendly": true, "displayUrl": "https://www.jigsawacademy.com/blogs/ai-ml/<b>recall-and-precision</b>", "snippet": "Accuracy = <b>True</b> <b>Positive</b> (<b>TP</b>) <b>True</b> Negatives (TN)/ <b>True</b> Positives (<b>TP</b>) False Positives (FP) <b>True</b> Negative (TN) False Negative (FN) A high accuracy value is indicative of an efficient model, and it is best for symmetric models. It is very easy to differentiate between accuracy and prediction when we compare the various models. Accuracy has some shortcomings, such as; this model is not very efficient for datasets containing two or more data classes as they might be neglected. Furthermore, if a ...", "dateLastCrawled": "2022-01-31T00:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Accurate diagnosis of prostate cancer using logistic regression", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8005780/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8005780", "snippet": "The LGR is a group of statistical techniques that aim to <b>test</b> hypotheses or causal relationships when the dependent variable is nominal. Despite its name, it is not an algorithm applied in regression problems, in which continuous values are dealt with, but it is a method for classification problems, in which a binary value, i.e., either 0 or 1 is obtained. For example, a classification problem is to identify if a given tumor is malignant or benign. With the LGR, the relationship between the ...", "dateLastCrawled": "2021-10-29T14:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "machine learning - Reason of having high <b>AUC</b> and low accuracy in a ...", "url": "https://stackoverflow.com/questions/38387913/reason-of-having-high-auc-and-low-accuracy-in-a-balanced-dataset", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/38387913", "snippet": "The area under the resulting ROC curve is called <b>AUC</b>. It measures for your training/<b>test</b> data, how well the classifier <b>can</b> discriminate between samples from the &quot;<b>positive</b>&quot; and the &quot;negative&quot; class. <b>A perfect</b> classifier&#39;s ROC curve would pass through the optimal point FPR ( t*) = 0 and TPR ( t*) = 1 and would yield an <b>AUC</b> of 1.", "dateLastCrawled": "2022-01-22T05:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>GitHub</b> - <b>iamtodor/data-science-interview-questions-and-answers</b>: Data ...", "url": "https://github.com/iamtodor/data-science-interview-questions-and-answers", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/<b>iamtodor/data-science-interview-questions-and-answers</b>", "snippet": "Statistical power or sensitivity of a binary hypothesis <b>test</b> is the probability that the <b>test</b> correctly rejects the null hypothesis (H0) when the alternative hypothesis (H1) is <b>true</b>. It <b>can</b> be equivalently <b>thought</b> of as the probability of accepting the alternative hypothesis (H1) when it is <b>true</b>\u2014that is, the ability of a <b>test</b> to detect an effect, if the effect actually exists.", "dateLastCrawled": "2022-02-03T02:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Accuracy, <b>Precision, Recall</b>, F1 <b>Score</b> and ROC curve \u2013 K\u0131van\u00e7 Y\u00fcksel ...", "url": "https://emkademy.com/research/toolbox/2020-03-02-accuracy-precision-recall", "isFamilyFriendly": true, "displayUrl": "https://emkademy.com/research/toolbox/2020-03-02-accuracy-<b>precision-recall</b>", "snippet": "Now we are <b>getting</b> somewhere. So, based on precision, our model is not that good after all\u2026 But, when to use precision? If the number of False Positives are crucial to you, then you should use precision. For example, if your model predicts whether an email is a spam, you would be very concerned with the number of False Positives you have. Because a False <b>Positive</b> would mean that an email is spam, and maybe a user would miss a very important email because of that decision.", "dateLastCrawled": "2022-02-02T16:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Comparing the performance of different machine learning</b> ... - Dibyendu Deb", "url": "https://dibyendudeb.com/comparing-machine-learning-algorithms/", "isFamilyFriendly": true, "displayUrl": "https://dibyendudeb.com/comparing-machine-learning-algorithms", "snippet": "Receiver Operating Characteristic (ROC) curve is a very important tool to diagnose the performance of MLAs by plotting the <b>true</b> <b>positive</b> rates against the false-<b>positive</b> rates at different threshold levels. The area under ROC curve often called AUC and it is also a good measure of the predictability of the machine learning algorithms. A higher AUC is an indication of more accurate prediction.", "dateLastCrawled": "2022-02-03T07:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How to Get <b>a Perfect</b> 1600 SAT <b>Score</b>, by a 2400 Expert Full Scorer", "url": "https://blog.prepscholar.com/how-to-get-a-perfect-sat-score-by-a-2400-sat-scorer", "isFamilyFriendly": true, "displayUrl": "https://blog.prepscholar.com/how-to-get-<b>a-perfect</b>-sat-<b>score</b>-by-a-2400-sat-<b>score</b>r", "snippet": "The maximum <b>score</b> on the SAT is a 1600. Out of the two million students who take the <b>test</b> every year, only about 500 get the highest possible SAT <b>score</b>. This elusive <b>perfect</b> <b>score</b> catapults you to the top of high school academic achievement and <b>can</b> be a big boost to your college applications. I scored <b>perfect</b> scores on the SAT.", "dateLastCrawled": "2022-02-02T11:12:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Model Evaluation Metrics in Machine Learning - KDnuggets", "url": "https://www.kdnuggets.com/2020/05/model-evaluation-metrics-machine-learning.html", "isFamilyFriendly": true, "displayUrl": "https://www.kdnuggets.com/2020/05/model-evaluation-metrics-machine-learning.html", "snippet": "<b>True</b> <b>Positive</b> (<b>TP</b>): Predicted <b>True</b> and ... The F1 <b>score</b> is the harmonic mean of the precision and recall, where an F1 <b>score</b> reaches its best value at 1 (<b>perfect</b> precision and recall) and worst at 0. Why harmonic mean? Since the harmonic mean of a list of numbers skews strongly toward the least elements of the list, it tends (<b>compared</b> to the arithmetic mean) to mitigate the impact of large outliers and aggravate the impact of small ones. An F1 <b>score</b> punishes extreme values more. Ideally, an ...", "dateLastCrawled": "2022-01-29T20:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "What is a <b>confusion matrix</b>?. Everything you Should Know about\u2026 | by ...", "url": "https://medium.com/analytics-vidhya/what-is-a-confusion-matrix-d1c0f8feda5", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/what-is-a-<b>confusion-matrix</b>-d1c0f8feda5", "snippet": "<b>True</b> <b>Positive</b> (<b>TP</b>) = 6. You predicted <b>positive</b> and it\u2019s <b>true</b>. You predicted that an animal is a cat and it actually is. <b>True</b> Negative (TN) = 11. You predicted negative and it\u2019s <b>true</b>. You ...", "dateLastCrawled": "2022-02-03T01:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Confusion Matrix in Machine Learning</b> - <b>GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/confusion-matrix-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/confusion-matrix-machine-learning", "snippet": "precision = (<b>TP</b>) / (<b>TP</b>+FP) <b>TP</b> is the number of <b>true</b> positives, and FP is the number of false positives. A trivial way to have <b>perfect</b> precision is to make one single <b>positive</b> prediction and ensure it is correct (precision = 1/1 = 100%). This would not be very useful since the classifier would ignore all but one <b>positive</b> instance. Recall recall = (<b>TP</b>) / (<b>TP</b>+FN) Python3 # Finding precision and recall. from sklearn.metrics import precision_<b>score</b>, recall_<b>score</b>. precision_<b>score</b>(y_train_5, y_train ...", "dateLastCrawled": "2022-02-02T19:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Confusion Matrix Calculator</b> - MDApp", "url": "https://www.mdapp.co/confusion-matrix-calculator-406/", "isFamilyFriendly": true, "displayUrl": "https://www.mdapp.co/<b>confusion-matrix-calculator</b>-406", "snippet": "The confusion matrix is the popular representation of the performance of classification models and includes the correctly and incorrectly classified values <b>compared</b> to the actual outcomes in the <b>test</b> data. The four variables are: <b>True</b> <b>positive</b> (<b>TP</b>) \u2013 which is the outcome where the model correctly predicts <b>positive</b> class (condition is ...", "dateLastCrawled": "2022-02-03T06:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Understanding and using sensitivity, specificity and predictive values", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2636062/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC2636062", "snippet": "If the prevalence (also known as the pre-<b>test</b> probability in this situation) of the disease is low, such as with glaucoma or sight-threatening diabetic retinopathy in the general population, the number of false-<b>positive</b> results will be far higher than the number of <b>true</b>-<b>positive</b> results.3 This leads to a number of problems, including labeling of normal as abnormal resulting in unnecessary treatment.", "dateLastCrawled": "2022-01-30T18:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Assessing the performance of prediction models: a framework for some ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3575184/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC3575184", "snippet": "The Brier <b>score</b> for a model <b>can</b> range from 0 for <b>a perfect</b> model to 0.25 for a non-informative model with a 50% incidence of the outcome. When the outcome incidence is lower, the maximum <b>score</b> for a non-informative model is lower, e.g. for 10%: 0.1*(1\u20130.1) 2 + (1\u20130.1)*0.1 2 =0.090. Similar to Nagelkerke\u2019s approach to the LR statistic, we could scale Brier by its maximum <b>score</b> under a non-informative model: Brier scaled = 1 \u2013 Brier / Brier max, where Brier max = mean(p)*(1 \u2013 mean(p ...", "dateLastCrawled": "2022-02-03T05:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Confusion Matrix</b> in <b>Machine Learning</b> with EXAMPLE", "url": "https://www.guru99.com/confusion-matrix-machine-learning-example.html", "isFamilyFriendly": true, "displayUrl": "https://www.guru99.com/<b>confusion-matrix</b>-<b>machine-learning</b>-example.html", "snippet": "You <b>can</b> consider it as a baseline metric to compare your classifier. F <b>Score</b>: F1 <b>score</b> is a weighted average <b>score</b> of the <b>true</b> <b>positive</b> (recall) and precision. Roc Curve: Roc curve shows the <b>true</b> <b>positive</b> rates against the false <b>positive</b> rate at various cut points. It also demonstrates a trade-off between sensitivity (recall and specificity or ...", "dateLastCrawled": "2022-02-02T20:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Machine learning: Evaluation metrics</b> | ML Cheat Sheet", "url": "https://medium.com/ml-cheat-sheet/machine-learning-evaluation-metrics-b89b8832e275", "isFamilyFriendly": true, "displayUrl": "https://medium.com/ml-cheat-sheet/<b>machine-learning-evaluation-metrics</b>-b89b8832e275", "snippet": "As we <b>can</b> see, from 1003 samples, we have 1002 samples correctly classified. The accuracy here is equal to 0,999.hallelujah, you have done a great job! (Now we <b>can</b> celebrate it with some champagne ...", "dateLastCrawled": "2022-02-02T23:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "3.3. Metrics and scoring: quantifying the quality of predictions ...", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "isFamilyFriendly": true, "displayUrl": "https://scikit-learn.org/stable/modules/model_evaluation.html", "snippet": "The second use case is to build a completely custom scorer object from a simple python function using make_scorer, which <b>can</b> take several parameters:. the python function you want to use (my_custom_loss_func in the example below)whether the python function returns a <b>score</b> (greater_is_better=<b>True</b>, the default) or a loss (greater_is_better=False).If a loss, the output of the python function is negated by the scorer object, conforming to the cross validation convention that scorers return ...", "dateLastCrawled": "2022-02-03T06:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>How to</b> Get 800 on SAT <b>Math</b>, by <b>a Perfect</b> Scorer", "url": "https://blog.prepscholar.com/how-to-get-800-on-sat-math-by-a-perfect-scorer", "isFamilyFriendly": true, "displayUrl": "https://blog.prepscholar.com/<b>how-to</b>-get-800-on-sat-<b>math</b>-by-<b>a-perfect</b>-<b>score</b>r", "snippet": "On every practice <b>test</b>, you need to aim for <b>a perfect</b> raw <b>score</b> for an 800. Whatever you&#39;re scoring now, take note of the difference you need to get to a 800. For example, if you&#39;re scoring a 700 now, you need to answer 8-9 more questions right to get to an 800. As a final example, here&#39;s a screenshot from my exact <b>score</b> report showing that I missed 0 questions and earned an 800. (This was from the previous 2400 version of the SAT.) OK\u2014so we&#39;ve covered why scoring a higher SAT <b>math</b> <b>score</b> ...", "dateLastCrawled": "2022-01-25T23:23:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Confusion Matrix in <b>Machine</b> <b>Learning</b> \u2013 Naukri <b>Learning</b>", "url": "https://www.naukri.com/learning/articles/confusion-matrix-in-machine-learning-naukri-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.naukri.com/<b>learning</b>/articles/confusion-matrix-in-<b>machine</b>-<b>learning</b>-naukri...", "snippet": "Let\u2019s understand <b>TP</b>, FP, FN, TN in terms of Coronavirus affected people <b>analogy</b>. <b>True</b> <b>Positive</b>: Interpretation: You predicted <b>positive</b> and it\u2019s <b>true</b>. You predicted that a person is Corona <b>positive</b> and he actually is having Corona. <b>True</b> Negative: Interpretation: You predicted negative and it\u2019s <b>true</b>.", "dateLastCrawled": "2022-02-07T08:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Understanding Confusion Matrix in <b>Machine</b> <b>Learning</b>, Let\u2019s Study ...", "url": "https://ulimazzadaislamy.medium.com/understanding-confusion-matrix-in-machine-learning-lets-study-together-7521090aaaf2", "isFamilyFriendly": true, "displayUrl": "https://ulimazzadaislamy.medium.com/understanding-confusion-matrix-in-<b>machine</b>-<b>learning</b>...", "snippet": "Let\u2019s understand <b>TP</b>, FP, FN, TN in terms of pregnancy <b>analogy</b>: The <b>analogy</b> from the picture: <b>True</b> <b>Positive</b>: Interpretation: You predicted <b>positive</b> and it\u2019s <b>true</b>. You predicted that a woman is pregnant and she actually is. <b>True</b> Negative: Interpretation: You predicted negative and it\u2019s <b>true</b>. You predicted that a man is not pregnant and he actually is not. False <b>Positive</b>: (Type 1 Error) Interpretation: You predicted <b>positive</b> and it\u2019s false. You predicted that a man is pregnant but he ...", "dateLastCrawled": "2022-01-13T10:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Machine Learning Accuracy</b>: <b>True</b> vs. False <b>Positive</b>/Negative", "url": "https://research.aimultiple.com/machine-learning-accuracy/", "isFamilyFriendly": true, "displayUrl": "https://research.aimultiple.com/<b>machine-learning-accuracy</b>", "snippet": "There are various theoretical approaches to measuring accuracy* of competing <b>machine</b> <b>learning</b> models however, in most commercial applications, you simply need to assign a business value to 4 types of results: <b>true</b> positives, <b>true</b> negatives, false positives and false negatives.By multiplying number of results in each bucket with the associated business values, you will ensure that you use the best model available.", "dateLastCrawled": "2022-02-03T04:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Evaluating a <b>Machine</b> <b>Learning</b> Model: Regression and Classification ...", "url": "https://arnavbansal-8232.medium.com/evaluating-a-machine-learning-model-regression-and-classification-metrics-4f2316e180b4", "isFamilyFriendly": true, "displayUrl": "https://arnavbansal-8232.medium.com/evaluating-a-<b>machine</b>-<b>learning</b>-model-regression-and...", "snippet": "<b>Machine</b> <b>Learning</b> and Deep <b>Learning</b> plays a vital role in all this. ... <b>True</b> <b>positive</b> (<b>TP</b>) \u2014 actual = 1; predicted = 1 (11 = 1) False <b>positive</b> (FP) \u2014 actual = 0; predicted = 1 (01 = 0) False negative (FN) \u2014 actual = 1; predicted = 0 (10 = 0) <b>True</b> negative (TN) \u2014 actual = 0; predicted = 0 (00 = 1) \u201cXNOR gate\u201d produces this kind of output. Our objective is to train the model, so that our algorithm predicts the same as the <b>true</b> output. Hence, our algorithm should produce more outputs ...", "dateLastCrawled": "2022-01-05T00:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "What is the best example for <b>false negative, false positive, true</b> ...", "url": "https://www.quora.com/What-is-the-best-example-for-false-negative-false-positive-true-negative-and-true-positive-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-the-best-example-for-<b>false-negative-false-positive-true</b>...", "snippet": "Answer (1 of 6): There was a funny picture I\u2019d come across a while ago [1]: Extending this example, a man whose test results say \u201cNot pregnant\u201d is <b>True</b> Negative, and a pregnant woman whose test results say \u201cPregnant\u201d is <b>True</b> <b>Positive</b>. A good way to understand it is this: * First, define what...", "dateLastCrawled": "2022-01-22T02:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "ROAL OF CONFUSION MATRIX IN CYBER CRIME | by HM | Medium", "url": "https://h17.medium.com/roal-of-confusion-matrix-in-cyber-crime-41e7c9c3f57b", "isFamilyFriendly": true, "displayUrl": "https://h17.medium.com/roal-of-confusion-matrix-in-cyber-crime-41e7c9c3f57b", "snippet": "Let\u2019s understand <b>TP</b>, FP, FN, TN in terms of pregnancy <b>analogy</b>. <b>True</b> <b>Positive</b>: Interpretation: You predicted <b>positive</b> and it\u2019s <b>true</b>. You predicted that a woman is pregnant and she actually is. <b>True</b> Negative: Interpretation: You predicted negative and it\u2019s <b>true</b>. You predi c ted that a man is not pregnant and he actually is not. False <b>Positive</b>: (Type 1 Error) Interpretation: You predicted <b>positive</b> and it\u2019s false. You predicted that a man is pregnant but he actually is not. False ...", "dateLastCrawled": "2022-01-10T21:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and-sample-codes ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/README.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "Model 1: accuracy = 92%. Model 2: accuracy = 95%. Model 2 has higher accuracy than model 1, but model 2 is useless. This is called accuracy paradox, which means the model with higher accuracy may not have better generalization power. In general, when <b>TP</b> &lt; FP, the accuracy will always increase when we change the classifier to always output &#39;negative&#39;.Conversely, when TN &lt; FN, the same will happen when we change the classifier to always output &#39;<b>positive</b>&#39; [1].. Recall@k", "dateLastCrawled": "2021-09-22T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Insurance claims \u2014 Fraud detection using <b>machine</b> <b>learning</b> | by Punith ...", "url": "https://medium.com/geekculture/insurance-claims-fraud-detection-using-machine-learning-78f04913097", "isFamilyFriendly": true, "displayUrl": "https://medium.com/geekculture/insurance-claims-fraud-detection-using-<b>machine</b>-<b>learning</b>...", "snippet": "Here our model predicts 196 <b>true</b> <b>positive</b> cases out of 218 <b>positive</b> cases and 190 <b>true</b> negative cases out of 234 cases. It predicts 22 false <b>positive</b> cases out of 218 <b>positive</b> cases and 44 false ...", "dateLastCrawled": "2022-02-03T04:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>machine</b> <b>learning</b> - Why is ROC insensitive to class distributions ...", "url": "https://stats.stackexchange.com/questions/545273/why-is-roc-insensitive-to-class-distributions", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/545273/why-is-roc-insensitive-to-class...", "snippet": "Basically, ROC curve shows false <b>positive</b> (FP) RATE and <b>true</b> <b>positive</b> (<b>TP</b>) RATE for each threshold of the model (score you decided as being the limit between classification &#39;1&#39; and &#39;0&#39;). So at the start, if your threshold is 1 (max possible score for your model), you classify everything as 0 and then there&#39;s 0% FP and 0% <b>TP</b>. If threshold is 0 (min possible score for your model), everything is classified as 1 and so your <b>TP</b> and FP rates are 100%. Using a threshold strictly between 0 and 1 ...", "dateLastCrawled": "2022-01-29T03:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>machine</b> <b>learning</b> - How to calculate precision and recall in a 3 x 3 ...", "url": "https://stats.stackexchange.com/questions/91044/how-to-calculate-precision-and-recall-in-a-3-x-3-confusion-matrix", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/91044/how-to-calculate-precision-and-recall...", "snippet": "We know Precision = <b>TP</b>/(<b>TP</b>+FP), so for Pa <b>true</b> <b>positive</b> will be Actual A predicted as A, i.e., 10, rest of the two cells in that column, whether it is B or C, make False <b>Positive</b>. So. Pa = 10/18 = 0.55 Ra = 10/17 = 0.59. Now precision and recall for class B are Pb and Rb. For class B, <b>true</b> <b>positive</b> is actual B predicted as B, that is the cell ...", "dateLastCrawled": "2022-01-30T05:56:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(true positive (tp))  is like +(getting a perfect score on a test)", "+(true positive (tp)) is similar to +(getting a perfect score on a test)", "+(true positive (tp)) can be thought of as +(getting a perfect score on a test)", "+(true positive (tp)) can be compared to +(getting a perfect score on a test)", "machine learning +(true positive (tp) AND analogy)", "machine learning +(\"true positive (tp) is like\")", "machine learning +(\"true positive (tp) is similar\")", "machine learning +(\"just as true positive (tp)\")", "machine learning +(\"true positive (tp) can be thought of as\")", "machine learning +(\"true positive (tp) can be compared to\")"]}