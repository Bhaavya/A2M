{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "On the character of Indian Stock Markets: <b>A Machine</b> <b>Learning</b> <b>Approach</b> ...", "url": "https://www.ijert.org/on-the-character-of-indian-stock-markets-a-machine-learning-approach", "isFamilyFriendly": true, "displayUrl": "https://www.ijert.org/on-the-character-of-indian-stock-markets-<b>a-machine</b>-<b>learning</b>-<b>approach</b>", "snippet": "On the character of Indian Stock Markets: <b>A Machine</b> <b>Learning</b> <b>Approach</b>. Shubham popli Northcap University Gurgaon, Haryana . Abstract- The enterprise of forecasting the stock market is as old as the market itself, ranging from the many traditional approaches <b>like</b> regression analysis and linear methods <b>like</b> AR, MA, ARIMA and ARMA, and of course fuzzier methods <b>like</b> experts intuitions and sentiment analysis of news cycles. But owing to the non-linear and dynamic nature of the markets, these ...", "dateLastCrawled": "2021-12-29T01:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Deep Learning (5/5): Sequence Models</b> - Dani&#39;s Braindump", "url": "https://tiefenauer.github.io/ml/deep-learning/5", "isFamilyFriendly": true, "displayUrl": "https://tiefenauer.github.io/ml/deep-<b>learning</b>/5", "snippet": "<b>Unidirectional</b> RNNs <b>only</b> consider already seen tokens at a time step to make a prediction. In contrast, bidirectional RNN ... which makes it a good fit for many <b>language</b>-related applications <b>like</b> <b>machine</b> translation. On the downside, because tokens from both directions are considered, the whole sequence needs to be processed before a prediction <b>can</b> be made. This makes it unsuitable for tasks <b>like</b> real-time speech recognition. Deep RNN. The RNNs we have seen so far consisted actually of <b>only</b> ...", "dateLastCrawled": "2022-02-03T17:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Intuitive Introduction to BERT \u2013 MachineCurve</b>", "url": "https://www.machinecurve.com/index.php/2021/01/04/intuitive-introduction-to-bert/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/index.php/2021/01/04/<b>intuitive-introduction-to-bert</b>", "snippet": "A BERT Transformer follows the so-called finetuning-based <b>approach</b> in Natural <b>Language</b> Processing. ... We do however often want to create <b>a machine</b> <b>learning</b> <b>model</b> that <b>can</b> perform <b>one</b> task really well. This is where finetuning comes in: using a labeled corpus, which is often smaller, we <b>can</b> then train the pretrained <b>model</b> further, with an additional or replacing NLP task. The end result is a <b>model</b> that has been pretrained on the large unlabeled corpus and which is finetuned to a specific ...", "dateLastCrawled": "2022-01-30T22:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A predictive <b>machine</b> <b>learning approach for microstructure optimization</b> ...", "url": "https://www.nature.com/articles/srep11551/", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/srep11551", "snippet": "The magnetostrictive strain that is optimized in our <b>machine</b> <b>learning</b> <b>approach</b> is the polycrystal averaged strain along the z\u2013<b>direction</b> as measured with respect to an initial unstressed crystal.", "dateLastCrawled": "2022-01-26T02:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Generalized Language Models: CoVe, ELMo</b> &amp; Cross-View Training", "url": "https://www.topbots.com/generalized-language-models-cove-elmo/", "isFamilyFriendly": true, "displayUrl": "https://www.topbots.com/<b>generalized-language-models-cove-elmo</b>", "snippet": "On labeled examples, all the <b>model</b> parameters are <b>updated</b> by standard supervised <b>learning</b>. The loss is the standard cross entropy. On unlabeled examples, the primary prediction module still <b>can</b> produce a \u201csoft\u201d target, even though we cannot know exactly how accurate they are. In a couple of auxiliary tasks, the predictor <b>only</b> sees and processes a restricted view of the input, such as <b>only</b> using encoder hidden state representation <b>in one</b> <b>direction</b>. The auxiliary task outputs are expected ...", "dateLastCrawled": "2022-01-17T20:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>A Machine</b> <b>Learning</b> <b>Approach</b> to Zeolite Synthesis Enabled by Automatic ...", "url": "https://pubs.acs.org/doi/10.1021/acscentsci.9b00193", "isFamilyFriendly": true, "displayUrl": "https://pubs.acs.org/doi/10.1021/acscentsci.9b00193", "snippet": "We built our <b>machine</b> <b>learning</b> <b>approach</b> relying on geometric features <b>only</b>, which are related to local geometry, structure, and porosity of a zeolite, to predict bulk and shear moduli of zeolites with an accuracy exceeding that of force field approaches. The development of this <b>model</b> has illustrated clear correlations between characteristic features of a zeolite and elastic moduli, providing exceptional insight into the mechanics of zeolitic frameworks. Finally, we employ this methodol. to ...", "dateLastCrawled": "2022-01-07T00:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Bidirectional LSTMs with TensorFlow 2.0</b> and Keras - MachineCurve", "url": "https://www.machinecurve.com/index.php/2021/01/11/bidirectional-lstms-with-tensorflow-and-keras/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/index.php/2021/01/11/<b>bidirectional-lstms-with-tensorflow</b>...", "snippet": "This immediately shows that LSTMs are <b>unidirectional</b>. In other words, the sequence is processed into <b>one</b> <b>direction</b>; here, from left to right. This makes common sense, as \u2013 except for a few languages \u2013 we read and write in a left-to-right fashion. For translation tasks, this is therefore not a problem, because you don\u2019t know what will be said in the future and hence have no business about knowing what will happen after your current input word. But unidirectionality <b>can</b> also limit the ...", "dateLastCrawled": "2022-02-02T19:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Machine Learning</b> Glossary | <b>Google Developers</b>", "url": "https://developers.google.com/machine-learning/glossary/", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine-learning</b>/glossary", "snippet": "A distributed <b>machine learning</b> <b>approach</b> that trains <b>machine learning</b> models using decentralized examples residing on devices such as smartphones. In federated <b>learning</b>, a subset of devices downloads the current <b>model</b> from a central coordinating server. The devices use the examples stored on the devices to make improvements to the <b>model</b>. The devices then upload the <b>model</b> improvements (but not the training examples) to the coordinating server, where they are aggregated with other updates to ...", "dateLastCrawled": "2022-02-03T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Unidirectional</b> Data Flow - GeeksforGeeks", "url": "https://www.geeksforgeeks.org/unidirectional-data-flow/", "isFamilyFriendly": true, "displayUrl": "https://www.geeksforgeeks.org/<b>unidirectional</b>-data-flow", "snippet": "<b>Unidirectional</b> data flow is a technique that is mainly found in functional reactive programming. It is also known as <b>one</b>-way data flow, which means the data has <b>one</b>, and <b>only</b> <b>one</b> way to be transferred to other parts of the application. In essence, this means child components are not able to update the data that is coming from the parent component. In React, data coming from a parent is called", "dateLastCrawled": "2022-02-01T07:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How to Code BERT Using <b>PyTorch</b> - Tutorial With Examples - neptune.ai", "url": "https://neptune.ai/blog/how-to-code-bert-using-pytorch-tutorial", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/how-to-code-bert-using-<b>pytorch</b>-tutorial", "snippet": "BERT uses two training paradigms: Pre-training and Fine-tuning. During pre-training, the <b>model</b> is trained on a large dataset to extract patterns. This is generally an unsupervised <b>learning</b> task <b>where the model</b> is trained on an unlabelled dataset <b>like</b> the data from a big corpus <b>like</b> Wikipedia.. During fine-tuning the <b>model</b> is trained for downstream tasks <b>like</b> Classification, Text-Generation, <b>Language</b> Translation, Question-Answering, and so forth. Essentially, you <b>can</b> download a pre-trained ...", "dateLastCrawled": "2022-02-03T01:57:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "On the comparability of pre-trained <b>language</b> models", "url": "http://ceur-ws.org/Vol-2624/paper2.pdf", "isFamilyFriendly": true, "displayUrl": "ceur-ws.org/Vol-2624/paper2.pdf", "snippet": "It is a <b>unidirectional</b> <b>language</b> <b>model</b> and it al-lows stacking task speci\ufb01c layers on top after pre-training, i.e. it is fully end-to-end trainable. The major difference between them is the internal ar-chitecture, where GPT uses a Transformer decoder architecture (Vaswani et al.,2017). Instead of processing <b>one</b> input token at a time, like recurrent architectures (LSTMs, GRUs) do, Trans-formers process whole sequences all at once. This is possible because they utilize a variant of the At ...", "dateLastCrawled": "2022-01-02T10:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "On the character of Indian Stock Markets: A <b>Machine</b> <b>Learning</b> <b>Approach</b> ...", "url": "https://www.ijert.org/on-the-character-of-indian-stock-markets-a-machine-learning-approach", "isFamilyFriendly": true, "displayUrl": "https://www.ijert.org/on-the-character-of-indian-stock-markets-a-<b>machine</b>-<b>learning</b>-<b>approach</b>", "snippet": "On the character of Indian Stock Markets: A <b>Machine</b> <b>Learning</b> <b>Approach</b>. Shubham popli Northcap University Gurgaon, Haryana. Abstract- The enterprise of forecasting the stock market is as old as the market itself, ranging from the many traditional approaches like regression analysis and linear methods like AR, MA, ARIMA and ARMA, and of course fuzzier methods like experts intuitions and sentiment analysis of news cycles.", "dateLastCrawled": "2021-12-29T01:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Deep Learning (5/5): Sequence Models</b> - Dani&#39;s Braindump", "url": "https://tiefenauer.github.io/ml/deep-learning/5", "isFamilyFriendly": true, "displayUrl": "https://tiefenauer.github.io/ml/deep-<b>learning</b>/5", "snippet": "<b>Unidirectional</b> RNNs <b>only</b> consider already seen tokens at a time step to make a prediction. In contrast, bidirectional RNN ... In that respect <b>one</b> <b>can</b> think of a <b>machine</b> translation <b>model</b> as kind of conditional <b>language</b> <b>model</b> that calculates the probability of a translated sentence given the input sentence in the original <b>language</b>. Beam search . Suppose we have the following french sentence as an input: Jane visite l&#39;Afrique en septembre. Possible translations to English for this sentence ...", "dateLastCrawled": "2022-02-03T17:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Machine Learning</b> Glossary | <b>Google Developers</b>", "url": "https://developers.google.com/machine-learning/glossary/", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine-learning</b>/glossary", "snippet": "A distributed <b>machine learning</b> <b>approach</b> that trains <b>machine learning</b> models using decentralized examples residing on devices such as smartphones. In federated <b>learning</b>, a subset of devices downloads the current <b>model</b> from a central coordinating server. The devices use the examples stored on the devices to make improvements to the <b>model</b>. The devices then upload the <b>model</b> improvements (but not the training examples) to the coordinating server, where they are aggregated with other updates to ...", "dateLastCrawled": "2022-02-03T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Intuitive Introduction to BERT \u2013 MachineCurve</b>", "url": "https://www.machinecurve.com/index.php/2021/01/04/intuitive-introduction-to-bert/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/index.php/2021/01/04/<b>intuitive-introduction-to-bert</b>", "snippet": "A BERT Transformer follows the so-called finetuning-based <b>approach</b> in Natural <b>Language</b> Processing. It is different than the feature-based ... We do however often want to create a <b>machine</b> <b>learning</b> <b>model</b> that <b>can</b> perform <b>one</b> task really well. This is where finetuning comes in: using a labeled corpus, which is often smaller, we <b>can</b> then train the pretrained <b>model</b> further, with an additional or replacing NLP task. The end result is a <b>model</b> that has been pretrained on the large unlabeled corpus ...", "dateLastCrawled": "2022-01-30T22:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Predicting the pandemic: sentiment evaluation and predictive analysis ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8007226/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8007226", "snippet": "<b>One</b> such <b>model</b> which is certainly under the focus is the convolutional neural network (CNN). The classic convolutional network has proven to be successful for experiments like text mining, polarity generation, information retrieval, and so on. Often for the discussed tasks, the convolutional network outperforms its other deep <b>learning</b> counterparts 21, 22]. For filtering sentence classification within a short scope, <b>unidirectional</b> convolution layers <b>can</b> be stated as translation invariant, as ...", "dateLastCrawled": "2022-01-09T20:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Google\u2019s <b>BERT</b> - NLP and Transformer Architecture That Are Reshaping AI ...", "url": "https://neptune.ai/blog/bert-and-the-transformer-architecture-reshaping-the-ai-landscape", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/<b>bert</b>-and-the-transformer-architecture-reshaping-the-ai-landscape", "snippet": "<b>BERT</b> <b>can</b> perform transfer <b>learning</b>: Transfer <b>learning</b> is a power concept which was first implemented for <b>machine</b> vision. Models trained on ImageNet were then available for other \u201cdownstream\u201d tasks, where they could build on the knowledge of the <b>model</b> trained on more data. In other words, these pre-trained models <b>can</b> \u201ctransfer\u201d the knowledge they learned on the large dataset to another <b>model</b>, which needs much less data to perform well at a specific task. For <b>machine</b> vision, the pre ...", "dateLastCrawled": "2022-02-02T00:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "RStudio AI Blog: <b>Time Series Forecasting with Recurrent Neural Networks</b>", "url": "https://blogs.rstudio.com/ai/posts/2017-12-20-time-series-forecasting-with-recurrent-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://blogs.rstudio.com/ai/posts/2017-12-20-<b>time-series-forecasting-with-recurrent</b>...", "snippet": "The first fully connected <b>approach</b> didn\u2019t do well, but that doesn\u2019t mean <b>machine</b> <b>learning</b> isn\u2019t applicable to this problem. The previous <b>approach</b> first flattened the time series, which removed the notion of time from the input data. Let\u2019s instead look at the data as what it is: a sequence, where causality and order matter. You\u2019ll try a recurrent-sequence processing <b>model</b> \u2013 it should be the perfect fit for such sequence data, precisely because it exploits the temporal ordering of ...", "dateLastCrawled": "2022-01-29T06:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A predictive <b>machine</b> <b>learning approach for microstructure optimization</b> ...", "url": "https://www.nature.com/articles/srep11551/", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/srep11551", "snippet": "The magnetostrictive strain that is optimized in our <b>machine</b> <b>learning</b> <b>approach</b> is the polycrystal averaged strain along the z\u2013<b>direction</b> as measured with respect to an initial unstressed crystal.", "dateLastCrawled": "2022-01-26T02:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How to Code BERT Using <b>PyTorch</b> - Tutorial With Examples - neptune.ai", "url": "https://neptune.ai/blog/how-to-code-bert-using-pytorch-tutorial", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/how-to-code-bert-using-<b>pytorch</b>-tutorial", "snippet": "They presented a <b>model</b> that <b>only</b> uses decoders from the transformer instead of encoders in a <b>unidirectional</b> <b>approach</b>. As a result, it outperformed all the previous models in various tasks like: Classification; Natural <b>Language</b> Inference; Semantic similarity; Question answering ; Multiple Choice. Even though the GPT used <b>only</b> the decoder, it could still retain long-term dependencies. Furthermore, it reduced fine-tuning to a minimum compared to what we saw in ULM-FiT. Below is the table that ...", "dateLastCrawled": "2022-02-03T01:57:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "An intelligent Chatbot using deep <b>learning</b> with Bidirectional RNN and ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7283081/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7283081", "snippet": "The main purpose of this work is to increase the perplexity and <b>learning</b> rate of the <b>model</b> and find Bleu Score for translation in same <b>language</b>. The experiments are conducted using Tensorflow using python 3.6. The perplexity, leaning rate, Bleu score and Average time per 1000 steps are 56.10, 0.0001, 30.16 and 4.5 respectively. <b>One</b> epoch is completed at 23,000 steps. The paper also study MacBook Air as a system for neural network and deep <b>learning</b>.", "dateLastCrawled": "2022-02-02T12:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Google\u2019s <b>BERT</b> - NLP and Transformer Architecture That Are Reshaping AI ...", "url": "https://neptune.ai/blog/bert-and-the-transformer-architecture-reshaping-the-ai-landscape", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/<b>bert</b>-and-the-transformer-architecture-reshaping-the-ai-landscape", "snippet": "<b>BERT</b> <b>can</b> perform transfer <b>learning</b>: Transfer <b>learning</b> is a power concept which was first implemented for <b>machine</b> vision. Models trained on ImageNet were then available for other \u201cdownstream\u201d tasks, where they could build on the knowledge of the <b>model</b> trained on more data. In other words, these pre-trained models <b>can</b> \u201ctransfer\u201d the knowledge they learned on the large dataset to another <b>model</b>, which needs much less data to perform well at a specific task. For <b>machine</b> vision, the pre ...", "dateLastCrawled": "2022-02-02T00:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Predicting the pandemic: sentiment evaluation and predictive analysis ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8007226/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8007226", "snippet": "In this phase, we also feed our CNN <b>model</b> with <b>one</b> layer of convolution on top of word vectors which were loaded from an unsupervised neural <b>language</b> <b>model</b>. These vectors are the non-zero elemental correlation produced by global vectors or GloVe . We feed the word vectors explicitly to make the <b>model</b> learn <b>only</b> the short text evaluation. In the ...", "dateLastCrawled": "2022-01-09T20:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Generalized Language Models: CoVe, ELMo</b> &amp; Cross-View Training", "url": "https://www.topbots.com/generalized-language-models-cove-elmo/", "isFamilyFriendly": true, "displayUrl": "https://www.topbots.com/<b>generalized-language-models-cove-elmo</b>", "snippet": "ELMo, short for Embeddings from <b>Language</b> <b>Model</b> (Peters, et al, 2018) ... On labeled examples, all the <b>model</b> parameters are <b>updated</b> by standard supervised <b>learning</b>. The loss is the standard cross entropy. On unlabeled examples, the primary prediction module still <b>can</b> produce a \u201csoft\u201d target, even though we cannot know exactly how accurate they are. In a couple of auxiliary tasks, the predictor <b>only</b> sees and processes a restricted view of the input, such as <b>only</b> using encoder hidden state ...", "dateLastCrawled": "2022-01-17T20:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Natural <b>Language QA Approaches using Reasoning with External Knowledge</b> ...", "url": "https://deepai.org/publication/natural-language-qa-approaches-using-reasoning-with-external-knowledge", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/natural-<b>language-qa-approaches-using-reasoning</b>-with...", "snippet": "Repositories of Unstructured Knowledge: Any natural <b>language</b> text or book or even the web <b>can</b> <b>be thought</b> of as a source of unstructured knowledge. Two large repositories that are commonly used are the Wikipedia Corpus and the Toronto BookCorpus. Wikipedia corpus contains 4.4M articles about varied fields and is crowd-curated. Toronto BookCorpus consists of 11K books on various topics. Both of these are used in several latest neural <b>language</b> models to learn word representations and NLU.", "dateLastCrawled": "2022-01-24T06:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "How to Develop a Bidirectional LSTM For Sequence Classification in ...", "url": "https://machinelearningmastery.com/develop-bidirectional-lstm-sequence-classification-python-keras/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/develop-bi<b>direction</b>al-lstm-sequence-classification-p", "snippet": "Bidirectional LSTMs are an extension of traditional LSTMs that <b>can</b> improve <b>model</b> performance on sequence classification problems. In problems where all timesteps of the input sequence are available, Bidirectional LSTMs train two instead of <b>one</b> LSTMs on the input sequence. The first on the input sequence as-is and the second on a reversed copy of the input sequence. This <b>can</b> provide", "dateLastCrawled": "2022-02-02T20:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) <b>Artificial Intelligence A Modern Approach</b> (4th Edition) | Fahim ...", "url": "https://www.academia.edu/45126798/Artificial_Intelligence_A_Modern_Approach_4th_Edition_", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/45126798/<b>Artificial_Intelligence_A_Modern_Approach</b>_4th_Edition_", "snippet": "Artificial Intelligence (AI) is a big field, and this is a big book. We have tried to explore the full breadth of the field, which encompasses logic, probability, and continuous mathematics; perception, reasoning, <b>learning</b>, and action; fairness,", "dateLastCrawled": "2022-02-02T22:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Framewise phoneme classification with bidirectional LSTM</b> and other ...", "url": "https://www.sciencedirect.com/science/article/pii/S0893608005001206", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0893608005001206", "snippet": "These blocks <b>can</b> <b>be thought</b> of as a differentiable version of the memory chips in a digital computer. Each <b>one</b> contains <b>one</b> or more recurrently connected memory cells and three multiplicative units\u2014the input, output and forget gates\u2014that provide continuous analogues of write, read and reset operations for the cells. More precisely, the input to the cells is multiplied by the activation of the input gate, the output to the net is multiplied by that of the output gate, and the previous ...", "dateLastCrawled": "2022-01-30T12:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "What is the state of the art in deep <b>learning</b> for document similarity ...", "url": "https://www.quora.com/What-is-the-state-of-the-art-in-deep-learning-for-document-similarity", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-the-state-of-the-art-in-deep-<b>learning</b>-for-document...", "snippet": "Answer: The best I have come across are siamese networks used to train over similar documents using vectors as inputs. [1] I think you <b>can</b> experiment with the below <b>approach</b> by making modifications to how you represent the vector for the input documents to the siamese network. [1] - https://cs2...", "dateLastCrawled": "2022-01-10T23:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(PDF) <b>Flow Clustering Using Machine Learning Techniques</b>", "url": "https://www.researchgate.net/publication/220850213_Flow_Clustering_Using_Machine_Learning_Techniques", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/220850213_Flow_Clustering_Using_<b>Machine</b>...", "snippet": "<b>Machine</b> <b>learning</b> techniques <b>can</b> also be used to cluster the \ufb02ows. present in the data and then to create a classi\ufb01cation from the clusters. This is. a multiple step process. The data is \ufb01rst ...", "dateLastCrawled": "2022-01-31T04:06:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "On the comparability of pre-trained <b>language</b> models", "url": "http://ceur-ws.org/Vol-2624/paper2.pdf", "isFamilyFriendly": true, "displayUrl": "ceur-ws.org/Vol-2624/paper2.pdf", "snippet": "It is a <b>unidirectional</b> <b>language</b> <b>model</b> and it al-lows stacking task speci\ufb01c layers on top after pre-training, i.e. it is fully end-to-end trainable. The major difference between them is the internal ar-chitecture, where GPT uses a Transformer decoder architecture (Vaswani et al.,2017). Instead of processing <b>one</b> input token at a time, like", "dateLastCrawled": "2022-01-02T10:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Deep Learning (5/5): Sequence Models</b> - Dani&#39;s Braindump", "url": "https://tiefenauer.github.io/ml/deep-learning/5", "isFamilyFriendly": true, "displayUrl": "https://tiefenauer.github.io/ml/deep-<b>learning</b>/5", "snippet": "<b>Unidirectional</b> RNNs <b>only</b> consider already seen tokens at a time step to make a prediction. In contrast, bidirectional RNN ... In that respect <b>one</b> <b>can</b> think of a <b>machine</b> translation <b>model</b> as kind of conditional <b>language</b> <b>model</b> that calculates the probability of a translated sentence given the input sentence in the original <b>language</b>. Beam search . Suppose we have the following french sentence as an input: Jane visite l&#39;Afrique en septembre. Possible translations to English for this sentence ...", "dateLastCrawled": "2022-02-03T17:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "On the character of Indian Stock Markets: A <b>Machine</b> <b>Learning</b> <b>Approach</b> ...", "url": "https://www.ijert.org/on-the-character-of-indian-stock-markets-a-machine-learning-approach", "isFamilyFriendly": true, "displayUrl": "https://www.ijert.org/on-the-character-of-indian-stock-markets-a-<b>machine</b>-<b>learning</b>-<b>approach</b>", "snippet": "On the character of Indian Stock Markets: A <b>Machine</b> <b>Learning</b> <b>Approach</b>. Shubham popli Northcap University Gurgaon, Haryana. Abstract- The enterprise of forecasting the stock market is as old as the market itself, ranging from the many traditional approaches like regression analysis and linear methods like AR, MA, ARIMA and ARMA, and of course fuzzier methods like experts intuitions and sentiment analysis of news cycles.", "dateLastCrawled": "2021-12-29T01:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Intuitive Introduction to BERT \u2013 MachineCurve</b>", "url": "https://www.machinecurve.com/index.php/2021/01/04/intuitive-introduction-to-bert/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/index.php/2021/01/04/<b>intuitive-introduction-to-bert</b>", "snippet": "A BERT Transformer follows the so-called finetuning-based <b>approach</b> in Natural <b>Language</b> Processing. It is different than the feature-based ... We do however often want to create a <b>machine</b> <b>learning</b> <b>model</b> that <b>can</b> perform <b>one</b> task really well. This is where finetuning comes in: using a labeled corpus, which is often smaller, we <b>can</b> then train the pretrained <b>model</b> further, with an additional or replacing NLP task. The end result is a <b>model</b> that has been pretrained on the large unlabeled corpus ...", "dateLastCrawled": "2022-01-30T22:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>BERT Explained: A Complete Guide with Theory and</b> Tutorial \u2013 Towards ...", "url": "https://towardsml.com/2019/09/17/bert-explained-a-complete-guide-with-theory-and-tutorial/", "isFamilyFriendly": true, "displayUrl": "https://towardsml.com/2019/09/17/<b>bert-explained-a-complete-guide-with-theory-and</b>-tutorial", "snippet": "This <b>one</b>-directional <b>approach</b> works well for generating sentences \u2014 we <b>can</b> predict the next word, append that to the sequence, then predict the next to next word until we have a complete sentence. Now enters BERT, a <b>language</b> <b>model</b> which is bidirectionally trained (this is also its key technical innovation). This means we <b>can</b> now have a deeper sense of <b>language</b> context and flow <b>compared</b> to the single-<b>direction</b> <b>language</b> models. Instead of predicting the next word in a sequence, BERT makes ...", "dateLastCrawled": "2022-02-02T06:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Angular</b> vs. React in 2022: Side-By-Side Comparison | Trio Developers", "url": "https://trio.dev/blog/angular-vs-react", "isFamilyFriendly": true, "displayUrl": "https://trio.dev/blog/<b>angular</b>-vs-react", "snippet": "In short, <b>Angular</b> uses two-way (bi-directional) data binding while React uses <b>one</b>-way (<b>unidirectional</b>) data binding computations. For AngularJS, this means that changing a UI element will also change the corresponding <b>model</b>\u2019s state as well. Conversely, if you change the <b>model</b> state, then the UI element will respectively change. However, with React\u2019s <b>one</b>-way data binding <b>approach</b>, the <b>model</b> state is initially <b>updated</b>, and finally, it renders the change in the UI element. But, here\u2019s the ...", "dateLastCrawled": "2022-02-02T19:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A <b>Machine</b> <b>Learning</b> <b>Approach</b> to Zeolite Synthesis Enabled by Automatic ...", "url": "https://pubs.acs.org/doi/10.1021/acscentsci.9b00193", "isFamilyFriendly": true, "displayUrl": "https://pubs.acs.org/doi/10.1021/acscentsci.9b00193", "snippet": "We built our <b>machine</b> <b>learning</b> <b>approach</b> relying on geometric features <b>only</b>, which are related to local geometry, structure, and porosity of a zeolite, to predict bulk and shear moduli of zeolites with an accuracy exceeding that of force field approaches. The development of this <b>model</b> has illustrated clear correlations between characteristic features of a zeolite and elastic moduli, providing exceptional insight into the mechanics of zeolitic frameworks. Finally, we employ this methodol. to ...", "dateLastCrawled": "2022-01-07T00:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A predictive <b>machine</b> <b>learning approach for microstructure optimization</b> ...", "url": "https://www.nature.com/articles/srep11551/", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/srep11551", "snippet": "The magnetostrictive strain that is optimized in our <b>machine</b> <b>learning</b> <b>approach</b> is the polycrystal averaged strain along the z\u2013<b>direction</b> as measured with respect to an initial unstressed crystal.", "dateLastCrawled": "2022-01-26T02:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A peridynamic-based <b>machine learning</b> <b>model</b> for <b>one</b>-dimensional and two ...", "url": "https://link.springer.com/article/10.1007/s00161-020-00905-0", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00161-020-00905-0", "snippet": "With the rapid growth of available data and computing resources, using data-driven models is a potential <b>approach</b> in many scientific disciplines and engineering. However, for complex physical phenomena that have limited data, the data-driven models are lacking robustness and fail to provide good predictions. Theory-guided data science is the recent technology that <b>can</b> take advantage of both physics-driven and data-driven models. This study presents a novel peridynamics-based <b>machine learning</b> ...", "dateLastCrawled": "2022-01-28T16:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Andrew-NG-Notes/andrewng-p-5-sequence-models.md at master ... - <b>GitHub</b>", "url": "https://github.com/ashishpatel26/Andrew-NG-Notes/blob/master/andrewng-p-5-sequence-models.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/.../Andrew-NG-Notes/blob/master/andrewng-p-5-sequence-<b>models</b>.md", "snippet": "10,000 <b>one</b> hot <b>compared</b> to 300 features vector. Word embeddings have an interesting relationship to the face recognition task: ... We want to build a <b>language</b> <b>model</b> so that we <b>can</b> predict the next word. So we use this neural network to learn the <b>language</b> <b>model</b>. We get e j by np.dot (E,o&lt;sub&gt;j&lt;/sub&gt;) NN layer has parameters W1 and b1 while softmax layer has parameters W2 and b2; Input dimension is (300*6, 1) if the window size is 6 (six previous words). Here we are optimizing E matrix and ...", "dateLastCrawled": "2022-02-03T05:14:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine</b> <b>Learning</b> Glossary: <b>Language</b> Evaluation | Google Developers", "url": "https://developers.google.com/machine-learning/glossary/language", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine</b>-<b>learning</b>/glossary/<b>language</b>", "snippet": "A term used to describe a system that evaluates the text that both precedes and follows a target section of text. In contrast, a <b>unidirectional</b> system only evaluates the text that precedes a target section of text. For example, consider a masked <b>language</b> <b>model</b> that must determine probabilities for the word(s) representing the underline in the following question:. What is the _____ with you? A <b>unidirectional</b> <b>language</b> <b>model</b> would have to base its probabilities only on the context provided by ...", "dateLastCrawled": "2022-01-29T02:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Word Acquisition in Neural <b>Language</b> Models | Transactions of the ...", "url": "https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00444/109271/Word-Acquisition-in-Neural-Language-Models", "isFamilyFriendly": true, "displayUrl": "https://direct.mit.edu/.../tacl_a_00444/109271/Word-Acquisition-in-Neural-<b>Language</b>-<b>Models</b>", "snippet": "A quadratic <b>model</b> of log-frequency also provided a slightly better fit for <b>unidirectional</b> <b>language</b> models (R 2 = 0.93 to 0.94), particularly for high-frequency words; in <b>language</b> models, this could be due either to a floor effect on age of acquisition for high-frequency words or to slower <b>learning</b> of function words. Regardless, significant effects of other predictors remained the same when using a quadratic <b>model</b> for log-frequency.", "dateLastCrawled": "2022-02-02T10:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>Machine learning, artificial neural networks and social</b> research", "url": "https://www.researchgate.net/publication/344171463_Machine_learning_artificial_neural_networks_and_social_research", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/344171463_<b>Machine</b>_<b>learning</b>_artificial_neural...", "snippet": "<b>Machine</b> <b>Learning</b> (ML) is an automatic <b>learning</b> process in which data sets are processed (Di Franco and Santurro, 2020). An ML system learns directly from the data and learns to connect one or more ...", "dateLastCrawled": "2022-02-02T23:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Fine-tuned <b>Language Models for Text Classification</b> | DeepAI", "url": "https://deepai.org/publication/fine-tuned-language-models-for-text-classification", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/fine-tuned-<b>language-models-for-text-classification</b>", "snippet": "In <b>analogy</b>, a hypercolumn for a word or sentence in NLP is the concatenation of embeddings at different layers in a pretrained <b>model</b>. and is used by peters2017semi, deepcontext2017, Wieting2017, Conneau2017, and Mccann2017 who use <b>language</b> modeling, paraphrasing, entailment, and <b>Machine</b> Translation (MT) respectively for pretraining. Specifically, deepcontext2017 require engineered custom architectures, while we show state-of-the-art performance with the same basic architecture across a range ...", "dateLastCrawled": "2021-12-23T06:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "On the character of Indian Stock Markets: A <b>Machine</b> <b>Learning</b> Approach ...", "url": "https://www.ijert.org/on-the-character-of-indian-stock-markets-a-machine-learning-approach", "isFamilyFriendly": true, "displayUrl": "https://www.ijert.org/on-the-character-of-indian-stock-markets-a-<b>machine</b>-<b>learning</b>-approach", "snippet": "On the character of Indian Stock Markets: A <b>Machine</b> <b>Learning</b> Approach. Shubham popli Northcap University Gurgaon, Haryana. Abstract- The enterprise of forecasting the stock market is as old as the market itself, ranging from the many traditional approaches like regression analysis and linear methods like AR, MA, ARIMA and ARMA, and of course fuzzier methods like experts intuitions and sentiment analysis of news cycles.", "dateLastCrawled": "2021-12-29T01:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "[<b>MCQ&#39;s] Machine Learning - Last Moment Tuitions</b>", "url": "https://lastmomenttuitions.com/mcqs-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://lastmomenttuitions.com/<b>mcqs-machine-learning</b>", "snippet": "13. Newton\u2019s method is seldom used in <b>machine</b> <b>learning</b> because a. common loss functions are not self-concordant b. Newton\u2019s method does not work well on noisy data c. <b>machine</b> <b>learning</b> researchers don\u2019t really understand linear algebra d. it is generally not practical to form or store the Hessian in such problems, due to large problem size ...", "dateLastCrawled": "2022-02-03T02:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Artificial intelligence and machine learning</b> in design of mechanical ...", "url": "https://pubs.rsc.org/en/content/articlelanding/2021/mh/d0mh01451f#!", "isFamilyFriendly": true, "displayUrl": "https://pubs.rsc.org/en/content/articlelanding/2021/mh/d0mh01451f", "snippet": "Artificial intelligence, especially <b>machine</b> <b>learning</b> (ML) and deep <b>learning</b> (DL) algorithms, is becoming an important tool in the fields of materials and mechanical engineering, attributed to its power to predict materials properties, design de novo materials and discover new mechanisms beyond intuitions. As the structural complexity of novel materials soars, the material design problem to optimize mechanical behaviors can involve massive design spaces that are intractable for conventional ...", "dateLastCrawled": "2022-01-26T13:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Machine Learning Gist</b> \u00b7 GitHub", "url": "https://gist.github.com/sgoyal1012/b30d70d12b6efad88bb285e8e709b161", "isFamilyFriendly": true, "displayUrl": "https://gist.github.com/sgoyal1012/b30d70d12b6efad88bb285e8e709b161", "snippet": "How can we train a <b>machine</b> <b>learning</b> <b>model</b> to generate text automatically, character-by-character? By showing the <b>model</b> many training examples so it can learn a pattern between input and output. It is a multiclass classification problem! Lesson 16: Generative Adversarial networks - Ian Goodfellow! Uses of GAN - to generate data Mostly done in the field of images. Example a description of a bird is used to generate images matching that description. Imitation <b>Learning</b>; How GANs work? Generative ...", "dateLastCrawled": "2022-01-29T03:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Intuitive Introduction to BERT \u2013 MachineCurve</b>", "url": "https://www.machinecurve.com/index.php/2021/01/04/intuitive-introduction-to-bert/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/index.php/2021/01/04/<b>intuitive-introduction-to-bert</b>", "snippet": "We do however often want to create a <b>machine</b> <b>learning</b> <b>model</b> that can perform one task really well. This is where finetuning comes in: using a labeled corpus, which is often smaller, we can then train the pretrained <b>model</b> further, with an additional or replacing NLP task. The end result is a <b>model</b> that has been pretrained on the large unlabeled corpus and which is finetuned to a specific <b>language</b> task, such as summarization, text generation in a particular domain, or translation.", "dateLastCrawled": "2022-01-30T22:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Jiajun Zhang - ACL Anthology", "url": "https://aclanthology.org/people/j/jiajun-zhang/", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/people/j/jiajun-zhang", "snippet": "But the current dominant paradigm of <b>machine</b> <b>learning</b> is still to train a <b>model</b> that works well on static datasets. When <b>learning</b> tasks in a stream where data distribution may fluctuate, fitting on new tasks often leads to forgetting on the previous ones. We propose a simple yet effective framework that continually learns natural <b>language</b> understanding tasks with one <b>model</b>. Our framework distills knowledge and replays experience from previous tasks when fitting on a new task, thus named DnR ...", "dateLastCrawled": "2022-01-16T04:50:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(unidirectional language model)  is like +(a machine learning approach where the model can only be updated in one direction)", "+(unidirectional language model) is similar to +(a machine learning approach where the model can only be updated in one direction)", "+(unidirectional language model) can be thought of as +(a machine learning approach where the model can only be updated in one direction)", "+(unidirectional language model) can be compared to +(a machine learning approach where the model can only be updated in one direction)", "machine learning +(unidirectional language model AND analogy)", "machine learning +(\"unidirectional language model is like\")", "machine learning +(\"unidirectional language model is similar\")", "machine learning +(\"just as unidirectional language model\")", "machine learning +(\"unidirectional language model can be thought of as\")", "machine learning +(\"unidirectional language model can be compared to\")"]}