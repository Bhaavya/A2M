{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Pre-Trained</b> Models: Past, Present and Future - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S2666651021000231", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2666651021000231", "snippet": "It shows the potential of multi-modal <b>pre-trained</b> models to bridge the gap between text descriptions and image generation, especially the excellent <b>ability</b> in combining different objects, such as \u201can armchair in the shape of an avocado\u201d. CogView improves the numerical precision and training stability by introducing sandwich transformer and sparse attention mechanism, and thus surpasses the DALLE in FID and . It is also the first text-to-image <b>model</b> in Chinese.", "dateLastCrawled": "2022-01-27T11:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Pre-Trained</b> Models: Past, Present and Future \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/2106.07139/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2106.07139", "snippet": "It shows the potential of multi-modal <b>pre-trained</b> models to bridge the gap between text descriptions and image generation, especially the excellent <b>ability</b> in combining different objects, such as \u201can armchair in the shape of an avocado&quot;. CogView improves the numerical precision and training stability by introducing sandwich transformer and sparse attention mechanism, and thus surpasses the DALLE in FID and . It is also the first text-to-image <b>model</b> in Chinese.", "dateLastCrawled": "2022-01-08T23:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Chatbots and GPT-3: Using <b>human knowledge and relevant context</b> for ...", "url": "https://www.techradar.com/news/chatbots-and-gpt-3-using-human-knowledge-and-relevant-context-for-better-chatbot-experiences", "isFamilyFriendly": true, "displayUrl": "https://<b>www.techradar.com</b>/news/chatbots-and-gpt-3-using-human-knowledge-and-relevant...", "snippet": "GPT, or Generative <b>Pre-trained</b> Transformer, is an autoregressive language <b>model</b> that uses deep learning to produce human-<b>like</b> texts. GPT-3 is the third generation of the GPT series launched by ...", "dateLastCrawled": "2022-02-01T09:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "AI Limits: Can Deep Learning Models <b>Like</b> <b>BERT</b> Ever Understand Language ...", "url": "https://neptune.ai/blog/ai-limits-can-deep-learning-models-like-bert-ever-understand-language", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/ai-limits-can-deep-<b>learn</b>ing-<b>models</b>-<b>like</b>-<b>bert</b>-ever-understand...", "snippet": "NLPs \u201cImageNet moment; <b>pre-trained</b> models: Originally, we all trained our own models, or you had to fully train a <b>model</b> for a specific task. One of the key milestones which enabled the rapid evolution in performance was the creation of <b>pre-trained</b> models which could be used \u201coff-the-shelf\u201d and tuned to your specific task with little effort and data, in a process known as transfer learning. Understanding this is key to seeing why these models have been, and continue to perform well in a ...", "dateLastCrawled": "2022-02-02T18:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "To Dissect an Octopus: Making Sense of the Form/Meaning Debate | Julian ...", "url": "https://julianmichael.org/blog/2020/07/23/to-dissect-an-octopus.html", "isFamilyFriendly": true, "displayUrl": "https://julianmichael.org/blog/2020/07/23/to-dissect-an-octopus.html", "snippet": "A fairer test of the difference between a language <b>model</b>\u2019s capabilities and a <b>human\u2019s</b> may be what the authors of GPT-3 did when they generated news articles from the <b>model</b> and asked <b>humans</b> to guess whether they were real or fake (Section 3.9.4). <b>Humans</b> barely beat chance, at 52% accuracy. However, while arguably fair, their test is very weak: it lacks the generality, interactivity, element of novelty, and long time horizon possessed by the octopus test.", "dateLastCrawled": "2022-02-02T01:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Development of a highly precise place recognition module for effective ...", "url": "https://aisel.aisnet.org/cgi/viewcontent.cgi?article=1063&context=hicss-53", "isFamilyFriendly": true, "displayUrl": "https://aisel.aisnet.org/cgi/viewcontent.cgi?article=1063&amp;context=hicss-53", "snippet": "Human-Robot-Interaction (HRI) is the robots <b>ability</b> to understand abstract spatial concepts and act accordingly [1, 2]. For example, a robot vacuum cleaner may be asked to clean the bedroom, therefore the robot\u2019s recognition of a bedroom should match the <b>human\u2019s</b> understanding of such a place [1]. Interaction with robots is becoming an important", "dateLastCrawled": "2022-01-01T02:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "GPT-3: <b>The First Artificial General Intelligence</b>? \u2013 sinhvientuoitre", "url": "https://sinhvientuoitre.wordpress.com/2020/09/11/gpt-3-the-first-artificial-general-intelligence/", "isFamilyFriendly": true, "displayUrl": "https://sinhvientuoitre.wordpress.com/2020/09/11/gpt-3-the-first-artificial-general...", "snippet": "BERT, for example, has been <b>pre-trained</b> by Google on a considerable text corpus \u2014 most of Wikipedia, and several additional corpora \u2014 using a cluster of high-performance TPUs. The <b>pre-trained</b> <b>model</b> can then be incorporated into a task-specific pipeline, much in the same way word2vec and GloVe were used and fine-tuned on a smaller training set. The resulting models are excellent. I\u2019m not aware of any pre-2017 benchmark that resisted the transformer onslaught. Transformer models come at ...", "dateLastCrawled": "2022-01-25T16:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Snippext</b>: Better <b>Opinion Mining With Less Training</b> Data - <b>Megagon Labs</b>", "url": "https://megagon.ai/blog/snippext-better-opinion-mining-with-less-training-data/", "isFamilyFriendly": true, "displayUrl": "https://megagon.ai/blog/<b>snippext</b>-better-<b>opinion-mining-with-less-training</b>-data", "snippet": "Online services <b>like</b> Amazon and Yelp are constantly extracting and analyzing aspects, opinions, and sentiments, such as (\u201croom\u201d, \u201cvery small\u201d, negative), from reviews and other sources of user-generated information. This data can provide profound insight into various facets of their operations, consumers, and products. On the other side of the equation, aggregating these extractions into user <b>experience</b> summaries can aid customers in decision-making and save them the hassle of ...", "dateLastCrawled": "2021-11-27T02:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Autocoder</b> - Finetuning GPT-2 for Auto Code Completion \u2013 Congcong Wang ...", "url": "https://wangcongcong123.github.io/AutoCoder/", "isFamilyFriendly": true, "displayUrl": "https://wangcongcong123.github.io/<b>AutoCoder</b>", "snippet": "Basically, transfer learning says training a <b>model</b> beforehand on general dataset <b>to learn</b> some general features from the dataset (pre-training) and then transfers the learnt features to downstream specific tasks (fine-tuning). To gain a broad enough features, the dataset required for pre-training tends to be large scale and thus the pre-training is usually conducted in an unsupervised way. In computer vision, the general dataset for pre-training usually refers to the large-score image ...", "dateLastCrawled": "2022-01-30T17:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Why <b>is deep learning so powerful? - Quora</b>", "url": "https://www.quora.com/Why-is-deep-learning-so-powerful", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Why-<b>is-deep-learning-so-powerful</b>", "snippet": "Answer (1 of 2): From my personal <b>experience</b> as an AI engineer I can name a few reasons why deep learning has been better at certain tasks compared to traditional ML models in my line of work: 1. Deep learning allows you to train on vast amounts of data without overfitting. The more high-quality...", "dateLastCrawled": "2022-01-17T13:14:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Pre-Trained</b> Models: Past, Present and Future - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S2666651021000231", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2666651021000231", "snippet": "It shows the potential of multi-modal <b>pre-trained</b> models to bridge the gap between text descriptions and image generation, especially the excellent <b>ability</b> in combining different objects, such as \u201can armchair in the shape of an avocado\u201d. CogView improves the numerical precision and training stability by introducing sandwich transformer and sparse attention mechanism, and thus surpasses the DALLE in FID and . It is also the first text-to-image <b>model</b> in Chinese.", "dateLastCrawled": "2022-01-27T11:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "OpenAI GPT-3: Everything You Need to Know | <b>Springboard Blog</b>", "url": "https://www.springboard.com/blog/ai-machine-learning/machine-learning-gpt-3-open-ai/", "isFamilyFriendly": true, "displayUrl": "https://www.springboard.com/blog/ai-machine-<b>learn</b>ing/machine-<b>learn</b>ing-gpt-3-open-ai", "snippet": "Generative <b>Pre-trained</b> Transformer 3 (GPT-3) is a language <b>model</b> that leverages deep learning to generate human-like text (output). Not only can it produce text, but it can also generate code, stories, poems, etc. For these capabilities and reasons, it has become such a hot topic in the area of natural language processing (NLP). GPT-3 was introduced by Open AI earlier in May 2020 as a successor to their previous language <b>model</b> (LM) GPT-2. It is considered to be better and bigger than GPT-2 ...", "dateLastCrawled": "2022-02-02T21:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Pre-Trained</b> Models: Past, Present and Future \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/2106.07139/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2106.07139", "snippet": "DALLE is the very first transformer-based text-to-image zero-shot <b>pre-trained</b> <b>model</b> with around 10 billiion parameters. It shows the potential of multi-modal <b>pre-trained</b> models to bridge the gap between text descriptions and image generation, especially the excellent <b>ability</b> in combining different objects, such as \u201can armchair in the shape of an avocado&quot;. CogView improves the numerical precision and training stability by introducing sandwich transformer and sparse attention mechanism, and ...", "dateLastCrawled": "2022-01-08T23:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "AI Limits: Can Deep Learning Models Like <b>BERT</b> Ever Understand Language ...", "url": "https://neptune.ai/blog/ai-limits-can-deep-learning-models-like-bert-ever-understand-language", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/ai-limits-can-deep-<b>learn</b>ing-<b>models</b>-like-<b>bert</b>-ever-understand...", "snippet": "NLPs \u201cImageNet moment; <b>pre-trained</b> models: Originally, we all trained our own models, or you had to fully train a <b>model</b> for a specific task. One of the key milestones which enabled the rapid evolution in performance was the creation of <b>pre-trained</b> models which could be used \u201coff-the-shelf\u201d and tuned to your specific task with little effort and data, in a process known as transfer learning. Understanding this is key to seeing why these models have been, and continue to perform well in a ...", "dateLastCrawled": "2022-02-02T18:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Snippext</b>: Better <b>Opinion Mining With Less Training</b> Data - <b>Megagon Labs</b>", "url": "https://megagon.ai/blog/snippext-better-opinion-mining-with-less-training-data/", "isFamilyFriendly": true, "displayUrl": "https://megagon.ai/blog/<b>snippext</b>-better-<b>opinion-mining-with-less-training</b>-data", "snippet": "With fine-tuning, <b>pre-trained</b> language models can obtain high-quality extractions from user reviews \u2014 but not all organizations have access to an adequate amount of training data to do so. To address this, we developed <b>Snippext</b>, an opinion mining system built over a language <b>model</b> that is fine-tuned through semi-supervised learning with augmented data.", "dateLastCrawled": "2021-11-27T02:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Put Chatbot into Its Interlocutor&#39;s Shoes: New Framework <b>to Learn</b> ...", "url": "https://www.readkong.com/page/put-chatbot-into-its-interlocutor-s-shoes-new-framework-to-8626778", "isFamilyFriendly": true, "displayUrl": "https://www.readkong.com/page/put-chatbot-into-its-interlocutor-s-shoes-new-framework...", "snippet": "The baseline performance is tested by the original guiding chatbot, the DialoGPT <b>pre-trained</b> <b>model</b> that has not yet trained with any interlocutor <b>model</b>. Higher scores for RL and RW indicate better performance. Lower scores for RE , CPPL, PPL, and SB-3 indicate better performance. The best results are boldfaced. 5.5 Human Evaluation Setups reflected that the guiding chatbot trained with the For human evaluation, we recruited participants Google <b>model</b> was forced to generate weird sen- online ...", "dateLastCrawled": "2022-01-21T00:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A holistic overview of deep learning approach in medical imaging", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8776556/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8776556", "snippet": "The robustness of autoencoders stems from the <b>ability</b> to reconstruct output data, which <b>is similar</b> to the input data, because it has cost function which applies penalties to the <b>model</b> when the output and input data are different. Moreover, autoencoders are considered as an automatic features detector, because they do not need labeled data <b>to learn</b> from due to the unsupervised manner. Autoencoders architecture <b>is similar</b> to a formal CNN <b>model</b>, but with the feature is that the number of input ...", "dateLastCrawled": "2022-02-01T15:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "To Dissect an Octopus: Making Sense of the Form/Meaning Debate | Julian ...", "url": "https://julianmichael.org/blog/2020/07/23/to-dissect-an-octopus.html", "isFamilyFriendly": true, "displayUrl": "https://julianmichael.org/blog/2020/07/23/to-dissect-an-octopus.html", "snippet": "A fairer test of the difference between a language <b>model</b>\u2019s capabilities and a <b>human\u2019s</b> may be what the authors of GPT-3 did when they generated news articles from the <b>model</b> and asked <b>humans</b> to guess whether they were real or fake (Section 3.9.4). <b>Humans</b> barely beat chance, at 52% accuracy. However, while arguably fair, their test is very weak: it lacks the generality, interactivity, element of novelty, and long time horizon possessed by the octopus test.", "dateLastCrawled": "2022-02-02T01:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "In deep learning, how do I compare someone&#39;s work with our work? In ...", "url": "https://www.quora.com/In-deep-learning-how-do-I-compare-someones-work-with-our-work-In-order-to-compare-the-results-in-deep-learning-should-we-have-the-same-dataset-for-training-What-if-the-dataset-is-different-for-a-similar-problem", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/In-deep-<b>learn</b>ing-how-do-I-compare-someones-work-with-our-work-In...", "snippet": "Answer (1 of 2): Consider Image Classification - An area deeply empowered by Deep Learning. All models are reported against the ImageNet dataset. That is how models can be compared for performance strictly. All Kaggle competitions are held against a data-set. All models trained on a given data-...", "dateLastCrawled": "2022-01-20T11:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "The Next 10 Years Look Golden for Natural Language Processing Research ...", "url": "https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/articles/next-10-years-natural-language-processing/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.microsoft.com</b>/en-us/research/lab/<b>microsoft-research</b>-asia/articles/next-10...", "snippet": "All of these show that deep learning technology and big data have great potential for mimicking a <b>human\u2019s</b> <b>ability</b> to create, and that they can be used to help artists and others to create. Taking the capability of lyrics generation as an example, the system will first generate a topic before writing the lyrics. For instance, if you would like to write a song related to \u201cautumn,\u201d \u201csundown,\u201d and \u201csigh with feeling,\u201d the user can add keywords such as \u201cautumn wind,\u201d \u201cflowing ...", "dateLastCrawled": "2022-02-03T01:52:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "AI Limits: <b>Can</b> Deep Learning Models Like <b>BERT</b> Ever Understand Language ...", "url": "https://neptune.ai/blog/ai-limits-can-deep-learning-models-like-bert-ever-understand-language", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/ai-limits-<b>can</b>-deep-<b>learn</b>ing-<b>models</b>-like-<b>bert</b>-ever-understand...", "snippet": "NLPs \u201cImageNet moment; <b>pre-trained</b> models: Originally, we all trained our own models, or you had to fully train a <b>model</b> for a specific task. One of the key milestones which enabled the rapid evolution in performance was the creation of <b>pre-trained</b> models which could be used \u201coff-the-shelf\u201d and tuned to your specific task with little effort and data, in a process known as transfer learning. Understanding this is key to seeing why these models have been, and continue to perform well in a ...", "dateLastCrawled": "2022-02-02T18:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "How to Use GPT-3 in <b>Your Product: GPT-3 Integration</b> \u2014 Clockwise Software", "url": "https://clockwise.software/blog/how-to-integrate-gpt3/", "isFamilyFriendly": true, "displayUrl": "https://clockwise.software/blog/how-to-integrate-gpt3", "snippet": "Uses of GPT-3: 7 skills of a <b>pre-trained</b> language <b>model</b> you <b>can</b> use for your startup. No matter how cool, innovative, or profitable your product is, there are multiple ways to make it better. Implementing AI is one of them. But GPT-3 is much more powerful than simple AI implementations used, for example, in a typical chatbot.", "dateLastCrawled": "2022-02-02T16:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>GPT-3</b>: The First Artificial General Intelligence? | by Julien Lauret ...", "url": "https://towardsdatascience.com/gpt-3-the-first-artificial-general-intelligence-b8d9b38557a1", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>gpt-3</b>-the-first-artificial-general-intelligence-b8d9b...", "snippet": "BERT, for example, has been <b>pre-trained</b> by Google on a considerable text corpus \u2014 most of Wikipedia, and several additional corpora \u2014 using a cluster of high-performance TPUs. The <b>pre-trained</b> <b>model</b> <b>can</b> then be incorporated into a task-specific pipeline, much in the same way word2vec and GloVe were used and fine-tuned on a smaller training set. The resulting models are excellent. I\u2019m not aware of any pre-2017 benchmark that resisted the transformer onslaught. Transformer models come at ...", "dateLastCrawled": "2022-01-14T06:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>What do the machines think</b>? \u2014 Pimloc", "url": "https://www.pimloc.com/blog-1/what-do-the-machines-think", "isFamilyFriendly": true, "displayUrl": "https://www.pimloc.com/blog-1/<b>what-do-the-machines-think</b>", "snippet": "Open AI, a San Franciscan AI research centre, created the GPT-3 (Generative <b>Pre-trained</b> Transformer 3) Language <b>model</b>. This <b>model</b> is the third version of its kind, it is a very large language <b>model</b> that uses deep learning technology to create human-like text. It generates text using algorithms that have been trained using around 570GB of internet text data (499 billion tokens - 2x orders of magnitude higher than GPT-2). This means that it <b>can</b> answer questions, write essays, translate ...", "dateLastCrawled": "2021-11-29T06:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Memory, Show the Way: Memory Based Few Shot Word Representation Learning", "url": "https://aclanthology.org/D18-1173.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/D18-1173.pdf", "snippet": "Based on a <b>pre-trained</b> embedding space, the proposed method delivers impressive perfor-mance on two challenging few-shot word sim-ilarity tasks. Embeddings learned with our method also lead to considerable improve-ments over strong baselines on NER and sen-timent classi\ufb01cation. 1 Introduction <b>Humans</b> <b>can</b> <b>learn</b> a new word quickly from min-imal exposure to its context, as in the following example: The Labrador runs happily towards me, barking and wagging its tail. Even this is the \ufb01rst time ...", "dateLastCrawled": "2021-11-19T18:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "To Dissect an Octopus: Making Sense of the Form/Meaning Debate | Julian ...", "url": "https://julianmichael.org/blog/2020/07/23/to-dissect-an-octopus.html", "isFamilyFriendly": true, "displayUrl": "https://julianmichael.org/blog/2020/07/23/to-dissect-an-octopus.html", "snippet": "A fairer test of the difference between a language <b>model</b>\u2019s capabilities and a <b>human\u2019s</b> may be what the authors of GPT-3 did when they generated news articles from the <b>model</b> and asked <b>humans</b> to guess whether they were real or fake (Section 3.9.4). <b>Humans</b> barely beat chance, at 52% accuracy. However, while arguably fair, their test is very weak: it lacks the generality, interactivity, element of novelty, and long time horizon possessed by the octopus test.", "dateLastCrawled": "2022-02-02T01:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) A neural network account of <b>memory replay and knowledge consolidation</b>", "url": "https://www.researchgate.net/publication/351890336_A_neural_network_account_of_memory_replay_and_knowledge_consolidation", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/351890336_A_neural_network_account_of_memory...", "snippet": "account for <b>human\u2019s</b> <b>ability</b> to quickly <b>learn</b> from limi ted <b>experience</b>. However, it also reveals that 252 . replayed representations are dynamic in nature, as the prototypes generated from that ...", "dateLastCrawled": "2021-11-26T09:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) <b>Reinforcement learning from simultaneous human</b> and MDP reward ...", "url": "https://www.academia.edu/2661208/Reinforcement_learning_from_simultaneous_human_and_MDP_reward", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/2661208", "snippet": "tamer+rl increases the human <b>model</b>\u2019s influence in areas bel for the last state-action pair.2 The output of the resul- of the state-action space that have recently received train- tant H\u02c6 function\u2014changing as the agent gains <b>experience</b>\u2014 ing and slowly decreases influence in the absence of training, determines the relative quality of potential actions, so that leaving the original MDP reward and base RL agent <b>to learn</b> \u02c6 a)]. the exploitative action is a = argmaxa [H(s, autonomously in ...", "dateLastCrawled": "2021-09-20T01:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "The Limitations of Deep Learning", "url": "https://hacker-news.news/post/14790251", "isFamilyFriendly": true, "displayUrl": "https://hacker-news.news/post/14790251", "snippet": "This <b>ability</b> to handle hypotheticals, to expand our mental <b>model</b> space far beyond what we <b>can</b> <b>experience</b> directly, in a word, to perform abstraction and reasoning, is arguably the defining characteristic of human cognition. I call it &quot;extreme generalization&quot;: an <b>ability</b> to adapt to novel, never experienced before situations, using very little data or even no new data at all. This stands in sharp contrast with what deep nets do, which I would call &quot;local generalization&quot;: the mapping from ...", "dateLastCrawled": "2022-01-28T20:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Why does deep learning work so well</b> in the real world? - Quora", "url": "https://www.quora.com/Why-does-deep-learning-work-so-well-in-the-real-world", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>Why-does-deep-learning-work-so-well</b>-in-the-real-world", "snippet": "Answer (1 of 3): I think if I <b>can</b> talk about people\u2019s reservations about why Deep Learning might not work really well in real world I would be making the same point as why it works in real world. 1. Automated Feature Extraction cannot beat <b>human\u2019s</b> <b>ability</b> to spot features: That is kind of true f...", "dateLastCrawled": "2022-01-12T20:15:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Pre-Trained</b> Models: Past, Present and Future - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S2666651021000231", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2666651021000231", "snippet": "Large-scale <b>pre-trained</b> models (PTMs) such as BERT and GPT have recently achieved great success and become a milestone in the field of artificial intelligence (AI). Owing to sophisticated pre-training objectives and huge <b>model</b> parameters, large-scale PTMs <b>can</b> effectively capture knowledge from massive labeled and unlabeled data. By storing knowledge into huge parameters and fine-tuning on specific tasks, the rich knowledge implicitly encoded in huge parameters <b>can</b> benefit a variety of ...", "dateLastCrawled": "2022-01-27T11:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Pre-Trained</b> Models: Past, Present and Future \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/2106.07139/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2106.07139", "snippet": "As <b>compared</b> to RNNs, Transformer is an encoder-decoder structure that applies a self-attention mechanism, which <b>can</b> <b>model</b> correlations between all words of the input sequence in parallel. Hence, owing to the parallel computation of the self-attention mechanism, Transformer could fully take advantage of advanced computing devices to train large-scale models. In both the encoding and decoding phases of Transformer, the self-attention mechanism of Transformer computes representations for all ...", "dateLastCrawled": "2022-01-08T23:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "OpenAI GPT-3: Everything You Need to Know | <b>Springboard Blog</b>", "url": "https://www.springboard.com/blog/ai-machine-learning/machine-learning-gpt-3-open-ai/", "isFamilyFriendly": true, "displayUrl": "https://www.springboard.com/blog/ai-machine-<b>learn</b>ing/machine-<b>learn</b>ing-gpt-3-open-ai", "snippet": "Generative <b>Pre-trained</b> Transformer 3 (GPT-3) is a language <b>model</b> that leverages deep learning to generate human-like text (output). Not only <b>can</b> it produce text, but it <b>can</b> also generate code, stories, poems, etc. For these capabilities and reasons, it has become such a hot topic in the area of natural language processing (NLP). GPT-3 was introduced by Open AI earlier in May 2020 as a successor to their previous language <b>model</b> (LM) GPT-2. It is considered to be better and bigger than GPT-2 ...", "dateLastCrawled": "2022-02-02T21:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Survey of Human-in-the-loop for Machine Learning | DeepAI", "url": "https://deepai.org/publication/a-survey-of-human-in-the-loop-for-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/a-survey-of-human-in-the-loop-for-machine-<b>learn</b>ing", "snippet": "Human-in-the-loop aims to train an accurate prediction <b>model</b> with minimum cost by integrating human knowledge and <b>experience</b>. <b>Humans</b> <b>can</b> provide training data for machine learning applications and directly accomplish some tasks that are hard for computers in the pipeline with the help of machine-based approaches. In this paper, we survey existing works on human-in-the-loop from a data perspective and classify them into three categories with a progressive relationship: (1) the work of ...", "dateLastCrawled": "2022-01-30T13:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "How to Use GPT-3 in <b>Your Product: GPT-3 Integration</b> \u2014 Clockwise Software", "url": "https://clockwise.software/blog/how-to-integrate-gpt3/", "isFamilyFriendly": true, "displayUrl": "https://clockwise.software/blog/how-to-integrate-gpt3", "snippet": "Uses of GPT-3: 7 skills of a <b>pre-trained</b> language <b>model</b> you <b>can</b> use for your startup. No matter how cool, innovative, or profitable your product is, there are multiple ways to make it better. Implementing AI is one of them. But GPT-3 is much more powerful than simple AI implementations used, for example, in a typical chatbot.", "dateLastCrawled": "2022-02-02T16:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Way Off-Policy Batch Deep Reinforcement Learning of Implicit Human ...", "url": "https://www.arxiv-vanity.com/papers/1907.00456/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1907.00456", "snippet": "Effective off-policy learning gives us the <b>ability</b> <b>to learn</b> from many different rewards post-hoc, ... The number of tokens in the vocabulary of our <b>pre-trained</b> <b>model</b> is 20,000, making the action space very high-dimensional, potentially compounding the problem of overestimation and making batch learning excessively difficult. However, initializing the Q-networks with the weights of the <b>pre-trained</b> language <b>model</b> provides a strong prior over the appropriate word to select. Here we consider ...", "dateLastCrawled": "2022-01-04T03:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "MetaIQA: Deep Meta-<b>learning for No-Reference Image Quality Assessment</b> ...", "url": "https://deepai.org/publication/metaiqa-deep-meta-learning-for-no-reference-image-quality-assessment", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/metaiqa-deep-meta-<b>learning-for-no-reference-image</b>...", "snippet": "The meta-knowledge serves as an ideal <b>pre-trained</b> <b>model</b> for fast adapting to unknown distortions. We have done extensive experiments on five public IQA databases containing both synthetic and authentic distortions. The results demonstrate that the proposed <b>model</b> significantly outperforms the state-of-the-art NR-IQA methods in terms of generalization <b>ability</b> and evaluation accuracy. 2 Related Work 2.1 <b>No-reference image quality assessment</b>. NR-IQA <b>can</b> be classified into distortion-specific ...", "dateLastCrawled": "2022-02-02T06:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A holistic overview of deep learning approach in medical imaging", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8776556/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8776556", "snippet": "A CNN <b>model</b> <b>can</b> get the help of the stochastic gradient descent <b>to learn</b> significant associations from the existing examples used for training. Thus, the benefit of a CNN usage is that it gradually reduces the feature map size before finally is get flatten to feed the fully connected layer which in turn computes the probability scores of the targeted classes for the classification. Fc-connected layer is the last layer in a CNN <b>model</b>, Furthermore, this layer processes the strongly extracted ...", "dateLastCrawled": "2022-02-01T15:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Development of a highly precise place recognition module for effective ...", "url": "https://aisel.aisnet.org/cgi/viewcontent.cgi?article=1063&context=hicss-53", "isFamilyFriendly": true, "displayUrl": "https://aisel.aisnet.org/cgi/viewcontent.cgi?article=1063&amp;context=hicss-53", "snippet": "Human-Robot-Interaction (HRI) is the robots <b>ability</b> to understand abstract spatial concepts and act accordingly [1, 2]. For example, a robot vacuum cleaner may be asked to clean the bedroom, therefore the robot\u2019s recognition of a bedroom should match the <b>human\u2019s</b> understanding of such a place [1]. Interaction with robots is becoming an important", "dateLastCrawled": "2022-01-01T02:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "In deep learning, how do I compare someone&#39;s work with our work? In ...", "url": "https://www.quora.com/In-deep-learning-how-do-I-compare-someones-work-with-our-work-In-order-to-compare-the-results-in-deep-learning-should-we-have-the-same-dataset-for-training-What-if-the-dataset-is-different-for-a-similar-problem", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/In-deep-<b>learn</b>ing-how-do-I-compare-someones-work-with-our-work-In...", "snippet": "Answer (1 of 2): Consider Image Classification - An area deeply empowered by Deep Learning. All models are reported against the ImageNet dataset. That is how models <b>can</b> <b>be compared</b> for performance strictly. All Kaggle competitions are held against a data-set. All models trained on a given data-...", "dateLastCrawled": "2022-01-20T11:38:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Pre-trained</b> Models - Value <b>Machine</b> <b>Learning</b> and Deep <b>Learning</b> Technology", "url": "https://valueml.com/transfer-learning-approach-pre-trained-models-classifying-imagenet-classes-with-resnet50-in-python/", "isFamilyFriendly": true, "displayUrl": "https://valueml.com/transfer-<b>learning</b>-<b>approach-pre-trained-models-classifying</b>-imagenet...", "snippet": "Transfer <b>Learning</b> enables us to use the <b>pre-trained</b> models from other people by making small relevant changes. Basically, Transfer <b>Learning</b> (TL) is a <b>Machine</b> <b>Learning</b> technique that trains a new <b>model</b> for a particular problem based on the knowledge gained by solving some other problem. For example, the knowledge gained while <b>learning</b> to recognize trucks could be applied to recognize cars.", "dateLastCrawled": "2022-01-21T09:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Finding the Word <b>Analogy</b> from given words using Word2Vec embeddings ...", "url": "https://www.geeksforgeeks.org/finding-the-word-analogy-from-given-words-using-word2vec-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/finding-the-word-<b>analogy</b>-from-given-words-using-word2vec...", "snippet": "In the word <b>analogy</b> task, we complete the sentence ... What if we can use a <b>Machine</b> <b>Learning</b> algorithm to automate this task of finding the word <b>analogy</b>. In this tutorial, we will be using Word2Vec <b>model</b> and a <b>pre-trained</b> <b>model</b> named \u2018GoogleNews-vectors-negative300.bin\u2018 which is trained on over 50 Billion words by Google. Each word inside the <b>pre-trained</b> dataset is embedded in a 300-dimensional space and the words which are similar in context/meaning are placed closer to each other in ...", "dateLastCrawled": "2022-01-26T14:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Should I Learn Machine Learning</b>? | GenUI", "url": "https://www.genui.com/resources/ml-for-developers", "isFamilyFriendly": true, "displayUrl": "https://www.genui.com/resources/ml-for-developers", "snippet": "But it will almost always be best to start with a <b>pre-trained</b> <b>model</b>, from a more general dataset, and then fine-tune it to fit your specific domain. For example, most image recognition models are based on <b>pre-trained</b> models from ImageNet, a dataset of more than 14 million, hand-labeled images divided into over 20,000 classes (like \u201cbicycle\u201d, \u201cstrawberry\u201d, \u201csky\u201d).", "dateLastCrawled": "2022-01-30T18:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Word2Vec in Gensim Explained for Creating Word Embedding Models ...", "url": "https://machinelearningknowledge.ai/word2vec-in-gensim-explained-for-creating-word-embedding-models-pretrained-and-custom/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>knowledge.ai/word2vec-in-gensim-explained-for-creating-word...", "snippet": "<b>Machine</b> <b>learning</b> and ... This is another way putting that word2vec can draw the <b>analogy</b> that if Man is to Woman then Kind is to Queen! The publicly released <b>model</b> of word2vec by Google consists of 300 features and the <b>model</b> is trained in the Google news dataset. The vocabulary size of the <b>model</b> is around 1.6 billion words. However, this might have taken a huge time for the <b>model</b> to be trained on but they have applied a method of simple subsampling approach to optimize the time. Word2Vec ...", "dateLastCrawled": "2022-02-02T15:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Transfer <b>Learning</b>: The Highest Leverage Deep <b>Learning</b> Skill You Can Learn", "url": "https://www.the-analytics.club/transfer-learning", "isFamilyFriendly": true, "displayUrl": "https://www.the-analytics.club/transfer-<b>learning</b>", "snippet": "Transfer <b>learning</b> is a <b>machine</b> <b>learning</b> technique in which a <b>model</b> trained on a specific task is reused as part of the training process for another, different task. Here is a simple <b>analogy</b> to help you understand how transfer <b>learning</b> works: imagine that one person has learned everything there is to know about dogs. In contrast, another person has learned everything about cats. If both people are asked, \u201cWhat\u2019s an animal with four legs, a tail, and barks?\u201d The person who knows all ...", "dateLastCrawled": "2022-01-29T09:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Using Deep <b>Learning</b> for Image Analogies | by Tomer Amit | Towards Data ...", "url": "https://towardsdatascience.com/using-deep-learning-for-image-analogies-aa2e7d7af337", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/using-deep-<b>learning</b>-for-image-analogies-aa2e7d7af337", "snippet": "I will use the <b>pre-trained</b> VGG16 image classification <b>model</b>. The <b>model</b> consists of CNN layers stacked one after another, connected by max pooling layers. The input of the network is a 244\u00d7244\u00d73 image (i.e image width and length are 244 pixels, and 3 channels), and after applying all the convolutional layers, we get a 7\u00d77\u00d7512 array. (diagram taken from deeplearning.ai course by Andrew Ng, \u201cConvolutional Neural Networks\u201d) At the end of the network we have an additional flattening layer ...", "dateLastCrawled": "2022-01-19T03:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Classifying and completing word analogies by <b>machine</b> <b>learning</b> | Request PDF", "url": "https://www.researchgate.net/publication/349152012_Classifying_and_completing_word_analogies_by_machine_learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/349152012_Classifying_and_completing_word...", "snippet": "In this paper, we depart from this assumption to adopt a <b>machine</b> <b>learning</b> approach, i.e., <b>learning</b> a substitute of the parallelogram <b>model</b>. To achieve our goal, we first review the formal modeling ...", "dateLastCrawled": "2021-11-11T17:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Transfer <b>Learning to solve a Classification Problem</b> :: InBlog", "url": "https://inblog.in/Transfer-Learning-to-solve-a-Classification-Problem-9bihoVsKsV", "isFamilyFriendly": true, "displayUrl": "https://inblog.in/Transfer-<b>Learning-to-solve-a-Classification-Problem</b>-9bihoVsKsV", "snippet": "Why we need <b>pre-Trained</b> <b>Model</b>? Transfer <b>Learning</b> via VGG16; Building a <b>Model</b>; Code Walk Through; Result and Evaluation; Introduction: Neural networks are very different type of the <b>model</b> as compared to the Supervised <b>Learning</b>,. The most important things about deep <b>learning</b> <b>model</b> is it is very hard to train. It requires lots of the resources that a small company can\u2019t bear. RAM on a <b>machine</b> is cheap and is available in plenty. You need hundreds of GB\u2019s of RAM to run a super complex ...", "dateLastCrawled": "2021-11-25T02:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>machine</b> <b>learning</b> - <b>Merging pretrained models in Word2Vec</b>? - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/30482669/merging-pretrained-models-in-word2vec", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/30482669", "snippet": "How do i merge these two huge <b>pre-trained</b> vectors? or how do i train a new <b>model</b> and update vectors on top of another? I see that C based word2vec does not support batch training. I am looking to compute word <b>analogy</b> from these two models. I believe that vectors learned from these two sources will produce pretty good results. <b>machine</b>-<b>learning</b> word2vec. Share. Follow edited May 28 &#39;15 at 14:04. pbu. asked May 27 &#39;15 at 12:37. pbu pbu. 2,706 7 7 gold badges 37 37 silver badges 62 62 bronze ...", "dateLastCrawled": "2022-01-22T22:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How to load <b>a pretrained model in TensorFlow</b> - Quora", "url": "https://www.quora.com/How-do-you-load-a-pretrained-model-in-TensorFlow", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/How-do-you-load-<b>a-pretrained-model-in-TensorFlow</b>", "snippet": "Answer: This is the site tensorflow/models where you can download various Tensorflow COCO- trained models, download any one of them and save it into the same folder in which your code file is saved or if you save it into another folder, then do not forget to mention the full path while calling th...", "dateLastCrawled": "2022-01-30T06:34:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(pre-trained model)  is like +(human's ability to learn from experience)", "+(pre-trained model) is similar to +(human's ability to learn from experience)", "+(pre-trained model) can be thought of as +(human's ability to learn from experience)", "+(pre-trained model) can be compared to +(human's ability to learn from experience)", "machine learning +(pre-trained model AND analogy)", "machine learning +(\"pre-trained model is like\")", "machine learning +(\"pre-trained model is similar\")", "machine learning +(\"just as pre-trained model\")", "machine learning +(\"pre-trained model can be thought of as\")", "machine learning +(\"pre-trained model can be compared to\")"]}