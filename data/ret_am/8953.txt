{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Markov</b> <b>Decision</b> Processes: A <b>Tool</b> for Sequential <b>Decision</b> Making ...", "url": "https://www.researchgate.net/publication/40821814_Markov_Decision_Processes_A_Tool_for_Sequential_Decision_Making_under_Uncertainty", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/40821814_<b>Markov</b>_<b>Decision</b>_<b>Process</b>es_A_<b>Tool</b>_for...", "snippet": "<b>Markov decision process</b> (<b>MDP</b>). MDPs build the theoretical foundation for modeling sequential <b>decision</b>-making problems under uncertainty [133, 134]. An <b>MDP</b> comprises the tuple (S, A, R, p, \u03b3 ...", "dateLastCrawled": "2021-12-31T09:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Markov Decision Process</b> (<b>MDP</b>) <b>Toolbox for Matlab</b>", "url": "https://www.cs.ubc.ca/~murphyk/Software/MDP/mdp.html", "isFamilyFriendly": true, "displayUrl": "https://www.cs.ubc.ca/~murphyk/Software/<b>MDP</b>/<b>mdp</b>.html", "snippet": "A <b>Markov Decision Process</b> (<b>MDP</b>) is just <b>like</b> a <b>Markov</b> Chain, except the transition matrix depends on the action taken by the <b>decision</b> maker (agent) at each time step. The agent receives a reward, which depends on the action and the state. The goal is to find a function, called a policy, which specifies which action to take in each state, so as to maximize some function (e.g., the mean or expected discounted sum) of the sequence of rewards. One can formalize this in terms of Bellman&#39;s ...", "dateLastCrawled": "2022-02-03T00:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Theory</b> and Application", "url": "https://refubium.fu-berlin.de/bitstream/handle/fub188/11428/MDPwithInformationCosts.pdf?sequence=1", "isFamilyFriendly": true, "displayUrl": "https://refubium.fu-berlin.de/bitstream/handle/fub188/11428/<b>MDP</b>withInformationCosts...", "snippet": "The <b>theory</b> of <b>Markov</b> <b>decision</b> processes (<b>MDP</b>) is a well established <b>tool</b> for an-alyzing situations in which the dynamics of a stochastic <b>process</b> can be in\ufb02uenced by a <b>decision</b> maker. It provides a framework for solving optimization problems that arise in a wide range of \ufb01elds <b>like</b> operations research, epidemic control or management science [74]. The research of <b>Markov</b> <b>decision</b> processes goes back to the 1950s. Of central relevance was the introduction of the dynamic programming concept ...", "dateLastCrawled": "2021-11-21T05:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "International Journal of Computational Intelligence and Informatics ...", "url": "https://www.periyaruniversity.ac.in/ijcii/issue/marnew/4_mar_18.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.periyaruniversity.ac.in/ijcii/issue/marnew/4_mar_18.pdf", "snippet": "3. <b>MARKOV DECISION PROCESS</b> <b>Markov Decision Process</b> (<b>MDP</b> for short) is a popular mathematical framework for sequential <b>decision</b> making under uncertainty. In recent years, the field has seen explosive growth because of new application areas thrown up by advances in technology. These have not only stretched the limits of the existing <b>theory</b>, but ...", "dateLastCrawled": "2022-01-19T12:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 4, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Markov decision process</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Markov_decision_process", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Markov_decision_process</b>", "snippet": "In mathematics, a <b>Markov decision process</b> (<b>MDP</b>) is a discrete-time stochastic control <b>process</b>. It provides a mathematical framework for modeling <b>decision</b> making in situations where outcomes are partly random and partly under the control of a <b>decision</b> maker. MDPs are useful for studying optimization problems solved via dynamic programming. MDPs were known at least as early as the 1950s; a core body of research on <b>Markov</b> <b>decision</b> processes resulted from Ronald Howard&#39;s 1960 book, Dynamic ...", "dateLastCrawled": "2022-02-02T15:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Interactive visualization for testing Markov Decision</b> Processes: <b>MDP</b> VIS", "url": "https://www.sciencedirect.com/science/article/pii/S1045926X16301951", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1045926X16301951", "snippet": "A high level overview of the <b>Markov Decision Process</b> visualization prototype: <b>MDP</b> VIS. The top row has the three parameter controls for (A) the reward specification, (B) the model modifiers, and (C) the policy definition. A fourth panel gives the history of Monte Carlo rollout sets generated under the parameters of panels (A) through (C). Changes to the parameters enable the optimization button found under the policy definition and the Monte Carlo rollouts button found under the Exploration ...", "dateLastCrawled": "2022-02-02T20:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "[PDF] <b>Markov Decision Processes: Concepts and Algorithms</b> | Semantic Scholar", "url": "https://www.semanticscholar.org/paper/Markov-Decision-Processes%3A-Concepts-and-Algorithms-Otterlo-Wiering/968bab782e52faf0f7957ca0f38b9e9078454afe", "isFamilyFriendly": true, "displayUrl": "https://www.semanticscholar.org/paper/<b>Markov-Decision-Process</b>es:-Concepts-and...", "snippet": "First the formal framework of <b>Markov decision process</b> is defined, accompanied by the definition of value functions and policies. The main part of this text deals with introducing foundational classes of algorithms for learning optimal behaviors, based on various definitions of optimality with respect to the goal of learning sequential decisions. Additionally, it surveys efficient extensions of the foundational algorithms, differing mainly in the way feedback given by the environment is used ...", "dateLastCrawled": "2022-02-02T11:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "27 questions with answers in <b>MARKOV DECISION PROCESS</b> | Science topic", "url": "https://www.researchgate.net/topic/Markov-Decision-Process", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/topic/<b>Markov-Decision-Process</b>", "snippet": "A <b>Markov</b> Game also known as Stochastic Game is an extension of <b>Markov Decision Process</b> (<b>MDP</b>) to the multi-agent case. There are a couple of third-party Matlab toolboxes for solving MDPs available ...", "dateLastCrawled": "2022-01-20T05:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A machine <b>tool</b> <b>matching method in cloud manufacturing</b> using <b>Markov</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0736584519300924", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0736584519300924", "snippet": "It is not difficult to see that the machine <b>tool</b> demand-resource matching is the constructing and solving <b>process</b> of a dynamic variable granularity model, as well as a typical <b>Markov decision process</b> (<b>MDP</b>), which can perform actions to achieve the balance of production operation status according to the current real-time state of environment, and get rewards.", "dateLastCrawled": "2021-11-10T15:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Reinforcement Learning Tutorial</b> - Javatpoint", "url": "https://www.javatpoint.com/reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://www.javatpoint.com/reinforcement-learning", "snippet": "<b>Markov Decision Process</b>. <b>Markov Decision Process</b> or <b>MDP</b>, is used to formalize the reinforcement learning problems. If the environment is completely observable, then its dynamic can be modeled as a <b>Markov</b> <b>Process</b>. In <b>MDP</b>, the agent constantly interacts with the environment and performs actions; at each action, the environment responds and generates a new state. <b>MDP</b> is used to describe the environment for the RL, and almost all the RL problem can be formalized using <b>MDP</b>. <b>MDP</b> contains a tuple ...", "dateLastCrawled": "2022-02-02T06:14:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Markov</b> <b>Decision</b> Processes: A <b>Tool</b> for Sequential <b>Decision</b> Making ...", "url": "https://www.researchgate.net/publication/40821814_Markov_Decision_Processes_A_Tool_for_Sequential_Decision_Making_under_Uncertainty", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/40821814_<b>Markov</b>_<b>Decision</b>_<b>Process</b>es_A_<b>Tool</b>_for...", "snippet": "<b>Markov decision process</b> (<b>MDP</b>). MDPs build the theoretical foundation for modeling sequential <b>decision</b>-making problems under uncertainty [133, 134]. An <b>MDP</b> comprises the tuple (S, A, R, p, \u03b3 ...", "dateLastCrawled": "2021-12-31T09:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Probabilistic modelling of deception-based security framework using ...", "url": "https://www.sciencedirect.com/science/article/pii/S0167404821004223", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0167404821004223", "snippet": "<b>Markov Decision Process</b> (<b>MDP</b>) allows to model systems which are stochastic in nature because it helps to make decisions under uncertainty (Alsheikh et al., 2015). MDPs are an extension of Discrete-time <b>Markov</b> Chains (DTMCs), allowing to make non-deterministic choices as well. <b>Similar</b> to DTMCs, possible configurations of the system are modelled as a set of states and transitions between them occur in the form of discrete time steps. However, there may exist a non-deterministic choice between ...", "dateLastCrawled": "2022-02-03T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Finding Markov Decision Processes related reference</b>", "url": "https://www.researchgate.net/post/Finding-Markov-Decision-Processes-related-reference", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/post/<b>Finding-Markov-Decision-Processes-related-reference</b>", "snippet": "The system is modelled as a <b>Markov Decision Process</b> (<b>MDP</b>). Combinations of Just-In-Time (pull) and MRP (push) policies are used as alternatives... Combinations of Just-In-Time (pull) and MRP (push ...", "dateLastCrawled": "2022-01-19T08:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Reinforcement Learning Tutorial</b> - Javatpoint", "url": "https://www.javatpoint.com/reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://www.javatpoint.com/reinforcement-learning", "snippet": "<b>Markov Decision Process</b> or <b>MDP</b>, is used to formalize the reinforcement learning problems. If the environment is completely observable, then its dynamic can be modeled as a <b>Markov</b> <b>Process</b> . In <b>MDP</b>, the agent constantly interacts with the environment and performs actions; at each action, the environment responds and generates a new state.", "dateLastCrawled": "2022-02-02T06:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Interactive visualization for testing Markov Decision</b> Processes: <b>MDP</b> VIS", "url": "https://www.sciencedirect.com/science/article/pii/S1045926X16301951", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1045926X16301951", "snippet": "To address a broader class of optimization problems, we target the common optimization formulation of a <b>Markov Decision Process</b> (<b>MDP</b>). In an <b>MDP</b>, the state of the world evolves stochastically from one state to another depending on the action chosen at each time step. A scalar reward is received at each time step depending on the system state and the chosen action. An <b>MDP</b> is solved by learning a <b>decision</b> making rule (policy) that maximizes the long-term sum of rewards.", "dateLastCrawled": "2022-02-02T20:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Measuring the Distance Between Finite <b>Markov</b> <b>Decision</b> Processes", "url": "https://personal.ntu.edu.sg/boan/papers/AAMAS16_MDP.pdf", "isFamilyFriendly": true, "displayUrl": "https://personal.ntu.edu.sg/boan/papers/AAMAS16_<b>MDP</b>.pdf", "snippet": "2.1 <b>Markov Decision Process</b> In this paper, we focus on \ufb01nite <b>Markov</b> <b>decision</b> processes. Definition 2.1. A \ufb01nite <b>Markov decision process</b> can be represented as a 4-tuple M = {S,A,P,R}, where S is a \ufb01nite set of states; A is a \ufb01nite set of actions; P: S \u00d7 A\u00d7S \u2192 [0,1] is the probability transition function; and R: S \u00d7A \u2192 \u211c is the ...", "dateLastCrawled": "2022-01-03T12:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "LNAI 4539 - Bounded Parameter <b>Markov</b> <b>Decision</b> Processes with Average ...", "url": "https://ambujtewari.github.io/research/tewari07bounded.pdf", "isFamilyFriendly": true, "displayUrl": "https://ambujtewari.github.io/research/tewari07bounded.pdf", "snippet": "A <b>Markov Decision Process</b> is a tuple S,A,R,{p (i,j)(a)}.HereS is a \ufb01nite set ofstates,Aa\ufb01nite set ofactions,R: S \u2192 [0,1]isthe rewardfunctionandp i,j(a) is the probability of moving to state j upon taking action a in state i. A policy \u03bc: S \u2192 A is a mapping from states to actions. Any policy induces a <b>Markov</b> chain on the state space of a ...", "dateLastCrawled": "2022-01-16T14:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Metrics for Finite Markov Decision Processes</b>", "url": "https://normferns.com/documents/uai04paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://normferns.com/documents/uai04paper.pdf", "snippet": "ity of states in a \ufb01nite <b>Markov decision process</b> (<b>MDP</b>). The formulation of our metrics is based on the notion of bisimulation for MDPs, with an aim towards solving discounted in\ufb01nite horizon reinforcement learning tasks. Such metrics can be used to aggregate states, as well as to bet-ter structure other value function approximators (e.g., memory-basedor nearest-neighborapprox-imators). We provide bounds that relate our met-ric distances to the optimal values of states in the given <b>MDP</b>. 1 ...", "dateLastCrawled": "2022-01-18T21:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A Utility Criterion for <b>Markov</b> <b>Decision</b> Processes", "url": "https://www.jstor.org/stable/2629755", "isFamilyFriendly": true, "displayUrl": "https://www.jstor.org/stable/2629755", "snippet": "In <b>Markov</b> <b>process</b> with rewards, the returns received in each time period are random. We study R(r), where R(7r) is the vector of random total discounted returns using policy 7; we interpret [R(7T)]i as the random discounted return starting in state i. We write R(7T) in the above notation as 00 R(T) E= 81Y.&#39;T(t)r(7T(t)). t =O 3. A Utility Criterion Economic decisions are usually not based on total or average return alone but rather on the utility of the return as viewed by the <b>decision</b> maker ...", "dateLastCrawled": "2021-11-27T03:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Elaboration Tolerant Representation of Markov Decision Process</b> via ...", "url": "https://deepai.org/publication/elaboration-tolerant-representation-of-markov-decision-process-via-decision-theoretic-extension-of-probabilistic-action-language-pbc", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>elaboration-tolerant-representation-of-markov</b>-<b>decision</b>...", "snippet": "Alternatively, the semantics of pBC+ can also be defined in terms of <b>Markov Decision Process</b> (<b>MDP</b>), which in turn allows for representing <b>MDP</b> in a succinct and elaboration tolerant way as well as to leverage an <b>MDP</b> solver to compute pBC+. The idea led to the design of the system pbcplus2mdp, which can find an optimal policy of a pBC+ action description using an <b>MDP</b> solver.", "dateLastCrawled": "2021-12-28T15:27:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Understanding Reinforcement Learning Hands-On</b>: <b>Markov</b> <b>Decision</b> ...", "url": "https://towardsdatascience.com/understanding-reinforcement-learning-hands-on-markov-decision-processes-7d8469a8a782", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>understanding-reinforcement-learning-hands-on</b>-<b>markov</b>...", "snippet": "In this article, we presented a framework for describing scenarios, called a <b>Markov Decision Process</b>. This framework allows us to visualize how an agent could traverse through different states using actions, and what reward would be received from each interaction. From that, we developed a variation of the Multi-Armed Bandit, in which we now have two states, and actions to transition from one to another. Lastly, we presented how to implement such <b>MDP</b> using the gym environment.", "dateLastCrawled": "2022-01-17T19:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Markov decision</b> processes - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/0377221789903482", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/0377221789903482", "snippet": "Received April 1988 search and control <b>theory</b> <b>can</b> then be applied, assuming that the resultant optimization, or con- trol model is tractable. The intent of this paper is to review a mathe- matically-based optimization model of discrete- stage, sequential <b>decision</b> making in a stochastic environment, called the <b>Markov decision process</b> (<b>MDP</b>). The ...", "dateLastCrawled": "2021-12-06T07:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Play to Grade: Testing Coding Games as Classifying <b>Markov Decision Process</b>", "url": "https://deepai.org/publication/play-to-grade-testing-coding-games-as-classifying-markov-decision-process", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/play-to-grade-testing-coding-games-as-classifying...", "snippet": "Definition 2.1. A deterministic <b>Markov Decision Process</b> (<b>MDP</b>) is a 4-tuple M = &lt;S,A,T,R&gt;, where S is a set of states; A is a set of actions; T:S\u00d7A\u2192S is the transition dynamics; and R:S\u00d7A\u2192R is the reward function. We <b>can</b> consider each student implementation of the interactive assignment as a separate <b>MDP</b>.", "dateLastCrawled": "2022-01-22T14:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "probability - &quot;<b>Markov Decision Process&quot; with target states</b> and shortest ...", "url": "https://math.stackexchange.com/questions/3860303/markov-decision-process-with-target-states-and-shortest-path-as-only-constrain", "isFamilyFriendly": true, "displayUrl": "https://math.<b>stackexchange</b>.com/questions/3860303/<b>markov-decision-process-with-target</b>...", "snippet": "In the following, I will use the vocabulary of <b>Markov</b> <b>Decision</b> Processes (<b>MDP</b>) because I first <b>thought</b> this problem was one of them. However now I&#39;m not so sure anymore. Description of the problem: There is a set of states, some of which are transient and others are final. Let {A, B, C} be the subset of final states. There is an initial state.", "dateLastCrawled": "2021-12-23T14:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Blackwell Online Learning for Markov Decision Processes</b>", "url": "https://www.researchgate.net/publication/347965553_Blackwell_Online_Learning_for_Markov_Decision_Processes", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/347965553_Blackwell_Online_Learning_for...", "snippet": "Abstract \u2014This work provides a novel interpretation of <b>Markov</b>. <b>Decision</b> Processes (<b>MDP</b>) from the online optimization view-. point. In such an online optimization context, the policy of the. <b>MDP</b> ...", "dateLastCrawled": "2022-01-25T11:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "On Joint Transfer of Energy and Information: A <b>Markov</b> <b>Decision</b> Problem ...", "url": "https://sangbumchoi.github.io/files/final-jkim-stk-choi.pdf", "isFamilyFriendly": true, "displayUrl": "https://sangbumchoi.github.io/files/final-jkim-stk-choi.pdf", "snippet": "as a <b>Markov decision process</b> (<b>MDP</b>). The solution approach is inspired by the classic river crossing problem [2] and its connection to the Bellman equation [3]. Due to the information structure of the problem, a stochastic <b>decision</b> making scenario is presented and the Bellman equation is presented. I. INTRODUCTION Optimal control <b>theory</b> serves as a powerful <b>tool</b> for ana-lyzing and interpreting a variety of problems in other \ufb01elds e.g.: machine learning, reinforcement learning, \ufb01ltering ...", "dateLastCrawled": "2021-07-23T03:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Comparative effectiveness research on patients with acute ischemic ...", "url": "https://core.ac.uk/download/pdf/8724424.pdf", "isFamilyFriendly": true, "displayUrl": "https://core.ac.uk/download/pdf/8724424.pdf", "snippet": "<b>MDP</b> <b>theory</b> is a versatile and powerful <b>tool</b> used to analyze sequential <b>decision</b> problems [16] with applica-tions in many areas, such as natural science, engineering technology, and medical care, and it increase the utiliza-tion of medical resources and optimize methods of diag-nosis or treatment. The <b>MDP</b> <b>theory</b> is also important", "dateLastCrawled": "2021-08-14T08:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Download [PDF] Competitive <b>Markov</b> <b>Decision</b> Processes Free \u2013 Usakochan PDF", "url": "https://usakochan.net/download/competitive-markov-decision-processes/", "isFamilyFriendly": true, "displayUrl": "https://usakochan.net/download/competitive-<b>markov</b>-<b>decision</b>-<b>process</b>es", "snippet": "<b>Markov decision process</b> (<b>MDP</b>) models are widely used for modeling sequential <b>decision</b>-making problems that arise in engineering, economics, computer science, and the social sciences. Many real-world problems modeled by MDPs have huge state and/or action spaces, giving an opening to the curse of dimensionality and so making practical solution of the resulting models intractable. In other cases, the system of interest is too complex to allow explicit specification of some of the <b>MDP</b> model ...", "dateLastCrawled": "2022-01-06T12:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Top 50 Artificial Intelligence Questions</b> and Answers (2022) - javatpoint", "url": "https://www.javatpoint.com/artificial-intelligence-interview-questions", "isFamilyFriendly": true, "displayUrl": "https://www.javatpoint.com/artificial-intelligence-interview-questions", "snippet": "<b>Theory</b> of Mind: It is the advanced AI that is capable of understanding human emotions, people, etc., ... What is <b>Markov</b>&#39;s <b>Decision</b> <b>process</b>? The solution for a reinforcement learning problem <b>can</b> be achieved using the <b>Markov decision process</b> or <b>MDP</b>. Hence, <b>MDP</b> is used to formalize the RL problem. It <b>can</b> be said as the mathematical approach to solve a reinforcement learning problem. The main aim of this <b>process</b> is to gain maximum positive rewards by choosing the optimum policy. <b>MDP</b> has four ...", "dateLastCrawled": "2022-01-31T12:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Management science specialization</b> - PhD in Business Administration ...", "url": "https://www.sauder.ubc.ca/programs/phd/phd-business-administration/specializations/management-science", "isFamilyFriendly": true, "displayUrl": "https://www.sauder.ubc.ca/programs/phd/phd-business-administration/specializations/...", "snippet": "The module will address some basic ideas of <b>decision</b> analysis and how they <b>can</b> be extended into a more general <b>Markov Decision Process</b> (<b>MDP</b>) model. These methods will be applied to a wide range of disciplines including management science, economics, telecommunications, and computer science. The objectives of the course are to teach students to formulate and solve <b>MDP</b> models under several optimality criteria. expand BAMS 518 <b>Markov</b> <b>Decision</b> Processes . The module will address some basic ideas ...", "dateLastCrawled": "2022-02-03T03:19:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) Deciding when to intervene: a <b>Markov decision process</b> approach ...", "url": "https://www.academia.edu/14366848/Deciding_when_to_intervene_a_Markov_decision_process_approach", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/14366848/Deciding_when_to_intervene_a_<b>Markov_decision_process</b>...", "snippet": "<b>MDP</b> and, as defined in [8] <b>can</b> include the Thus, it is assumed that, at each time following nodes: point, the <b>decision</b> maker may observe the current state of the <b>Markov</b> <b>process</b> being 2.2.1. State nodes controlled. At the same time, the <b>decision</b> Each state node represents a variable ob- maker takes one action, from a finite set of tained from the factorization 1 of the <b>MDP</b> possible actions, relying on the current state space state. Since the IV specifies a <b>MDP</b> itself. Therefore the stochastic ...", "dateLastCrawled": "2022-01-07T21:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A <b>Theory of Regularized Markov Decision Processes</b> | DeepAI", "url": "https://deepai.org/publication/a-theory-of-regularized-markov-decision-processes", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/a-<b>theory-of-regularized-markov-decision-processes</b>", "snippet": "A <b>Markov decision process</b> (<b>MDP</b>) is a tuple {S, A, P, r, \u03b3} with S the finite 1 1 1 We assume a finite space for simplicity of exposition, our results extend to more general cases. state space, A the finite action space, P \u2208 \u0394 S \u00d7 A S the Markovian transition kernel (P (s \u2032 | s, a) denotes the probability of transiting to s \u2032 when action a is applied in state s), r \u2208 R S \u00d7 A the reward function and \u03b3 \u2208 (0, 1) the discount factor.", "dateLastCrawled": "2021-12-20T18:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Robust Decomposable <b>Markov</b> <b>Decision</b> Processes Motivated by Allocating ...", "url": "http://neddimitrov.org/uploads/pdfs/Dimitrov-RobustDecomposableMDPJournal.pdf", "isFamilyFriendly": true, "displayUrl": "neddimitrov.org/uploads/pdfs/Dimitrov-RobustDecomposable<b>MDP</b>Journal.pdf", "snippet": "decomposable <b>Markov decision process</b> (<b>MDP</b>). A robust decomposable <b>MDP</b> model applies to situations where several MDPs, with the transition probabilities in each only known through an uncertainty set, are coupled together by joint resource constraints. Robust decomposable MDPs are di erent than both decomposable MDPs, and robust MDPs and <b>can</b> not be solved by a direct application of the solution methods from either of those areas. In fact, to the best of our knowledge, there is no known method ...", "dateLastCrawled": "2021-12-22T18:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A finite-horizon <b>Markov decision process</b> model for cancer chemotherapy ...", "url": "https://link.springer.com/article/10.1007/s10479-020-03706-5", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10479-020-03706-5", "snippet": "Cancer is one of the major diseases that seriously threaten the human life. Increasing interest in cancer treatment strategies for chemotherapy treatment planning and optimal drug administration has created new applications for mathematical modeling. In this paper, we develop a finite-horizon <b>Markov decision process</b> (<b>MDP</b>) model for cancer chemotherapy treatment planning that could advise selection of the optimal policy for the chemotherapy regimen according to the patient\u2019s condition. The ...", "dateLastCrawled": "2022-01-06T20:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>A Markov decision process approach to vacant</b> taxi routing with e ...", "url": "https://www.sciencedirect.com/science/article/pii/S0191261518303837", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0191261518303837", "snippet": "A queueing <b>theory</b>-based model for matching taxis and passengers is proposed to account for competition from other taxis and use of e-hailing apps. \u2022 The problem is formulated as a <b>Markov decision process</b> (<b>MDP</b>), taking into account the impact of current decisions on future return over multiple pickups and drop-offs. \u2022 An efficient implementation of the value iteration algorithm for solving the <b>MDP</b> problem is proposed making use of efficient matrix operations. \u2022 The <b>MDP</b> formulation ...", "dateLastCrawled": "2022-01-07T15:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Cognitive Radio Jamming Mitigation using <b>Markov Decision Process</b> and ...", "url": "http://www.sic.rma.ac.be/~vlenir/publications/Slimeni15b.pdf", "isFamilyFriendly": true, "displayUrl": "www.sic.rma.ac.be/~vlenir/publications/Slimeni15b.pdf", "snippet": "<b>Markov decision process</b> (<b>MDP</b>) has been widely exploited as a stochastic <b>tool</b> to model the CR <b>decision</b> making problem in jamming scenarios with \ufb01xed strategy, i.e. assuming that the jammer preserves the same tactic. The <b>MDP</b> is a discrete time stochastic control <b>process</b>. It provides a mathematical framework to model the <b>decision</b>", "dateLastCrawled": "2022-01-28T16:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Easy Affine <b>Markov</b> <b>Decision</b> Processes: <b>Theory</b> | Request PDF", "url": "https://www.researchgate.net/publication/316716307_Easy_Affine_Markov_Decision_Processes_Theory", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/.../316716307_Easy_Affine_<b>Markov</b>_<b>Decision</b>_<b>Process</b>es_<b>Theory</b>", "snippet": "<b>Markov decision process</b> (<b>MDP</b>) models, used previously to integrate stochastic stand growth and prices, <b>can</b> be extended to include variable interest rates as well. This method was applied to ...", "dateLastCrawled": "2021-12-17T15:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A machine <b>tool</b> <b>matching method in cloud manufacturing</b> using <b>Markov</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0736584519300924", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0736584519300924", "snippet": "It is not difficult to see that the machine <b>tool</b> demand-resource matching is the constructing and solving <b>process</b> of a dynamic variable granularity model, as well as a typical <b>Markov decision process</b> (<b>MDP</b>), which <b>can</b> perform actions to achieve the balance of production operation status according to the current real-time state of environment, and get rewards.", "dateLastCrawled": "2021-11-10T15:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "The Mathematics of 2048: <b>Optimal Play with Markov Decision Processes</b>", "url": "https://jdlm.info/articles/2018/03/18/markov-decision-process-2048.html", "isFamilyFriendly": true, "displayUrl": "https://<b>jdlm.info</b>/articles/2018/03/18/<b>markov-decision-process</b>-2048.html", "snippet": "So far in this series on the mathematics of 2048, we\u2019ve used <b>Markov</b> chains to learn that it takes at least 938.8 moves on average to win, and we\u2019ve explored the number of possible board configurations in the game using combinatorics and then exhaustive enumeration.. In this post, we\u2019ll use a mathematical framework called a <b>Markov Decision Process</b> to find provably optimal strategies for 2048 when played on the 2x2 and 3x3 boards, and also on the 4x4 board up to the 64 tile. For example ...", "dateLastCrawled": "2022-01-27T06:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Answer Set <b>Programming for Non-Stationary Markov Decision Processes</b> ...", "url": "https://deepai.org/publication/answer-set-programming-for-non-stationary-markov-decision-processes", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/.../answer-set-<b>programming-for-non-stationary-markov-decision-processes</b>", "snippet": "One formalism that <b>can</b> be used to model the kind of situations described above is a non-stationary <b>Markov Decision Process</b> (<b>MDP</b>), where the set of states represented by observations of the environment (facts) <b>can</b> suffer changes over time such that states <b>can</b> be added to, or removed from, the <b>decision</b> <b>process</b>.", "dateLastCrawled": "2022-01-31T11:47:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Why does <b>Markov Decision Process</b> matter in Reinforcement <b>Learning</b>? | by ...", "url": "https://towardsdatascience.com/why-does-malkov-decision-process-matter-in-reinforcement-learning-b111b46b41bd", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/why-does-malkov-<b>decision</b>-<b>process</b>-matter-in...", "snippet": "It is named by <b>analogy</b> to \u201cone-armed bandit\u201d(= a slot <b>machine</b>) although the framework has k levers instead of one. ... we introduce <b>Markov Decision Process</b>(<b>MDP</b>) to solve such a problem. An <b>MDP</b> consists of two elements; the agent and the environment. The agent is a learner or <b>decision</b>-maker. In the above example, the agent is the rabbit. The environment is everything surrounding the agent. In the example, the environment includes everything in the field where the rabbit is with food and ...", "dateLastCrawled": "2022-01-31T01:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Markov Decision Process</b>: How Does Value Iteration Work? | Baeldung on ...", "url": "https://www.baeldung.com/cs/mdp-value-iteration", "isFamilyFriendly": true, "displayUrl": "https://www.baeldung.com/cs/<b>mdp</b>-value-iteration", "snippet": "From this point, we can make an <b>analogy</b> with the <b>Markov</b> model since the solution for this problem is a sequence of actions. A <b>Markov Decision Process</b> is used to model the agent, considering that the agent itself generates a series of actions. In the real world, we can have observable, hidden, or partially observed states, depending on the ...", "dateLastCrawled": "2022-01-30T20:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Introduction to Machine Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/24/introduction-to-machine-learning-2/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/24/<b>introduction-to-machine-learning</b>-2", "snippet": "<b>Markov decision process</b> (<b>MDP</b>): In reinforcement <b>learning</b>, <b>MDP</b> is a mathematical framework for modeling <b>decision</b>-making of an agent in situations or environments where outcomes are partly random and partly under control. In this model, environment is modeled as a set of states and actions that can be performed by an agent to control the system\u2019s state. The objective is to control the system in such a way that the agent\u2019s total payoff is maximized.", "dateLastCrawled": "2022-01-17T05:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "CSE599i: Online and Adaptive <b>Machine</b> <b>Learning</b> Winter 2018 Lecture 19 ...", "url": "https://courses.cs.washington.edu/courses/cse599i/18wi/resources/lecture19/lecture19.pdf", "isFamilyFriendly": true, "displayUrl": "https://courses.cs.washington.edu/courses/cse599i/18wi/resources/lecture19/lecture19.pdf", "snippet": "1.1Summary of <b>Markov</b> <b>Decision</b> Processes A <b>Markov Decision Process</b> (<b>MDP</b>) is a probabilistic model for reward-incentivized, memoryless, sequential <b>decision</b>-making. An <b>MDP</b> models a scenario in which an agent (the <b>decision</b> maker) iteratively observes the", "dateLastCrawled": "2021-09-07T09:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Markov decision process</b>: value iteration with code implementation | by ...", "url": "https://medium.com/@ngao7/markov-decision-process-value-iteration-2d161d50a6ff", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@ngao7/<b>markov-decision-process</b>-value-iteration-2d161d50a6ff", "snippet": "<b>Markov decision process</b>, <b>MDP</b>, value iteration, policy iteration, policy evaluation, policy improvement, sweep, iterative policy evaluation, policy, optimal policy ...", "dateLastCrawled": "2022-01-08T01:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Markov decision process</b>: policy iteration with code implementation | by ...", "url": "https://medium.com/@ngao7/markov-decision-process-policy-iteration-42d35ee87c82?source=post_internal_links---------0-------------------------------", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@ngao7/<b>markov-decision-process</b>-policy-iteration-42d35ee87c82?source=...", "snippet": "<b>Markov decision process</b>: policy iteration with code implementation . Nan. Dec 19, 2021 \u00b7 16 min read. In today\u2019s story we focus on policy iteration of <b>MDP</b>. We are still using the grid world ...", "dateLastCrawled": "2022-01-22T13:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "18.1. <b>Markov Decision Process</b> (<b>MDP</b>) \u2014 Dive into Deep <b>Learning</b> 0.17.0 ...", "url": "http://preview.d2l.ai.s3-website-us-west-2.amazonaws.com/d2l-en/master/chapter_reinforcement_learning/mdp.html", "isFamilyFriendly": true, "displayUrl": "preview.d2l.ai.s3-website-us-west-2.amazonaws.com/...reinforcement_<b>learning</b>/<b>mdp</b>.html", "snippet": "In this section, we will discuss how to formulate reinforcement <b>learning</b> problems using <b>Markov</b> <b>decision</b> processes (MDPs) and describe in detail various components of MDPs. Definition of an <b>MDP</b> \u00b6 A <b>Markov decision process</b> (<b>MDP</b>) is a model for how the state of a system evolves as different actions are applied to it.", "dateLastCrawled": "2022-01-27T05:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Meta-<b>Learning</b> as a <b>Markov Decision Process</b>", "url": "https://hal.archives-ouvertes.fr/tel-02422144v2/document", "isFamilyFriendly": true, "displayUrl": "https://hal.archives-ouvertes.fr/tel-02422144v2/document", "snippet": "Meta-<b>Learning</b> as a <b>Markov Decision Process</b>. <b>Machine</b> <b>Learning</b> [cs.LG]. Uni-versit\u00e9 Paris Saclay (COmUE), 2019. English. \uffffNNT: 2019SACLS588\uffff. \ufffftel-02422144v2\uffff I would like to dedicate this thesis to my loving parents doctorat CLS588 Meta-<b>Learning</b> as a <b>Markov Decision Process</b> Th\u00e8se de doctorat de l\u2019Universit\u00e9 Paris-Saclay pr\u00e9par\u00e9e \u00e0 l\u2019Universit\u00e9 Paris-Sud Ecole doctorale n 580 Sciences et Technologies de l\u2019Information et de la Communication (STIC) Sp\u00e9cialit\u00e9 de doctorat ...", "dateLastCrawled": "2022-01-13T07:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>machine</b> <b>learning</b> - Why is the <b>optimal policy</b> in <b>Markov Decision Process</b> ...", "url": "https://stats.stackexchange.com/questions/132890/why-is-the-optimal-policy-in-markov-decision-process-mdp-independent-of-the-i", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/132890", "snippet": "The intuition behind the argument saying that the <b>optimal policy</b> is independent of initial state is the following: The <b>optimal policy</b> is defined by a function that selects an action for every possible state and actions in different states are independent.. Formally speaking, for an unknown initial distribution, the value function to maximize would be the following (not conditioned on initial state)", "dateLastCrawled": "2022-01-25T23:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Real-life <b>examples</b> of <b>Markov</b> <b>Decision</b> Processes - Cross Validated", "url": "https://stats.stackexchange.com/questions/145122/real-life-examples-of-markov-decision-processes", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/145122", "snippet": "A Markovian <b>Decision</b> <b>Process</b> indeed has to do with going from one state to another and is mainly used for planning and <b>decision</b> making. The theory. Just repeating the theory quickly, an <b>MDP</b> is: $$\\text{<b>MDP</b>} = \\langle S,A,T,R,\\gamma \\rangle$$", "dateLastCrawled": "2022-01-24T10:32:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Overview: Representation Techniques", "url": "https://webcms3.cse.unsw.edu.au/static/uploads/course/COMP4418/17s2/9c0e19e0e02df66fffb5d0bd4c20697922f5ffbf9a602b66bec3f74ac83fb77c/DecisionMaking.pdf", "isFamilyFriendly": true, "displayUrl": "https://webcms3.cse.unsw.edu.au/static/uploads/course/COMP4418/17s2/9c0e19e0e02df66...", "snippet": "<b>Markov Decision Process MDP is like</b> a Markov process, except every round we make a decision Transition probabilities depend on actions taken P(St+1 = S&#39; | St = s, At = a) = P(S, a, S&#39;) Rewards for every state, action pair u(St = s, At = a) Discount factor \u03b4 Example. A <b>machine</b> can be in one of three states: good, deteriorating, broken Can take ...", "dateLastCrawled": "2022-01-21T05:20:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], []], "all_bing_queries": ["+(markov decision process (mdp))  is like +(decision theory tool)", "+(markov decision process (mdp)) is similar to +(decision theory tool)", "+(markov decision process (mdp)) can be thought of as +(decision theory tool)", "+(markov decision process (mdp)) can be compared to +(decision theory tool)", "machine learning +(markov decision process (mdp) AND analogy)", "machine learning +(\"markov decision process (mdp) is like\")", "machine learning +(\"markov decision process (mdp) is similar\")", "machine learning +(\"just as markov decision process (mdp)\")", "machine learning +(\"markov decision process (mdp) can be thought of as\")", "machine learning +(\"markov decision process (mdp) can be compared to\")"]}