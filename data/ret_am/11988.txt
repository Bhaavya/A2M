{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Analyzing Multi-Head Self-Attention: Specialized Heads</b> Do the ...", "url": "https://www.researchgate.net/publication/333337639_Analyzing_Multi-Head_Self-Attention_Specialized_Heads_Do_the_Heavy_Lifting_the_Rest_Can_Be_Pruned", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/333337639_<b>Analyzing_Multi-Head_Self-Attention</b>...", "snippet": "<b>Multi-head</b> <b>self-attention</b> is a key component of the Transformer, a state-of-the-art architecture for neural machine translation. In this work we evaluate the contribution made by individual ...", "dateLastCrawled": "2021-08-10T08:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Deep reinforcement and transfer learning for abstractive text ...", "url": "https://www.sciencedirect.com/science/article/pii/S0885230821000796", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0885230821000796", "snippet": "One of the commonly used forms of <b>attention</b> is the <b>multi-head</b> <b>self-attention</b> used by the standard Transformer , which uses <b>multiple</b> <b>self-attention</b> layers running in parallel to learn various <b>attention</b> distributions over the source sequence. Moreover, Sparse fixed and stride attentions are also proposed for better dealing with long sequences and reduce memory complexity. Furthermore, other types of <b>attention</b> are proposed and discussed in", "dateLastCrawled": "2022-02-02T22:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Attention</b>? <b>Attention</b>!", "url": "https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html", "isFamilyFriendly": true, "displayUrl": "https://lilianweng.github.io/lil-log/2018/06/24/<b>attention</b>-<b>attention</b>.html", "snippet": "(&amp;) Also, referred to as \u201cintra-<b>attention</b>\u201d in Cheng et al., 2016 and some other papers. <b>Self-Attention</b>. <b>Self-attention</b>, also known as intra-<b>attention</b>, is an <b>attention</b> mechanism relating different positions of a <b>single</b> sequence in order to compute a representation of the same sequence.It has been shown to be very useful in machine reading, abstractive summarization, or image description generation.", "dateLastCrawled": "2022-01-29T18:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "FROM Pre-trained Word Embeddings TO Pre-trained Language Models \u2014 Focus ...", "url": "https://towardsdatascience.com/from-pre-trained-word-embeddings-to-pre-trained-language-models-focus-on-bert-343815627598", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/from-pre-trained-word-<b>embedding</b>s-to-pre-trained...", "snippet": "So the concatenated vector resulting from all <b>Multi-head</b> <b>Self-Attention</b> process would be [length of input sequences] x ([64] x [8]) = [length of input sequences] x ([512]) How <b>Self-Attention</b> works? Query q: the query vector q encodes the word/position on the left that is <b>paying</b> <b>attention</b>, i.e. the one that is \u201cquerying\u201d the other words. In the example above, the query vector for \u201cthe\u201d (the selected word) is highlighted. Key k: the key vector k encodes the word on the right to which ...", "dateLastCrawled": "2022-01-29T16:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Transfer learning in NLP Part III: Fine-tuning a pre-trained model ...", "url": "https://mohcinemadkour.github.io/posts/2019/07/Machine%20Learning,%20July%202019,%20Transfer%20learning,%20filtering/", "isFamilyFriendly": true, "displayUrl": "https://mohcinemadkour.github.io/posts/2019/07/Machine Learning, July 2019, Transfer...", "snippet": "And for <b>self-attention</b> in the decoder, K, Q and V all come from the decoder, but here it is masked to prevent using future values &quot;&quot;&quot; def __init__ (self, h, d_model, dropout = 0.1, scale = False): &quot;&quot;&quot;Constructor for multi-headed <b>attention</b>:param h: The number of heads:param d_model: The model hidden size:param dropout (``float``): The amount of dropout to use:param attn_fn: A function to apply <b>attention</b>, defaults to SDP &quot;&quot;&quot; super (MultiHeadedAttention, self). __init__ assert d_model % h == 0 ...", "dateLastCrawled": "2022-01-12T09:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "New submissions for Tue, 16 Nov 21 \u00b7 Issue #463 \u00b7 kobiso/daily-arxiv ...", "url": "https://github.com/kobiso/daily-arxiv-noti/issues/463", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/kobiso/daily-arxiv-noti/issues/463", "snippet": "In this paper we propose a novel <b>self-attention</b> module that can be easily integrated in virtually every convolutional neural network and that is specifically designed for computer vision, the LHC: Local (<b>multi) Head</b> Channel (<b>self-attention</b>). LHC is based on two main ideas: first, we think that in computer vision the best way to leverage the <b>self-attention</b> paradigm is the channel-wise application instead of the more explored spatial <b>attention</b> and that convolution will not be replaced by ...", "dateLastCrawled": "2022-02-03T15:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Attention</b> is all you need: Discovering the Transformer model | Eduardo ...", "url": "https://edumunozsala.github.io/BlogEms/transformer/attention/encoder-decoder/tensorflow%202/2020/10/29/Transformer-NMT-en-es.html", "isFamilyFriendly": true, "displayUrl": "https://edumunozsala.github.io/BlogEms/transformer/<b>attention</b>/encoder-decoder/tensorflow...", "snippet": "Instead, we would <b>like</b> to attend to different segments of the words. We can give the <b>self attention</b> greater power of discrimination, by combining several <b>self attention</b> heads, dividing the words vectors into a fixed number (h, number of heads) of chunks, and then <b>self-attention</b> is applied on the corresponding chunks, using Q, K and V sub-matrices.", "dateLastCrawled": "2022-01-21T04:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "GTA: Graph Truncated <b>Attention</b> for Retrosynthesis", "url": "https://www.readkong.com/page/gta-graph-truncated-attention-for-retrosynthesis-8732215", "isFamilyFriendly": true, "displayUrl": "https://www.readkong.com/page/gta-graph-truncated-<b>attention</b>-for-retrosynthesis-8732215", "snippet": "Then, a <b>multi-head</b> <b>self-attention</b> model or Trans- difference is that the template is a set of reaction center and former (Vaswani et al. 2017a) was adopted. (Karpov, Godin, functional groups, but G2Gs predict them separately. Thus, and Tetko 2019) reported that character-wise tokenization G2Gs share the coverage limitation issue with template- and cyclic learning rate scheduling improved the model per- based models. formance without modification of the Transformer. (Chen In this paper, we ...", "dateLastCrawled": "2022-01-20T11:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "New submissions for Fri, 1 Oct 21 \u00b7 Issue #431 \u00b7 kobiso/daily-arxiv ...", "url": "https://github.com/kobiso/daily-arxiv-noti/issues/431", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/kobiso/daily-arxiv-noti/issues/431", "snippet": "In this work, we investigate the problem of approximating the two central components of the Transformer -- <b>multi-head</b> <b>self-attention</b> and point-wise feed-forward transformation, with reduced parameter space and computational complexity. We build upon recent developments in analyzing deep neural networks as numerical solvers of ordinary differential equations. Taking advantage of an analogy between Transformer stages and the evolution of a dynamical system of <b>multiple</b> interacting particles, we ...", "dateLastCrawled": "2021-12-31T06:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "PinText 2: Attentive Bag of Annotations Embedding", "url": "https://dlp-kdd.github.io/dlp-kdd2020/assets/pdf/a15-zhuang.pdf", "isFamilyFriendly": true, "displayUrl": "https://dlp-kdd.github.io/dlp-kdd2020/assets/pdf/a15-zhuang.pdf", "snippet": "problems. For example, if the <b>object</b> is a sequence <b>like</b> a natural lan-guage sentence, then probably word order is important. If the <b>object</b> is a collection, the learned embedding should be independent of the text orders inside the collection. As a visual discovery platform, it is not uncommon that pins do not have direct text information associated with it. We rely on an annotation system1 which predicts a set of annotation terms describing the content of the pin. Figure 1 gives a real ...", "dateLastCrawled": "2021-12-03T23:39:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Analyzing Multi-Head Self-Attention: Specialized Heads</b> Do the ...", "url": "https://www.researchgate.net/publication/333337639_Analyzing_Multi-Head_Self-Attention_Specialized_Heads_Do_the_Heavy_Lifting_the_Rest_Can_Be_Pruned", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/333337639_<b>Analyzing_Multi-Head_Self-Attention</b>...", "snippet": "<b>Multi-head</b> <b>self-attention</b> is a key component of the Transformer, a state-of-the-art architecture for neural machine translation. In this work we evaluate the contribution made by individual ...", "dateLastCrawled": "2021-08-10T08:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "FROM Pre-trained Word Embeddings TO Pre-trained Language Models \u2014 Focus ...", "url": "https://towardsdatascience.com/from-pre-trained-word-embeddings-to-pre-trained-language-models-focus-on-bert-343815627598", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/from-pre-trained-word-<b>embedding</b>s-to-pre-trained...", "snippet": "So the concatenated vector resulting from all <b>Multi-head</b> <b>Self-Attention</b> process would be [length of input sequences] x ([64] x [8]) = [length of input sequences] x ([512]) How <b>Self-Attention</b> works? Query q: the query vector q encodes the word/position on the left that is <b>paying</b> <b>attention</b>, i.e. the one that is \u201cquerying\u201d the other words. In the example above, the query vector for \u201cthe\u201d (the selected word) is highlighted. Key k: the key vector k encodes the word on the right to which ...", "dateLastCrawled": "2022-01-29T16:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Attention</b>? <b>Attention</b>!", "url": "https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html", "isFamilyFriendly": true, "displayUrl": "https://lilianweng.github.io/lil-log/2018/06/24/<b>attention</b>-<b>attention</b>.html", "snippet": "(&amp;) Also, referred to as \u201cintra-<b>attention</b>\u201d in Cheng et al., 2016 and some other papers. <b>Self-Attention</b>. <b>Self-attention</b>, also known as intra-<b>attention</b>, is an <b>attention</b> mechanism relating different positions of a <b>single</b> sequence in order to compute a representation of the same sequence.It has been shown to be very useful in machine reading, abstractive summarization, or image description generation.", "dateLastCrawled": "2022-01-29T18:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Transfer learning in NLP Part III: Fine-tuning a pre-trained model ...", "url": "https://mohcinemadkour.github.io/posts/2019/07/Machine%20Learning,%20July%202019,%20Transfer%20learning,%20filtering/", "isFamilyFriendly": true, "displayUrl": "https://mohcinemadkour.github.io/posts/2019/07/Machine Learning, July 2019, Transfer...", "snippet": "And for <b>self-attention</b> in the decoder, K, Q and V all come from the decoder, but here it is masked to prevent using future values &quot;&quot;&quot; def __init__ (self, h, d_model, dropout = 0.1, scale = False): &quot;&quot;&quot;Constructor for multi-headed <b>attention</b>:param h: The number of heads:param d_model: The model hidden size:param dropout (``float``): The amount of dropout to use:param attn_fn: A function to apply <b>attention</b>, defaults to SDP &quot;&quot;&quot; super (MultiHeadedAttention, self). __init__ assert d_model % h == 0 ...", "dateLastCrawled": "2022-01-12T09:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Yllias Chali - ACL Anthology", "url": "https://aclanthology.org/people/y/yllias-chali/", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/<b>people</b>/y/yllias-chali", "snippet": "The first layer is a transformer model containing 6 stacked identical layers with <b>multi-head</b> <b>self attention</b>, while the second-layer is a seq2seq model with gated recurrent units (GRU-RNN). The transformer encoder layer learns to capture long-term dependencies, together with syntactic and semantic properties of the input sentence. This rich vector representation learned by the transformer serves as input to the GRU-RNN encoder responsible for producing the state vector for decoding ...", "dateLastCrawled": "2022-01-22T17:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "New submissions for Tue, 16 Nov 21 \u00b7 Issue #463 \u00b7 kobiso/daily-arxiv ...", "url": "https://github.com/kobiso/daily-arxiv-noti/issues/463", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/kobiso/daily-arxiv-noti/issues/463", "snippet": "In this paper we propose a novel <b>self-attention</b> module that can be easily integrated in virtually every convolutional neural network and that is specifically designed for computer vision, the LHC: Local (<b>multi) Head</b> Channel (<b>self-attention</b>). LHC is based on two main ideas: first, we think that in computer vision the best way to leverage the <b>self-attention</b> paradigm is the channel-wise application instead of the more explored spatial <b>attention</b> and that convolution will not be replaced by ...", "dateLastCrawled": "2022-02-03T15:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A Video Is <b>Worth Three Views: Trigeminal Transformers for</b> Video-based ...", "url": "https://deepai.org/publication/a-video-is-worth-three-views-trigeminal-transformers-for-video-based-person-re-identification", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/a-video-is-<b>worth-three-views-trigeminal-transformers</b>...", "snippet": "The spatial <b>self-attention</b> pooling and temporal <b>self-attention</b> pooling have the <b>similar</b> structures and are shown ... the feature with position is passed through the <b>multi-head</b> <b>self-attention</b> layer. In each head, the feature is firstly feed into three linear transformations to generate feature . Q, K and V, where Q, K, V \u2208 R T \u00d7 d. d = C N h and N h is the number of heads. The <b>self attention</b> operation is defined as: A h = Softmax (Q K T \u221a d) V, (6) The outputs of <b>multiple</b> heads, A 1, \u22ef ...", "dateLastCrawled": "2022-01-30T20:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "New submissions for Fri, 1 Oct 21 \u00b7 Issue #431 \u00b7 kobiso/daily-arxiv ...", "url": "https://github.com/kobiso/daily-arxiv-noti/issues/431", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/kobiso/daily-arxiv-noti/issues/431", "snippet": "In this work, we investigate the problem of approximating the two central components of the Transformer -- <b>multi-head</b> <b>self-attention</b> and point-wise feed-forward transformation, with reduced parameter space and computational complexity. We build upon recent developments in analyzing deep neural networks as numerical solvers of ordinary differential equations. Taking advantage of an analogy between Transformer stages and the evolution of a dynamical system of <b>multiple</b> interacting particles, we ...", "dateLastCrawled": "2021-12-31T06:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "GTA: Graph Truncated <b>Attention</b> for Retrosynthesis", "url": "https://www.readkong.com/page/gta-graph-truncated-attention-for-retrosynthesis-8732215", "isFamilyFriendly": true, "displayUrl": "https://www.readkong.com/page/gta-graph-truncated-<b>attention</b>-for-retrosynthesis-8732215", "snippet": "Then, a <b>multi-head</b> <b>self-attention</b> model or Trans- difference is that the template is a set of reaction center and former (Vaswani et al. 2017a) was adopted. (Karpov, Godin, functional groups, but G2Gs predict them separately. Thus, and Tetko 2019) reported that character-wise tokenization G2Gs share the coverage limitation issue with template- and cyclic learning rate scheduling improved the model per- based models. formance without modification of the Transformer. (Chen In this paper, we ...", "dateLastCrawled": "2022-01-20T11:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Attention</b> is all you need: Discovering the Transformer model | Eduardo ...", "url": "https://edumunozsala.github.io/BlogEms/transformer/attention/encoder-decoder/tensorflow%202/2020/10/29/Transformer-NMT-en-es.html", "isFamilyFriendly": true, "displayUrl": "https://edumunozsala.github.io/BlogEms/transformer/<b>attention</b>/encoder-decoder/tensorflow...", "snippet": "Instead of <b>paying</b> <b>attention</b> to the last state of the encoder as is usually done with the RNNs, in each step of the decoder we look at all the states of the encoder, being able to access information about all the elements of the input sequence. This is what <b>attention</b> does, it extracts information from the whole sequence, a weighted sum of all the past encoder states. This allows the decoder to assign greater weight or importance to a certain element of the input for each element of the output ...", "dateLastCrawled": "2022-01-21T04:21:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "NLP &quot;Tranformer and <b>Self-Attention</b>&quot; - <b>Programmer Sought</b>", "url": "https://www.programmersought.com/article/33326345603/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>programmersought</b>.com/article/33326345603", "snippet": "2: <b>Multi-head</b> <b>self-attention</b> (muti-head <b>self-attention</b>) To put it simply, we want to look at the positional <b>attention</b> from a perspective that we cannot. The implementation is also simple, that is, set up <b>multiple</b> completely independent Wq, Wk, Wv matrices, <b>multiple</b> groups of independent operations, and get their own output B respectively.", "dateLastCrawled": "2022-01-23T16:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "What Is Input <b>Attention</b> - Know Anything | WhatisAnything.com", "url": "https://whatisanything.com/what-is-input-attention/", "isFamilyFriendly": true, "displayUrl": "https://whatisanything.com/what-is-input-<b>attention</b>", "snippet": "<b>Multi-head</b> <b>Attention</b> is a module for <b>attention</b> mechanisms which runs through an <b>attention</b> mechanism several times in parallel. Intuitively, <b>multiple</b> <b>attention</b> heads allows for attending to parts of the sequence differently (e.g. longer-term dependencies versus shorter-term dependencies). What is stimulus input psychology? Perception is the way that sensory information is chosen and transformed so that it has meaning. Once sensory input starts, an individual uses perceptual processes to ...", "dateLastCrawled": "2021-12-24T04:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Multi-Granularity Self-Attention for Neural Machine Translation</b> ...", "url": "https://www.researchgate.net/publication/336998986_Multi-Granularity_Self-Attention_for_Neural_Machine_Translation", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/336998986_Multi-Granularity_<b>Self-Attention</b>...", "snippet": "Hao et al. (2019a) further make use of the <b>multi-head</b> <b>attention</b> to form the multi-granularity <b>self-attention</b>, to capture the different granularity phrases in source sentences. The difference is ...", "dateLastCrawled": "2021-12-20T22:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Attention</b>? <b>Attention</b>!", "url": "https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html", "isFamilyFriendly": true, "displayUrl": "https://lilianweng.github.io/lil-log/2018/06/24/<b>attention</b>-<b>attention</b>.html", "snippet": "(&amp;) Also, referred to as \u201cintra-<b>attention</b>\u201d in Cheng et al., 2016 and some other papers. <b>Self-Attention</b>. <b>Self-attention</b>, also known as intra-<b>attention</b>, is an <b>attention</b> mechanism relating different positions of a <b>single</b> sequence in order to compute a representation of the same sequence.It has been shown to be very useful in machine reading, abstractive summarization, or image description generation.", "dateLastCrawled": "2022-01-29T18:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "160 questions with answers in <b>ATTENTION</b> | Science topic", "url": "https://www.researchgate.net/topic/Attention", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/topic/<b>Attention</b>", "snippet": "How does <b>multi-head</b> <b>attention</b> on &quot;<b>multiple</b> <b>attention</b> axes&quot; works ? Question. 1 answer. Apr 8, 2021; Hello, I would like to apply an <b>self-attention</b> mechanism on a multichannel audio spectrogram, so ...", "dateLastCrawled": "2022-02-03T07:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "New submissions for Tue, 23 Nov 21 \u00b7 Issue #468 \u00b7 kobiso/daily-arxiv ...", "url": "https://github.com/kobiso/daily-arxiv-noti/issues/468", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/kobiso/daily-arxiv-noti/issues/468", "snippet": "Based on these findings, we develop an efficient and flexible MViT architecture using multi-scale feature processing and deformable <b>self-attention</b> that <b>can</b> adaptively generate proposals given a specific language query. We show the significance of MViT proposals in a diverse range of applications including open-world <b>object</b> detection, salient and camouflage <b>object</b> detection, supervised and self-supervised detection tasks. Further, MViTs offer enhanced interactability with intelligible text ...", "dateLastCrawled": "2021-12-17T04:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Accepted Papers: Main Conference</b> | COLING\u20192020", "url": "https://coling2020.org/pages/accepted_papers_main_conference.html", "isFamilyFriendly": true, "displayUrl": "https://coling2020.org/pages/<b>accepted_papers_main_conference</b>.html", "snippet": "The triple-level <b>self-attention</b> treats head entity, relation, and tail entity as a sequence and captures the dependency within a triple. At the same time the pseudo residual connection retains primitive semantic features. Furthermore, to deal with symmetric and antisymmetric relations, two schemas of score function are designed via a position-adaptive mechanism. Experimental results on public datasets demonstrate that our model <b>can</b> produce expressive knowledge embedding and significantly ...", "dateLastCrawled": "2022-01-30T19:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Proceedings of the 2021 Conference on Empirical Methods in Natural ...", "url": "https://aclanthology.org/volumes/2021.emnlp-main/", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/volumes/2021.emnlp-main", "snippet": "Recent efforts to improve the efficiency of <b>self-attention</b> have led to a proliferation of long-range Transformer language models, which <b>can</b> process much longer sequences than models of the past. However, the ways in which such models take advantage of the long-range context remain unclear. In this paper, we perform a fine-grained analysis of two long-range Transformer language models (including the Routing Transformer, which achieves state-of-the-art perplexity on the PG-19 long-sequence LM ...", "dateLastCrawled": "2022-01-24T04:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) Deep <b>Learning for NLP and Speech Recognition</b> | William Jacome ...", "url": "https://www.academia.edu/43190210/Deep_Learning_for_NLP_and_Speech_Recognition", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/43190210/Deep_<b>Learning_for_NLP_and_Speech_Recognition</b>", "snippet": "Academia.edu is a platform for academics to share research papers.", "dateLastCrawled": "2022-02-02T18:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "NeurIPS 2021 papers", "url": "https://tanelp.github.io/neurips2021/", "isFamilyFriendly": true, "displayUrl": "https://tanelp.github.io/neurips2021", "snippet": "Thus, we <b>can</b> simultaneously process <b>multiple</b> objects&#39; matching and segmentation decoding as efficiently as processing a <b>single</b> <b>object</b>. For sufficiently modeling multi-<b>object</b> association, a Long Short-Term Transformer is designed for constructing hierarchical matching and propagation. We conduct extensive experiments on both multi-<b>object</b> and <b>single</b>-<b>object</b> benchmarks to examine AOT variant networks with different complexities. Particularly, our R50-AOT-L outperforms all the state-of-the-art ...", "dateLastCrawled": "2022-02-03T00:55:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "New submissions for Tue, 16 Nov 21 \u00b7 Issue #463 \u00b7 kobiso/daily-arxiv ...", "url": "https://github.com/kobiso/daily-arxiv-noti/issues/463", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/kobiso/daily-arxiv-noti/issues/463", "snippet": "In this paper we propose a novel <b>self-attention</b> module that <b>can</b> be easily integrated in virtually every convolutional neural network and that is specifically designed for computer vision, the LHC: Local (<b>multi) Head</b> Channel (<b>self-attention</b>). LHC is based on two main ideas: first, we think that in computer vision the best way to leverage the <b>self-attention</b> paradigm is the channel-wise application instead of the more explored spatial <b>attention</b> and that convolution will not be replaced by ...", "dateLastCrawled": "2022-02-03T15:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A Video Is <b>Worth Three Views: Trigeminal Transformers for</b> Video-based ...", "url": "https://deepai.org/publication/a-video-is-worth-three-views-trigeminal-transformers-for-video-based-person-re-identification", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/a-video-is-<b>worth-three-views-trigeminal-transformers</b>...", "snippet": "Each block of transformer is composed of a <b>multi-head</b> <b>self-attention</b> layer, a ... The temporal or spatial-temporal transformer is applied to the baseline for <b>single</b>-view observation. We <b>can</b> see that, the temporal and spatial-temporal transformer are sensitive to the length of the sequences. When varying the length of the sequence to 8, both of temporal and spatial-temporal transformers get best performance. Effect of the size of spatial feature map. We also perform ablation experiments to ...", "dateLastCrawled": "2022-01-30T20:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>Multiple-Aspect Attentional Graph Neural Networks</b> for Online ...", "url": "https://www.researchgate.net/publication/341299009_Multiple-Aspect_Attentional_Graph_Neural_Networks_for_Online_Social_Network_User_Localization", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/341299009_<b>Multiple</b>-Aspect_<b>Attention</b>al_Graph...", "snippet": "The framework of MAGNN fusing the content features and networks information with <b>multi-head</b> <b>attention</b> to predict the home location of Twitter users. The results on Macro-Recall (a) and Macro-F1 (b).", "dateLastCrawled": "2022-01-10T04:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Multiple Relational Attention Network for Multi</b>-task Learning | Request PDF", "url": "https://www.researchgate.net/publication/334715539_Multiple_Relational_Attention_Network_for_Multi-task_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/334715539_<b>Multiple</b>_Relational_<b>Attention</b>...", "snippet": "Similarly, MRAN [41] applies <b>multi-head</b> <b>self-attention</b> to learn different representation subspaces at different feature sets. Cross-Stitch [20] uses linear cross-stitch units to learn an optimal ...", "dateLastCrawled": "2022-01-31T06:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "GTA: Graph Truncated <b>Attention</b> for Retrosynthesis", "url": "https://www.readkong.com/page/gta-graph-truncated-attention-for-retrosynthesis-8732215", "isFamilyFriendly": true, "displayUrl": "https://www.readkong.com/page/gta-graph-truncated-<b>attention</b>-for-retrosynthesis-8732215", "snippet": "Then, a <b>multi-head</b> <b>self-attention</b> model or Trans- difference is that the template is a set of reaction center and former (Vaswani et al. 2017a) was adopted. (Karpov, Godin, functional groups, but G2Gs predict them separately. Thus, and Tetko 2019) reported that character-wise tokenization G2Gs share the coverage limitation issue with template- and cyclic learning rate scheduling improved the model per- based models. formance without modification of the Transformer. (Chen In this paper, we ...", "dateLastCrawled": "2022-01-20T11:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "PinText 2: Attentive Bag of Annotations Embedding", "url": "https://dlp-kdd.github.io/dlp-kdd2020/assets/pdf/a15-zhuang.pdf", "isFamilyFriendly": true, "displayUrl": "https://dlp-kdd.github.io/dlp-kdd2020/assets/pdf/a15-zhuang.pdf", "snippet": "concrete <b>object</b> to a real vector representation, including Graph-SAGE embedding [12] on pins, Act-A-Like embedding [7] on users, Universal Visual Embedding [39] on images, and PinText embed-ding [40] on text snippets like search query or pin\u2019s title, are super useful due to the fact that they <b>can</b> be easily plugged into existing models or systems taking real vectors as input. They also produce off-the-shelf feature vectors for a cold start on a new application. Those embedding solutions ...", "dateLastCrawled": "2021-12-03T23:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "New submissions for Fri, 1 Oct 21 \u00b7 Issue #431 \u00b7 kobiso/daily-arxiv ...", "url": "https://github.com/kobiso/daily-arxiv-noti/issues/431", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/kobiso/daily-arxiv-noti/issues/431", "snippet": "In this work, we investigate the problem of approximating the two central components of the Transformer -- <b>multi-head</b> <b>self-attention</b> and point-wise feed-forward transformation, with reduced parameter space and computational complexity. We build upon recent developments in analyzing deep neural networks as numerical solvers of ordinary differential equations. Taking advantage of an analogy between Transformer stages and the evolution of a dynamical system of <b>multiple</b> interacting particles, we ...", "dateLastCrawled": "2021-12-31T06:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Attention</b> is all you need: Discovering the Transformer model | Eduardo ...", "url": "https://edumunozsala.github.io/BlogEms/transformer/attention/encoder-decoder/tensorflow%202/2020/10/29/Transformer-NMT-en-es.html", "isFamilyFriendly": true, "displayUrl": "https://edumunozsala.github.io/BlogEms/transformer/<b>attention</b>/encoder-decoder/tensorflow...", "snippet": "Instead of <b>paying</b> <b>attention</b> to the last state of the encoder as is usually done with the RNNs, in each step of the decoder we look at all the states of the encoder, being able to access information about all the elements of the input sequence. This is what <b>attention</b> does, it extracts information from the whole sequence, a weighted sum of all the past encoder states. This allows the decoder to assign greater weight or importance to a certain element of the input for each element of the output ...", "dateLastCrawled": "2022-01-21T04:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Improving Face-Based Age Estimation with <b>Attention</b>-Based Dynamic Patch ...", "url": "https://vertexdoc.com/doc/improving-face-based-age-estimation-with-attention-based-dynamic-patch-fusion", "isFamilyFriendly": true, "displayUrl": "https://vertexdoc.com/doc/improving-face-based-age-estimation-with-<b>attention</b>-based...", "snippet": "The AttentionNet dynamically locates and ranks age-specific patches by employing a novel Ranking-guided <b>Multi-Head</b> Hybrid <b>Attention</b> (RMHHA) mechanism. The FusionNet uses the discovered patches along with the facial image to predict the age of the subject. Since the proposed RMHHA mechanism ranks the discovered patches based on their importance, the length of the learning path of each patch in the FusionNet is proportional to the amount of information it carries (the longer, the more ...", "dateLastCrawled": "2022-01-19T07:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Dual <b>attention</b> convolutional network for action recognition - Li - 2020 ...", "url": "https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/iet-ipr.2019.0963", "isFamilyFriendly": true, "displayUrl": "https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/iet-ipr.2019.0963", "snippet": "<b>Self-attention</b> is an <b>attention</b> mechanism relating different positions of a <b>single</b> sequence so as to compute a new representation of this sequence. Inspired by this work, our module is designed to relate different frames of a video frame sequence to get a new representation of the video. The new representation contains the information of other frames and provides temporal features for later analysis.", "dateLastCrawled": "2021-10-24T17:25:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "10.5. <b>Multi-Head Attention</b> \u2014 Dive into Deep <b>Learning</b> 0.17.2 documentation", "url": "http://d2l.ai/chapter_attention-mechanisms/multihead-attention.html", "isFamilyFriendly": true, "displayUrl": "d2l.ai/chapter_attention-mechanisms/<b>multihead-attention</b>.html", "snippet": "This design is called <b>multi-head attention</b>, where each of the \\(h\\) attention pooling outputs is a head [Vaswani et al., 2017]. Using fully-connected layers to perform learnable linear transformations, Fig. 10.5.1 describes <b>multi-head attention</b>.", "dateLastCrawled": "2022-02-02T01:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Introduction to Transformers in Machine Learning</b>", "url": "https://www.machinecurve.com/index.php/2020/12/28/introduction-to-transformers-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/.../12/28/<b>introduction-to-transformers-in-machine-learning</b>", "snippet": "The masked <b>multi-head</b> attention segment, which performs <b>multi-head</b> <b>self-attention</b> on the outputs, but does so in a masked way, so that positions depend on the past only. The <b>multi-head</b> attention segment , which performs <b>multi-head</b> <b>self-attention</b> on a combination of the ( encoded ) inputs and the outputs, so that the model learns to correlate encoded inputs with desired outputs.", "dateLastCrawled": "2022-02-03T03:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Machine</b> <b>Learning</b> Glossary: Language Evaluation | Google Developers", "url": "https://developers.google.com/machine-learning/glossary/language", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine</b>-<b>learning</b>/glossary/language", "snippet": "Refer also to <b>self-attention</b> and <b>multi-head</b> <b>self-attention</b>, which are the building blocks of Transformers. B. bag of words. #language. A representation of the words in a phrase or passage, irrespective of order. For example, bag of words represents the following three phrases identically: the dog jumps; jumps the dog; dog jumps the; Each word is mapped to an index in a sparse vector, where the vector has an index for every word in the vocabulary. For example, the phrase the dog jumps is ...", "dateLastCrawled": "2022-01-29T02:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Attending to Attention. A summary of a revolutionary paper\u2026 | by Akash ...", "url": "https://towardsdatascience.com/attending-to-attention-eba798f0e940", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/attending-to-attention-eba798f0e940", "snippet": "The first is a <b>multi-head</b> <b>self-attention</b> mechanism, and the second is a simple, position-wise fully connected feed-forward network. We employ a residual connection around each of the two sub-layers, followed by layer normalization. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of ...", "dateLastCrawled": "2022-01-25T10:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "11. Attention Mechanisms \u2014 Dive into Deep <b>Learning</b> 0.17.0 documentation", "url": "http://preview.d2l.ai/d2l-en/master/chapter_attention-mechanisms/index.html", "isFamilyFriendly": true, "displayUrl": "preview.d2l.ai/d2l-en/master/chapter_attention-mechanisms/index.html", "snippet": "In the end, equipped with the more recent <b>multi-head</b> attention and <b>self-attention</b> designs, we will describe the transformer architecture based solely on attention mechanisms. Since their proposal in 2017, transformers have been pervasive in modern deep <b>learning</b> applications, such as in areas of language, vision, speech, and reinforcement <b>learning</b>.", "dateLastCrawled": "2022-01-18T14:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Intuitive Introduction to BERT \u2013 MachineCurve</b>", "url": "https://www.machinecurve.com/index.php/2021/01/04/intuitive-introduction-to-bert/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/index.php/2021/01/04/<b>intuitive-introduction-to-bert</b>", "snippet": "From our article about GPT: \u201cThe input is then served to a masked <b>multi-head</b> attention segment, which computes <b>self-attention</b> in a unidirectional way.Here, the residual is added and the result is layer normalized.\u201d Indeed, GPT (which uses the Transformer decoder segment autoregressively during pretraining) and the original Transformer (which performs Seq2Seq), apply a mask in one of the attention modules \u2013 the masked <b>multi-head</b> <b>self-attention</b> subsegment in the decoder segment.. For any ...", "dateLastCrawled": "2022-01-30T22:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "CoAtNet: how to perfectly combine CNNs and Transformers | by Leonardo ...", "url": "https://medium.com/codex/coatnet-how-to-perfectly-combine-cnns-and-transformer-9632e187ecbf", "isFamilyFriendly": true, "displayUrl": "https://medium.com/codex/coatnet-how-to-perfectly-combine-cnns-and-transformer-9632e...", "snippet": "The <b>multi-head</b> attention block computes <b>self-attention</b> several times with different weight matrices and then concatenates the results together, which are resized to the embedding dimension using ...", "dateLastCrawled": "2022-01-26T06:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Capturing Multi-Resolution Context by Dilated <b>Self-Attention</b>", "url": "https://www.merl.com/publications/docs/TR2021-036.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.merl.com/publications/docs/TR2021-036.pdf", "snippet": "to <b>machine</b> translation or language modeling, where close-by words are more likely to have a dependent relationship, while only a few distant words or word groups are relevant to trace the semantic con-text and syntax of a sentence [15]. This hypothesis is investigated in this work by combining re-stricted (or time-restricted) <b>self-attention</b> with a dilation mechanism, whereby a high <b>self-attention</b> resolution for neighboring frames and a lower <b>self-attention</b> resolution for distant information ...", "dateLastCrawled": "2021-12-02T00:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Transformers in NLP: A beginner friendly explanation | Towards Data Science", "url": "https://towardsdatascience.com/transformers-89034557de14", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>transformer</b>s-89034557de14", "snippet": "If you are looking for an <b>analogy</b> between <b>self attention</b> and attention, think of z serving the purpose of context vectors and not global alignment weights. The <b>Transformer</b> . \u26a0\ufe0f A word of caution: the contents of this image may appear exponentially more complicated than they are. We will break this scary beast down into small baby beasts and it will all make sense. (I promise #2) (left) The <b>Transformer</b> architecture. Source: paper. (right) An abstracted version of the same for better ...", "dateLastCrawled": "2022-02-02T13:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Dive into Deep <b>Learning</b> \u2014 Dive into Deep <b>Learning</b> 0.17.2 documentation", "url": "http://d2l.ai/", "isFamilyFriendly": true, "displayUrl": "d2l.ai", "snippet": "[Dec 2021] We added a new option to run this book for free: check out SageMaker Studio Lab. [Jul 2021] We have improved the content and added TensorFlow implementations up to Chapter 11. To keep track of the latest updates, just follow D2L&#39;s open-source project. [Jan 2021] Check out the brand-new Chapter: Attention Mechanisms.We have also added PyTorch implementations.", "dateLastCrawled": "2022-01-30T00:18:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(multi-head self-attention)  is like +(multiple people paying attention to a single object)", "+(multi-head self-attention) is similar to +(multiple people paying attention to a single object)", "+(multi-head self-attention) can be thought of as +(multiple people paying attention to a single object)", "+(multi-head self-attention) can be compared to +(multiple people paying attention to a single object)", "machine learning +(multi-head self-attention AND analogy)", "machine learning +(\"multi-head self-attention is like\")", "machine learning +(\"multi-head self-attention is similar\")", "machine learning +(\"just as multi-head self-attention\")", "machine learning +(\"multi-head self-attention can be thought of as\")", "machine learning +(\"multi-head self-attention can be compared to\")"]}