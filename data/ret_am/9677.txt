{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Spectra - <b>Information Theory</b> <b>in Machine Learning</b>", "url": "https://spectra.mathpix.com/ml/info-theory", "isFamilyFriendly": true, "displayUrl": "https://spectra.mathpix.com/ml/info-theory", "snippet": "The variation of <b>information</b> is the <b>information</b> which isn\u2019t <b>shared</b> <b>between</b> these <b>two</b> variables (\u2018x\u2019 and \u2018y\u2019). We can define it <b>like</b> so: The variation of <b>information</b> <b>between</b> <b>two</b> variables is zero if knowing the value of one tells you the value of the other and increases as they become more independent. Conditional Mutual <b>Information</b>. The conditional mutual <b>information</b> is defined as the expected value of the mutual <b>information</b> of <b>two</b> random variables given the value of a third. For ...", "dateLastCrawled": "2022-02-01T08:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "regression - Why is <b>cross entropy</b> based on Bernoulli or Multinoulli ...", "url": "https://datascience.stackexchange.com/questions/103924/why-is-cross-entropy-based-on-bernoulli-or-multinoulli-probability-distribution", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/103924/why-is-<b>cross-entropy</b>-based-on...", "snippet": "The concept of <b>Cross Entropy</b> is inherited from <b>Information</b> theory where it is applied to understand and measure the difference in the distributions of <b>two</b> or more <b>events</b>. <b>Events</b> as you would appreciate are a discrete concept and translate to classes in the case of a ML classification problems. This is the reason that <b>Cross Entropy</b> is only ...", "dateLastCrawled": "2022-01-26T13:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "18.11. <b>Information Theory</b> \u2014 Dive into Deep Learning 0.17.2 documentation", "url": "https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/information-theory.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/<b>information-theory</b>.html", "snippet": "We wish to find the <b>information</b> <b>shared</b> <b>between</b> <b>two</b> random variables. One way we could try to do this is to start with all the <b>information</b> contained in both \\(X\\) and \\(Y\\) together, and then we take off the parts that are <b>not</b> <b>shared</b>. The <b>information</b> contained in both \\(X\\) and \\(Y\\) together is written as \\(H(X, Y)\\).", "dateLastCrawled": "2022-02-02T03:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "ENTROPIES <b>AND CROSS-ENTROPIES OF EXPONENTIAL FAMILIES</b>", "url": "https://www.lix.polytechnique.fr/~nielsen/EntropyEF-ICIP2010.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.lix.polytechnique.fr/~nielsen/EntropyEF-ICIP2010.pdf", "snippet": "normalization constant that is <b>shared</b> by all members of that class. Thus even if this constant is <b>not</b> in closed-form, it is <b>not</b> necessary to take it into account for comparing entropies and cross-entropies of members of the same class of exponential families. The paper is organized as follows: Section 2 introduces the concepts of Shannon entropy, <b>cross-entropy</b> and relative entropy as a measure for statistical distributions. Section 3 de-scribes concisely exponential families. We prove in ...", "dateLastCrawled": "2022-02-03T02:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>What Are Loss Functions in ML</b> <b>And Why Are They Important</b>", "url": "https://analyticsindiamag.com/what-are-loss-functions-in-ml-and-why-are-they-important/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/<b>what-are-loss-functions-in-ml</b>-<b>and-why-are-they-important</b>", "snippet": "<b>Cross entropy</b> is a widely popular concept of <b>information</b> theory. It is the measure of number of bits that are needed to encode certain <b>information</b> based on an initial hypotheses. In machine learning, this function is used in classification where a network has to give <b>two</b> distinct outputs. The right side of the expression has an addition to account for the false positives in the results and penalise it. It is also written as: As the predicted probability moves away from the target or a label ...", "dateLastCrawled": "2022-02-02T06:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Combinatorial Optimization, <b>Cross-Entropy</b>, Ants and Rare <b>Events</b>", "url": "https://www.researchgate.net/publication/268007374_Combinatorial_Optimization_Cross-Entropy_Ants_and_Rare_Events", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/268007374_Combinatorial_Optimization_Cross...", "snippet": "The <b>Cross-Entropy</b> (CE) algorithm [22] is a relatively new method solving combinatorial optimization problems, and it was initially used for estimating probabilities of rare <b>events</b> in complex ...", "dateLastCrawled": "2022-01-29T14:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Lecture 1: Entropy and mutual <b>information</b>", "url": "http://www.ece.tufts.edu/ee/194NIT/lect01.pdf", "isFamilyFriendly": true, "displayUrl": "www.ece.tufts.edu/ee/194NIT/lect01.pdf", "snippet": "De\ufb01nition The mutual <b>information</b> <b>between</b> <b>two</b> continuous random variables X,Y with joint p.d.f f(x,y) is given by I(X;Y) = ZZ f(x,y)log f(x,y) f(x)f(y) dxdy. (26) For <b>two</b> variables it is possible to represent the di\ufb00erent entropic quantities with an analogy to set theory. In Figure 4 we see the di\ufb00erent quantities, and how the mutual <b>information</b> is the uncertainty that is common to both X and Y. H(X) H(X|Y) I(X : Y) H(Y|X) H(Y) Figure 1: Graphical representation of the conditional ...", "dateLastCrawled": "2022-02-03T06:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "When is it <b>recommended to use squared error instead of cross entropy</b> ...", "url": "https://www.quora.com/When-is-it-recommended-to-use-squared-error-instead-of-cross-entropy-for-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/When-is-it-<b>recommended-to-use-squared-error-instead-of-cross</b>...", "snippet": "Answer: I will try to explain neuron saturation problem. Why we encounter with neuron saturation in classification problems ? And how <b>cross-entropy</b> cost function ...", "dateLastCrawled": "2022-01-13T06:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Visual Information Theory</b> -- colah&#39;s blog", "url": "http://colah.github.io/posts/2015-09-Visual-Information/", "isFamilyFriendly": true, "displayUrl": "colah.github.io/posts/2015-09-Visual-<b>Information</b>", "snippet": "Let\u2019s picture a concrete example where we need to communicate which of <b>two</b> possible <b>events</b> happened. Event \\(a\\) happens \\(p(a)\\) ... A lot of this seems to revolve around the <b>information</b> <b>shared</b> <b>between</b> the variables, the intersection of their <b>information</b>. We call this \u201cmutual <b>information</b>,\u201d \\(I (X,Y)\\), defined as: 8 \\[I(X,Y) = H(X) + H(Y) - H(X,Y)\\] This definition works because \\(H(X) + H(Y)\\) has <b>two</b> copies of the mutual <b>information</b>, since it\u2019s in both \\(X\\) and \\(Y\\), while \\(H(X ...", "dateLastCrawled": "2022-02-01T11:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Why is Kullback Leibler divergence a good way to compare <b>two</b> ...", "url": "https://www.reddit.com/r/statistics/comments/9h07jo/why_is_kullback_leibler_divergence_a_good_way_to/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/statistics/comments/9h07jo/why_is_kullback_leibler_divergence...", "snippet": "Ok so. What if we do <b>not</b> know the distribution of the balls p(x) but try to choose the code according to an estimate q(x). Originally our average number of bits was given by E_p[-log p(x)]. E_p denotes the expectation under our true distribution p(x). If we encode using code words optimised for q(x) we have E_p[-log q(x)] (the <b>cross entropy</b> ...", "dateLastCrawled": "2021-03-08T15:14:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Spectra - <b>Information Theory</b> <b>in Machine Learning</b>", "url": "https://spectra.mathpix.com/ml/info-theory", "isFamilyFriendly": true, "displayUrl": "https://spectra.mathpix.com/ml/info-theory", "snippet": "The variation of <b>information</b> is the <b>information</b> which isn\u2019t <b>shared</b> <b>between</b> these <b>two</b> variables (\u2018x\u2019 and \u2018y\u2019). We can define it like so: The variation of <b>information</b> <b>between</b> <b>two</b> variables is zero if knowing the value of one tells you the value of the other and increases as they become more independent. Conditional Mutual <b>Information</b>. The conditional mutual <b>information</b> is defined as the expected value of the mutual <b>information</b> of <b>two</b> random variables given the value of a third. For ...", "dateLastCrawled": "2022-02-01T08:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "regression - Why is <b>cross entropy</b> based on Bernoulli or Multinoulli ...", "url": "https://datascience.stackexchange.com/questions/103924/why-is-cross-entropy-based-on-bernoulli-or-multinoulli-probability-distribution", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/103924/why-is-<b>cross-entropy</b>-based-on...", "snippet": "The concept of <b>Cross Entropy</b> is inherited from <b>Information</b> theory where it is applied to understand and measure the difference in the distributions of <b>two</b> or more <b>events</b>. <b>Events</b> as you would appreciate are a discrete concept and translate to classes in the case of a ML classification problems. This is the reason that <b>Cross Entropy</b> is only ...", "dateLastCrawled": "2022-01-26T13:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Newest &#39;cross-entropy&#39; Questions</b> - Cross Validated", "url": "https://stats.stackexchange.com/questions/tagged/cross-entropy", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/tagged/<b>cross-entropy</b>", "snippet": "I&#39;m having trouble understanding how to calculate the <b>cross-entropy</b> when I have <b>two</b> differently sized probability distributions. How would I go about calculating the <b>cross-entropy</b> of for example: p(x) ... entropy <b>information</b>-theory <b>cross-entropy</b> discrete-distributions. asked Nov 12 &#39;21 at 18:11. DeadPixel. 1 1 1 bronze badge. 1. vote. 1answer 87 views How to prove a loss function is classification-calibrated? Assuming that I have a custom <b>cross-entropy</b>-like loss function defined as below ...", "dateLastCrawled": "2022-01-07T06:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "18.11. <b>Information Theory</b> \u2014 Dive into Deep Learning 0.17.2 documentation", "url": "https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/information-theory.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/<b>information-theory</b>.html", "snippet": "We wish to find the <b>information</b> <b>shared</b> <b>between</b> <b>two</b> random variables. One way we could try to do this is to start with all the <b>information</b> contained in both \\(X\\) and \\(Y\\) together, and then we take off the parts that are <b>not</b> <b>shared</b>. The <b>information</b> contained in both \\(X\\) and \\(Y\\) together is written as \\(H(X, Y)\\).", "dateLastCrawled": "2022-02-02T03:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Visual Information Theory</b> -- colah&#39;s blog", "url": "http://colah.github.io/posts/2015-09-Visual-Information/", "isFamilyFriendly": true, "displayUrl": "colah.github.io/posts/2015-09-Visual-<b>Information</b>", "snippet": "The really interesting thing is the difference <b>between</b> the entropy and the <b>cross-entropy</b>. That difference is how much longer our messages are because we used a code optimized for a different distribution. If the distributions are the same, this difference will be zero. As the difference grows, it will get bigger. We call this difference the Kullback\u2013Leibler divergence, or just the KL divergence. The KL divergence of \\(p\\) with respect to \\(q\\), \\(D_q(p)\\), 5 is defined: 6 \\[D_q(p) = H_q(p ...", "dateLastCrawled": "2022-02-01T11:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Continuous Similarity Learning with <b>Shared</b> Neural Semantic ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7752253/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7752253", "snippet": "Then, <b>two</b> similarity metrics about a pair of news documents or <b>events</b> are learned continuously by neural stacking, so that <b>information</b> is better <b>shared</b> <b>between</b> the predecessor event detection and successor event evolution networks. Empirical experiments on a newly annotated real-world dataset EDENS demonstrate the superior performance of our model over several baseline models on both subtasks. In summary, our model is able to effectively deal with event detection and evolution online for ...", "dateLastCrawled": "2022-01-12T20:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Combinatorial Optimization, <b>Cross-Entropy</b>, Ants and Rare <b>Events</b>", "url": "https://www.researchgate.net/publication/268007374_Combinatorial_Optimization_Cross-Entropy_Ants_and_Rare_Events", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/268007374_Combinatorial_Optimization_Cross...", "snippet": "The <b>Cross-Entropy</b> (CE) algorithm [22] is a relatively new method solving combinatorial optimization problems, and it was initially used for estimating probabilities of rare <b>events</b> in complex ...", "dateLastCrawled": "2022-01-29T14:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "When is it <b>recommended to use squared error instead of cross entropy</b> ...", "url": "https://www.quora.com/When-is-it-recommended-to-use-squared-error-instead-of-cross-entropy-for-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/When-is-it-<b>recommended-to-use-squared-error-instead-of-cross</b>...", "snippet": "Answer: I will try to explain neuron saturation problem. Why we encounter with neuron saturation in classification problems ? And how <b>cross-entropy</b> cost function ...", "dateLastCrawled": "2022-01-13T06:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Basics of Information Theory with Python</b> \u00b7 Data Science Fabric", "url": "https://dsfabric.org/basics-of-information-theory-with-python", "isFamilyFriendly": true, "displayUrl": "https://dsfabric.org/<b>basics-of-information-theory-with-python</b>", "snippet": "The main takeaway here is that if a particular event has 100% probability, its self-<b>information</b> is \\(-\\log_2(1) = 0\\), meaning that it does <b>not</b> carry any <b>information</b>, and we have no surprise at all.Whereas, if the probability would be close to zero, or we can effectively say it&#39;s zero, then self-<b>information</b> is \\(-\\log_2(0) = \\infty\\).This implies that the rare <b>events</b> have high surprisal or high <b>information</b> content.", "dateLastCrawled": "2022-01-26T08:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "[D] Is <b>there any reason besides theory</b> <b>not</b> to use binary <b>cross-entropy</b> ...", "url": "https://www.reddit.com/r/MachineLearning/comments/bysvon/d_is_there_any_reason_besides_theory_not_to_use/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/.../comments/bysvon/d_is_<b>there_any_reason_besides_theory</b>_<b>not</b>_to_use", "snippet": "Consider instead computing the binary <b>cross-entropy</b> at each terminal node. Then there is gradient <b>information</b> at every terminal node and a more solid update signal. Plus, it&#39;s <b>not</b> exactly a stretch to consider each class label as independent a priori. I liken this to the difference <b>between</b> multinomial and one-vs-rest (OVR) logistic regression ...", "dateLastCrawled": "2022-02-03T10:47:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "18.11. <b>Information Theory</b> \u2014 Dive into Deep Learning 0.17.2 documentation", "url": "https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/information-theory.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/<b>information-theory</b>.html", "snippet": "Summary \u00b6. <b>Information theory</b> is a field of study about encoding, decoding, transmitting, and manipulating <b>information</b>. Entropy is the unit to measure how much <b>information</b> is presented in different signals. KL divergence <b>can</b> also measure the divergence <b>between</b> <b>two</b> distributions.", "dateLastCrawled": "2022-02-02T03:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Concepts and Applications of <b>Information</b> Theory to Immuno-Oncology ...", "url": "https://www.sciencedirect.com/science/article/pii/S240580332030340X", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S240580332030340X", "snippet": "Specifically, using concepts including entropy, mutual <b>information</b>, and channel capacity, one <b>can</b> quantify the storage, transmission, encoding, and flow of <b>information</b> within and <b>between</b> cellular components of the immune system on multiple temporal and spatial scales. To understand, at the quantitative level, immune signaling function and dysfunction in cancer, we present a methodology-oriented review of <b>information</b>-theoretic treatment of biochemical signal transduction and transmission ...", "dateLastCrawled": "2021-10-30T10:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "When is it <b>recommended to use squared error instead of cross entropy</b> ...", "url": "https://www.quora.com/When-is-it-recommended-to-use-squared-error-instead-of-cross-entropy-for-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/When-is-it-<b>recommended-to-use-squared-error-instead-of-cross</b>...", "snippet": "Answer: I will try to explain neuron saturation problem. Why we encounter with neuron saturation in classification problems ? And how <b>cross-entropy</b> cost function ...", "dateLastCrawled": "2022-01-13T06:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Information</b> theory.pdf - 7:38 PM <b>Information</b> theory Wikipedia ...", "url": "https://www.coursehero.com/file/125425579/Information-theorypdf/", "isFamilyFriendly": true, "displayUrl": "https://www.coursehero.com/file/125425579/<b>Information</b>-theorypdf", "snippet": "1/12/22, 7:38 PM <b>Information</b> theory - Wikipedia 5/18 The entropy of a Bernoulli trial as a function of success probability, often called the binary entropy function, H b (p).The entropy is maximized at 1 bit per trial when the <b>two</b> possible outcomes are equally probable, as in an unbiased coin toss. where p i is the probability of occurrence of the i-th possible value of the source symbol.This equation gives the entropy in the units of &quot;bits&quot; (per symbol) because it uses a logarithm of base 2 ...", "dateLastCrawled": "2022-01-13T13:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 4, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Mutual information</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Mutual_information", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Mutual_information</b>", "snippet": "In probability theory and <b>information</b> theory, the <b>mutual information</b> (MI) of <b>two</b> random variables is a measure of the mutual dependence <b>between</b> the <b>two</b> variables. More specifically, it quantifies the &quot;amount of <b>information</b>&quot; (in units such as shannons (), nats or hartleys) obtained about one random variable by observing the other random variable.The concept of <b>mutual information</b> is intimately linked to that of entropy of a random variable, a fundamental notion in <b>information</b> theory that ...", "dateLastCrawled": "2022-02-03T00:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "GitHub - jeevu94/ml-notes: Personal notes on ML", "url": "https://github.com/jeevu94/ml-notes", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/jeevu94/ml-<b>not</b>es", "snippet": "Essentially, <b>cross entropy</b> is used to compare the similarity <b>between</b> <b>two</b> probability distribution. In classification problem, we use activation function like softmax which produces probabilities for each class, and <b>cross entropy</b> is a loss function which is used in such problems to evaluate model. For example, for a 3 class classification ...", "dateLastCrawled": "2022-01-20T20:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "What <b>can</b> we accomplish without changing the architecture? A <b>thought</b> ...", "url": "https://hazyresearch.stanford.edu/blog/2021-10-14-metadata", "isFamilyFriendly": true, "displayUrl": "https://hazyresearch.stanford.edu/blog/2021-10-14-metadata", "snippet": "Data: Metadata (e.g., \u201cpolitician\u201d) <b>shared</b> <b>between</b> a popular and rare entity (e.g., \u201cBarack Obama\u201d vs. \u201cDaniel Dugl\u00e9ry\u201d) are explicitly provided as inputs to the LM. So, the LM <b>can</b> use patterns observed for popular entities, to better reason about rare entities with the same metadata, without generating any new samples. 2. It <b>can</b> ...", "dateLastCrawled": "2022-01-17T13:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Linking ADHD and Behavioral Assessment Through Identification of <b>Shared</b> ...", "url": "https://www.frontiersin.org/articles/10.3389/fphys.2020.583005/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/fphys.2020.583005", "snippet": "Though we used the selection filter to identify an initial set of COIs, connection strength was estimated as the cross-mutual <b>information</b> (XMI) (Abarbanel and Gollub, 1996) <b>between</b> ROI time series vectors because mutual <b>information</b> is more sensitive to the general dependency <b>between</b> <b>two</b> variables, which may or may <b>not</b> be linear , is more robust to non-stationary processes commonly found in neural time series (Wollstadt et al., 2014), and may be more sensitive to synchronization in noisy ...", "dateLastCrawled": "2022-02-03T18:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "What <b>is the difference between entropy and atrophy</b>? - Quora", "url": "https://www.quora.com/What-is-the-difference-between-entropy-and-atrophy", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-<b>is-the-difference-between-entropy-and-atrophy</b>", "snippet": "Answer: Entropy is a thermodynamic measurement which accounts for loss of order and is needed in some advanced scientific calculations. Atrophy is completely unrelated and usually associated to degradation of the human body such as a muscle that is wasting away or a bone disease which degrades bo...", "dateLastCrawled": "2022-01-11T02:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Why is Kullback Leibler divergence a good way to compare <b>two</b> ...", "url": "https://www.reddit.com/r/statistics/comments/9h07jo/why_is_kullback_leibler_divergence_a_good_way_to/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/statistics/comments/9h07jo/why_is_kullback_leibler_divergence...", "snippet": "Any advice/<b>shared</b> experience would be enormously helpful! EDIT: Going through the responses as quickly as I <b>can</b>! Thank you so much for all of the responses/support. I feel reassured that this isn\u2019t how it is everywhere and now I\u2019m armed with a couple of ideas I <b>can</b> try out (thanks to your help) until I\u2019m in a better position to hop ship.", "dateLastCrawled": "2021-03-08T15:14:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) A <b>Cross-Entropy</b>-based Method to Perform <b>Information</b>-based Feature ...", "url": "https://www.academia.edu/67508455/A_Cross_Entropy_based_Method_to_Perform_Information_based_Feature_Selection", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/67508455/A_<b>Cross_Entropy</b>_based_Method_to_Perform_<b>Information</b>...", "snippet": "Instead, the <b>shared</b> <b>information</b> <b>between</b> Uj\u22121 and y is the sum of the dashed and dotted areas. Then, if we want to evaluate the <b>information</b> about y carried by the new feature given the set Uj\u22121 , we need to take into account only the gray area, which is greater in the case of A than in the case of B. This means that Uj\u22121 contains most of the <b>information</b> carried by B about y, then B is redundant and <b>can</b> be eliminated. In fact, the dashed area is greatest in the case of B. On the opposite ...", "dateLastCrawled": "2022-01-28T16:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Combinatorial Optimization, <b>Cross-Entropy</b>, Ants and Rare <b>Events</b>", "url": "https://www.researchgate.net/publication/268007374_Combinatorial_Optimization_Cross-Entropy_Ants_and_Rare_Events", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/268007374_Combinatorial_Optimization_Cross...", "snippet": "The <b>Cross-Entropy</b> (CE) algorithm [22] is a relatively new method solving combinatorial optimization problems, and it was initially used for estimating probabilities of rare <b>events</b> in complex ...", "dateLastCrawled": "2022-01-29T14:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>Cross-Entropy</b>-Based Replay of Concurrent Programs | Eitan Farchi ...", "url": "https://www.academia.edu/24590072/Cross_Entropy_Based_Replay_of_Concurrent_Programs", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/24590072/<b>Cross_Entropy</b>_Based_Replay_of_Concurrent_Programs", "snippet": "It is an iterative approach based on minimizing the <b>cross-entropy</b> or the Kullback-Leibler distance <b>between</b> <b>two</b> probability distributions. The CE method was motivated by an adaptive algorithm for estimating probabilities of rare <b>events</b> in complex stochastic networks [22]. Then, it was realized that a simple modification al- lows to use this method also for solving hard combinatorial optimization problems, in which there is a performance function associated with the inputs. The CE method in ...", "dateLastCrawled": "2020-12-30T18:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "18.11. <b>Information Theory</b> \u2014 Dive into Deep Learning 0.17.2 documentation", "url": "https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/information-theory.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/<b>information-theory</b>.html", "snippet": "Summary \u00b6. <b>Information theory</b> is a field of study about encoding, decoding, transmitting, and manipulating <b>information</b>. Entropy is the unit to measure how much <b>information</b> is presented in different signals. KL divergence <b>can</b> also measure the divergence <b>between</b> <b>two</b> distributions.", "dateLastCrawled": "2022-02-02T03:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>What Are Loss Functions in ML</b> <b>And Why Are They Important</b>", "url": "https://analyticsindiamag.com/what-are-loss-functions-in-ml-and-why-are-they-important/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/<b>what-are-loss-functions-in-ml</b>-<b>and-why-are-they-important</b>", "snippet": "<b>Cross-Entropy</b>. <b>Cross entropy</b> is a widely popular concept of <b>information</b> theory. It is the measure of number of bits that are needed to encode certain <b>information</b> based on an initial hypotheses. In machine learning, this function is used in classification where a network has to give <b>two</b> distinct outputs. The right side of the expression has an ...", "dateLastCrawled": "2022-02-02T06:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Cross-Entropy</b> and Rare <b>Events</b> for <b>Maximal Cut and Partition Problems</b>", "url": "https://www.researchgate.net/publication/2396030_Cross-Entropy_and_Rare_Events_for_Maximal_Cut_and_Partition_Problems", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/2396030_<b>Cross-Entropy</b>_and_Rare_<b>Events</b>_for...", "snippet": "In APHS, the expected value of the search probability distribution is adapted using a sample of \u201cgood\u201d vectors among the population to minimize the <b>cross entropy</b> <b>between</b> the actual ...", "dateLastCrawled": "2021-11-28T13:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Basics of Information Theory with Python</b> \u00b7 Data Science Fabric", "url": "https://dsfabric.org/basics-of-information-theory-with-python", "isFamilyFriendly": true, "displayUrl": "https://dsfabric.org/<b>basics-of-information-theory-with-python</b>", "snippet": "The main takeaway here is that if a particular event has 100% probability, its self-<b>information</b> is \\(-\\log_2(1) = 0\\), meaning that it does <b>not</b> carry any <b>information</b>, and we have no surprise at all.Whereas, if the probability would be close to zero, or we <b>can</b> effectively say it&#39;s zero, then self-<b>information</b> is \\(-\\log_2(0) = \\infty\\).This implies that the rare <b>events</b> have high surprisal or high <b>information</b> content.", "dateLastCrawled": "2022-01-26T08:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "python - How to use torch.nn.<b>CrossEntropyLoss</b> as autoencoder&#39;s ...", "url": "https://stackoverflow.com/questions/55651920/how-to-use-torch-nn-crossentropyloss-as-autoencoders-reconstruction-loss", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/55651920", "snippet": "ae_criterion = nn.<b>CrossEntropyLoss</b> () ae_loss = ae_criterion (X, Y) where X is the autoencoder&#39;s reconstruction and Y is the target (since it is an autoencoder, Y is the same as the original input X ). Both X and Y have shape [42, 32, 130] = [batch_size, timesteps, number_of_classes]. When I run the code above I get the following error:", "dateLastCrawled": "2022-01-25T08:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Event Extraction by Associating Event Types and Argument Roles | DeepAI", "url": "https://deepai.org/publication/event-extraction-by-associating-event-types-and-argument-roles", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/event-extraction-by-associating-event-types-and...", "snippet": "Our universal event schema <b>can</b> <b>not</b> only make use of the <b>shared</b> <b>information</b> <b>between</b> categories, but also be suitable for the event extraction task with only a small amount of labeled data. Furthermore, unified template <b>can</b> alleviate data imbalance by the combination the multiple roles. We design an event extraction framework AEE for learning associations among event types and arguments roles. The whole framework is divided into <b>two</b> parts:", "dateLastCrawled": "2022-01-29T09:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Entropy-Based Anomaly Detection in a Network | SpringerLink", "url": "https://link.springer.com/article/10.1007/s11277-018-5288-2", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11277-018-5288-2", "snippet": "Every computer on the Internet these days is a potential target for a new attack at any moment. In this paper we propose a method to enhance network security using entropy based anomaly detection. Intrusion detection system Snort is used for collecting the complete network traffic. Snort alert is then processed for selecting the attributes. Then Shannon entropies are calculated to analyze source IP address, source port address, destination IP address, destination port address, source IP ...", "dateLastCrawled": "2021-12-31T09:24:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Why and How to use <b>Cross Entropy</b>. The fundamental reasons for ...", "url": "https://towardsdatascience.com/why-and-how-to-use-cross-entropy-4e983cbdd873", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/why-and-how-to-use-<b>cross-entropy</b>-4e983cbdd873", "snippet": "The fundamental reasons for minimizing binary <b>cross entropy</b> (log loss) with probabilistic classification models . Will Arliss. Sep 26, 2020 \u00b7 7 min read. Introduction. This post discusses why logistic regression necessarily uses a different loss function than linear regression. First, the simple yet inefficient way to solve logistic regression will be presented, then the slightly less simple but much more efficient way will be explained and compared. The simple way. Linear regression is the ...", "dateLastCrawled": "2022-01-31T14:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Cross-Entropy</b> Demystified. What is it? Is there any relation to\u2026 | by ...", "url": "https://naokishibuya.medium.com/demystifying-cross-entropy-e80e3ad54a8", "isFamilyFriendly": true, "displayUrl": "https://naokishibuya.medium.com/demystifying-<b>cross-entropy</b>-e80e3ad54a8", "snippet": "However, the <b>machine</b> <b>learning</b> application uses the base e logarithm for implementation convenience. Binary <b>Cross-Entropy</b>. We can use the binary <b>cross-entropy</b> for binary classification where we have yes/no answer. For example, there are only dogs or cats in images. For the binary classifications, the <b>cross-entropy</b> formula contains only two ...", "dateLastCrawled": "2022-01-26T13:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A Gentle Introduction to Information Entropy - <b>Machine</b> <b>Learning</b> Mastery", "url": "https://machinelearningmastery.com/what-is-information-entropy/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/what-is-information-entropy", "snippet": "Calculating information and entropy is a useful tool in <b>machine</b> <b>learning</b> and is used as the basis for techniques such as feature selection, building decision trees, and, more generally, fitting classification models. As such, a <b>machine</b> <b>learning</b> practitioner requires a strong understanding and intuition for information and entropy. In this post, you will discover a gentle introduction to information entropy. After reading this post, you will know: Information theory is concerned with data ...", "dateLastCrawled": "2022-02-02T13:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>machine</b> <b>learning</b> - <b>Cross-entropy loss</b> explanation - Data Science Stack ...", "url": "https://datascience.stackexchange.com/questions/20296/cross-entropy-loss-explanation", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/20296", "snippet": "The answer from Neil is correct. However I think its important to point out that while the loss does not depend on the distribution between the incorrect classes (only the distribution between the correct class and the rest), the gradient of this loss function does effect the incorrect classes differently depending on how wrong they are. So when you use cross-ent in <b>machine</b> <b>learning</b> you will change weights differently for [0.1 0.5 0.1 0.1 0.2] and [0.1 0.6 0.1 0.1 0.1].", "dateLastCrawled": "2022-01-27T03:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine</b> <b>Learning</b> Concepts for Revision | by Raunak Sarada | Medium", "url": "https://raunaksarada-cse21.medium.com/machine-learning-concepts-for-revision-491384952d27", "isFamilyFriendly": true, "displayUrl": "https://raunaksarada-cse21.medium.com/<b>machine</b>-<b>learning</b>-concepts-for-revision-491384952d27", "snippet": "ML Concepts. A.I \u2014 Intelligence showed by machines which is common for humans <b>Machine</b> <b>Learning</b>- Recognize the pattern in data and automatically learn and improve through experience without explicitly being programmed Deep <b>Learning</b>- branch of <b>machine</b> <b>learning</b>.We have to deal with lots of data so in that case problems can\u2019t be solved with simple ML algorithms.", "dateLastCrawled": "2022-01-25T20:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Softmax and <b>Cross-entropy for multi-class classification</b>. - AppliedAICourse", "url": "https://www.appliedaicourse.com/lecture/11/applied-machine-learning-online-course/3384/softmax-and-cross-entropy-for-multi-class-classification/8/module-8-neural-networks-computer-vision-and-deep-learning", "isFamilyFriendly": true, "displayUrl": "https://www.appliedaicourse.com/lecture/11/applied-<b>machine</b>-<b>learning</b>-online-course/3384/...", "snippet": "Home Courses Applied <b>Machine</b> <b>Learning</b> Online Course Softmax and <b>Cross-entropy for multi-class classification</b>. Softmax and <b>Cross-entropy for multi-class classification</b>. Instructor: Applied AI Course Duration: 25 mins . Close. This content is restricted. Please Login. Prev. Next. Gradient Checking and clipping . How to train a Deep MLP? Deep <b>Learning</b>:Neural Networks. 1.1 History of Neural networks and Deep <b>Learning</b>. 25 min. 1.2 How Biological Neurons work? 8 min. 1.3 Growth of biological ...", "dateLastCrawled": "2022-02-02T02:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Shannon <b>entropy</b> in the context of <b>machine</b> <b>learning</b> and AI | by Frank ...", "url": "https://medium.com/swlh/shannon-entropy-in-the-context-of-machine-learning-and-ai-24aee2709e32", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/shannon-<b>entropy</b>-in-the-context-of-<b>machine</b>-<b>learning</b>-and-ai-24...", "snippet": "Closely related to <b>cross entropy</b>, the KL divergence from q to p, written DKL(p||q), is another similarity measure often used in <b>machine</b> <b>learning</b>. In the language of Bayesian Inference, DKL(p||q ...", "dateLastCrawled": "2022-01-30T12:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>machine</b> <b>learning</b> - Is there an alternative to categorical <b>cross-entropy</b> ...", "url": "https://stats.stackexchange.com/questions/367823/is-there-an-alternative-to-categorical-cross-entropy-with-a-notion-of-class-dis", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/367823", "snippet": "In the dirt pile <b>analogy</b>, each class corresponds to a location, and the predicted probability defines the amount of dirt. For each point in the training set, you have a target class. This corresponds to a probability distribution that takes the value one for the target class and zero for all others, i.e. all dirt is piled up at a single location.", "dateLastCrawled": "2022-02-03T17:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Lecture 4 Fundamentals of deep <b>learning</b> and neural networks", "url": "https://web.stanford.edu/class/biods388/downloads/BIODS388_Lecture_4.pdf", "isFamilyFriendly": true, "displayUrl": "https://web.stanford.edu/class/biods388/downloads/BIODS388_Lecture_4.pdf", "snippet": "Deep <b>learning</b>: <b>Machine</b> <b>learning</b> models based on \u201cdeep\u201d neural networks comprising millions (sometimes billions) of parameters organized into hierarchical layers. Features are multiplied and added together repeatedly, with the outputs from one layer of parameters being fed into the next layer -- before a prediction is made. Contrast with linear regression: Agenda for today - More on the structure of neural network models - <b>Machine</b> <b>learning</b> training loop and concept of loss, in the context ...", "dateLastCrawled": "2022-02-02T09:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "[D] A Short Introduction to Entropy, <b>Cross-Entropy</b> and KL-Divergence ...", "url": "https://www.reddit.com/r/MachineLearning/comments/7vhmp7/d_a_short_introduction_to_entropy_crossentropy/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/<b>MachineLearning</b>/comments/7vhmp7/d_a_short_introduction_to...", "snippet": "I am having trouble reconciling the concept with the <b>analogy</b>. At 2:35 even if a rainy day was 25% likely, there&#39;s still only two states, rainy and sunny, and therefor only 1 bit of information is needed to convey that, so only one bit of data needs to be sent, even though the 1 bit of data reduces the uncertainty of a rainy day by a factor of 4. I quite don&#39;t get what he means by this being 2 bits of information. I guess where I am stuck is how the uncertainty reduction factor translates to ...", "dateLastCrawled": "2021-08-20T08:03:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Beat the Bookmakers With Tree-Based <b>Machine</b> <b>Learning</b> Algorithms | by ...", "url": "https://medium.com/analytics-vidhya/beat-the-bookmakers-with-tree-based-machine-learning-algorithms-1d349335b54", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/beat-the-bookmakers-with-tree-based-<b>machine</b>...", "snippet": "<b>Cross-entropy is similar</b> to Gini Impurity, but it involves using the concept of entropy from information theory. This article won\u2019t go in depth about it, but essentially, as the cross-entropy ...", "dateLastCrawled": "2022-01-26T04:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>A Traveler\u2019s Diary on the Road to Machine</b> <b>Learning</b> - Chapter 1 | by ...", "url": "https://medium.com/swlh/a-travelers-diary-on-the-road-to-machine-learning-chapter-1-8850ec5b4243", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/<b>a-travelers-diary-on-the-road-to-machine</b>-<b>learning</b>-chapter-1...", "snippet": "Types of <b>Machine</b> <b>Learning</b> algorithms: ... Sparse categorical <b>cross entropy is similar</b> to categorical cross entropy, only difference is it uses only one value as target. It saves memory as well as ...", "dateLastCrawled": "2021-05-21T04:27:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Deep Learning for Computer Architects</b> | Chen Jeff - Academia.edu", "url": "https://www.academia.edu/40860009/Deep_Learning_for_Computer_Architects", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/40860009/<b>Deep_Learning_for_Computer_Architects</b>", "snippet": "This text serves as a primer for computer architects in a new and rapidly evolving \ufb01eld. We review how <b>machine</b> <b>learning</b> has evolved since its inception in the 1960s and track the key developments leading up to the emergence of the powerful deep <b>learning</b> techniques that emerged in the last decade.", "dateLastCrawled": "2022-01-28T02:18:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(cross-entropy)  is like +(information not shared between two events)", "+(cross-entropy) is similar to +(information not shared between two events)", "+(cross-entropy) can be thought of as +(information not shared between two events)", "+(cross-entropy) can be compared to +(information not shared between two events)", "machine learning +(cross-entropy AND analogy)", "machine learning +(\"cross-entropy is like\")", "machine learning +(\"cross-entropy is similar\")", "machine learning +(\"just as cross-entropy\")", "machine learning +(\"cross-entropy can be thought of as\")", "machine learning +(\"cross-entropy can be compared to\")"]}