{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Interrater</b> reliability: the kappa statistic", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3900052/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC3900052", "snippet": "<b>Like</b> most correlation statistics, the kappa can range from \u22121 to +1. While the kappa is one of the most commonly used statistics to test <b>interrater</b> reliability, it has limitations. Judgments about what level of kappa should be acceptable for health research are questioned. Cohen\u2019s suggested interpretation may be too lenient for health related studies because it implies that a score as low as 0.41 might be acceptable. Kappa and percent <b>agreement</b> are compared, and levels for both kappa and ...", "dateLastCrawled": "2022-01-26T07:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Computing <b>Inter-Rater</b> Reliability for Observational Data: An Overview ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3402032/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC3402032", "snippet": "The assessment of <b>inter-rater</b> reliability (IRR, also called <b>inter-rater</b> <b>agreement</b>) is often necessary for research designs where data are collected through <b>ratings</b> provided by trained or untrained coders. However, many studies use incorrect statistical analyses to compute IRR, misinterpret the results from IRR analyses, or fail to consider the implications that IRR estimates have on statistical power for subsequent analyses. This paper will provide an overview of methodological issues ...", "dateLastCrawled": "2022-02-03T04:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Comparing inter-rater agreement between classes</b> of raters - <b>Cross Validated</b>", "url": "https://stats.stackexchange.com/questions/132609/comparing-inter-rater-agreement-between-classes-of-raters", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/132609/<b>comparing</b>-<b>inter-rater</b>-<b>agreement</b>...", "snippet": "3) Sufficient <b>inter-rater</b> reliability for semi-experts is not adequate, of course, if they are not in <b>agreement</b> with the experts. Here you could do a statistical comparison of the <b>two</b> groups distributions and central tendency - if you are comfortable <b>comparing</b> means on ordinal data, you have sufficient observations, and the data look normal, use standard tests <b>like</b> a t-test or ANOVA. Otherwise a crosstab and $\\chi^2$ test may be more appropriate (just keep in mind sample size sensitivities ...", "dateLastCrawled": "2022-01-22T06:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Guide to <b>Inter-rater</b> <b>Agreement</b>", "url": "https://www.cde.state.co.us/educatoreffectiveness/iraguide", "isFamilyFriendly": true, "displayUrl": "https://<b>www.cde.state.co.us</b>/educatoreffectiveness/iraguide", "snippet": "<b>Inter-rater</b> . reliability . is . t. he degree of <b>agreement</b> in the <b>ratings</b> that <b>two</b> or more observers assign to the same behavior or observation (McREL, 2004). In other words, when one rates a. Version 1For feedback, comments or questions, please email Educator_Effectiveness@cde.<b>state.co.us</b>. teacher a three overall, the other consistently rates ...", "dateLastCrawled": "2022-01-31T22:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Which measure of <b>inter-rater agreement is appropriate with diverse</b> ...", "url": "https://www.researchgate.net/post/Which-measure-of-inter-rater-agreement-is-appropriate-with-diverse-multiple-raters", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/post/Which-measure-of-<b>inter-rater</b>-<b>agreement</b>-is...", "snippet": "It looks <b>like</b> a multi-rater multi-reader problem for categorical data. As you mentioned Gwet AC1 is an alternative to Kappa. He claims some advantages of AC1 over kappa, as you noticed, and shows ...", "dateLastCrawled": "2022-02-02T16:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Understanding Interobserver <b>Agreement</b>: The Kappa Statistic", "url": "http://web2.cs.columbia.edu/~julia/courses/CS6998/Interrater_agreement.Kappa_statistic.pdf", "isFamilyFriendly": true, "displayUrl": "web2.cs.columbia.edu/~julia/courses/CS6998/<b>Interrater</b>_<b>agreement</b>.Kappa_statistic.pdf", "snippet": "The data layout might look <b>like</b> Table 3. The observed <b>agreement</b> is high at 85%. However, the kappa (calculation shown in Table 3) is low at .04, suggesting only poor to slight <b>agreement</b> when accounting for chance. One method to account for this paradox, put simply, is to distinguish between <b>agreement</b> on the <b>two</b> levels of the finding (eg, <b>agreement</b> on positive <b>ratings</b> compared to <b>agreement</b> on negative <b>ratings</b>). Feinstein and Cicchetti have published detailed papers on this paradox and methods ...", "dateLastCrawled": "2022-01-28T23:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Inter-rater reliability in qualitative research</b> | psuc6b", "url": "https://psuc6b.wordpress.com/2011/12/08/inter-rater-reliability-in-qualitative-research/", "isFamilyFriendly": true, "displayUrl": "https://psuc6b.wordpress.com/2011/12/08/<b>inter-rater-reliability-in-qualitative-research</b>", "snippet": "The advantages of <b>inter-rater</b> reliability: \u2013 <b>Inter-rater</b> reliability is a great tool for consolidation of the research. As you are <b>comparing</b> several <b>people</b>\u2019s analysis, it is likely to reduce bias and increase objectivity in the interpretation of the data. \u2013 Qualitative research is discarded by many scientists. Showing that an infinite ...", "dateLastCrawled": "2022-01-30T21:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Cohen&#39;s <b>kappa</b> in SPSS Statistics - Procedure, output and interpretation ...", "url": "https://statistics.laerd.com/spss-tutorials/cohens-kappa-in-spss-statistics.php", "isFamilyFriendly": true, "displayUrl": "https://<b>statistics.laerd.com</b>/spss-tutorials/cohens-<b>kappa</b>-in-spss-statistics.php", "snippet": "Cohen&#39;s <b>kappa</b> (\u03ba) is such a measure of <b>inter-rater</b> <b>agreement</b> for categorical scales when there are <b>two</b> raters (where \u03ba is the lower-case Greek letter &#39;<b>kappa</b>&#39;). There are many occasions when you need to determine the <b>agreement</b> between <b>two</b> raters. For example, the head of a local medical practice might want to determine whether <b>two</b> experienced ...", "dateLastCrawled": "2022-01-30T18:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Types of Reliability</b> - Research Methods Knowledge Base", "url": "https://conjointly.com/kb/types-of-reliability/", "isFamilyFriendly": true, "displayUrl": "https://conjointly.com/kb/<b>types-of-reliability</b>", "snippet": "There are <b>two</b> major ways to actually estimate <b>inter-rater</b> reliability. If your measurement consists of categories \u2013 the raters are checking off which category each observation falls in \u2013 you can calculate the percent of <b>agreement</b> between the raters. For instance, let\u2019s say you had 100 observations that were being rated by <b>two</b> raters. For each observation, the rater could check one of three categories. Imagine that on 86 of the 100 observations the raters checked the same category. In ...", "dateLastCrawled": "2022-02-02T02:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Reliability and Validity: Schizophrenia Flashcards | Quizlet", "url": "https://quizlet.com/gb/635155966/reliability-and-validity-schizophrenia-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/gb/635155966/reliability-and-validity-schizophrenia-flash-cards", "snippet": "Start studying Reliability and Validity: Schizophrenia. Learn vocabulary, terms, and more with flashcards, games, and other study tools.", "dateLastCrawled": "2021-11-09T08:52:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Comparing inter-rater agreement between classes</b> of raters - <b>Cross Validated</b>", "url": "https://stats.stackexchange.com/questions/132609/comparing-inter-rater-agreement-between-classes-of-raters", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/132609/<b>comparing</b>-<b>inter-rater</b>-<b>agreement</b>...", "snippet": "3) Sufficient <b>inter-rater</b> reliability for semi-experts is not adequate, of course, if they are not in <b>agreement</b> with the experts. Here you could do a statistical comparison of the <b>two</b> groups distributions and central tendency - if you are comfortable <b>comparing</b> means on ordinal data, you have sufficient observations, and the data look normal, use standard tests like a t-test or ANOVA. Otherwise a crosstab and $\\chi^2$ test may be more appropriate (just keep in mind sample size sensitivities ...", "dateLastCrawled": "2022-01-22T06:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "How to assess and compare <b>inter-rater</b> reliability, <b>agreement</b> and ...", "url": "https://www.frontiersin.org/articles/10.3389/fpsyg.2014.00509/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/fpsyg.2014.00509", "snippet": "In summary, this report has <b>two</b> main goals: to provide a methodological tutorial for assessing <b>inter-rater</b> reliability, <b>agreement</b> and linear correlation of rating pairs, and to evaluate whether the German parent questionnaire ELAN (Bockmann and Kiese-Himmel, 2006) can be reliably employed also with daycare teachers when assessing early expressive vocabulary development. We compared mother\u2013father and parent\u2013teacher <b>ratings</b> with regard to <b>agreement</b>, correlation as well as reliability of ...", "dateLastCrawled": "2022-01-29T02:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Interrater</b> reliability: the kappa statistic", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3900052/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC3900052", "snippet": "A partial list includes percent <b>agreement</b>, Cohen\u2019s kappa (for <b>two</b> raters), the Fleiss kappa (adaptation of Cohen\u2019s kappa for 3 or more raters) the contingency coefficient, the Pearson r and the Spearman Rho, the intra-class correlation coefficient, the concordance correlation coefficient, and Krippendorff\u2019s alpha (useful when there are multiple raters and multiple possible <b>ratings</b>). Use of correlation coefficients such as Pearson\u2019s r may be a poor reflection of the amount of ...", "dateLastCrawled": "2022-01-26T07:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Computing <b>Inter-Rater</b> Reliability for Observational Data: An Overview ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3402032/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC3402032", "snippet": "The assessment of <b>inter-rater</b> reliability (IRR, also called <b>inter-rater</b> <b>agreement</b>) is often necessary for research designs where data are collected through <b>ratings</b> provided by trained or untrained coders. However, many studies use incorrect statistical analyses to compute IRR, misinterpret the results from IRR analyses, or fail to consider the implications that IRR estimates have on statistical power for subsequent analyses. This paper will provide an overview of methodological issues ...", "dateLastCrawled": "2022-02-03T04:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Evaluation of <b>Inter-Rater</b> <b>Agreement</b> and <b>Inter-Rater</b> Reliability for ...", "url": "https://www.researchgate.net/profile/Ram-Bajpai-3/publication/273451591_Evaluation_of_Inter-Rater_Agreement_and_Inter-Rater_Reliability_for_Observational_Data_An_Overview_of_Concepts_and_Methods/links/55026ded0cf231de076e6af6/Evaluation-of-Inter-Rater-Agreement-and-Inter-Rater-Reliability-for-Observational-Data-An-Overview-of-Concepts-and-Methods.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/profile/Ram-Bajpai-3/publication/273451591_Evaluation_of...", "snippet": "how frequently <b>two</b> or more evaluators assign exactly the same rating (e.g., if both give a rating of \u201c4\u201d they are in <b>agreement</b>), and reliability", "dateLastCrawled": "2022-01-20T10:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) Evaluation of <b>Inter-Rater</b> <b>Agreement</b> and <b>Inter-Rater</b> Reliability ...", "url": "https://www.researchgate.net/publication/273451591_Evaluation_of_Inter-Rater_Agreement_and_Inter-Rater_Reliability_for_Observational_Data_An_Overview_of_Concepts_and_Methods", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/273451591_Evaluation_of_<b>Inter-Rater</b>_<b>Agreement</b>...", "snippet": "<b>Inter-rater</b> <b>agreement</b> and reliability are fundamental to the evaluation of new approaches or methods in various fields [26].In this study, we calculated the <b>agreement</b> and reliability indices of ...", "dateLastCrawled": "2022-01-26T02:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A Guide to <b>Inter-rater</b> <b>Agreement</b>", "url": "https://www.cde.state.co.us/educatoreffectiveness/iraguide", "isFamilyFriendly": true, "displayUrl": "https://<b>www.cde.state.co.us</b>/educatoreffectiveness/iraguide", "snippet": "<b>Inter-rater</b> . reliability . is . t. he degree of <b>agreement</b> in the <b>ratings</b> that <b>two</b> or more observers assign to the same behavior or observation (McREL, 2004). In other words, when one rates a. Version 1For feedback, comments or questions, please email Educator_Effectiveness@cde.<b>state.co.us</b>. teacher a three overall, the other consistently rates ...", "dateLastCrawled": "2022-01-31T22:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Inter-rater reliability in qualitative research</b> | psuc6b", "url": "https://psuc6b.wordpress.com/2011/12/08/inter-rater-reliability-in-qualitative-research/", "isFamilyFriendly": true, "displayUrl": "https://psuc6b.wordpress.com/2011/12/08/<b>inter-rater-reliability-in-qualitative-research</b>", "snippet": "The advantages of <b>inter-rater</b> reliability: \u2013 <b>Inter-rater</b> reliability is a great tool for consolidation of the research. As you are <b>comparing</b> several <b>people</b>\u2019s analysis, it is likely to reduce bias and increase objectivity in the interpretation of the data. \u2013 Qualitative research is discarded by many scientists. Showing that an infinite ...", "dateLastCrawled": "2022-01-30T21:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Types of Reliability and Validity in Clinical Psychology Research ...", "url": "https://quizlet.com/191253081/types-of-reliability-and-validity-in-clinical-psychology-research-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/191253081/types-of-reliability-and-validity-in-clinical-psychology...", "snippet": "degree of <b>agreement</b> in the <b>ratings</b> provided by <b>two</b> observers of the same behavior. AKA many <b>people</b> will do the research and see what they will get. KAPPA is the measurement for this. <b>inter-rater</b> reliability example. watching any sport using judges, such as Olympics ice skating or a dog show, relies upon human observers maintaining a great degree of consistency between observers. internal consistency reliability. degree to which responses (e.g., items on a test) are correlated with one ...", "dateLastCrawled": "2018-10-14T03:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>agreement</b> statistics - Can I test <b>interrater</b> reliability between 2 ...", "url": "https://stats.stackexchange.com/questions/507931/can-i-test-interrater-reliability-between-2-groups-of-unequal-sample-size", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/507931/can-i-test-<b>interrater</b>-reliability...", "snippet": "I would like to explore <b>inter-rater</b> reliability of <b>ratings</b> for each item (problem areas and supports) and totals. However, recruitment has been more positive towards one group; in fact double. However total sample size is small (n=42). Can I reliably explore <b>inter-rater</b> reliability by adjusting somehow. If so, any help would be gratefully appreciated. Totally new to stats and SPSS! TIA . small-sample <b>agreement</b>-statistics. Share. Cite. Improve this question. Follow asked Feb 3 &#39;21 at 21:57 ...", "dateLastCrawled": "2022-01-26T04:46:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Comparing inter-rater agreement between classes</b> of raters - <b>Cross Validated</b>", "url": "https://stats.stackexchange.com/questions/132609/comparing-inter-rater-agreement-between-classes-of-raters", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/132609/<b>comparing</b>-<b>inter-rater</b>-<b>agreement</b>...", "snippet": "<b>Agreement</b>. This focuses on absolute <b>agreement</b> between raters - if I give it a 2, you will give it a 2. Here are the steps I would take: 1) Krippendorff&#39;s $\\alpha$ across both groups. This is going to be an overall benchmark. 2) Krippendorff&#39;s $\\alpha$ for each group separately. Compare the <b>two</b> coefficients, and see which group has a higher ...", "dateLastCrawled": "2022-01-22T06:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Computing <b>Inter-Rater</b> Reliability for Observational Data: An Overview ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3402032/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC3402032", "snippet": "The assessment of <b>inter-rater</b> reliability (IRR, also called <b>inter-rater</b> <b>agreement</b>) is often necessary for research designs where data are collected through <b>ratings</b> provided by trained or untrained coders. However, many studies use incorrect statistical analyses to compute IRR, misinterpret the results from IRR analyses, or fail to consider the implications that IRR estimates have on statistical power for subsequent analyses. This paper will provide an overview of methodological issues ...", "dateLastCrawled": "2022-02-03T04:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "How to assess and compare <b>inter-rater</b> reliability, <b>agreement</b> and ...", "url": "https://www.frontiersin.org/articles/10.3389/fpsyg.2014.00509/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/fpsyg.2014.00509", "snippet": "In summary, this report has <b>two</b> main goals: to provide a methodological tutorial for assessing <b>inter-rater</b> reliability, <b>agreement</b> and linear correlation of rating pairs, and to evaluate whether the German parent questionnaire ELAN (Bockmann and Kiese-Himmel, 2006) <b>can</b> be reliably employed also with daycare teachers when assessing early expressive vocabulary development. We compared mother\u2013father and parent\u2013teacher <b>ratings</b> with regard to <b>agreement</b>, correlation as well as reliability of ...", "dateLastCrawled": "2022-01-29T02:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Guide to <b>Inter-rater</b> <b>Agreement</b>", "url": "https://www.cde.state.co.us/educatoreffectiveness/iraguide", "isFamilyFriendly": true, "displayUrl": "https://<b>www.cde.state.co.us</b>/educatoreffectiveness/iraguide", "snippet": "<b>Inter-rater</b> . reliability . is . t. he degree of <b>agreement</b> in the <b>ratings</b> that <b>two</b> or more observers assign to the same behavior or observation (McREL, 2004). In other words, when one rates a. Version 1For feedback, comments or questions, please email Educator_Effectiveness@cde.<b>state.co.us</b>. teacher a three overall, the other consistently rates ...", "dateLastCrawled": "2022-01-31T22:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Inter-rater</b> reliability of the QuIS as an assessment of the quality of ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5142422/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC5142422", "snippet": "We summarise <b>inter-rater</b> reliability using kappa (\u03ba) which quantifies the extent to which <b>two</b> raters agree in their <b>ratings</b>, over and above the <b>agreement</b> expected through chance alone. This is the most frequently used presentation of <b>inter-rater</b> reliability in applied health research, and is thus familiar to researchers in the area. When \u03ba is calculated all differences in <b>ratings</b> are treated equally. Varying severity of disagreement between raters depending on the categories concerned <b>can</b> ...", "dateLastCrawled": "2021-07-02T11:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Inter-rater agreement with a variable number of raters, which solution</b> ...", "url": "https://www.researchgate.net/post/Inter-rater_agreement_with_a_variable_number_of_raters_which_solution_is_the_best", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/post/<b>Inter-rater_agreement_with_a_variable_number</b>_of...", "snippet": "Cohen\u2019s kappa coefficient is commonly used for assessing <b>agreement</b> between classifications <b>of two</b> raters on a nominal scale. Three variants of Cohen\u2019s kappa that <b>can</b> handle missing data are ...", "dateLastCrawled": "2022-01-14T06:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Intraclass Correlations (ICC</b>) and <b>Interrater Reliability in SPSS</b>", "url": "https://neoacademic.com/2011/11/16/computing-intraclass-correlations-icc-as-estimates-of-interrater-reliability-in-spss/", "isFamilyFriendly": true, "displayUrl": "https://neoacademic.com/2011/11/16/computing-<b>intraclass-correlations-icc</b>-as", "snippet": "For <b>inter-rater</b> reliability, I have taken the mean of the <b>two</b> days for each rater, and used ICC(2,1) as I am interested in absolute <b>agreement</b> and single measures. However, what statistic would you use for intra-rater reliability/ test-retest between the days? I know you mentioned Pearsons earlier in response to a post, but recent texts have recommended that because Pearsons is a relative form of reliability, a better approach is to use ICC for the relative form of reliability and SEM as an ...", "dateLastCrawled": "2022-01-31T10:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Inter/Intrarater reliability - which statistical test? | Statistics ...", "url": "http://www.talkstats.com/threads/inter-intrarater-reliability-which-statistical-test.19158/", "isFamilyFriendly": true, "displayUrl": "www.talkstats.com/threads/inter-intrarater-reliability-which-statistical-test.19158", "snippet": "If we are measuring the voice quality <b>of two</b> voice recordings rec_1 and rec_2. The <b>two</b> hypothetical <b>ratings</b> could be (for me and then my colleague): My Rating. Rec_1: G = 2, R = 0, B = 2, A = 0, S = 0 ; Rec_2 G = 2, R = 2, B = 0, A = 0, S = 0; My Colleague&#39;s Rating. Rec_1: G = 2, R = 2, B = 0, A = 0, S = 0; Rec_2: G = 2, R = 0, B = 2, A = 0, S = 0; Here the totals are the same for each of our <b>ratings</b>. However, there&#39;s a difference - I <b>thought</b> that Rec_1 had moderately breathy voice and no ...", "dateLastCrawled": "2021-12-14T01:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "hypothesis testing - Compare a diagnostic test to <b>gold standard</b> - Cross ...", "url": "https://stats.stackexchange.com/questions/352269/compare-a-diagnostic-test-to-gold-standard", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/352269", "snippet": "At first I <b>thought</b> this was a case for McNemar&#39;s test, but it seems this test is meant to compare 2 experimental test results against the ... an <b>inter-rater</b> agreeement test like Kappa certainly seems more appropriate. $\\endgroup$ \u2013 klumbard. Jun 20 &#39;18 at 14:56 . 2 $\\begingroup$ Your question has already been answered but I&#39;d like to point out that your data set is criminally under-powered for the question you&#39;re asking. Only the off-diagonal cells contribute to McNemar&#39;s so you have an ...", "dateLastCrawled": "2022-01-24T03:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Reliability, validity and relevance of</b> needs assessment inst... : JBI ...", "url": "https://journals.lww.com/jbisrir/Fulltext/2020/04000/Reliability,_validity_and_relevance_of_needs.4.aspx", "isFamilyFriendly": true, "displayUrl": "https://<b>journals.lww.com</b>/jbisrir/Fulltext/2020/04000/Reliability,_validity_and...", "snippet": "For CNA-D, test-retest and <b>inter-rater</b> <b>agreement</b> were evaluated in a combined way by having <b>two</b> interviews conducted by <b>two</b> different persons <b>two</b> weeks apart. For the CARENAP, only the <b>inter-rater</b> <b>agreement</b> was tested, based on a simultaneous evaluation made by an interviewer and an observer. The time interval between the <b>two</b> measurements (<b>two</b> or three weeks) was clearly stated (Q6) in four of the five studies. The exception was Tayside, where the time interval was highly variable, with an ...", "dateLastCrawled": "2022-01-24T05:13:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Comparing inter-rater agreement between classes</b> of raters - <b>Cross Validated</b>", "url": "https://stats.stackexchange.com/questions/132609/comparing-inter-rater-agreement-between-classes-of-raters", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/132609/<b>comparing</b>-<b>inter-rater</b>-<b>agreement</b>...", "snippet": "<b>Agreement</b>. This focuses on absolute <b>agreement</b> between raters - if I give it a 2, you will give it a 2. Here are the steps I would take: 1) Krippendorff&#39;s $\\alpha$ across both groups. This is going to be an overall benchmark. 2) Krippendorff&#39;s $\\alpha$ for each group separately. Compare the <b>two</b> coefficients, and see which group has a higher ...", "dateLastCrawled": "2022-01-22T06:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "How to assess and compare <b>inter-rater</b> reliability, <b>agreement</b> and ...", "url": "https://www.frontiersin.org/articles/10.3389/fpsyg.2014.00509/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/fpsyg.2014.00509", "snippet": "In summary, this report has <b>two</b> main goals: to provide a methodological tutorial for assessing <b>inter-rater</b> reliability, <b>agreement</b> and linear correlation of rating pairs, and to evaluate whether the German parent questionnaire ELAN (Bockmann and Kiese-Himmel, 2006) <b>can</b> be reliably employed also with daycare teachers when assessing early expressive vocabulary development. We <b>compared</b> mother\u2013father and parent\u2013teacher <b>ratings</b> with regard to <b>agreement</b>, correlation as well as reliability of ...", "dateLastCrawled": "2022-01-29T02:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Interrater</b> reliability: the kappa statistic", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3900052/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC3900052", "snippet": "For percent <b>agreement</b>, 61% <b>agreement</b> <b>can</b> immediately be seen as problematic. Almost 40% of the data in the dataset represent faulty data. In healthcare research, this could lead to recommendations for changing practice based on faulty evidence. For a clinical laboratory, having 40% of the sample evaluations being wrong would be an extremely serious quality problem. This is the reason that many texts recommend 80% <b>agreement</b> as the minimum acceptable <b>interrater</b> <b>agreement</b>. Given the reduction ...", "dateLastCrawled": "2022-01-26T07:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) Evaluation of <b>Inter-Rater</b> <b>Agreement</b> and <b>Inter-Rater</b> Reliability ...", "url": "https://www.researchgate.net/publication/273451591_Evaluation_of_Inter-Rater_Agreement_and_Inter-Rater_Reliability_for_Observational_Data_An_Overview_of_Concepts_and_Methods", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/273451591_Evaluation_of_<b>Inter-Rater</b>_<b>Agreement</b>...", "snippet": "<b>Inter-rater</b> <b>agreement</b> and reliability are fundamental to the evaluation of new approaches or methods in various fields [26].In this study, we calculated the <b>agreement</b> and reliability indices of ...", "dateLastCrawled": "2022-01-26T02:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Comparing</b> and Improving <b>Inter-Rater Reliability</b> for Job References", "url": "https://ukdiss.com/examples/inter-rater-reliability-job-references.php", "isFamilyFriendly": true, "displayUrl": "https://ukdiss.com/examples/<b>inter-rater-reliability</b>-job-references.php", "snippet": "** significant at p = &lt;.001. <b>Inter-rater reliability</b> results. All analyses for <b>inter-rater reliability</b> results were run in R 3.5.2. Since raters were randomly assigned and the level of <b>agreement</b> between raters was the target variable, analyses for absolute <b>agreement</b>, one-way random effect ICCs (1, k) (Shrout &amp; Fleiss, 1979) were performed. A <b>two</b>-way F-test for equality of SD was performed to test for significant difference in variance between the conditions.A significantly lower SD value ...", "dateLastCrawled": "2022-01-23T08:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Which measure of <b>inter-rater agreement is appropriate with diverse</b> ...", "url": "https://www.researchgate.net/post/Which-measure-of-inter-rater-agreement-is-appropriate-with-diverse-multiple-raters", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/post/Which-measure-of-<b>inter-rater</b>-<b>agreement</b>-is...", "snippet": "Siong Ting Wong. Ministry of Health Malaysia. You <b>can</b> consider Gwet&#39;s AC1 using R program. Gwet&#39;s AC1 is an alternative to Fleiss&#39; kappa which critics of prevalence bias (low kappa value despite ...", "dateLastCrawled": "2022-02-02T16:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "What is <b>inter-rater</b> reliability and why is it important?", "url": "https://psichologyanswers.com/library/lecture/read/341855-what-is-inter-rater-reliability-and-why-is-it-important", "isFamilyFriendly": true, "displayUrl": "https://psichologyanswers.com/library/lecture/read/341855-what-is-<b>inter-rater</b>...", "snippet": "<b>Inter-Rater</b> Reliability Methods. Count the number of <b>ratings</b> in <b>agreement</b>. In the above table, that&#39;s 3. Count the total number of <b>ratings</b>. For this example, that&#39;s 5. Divide the total by the number in <b>agreement</b> to get a fraction: 3/5. Convert to a percentage: 3/5 = 60%. How do you improve <b>inter rater</b> reliability in psychology? Where observer scores do not significantly correlate then reliability <b>can</b> be improved by: Training observers in the observation techniques being used and making sure ...", "dateLastCrawled": "2022-02-02T14:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Understanding Interobserver <b>Agreement</b>: The Kappa Statistic", "url": "http://web2.cs.columbia.edu/~julia/courses/CS6998/Interrater_agreement.Kappa_statistic.pdf", "isFamilyFriendly": true, "displayUrl": "web2.cs.columbia.edu/~julia/courses/CS6998/<b>Interrater</b>_<b>agreement</b>.Kappa_statistic.pdf", "snippet": "But if the <b>people</b> who ac-tually interpret the test cannot agree on the interpreta-tion, the test results will be of little use. Let us suppose that you are preparing to give a lec-ture on community-acquired pneumonia. As you pre-pare for the lecture, you read an article titled, \u201cDiag-nosing Pneumonia by History and Physical Examina-tion,\u201d published in the Journal of the American Medi-cal Association in 1997. 1 You come across a table in the article that shows <b>agreement</b> on physical ...", "dateLastCrawled": "2022-01-28T23:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Cohen&#39;s <b>kappa</b> in SPSS Statistics - Procedure, output and interpretation ...", "url": "https://statistics.laerd.com/spss-tutorials/cohens-kappa-in-spss-statistics.php", "isFamilyFriendly": true, "displayUrl": "https://<b>statistics.laerd.com</b>/spss-tutorials/cohens-<b>kappa</b>-in-spss-statistics.php", "snippet": "Cohen&#39;s <b>kappa</b> (\u03ba) is such a measure of <b>inter-rater</b> <b>agreement</b> for categorical scales when there are <b>two</b> raters (where \u03ba is the lower-case Greek letter &#39;<b>kappa</b>&#39;). There are many occasions when you need to determine the <b>agreement</b> between <b>two</b> raters. For example, the head of a local medical practice might want to determine whether <b>two</b> experienced ...", "dateLastCrawled": "2022-01-30T18:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "The Barthel <b>Index: comparing inter-rater reliability between</b> Nurses and ...", "url": "https://www.sciencedirect.com/science/article/pii/S0897189709001256", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0897189709001256", "snippet": "Because no gold standard exists against which to measure physical function, this study <b>compared</b> <b>two</b> nurses&#39; BI scores with <b>two</b> doctors&#39; BI scores, and the professional group with higher intra-<b>agreement</b> indicated the most accurate BI score and the more reliable method. Providing the guidelines on the reverse side of the BI allowed clear definition of each performance level for each item in the BI, hence standardizing the method of assessment. There was a strong rank correlation between ...", "dateLastCrawled": "2021-10-24T07:38:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Understanding Interobserver <b>Agreement</b>: The Kappa Statistic", "url": "http://web2.cs.columbia.edu/~julia/courses/CS6998/Interrater_agreement.Kappa_statistic.pdf", "isFamilyFriendly": true, "displayUrl": "web2.cs.columbia.edu/~julia/courses/CS6998/<b>Interrater</b>_<b>agreement</b>.Kappa_statistic.pdf", "snippet": "call the <b>analogy</b> of a target and how close we get to the bull\u2019s-eye (Figure 1). If we actually hit the bull\u2019s-eye (representing <b>agreement</b> with the gold standard), we are accurate. If all our shots land together, we have good precision (good reliability). If all our shots land together and we hit the bull\u2019s-eye, we are accurate as well as precise. It is possible, however, to hit the bull\u2019s-eye purely by chance. Referring to Figure 1, only the center black dot in target A is accurate ...", "dateLastCrawled": "2022-01-28T23:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Leveraging Inter-rater Agreement for Audio-Visual Emotion Recognition</b>", "url": "https://www.researchgate.net/publication/283487589_Leveraging_Inter-rater_Agreement_for_Audio-Visual_Emotion_Recognition", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/283487589_Leveraging_<b>Inter-rater</b>_<b>Agreement</b>...", "snippet": "In <b>machine</b> <b>learning</b> tasks an actual \u2018ground truth\u2019 may not be available. Then, machines often have to rely on human labelling of data. This becomes challenging the more subjective the <b>learning</b> ...", "dateLastCrawled": "2021-08-28T14:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Machine</b> <b>learning</b> glossary - DataTime", "url": "https://www.dtalg.com/article-1/", "isFamilyFriendly": true, "displayUrl": "https://www.dtalg.com/article-1", "snippet": "See also Cohen\u2019s kappa, which is one of the most popular <b>inter-rater</b> <b>agreement</b> measurements. intersection over union (IoU) #image. The intersection of two sets divided by their union. In <b>machine</b>-<b>learning</b> image-detection tasks, IoU is used to measure the accuracy of the model\u2019s predicted bounding box with respect to the ground-truth bounding ...", "dateLastCrawled": "2022-01-25T17:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Multilingual <b>Twitter Sentiment Classification</b>: The Role of Human ... - PLOS", "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0155036", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0155036", "snippet": "The researchers in the fields of <b>inter-rater</b> <b>agreement</b> and <b>machine</b> <b>learning</b> typically employ different evaluation measures. We report all the results in terms of four selected measures which we deem appropriate for the three-valued sentiment classification task (the details are in the Evaluation measures subsection in Methods). In this section, however, the results are summarized only in terms of Krippendorff\u2019s Alpha-reliability Alpha) , to highlight the main conclusions. Alpha is a ...", "dateLastCrawled": "2021-03-30T23:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Clinician perspectives on <b>machine</b> <b>learning</b> prognostic algorithms in the ...", "url": "https://link.springer.com/article/10.1007/s00520-021-06774-w", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00520-021-06774-w", "snippet": "<b>Machine</b> <b>learning</b> algorithms may accurately predict mortality risk in cancer, but it is unclear how oncology clinicians would use such algorithms in practice. The purpose of this qualitative study was to assess oncology clinicians\u2019 perceptions on the utility and barriers of <b>machine</b> <b>learning</b> prognostic algorithms to prompt advance care planning. Participants included medical oncology physicians and advanced practice providers (APPs) practicing in tertiary and community practices within a ...", "dateLastCrawled": "2022-01-30T10:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Analyzing and Interpreting Data From Rating Scales</b> | by Kevin C Lee ...", "url": "https://towardsdatascience.com/analyzing-and-interpreting-data-from-rating-scales-d169d66211db", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>analyzing-and-interpreting-data-from-rating-scales</b>-d169...", "snippet": "<b>Inter-Rater</b> Reliability. In B), we plot the pairwise correlations between the students with a heatmap. Most of the correlations are &gt; 0.6 with a few exceptions. A small number of respondents showing low correlations with others is acceptable as long as most students are able to respond similarly. P.S. The use of Pearson Correlation is only ...", "dateLastCrawled": "2022-01-29T00:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Target <b>analogy</b> of accuracy and precision | Download Scientific Diagram", "url": "https://researchgate.net/figure/Target-analogy-of-accuracy-and-precision_fig1_24399044", "isFamilyFriendly": true, "displayUrl": "https://researchgate.net/figure/Target-<b>analogy</b>-of-accuracy-and-precision_fig1_24399044", "snippet": "The intraclass correlation coefficient (ICC) was calculated to assess intra-rater and <b>inter-rater</b> <b>agreement</b> of I 3M . 31 A sample of OPTs was randomly divided into training dataset (819) and test ...", "dateLastCrawled": "2021-06-28T05:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Use of analogies, metaphors, and similes by students and reviewers at ...", "url": "https://www.cambridge.org/core/journals/ai-edam/article/use-of-analogies-metaphors-and-similes-by-students-and-reviewers-at-an-undergraduate-architectural-design-review/FB80EB57099A898FE15564497D5B06C7", "isFamilyFriendly": true, "displayUrl": "https://www.cambridge.org/core/journals/ai-edam/article/use-of-analogies-metaphors-and...", "snippet": "We used the Delphi Method to determine the <b>inter-rater</b> <b>agreement</b>. In the first step after the second round of discussion, there was 66.67% <b>agreement</b> between the authors\u2019 coding and that of the independent coder. In the second step, <b>agreement</b> on the type of similarities was determined using the Delphi Method. At the end of second round of discussions, there was 90.1% <b>agreement</b>. Table 1. Categories and sub-categories used for coding the reviews. Any statement which explicitly or implicitly ...", "dateLastCrawled": "2022-02-02T16:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Frontiers | From What to Why, the Growing Need for a Focus Shift Toward ...", "url": "https://www.frontiersin.org/articles/10.3389/fphys.2021.821217/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/fphys.2021.821217", "snippet": "Explainable AI is far from a novel concept in the <b>machine</b> <b>learning</b> (ML) community (Goebel et al., 2018; Tosun et al., 2020a,b). While the presentation of new approaches for post-hoc explainers of deep convolutional neural networks (CNNs) is outside of the scope of this review, there are a few simple steps that can increase the interpretability and explainability of an AI-driven study ( Figure 1 ).", "dateLastCrawled": "2022-02-03T06:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Quadratic weighted kappa</b> strength of <b>agreement</b> - Cross Validated", "url": "https://stats.stackexchange.com/questions/46296/quadratic-weighted-kappa-strength-of-agreement", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/46296", "snippet": "In the case of the kappa-value there are some attempts to qualify how good or bad the agreements are. For example Landis &amp; Koch in the article The Measurement of Observer <b>Agreement</b> for Categorical Data talks about &quot;strength of <b>agreement</b>&quot; based on kappa values:. Kappa Strength of <b>agreement</b> ===== ===== 0.0-0.20 Slight 0.21-0.40 Fair 0.41-0.60 Moderate 0.61-0.80 Substantial 0.81-0.90 Almost perfect", "dateLastCrawled": "2022-01-20T17:56:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Reliability and Learnability of Human Bandit Feedback for Sequence-to ...", "url": "https://aclanthology.org/P18-1165.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/P18-1165.pdf", "snippet": "intra- and <b>inter-rater agreement is similar</b> for both tasks, with highest inter-rater reliability for stan-dardized 5-point ratings. In a next step, we address the issue of <b>machine</b> learnability of human rewards. We use deep learn- ing models to train reward estimators by regres-sion against cardinal feedback, and by \ufb01tting a Bradley-Terry model (Bradley and Terry,1952) to ordinal feedback. Learnability is understood by a slight misuse of the <b>machine</b> <b>learning</b> notion of learnability (Shalev ...", "dateLastCrawled": "2021-12-22T00:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "arXiv:1805.10627v3 [cs.CL] 13 Dec 2018", "url": "https://www.researchgate.net/profile/Joshua-Uyheng/publication/325413588_Reliability_and_Learnability_of_Human_Bandit_Feedback_for_Sequence-to-Sequence_Reinforcement_Learning/links/5ea04de5a6fdccd7cee0eebe/Reliability-and-Learnability-of-Human-Bandit-Feedback-for-Sequence-to-Sequence-Reinforcement-Learning.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/profile/Joshua-Uyheng/publication/325413588_Reliability...", "snippet": "\ufb01ed by bandit <b>learning</b> for neural <b>machine</b> trans-lation (NMT). Our aim is to show that successful <b>learning</b> from simulated bandit feedback (Sokolov et al.,2016b;Kreutzer et al.,2017;Nguyen et al ...", "dateLastCrawled": "2021-08-22T12:54:00.0000000Z", "language": "en", "isNavigational": false}], [], [], []], "all_bing_queries": ["+(inter-rater agreement)  is like +(comparing ratings of two people)", "+(inter-rater agreement) is similar to +(comparing ratings of two people)", "+(inter-rater agreement) can be thought of as +(comparing ratings of two people)", "+(inter-rater agreement) can be compared to +(comparing ratings of two people)", "machine learning +(inter-rater agreement AND analogy)", "machine learning +(\"inter-rater agreement is like\")", "machine learning +(\"inter-rater agreement is similar\")", "machine learning +(\"just as inter-rater agreement\")", "machine learning +(\"inter-rater agreement can be thought of as\")", "machine learning +(\"inter-rater agreement can be compared to\")"]}