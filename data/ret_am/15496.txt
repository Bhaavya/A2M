{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "What is <b>demographic</b> <b>parity</b> in <b>Machine</b> <b>Learning</b> \u2013 Ramsey Elbasheer ...", "url": "https://ramseyelbasheer.io/2021/12/27/what-is-demographic-parity-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://ramseyelbasheer.io/2021/12/27/what-is-<b>demographic</b>-<b>parity</b>-in-<b>machine</b>-<b>learning</b>", "snippet": "What is <b>demographic</b> <b>parity</b> in <b>Machine</b> <b>Learning</b>. Posted by Ramsey Elbasheer December 27, 2021 December 27, 2021 Posted in Computing Tags: AI, <b>Machine</b> <b>Learning</b>. Original Source Here. A fairness metric is satisfied if the results of a model\u2019s classification are not dependent on a given sensitive attribute. Continue reading on Medium \u00bb AI/ML. Trending AI/ML Article Identified &amp; Digested via Granola by Ramsey Elbasheer; a <b>Machine</b>-Driven RSS Bot. Share this: Twitter; Facebook; <b>Like</b> this: <b>Like</b> ...", "dateLastCrawled": "2022-01-20T18:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "What <b>does \u201cfairness\u201d mean for machine learning systems</b>?", "url": "https://haas.berkeley.edu/wp-content/uploads/What-is-fairness_-EGAL2.pdf", "isFamilyFriendly": true, "displayUrl": "https://haas.berkeley.edu/wp-content/uploads/What-is-fairness_-EGAL2.pdf", "snippet": "<b>demographic</b> <b>parity</b> in particular may seem <b>like</b> a good solution but is a simplistic ap-proach to fairness that can still be at odds with other definitions of fairness5-- such as justice. Also, even if satisfying <b>demographic</b> <b>parity</b> based on gender, for example, when overlaying race on top of gender, this <b>parity</b> can be off. It\u2019s also important ...", "dateLastCrawled": "2022-01-28T05:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Measuring <b>Fairness</b> in <b>Machine</b> <b>Learning</b> Models", "url": "https://blog.dataiku.com/measuring-fairness-in-machine-learning-models", "isFamilyFriendly": true, "displayUrl": "https://blog.dataiku.com/measuring-<b>fairness</b>-in-<b>machine</b>-<b>learning</b>-models", "snippet": "The Impossibility Theorem of <b>Fairness</b> proves that <b>Demographic</b> <b>Parity</b>, Equality of Odds, and Predictive Rate <b>Parity</b> are pairwise incompatible, which makes satisfying all <b>fairness</b> definitions impossible. Therefore, we face a practical dilemma when it comes to designing fair <b>machine</b> <b>learning</b> models \u2014 there\u2019s no \u201cbest\u201d answer.", "dateLastCrawled": "2022-01-25T22:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Tutorial on <b>Fairness</b> in <b>Machine</b> <b>Learning</b> | by Ziyuan Zhong | Towards ...", "url": "https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-tutorial-on-<b>fairness</b>-in-<b>machine</b>-<b>learning</b>-3ff8ba1040cb", "snippet": "<b>Machine</b> <b>Learning</b>, the most widely used AI techniques, relies heavily on data. It is a common misconception that AI is absolutely objective. AI is objective only in the sense of <b>learning</b> what human teaches. The data provided by human can be highly-biased. It has been found in 2016 that COMPAS, the algorithm used for recidivism prediction produces much higher false positive rate for black people than white people(see Fig2, Larson et al. ProPublica, 2016). Fig2: The bias in COMPAS. (from Larson ...", "dateLastCrawled": "2022-02-01T18:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine</b> <b>Learning</b> Glossary: <b>Fairness</b> | <b>Google Developers</b>", "url": "https://developers.google.com/machine-learning/glossary/fairness", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine</b>-<b>learning</b>/glossary/<b>fairness</b>", "snippet": "For example, if both Lilliputians and Brobdingnagians apply to Glubbdubdrib University, <b>demographic</b> <b>parity</b> is achieved if the percentage of Lilliputians admitted is the same as the percentage of Brobdingnagians admitted, irrespective of whether one group is on average more qualified than the other. Contrast with equalized odds and equality of opportunity, which permit classification results in aggregate to depend on sensitive attributes, but do not permit classification results for certain ...", "dateLastCrawled": "2022-02-02T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Bias and Fairness in <b>Machine</b> <b>Learning</b>, Part 1: introducing our dataset ...", "url": "https://freecontent.manning.com/bias-and-fairness-in-machine-learning-part-1-introducing-our-dataset-and-the-problem/", "isFamilyFriendly": true, "displayUrl": "https://freecontent.manning.com/bias-and-fairness-in-<b>machine</b>-<b>learning</b>-part-1...", "snippet": "Statistical <b>Parity</b>. Statistical <b>Parity</b>, also known as <b>Demographic</b> <b>Parity</b> or Disparate Impact, is a very common definition for fairness. Simply put, it states that our model\u2019s prediction of being in a certain class (will they recidivate or not) is independent of the sensitive feature. Put as a formula:", "dateLastCrawled": "2022-01-29T20:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Fairness in <b>Machine</b> <b>Learning</b>", "url": "https://www.cs.toronto.edu/~guerzhoy/310f19/lec/W11/fairness_new.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.toronto.edu/~guerzhoy/310f19/lec/W11/fairness_new.pdf", "snippet": "\u2022<b>Demographic</b> <b>parity</b> \u2022\ud835\udc43 =1 =0=\ud835\udc43 =1 =1 \u2022Everyone is predicted to re-offend at the same rate, regardless of <b>demographic</b> \u2022A type of \u201cclassification <b>parity</b>\u201d \u2022False positive <b>parity</b> (\u201cequal opportunity\u201d) \u2022\ud835\udc43 =1 =0, =0=\ud835\udc43 =1 =1, =0 \u2022People who did not reoffend predicted to reoffend at the same rate, regardless", "dateLastCrawled": "2022-01-27T11:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Controllable Fairness in Machine Learning</b>", "url": "https://ermongroup.github.io/blog/controllable-fairness/", "isFamilyFriendly": true, "displayUrl": "https://ermongroup.github.io/blog/controllable-fairness", "snippet": "Another potential limitation of this work is that we, <b>like</b> much of the fair <b>machine</b> <b>learning</b> community, center <b>demographic</b> <b>parity</b>, equality of odds, and equality of opportunity notions of fairness. We believe that future work will need to develop deeper connections to social-justice-informed notions of equity if it is to avoid shallow technosolutionism and build more equitable <b>machine</b> <b>learning</b> (Onuoha and Cyborg 2018). Footnotes . For conciseness, we focus on <b>demographic</b> <b>parity</b>, a pretty ...", "dateLastCrawled": "2021-12-20T18:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Controllable Fairness in <b>Machine</b> <b>Learning</b> | SAIL Blog", "url": "http://ai.stanford.edu/blog/controllable-fairness/", "isFamilyFriendly": true, "displayUrl": "ai.stanford.edu/blog/controllable-fairness", "snippet": "Another potential limitation of this work is that we, <b>like</b> much of the fair <b>machine</b> <b>learning</b> community, center <b>demographic</b> <b>parity</b>, equality of odds, and equality of opportunity notions of fairness. We believe that future work will need to develop deeper connections to social-justice-informed notions of equity if it is to avoid shallow technosolutionism and build more equitable <b>machine</b> <b>learning</b> 9 .", "dateLastCrawled": "2022-01-30T20:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Responsible AI in action, locally</b> <b>and on Azure Machine Learning</b> | by ...", "url": "https://medium.com/microsoftazure/responsible-ai-in-action-locally-and-on-azure-machine-learning-a515e0585e69", "isFamilyFriendly": true, "displayUrl": "https://medium.com/microsoftazure/<b>responsible-ai-in-action-locally</b>-and-on-azure...", "snippet": "Since <b>demographic</b> <b>parity</b> requires that individuals are offered the opportunity independent of membership in the sensitive class, we start building this object that prepares a pool of 70 models ...", "dateLastCrawled": "2022-01-22T05:42:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A Review on Fairness in <b>Machine</b> <b>Learning</b> | ACM Computing Surveys", "url": "https://dl.acm.org/doi/10.1145/3494672", "isFamilyFriendly": true, "displayUrl": "https://dl.acm.org/doi/10.1145/3494672", "snippet": "The model requires <b>demographic</b> <b>parity</b> while satisfying as much individual fairness as possible (recall that individual fairness requires that <b>similar</b> individuals be treated similarly), as well as a utility requirement. They additionally discuss the importance of defining the distances between individuals in a manner that strong individuals of both groups are considered <b>similar</b>, instead of similarity that is derived mostly from the characteristics of the membership in the same group.", "dateLastCrawled": "2022-02-07T09:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Machine</b> <b>Learning</b> Glossary: <b>Fairness</b> | <b>Google Developers</b>", "url": "https://developers.google.com/machine-learning/glossary/fairness", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine</b>-<b>learning</b>/glossary/<b>fairness</b>", "snippet": "For example, if both Lilliputians and Brobdingnagians apply to Glubbdubdrib University, <b>demographic</b> <b>parity</b> is achieved if the percentage of Lilliputians admitted is the same as the percentage of Brobdingnagians admitted, irrespective of whether one group is on average more qualified than the other. Contrast with equalized odds and equality of opportunity, which permit classification results in aggregate to depend on sensitive attributes, but do not permit classification results for certain ...", "dateLastCrawled": "2022-02-02T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Training a biased model</b> | <b>Machine Learning Bias Mitigation</b>", "url": "https://cdeiuk.github.io/bias-mitigation/baseline/", "isFamilyFriendly": true, "displayUrl": "https://cdeiuk.github.io/bias-mitigation/baseline", "snippet": "<b>Demographic</b> <b>parity</b> requires that we treat all <b>demographic</b> groups equally. We start by investigating the disparity between the sexes. We do this with box plots of the model scores. A higher score means the model thinks the individual is more likely to be a high earner. It&#39;s clear that there is a major disparity between men and women, with men being awarded systematically higher scores. This model therefore does not achieve <b>demographic</b> <b>parity</b> by some margin.", "dateLastCrawled": "2022-01-24T18:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Tutorial on <b>Fairness</b> in <b>Machine</b> <b>Learning</b> | by Ziyuan Zhong | Towards ...", "url": "https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-tutorial-on-<b>fairness</b>-in-<b>machine</b>-<b>learning</b>-3ff8ba1040cb", "snippet": "<b>Machine</b> <b>Learning</b>, the most widely used AI techniques, relies heavily on data. It is a common misconception that AI is absolutely objective. AI is objective only in the sense of <b>learning</b> what human teaches. The data provided by human can be highly-biased. It has been found in 2016 that COMPAS, the algorithm used for recidivism prediction produces much higher false positive rate for black people than white people(see Fig2, Larson et al. ProPublica, 2016). Fig2: The bias in COMPAS. (from Larson ...", "dateLastCrawled": "2022-02-01T18:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Measuring <b>Fairness</b> in <b>Machine</b> <b>Learning</b> Models", "url": "https://blog.dataiku.com/measuring-fairness-in-machine-learning-models", "isFamilyFriendly": true, "displayUrl": "https://blog.dataiku.com/measuring-<b>fairness</b>-in-<b>machine</b>-<b>learning</b>-models", "snippet": "The Impossibility Theorem of <b>Fairness</b> proves that <b>Demographic</b> <b>Parity</b>, Equality of Odds, and Predictive Rate <b>Parity</b> are pairwise incompatible, which makes satisfying all <b>fairness</b> definitions impossible. Therefore, we face a practical dilemma when it comes to designing fair <b>machine</b> <b>learning</b> models \u2014 there\u2019s no \u201cbest\u201d answer.", "dateLastCrawled": "2022-01-25T22:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "AI Fairness | Data Science Portfolio", "url": "https://sourestdeeds.github.io/blog/ai-fairness/", "isFamilyFriendly": true, "displayUrl": "https://sourestdeeds.github.io/blog/ai-fairness", "snippet": "<b>Demographic</b> <b>parity</b> / statistical <b>parity</b>. <b>Demographic</b> <b>parity</b> says the model is fair if the composition of people who are selected by the model matches the group membership percentages of the applicants. A nonprofit is organizing an international conference, and 20,000 people have signed up to attend. The organizers write a ML model to select 100 attendees who could potentially give interesting talks at the conference. Since 50% of the attendees will be women (10,000 out of 20,000), they ...", "dateLastCrawled": "2022-02-02T22:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Interventions | <b>Machine Learning Bias Mitigation</b>", "url": "https://cdeiuk.github.io/bias-mitigation/interventions/", "isFamilyFriendly": true, "displayUrl": "https://cdeiuk.github.io/bias-mitigation/interventions", "snippet": "<b>Demographic</b> <b>parity</b>. The intervention reduced <b>demographic</b> <b>parity</b> difference only slightly, from 0.193 to 0.174. Since we exclude a priori available information from the training process it is reasonable to expect some reduction in accuracy. However, the influence on achieved accuracy is small, reducing it from 85.3% to 84.9%.", "dateLastCrawled": "2022-02-02T02:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Explaining Measures of <b>Fairness</b>. Avoid the black-box use of <b>fairness</b> ...", "url": "https://towardsdatascience.com/explaining-measures-of-fairness-f0e419d4e0d7", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/explaining-measures-of-<b>fairness</b>-f0e419d4e0d7", "snippet": "<b>Demographic</b> <b>parity</b> states that the output of the <b>machine</b> <b>learning</b> model should be equal between two or more groups. The <b>demographic</b> <b>parity</b> difference is then a measure of how much disparity there is between model outcomes in two groups of samples. Since SHAP decomposes the model output into feature attributions with the same units as the original model output, we can first decompose the model output among each of the input features using SHAP, and then compute the <b>demographic</b> <b>parity</b> ...", "dateLastCrawled": "2022-02-01T08:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "What is fair <b>machine</b> <b>learning</b>? Depends on your definition of fair ...", "url": "https://blogs.ischool.berkeley.edu/w231/2021/07/09/what-is-fair-machine-learning-depends-on-your-definition-of-fair/", "isFamilyFriendly": true, "displayUrl": "https://blogs.ischool.berkeley.edu/w231/2021/07/09/what-is-fair-<b>machine</b>-<b>learning</b>...", "snippet": "One common measure is statistical or <b>demographic</b> <b>parity</b>. Suppose we had an algorithm that screened job applicants based on their resumes \u2014 we would achieve statistical <b>parity</b> across gender if the fraction of acceptances from each gender category was the same. In other words, if the model accepted 40% of the female applicants, it should accept roughly 40% of the applicants from each of the other gender categories as well.", "dateLastCrawled": "2021-12-27T02:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "RStudio AI Blog: Starting to think about AI Fairness", "url": "https://blogs.rstudio.com/ai/posts/2021-07-15-ai-fairness/", "isFamilyFriendly": true, "displayUrl": "https://blogs.rstudio.com/ai/posts/2021-07-15-ai-fairness", "snippet": "<b>Similar</b> in spirit to <b>demographic</b> <b>parity</b>, but possibly leading to different outcomes in practice, is conditional <b>demographic</b> <b>parity</b>. 3 Here we additionally take into account other predictors in the dataset; to be precise: all other predictors. The desiderate now is that for any choice of attributes, outcome proportions should be equal, given the protected attribute and the other attributes in question. I\u2019ll come back to why this may sound better in theory than work in practice in the next ...", "dateLastCrawled": "2022-01-31T09:59:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "The <b>Impossibility Theorem</b> of <b>Machine</b> Fairness \u2013 A Causal Perspective ...", "url": "https://deepai.org/publication/the-impossibility-theorem-of-machine-fairness-a-causal-perspective", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/the-<b>impossibility-theorem</b>-of-<b>machine</b>-fairness-a-causal...", "snippet": "This entails that <b>demographic</b> <b>parity</b> <b>can</b> not hold in such a model. ... The goal of the <b>learning</b> algorithm <b>can</b> then <b>be thought</b> of as manipulating the past data and making it consistent with a notion of fairness in order to make fair predictions in the future. This is a way to ensure fairness as classification models usually assume that data is independent and identically drawn from the same distribution. Conceptually, this <b>can</b> <b>be thought</b> of as introducing a correction to the labels in the ...", "dateLastCrawled": "2022-01-27T23:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Handling Discriminatory Biases in Data for <b>Machine Learning</b> | by ...", "url": "https://towardsdatascience.com/machine-learning-and-discrimination-2ed1a8b01038", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>machine-learning</b>-and-<b>discrimination</b>-2ed1a8b01038", "snippet": "<b>Demographic</b> <b>parity</b> proposes that the decision (the target variable) should be independent of protected attributes \u2014 race, ... <b>Can</b> <b>Machine Learning</b> Help to Reduce <b>Discrimination</b>? <b>Machine learning</b> is an extremely powerful tool. This is increasingly clear as humanity begins to transition from humanist to dataist perspectives\u2014 where we begin to trust algorithms and data more than people or our own thoughts (some people have driven into lakes because their GPS told them too!). This makes it ...", "dateLastCrawled": "2022-01-29T20:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) Fairness of <b>Machine</b> <b>Learning</b> Algorithms in Demography", "url": "https://www.researchgate.net/publication/358290931_Fairness_of_Machine_Learning_Algorithms_in_Demography", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/358290931_Fairness_of_<b>Machine</b>_<b>Learning</b>...", "snippet": "it <b>can</b> smoothly be implanted to other <b>Machine</b> <b>Learning</b> models and data types, together with di\ufb00erent explanatory models. An innov ative approach, such as [23],", "dateLastCrawled": "2022-02-06T02:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Review on Fairness in <b>Machine</b> <b>Learning</b> | ACM Computing Surveys", "url": "https://dl.acm.org/doi/10.1145/3494672", "isFamilyFriendly": true, "displayUrl": "https://dl.acm.org/doi/10.1145/3494672", "snippet": "As mentioned, one possible incompatibility <b>can</b> occur between the measures of equalized odds and <b>demographic</b> <b>parity</b> in cases when base rates are different (i.e., different proportions of actual positive outcomes). The intuition <b>can</b> be demonstrated, for example, by considering the case of pursuing <b>demographic</b> <b>parity</b> by requiring that the proportion of inmates selected for parole would be the same for men and women. Note that following the preceding requirement may harm the fairness notion of ...", "dateLastCrawled": "2022-02-07T09:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) On Fair Representation in <b>Machine</b> <b>Learning</b>", "url": "https://www.researchgate.net/publication/341736051_On_Fair_Representation_in_Machine_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/.../341736051_On_Fair_Representation_in_<b>Machine</b>_<b>Learning</b>", "snippet": "<b>demographic</b> <b>parity</b> and metrics such as equalized odds are either ignored ( e.g. [21], [22]) or assumed to be automatically satis\ufb01ed by &quot;removing&quot; information about sensitive attributes", "dateLastCrawled": "2022-01-26T19:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A <b>Gentle Introduction to Algorithmic Fairness</b> in Lending", "url": "https://blog.fiddler.ai/2019/04/a-gentle-introduction-to-algorithmic-fairness/", "isFamilyFriendly": true, "displayUrl": "https://blog.fiddler.ai/2019/04/a-<b>gentle-introduction-to-algorithmic-fairness</b>", "snippet": "As <b>machine</b> <b>learning</b> (ML) becomes widespread, there is growing interest in fairness, accountability, and transparency in ML (e.g., ... harming the model\u2019s utility to society. [Hard2016] discusses this issue (using the term \u201c<b>demographic</b> <b>parity</b>\u201d) in its introduction. Corbett et al. [Corb2018] argue at length that classification <b>parity</b> is naturally violated: \u201cwhen base rates of violent recidivism differ across groups, the true risk distributions will necessarily differ as well \u2014 and ...", "dateLastCrawled": "2022-01-27T00:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Responsibility Cannot Be Delegated to an Algorithm \u2014 <b>MACHINE</b> <b>LEARNING</b> ...", "url": "https://www.machinelearningforscience.de/en/responsibility-cannot-be-delegated-to-an-algorithm/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machinelearning</b>forscience.de/en/responsibility-<b>can</b>not-be-delegated-to-an...", "snippet": "<b>Machine</b> <b>Learning</b> for Science: ... There are a few standard definitions. One, for example, is \u201c<b>demographic</b> <b>parity</b>\u201d. You could say, for example, a university, when admitting students should reflect the ratio of men to women in the general population, so half the share of those admitted should be women and the other half, men. Yet in doing that you\u2019re looking at an absolute quantity, not at the qualifications of individual applicants. Another fairness concept is \u201cequalized odds\u201d or ...", "dateLastCrawled": "2021-11-29T09:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Equality of opportunity in supervised learning</b> | the morning paper", "url": "https://blog.acolyer.org/2018/05/07/equality-of-opportunity-in-supervised-learning/", "isFamilyFriendly": true, "displayUrl": "https://blog.acolyer.org/2018/05/07/<b>equality-of-opportunity-in-supervised-learning</b>", "snippet": "Moving on, a popular approach to fixing the issues with the race blind is <b>demographic</b> <b>parity</b>. ... One nice characteristic of these models is that you <b>can</b> start by <b>learning</b> a possibly discriminator learned binary predictor (or score R), and then derive an equalized odds or equal opportunity predictor from it. So we <b>can</b> keep an existing training pipeline untouched, and add an anti-discriminatory step on the back-end of it. Section 4 in the paper shows how to do this for a binary predictor, and ...", "dateLastCrawled": "2022-02-03T12:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Fairness in Machine Learning: A Survey</b> | DeepAI", "url": "https://deepai.org/publication/fairness-in-machine-learning-a-survey", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>fairness-in-machine-learning-a-survey</b>", "snippet": "This article seeks to provide an overview of the different schools of <b>thought</b> and approaches to mitigating (social) biases and increase fairness in the <b>Machine</b> <b>Learning</b> literature. It organises approaches into the widely accepted framework of pre-processing, in-processing, and post-processing methods, subcategorizing into a further 11 method areas. Although much of the literature emphasizes binary classification, a discussion of fairness in regression, recommender systems,", "dateLastCrawled": "2022-01-18T13:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Towards fair machine learning models</b> - integrate.ai", "url": "https://integrate.ai/blog/towards-fair-machine-learning-models/", "isFamilyFriendly": true, "displayUrl": "https://integrate.ai/blog/<b>towards-fair-machine-learning-models</b>", "snippet": "Imagine your team has built a relatively accurate <b>machine</b> <b>learning</b> model: it predicts the right medical device for a user 92% of the time. But after digging a little deeper, you discover that for people from a specific ethnic background, the model is almost always wrong. What now? In this post, we\u2019ll explore why accuracy alone is not always a good metric to judge a model and how algorithmic fairness <b>can</b> help. Fairness is a topic that\u2019s expansive, essential, and all too often neglected ...", "dateLastCrawled": "2022-01-10T17:21:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "How to define <b>fairness</b> to detect and prevent discriminatory outcomes in ...", "url": "https://towardsdatascience.com/how-to-define-fairness-to-detect-and-prevent-discriminatory-outcomes-in-machine-learning-ef23fd408ef2", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/how-to-define-<b>fairness</b>-to-detect-and-prevent...", "snippet": "In order to detect discriminatory outcomes in <b>Machine</b> <b>Learning</b> predictions, we need to compare how well our model treats different user segments. Valeria Cortez. Sep 23, 2019 \u00b7 9 min read. This <b>can</b> be achieved is by defining a metric that describes the notion of <b>fairness</b> in our model. For example, when looking at university admissions, we <b>can</b> compare admission rates of men and women. This corresponds to the use of <b>Demographic</b> <b>Parity</b> as the mathematical definition of <b>fairness</b>. It states that ...", "dateLastCrawled": "2022-01-28T08:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Fairness Criteria</b> | Module 3: Pedagogical Framework for Addressing ...", "url": "https://ocw.mit.edu/resources/res-ec-001-exploring-fairness-in-machine-learning-for-international-development-spring-2020/module-three-framework/fairness-criteria/", "isFamilyFriendly": true, "displayUrl": "https://<b>ocw.mit.edu</b>/resources/res-ec-001-exploring-fairness-in-<b>machine</b>-<b>learning</b>-for...", "snippet": "<b>Demographic</b> <b>parity</b> almost always cannot be implemented if individuals are members of multiple protected groups because you may not be able to impose the equal probabilities across all groups. <b>Demographic</b> <b>parity</b> <b>can</b> also be fair at a group level, but unfair at an individual level. For example, if qualifications are different across a protected attribute, imposing <b>demographic</b> <b>parity</b> may mean someone who is less qualified may get hired. Therefore, if a large number of unqualified male ...", "dateLastCrawled": "2022-01-04T22:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Editorial: Ethical <b>Machine</b> <b>Learning</b> and Artificial Intelligence", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8387579/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8387579", "snippet": "Ultimately, this leads to their view that \u201cmoral biases <b>can</b> be extracted, quantified, tracked, and <b>compared</b> across cultures and over time\u201d. As future work, the authors name the possibility to alter the embeddings in targeted ways, such as to eliminate gender stereotypes. They further suggest having the moral choice <b>machine</b> in interactive robots enabled with active <b>learning</b> to have users correct potential biases. Finally, they suggest targeted alteration of the text sources for ...", "dateLastCrawled": "2021-12-20T03:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "&#39;<b>Un&#39;Fair Machine Learning Algorithms</b>", "url": "https://www.ftc.gov/system/files/documents/public_events/1567421/fuaserisinghsrinivasan_updated2.pdf", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ftc.gov</b>/system/files/documents/public_events/1567421/fuaserisinghsriniva...", "snippet": "<b>Compared</b> to the current law, which requires treatment <b>parity</b>, these \u201cfair\u201d algorithms, which require impact <b>parity</b>, limit the bene\ufb01ts of a more accurate algorithm for a \ufb01rm. As a result, pro\ufb01t maximizing \ufb01rms could under-invest in <b>learning</b>, i.e., improving the accuracy of their <b>machine</b> <b>learning</b> algorithms. We show that the investment in <b>learning</b> decreases when misclassi\ufb01cation is costly, which is exactly the case when greater accuracy is otherwise desired. Our paper highlights ...", "dateLastCrawled": "2022-02-03T08:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Interventions | <b>Machine Learning Bias Mitigation</b>", "url": "https://cdeiuk.github.io/bias-mitigation/interventions/", "isFamilyFriendly": true, "displayUrl": "https://cdeiuk.github.io/bias-mitigation/interventions", "snippet": "The adversarial debiasing technique is extremely effective for <b>demographic</b> <b>parity</b> and conditional <b>demographic</b> <b>parity</b>. It is less effective for equalised odds, likely due to a combination of the fact that the model of course does not see the labels which get passed to the discriminator, which means it&#39;s hard for it to know what information it <b>can</b> use, and because adversarial methods are inherently unstable.", "dateLastCrawled": "2022-02-02T02:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Controllable Fairness in <b>Machine</b> <b>Learning</b> | SAIL Blog", "url": "http://ai.stanford.edu/blog/controllable-fairness/", "isFamilyFriendly": true, "displayUrl": "ai.stanford.edu/blog/controllable-fairness", "snippet": "Another potential limitation of this work is that we, like much of the fair <b>machine</b> <b>learning</b> community, center <b>demographic</b> <b>parity</b>, equality of odds, and equality of opportunity notions of fairness. We believe that future work will need to develop deeper connections to social-justice-informed notions of equity if it is to avoid shallow technosolutionism and build more equitable <b>machine</b> <b>learning</b> 9 .", "dateLastCrawled": "2022-01-30T20:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Model understanding with Azure Machine Learning</b>", "url": "https://techcommunity.microsoft.com/t5/azure-ai-blog/model-understanding-with-azure-machine-learning/ba-p/2201141", "isFamilyFriendly": true, "displayUrl": "https://techcommunity.microsoft.com/t5/azure-ai-blog/model-understanding-with-azure...", "snippet": "Azure <b>Machine</b> <b>Learning</b>\u2019s (AzureML) interpretability and fairness toolkits <b>can</b> be run both locally and remotely. If run locally, the libraries will not contact any Azure services. Alternatively, you <b>can</b> run the algorithms remotely on AzureML compute and log all the explainability and fairness information into AzurML\u2019s run history via the AzureML SDK to save and share them with other team members or stakeholders in AzureML studio.", "dateLastCrawled": "2022-01-17T06:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Identifying and Correcting Label <b>Bias</b> in <b>Machine</b> <b>Learning</b> | by Rani ...", "url": "https://towardsdatascience.com/identifying-and-correcting-label-bias-in-machine-learning-ed177d30349e", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/identifying-and-correcting-label-<b>bias</b>-in-<b>machine</b>...", "snippet": "As <b>machine</b> <b>learning</b> (ML) becomes more effective and widespread it is becoming more prevalent in systems with real-life impact, from loan recommendations to job application decisions. With the growing usage comes the risk of <b>bias</b> \u2014 biased training data could lead to biased ML algorithms, which in turn could perpetuate discrimination and <b>bias</b> in society. In a new paper from Google, researchers propo s e a novel technique to train <b>machine</b> <b>learning</b> algorithms fairly even with a biased dataset ...", "dateLastCrawled": "2022-01-30T18:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Bias and Fairness in Machine Learning</b> - Abhishek Tiwari", "url": "https://www.abhishek-tiwari.com/bias-and-fairness-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.abhishek-tiwari.com/<b>bias-and-fairness-in-machine-learning</b>", "snippet": "In <b>machine</b> <b>learning</b>, algorithmic biases are new kinds of bugs. These bugs generically referred as unwarranted associations. Such bugs <b>can</b> be harmful to both people and businesses. Tramer et al. argue we should proactively check for unwarranted associations, debug, and fix them with the same rigor as we do to other security and privacy bugs ...", "dateLastCrawled": "2022-01-29T16:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Classification - Fairness and <b>machine</b> <b>learning</b>", "url": "https://fairmlbook.org/classification.html", "isFamilyFriendly": true, "displayUrl": "https://fairmlbook.org/classification.html", "snippet": "Simply put, the goal of classification is to determine a plausible value for an unknown variable Y given an observed variable X.For example, we might try to predict whether a loan applicant will pay back her loan by looking at various characteristics such as credit history, income, and net worth. Classification also applies in situations where the variable Y does not refer to an event that lies in the future. For example, we <b>can</b> try to determine if an image contains a cat by looking at the ...", "dateLastCrawled": "2022-02-03T07:41:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "An example of prediction which complies with <b>Demographic</b> <b>Parity</b> and ...", "url": "https://vertexdoc.com/doc/an-example-of-prediction-which-complies-with-demographic-parity-and-equalizes-group-wise-risks-in-the-context", "isFamilyFriendly": true, "displayUrl": "https://vertexdoc.com/doc/an-example-of-prediction-which-complies-with-<b>demographic</b>...", "snippet": "However, <b>Demographic</b> <b>Parity</b> and EGWR only define fairness on the group level and inspecting the individual level reveals a critical flow of this prediction rule. We have constrained our predictors to those that do not produce Disparate Treatment by prohibiting them from having the sensitive variable as direct input. Nevertheless, enforcing group level fairness constraints (such as DP and EGWR) forces the prediction rule to guess the sensitive attribute corresponding to a given feature vector", "dateLastCrawled": "2022-02-05T21:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Pandas <b>Machine</b> <b>Learning</b> Example", "url": "https://groups.google.com/g/hslogb/c/-BvVGlSI3Ek", "isFamilyFriendly": true, "displayUrl": "https://groups.google.com/g/hslogb/c/-BvVGlSI3Ek", "snippet": "Regardless of your dataset, <b>demographic</b> <b>parity</b> is a <b>machine</b> <b>learning</b> algorithms. Data Munging It helps us to missing data of wedge form with another. Python with datetime module, i should equal to bring new example <b>machine</b>. Quite possibly the state important part clean the <b>machine</b> <b>learning</b> process is understanding the data you are working with and advantage it relates to reflect task you front to solve. Viewing the corresponding number of dropping down arrow illustrates that are not only all ...", "dateLastCrawled": "2022-01-24T06:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "RStudio AI Blog: Starting to think about AI Fairness", "url": "https://blogs.rstudio.com/ai/posts/2021-07-15-ai-fairness/", "isFamilyFriendly": true, "displayUrl": "https://blogs.rstudio.com/ai/posts/2021-07-15-ai-fairness", "snippet": "Heidari et al. have written a paper comparing the three criteria \u2013 <b>demographic</b> <b>parity</b>, equality of opportunity, and predictive <b>parity</b> \u2013 to egalitarianism, equality of opportunity (EOP) in the Rawlsian sense, and EOP seen through the glass of luck egalitarianism, respectively. While the <b>analogy</b> is fascinating, it too assumes that we may take what is in the data at face value. In their likening predictive <b>parity</b> to luck egalitarianism, they have to go to especially great lengths, in ...", "dateLastCrawled": "2022-01-31T09:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Mitigating Unwanted Biases with Adversarial <b>Learning</b> - arxiv-vanity.com", "url": "https://www.arxiv-vanity.com/papers/1801.07593/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1801.07593", "snippet": "<b>Machine</b> <b>learning</b> is a tool for building models that accurately represent input training data. When undesired biases concerning <b>demographic</b> groups are in the training data, well-trained models will reflect those biases. We present a framework for mitigating such biases by including a variable for the group of interest and simultaneously <b>learning</b> a predictor and an adversary. The input to the network X, here text or census data, produces a prediction Y, such as an <b>analogy</b> completion or income ...", "dateLastCrawled": "2021-10-04T02:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Mitigating Unwanted Biases with Adversarial <b>Learning</b>", "url": "http://www.m-mitchell.com/papers/Adversarial_Bias_Mitigation.pdf", "isFamilyFriendly": true, "displayUrl": "www.m-mitchell.com/papers/Adversarial_Bias_Mitigation.pdf", "snippet": "<b>Machine</b> <b>learning</b> is a tool for building models that accurately represent input training data. When undesired biases concern-ing <b>demographic</b> groups are in the training data, well-trained models will re\ufb02ect those biases. We present a framework for mitigating such biases by including a variable for the group of interest and simultaneously <b>learning</b> a predictor and an ad-versary. The input to the network X, here text or census data, produces a prediction Y, such as an <b>analogy</b> completion or in ...", "dateLastCrawled": "2022-01-23T20:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A Human-centric Approach to Fairness in AI", "url": "https://www.timlrx.com/blog/a-human-centric-approach-to-fairness-in-ai", "isFamilyFriendly": true, "displayUrl": "https://www.timlrx.com/blog/a-human-centric-approach-to-fairness-in-ai", "snippet": "This includes measures such as <b>Demographic</b> <b>Parity</b> / Statistical <b>Parity</b> (Dwork et al., 2012), Equalized Odds Metric (Hardt et al., 2016) and Calibration within Groups (Chouldechova, 2017). They are all statistical measures derived from the predictions of a classification model and differ in terms of which element(s) of the confusion matrix they are trying to test for equivalence. In another survey of fairness definitions, Verma &amp; Rubin (2018) listed 20 definitions of fairness, 13 belonging to ...", "dateLastCrawled": "2022-02-03T02:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Adversarial Approaches to Debiasing Word Embeddings", "url": "https://web.stanford.edu/class/cs224n/reports/final_reports/report101.pdf", "isFamilyFriendly": true, "displayUrl": "https://web.stanford.edu/class/cs224n/reports/final_reports/report101.pdf", "snippet": "<b>Machine</b> <b>learning</b> for natural language processing (NLP) leverages valuable data from human language for useful downstream applications such as <b>machine</b> translation and sentiment analysis. Recent studies, however, have shown that training data in these applications are prone to harboring stereotypes and unwanted biases commonly exhibited in human language. Since NLP systems are designed to understand novel associations within training data, they are similarly vulnerable to propagating these ...", "dateLastCrawled": "2022-01-25T20:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Classification - Fairness and <b>machine</b> <b>learning</b>", "url": "https://fairmlbook.org/classification.html", "isFamilyFriendly": true, "displayUrl": "https://fairmlbook.org/classification.html", "snippet": "Simply put, the goal of classification is to determine a plausible value for an unknown variable Y given an observed variable X.For example, we might try to predict whether a loan applicant will pay back her loan by looking at various characteristics such as credit history, income, and net worth. Classification also applies in situations where the variable Y does not refer to an event that lies in the future. For example, we can try to determine if an image contains a cat by looking at the ...", "dateLastCrawled": "2022-02-03T07:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "The <b>measure and mismeasure of fairness: a critical review</b> of fair ...", "url": "https://blog.acolyer.org/2020/02/03/measure-mismeasure-fairness/", "isFamilyFriendly": true, "displayUrl": "https://blog.acolyer.org/2020/02/03/measure-mismeasure-fairness", "snippet": "In case you\u2019re wondering where on earth I\u2019m going with this\u2026 it\u2019s a very stretched <b>analogy</b> I\u2019ve been playing with in my mind. One premise of many models of fairness in <b>machine</b> <b>learning</b> is that you can measure (\u2018prove\u2019) fairness of a <b>machine</b> <b>learning</b> model from within the system \u2013 i.e. from properties of the model itself and perhaps the data it is trained on. Beyond the questions of whether any one model of fairness is better or worse than another, I\u2019m coming to the ...", "dateLastCrawled": "2022-01-30T11:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Fighting Money Laundering with Statistics and <b>Machine</b> <b>Learning</b>: An ...", "url": "https://deepai.org/publication/fighting-money-laundering-with-statistics-and-machine-learning-an-introduction-and-review", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/fighting-money-laundering-with-statistics-and-<b>machine</b>...", "snippet": "Statistics and <b>machine</b> <b>learning</b> have long promised efficient and robust techniques for AML. So far, though, these remain to manifest [Grint2001]. One reason is that the academic literature is small and fragmented [Leite2019, Ngai2011]. With this paper, we aim to do three things. First, we propose a unified terminology to homogenize and associate research methodologies. Second, we review selected, exemplary methods. Third, we present recent <b>machine</b> <b>learning</b> concepts that have the potential to ...", "dateLastCrawled": "2022-01-28T21:12:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(demographic parity)  is like +(machine learning)", "+(demographic parity) is similar to +(machine learning)", "+(demographic parity) can be thought of as +(machine learning)", "+(demographic parity) can be compared to +(machine learning)", "machine learning +(demographic parity AND analogy)", "machine learning +(\"demographic parity is like\")", "machine learning +(\"demographic parity is similar\")", "machine learning +(\"just as demographic parity\")", "machine learning +(\"demographic parity can be thought of as\")", "machine learning +(\"demographic parity can be compared to\")"]}