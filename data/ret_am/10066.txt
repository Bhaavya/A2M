{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Biases in <b>machine</b> <b>learning</b> models and big <b>data</b> analytics: The ...", "url": "http://international-review.icrc.org/articles/biases-machine-learning-big-data-analytics-ihl-implications-913", "isFamilyFriendly": true, "displayUrl": "international-review.icrc.org/articles/<b>bias</b>es-<b>machine</b>-<b>learning</b>-big-<b>data</b>-analytics-ihl...", "snippet": "<b>Data</b> sets often contain biases which have the potential to unfairly disadvantage <b>certain</b> groups or to over-focus on <b>certain</b> activities to the detriment of others, and ML models or big <b>data</b> analytics <b>trained</b> on such <b>data</b> sets can inherit these biases.22 The following section discusses human biases that most commonly appear in <b>data</b> sets used for ML models and thus are most likely to impact ICL investigations and IHL considerations: implicit <b>bias</b>, selection <b>bias</b>, reporting <b>bias</b>, group ...", "dateLastCrawled": "2022-02-01T10:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Managing <b>bias and unfairness</b> in <b>data</b> for decision support: a survey of ...", "url": "https://link.springer.com/article/10.1007/s00778-021-00671-8", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00778-021-00671-8", "snippet": "This function is typically a <b>machine</b> <b>learning</b> model (<b>like</b> a classifier, regression, or a ranking <b>algorithm</b>, etc.) <b>trained</b> on labelled <b>data</b> and exposed to unseen instances after deployment. Decisions, whether made by people or systems, may show <b>bias</b>. A <b>bias</b> is observed if <b>data</b> instances belonging to <b>certain</b> classes show a systematically ...", "dateLastCrawled": "2022-02-01T12:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Biased Algorithms Learn From Biased <b>Data</b>: 3 Kinds Biases Found In AI ...", "url": "https://www.forbes.com/sites/cognitiveworld/2020/02/07/biased-algorithms/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.forbes.com</b>/sites/cognitiveworld/2020/02/07/<b>bias</b>ed-<b>algorithms</b>", "snippet": "\u201cSelection <b>bias</b> occurs when a <b>data</b> <b>set</b> contains vastly more information on one subgroup and not another,\u201d says White. For instance, many <b>machine</b> <b>learning</b> algorithms are taught by scraping the ...", "dateLastCrawled": "2022-01-30T20:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Machine</b> <b>Learning</b> Applications in Endocrinology and Metabolism Research ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7090299/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7090299", "snippet": "<b>Machine</b> <b>learning</b> (ML) applications have received extensive attention in endocrinology research during the last decade. This review summarizes the basic concepts of ML and <b>certain</b> research topics in endocrinology and metabolism where ML principles have been actively deployed. Relevant studies are discussed to provide an overview of the methodology, main findings, and limitations of ML, with the goal of stimulating insights into future research directions. Clear, testable study hypotheses stem ...", "dateLastCrawled": "2022-01-19T02:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Engineering Bias Out of AI</b> - IEEE Spectrum", "url": "https://spectrum.ieee.org/engineering-bias-out-of-ai", "isFamilyFriendly": true, "displayUrl": "https://spectrum.ieee.org/<b>engineering-bias-out-of-ai</b>", "snippet": "Other algorithms tweak the <b>machine</b>-<b>learning</b> process instead of the training <b>data</b>. The goal of <b>machine</b>-<b>learning</b> models is to predict an outcome as accurately as possible. So if the training <b>set</b> has ...", "dateLastCrawled": "2022-02-03T08:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Comparison of <b>machine</b> <b>learning</b> and logistic regression models in ...", "url": "https://www.sciencedirect.com/science/article/pii/S1386505621001106", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1386505621001106", "snippet": "<b>Machine</b> <b>learning</b> performance to predict acute kidney injury is variable and depends on the predictor variables included in the model as well as the type of <b>algorithm</b> deployed. While common biomarkers (creatinine, BUN) for acute kidney injury were important for model performance, there are a large number of predictor variables that have been identified and are currently being used in <b>machine</b> <b>learning</b> models.", "dateLastCrawled": "2022-01-27T13:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "What is <b>the required machine learning algorithm for virtual assistant</b> ...", "url": "https://www.quora.com/What-is-the-required-machine-learning-algorithm-for-virtual-assistant", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>the-required-machine-learning-algorithm-for</b>-virtual...", "snippet": "Answer (1 of 2): It is not one <b>algorithm</b>, it\u2019s the combination of many, because for a virtual assistant you\u2019ll have to solve a couple of problems. I have a list for you, that is not complete. Every part of the problem opens several underlying problems and more algorithms you need to implement. Bu...", "dateLastCrawled": "2022-01-24T10:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Why Do I Get Different Results Each Time in <b>Machine Learning</b>?", "url": "https://machinelearningmastery.com/different-results-each-time-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/different-results-each-time-in-<b>machine</b>-learn", "snippet": "In applied <b>machine learning</b>, we run a <b>machine learning</b> \u201c<b>algorithm</b>\u201d on a dataset to get a <b>machine learning</b> \u201cmodel.\u201d The model can then be evaluated on <b>data</b> not used during training or used to make predictions on new <b>data</b>, also not seen during training. <b>Algorithm</b>: Procedure run on <b>data</b> that results in a model (e.g. training or <b>learning</b>).", "dateLastCrawled": "2022-02-03T17:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Machine Learning with Python - Algorithms</b>", "url": "https://www.tutorialspoint.com/machine_learning_with_python/machine_learning_with_python_algorithms.htm", "isFamilyFriendly": true, "displayUrl": "https://<b>www.tutorialspoint.com</b>/.../<b>machine_learning_with_python_algorithms</b>.htm", "snippet": "It is used for clustering a given <b>data</b> <b>set</b> into different groups, which is widely used for segmenting customers into different groups for specific intervention. Apriori <b>algorithm</b> and K-means are some of the examples of Unsupervised <b>Learning</b>. Reinforcement <b>Learning</b>. Using this <b>algorithm</b>, the <b>machine</b> is <b>trained</b> to make specific decisions. Here ...", "dateLastCrawled": "2022-02-02T19:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>machine</b> <b>learning</b> - Should <b>Feature Selection</b> be done before Train-Test ...", "url": "https://stackoverflow.com/questions/56308116/should-feature-selection-be-done-before-train-test-split-or-after", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/56308116", "snippet": "Secondly, if only Training <b>Set</b> is used for <b>feature selection</b>, then the test <b>set</b> may contain <b>certain</b> <b>set</b> of instances that defies/contradicts the <b>feature selection</b> done only on the Training <b>Set</b> as the overall historical <b>data</b> is not analyzed. Moreover, feature importance scores can only be evaluated when, given a <b>set</b> of instances rather than a single test/unknown instance. <b>machine</b>-<b>learning</b> <b>feature-selection</b> train-test-split. Share. Follow edited Jun 11 &#39;19 at 16:49. desertnaut. 50.4k 19 19 ...", "dateLastCrawled": "2022-02-03T01:27:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Biases in <b>machine</b> <b>learning</b> models and big <b>data</b> analytics: The ...", "url": "http://international-review.icrc.org/articles/biases-machine-learning-big-data-analytics-ihl-implications-913", "isFamilyFriendly": true, "displayUrl": "international-review.icrc.org/articles/<b>bias</b>es-<b>machine</b>-<b>learning</b>-big-<b>data</b>-analytics-ihl...", "snippet": "<b>Data</b> sets often contain biases which have the potential to unfairly disadvantage <b>certain</b> groups or to over-focus on <b>certain</b> activities to the detriment of others, and ML models or big <b>data</b> analytics <b>trained</b> on such <b>data</b> sets can inherit these biases.22 The following section discusses human biases that most commonly appear in <b>data</b> sets used for ML models and thus are most likely to impact ICL investigations and IHL considerations: implicit <b>bias</b>, selection <b>bias</b>, reporting <b>bias</b>, group ...", "dateLastCrawled": "2022-02-01T10:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Bias</b> does not equal <b>bias</b>. A socio-technical typology of <b>bias</b> in <b>data</b> ...", "url": "https://policyreview.info/articles/analysis/bias-does-not-equal-bias-socio-technical-typology-bias-data-based-algorithmic", "isFamilyFriendly": true, "displayUrl": "https://policyreview.info/articles/analysis/<b>bias</b>-does-not-equal-<b>bias</b>-socio-technical...", "snippet": "This paper introduces a socio-technical typology of <b>bias</b> in <b>data</b>-driven <b>machine</b> <b>learning</b> and artificial intelligence systems. The typology is linked to the conceptualisations of legal anti-discrimination regulations, so that the concept of structural inequality\u2014and, therefore, of undesirable <b>bias</b>\u2014is defined accordingly. By analysing the controversial Austrian \u201cAMS <b>algorithm</b>\u201d as a case study as well as examples in the contexts of face detection, risk assessment and health care ...", "dateLastCrawled": "2022-01-22T12:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Engineering Bias Out of AI</b> - IEEE Spectrum", "url": "https://spectrum.ieee.org/engineering-bias-out-of-ai", "isFamilyFriendly": true, "displayUrl": "https://spectrum.ieee.org/<b>engineering-bias-out-of-ai</b>", "snippet": "Other algorithms tweak the <b>machine</b>-<b>learning</b> process instead of the training <b>data</b>. The goal of <b>machine</b>-<b>learning</b> models is to predict an outcome as accurately as possible. So if the training <b>set</b> has ...", "dateLastCrawled": "2022-02-03T08:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Comparison of <b>machine</b> <b>learning</b> and logistic regression models in ...", "url": "https://www.sciencedirect.com/science/article/pii/S1386505621001106", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1386505621001106", "snippet": "<b>Machine</b> <b>learning</b> performance to predict acute kidney injury is variable and depends on the predictor variables included in the model as well as the type of <b>algorithm</b> deployed. While common biomarkers (creatinine, BUN) for acute kidney injury were important for model performance, there are a large number of predictor variables that have been identified and are currently being used in <b>machine</b> <b>learning</b> models.", "dateLastCrawled": "2022-01-27T13:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) Managing <b>bias</b> and unfairness in <b>data</b> for decision support: a ...", "url": "https://www.researchgate.net/publication/351356019_Managing_bias_and_unfairness_in_data_for_decision_support_a_survey_of_machine_learning_and_data_engineering_approaches_to_identify_and_mitigate_bias_and_unfairness_within_data_management_and_analytics", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/351356019_Managing_<b>bias</b>_and_unfairness_in...", "snippet": "Managing <b>bias</b> and unfairness in <b>data</b> for decision support: a survey of <b>machine</b> <b>learning</b>\u2026 753 into a causal paradigm and specify the potential causal depen- dencies between attributes.", "dateLastCrawled": "2022-01-16T17:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Bias</b>, awareness, and ignorance in deep-<b>learning</b>-based face recognition ...", "url": "https://link.springer.com/article/10.1007%2Fs43681-021-00108-6", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s43681-021-00108-6", "snippet": "Given a <b>data</b> <b>set</b> with labels for sensitive attributes such as ethnicity and gender, the images can be grouped into clusters based on these labels. To investigate the influence of this clustering on the face recognition rates, we propose a blinding procedure to remove the information related to the separation of these clusters in the embedding space. The procedure is a linear operation and uses the following steps: 1. Compute the centroids of the clusters defined by the sensitive attributes ...", "dateLastCrawled": "2022-02-03T10:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Managing <b>bias and unfairness</b> in <b>data</b> for decision support: a survey of ...", "url": "https://link.springer.com/article/10.1007/s00778-021-00671-8", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00778-021-00671-8", "snippet": "At training time, this would increase the likelihood that the outputs of a model <b>trained</b> on such dataset are fair (note: an \u201cunbiased\u201d training dataset does not guarantee an unbiased resulting system since new unwanted biases might arise from the <b>machine</b> <b>learning</b> <b>algorithm</b> used or small unwanted biases in the <b>data</b> might be reinforced by the <b>machine</b> <b>learning</b> model, but helps); while at deployment time, it would monitor whether the predictions made for new <b>data</b> points are fair. Constraints ...", "dateLastCrawled": "2022-02-01T12:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Top 5 Predictive Analytics Models and Algorithms | Logi Analytics Blog", "url": "https://www.logianalytics.com/predictive-analytics/predictive-algorithms-and-models/", "isFamilyFriendly": true, "displayUrl": "https://www.logianalytics.com/predictive-analytics/predictive-<b>algorithms</b>-and-models", "snippet": "<b>Machine</b> <b>learning</b> involves structural <b>data</b> that we see in a table. Algorithms for this comprise both linear and nonlinear varieties. Linear algorithms train more quickly, while nonlinear are better optimized for the problems they are likely to face (which are often nonlinear). Deep <b>learning</b> is a subset of <b>machine</b> <b>learning</b> that is more popular to deal with audio, video, text, and images. With <b>machine</b> <b>learning</b> predictive modeling, there are several different algorithms that can be applied ...", "dateLastCrawled": "2022-02-02T22:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Interventions | <b>Machine Learning Bias Mitigation</b>", "url": "https://cdeiuk.github.io/bias-mitigation/interventions/", "isFamilyFriendly": true, "displayUrl": "https://cdeiuk.github.io/<b>bias</b>-mitigation/interventions", "snippet": "There was a small drop in accuracy, whereas our baseline achieved 85.3% test <b>set</b> accuracy, the model <b>trained</b> on the fair <b>data</b> achieved 85.0%. However there was also hardly any change in demographic parity difference, going from 0.193 to 0.186. Below we show a box plot of the score distributions for the two models. They appear very <b>similar</b>.", "dateLastCrawled": "2022-02-02T02:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Programming Fairness in Algorithms</b> - Towards <b>Data</b> Science", "url": "https://towardsdatascience.com/programming-fairness-in-algorithms-4943a13dd9f8", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/<b>programming-fairness-in-algorithms</b>-4943a13dd9f8", "snippet": "<b>Machine</b> <b>learning</b> fairness is a young subfield of <b>machine</b> <b>learning</b> that has been growing in popularity over the last few years in response to the rapid integration of <b>machine</b> <b>learning</b> into social realms. Computer scientists, unlike doctors, are not necessarily <b>trained</b> to consider the ethical implications of their actions. It is only relatively recently (one could argue since the advent of social media) that the designs or inventions of computer scientists were able to take on an ethical ...", "dateLastCrawled": "2022-01-23T21:47:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Bias</b> does not equal <b>bias</b>. A socio-technical typology of <b>bias</b> in <b>data</b> ...", "url": "https://policyreview.info/articles/analysis/bias-does-not-equal-bias-socio-technical-typology-bias-data-based-algorithmic", "isFamilyFriendly": true, "displayUrl": "https://policyreview.info/articles/analysis/<b>bias</b>-does-not-equal-<b>bias</b>-socio-technical...", "snippet": "Zooming into the technical details of a <b>machine</b> <b>learning</b> system\u2019s life cycle, Suresh and Guttag (2020) described various issues that <b>can</b> introduce <b>bias</b> into a system: historical <b>bias</b>, representation <b>bias</b>, measurement <b>bias</b>, aggregation <b>bias</b>, <b>learning</b> <b>bias</b>, evaluation <b>bias</b> and deployment <b>bias</b>. Some of these types <b>of data</b> <b>bias</b> <b>can</b> only be identified through extensive knowledge and close examination of the development process of a particular system including the underlying <b>data</b> used to build ...", "dateLastCrawled": "2022-01-22T12:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Bias</b>, awareness, and ignorance in deep-<b>learning</b>-based face recognition ...", "url": "https://link.springer.com/article/10.1007%2Fs43681-021-00108-6", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s43681-021-00108-6", "snippet": "Given a <b>data</b> <b>set</b> with labels for sensitive attributes such as ethnicity and gender, the images <b>can</b> be grouped into clusters based on these labels. To investigate the influence of this clustering on the face recognition rates, we propose a blinding procedure to remove the information related to the separation of these clusters in the embedding space. The procedure is a linear operation and uses the following steps: 1. Compute the centroids of the clusters defined by the sensitive attributes ...", "dateLastCrawled": "2022-02-03T10:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Ethical machines: The human-centric use of artificial intelligence", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7973859/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7973859", "snippet": "Biases and discriminatory effects. In legal terms, discrimination occurs when two different rules are applied to similar situations, or the same rule is applied to different situations (Tobler, 2008).Turning our attention to the use of <b>machine</b> <b>learning</b> in decision-making processes, discriminatory effects and biases could be the result of the way input <b>data</b> are collected and/or of the <b>learning</b> process itself (Barocas and Selbst, 2016; Barocas et al., 2018).First of all, specific features and ...", "dateLastCrawled": "2022-01-29T00:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Amazon SageMaker Clarify: <b>Machine</b> <b>Learning</b> <b>Bias</b> Detection and ...", "url": "https://deepai.org/publication/amazon-sagemaker-clarify-machine-learning-bias-detection-and-explainability-in-the-cloud", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/amazon-sagemaker-clarify-<b>machine</b>-<b>learning</b>-<b>bias</b>...", "snippet": "Once a model is <b>trained</b>, we <b>can</b> compute post-training <b>bias</b> metrics that take into account how its predictions differ across groups. We introduce the notation before defining the metrics. 4.1.1. Setup and Notation. Without loss of generality, we use d to denote the group that is potentially disadvantaged by the <b>bias</b> and compare it to the reminder of the examples in the dataset a (for advantaged group). The selection of the groups often requires domain information 9 9 9 https://plato.stanford ...", "dateLastCrawled": "2022-01-25T08:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Algorithmic <b>bias</b>: Senses, sources, solutions - Fazelpour - 2021 ...", "url": "https://onlinelibrary.wiley.com/doi/10.1111/phc3.12760?af=R", "isFamilyFriendly": true, "displayUrl": "https://onlinelibrary.wiley.com/doi/10.1111/phc3.12760?af=R", "snippet": "Given a dataset, developers use a <b>learning</b> <b>algorithm</b> to \u2018fit\u2019 a predictive model on a portion of the <b>data</b>\u2013the \u2018training <b>set</b>\u2019\u2013whose performance is validated on a previously unseen portion of the dataset\u2013the \u2018test <b>set</b>\u2019. 3 These modeling and validation processes, often conducted iteratively, almost always optimize and evaluate model performance relative to some criteria of success. While this stage <b>can</b> be highly technical, it also involves multiple value judgments. As noted ...", "dateLastCrawled": "2022-01-01T23:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Lecture 23: Fairness | Lecture Videos | <b>Machine</b> <b>Learning</b> for Healthcare ...", "url": "https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-s897-machine-learning-for-healthcare-spring-2019/lecture-videos/lecture-23-fairness/", "isFamilyFriendly": true, "displayUrl": "https://<b>ocw.mit.edu</b>/courses/electrical-engineering-and-computer-science/6-s897-<b>machine</b>...", "snippet": "So this is an idea that I&#39;ve seen used in <b>machine</b> <b>learning</b> for robustness rather than for fairness, where people say, the problem is that given a particular <b>data</b> <b>set</b>, you <b>can</b> overfit to that <b>data</b> <b>set</b>, and so one of the ideas is to do a Gann-like method where you say, I want to train my classifier, let&#39;s say, not only to work well on getting the right answer, but also to work as poorly as possible on identifying which <b>data</b> <b>set</b> my example came from.", "dateLastCrawled": "2022-01-30T18:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Fairness in machine learning with tractable models</b> - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S0950705120308443", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0950705120308443", "snippet": "This prevents the <b>algorithm</b> <b>learning</b> direct <b>bias</b> on the basis of the protected attribute, but does not prevent indirect <b>bias</b>, ... When a support vector <b>machine</b> was <b>trained</b> on the same <b>data</b>, the result was 0.036, which is only a little lower than the disparate impact for the logistic regression model <b>trained</b> on the \u201cfairness through unawareness\u201d <b>data</b>. This is most likely attributable to the inherently linear <b>data</b> partition imposed by the logistic regression model. We have attempted to ...", "dateLastCrawled": "2022-01-20T21:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Machine</b> <b>Learning for Precision Psychiatry: Opportunities and Challenges</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S2451902217302069", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2451902217302069", "snippet": "Indeed, supplementing traditional null-hypothesis testing, <b>machine</b> <b>learning</b> has a rich legacy of <b>algorithm</b> developments that <b>can</b> now be repurposed to automatically extract from <b>data</b> manifolds that describe behavior, life experience, brain, or genetics. Representation-<b>learning</b> algorithms operate on the assumption that the measured <b>data</b> have been generated by a <b>set</b> of underlying constituent factors. Unfortunately, however, many traditional clustering algorithms, such as hierarchical and", "dateLastCrawled": "2022-01-05T06:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Which machine learning algorithms are optimal for</b> boolean features? - Quora", "url": "https://www.quora.com/Which-machine-learning-algorithms-are-optimal-for-boolean-features", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>Which-machine-learning-algorithms-are-optimal-for</b>-boolean-features", "snippet": "Answer (1 of 5): Boosted decision trees work very well for categorical (in this case, 2-categorical) features. The information gain criterion [1] works especially ...", "dateLastCrawled": "2022-01-28T21:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "From Text to <b>Thought</b>: How Analyzing Language <b>Can</b> Advance Psychological ...", "url": "https://journals.sagepub.com/doi/full/10.1177/17456916211004899", "isFamilyFriendly": true, "displayUrl": "https://journals.sagepub.com/doi/full/10.1177/17456916211004899", "snippet": "For example, an unsupervised <b>machine</b>-<b>learning</b> <b>algorithm</b> could use a corpus of speeches to automatically identify major semantic themes on the basis of co-occurring words, whereas a supervised <b>algorithm</b> could be <b>trained</b> to recognize that negative words frequently precede positive words or even to recognize metaphors (Jacobs &amp; Kinder, 2017). When applied to King\u2019s speech, this <b>algorithm</b> would be able to do far more than a simple word-counting technique by potentially revealing themes of ...", "dateLastCrawled": "2022-02-03T01:09:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Biases in <b>machine</b> <b>learning</b> models and big <b>data</b> analytics: The ...", "url": "http://international-review.icrc.org/articles/biases-machine-learning-big-data-analytics-ihl-implications-913", "isFamilyFriendly": true, "displayUrl": "international-review.icrc.org/articles/<b>bias</b>es-<b>machine</b>-<b>learning</b>-big-<b>data</b>-analytics-ihl...", "snippet": "<b>Data</b> sets often contain biases which have the potential to unfairly disadvantage <b>certain</b> groups or to over-focus on <b>certain</b> activities to the detriment of others, and ML models or big <b>data</b> analytics <b>trained</b> on such <b>data</b> sets <b>can</b> inherit these biases.22 The following section discusses human biases that most commonly appear in <b>data</b> sets used for ML models and thus are most likely to impact ICL investigations and IHL considerations: implicit <b>bias</b>, selection <b>bias</b>, reporting <b>bias</b>, group ...", "dateLastCrawled": "2022-02-01T10:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Comparison of <b>machine</b> <b>learning</b> and logistic regression models in ...", "url": "https://www.sciencedirect.com/science/article/pii/S1386505621001106", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1386505621001106", "snippet": "An <b>in-group</b> comparison of ML models revealed that gradient boosting, consisting of gradient boosting and extreme gradient boosting models, possessed superior performance at predicting AKI as <b>compared</b> to artificial neural network, support vector <b>machine</b>, and Bayesian network models. Variables related to creatinine were the most common, significant predictive variables described, while representing a relatively small amount of total significant predictive variables. Collectively, these <b>data</b> ...", "dateLastCrawled": "2022-01-27T13:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Machine</b> <b>Learning</b> Applications in Endocrinology and Metabolism Research ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7090299/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7090299", "snippet": "<b>Machine</b> <b>learning</b> (ML) applications have received extensive attention in endocrinology research during the last decade. This review summarizes the basic concepts of ML and <b>certain</b> research topics in endocrinology and metabolism where ML principles have been actively deployed. Relevant studies are discussed to provide an overview of the methodology, main findings, and limitations of ML, with the goal of stimulating insights into future research directions. Clear, testable study hypotheses stem ...", "dateLastCrawled": "2022-01-19T02:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) Managing <b>bias</b> and unfairness in <b>data</b> for decision support: a ...", "url": "https://www.researchgate.net/publication/351356019_Managing_bias_and_unfairness_in_data_for_decision_support_a_survey_of_machine_learning_and_data_engineering_approaches_to_identify_and_mitigate_bias_and_unfairness_within_data_management_and_analytics", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/351356019_Managing_<b>bias</b>_and_unfairness_in...", "snippet": "Managing <b>bias</b> and unfairness in <b>data</b> for decision support: a survey of <b>machine</b> <b>learning</b>\u2026 753 into a causal paradigm and specify the potential causal depen- dencies between attributes.", "dateLastCrawled": "2022-01-16T17:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Engineering Bias Out of AI</b> - IEEE Spectrum", "url": "https://spectrum.ieee.org/engineering-bias-out-of-ai", "isFamilyFriendly": true, "displayUrl": "https://spectrum.ieee.org/<b>engineering-bias-out-of-ai</b>", "snippet": "Other algorithms tweak the <b>machine</b>-<b>learning</b> process instead of the training <b>data</b>. The goal of <b>machine</b>-<b>learning</b> models is to predict an outcome as accurately as possible. So if the training <b>set</b> has ...", "dateLastCrawled": "2022-02-03T08:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Bias</b>, awareness, and ignorance in deep-<b>learning</b>-based face recognition ...", "url": "https://link.springer.com/article/10.1007%2Fs43681-021-00108-6", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s43681-021-00108-6", "snippet": "Given a <b>data</b> <b>set</b> with labels for sensitive attributes such as ethnicity and gender, the images <b>can</b> be grouped into clusters based on these labels. To investigate the influence of this clustering on the face recognition rates, we propose a blinding procedure to remove the information related to the separation of these clusters in the embedding space. The procedure is a linear operation and uses the following steps: 1. Compute the centroids of the clusters defined by the sensitive attributes ...", "dateLastCrawled": "2022-02-03T10:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Managing <b>bias and unfairness</b> in <b>data</b> for decision support: a survey of ...", "url": "https://link.springer.com/article/10.1007/s00778-021-00671-8", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00778-021-00671-8", "snippet": "At training time, this would increase the likelihood that the outputs of a model <b>trained</b> on such dataset are fair (note: an \u201cunbiased\u201d training dataset does not guarantee an unbiased resulting system since new unwanted biases might arise from the <b>machine</b> <b>learning</b> <b>algorithm</b> used or small unwanted biases in the <b>data</b> might be reinforced by the <b>machine</b> <b>learning</b> model, but helps); while at deployment time, it would monitor whether the predictions made for new <b>data</b> points are fair. Constraints ...", "dateLastCrawled": "2022-02-01T12:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Interventions | <b>Machine Learning Bias Mitigation</b>", "url": "https://cdeiuk.github.io/bias-mitigation/interventions/", "isFamilyFriendly": true, "displayUrl": "https://cdeiuk.github.io/<b>bias</b>-mitigation/interventions", "snippet": "There was a small drop in accuracy, whereas our baseline achieved 85.3% test <b>set</b> accuracy, the model <b>trained</b> on the fair <b>data</b> achieved 85.0%. However there was also hardly any change in demographic parity difference, going from 0.193 to 0.186. Below we show a box plot of the score distributions for the two models. They appear very similar.", "dateLastCrawled": "2022-02-02T02:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "DeePhage: distinguishing virulent and temperate phage-derived sequences ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8427542/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8427542", "snippet": "Because it is hard to gain an intuitive display about high-dimension features, we used t-distributed stochastic neighbour embedding (t-SNE) , which is a <b>machine</b> <b>learning</b> <b>algorithm</b> for dimensionality reduction, for the visualization of high-dimensional <b>data</b> in a 2D projected space. After the training process, we first used PCA to reduce the features of the 5 aforementioned layers into a 10D space and then used t-SNE to reduce them into a 2D space using the sequences from Group D. The ...", "dateLastCrawled": "2022-02-02T15:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Algorithmic <b>bias</b>: Senses, sources, solutions - Fazelpour - 2021 ...", "url": "https://onlinelibrary.wiley.com/doi/10.1111/phc3.12760?af=R", "isFamilyFriendly": true, "displayUrl": "https://onlinelibrary.wiley.com/doi/10.1111/phc3.12760?af=R", "snippet": "Given a dataset, developers use a <b>learning</b> <b>algorithm</b> to \u2018fit\u2019 a predictive model on a portion of the <b>data</b>\u2013the \u2018training <b>set</b>\u2019\u2013whose performance is validated on a previously unseen portion of the dataset\u2013the \u2018test <b>set</b>\u2019. 3 These modeling and validation processes, often conducted iteratively, almost always optimize and evaluate model performance relative to some criteria of success. While this stage <b>can</b> be highly technical, it also involves multiple value judgments. As noted ...", "dateLastCrawled": "2022-01-01T23:38:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A Gentle Introduction to <b>Machine Learning</b> Concepts | by Robbie Allen ...", "url": "https://medium.com/machine-learning-in-practice/a-gentle-introduction-to-machine-learning-concepts-cfe710910eb", "isFamilyFriendly": true, "displayUrl": "https://medium.com/<b>machine-learning</b>-in-practice/a-gentle-introduction-to-<b>machine</b>...", "snippet": "<b>Machine learning</b> doesn\u2019t just happen in the ether. All that computation has to take place somewhere. Whether you do your calculations on-site or in the cloud, <b>machine learning</b> is a physical ...", "dateLastCrawled": "2022-02-03T15:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Machine</b> <b>Learning Techniques for Group Technology Applications</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0007850607627450", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0007850607627450", "snippet": "Based on the relative roles of teacher and learner, <b>learning</b> can be classified as 1131: <b>learning</b> by rote, <b>learning</b> by instruction, <b>learning</b> by <b>analogy</b>, <b>learning</b> from examples, <b>learning</b> from observation and discovery. The laat two kinds require inductive <b>learning</b> which is the process of acquiring new knowledge by making inductive inferences from facts provided by a teacher or environment. For a typical inductive <b>learning</b> problem, the givens are (1) a set of observational statements that ...", "dateLastCrawled": "2022-01-19T03:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) A Conceptual Framework for Investigating and Mitigating <b>Machine</b> ...", "url": "https://www.researchgate.net/publication/353587790_A_Conceptual_Framework_for_Investigating_and_Mitigating_Machine_Learning_Measurement_Bias_MLMB_in_Psychological_Assessment", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/353587790_A_Conceptual_Framework_for...", "snippet": "Given significant concerns about fairness and <b>bias</b> in the use of artificial intelligence (AI) and <b>machine</b> <b>learning</b> (ML) for assessing psychological constructs, we provide a conceptual framework ...", "dateLastCrawled": "2021-12-15T19:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Fairness in Machine Learning with Tractable Models</b> | DeepAI", "url": "https://deepai.org/publication/fairness-in-machine-learning-with-tractable-models", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>fairness-in-machine-learning-with-tractable-models</b>", "snippet": "<b>Fairness in Machine Learning with Tractable Models</b>. 05/16/2019 \u2219 by Michael Varley, et al. \u2219 0 \u2219 share . <b>Machine</b> <b>Learning</b> techniques have become pervasive across a range of different applications, and are now widely used in areas as disparate as recidivism prediction, consumer credit-risk analysis and insurance pricing. The prevalence of <b>machine</b> <b>learning</b> techniques has raised concerns about the potential for learned algorithms to become biased against certain groups.", "dateLastCrawled": "2021-11-27T09:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Bow-tie signaling in c-di-GMP: <b>Machine</b> <b>learning</b> in a simple biochemical ...", "url": "https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005677", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005677", "snippet": "The <b>analogy</b> between c-di-GMP signaling and a <b>machine</b> <b>learning</b> classifier explains that weak selection favors generalist bacteria; generalists integrate environmental stimuli and decide between biofilm and swarming according to the environmental fluctuations experienced in their evolutionary history. Evolution in strong selection, on the other hand, favors specialists. This is similar to how small data sets tend to produce biased classifiers.", "dateLastCrawled": "2019-11-12T12:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "The human factor: Working with machines to make big decisions", "url": "https://images.forbes.com/forbesinsights/StudyPDFs/PwC-The_Human_Factor-REPORT.pdf", "isFamilyFriendly": true, "displayUrl": "https://images.forbes.com/forbesinsights/StudyPDFs/PwC-The_Human_Factor-REPORT.pdf", "snippet": "role of <b>machine</b> <b>learning</b> in the business world. \u201cArtificial intelligence can help people make faster, better, and cheaper decisions. For that to happen, first and foremost, you need an openness of mind to collaborate with the <b>machine</b>, as opposed to treating the technology as either a servant or an overlord.\u201d This balance of mind and machines is just taking hold as companies experiment. Executives say their internal cultures could be more data-driven, with a greater emphasis on data ...", "dateLastCrawled": "2022-01-29T18:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Comparing diversity between groups - drive5", "url": "https://www.drive5.com/usearch/manual/diversity_metrics_compare_groups.html", "isFamilyFriendly": true, "displayUrl": "https://www.drive5.com/usearch/manual/diversity_metrics_compare_groups.html", "snippet": "<b>Machine</b> <b>learning</b> Chimeras Read quality Paired reads OTU errors and biases. Publications. Comparing diversity between sample groups. See also alpha_div_sig command Which alpha and beta diversity metrics are recommended? Interpreting alpha and beta diversity Alpha diversity Beta diversity Abundance <b>bias</b> Cross-talk. Diversity metrics can be compared between groups by amplicon sequencing It is not possible to measure alpha diversity of a single sample. However, diversity metrics can nevertheless ...", "dateLastCrawled": "2022-02-01T14:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Recommendation System Series Part 7: The 3 Variants of Boltzmann ...", "url": "https://towardsdatascience.com/recsys-series-part-7-the-3-variants-of-boltzmann-machines-for-collaborative-filtering-4c002af258f9", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/recsys-series-part-7-the-3-variants-of-boltzmann...", "snippet": "The <b>learning</b> algorithm is intuitive: They subtracted the sleep phase correlation from the wake <b>learning</b> phase and then adjusted the weights accordingly. With a big enough dataset, this algorithm can effectively learn arbitrary mappings between input and output. The Boltzmann <b>machine</b> <b>analogy</b> turns out to be a good insight into what\u2019s happing in the human brain during sleep. In cognitive science, there\u2019s a concept called replay, where the hippocampus plays back our memories and experiences ...", "dateLastCrawled": "2022-01-31T08:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "15 Examples of Implicit <b>Bias</b> in the Workplace \u2014 viaMaven: The #1 ...", "url": "https://www.viamaven.com/blog/examples-of-implicit-bias-at-work", "isFamilyFriendly": true, "displayUrl": "https://www.viamaven.com/blog/examples-of-implicit-<b>bias</b>-at-work", "snippet": "There can be an implicit <b>bias</b> to encourage marginalized groups to behave a certain way. For example, managers may overpraise women or black men for being on \u201cgood behavior.\u201d. Example 3: Assertive women. Women may be afraid to be assertive because it will be viewed as being \u201cdifficult.\u201d. Example 4: Black men and tempers.", "dateLastCrawled": "2022-02-03T02:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What does the word &#39;fit&#39; mean in <b>machine</b> <b>learning</b>? - Quora", "url": "https://www.quora.com/What-does-the-word-fit-mean-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-does-the-word-fit-mean-in-<b>machine</b>-<b>learning</b>", "snippet": "Answer (1 of 3): \u201cFit\u201d is a term that belongs to the process of back propagation, where the desired outcome is pre-set and the AI is rewarded based on how close it comes to that outcome. What you want the AI to learn is called \u201cfit\u201d. For example, if an AI is trying to learn to play guitar, \u201cfit\u201d ...", "dateLastCrawled": "2022-01-26T08:00:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(in-group bias)  is like +(machine learning algorithm \"trained\" on a certain set of data)", "+(in-group bias) is similar to +(machine learning algorithm \"trained\" on a certain set of data)", "+(in-group bias) can be thought of as +(machine learning algorithm \"trained\" on a certain set of data)", "+(in-group bias) can be compared to +(machine learning algorithm \"trained\" on a certain set of data)", "machine learning +(in-group bias AND analogy)", "machine learning +(\"in-group bias is like\")", "machine learning +(\"in-group bias is similar\")", "machine learning +(\"just as in-group bias\")", "machine learning +(\"in-group bias can be thought of as\")", "machine learning +(\"in-group bias can be compared to\")"]}