{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "How to <b>Develop Word-Based Neural Language Models in Python</b> with Keras", "url": "https://machinelearningmastery.com/develop-word-based-neural-language-models-python-keras/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>develop-word-based-neural-language-models</b>-python-keras", "snippet": "A statistical <b>language</b> <b>model</b> is <b>learned</b> from raw text and predicts the probability of the next word in the sequence given the words already present in the sequence. <b>Language</b> models are a key component in larger models for challenging natural <b>language</b> processing problems, <b>like</b> machine translation and speech recognition. They can also be developed as standalone models and used for generating new sequences that have the same statistical properties as the source text. <b>Language</b> models both learn ...", "dateLastCrawled": "2022-02-02T23:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Language Modeling</b> - GitHub Pages", "url": "https://lena-voita.github.io/nlp_course/language_modeling.html", "isFamilyFriendly": true, "displayUrl": "https://lena-voita.github.io/nlp_course/<b>language_modeling</b>.html", "snippet": "They trained a character-level LM with multiplicative LSTM on a corpus of 82 million Amazon reviews. Turned out, the <b>model</b> <b>learned</b> to track sentiment! Note that this result is qualitatively different from the previous one. In the previous examples, neurons were of course very fun, but those things relate to the <b>language modeling</b> task in an ...", "dateLastCrawled": "2022-01-29T13:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Oromo</b> <b>Language</b> (Afaan Oromoo) | Beekan Erena", "url": "https://scholar.harvard.edu/erena/oromo-language-afaan-oromoo", "isFamilyFriendly": true, "displayUrl": "https://scholar.harvard.edu/erena/<b>oromo</b>-<b>language</b>-afaan-<b>oromo</b>o", "snippet": "<b>Oromo</b> <b>language</b> is a Cushitic <b>language</b> spoken by more than about 50 million people in Ethiopia, Kenya, Somalia, and Egypt and is the 3rd largest <b>language</b> in Africa. There are more <b>Oromo</b> speakers abroad than the resident population in Ethiopia. In United States, Australia, Canada and different Europe cities people are speaking and communities are teaching their kids and foreigners those interested communications in Afaan Oromoo also taking the <b>Oromo</b> class. In Oromia, it <b>has</b> the status of an ...", "dateLastCrawled": "2022-02-02T21:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Verbal</b> &amp; Non-<b>Verbal</b> Communication Strategies for Students - Video ...", "url": "https://study.com/academy/lesson/verbal-non-verbal-communication-strategies-for-students.html", "isFamilyFriendly": true, "displayUrl": "https://study.com/academy/lesson/<b>verbal</b>-non-<b>verbal</b>-communication-strategies-for...", "snippet": "<b>Verbal</b> communication is the use of words to convey meaning. However, nonverbal communication, which includes all aspects of body <b>language</b>, is more than half of all perceived communication ...", "dateLastCrawled": "2022-02-02T23:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) <b>The Revised Hierarchical Model: A critical review</b> and assessment", "url": "https://www.researchgate.net/publication/45441239_The_Revised_Hierarchical_Model_A_critical_review_and_assessment", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/45441239", "snippet": "The <b>model</b> <b>has</b> been tested in over 100 studies of children, adults, second <b>language</b> learners, and people with aphasia in 18 different languages. The studies that Csaba Pl\u00e9h conducted in this ...", "dateLastCrawled": "2022-02-02T21:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "\u201cA <b>Masked</b> Beauty\u201d \u2013 Three Models of Mind and Consciousness \u2014 <b>Nathan Smith</b>", "url": "https://www.nathansmithbooks.com/blog/2018/11/14/a-masked-beauty-three-models-of-mind-and-consciousness", "isFamilyFriendly": true, "displayUrl": "https://www.<b>nathansmith</b>books.com/blog/2018/11/14/a-<b>masked</b>-beauty-three-<b>models</b>-of-mind...", "snippet": "Mind is the most fundamental yet enigmatic layer of a being. Arthur Schopenhauer, describing Kantian philosophy\u2019s relation to Christianity, may have described the human being\u2019s relation to their own mind: \u201ca man who at a ball <b>has</b> been flirting the whole evening with a <b>masked</b> beauty, in hopes of making a conquest; till at last, throwing off her disguise, she reveals herself \u2014 as his wife\u201d (Schopenhauer Basis 105). Intimate yet occulted by proximity, mind and consciousness have ...", "dateLastCrawled": "2022-01-26T07:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Between-language competition as a</b> <b>driving force in foreign language</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0010027720300378", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0010027720300378", "snippet": "We assess the role of between-<b>language</b> competition in <b>foreign</b> <b>language</b> attrition by means of a modified retrieval-induced forgetting paradigm consisting of three different phases: an L3 Spanish study phase, an interference phase (corresponding to the retrieval practice phase in RIF studies) in which the participants (native speakers of Dutch) are asked to retrieve half of the recently <b>learned</b> words in another <b>language</b>, and a final L3 Spanish test phase. We hypothesize that the retrieval of ...", "dateLastCrawled": "2022-01-16T07:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Top 8 <b>Pre-Trained NLP Models Developers Must Know</b>", "url": "https://analyticsindiamag.com/top-8-pre-trained-nlp-models-developers-must-know/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/top-8-<b>pre-trained-nlp-models-developers-must-know</b>", "snippet": "This large scale transformer-based <b>language</b> <b>model</b> <b>has</b> been trained on 175 billion parameters, which is ten times more than any previous non-sparse <b>language</b> <b>model</b> available. The <b>model</b> <b>has</b> been trained to achieve strong performance on many NLP datasets, including tasks <b>like</b> translation, answering questions, as well as several tasks that require on-the-fly reasoning such as unscrambling words. With its recent advancements, it <b>has</b> been used even to write news articles and generate codes helping ...", "dateLastCrawled": "2022-02-03T02:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Variability and Consistency in First and Second <b>Language</b> Processing: A ...", "url": "https://onlinelibrary.wiley.com/doi/full/10.1111/lang.12370", "isFamilyFriendly": true, "displayUrl": "https://onlinelibrary.wiley.com/doi/full/10.1111/lang.12370", "snippet": "A number of previous studies found that despite having reached a high level of proficiency in a given <b>language</b>, L2 speakers may show reduced <b>masked</b> priming effects relative to L1 speakers, particularly for regularly inflected word forms (Jacob et al., 2018; Kirkici &amp; Clahsen, 2013; Silva &amp; Clahsen, 2008). Furthermore, L2 processing of morphologically complex words <b>has</b> been found to be more susceptible to surface form prime\u2013target overlap than L1 processing. Unlike L1 control groups ...", "dateLastCrawled": "2021-11-24T15:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Psychology Chapter 12 and 13 Practice questions</b> - <b>Quizlet</b>", "url": "https://quizlet.com/454705372/psychology-chapter-12-and-13-practice-questions-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/454705372/<b>psychology-chapter-12-and-13-practice-questions</b>-flash-cards", "snippet": "Marta <b>has</b> a successful job, is married to a handsome and kind <b>person</b>, and <b>has</b> three intelligent children. What would you predict about Marta&#39;s self-esteem based on this information? 1)She will have high self-esteem but only in specific situations. 2)She will have high self-esteem in general because she is successful in many areas of her life.", "dateLastCrawled": "2021-12-25T04:31:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "How to <b>Develop Word-Based Neural Language Models in Python</b> with Keras", "url": "https://machinelearningmastery.com/develop-word-based-neural-language-models-python-keras/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>develop-word-based-neural-language-models</b>-python-keras", "snippet": "A statistical <b>language</b> <b>model</b> is <b>learned</b> from raw text and predicts the probability of the next word in the sequence given the words already present in the sequence. <b>Language</b> models are a key component in larger models for challenging natural <b>language</b> processing problems, like machine translation and speech recognition. They can also be developed as standalone models and used for generating new sequences that have the same statistical properties as the source text. <b>Language</b> models both learn ...", "dateLastCrawled": "2022-02-02T23:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Predictors of <b>Language</b> Dominance: An Integrated Analysis of First ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6110303/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC6110303", "snippet": "Given that L1At populations command two languages which are <b>learned</b> under <b>similar</b> conditions (naturalistic learning through immersion in the linguistic community) but at different stages in life \u2013 that is, speakers who unambiguously have one native and one non-native <b>language</b> \u2013 a fruitful framework for the assessment of proficiency is the <b>model</b> proposed by Hulstijn (2011, 2015) which distinguishes Shared/Basic <b>Language</b> Cognition (BLC) and Extended/Higher <b>Language</b> Cognition (HLC). In this ...", "dateLastCrawled": "2022-02-01T06:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Language Modeling</b> - GitHub Pages", "url": "https://lena-voita.github.io/nlp_course/language_modeling.html", "isFamilyFriendly": true, "displayUrl": "https://lena-voita.github.io/nlp_course/<b>language_modeling</b>.html", "snippet": "They trained a character-level LM with multiplicative LSTM on a corpus of 82 million Amazon reviews. Turned out, the <b>model</b> <b>learned</b> to track sentiment! Note that this result is qualitatively different from the previous one. In the previous examples, neurons were of course very fun, but those things relate to the <b>language modeling</b> task in an ...", "dateLastCrawled": "2022-01-29T13:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Variability and Consistency in First and Second <b>Language</b> Processing: A ...", "url": "https://onlinelibrary.wiley.com/doi/full/10.1111/lang.12370", "isFamilyFriendly": true, "displayUrl": "https://onlinelibrary.wiley.com/doi/full/10.1111/lang.12370", "snippet": "A number of previous studies found that despite having reached a high level of proficiency in a given <b>language</b>, L2 speakers may show reduced <b>masked</b> priming effects relative to L1 speakers, particularly for regularly inflected word forms (Jacob et al., 2018; Kirkici &amp; Clahsen, 2013; Silva &amp; Clahsen, 2008). Furthermore, L2 processing of morphologically complex words <b>has</b> been found to be more susceptible to surface form prime\u2013target overlap than L1 processing. Unlike L1 control groups ...", "dateLastCrawled": "2021-11-24T15:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "\u201cA <b>Masked</b> Beauty\u201d \u2013 Three Models of Mind and Consciousness \u2014 <b>Nathan Smith</b>", "url": "https://www.nathansmithbooks.com/blog/2018/11/14/a-masked-beauty-three-models-of-mind-and-consciousness", "isFamilyFriendly": true, "displayUrl": "https://www.<b>nathansmith</b>books.com/blog/2018/11/14/a-<b>masked</b>-beauty-three-<b>models</b>-of-mind...", "snippet": "Mind is the most fundamental yet enigmatic layer of a being. Arthur Schopenhauer, describing Kantian philosophy\u2019s relation to Christianity, may have described the human being\u2019s relation to their own mind: \u201ca man who at a ball <b>has</b> been flirting the whole evening with a <b>masked</b> beauty, in hopes of making a conquest; till at last, throwing off her disguise, she reveals herself \u2014 as his wife\u201d (Schopenhauer Basis 105). Intimate yet occulted by proximity, mind and consciousness have ...", "dateLastCrawled": "2022-01-26T07:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Top 8 <b>Pre-Trained NLP Models Developers Must Know</b>", "url": "https://analyticsindiamag.com/top-8-pre-trained-nlp-models-developers-must-know/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/top-8-<b>pre-trained-nlp-models-developers-must-know</b>", "snippet": "The <b>model</b> <b>has</b> been released as an open-source implementation on the TensorFlow framework and includes many ready-to-use pertained <b>language</b> representation models. The <b>model</b> further uses 89% fewer parameters than the BERT <b>model</b> \u2014 only 12M parameters and with way less loss of accuracy while evaluating, with an average of 80.1% accuracy. The <b>model</b> uses two optimisations to reduce <b>model</b> size \u2014 factorisation of the embedding layer and parameter-sharing across the hidden layers of the network.", "dateLastCrawled": "2022-02-03T02:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Verbal</b> &amp; Non-<b>Verbal</b> Communication Strategies for Students - Video ...", "url": "https://study.com/academy/lesson/verbal-non-verbal-communication-strategies-for-students.html", "isFamilyFriendly": true, "displayUrl": "https://study.com/academy/lesson/<b>verbal</b>-non-<b>verbal</b>-communication-strategies-for...", "snippet": "<b>Verbal</b> communication is the use of words to convey meaning. However, nonverbal communication, which includes all aspects of body <b>language</b>, is more than half of all perceived communication ...", "dateLastCrawled": "2022-02-02T23:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Between-language competition as a</b> <b>driving force in foreign language</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0010027720300378", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0010027720300378", "snippet": "Here we ask whether <b>similar</b> dynamics are at play in <b>foreign</b> <b>language</b> (FL) attrition. We tested whether interference from translation equivalents in other, more recently used languages causes subsequent retrieval failure in L3. In Experiment 1, we investigated whether interference from the native <b>language</b> (L1) and/or from another <b>foreign</b> <b>language</b> (L2) affected L3 vocabulary retention. On day 1, Dutch native speakers <b>learned</b> 40 new Spanish (L3) words. On day 2, they performed a number of ...", "dateLastCrawled": "2022-01-16T07:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Language</b> Ch 11 &amp; 13 Flashcards | Quizlet", "url": "https://quizlet.com/363524494/language-ch-11-13-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/363524494/<b>language</b>-ch-11-13-flash-cards", "snippet": "BUT if <b>foreign</b> <b>language</b>: no phonemic restoration effect!-don&#39;t understand words -&gt; don&#39;t have linguistic info to complete missing info . factors affecting phonemic restoration. 1) MEANING of the following words in sentence (*ave goodbye) 2) Word LENGTH (longer words -&gt; increase likelihood of phonemic restoration) 3) More restoration for REAL WORD vs non word 4) Better restoration when masking sound &amp; <b>masked</b> phoneme = <b>SIMILAR</b> 5) Noise must be LOUD. phonemes and meaning experiment. easier to ...", "dateLastCrawled": "2022-01-16T03:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Psychology Chapter 12 and 13 Practice questions</b> - <b>Quizlet</b>", "url": "https://quizlet.com/454705372/psychology-chapter-12-and-13-practice-questions-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/454705372/<b>psychology-chapter-12-and-13-practice-questions</b>-flash-cards", "snippet": "Freud&#39;s psychodynamic theory and Roger&#39;s <b>person</b>-centered theory are <b>similar</b> in that they both 1)state that unconscious influences can impact personality. 2)focus primarily on early childhood experiences. 3)emphasize the parent-child relationship in personality development. 4)encourage unconditional positive regard to resolve personality fixations.", "dateLastCrawled": "2021-12-25T04:31:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Word-finding difficulty: a clinical analysis of the progressive aphasias", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2373641/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC2373641", "snippet": "Furthermore, the stimulus of the focal <b>language</b>-based dementias <b>has</b> led to a wider appreciation of speech and <b>language</b> dysfunction in other neurodegenerative conditions, including Alzheimer&#39;s disease (AD) (Emery, 2000; Croot et al., 2000) and the problem of the differential diagnosis of \u2018progressive aphasia\u2019 in this broader sense. Accordingly, a conceptual framework is needed to allow the clinician to interpret the patient&#39;s complaint of word-finding difficulty in line with emerging ...", "dateLastCrawled": "2022-02-03T00:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "How to <b>Develop Word-Based Neural Language Models in Python</b> with Keras", "url": "https://machinelearningmastery.com/develop-word-based-neural-language-models-python-keras/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>develop-word-based-neural-language-models</b>-python-keras", "snippet": "A statistical <b>language</b> <b>model</b> is <b>learned</b> from raw text and predicts the probability of the next word in the sequence given the words already present in the sequence. <b>Language</b> models are a key component in larger models for challenging natural <b>language</b> processing problems, like machine translation and speech recognition. They <b>can</b> also be developed as standalone models and used for generating new sequences that have the same statistical properties as the source text. <b>Language</b> models both learn ...", "dateLastCrawled": "2022-02-02T23:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 2, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Michel Foucault</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Michel_Foucault", "thumbnailUrl": "https://www.bing.com/th?id=OIP.KarKUl3VQRRg8WvBnE20lwAAAA&pid=Api", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Michel_Foucault</b>", "snippet": "Paul-<b>Michel Foucault</b> (UK: / \u02c8 f u\u02d0 k o\u028a /, US: / f u\u02d0 \u02c8 k o\u028a /; French: [p\u0254l mi\u0283\u025bl fuko]; 15 October 1926 \u2013 25 June 1984) was a French philosopher, historian of ideas, writer, political activist, and literary critic.. Foucault&#39;s theories primarily address the relationship between power and knowledge, and how they are used as a form of social control through societal institutions. Though often cited as a structuralist and postmodernist, Foucault rejected these labels. His <b>thought</b> ...", "dateLastCrawled": "2022-02-02T22:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Is it time to leave behind the Revised Hierarchical <b>Model</b> of bilingual ...", "url": "https://www.cambridge.org/core/journals/bilingualism-language-and-cognition/article/is-it-time-to-leave-behind-the-revised-hierarchical-model-of-bilingual-language-processing-after-fifteen-years-of-service/37234AB8CEA7FB7CD614C5530E5653CA", "isFamilyFriendly": true, "displayUrl": "https://www.cambridge.org/core/journals/bilingualism-<b>language</b>-and-cognition/article/is...", "snippet": "<b>Masked</b> translation priming <b>can</b> be observed when the primes and the targets share the same alphabet, ... Importantly, these effects were observed even when using a set of newly <b>learned</b> number words (a so-called <b>foreign</b> <b>language</b>), which the participants acquired only one hour before testing. On the basis of these findings, Duyck and Brysbaert Reference Duyck and Brysbaert 2004) hypothesized that when there is a complete overlap of meaning between L1 and L2 words, new L2 words are not acquired ...", "dateLastCrawled": "2022-01-02T02:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Chapter 1: Theory of Markets and Privacy</b> | National Telecommunications ...", "url": "https://www.ntia.doc.gov/page/chapter-1-theory-markets-and-privacy", "isFamilyFriendly": true, "displayUrl": "https://www.ntia.doc.gov/page/chapter-1-theory-markets-and-privacy", "snippet": "The pure market <b>model</b> thus <b>has</b> a dynamic component, in which both customer preferences and company practices <b>can</b> evolve over time as awareness and concern about privacy themselves evolve. The effectiveness of publicity as a constraint on companies will depend on factors such as how well the media <b>can</b> detect privacy problems, how widespread reporting on the issue becomes, and how strongly customers will react to the stories. At the opposite extreme from the pure market <b>model</b> is the pure ...", "dateLastCrawled": "2022-02-02T21:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Subliminal messages exert long-term effects on decision-making", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6204644/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC6204644", "snippet": "This suggests that subliminal long-term effects are achieved only if familiar information is presented but not if novel relational information <b>has</b> to be <b>learned</b>. We asked if humans <b>can</b> rapidly integrate and store novel relational information (e.g. \u201c<b>person</b> X is a manager\u201d, see Fig. 1a ) from subliminal messages for later use in a decision-making situation (e.g. \u201cguess the income of X\u201d).", "dateLastCrawled": "2022-01-26T21:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Needs of Austin\u2019s Asian American population <b>masked</b> by <b>model</b> minority myth", "url": "http://specials.mystatesman.com/austin-asian-population/", "isFamilyFriendly": true, "displayUrl": "specials.mystatesman.com/austin-asian-population", "snippet": "Dual <b>language</b> programs are designed to help students achieve proficiency in two languages. The school district\u2019s only other dual <b>language</b> program is a Spanish one. <b>Language</b> <b>can</b> present a barrier for Asian Americans in subtler ways. Nguyen mostly <b>learned</b> English after arriving in the U.S. Though she\u2019s been here 40 years, she is still ...", "dateLastCrawled": "2021-12-10T21:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Attitude</b> - Concepts Made Simple; With Examples - Clear IAS", "url": "https://www.clearias.com/attitude/", "isFamilyFriendly": true, "displayUrl": "https://www.clearias.com/<b>attitude</b>", "snippet": "This multi-component <b>model</b> is known as the ABC <b>Model</b> or CAB <b>Model</b>. Let\u2019s see the components of the CAB <b>model</b>. Cognitive Component \u2013 This involves the <b>person</b>\u2019s learning, knowledge, beliefs, and thoughts about the <b>attitude</b>-object (in our case, Honda cars). For example, if you have <b>learned</b> previously that Honda cars give more than 20 km/litre mileage on petrol \u2013 that <b>can</b> create a positive <b>attitude</b> towards the brand.", "dateLastCrawled": "2022-02-02T16:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Reflections\u2014and Lessons <b>Learned</b>\u2014From Remote Learning | <b>Edutopia</b>", "url": "https://www.edutopia.org/article/reflections-and-lessons-learned-remote-learning", "isFamilyFriendly": true, "displayUrl": "https://<b>www.edutopia.org</b>/article/reflections-and-lessons-<b>learned</b>-remote-learning", "snippet": "Reflections\u2014and Lessons <b>Learned</b>\u2014From Remote Learning. Experiences during remote learning inspire a teacher to reconsider\u2014and refresh\u2014her curriculum for the fall. By Carly Berwick. July 24, 2020. Brian Stauffer / The iSpot. Our last day of the school year, three months and a week after we went virtual, brought a strange sadness. After the whirlwind and stress of the last few months, the end felt like a slow exhale instead of a bang\u2014or, as a student said, one of those balloons that ...", "dateLastCrawled": "2022-02-01T19:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Why I Won&#39;t Learn <b>Esperanto</b> - <b>Language</b> Learning Made Simple", "url": "https://www.mezzoguild.com/esperanto/", "isFamilyFriendly": true, "displayUrl": "https://www.mezzoguild.com/<b>esperanto</b>", "snippet": "An English speaker <b>who has</b> never <b>learned</b> another <b>language</b> is as unaware of the soft mushy walls of the English <b>language</b> as a fish in an aquarium is aware of the water it swims in. If you study <b>Esperanto</b> seriously on the other hand, you become vividly aware of the foggy, ambiguous nature of the English <b>language</b>, It will force him to recognize the need for rules and discipline to give English the clarity it needs to be effective. I <b>learned</b> long ago when I read <b>Esperanto</b> to stop trying to ...", "dateLastCrawled": "2022-02-02T13:17:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "The Proximate Phonological Unit of Chinese-English Bilinguals ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3640013/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC3640013", "snippet": "An essential step to create phonology according to the <b>language</b> production <b>model</b> by Levelt, Roelofs and Meyer is to assemble phonemes into a metrical frame. However, recently, it <b>has</b> been proposed that different languages may rely on different grain sizes of phonological units to construct phonology. For instance, it <b>has</b> been proposed that, instead of phonemes, Mandarin Chinese uses syllables and Japanese uses moras to fill the metrical frame. In this study, we used a <b>masked</b> priming-naming ...", "dateLastCrawled": "2021-12-20T22:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Inflection and derivation in native and non-native <b>language</b> processing ...", "url": "https://www.cambridge.org/core/journals/bilingualism-language-and-cognition/article/inflection-and-derivation-in-native-and-nonnative-language-processing-masked-priming-experiments-on-turkish/2B3538288EF8192A2BC620184FEAEDDA", "isFamilyFriendly": true, "displayUrl": "https://www.cambridge.org/core/journals/bilingualism-<b>language</b>-and-cognition/article/...", "snippet": "<b>Masked</b> priming experiments with non-native speakers yielded an unusual pattern of results <b>compared</b> to what is known about <b>masked</b> morphological priming effects in the L1 in that priming for irregular inflection and for derived word forms paired with no priming for regular inflection <b>has</b> not been reported in any study of L1 processing. The L2 <b>masked</b> priming pattern was found to be parallel across a heterogeneous set of L1 backgrounds and different target languages. Despite typological ...", "dateLastCrawled": "2022-01-24T18:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Grounded cognition: Comparing <b>Language</b> \u00d7 Space interactions in first ...", "url": "https://www.cambridge.org/core/journals/applied-psycholinguistics/article/grounded-cognition-comparing-language-space-interactions-in-first-language-and-second-language/A405DDAB4B044C5AC497CFB2F24C30D2", "isFamilyFriendly": true, "displayUrl": "https://www.cambridge.org/core/journals/applied-psycholinguistics/article/grounded...", "snippet": "Furthermore, it would be interesting to investigate the effects of <b>language</b> proficiency using a more objective measure <b>compared</b> to the rather subjective indication of <b>language</b> proficiency that we used in the present study. Once we know more about how experiential traces are built and used during L2 acquisition and which moderating factors are at work, we might be able to gain a better understanding of the precise difficulties to learn spatial prepositions in an L2.", "dateLastCrawled": "2022-01-25T16:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Language</b> | <b>Encyclopedia.com</b>", "url": "https://www.encyclopedia.com/literature-and-arts/language-linguistics-and-literary-terms/language-and-linguistics/language-0", "isFamilyFriendly": true, "displayUrl": "https://<b>www.encyclopedia.com</b>/literature-and-arts/<b>language</b>-linguistics-and-literary...", "snippet": "For this reason, speech pathologists must be interested in the total adjustment of the <b>person</b> <b>who has</b> a speech problem. Etiology and therapy. If speech problems are examined from the standpoint of etiology, they <b>can</b> be categorized as organic or functional. In actuality it is extremely difficult to fit all speech problems neatly into one or the other of these two broad classifications. The two classifications are presented because they frequently appear in the literature. Organic and ...", "dateLastCrawled": "2022-01-29T01:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Strategies for Teaching Students Online</b> and Face to Face at the Same ...", "url": "https://www.edweek.org/teaching-learning/opinion-strategies-for-teaching-students-online-face-to-face-at-the-same-time/2021/02", "isFamilyFriendly": true, "displayUrl": "https://<b>www.edweek.org</b>/teaching-learning/opinion-strategies-for-teaching-students...", "snippet": "Christina Diaz <b>has</b> been teaching EL and bilingual students for 12 years. She is currently a 4th and 5th grade dual-<b>language</b> teacher in Downers Grove, Ill. You <b>can</b> follow her on Twitter at ...", "dateLastCrawled": "2022-01-29T00:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Word learning in the field: Adapting a laboratory-based task for ...", "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0257393", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0257393", "snippet": "Adapting laboratory psycholinguistic methods to fieldwork contexts <b>can</b> be fraught with difficulties. However, successful implementation of such methods in the field enhances our ability to learn the true extent and limitations of human behavior. This paper reports two attempts to run word learning experiments with the small community of Nungon speakers in Towet village in the Saruwaged Mountains, Papua New Guinea. A first attempt involved running a cross-situational task in which word-object ...", "dateLastCrawled": "2021-09-21T19:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "pen.el/nlp-natural-<b>language</b>-processing.txt at master \u00b7 semiosis/pen.el ...", "url": "https://github.com/semiosis/pen.el/blob/master/docs/glossaries/nlp-natural-language-processing.txt", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/semiosis/pen.el/blob/master/docs/glossaries/nlp-natural-<b>language</b>...", "snippet": "pen.el is a package for prompt engineering in emacs. It facilitates the creation, ongoing development, discovery and usage of prompts to a <b>language</b> <b>model</b> such as OpenAI&#39;s GPT-3 or EleutherAI&#39;s GPT-j. - pen.el/nlp-natural-<b>language</b>-processing.txt at master \u00b7 semiosis/pen.el", "dateLastCrawled": "2021-09-01T16:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Why I Won&#39;t Learn <b>Esperanto</b> - <b>Language</b> Learning Made Simple", "url": "https://www.mezzoguild.com/esperanto/", "isFamilyFriendly": true, "displayUrl": "https://www.mezzoguild.com/<b>esperanto</b>", "snippet": "An English speaker <b>who has</b> never <b>learned</b> another <b>language</b> is as unaware of the soft mushy walls of the English <b>language</b> as a fish in an aquarium is aware of the water it swims in. If you study <b>Esperanto</b> seriously on the other hand, you become vividly aware of the foggy, ambiguous nature of the English <b>language</b>, It will force him to recognize the need for rules and discipline to give English the clarity it needs to be effective. I <b>learned</b> long ago when I read <b>Esperanto</b> to stop trying to ...", "dateLastCrawled": "2022-02-02T13:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Democracy Is Inevitable</b> - <b>Harvard Business Review</b>", "url": "https://hbr.org/1990/09/democracy-is-inevitable", "isFamilyFriendly": true, "displayUrl": "https://<b>hbr.org</b>/1990/09/<b>democracy-is-inevitable</b>", "snippet": "1. Full and free communication, regardless of rank and power. 2. A reliance on consensus rather than on coercion or compromise to manage conflict. 3. The idea that influence is based on technical ...", "dateLastCrawled": "2022-02-02T18:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>The Impact of a Pandemic</b> - 799 Words | Bartleby", "url": "https://www.bartleby.com/essay/The-Impact-of-a-Pandemic-PKHWJJ94CDMRA", "isFamilyFriendly": true, "displayUrl": "https://<b>www.bartleby.com</b>/essay/<b>The-Impact-of-a-Pandemic</b>-PKHWJJ94CDMRA", "snippet": "A pandemic <b>has</b> a much higher infectious rate and an even larger death rate <b>compared</b> to an epidemic. The world <b>has</b> experienced a total of four pandemics in the twentieth century starting in 1918 until present. In 1918, the spanish flu caught worldwide attention when it infected close to half the population of the world, claiming more than 40 million lives. What made the spanish flu capable of infecting over a billion people was the ability to quickly transfer from <b>person</b> <b>to person</b>. At the ...", "dateLastCrawled": "2022-01-29T11:13:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine</b> <b>Learning</b> Glossary: <b>Language</b> Evaluation | Google Developers", "url": "https://developers.google.com/machine-learning/glossary/language", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine</b>-<b>learning</b>/glossary/<b>language</b>", "snippet": "For instance, a <b>masked</b> <b>language</b> <b>model</b> can calculate probabilities for candidate word(s) to replace the underline in the following sentence: The ____ in the hat came back. The literature typically uses the string &quot;MASK&quot; instead of an underline. For example: The &quot;MASK&quot; in the hat came back. Most modern <b>masked</b> <b>language</b> models are bidirectional.", "dateLastCrawled": "2022-01-29T02:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "The Illustrated GPT-2 (Visualizing Transformer <b>Language</b> Models) \u2013 Jay ...", "url": "http://jalammar.github.io/illustrated-gpt2/", "isFamilyFriendly": true, "displayUrl": "jalammar.github.io/illustrated-gpt2", "snippet": "GPT-2 <b>Masked</b> Self-Attention; Beyond <b>Language</b> modeling; You\u2019ve Made it! Part 3: Beyond <b>Language</b> Modeling. <b>Machine</b> Translation; Summarization ; Transfer <b>Learning</b>; Music Generation; Part #1: GPT2 And <b>Language</b> Modeling # So what exactly is a <b>language</b> <b>model</b>? What is a <b>Language</b> <b>Model</b>. In The Illustrated Word2vec, we\u2019ve looked at what a <b>language</b> <b>model</b> is \u2013 basically a <b>machine</b> <b>learning</b> <b>model</b> that is able to look at part of a sentence and predict the next word. The most famous <b>language</b> models ...", "dateLastCrawled": "2022-02-01T04:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "The <b>language</b> of <b>proteins: NLP, machine learning &amp; protein sequences</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S2001037021000945", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2001037021000945", "snippet": "For example, in the <b>masked</b> <b>language</b> task, some fraction of the tokens in the original text are <b>masked</b> at random, and the <b>language</b> <b>model</b> attempts to predict the original text. (B) (Pre-)trained <b>language</b> models are commonly fine-tuned on downstream tasks over labeled text, through a standard supervised-<b>learning</b> approach. Fine-tuning is typically much faster and provides superior performance than training a <b>model</b> from scratch, especially when labeled data is scarce.", "dateLastCrawled": "2022-02-03T02:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "An introduction to Deep <b>Learning</b> in Natural <b>Language</b> Processing: Models ...", "url": "https://www.sciencedirect.com/science/article/pii/S0925231221010997", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0925231221010997", "snippet": "The pre-training was driven by two <b>language</b> <b>model</b> objectives, i.e. <b>Masked</b> <b>Language</b> <b>Model</b> (MLM) and Next Sentence Prediction (NSP). In MLM, showed in Fig. 8 , the network masks a small number of words of the input sequence and it tries to predict them in output, whereas in NSP the network tries to understand the relations between sentences by means of a binary loss.", "dateLastCrawled": "2022-01-04T17:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Natrual <b>language</b> processing basic concepts - <b>language</b> <b>model</b> - word ...", "url": "https://shuffleai.blog/blog/nlp_concepts_part_1.html", "isFamilyFriendly": true, "displayUrl": "https://shuffleai.blog/blog/nlp_concepts_part_1.html", "snippet": "Before deep <b>learning</b>&#39;s domination in natural <b>language</b> processing, a <b>language</b> <b>model</b> is basically a large lookup table, recording frequencies of different combinations of words&#39; occurrences in a large corpus. Now it&#39;s a neural network trained on a corpus or dataset. In addition, a causal <b>language</b> <b>model</b>(e.g., GPT) predicts the next word, and a <b>masked</b> <b>language</b> <b>model</b>(e.g., BERT) fills the blank given the rest of a sentence. If you input &quot;The man ____ to the store&quot; to BERT, it will predict the ...", "dateLastCrawled": "2021-12-24T07:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Text Style Transfer for Bias Mitigation using <b>Masked</b> <b>Language</b> Modeling ...", "url": "https://www.researchgate.net/publication/358145352_Text_Style_Transfer_for_Bias_Mitigation_using_Masked_Language_Modeling", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/358145352_Text_Style_Transfer_for_Bias...", "snippet": "In the \\emph{infill} step, we utilize a pre-trained <b>Masked</b> <b>Language</b> <b>Model</b> (MLM) to infill the <b>masked</b> positions by predicting words or phrases conditioned on the context\\footnote{In this paper ...", "dateLastCrawled": "2022-01-29T13:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>GitHub</b> - <b>rosinality/ml-papers</b>: My collection of <b>machine</b> <b>learning</b> papers", "url": "https://github.com/rosinality/ml-papers", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/rosinality/ml-papers", "snippet": "210413 <b>Masked</b> <b>Language</b> Modeling and the Distributional Hypothesis #<b>language</b>_<b>model</b> #mlm; 210417 mT6 #<b>language</b>_<b>model</b>; 210418 Data-Efficient <b>Language</b>-Supervised Zero-Shot <b>Learning</b> with #multimodal; 210422 ImageNet-21K Pretraining for the Masses #backbone; 210510 Are Pre-trained Convolutions Better than Pre-trained Transformers #nlp #convolution # ...", "dateLastCrawled": "2022-01-31T15:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "14.8. <b>Bidirectional Encoder</b> Representations from Transformers (BERT ...", "url": "https://d2l.ai/chapter_natural-language-processing-pretraining/bert.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_natural-<b>language</b>-processing-pretraining/bert.html", "snippet": "As illustrated in Section 8.3, a <b>language</b> <b>model</b> predicts a token using the context on its left. To encode context bidirectionally for representing each token, BERT randomly masks tokens and uses tokens from the bidirectional context to predict the <b>masked</b> tokens in a self-supervised fashion. This task is referred to as a <b>masked</b> <b>language</b> <b>model</b>.", "dateLastCrawled": "2022-02-02T10:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Word Embeddings, WordPiece and Language-Agnostic BERT</b> (LaBSE) | by ...", "url": "https://medium.com/mlearning-ai/word-embeddings-wordpiece-and-language-agnostic-bert-labse-98c7626878c7", "isFamilyFriendly": true, "displayUrl": "https://medium.com/m<b>learning</b>-ai/<b>word-embeddings-wordpiece-and-language-agnostic-bert</b>...", "snippet": "Word embeddings are the representation of words in a numeric format, which can be understood by a computer. Simplest example would be (Yes, No) represented as (1, 0). But when we are dealing with\u2026", "dateLastCrawled": "2022-02-03T17:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Intuitive Introduction to BERT \u2013 MachineCurve</b>", "url": "https://www.machinecurve.com/index.php/2021/01/04/intuitive-introduction-to-bert/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/index.php/2021/01/04/<b>intuitive-introduction-to-bert</b>", "snippet": "We do however often want to create a <b>machine</b> <b>learning</b> <b>model</b> that can perform one task really well. This is where finetuning comes in: using a labeled corpus, which is often smaller, we can then train the pretrained <b>model</b> further, with an additional or replacing NLP task. The end result is a <b>model</b> that has been pretrained on the large unlabeled corpus and which is finetuned to a specific <b>language</b> task, such as summarization, text generation in a particular domain, or translation.", "dateLastCrawled": "2022-01-30T22:30:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Improving Text Generation with Dynamic Masking and Recovering", "url": "https://www.ijcai.org/proceedings/2021/0534.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.ijcai.org/proceedings/2021/0534.pdf", "snippet": "tokens, <b>just as masked language model</b> does. Therefore, our approach jointly maximizes both the likelihoods of both sen-tence generation and prediction of masked tokens. We verify the effectiveness and generality of our ap-proach on three types of text generation tasks which use var-ious forms of input data including text, graph, and image. For sequence-to-sequence (seq2seq) generation task (specif-ically, <b>machine</b> translation), our model obtains signi\ufb01cant improvement of 1.01 and 0.90 BLEU ...", "dateLastCrawled": "2022-01-29T07:50:00.0000000Z", "language": "en", "isNavigational": false}], [], []], "all_bing_queries": ["+(masked language model)  is like +(person who has learned a foreign language)", "+(masked language model) is similar to +(person who has learned a foreign language)", "+(masked language model) can be thought of as +(person who has learned a foreign language)", "+(masked language model) can be compared to +(person who has learned a foreign language)", "machine learning +(masked language model AND analogy)", "machine learning +(\"masked language model is like\")", "machine learning +(\"masked language model is similar\")", "machine learning +(\"just as masked language model\")", "machine learning +(\"masked language model can be thought of as\")", "machine learning +(\"masked language model can be compared to\")"]}