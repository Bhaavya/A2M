{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Calculating a <b>Least</b> <b>Squares</b> <b>Regression</b> <b>Line</b>: Equation, Example ...", "url": "https://technologynetworks.com/informatics/articles/calculating-a-least-squares-regression-line-equation-example-explanation-310265", "isFamilyFriendly": true, "displayUrl": "https://technologynetworks.com/informatics/articles/calculating-a-<b>least</b>-<b>squares</b>...", "snippet": "<b>Drawing</b> a <b>least</b> <b>squares</b> <b>regression</b> <b>line</b> by hand. If we wanted to draw a <b>line</b> of best fit, we could calculate the estimated grade for <b>a series</b> of time values and then connect them with a ruler. As we mentioned before, this <b>line</b> should cross the means of both the time spent on the essay and the mean grade received.", "dateLastCrawled": "2022-01-28T04:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Statistics review 7: Correlation and <b>regression</b>", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC374386/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC374386", "snippet": "Method of <b>least</b> <b>squares</b>. The <b>regression</b> <b>line</b> is obtained using the method of <b>least</b> <b>squares</b>. Any <b>line</b> y = a + bx that we draw <b>through</b> the points gives a predicted or fitted value of y for each value of x in the data set. For a particular value of x the vertical difference between the observed and fitted value of y is known as the deviation, or residual (Fig. (Fig.8). 8). The method of <b>least</b> <b>squares</b> finds the values of a and b that minimise the sum of the <b>squares</b> of all the deviations. This ...", "dateLastCrawled": "2022-02-02T06:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Least Squares</b> Linear <b>Regression</b> In Python | by Cory Maklin | Towards ...", "url": "https://towardsdatascience.com/least-squares-linear-regression-in-python-54b87fc49e77", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>least-squares</b>-<b>line</b>ar-<b>regression</b>-in-python-54b87fc49e77", "snippet": "As the name implies, the method of <b>Least Squares</b> minimizes the sum of the <b>squares</b> of the residuals between the observed targets in the dataset, and the targets predicted by the linear approximation. In this proceeding article, we\u2019ll see how we can go about finding the best fitting <b>line</b> using linear algebra as opposed to something <b>like</b> gradient descent.", "dateLastCrawled": "2022-02-02T08:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Common pitfalls in statistical analysis: Linear <b>regression</b> analysis", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5384397/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC5384397", "snippet": "Linear <b>regression</b> analysis of observations on two variables (x and y) in a sample can be looked upon as plotting the data and <b>drawing</b> a best fit <b>line</b> <b>through</b> these. This \u201cbest fit\u201d <b>line</b> is so chosen that the sum of <b>squares</b> of all the residuals (the vertical distance of each point from the <b>line</b>) is a minimum \u2013 the so-called \u201c<b>least</b> <b>squares</b> <b>line</b>\u201d [ Figure 1 ].", "dateLastCrawled": "2022-01-21T05:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Method of Least Squares</b> | Real Statistics Using Excel", "url": "https://www.real-statistics.com/regression/least-squares-method/", "isFamilyFriendly": true, "displayUrl": "https://www.real-statistics.com/<b>regression</b>/<b>least</b>-<b>squares</b>-method", "snippet": "In Correlation we study the linear correlation between two random variables x and y. We now look at the <b>line</b> in the xy plane that best fits the data (x 1, y 1), \u2026, (x n, y n).. Recall that the equation for <b>a straight</b> <b>line</b> is y = bx + a, where b = the slope of the <b>line</b> a = y-intercept, i.e. the value of y where the <b>line</b> intersects with the y-axis. For our purposes, we write the equation of the best fit <b>line</b> as", "dateLastCrawled": "2022-02-02T13:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Regression</b> Lines", "url": "http://www.aiqsystems.com/docs/Regression%20Lines.pdf", "isFamilyFriendly": true, "displayUrl": "www.aiqsystems.com/docs/<b>Regression</b> <b>Line</b>s.pdf", "snippet": "Simply put, a Linear <b>Regression</b> trendline is <b>a straight</b> <b>line</b> <b>through</b> <b>a series</b> of data values that is drawn to represent an average of all the values. The <b>least</b> <b>squares</b> method is used to determine the <b>line</b> that best fits the data. In this method, the objective is to minimize the distances between the resulting <b>line</b> and the data values. If you had to guess what a particular security\u2019s price would be tomorrow, a logical guess would be \u201cfairly close to today\u2019s price.\u201d If prices are ...", "dateLastCrawled": "2021-08-31T16:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Regression</b>: Finding the equation of the <b>line</b> of best fit", "url": "http://www.ams.sunysb.edu/~zhu/ams571/Regression.pdf", "isFamilyFriendly": true, "displayUrl": "www.ams.sunysb.edu/~zhu/ams571/<b>Regression</b>.pdf", "snippet": "Objectives: To find the equation of the <b>least</b> <b>squares</b> <b>regression</b> <b>line</b> of y on x. Background and general principle The aim of <b>regression</b> is to find the linear relationship between two variables. This is in turn translated into a mathematical problem of finding the equation of the <b>line</b> that is closest to all points observed. Consider the scatter plot on the right. One possible <b>line</b> of best fit has been drawn on the diagram. Some of the points lie above the <b>line</b> and some lie below it. The ...", "dateLastCrawled": "2022-02-02T13:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Chapter 8 <b>Regression</b> analysis (a refresher) | Crime Mapping in R", "url": "https://maczokni.github.io/crime_mapping_textbook/regression-analysis-a-refresher.html", "isFamilyFriendly": true, "displayUrl": "https://maczokni.github.io/crime_mapping_textbook/<b>regression</b>-analysis-a-refresher.html", "snippet": "Today we will cover linear <b>regression</b> or ordinary <b>least</b> <b>squares</b> <b>regression</b> (OLS), which is a technique that you use when you are interested in explaining variation in an interval level variable. First we will see how you can use <b>regression</b> analysis when you only have one input and then we will move to situations when we have several explanatory variables or inputs. For those of you already familiar with <b>regression</b> analysis this session can be a bit of a refresher, for those that aren\u2019t a ...", "dateLastCrawled": "2022-01-26T03:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Why would the standard deviation at a point in a linear <b>least</b> square ...", "url": "https://www.quora.com/Why-would-the-standard-deviation-at-a-point-in-a-linear-least-square-regression-model-be-larger-or-smaller-in-comparison-to-another-point-For-example-Sy-0-190-when-x-600-and-Sy-0-256-when-x-750-whereas-for-the", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Why-would-the-standard-deviation-at-a-point-in-a-<b>line</b>ar-<b>least</b>...", "snippet": "Answer (1 of 2): The question is not completely clear. If you mean the standard error of the estimate it is lowest in the middle of the data. It increases rapidly ...", "dateLastCrawled": "2022-01-20T22:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Drawing</b> <b>Regression</b> Lines in SPSS - The Ultimate Guide", "url": "https://www.spss-tutorials.com/how-to-draw-a-regression-line-in-spss/", "isFamilyFriendly": true, "displayUrl": "https://www.spss-tutorials.com/how-<b>to-draw-a-regression-line-in-spss</b>", "snippet": "This tutorial walks you <b>through</b> different options for <b>drawing</b> (non)linear <b>regression</b> lines for either all cases or subgroups. All examples use bank-clean.sav, partly shown below. Method A - Legacy Dialogs . A simple option for <b>drawing</b> linear <b>regression</b> lines is found under Graphs Legacy Dialogs Scatter/Dot as shown below. Completing these steps results in the SPSS syntax below. Running it creates a scatterplot to which we can easily add our <b>regression</b> <b>line</b> in the next step. *SCATTERPLOT FROM ...", "dateLastCrawled": "2022-01-31T10:14:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Statistics review 7: Correlation and <b>regression</b>", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC374386/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC374386", "snippet": "Method of <b>least</b> <b>squares</b>. The <b>regression</b> <b>line</b> is obtained using the method of <b>least</b> <b>squares</b>. Any <b>line</b> y = a + bx that we draw <b>through</b> the points gives a predicted or fitted value of y for each value of x in the data set. For a particular value of x the vertical difference between the observed and fitted value of y is known as the deviation, or residual (Fig. (Fig.8). 8). The method of <b>least</b> <b>squares</b> finds the values of a and b that minimise the sum of the <b>squares</b> of all the deviations. This ...", "dateLastCrawled": "2022-02-02T06:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Common pitfalls in statistical analysis: Linear <b>regression</b> analysis", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5384397/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC5384397", "snippet": "Linear <b>regression</b> analysis of observations on two variables (x and y) in a sample can be looked upon as plotting the data and <b>drawing</b> a best fit <b>line</b> <b>through</b> these. This \u201cbest fit\u201d <b>line</b> is so chosen that the sum of <b>squares</b> of all the residuals (the vertical distance of each point from the <b>line</b>) is a minimum \u2013 the so-called \u201c<b>least</b> <b>squares</b> <b>line</b>\u201d [ Figure 1 ].", "dateLastCrawled": "2022-01-21T05:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Least Squares</b> Linear <b>Regression</b> In Python | by Cory Maklin | Towards ...", "url": "https://towardsdatascience.com/least-squares-linear-regression-in-python-54b87fc49e77", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>least-squares</b>-<b>line</b>ar-<b>regression</b>-in-python-54b87fc49e77", "snippet": "As the name implies, the method of <b>Least Squares</b> minimizes the sum of the <b>squares</b> of the residuals between the observed targets in the dataset, and the targets predicted by the linear approximation. In this proceeding article, we\u2019ll see how we can go about finding the best fitting <b>line</b> using linear algebra as opposed to something like gradient descent.", "dateLastCrawled": "2022-02-02T08:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Method of Least Squares</b> | Real Statistics Using Excel", "url": "https://www.real-statistics.com/regression/least-squares-method/", "isFamilyFriendly": true, "displayUrl": "https://www.real-statistics.com/<b>regression</b>/<b>least</b>-<b>squares</b>-method", "snippet": "In Correlation we study the linear correlation between two random variables x and y. We now look at the <b>line</b> in the xy plane that best fits the data (x 1, y 1), \u2026, (x n, y n).. Recall that the equation for <b>a straight</b> <b>line</b> is y = bx + a, where b = the slope of the <b>line</b> a = y-intercept, i.e. the value of y where the <b>line</b> intersects with the y-axis. For our purposes, we write the equation of the best fit <b>line</b> as", "dateLastCrawled": "2022-02-02T13:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "An Introduction to <b>Linear Regression</b> | by James Andrew Godwin | Towards ...", "url": "https://towardsdatascience.com/an-introduction-to-linear-regression-9cbb64b52d23", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/an-introduction-to-<b>linear-regression</b>-9cbb64b52d23", "snippet": "Polynomial \u2014 when <b>a straight</b> <b>line</b> is not the best fit; Stepwise Ridge, and Lasso (common in ML) <b>Least</b>-<b>squares</b> <b>linear regression</b>. <b>Least</b>-<b>squares</b> <b>regression</b> is the type I will be expounding on today. Caution, most people will conflate this with <b>linear regression</b>. Now the two need not be the same, but just warning you that most people will conflate these two. <b>Linear regression</b> just means that you are going to do something using a linear collection of parameters. There are a variety of other ...", "dateLastCrawled": "2022-01-30T01:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Chapter 8 <b>Regression</b> analysis (a refresher) | Crime Mapping in R", "url": "https://maczokni.github.io/crime_mapping_textbook/regression-analysis-a-refresher.html", "isFamilyFriendly": true, "displayUrl": "https://maczokni.github.io/crime_mapping_textbook/<b>regression</b>-analysis-a-refresher.html", "snippet": "Simple linear <b>regression</b> draws a single <b>straight</b> <b>line</b> of predicted values as the model for the data. This <b>line</b> would be a model , a simplification of the real world like any other model (e.g., a toy pistol, an architectural <b>drawing</b>, a subway map), that assumes that there is approximately a linear relationship between X and Y.", "dateLastCrawled": "2022-01-26T03:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Why would the standard deviation at a point in a linear <b>least</b> square ...", "url": "https://www.quora.com/Why-would-the-standard-deviation-at-a-point-in-a-linear-least-square-regression-model-be-larger-or-smaller-in-comparison-to-another-point-For-example-Sy-0-190-when-x-600-and-Sy-0-256-when-x-750-whereas-for-the", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Why-would-the-standard-deviation-at-a-point-in-a-<b>line</b>ar-<b>least</b>...", "snippet": "Answer (1 of 2): The question is not completely clear. If you mean the standard error of the estimate it is lowest in the middle of the data. It increases rapidly ...", "dateLastCrawled": "2022-01-20T22:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Chart Lines and Objects", "url": "https://help.streetsmart.schwab.com/edge/1.6/Content/Chart%20Lines%20and%20Objects.htm", "isFamilyFriendly": true, "displayUrl": "https://help.streetsmart.schwab.com/edge/1.6/Content/Chart <b>Line</b>s and Objects.htm", "snippet": "<b>Regression</b> Trend <b>line</b>: A Linear <b>Regression</b> trend <b>line</b> uses the <b>least</b> <b>squares</b> method to plot <b>a straight</b> <b>line</b> <b>through</b> prices so as to minimize the distances between the prices and the resulting trend <b>line</b>. Segment <b>line</b>: <b>Similar</b> to a Trend <b>line</b> but does not extend to infinity. With your mouse pointer on the spot where you want to start the trend <b>line</b>, click and drag the other end of the <b>line</b> to the angle you want. To adjust the <b>line</b>, click on it so it becomes bold and grab one of the square ...", "dateLastCrawled": "2022-02-02T11:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Linear <b>regression</b> analysis in Excel - <b>Ablebits.com</b>", "url": "https://www.ablebits.com/office-addins-blog/2018/08/01/linear-regression-analysis-excel/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ablebits.com</b>/office-addins-blog/2018/08/01/<b>line</b>ar-<b>regression</b>-", "snippet": "If the dependent variable is modeled as a non-linear function because the data relationships do not follow <b>a straight</b> <b>line</b>, use nonlinear <b>regression</b> instead. The focus of this tutorial will be on a simple linear <b>regression</b>. As an example, let&#39;s take sales numbers for umbrellas for the last 24 months and find out the average monthly rainfall for the same period. Plot this information on a chart, and the <b>regression</b> <b>line</b> will demonstrate the relationship between the independent variable ...", "dateLastCrawled": "2022-02-03T00:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Why do I need 2 <b>regression</b> models? - Quora", "url": "https://www.quora.com/Why-do-I-need-2-regression-models", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Why-do-I-need-2-<b>regression</b>-models", "snippet": "Answer (1 of 2): In a published report or internal document, you will often see <b>a series</b> of models being reported, with increasing numbers of independent variables, transformations, interaction effects and so forth. These might be labelled Model 1, Model 2, Model 3, etc. The process of building ...", "dateLastCrawled": "2022-01-12T06:35:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Statistics review 7: Correlation and <b>regression</b>", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC374386/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC374386", "snippet": "Method of <b>least</b> <b>squares</b>. The <b>regression</b> <b>line</b> is obtained using the method of <b>least</b> <b>squares</b>. Any <b>line</b> y = a + bx that we draw <b>through</b> the points gives a predicted or fitted value of y for each value of x in the data set. For a particular value of x the vertical difference between the observed and fitted value of y is known as the deviation, or residual (Fig. (Fig.8). 8). The method of <b>least</b> <b>squares</b> finds the values of a and b that minimise the sum of the <b>squares</b> of all the deviations. This ...", "dateLastCrawled": "2022-02-02T06:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Least Squares</b> Linear <b>Regression</b> In Python | by Cory Maklin | Towards ...", "url": "https://towardsdatascience.com/least-squares-linear-regression-in-python-54b87fc49e77", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>least-squares</b>-<b>line</b>ar-<b>regression</b>-in-python-54b87fc49e77", "snippet": "As the name implies, the method of <b>Least Squares</b> minimizes the sum of the <b>squares</b> of the residuals between the observed targets in the dataset, and the targets predicted by the linear approximation. In this proceeding article, we\u2019ll see how we <b>can</b> go about finding the best fitting <b>line</b> using linear algebra as opposed to something like gradient descent.", "dateLastCrawled": "2022-02-02T08:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Method of Least Squares</b> | Real Statistics Using Excel", "url": "https://www.real-statistics.com/regression/least-squares-method/", "isFamilyFriendly": true, "displayUrl": "https://www.real-statistics.com/<b>regression</b>/<b>least</b>-<b>squares</b>-method", "snippet": "In Correlation we study the linear correlation between two random variables x and y. We now look at the <b>line</b> in the xy plane that best fits the data (x 1, y 1), \u2026, (x n, y n).. Recall that the equation for <b>a straight</b> <b>line</b> is y = bx + a, where b = the slope of the <b>line</b> a = y-intercept, i.e. the value of y where the <b>line</b> intersects with the y-axis. For our purposes, we write the equation of the best fit <b>line</b> as", "dateLastCrawled": "2022-02-02T13:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Line</b> of Best Fit (<b>Least Square Method</b>) - Varsity Tutors", "url": "https://www.varsitytutors.com/hotmath/hotmath_help/topics/line-of-best-fit", "isFamilyFriendly": true, "displayUrl": "https://www.varsitytutors.com/hotmath/hotmath_help/topics/<b>line</b>-of-best-fit", "snippet": "A <b>line</b> of best fit is <b>a straight</b> <b>line</b> that is the best approximation of the given set of data. It is used to study the nature of the relation between two variables. (We&#39;re only considering the two-dimensional case, here.) A <b>line</b> of best fit <b>can</b> be roughly determined using an eyeball method by <b>drawing</b> <b>a straight</b> <b>line</b> on a scatter plot so that the number of points above the <b>line</b> and below the <b>line</b> is about equal (and the <b>line</b> passes <b>through</b> as many points as possible). A more accurate way of ...", "dateLastCrawled": "2022-02-03T06:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Chapter 8 <b>Regression</b> analysis (a refresher) | Crime Mapping in R", "url": "https://maczokni.github.io/crime_mapping_textbook/regression-analysis-a-refresher.html", "isFamilyFriendly": true, "displayUrl": "https://maczokni.github.io/crime_mapping_textbook/<b>regression</b>-analysis-a-refresher.html", "snippet": "Today we will cover linear <b>regression</b> or ordinary <b>least</b> <b>squares</b> <b>regression</b> (OLS), which is a technique that you use when you are interested in explaining variation in an interval level variable. First we will see how you <b>can</b> use <b>regression</b> analysis when you only have one input and then we will move to situations when we have several explanatory variables or inputs. For those of you already familiar with <b>regression</b> analysis this session <b>can</b> be a bit of a refresher, for those that aren\u2019t a ...", "dateLastCrawled": "2022-01-26T03:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Chapter 8 An introduction to <b>regression</b> | Modelling Criminological Data ...", "url": "https://jjmedinaariza.github.io/modelling_book/an-introduction-to-regression.html", "isFamilyFriendly": true, "displayUrl": "https://jjmedinaariza.github.io/modelling_book/an-introduction-to-<b>regression</b>.html", "snippet": "In real applications, we have access to a set of observations from which we <b>can</b> compute the <b>least</b> <b>squares</b> <b>line</b>, but the population <b>regression</b> <b>line</b> is unobserved. So our <b>regression</b> <b>line</b> is one of many that could be estimated. A different sample would produce a different <b>regression</b> <b>line</b>. The same sort of ideas that we introduced when discussing the estimation of sample means or proportions also apply here.", "dateLastCrawled": "2022-01-31T07:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>regression</b> - <b>Line of best fit</b> does not look like a good fit. Why ...", "url": "https://stats.stackexchange.com/questions/332819/line-of-best-fit-does-not-look-like-a-good-fit-why", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/332819", "snippet": "Your confuse ordinary <b>least</b> <b>squares</b> (OLS) <b>regression</b> (which minimizes the sum of the squared deviation about the predicted values, (observed-predicted)^2) and major axis <b>regression</b> (which minimizes the sums of <b>squares</b> of the perpendicular distance between each point and the <b>regression</b> <b>line</b>, sometimes this is referred to as Type II <b>regression</b>, orthogonal <b>regression</b> or standardized principal component <b>regression</b>).", "dateLastCrawled": "2022-01-23T08:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Linear Regression in Python</b> using numpy + polyfit (with code base)", "url": "https://data36.com/linear-regression-in-python-numpy-polyfit/", "isFamilyFriendly": true, "displayUrl": "https://data36.com/<b>linear-regression-in-python</b>-numpy-polyfit", "snippet": "If you put all the x\u2013y value pairs on a graph, you\u2019ll get <b>a straight</b> <b>line</b>:. The relationship between x and y is linear.. Using the equation of this specific <b>line</b> (y = 2 * x + 5), if you change x by 1, y will always change by 2.And it doesn\u2019t matter what a and b values you use, your graph will always show the same characteristics: it will always be <b>a straight</b> <b>line</b>, only its position and slope change. It also means that x and y will always be in linear relationship.. In the linear ...", "dateLastCrawled": "2022-02-02T22:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Psychology research</b> Flashcards | Quizlet", "url": "https://quizlet.com/1467821/psychology-research-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/1467821/<b>psychology-research</b>-flash-cards", "snippet": "<b>least</b> <b>squares</b> method. a method of calculating the <b>line</b> of best fit using the distance each point is from the <b>line</b> of best fit . Pearson product-moment correlation coefficient. a measure of how well the <b>regression</b> equation fits the data. r. the correlation coefficient that varies from 0 to +/- 1. <b>regression</b> equation. the equation found to represent a set of data. causation. when one event causes a second event. necessary condition. a correlation needed for causation. sufficient condition. a ...", "dateLastCrawled": "2022-01-28T06:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Constructing a <b>best fit</b> <b>line</b>", "url": "https://serc.carleton.edu/mathyouneed/graphing/bestfit.html", "isFamilyFriendly": true, "displayUrl": "https://serc.carleton.edu/mathyouneed/graphing/<b>bestfit</b>.html", "snippet": "If you find yourself faced with a question that asks you to draw a trend <b>line</b>, linear <b>regression</b> or <b>best-fit</b> <b>line</b>, you are most certainly being asked to draw a <b>line</b> <b>through</b> data points on a scatter plot. You may also be asked to approximate the trend, or sketch in a <b>line</b> that mimics the data. This page is designed to help you complete any of these types of questions. Work <b>through</b> it and the sample problems if you are unsure of how to complete questions about trends and <b>best-fit</b> lines.", "dateLastCrawled": "2022-02-02T16:03:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Statistics review 7: Correlation and <b>regression</b>", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC374386/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC374386", "snippet": "Method of <b>least</b> <b>squares</b>. The <b>regression</b> <b>line</b> is obtained using the method of <b>least</b> <b>squares</b>. Any <b>line</b> y = a + bx that we draw <b>through</b> the points gives a predicted or fitted value of y for each value of x in the data set. For a particular value of x the vertical difference between the observed and fitted value of y is known as the deviation, or residual (Fig. (Fig.8). 8). The method of <b>least</b> <b>squares</b> finds the values of a and b that minimise the sum of the <b>squares</b> of all the deviations. This ...", "dateLastCrawled": "2022-02-02T06:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Why would the standard deviation at a point in a linear <b>least</b> square ...", "url": "https://www.quora.com/Why-would-the-standard-deviation-at-a-point-in-a-linear-least-square-regression-model-be-larger-or-smaller-in-comparison-to-another-point-For-example-Sy-0-190-when-x-600-and-Sy-0-256-when-x-750-whereas-for-the", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Why-would-the-standard-deviation-at-a-point-in-a-<b>line</b>ar-<b>least</b>...", "snippet": "Answer (1 of 2): The question is not completely clear. If you mean the standard error of the estimate it is lowest in the middle of the data. It increases rapidly ...", "dateLastCrawled": "2022-01-20T22:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Line</b> of Best Fit (<b>Least Square Method</b>) - Varsity Tutors", "url": "https://www.varsitytutors.com/hotmath/hotmath_help/topics/line-of-best-fit", "isFamilyFriendly": true, "displayUrl": "https://www.varsitytutors.com/hotmath/hotmath_help/topics/<b>line</b>-of-best-fit", "snippet": "A <b>line</b> of best fit is <b>a straight</b> <b>line</b> that is the best approximation of the given set of data. It is used to study the nature of the relation between two variables. (We&#39;re only considering the two-dimensional case, here.) A <b>line</b> of best fit <b>can</b> be roughly determined using an eyeball method by <b>drawing</b> <b>a straight</b> <b>line</b> on a scatter plot so that the number of points above the <b>line</b> and below the <b>line</b> is about equal (and the <b>line</b> passes <b>through</b> as many points as possible). A more accurate way of ...", "dateLastCrawled": "2022-02-03T06:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Correlation and linear <b>regression</b> - <b>Handbook of Biological Statistics</b>", "url": "http://www.biostathandbook.com/linearregression.html", "isFamilyFriendly": true, "displayUrl": "www.biostathandbook.com/<b>line</b>ar<b>regression</b>.html", "snippet": "The <b>least</b>-<b>squares</b> <b>regression</b> <b>line</b> does depend on which variable is the X and which is the Y; the two lines <b>can</b> be quite different if the r 2 is low. If you&#39;re truly interested only in whether the two variables covary, and you are not trying to infer a cause-and-effect relationship, you may want to avoid using the linear <b>regression</b> <b>line</b> as decoration on your graph.", "dateLastCrawled": "2022-02-01T17:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Scatter Plot: Is there a relationship between two variables?", "url": "https://www.skymark.com/resources/tools/scatter_plots.asp", "isFamilyFriendly": true, "displayUrl": "https://www.skymark.com/resources/tools/<b>scatter_plots</b>.asp", "snippet": "<b>Scatter Plots</b> (also called scatter diagrams) are used to investigate the possible relationship between two variables that both relate to the same &quot;event.&quot; <b>A straight</b> <b>line</b> of best fit (using the <b>least</b> <b>squares</b> method) is often included. If the points cluster in a band running from lower left to upper right, there is a positive correlation (if x ...", "dateLastCrawled": "2022-02-02T12:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Chapter 8 An introduction to <b>regression</b> | Modelling Criminological Data ...", "url": "https://jjmedinaariza.github.io/modelling_book/an-introduction-to-regression.html", "isFamilyFriendly": true, "displayUrl": "https://jjmedinaariza.github.io/modelling_book/an-introduction-to-<b>regression</b>.html", "snippet": "In real applications, we have access to a set of observations from which we <b>can</b> compute the <b>least</b> <b>squares</b> <b>line</b>, but the population <b>regression</b> <b>line</b> is unobserved. So our <b>regression</b> <b>line</b> is one of many that could be estimated. A different sample would produce a different <b>regression</b> <b>line</b>. The same sort of ideas that we introduced when discussing the estimation of sample means or proportions also apply here.", "dateLastCrawled": "2022-01-31T07:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Linear <b>regression</b> analysis in Excel - <b>Ablebits.com</b>", "url": "https://www.ablebits.com/office-addins-blog/2018/08/01/linear-regression-analysis-excel/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ablebits.com</b>/office-addins-blog/2018/08/01/<b>line</b>ar-<b>regression</b>-", "snippet": "SS is the sum of <b>squares</b>. The smaller the Residual SS <b>compared</b> with the Total SS, the better your model fits the data. MS is the mean square. F is the F statistic, or F-test for the null hypothesis. It is used to test the overall significance of the model. Significance F is the P-value of F. The ANOVA part is rarely used for a simple linear <b>regression</b> analysis in Excel, but you should definitely have a close look at the last component. The Significance F value gives an idea of how reliable ...", "dateLastCrawled": "2022-02-03T00:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Why do I need 2 <b>regression</b> models? - Quora", "url": "https://www.quora.com/Why-do-I-need-2-regression-models", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Why-do-I-need-2-<b>regression</b>-models", "snippet": "Answer (1 of 2): In a published report or internal document, you will often see <b>a series</b> of models being reported, with increasing numbers of independent variables, transformations, interaction effects and so forth. These might be labelled Model 1, Model 2, Model 3, etc. The process of building ...", "dateLastCrawled": "2022-01-12T06:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Scatter Diagram</b>: Explanation, Types with Examples and Plots", "url": "https://www.toppr.com/guides/business-mathematics-and-statistics/correlation-and-regression/scatter-diagram/", "isFamilyFriendly": true, "displayUrl": "https://www.toppr.com/.../correlation-and-<b>regression</b>/<b>scatter-diagram</b>", "snippet": "Interpretation of Scatter Diagrams. The Scatter Diagrams between two random variables feature the variables as their x and y-axes. We <b>can</b> take any variable as the independent variable in such a case (the other variable being the dependent one), and correspondingly plot every data point on the graph (x i,y i).The totality of all the plotted points forms the <b>scatter diagram</b>.", "dateLastCrawled": "2022-01-30T06:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "QM - <b>Module 4 - Simple &amp; Multiple Regression</b> Flashcards | <b>Quizlet</b>", "url": "https://quizlet.com/7685473/qm-module-4-simple-multiple-regression-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/7685473/qm-<b>module-4-simple-multiple-regression</b>-flash-cards", "snippet": "R2 = (sum of <b>squares</b> for <b>regression</b>/sum of <b>squares</b> total) = SSR/SST = explained variation/variation in y. R2 <b>can</b> take values between 0 and 1. If R2=0 the model doesn&#39;t have any explanatory power, whereas R2=1 means that all \u03b5i are 0. R2 <b>can</b> only be negative if there is no constant in the model. It increases with adding more regressors (= drawback, because it also increases even if additional variables have no explanatory power --&gt; using adjusted R2). Example: Hedge-Fund replications ...", "dateLastCrawled": "2020-09-29T21:35:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "CS 189/289A: Introduction to <b>Machine</b> <b>Learning</b>", "url": "https://people.eecs.berkeley.edu/~jrs/189s21/", "isFamilyFriendly": true, "displayUrl": "https://people.eecs.berkeley.edu/~jrs/189s21", "snippet": "LDA vs. logistic <b>regression</b>: advantages and disadvantages. ROC curves. Weighted <b>least</b>-<b>squares</b> <b>regression</b>. <b>Least</b>-<b>squares</b> polynomial <b>regression</b>. Read ISL, Sections 4.4.3, 7.1, 9.3.3; ESL, Section 4.4.1. Optional: here is a fine short discussion of ROC curves\u2014but skip the incoherent question at the top and jump straight to the answer.", "dateLastCrawled": "2022-01-31T03:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>CS 189/289A</b>: Introduction to <b>Machine</b> <b>Learning</b>", "url": "https://people.eecs.berkeley.edu/~jrs/189/", "isFamilyFriendly": true, "displayUrl": "https://people.eecs.berkeley.edu/~jrs/189", "snippet": "<b>regression</b>: <b>least</b>-<b>squares</b> linear <b>regression</b>, logistic <b>regression</b>, polynomial <b>regression</b>, ridge <b>regression</b>, Lasso; density estimation: maximum likelihood estimation (MLE); dimensionality reduction: principal components analysis (PCA), random projection; and clustering: k-means clustering, hierarchical clustering, spectral graph clustering. Useful Links. Access the <b>CS 189/289A</b> Piazza discussion group. If you want an instructional account, you can get one online. Go to the same link if you ...", "dateLastCrawled": "2022-02-02T17:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "A difficult <b>regression</b> parameter estimation problem is posed when the data sample is hypothesized to have been generated by more than a single <b>regression</b> model. To find the best-fitting number and ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "LSEbA: <b>least squares regression and estimation by analogy</b> in a semi ...", "url": "https://link.springer.com/article/10.1007/s10664-010-9128-6", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10664-010-9128-6", "snippet": "In this study, we indicatively applied the ordinary <b>least</b> <b>squares</b> <b>regression</b> and the estimation by <b>analogy</b> technique for the computation of the parametric and non-parametric part, respectively. However, there are lots of other well-known methods that can substitute the abovementioned methods and can be used for evaluation of these components. For example, practitioners may use a robust <b>regression</b> in the computation of the parametric portion of the proposed model in order to have a model less ...", "dateLastCrawled": "2021-12-03T23:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine Learning by Analogy</b> - SlideShare", "url": "https://www.slideshare.net/ColleenFarrelly/machine-learning-by-analogy-59094152", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ColleenFarrelly/<b>machine-learning-by-analogy</b>-59094152", "snippet": "<b>Machine Learning by Analogy</b> 1. Colleen M. Farrelly 2. Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics behind these ...", "dateLastCrawled": "2022-01-31T07:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A Big Problem with Linear <b>Regression</b> and How to Solve It | Towards Data ...", "url": "https://towardsdatascience.com/robust-regression-23b633e5d6a5", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/robust-<b>regression</b>-23b633e5d6a5", "snippet": "Introduction to Robust <b>Regression</b> in <b>Machine</b> <b>Learning</b>. Hussein Abdulrahman . Just now \u00b7 7 min read. The idea behind classic linear <b>regression</b> is simple: draw a \u201cbest-fit\u201d line across the data points that minimizes the mean squared errors: Classic linear <b>regression</b> with ordinary <b>least</b> <b>squares</b>. (Image by author) Looks good. But we don\u2019t always get such clean, well behaved data in real life. Instead, we may get something like this: Same algorithm as above, but now performing poorly due ...", "dateLastCrawled": "2022-02-01T17:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PPT) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> | Colleen Farrelly - Academia.edu", "url": "https://www.academia.edu/30404581/Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/30404581/<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "<b>Machine</b> <b>Learning</b> by <b>Analogy</b> Colleen M. Farrelly Overview of Problem Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics ...", "dateLastCrawled": "2022-01-02T06:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Introduction to Machine Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/24/introduction-to-machine-learning-2/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/24/<b>introduction-to-machine-learning</b>-2", "snippet": "<b>Machine</b> <b>learning</b> terminology for model building and validation There seems to be an <b>analogy</b> between statistical modeling and <b>machine</b> <b>learning</b> that we will cover in subsequent chapters in depth. However, a quick view has been provided as follows: in statistical modeling, linear <b>regression</b> with two independent variables is trying to fit the best plane with\u2026", "dateLastCrawled": "2022-01-17T05:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Trends <b>in artificial intelligence, machine learning, and chemometrics</b> ...", "url": "https://chemistry-europe.onlinelibrary.wiley.com/doi/10.1002/ansa.202000162", "isFamilyFriendly": true, "displayUrl": "https://chemistry-europe.onlinelibrary.wiley.com/doi/10.1002/ansa.202000162", "snippet": "The derived spectra were analyzed for classification and quantification purposes using soft independent modeling of class <b>analogy</b> (SIMCA), artificial neural network (ANN), and partial <b>least</b> <b>squares</b> <b>regression</b> (PLSR). A good classification of tomatoes based on their carotenoid profile of 93% and 100% is shown using SIMCA and ANN, respectively. Besides this result, PLSR and ANN were able to achieve a good quantification of all-", "dateLastCrawled": "2022-02-01T19:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "econometrics - Principle of <b>Analogy</b> and Method of Moments - Cross Validated", "url": "https://stats.stackexchange.com/questions/272803/principle-of-analogy-and-method-of-moments", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/272803/principle-of-<b>analogy</b>-and-method-of...", "snippet": "<b>Least</b> <b>squares</b> estimator in the classical linear <b>regression</b> model is a Method of Moments estimator. The model is. y = X \u03b2 + u. Instead of minimizing the sum of squared residuals, we can obtain the OLS estimator by noting that under the assumptions of the specific model, it holds that (&quot;orhtogonality condition&quot;) E ( X \u2032 u) = 0.", "dateLastCrawled": "2022-01-25T20:40:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Bayesian <b>Learning</b> - Rebellion Research", "url": "https://www.rebellionresearch.com/bayesian-learning", "isFamilyFriendly": true, "displayUrl": "https://www.rebellionresearch.com/bayesian-<b>learning</b>", "snippet": "Linear Regression example of <b>machine learning Least Squares Regression can be thought of as</b> a very limited <b>learning</b> algorithm, where the training set consists of a number of x and y data pairs. The task would be trying to predict the y value, and the performance measure would be the sum of the squared differences between the predicted and actual y\u2019s.", "dateLastCrawled": "2022-01-19T02:15:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(least squares regression)  is like +(drawing a straight line through a series of dots)", "+(least squares regression) is similar to +(drawing a straight line through a series of dots)", "+(least squares regression) can be thought of as +(drawing a straight line through a series of dots)", "+(least squares regression) can be compared to +(drawing a straight line through a series of dots)", "machine learning +(least squares regression AND analogy)", "machine learning +(\"least squares regression is like\")", "machine learning +(\"least squares regression is similar\")", "machine learning +(\"just as least squares regression\")", "machine learning +(\"least squares regression can be thought of as\")", "machine learning +(\"least squares regression can be compared to\")"]}