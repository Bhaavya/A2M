{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine Learning</b> Glossary | <b>Google Developers</b>", "url": "https://developers.google.com/machine-learning/glossary/", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine-learning</b>/glossary", "snippet": "<b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) Choosing the function that minimizes loss on the training set. Contrast with structural <b>risk</b> <b>minimization</b>. encoder. #language . In general, any ML system that converts from a raw, sparse, or external representation into a more processed, denser, or more internal representation. Encoders are often a component of a larger model, where they are frequently paired with a decoder. Some Transformers pair encoders with decoders, though other Transformers use only ...", "dateLastCrawled": "2022-02-03T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Paper Digest: <b>NIPS 2015 Highlights</b> \u2013 Paper Digest", "url": "https://www.paperdigest.org/2015/12/nips-2015-highlights/", "isFamilyFriendly": true, "displayUrl": "https://www.paperdigest.org/2015/12/<b>nips-2015-highlights</b>", "snippet": "We consider <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) in the context of stochastic optimization with exp-concave and smooth losses\u2014a general optimization framework that captures several important <b>learning</b> problems including linear and logistic regression, <b>learning</b> SVMs with the squared hinge-loss, portfolio selection and more. 166: Deep Generative Image Models using a ?Laplacian Pyramid of Adversarial Networks: Emily L. Denton, Soumith Chintala, arthur szlam, Rob Fergus: In this paper we introduce ...", "dateLastCrawled": "2021-12-23T02:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Simulation of Learners&#39; Behaviors Based</b> on the Modified Cellular ...", "url": "https://www.researchgate.net/publication/220518156_Simulation_of_Learners'_Behaviors_Based_on_the_Modified_Cellular_Automata_Model", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/220518156_Simulation_of_Learners", "snippet": "A milestone in <b>learning</b> theory was a characterization of conditions on the hypothesis space that ensure generalization for the natural class of <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) <b>learning</b> ...", "dateLastCrawled": "2022-01-20T03:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>ICML 2018 Abstracts</b> \u00b7 GitHub", "url": "https://gist.github.com/cwhy/2db49912f1d22495d2788ea89e933045", "isFamilyFriendly": true, "displayUrl": "https://gist.github.com/cwhy/2db49912f1d22495d2788ea89e933045", "snippet": "In this paper, we derive bounds on the mutual information of the <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) procedure for both 0-1 and strongly-convex loss classes. We prove that under the Axiom of Choice, the existence of an <b>ERM</b> <b>learning</b> rule with a vanishing mutual information is equivalent to the assertion that the loss class has a finite VC dimension, thus bridging information theory with statistical <b>learning</b> theory. Similarly, an asymptotic bound on the mutual information is established for ...", "dateLastCrawled": "2022-01-19T20:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>NIPS 2018 Abstract</b> \u00b7 GitHub", "url": "https://gist.github.com/cwhy/3d9fdd54a75a6f698c929f691e9d4d83", "isFamilyFriendly": true, "displayUrl": "https://<b>gist</b>.github.com/cwhy/3d9fdd54a75a6f698c929f691e9d4d83", "snippet": "As opposed to standard <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>), distributionally robust optimization aims to minimize the worst-case <b>risk</b> over a larger ambiguity set containing the original <b>empirical</b> distribution of the training data. In this work, we describe a minimax framework for statistical <b>learning</b> with ambiguity sets given by balls in Wasserstein space. In particular, we prove a generalization bound that involves the covering number properties of the original <b>ERM</b> problem. As an illustrative ...", "dateLastCrawled": "2022-01-02T07:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "comp.ai.neural-nets FAQ, Part 4 of 7: Books, data, etc.", "url": "http://www.faqs.org/faqs/ai-faq/neural-nets/part4/index.html", "isFamilyFriendly": true, "displayUrl": "www.faqs.org/faqs/ai-faq/neural-nets/part4/index.html", "snippet": "Conditions for Consistency of <b>Empirical</b> <b>Risk</b> <b>Minimization</b> Principle; 4. Bounds on the <b>Risk</b> for Indicator Loss Functions; Appendix: Lower Bounds on the <b>Risk</b> of the <b>ERM</b> Principle; 5. Bounds on the <b>Risk</b> for Real-Valued Loss Functions; 6. The Structural <b>Risk</b> <b>Minimization</b> Principle; Appendix: Estimating Functions on the Basis of Indirect ...", "dateLastCrawled": "2021-10-15T05:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>ICML2020</b>/<b>icml2020</b>_1.md at master \u00b7 <b>haozhangcn/ICML2020</b> \u00b7 <b>GitHub</b>", "url": "https://github.com/haozhangcn/ICML2020/blob/master/icml2020_1.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/haozhangcn/<b>ICML2020</b>/blob/master/<b>icml2020</b>_1.md", "snippet": "27.Training Binary Neural Networks <b>through</b> <b>Learning</b> with Noisy Supervision ... We show that in the PAC model for passive <b>learning</b>, any {\\em <b>empirical</b> <b>risk</b> minimizer} has a sample complexity that is optimal up to a factor of $\\widetilde{O}(k)$. 44.Rigging the Lottery: Making All Tickets Winners \ud83d\udd17. Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, Erich Elsen Metadata Supplemental. Compared to dense networks, sparse neural networks are shown to be more parameter efficient, more ...", "dateLastCrawled": "2022-01-29T11:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Paper Digest: <b>ICML 2019 Highlights</b> \u2013 Paper Digest", "url": "https://www.paperdigest.org/2019/05/icml-2019-highlights/", "isFamilyFriendly": true, "displayUrl": "https://www.paperdigest.org/2019/05/<b>icml-2019-highlights</b>", "snippet": "Here, we characterize this trade-off for an <b>empirical</b> <b>risk</b> <b>minimization</b> setting, showing that in general there is a \u201csweet spot\u201d that depends on measurable properties of the dataset, but that there is also a concrete cost to privacy that cannot be avoided simply by collecting more data. 28: Explaining Deep Neural Networks with a Polynomial Time Algorithm for Shapley Value Approximation: Marco Ancona, Cengiz Oztireli, Markus Gross: In this work, by leveraging recent results on uncertainty ...", "dateLastCrawled": "2022-01-22T07:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Track: Poster Session 4", "url": "https://nips.cc/virtual/2020/session/21245", "isFamilyFriendly": true, "displayUrl": "https://nips.cc/virtual/2020/session/21245", "snippet": "Recently, different machine <b>learning</b> methods have been introduced to tackle the challenging few-shot <b>learning</b> scenario that is, <b>learning</b> from a small labeled dataset related to a specific task. Common approaches have taken the form of meta-<b>learning</b>: <b>learning</b> to learn on the new problem given the old. Following the recognition that meta-<b>learning</b> is implementing <b>learning</b> in a multi-level model, we present a Bayesian treatment for the meta-<b>learning</b> inner loop <b>through</b> the use of deep kernels. As ...", "dateLastCrawled": "2021-11-20T12:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(PDF) <b>Doctoral Study - DBA</b> | Dr. Robert E Davis - Academia.edu", "url": "https://www.academia.edu/38331489/Doctoral_Study_DBA", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/38331489", "snippet": "Walden University College of Management and Technology This is to certify that the doctoral study by Robert Davis has been found to be complete and satisfactory in all respects, and that any and all revisions required by the review committee have been made. Review Committee Dr. Alexandre Lazo, Committee Chairperson, Doctor of Business ...", "dateLastCrawled": "2021-09-19T03:54:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine Learning</b> Glossary | <b>Google Developers</b>", "url": "https://developers.google.com/machine-learning/glossary/", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine-learning</b>/glossary", "snippet": "<b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) Choosing the function that minimizes loss on the training set. Contrast with structural <b>risk</b> <b>minimization</b>. encoder. #language . In general, any ML system that converts from a raw, sparse, or external representation into a more processed, denser, or more internal representation. Encoders are often a component of a larger model, where they are frequently paired with a decoder. Some Transformers pair encoders with decoders, though other Transformers use only ...", "dateLastCrawled": "2022-02-03T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Paper Digest: <b>NIPS 2015 Highlights</b> \u2013 Paper Digest", "url": "https://www.paperdigest.org/2015/12/nips-2015-highlights/", "isFamilyFriendly": true, "displayUrl": "https://www.paperdigest.org/2015/12/<b>nips-2015-highlights</b>", "snippet": "We consider <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) in the context of stochastic optimization with exp-concave and smooth losses\u2014a general optimization framework that captures several important <b>learning</b> problems including linear and logistic regression, <b>learning</b> SVMs with the squared hinge-loss, portfolio selection and more. 166: Deep Generative Image Models using a ?Laplacian Pyramid of Adversarial Networks: Emily L. Denton, Soumith Chintala, arthur szlam, Rob Fergus: In this paper we introduce ...", "dateLastCrawled": "2021-12-23T02:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>Penalty Functions for Genetic Programming Algorithms</b>", "url": "https://www.researchgate.net/publication/221435193_Penalty_Functions_for_Genetic_Programming_Algorithms", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221435193_Penalty_Functions_for_Genetic...", "snippet": "Two statistical methods are compared: model selection based on <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) and model selection based on Structural <b>Risk</b> <b>Minimization</b> (SRM). For this purpose we have ...", "dateLastCrawled": "2021-11-11T08:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>ICML 2018 Abstracts</b> \u00b7 GitHub", "url": "https://gist.github.com/cwhy/2db49912f1d22495d2788ea89e933045", "isFamilyFriendly": true, "displayUrl": "https://gist.github.com/cwhy/2db49912f1d22495d2788ea89e933045", "snippet": "In this paper, we derive bounds on the mutual information of the <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) procedure for both 0-1 and strongly-convex loss classes. We prove that under the Axiom of Choice, the existence of an <b>ERM</b> <b>learning</b> rule with a vanishing mutual information is equivalent to the assertion that the loss class has a finite VC dimension, thus bridging information theory with statistical <b>learning</b> theory. Similarly, an asymptotic bound on the mutual information is established for ...", "dateLastCrawled": "2022-01-19T20:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>NIPS 2018 Abstract</b> \u00b7 GitHub", "url": "https://gist.github.com/cwhy/3d9fdd54a75a6f698c929f691e9d4d83", "isFamilyFriendly": true, "displayUrl": "https://<b>gist</b>.github.com/cwhy/3d9fdd54a75a6f698c929f691e9d4d83", "snippet": "As opposed to standard <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>), distributionally robust optimization aims to minimize the worst-case <b>risk</b> over a larger ambiguity set containing the original <b>empirical</b> distribution of the training data. In this work, we describe a minimax framework for statistical <b>learning</b> with ambiguity sets given by balls in Wasserstein space. In particular, we prove a generalization bound that involves the covering number properties of the original <b>ERM</b> problem. As an illustrative ...", "dateLastCrawled": "2022-01-02T07:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "comp.ai.neural-nets FAQ, Part 4 of 7: Books, data, etc.", "url": "http://www.faqs.org/faqs/ai-faq/neural-nets/part4/index.html", "isFamilyFriendly": true, "displayUrl": "www.faqs.org/faqs/ai-faq/neural-nets/part4/index.html", "snippet": "Conditions for Consistency of <b>Empirical</b> <b>Risk</b> <b>Minimization</b> Principle; 4. Bounds on the <b>Risk</b> for Indicator Loss Functions; Appendix: Lower Bounds on the <b>Risk</b> of the <b>ERM</b> Principle; 5. Bounds on the <b>Risk</b> for Real-Valued Loss Functions; 6. The Structural <b>Risk</b> <b>Minimization</b> Principle; Appendix: Estimating Functions on the Basis of Indirect ...", "dateLastCrawled": "2021-10-15T05:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>ICML2020</b>/<b>icml2020</b>_1.md at master \u00b7 <b>haozhangcn/ICML2020</b> \u00b7 <b>GitHub</b>", "url": "https://github.com/haozhangcn/ICML2020/blob/master/icml2020_1.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/haozhangcn/<b>ICML2020</b>/blob/master/<b>icml2020</b>_1.md", "snippet": "We show that in the PAC model for passive <b>learning</b>, any {\\em <b>empirical</b> <b>risk</b> minimizer} has a sample complexity that is optimal up to a factor of $\\widetilde{O}(k)$. 44.Rigging the Lottery: Making All Tickets Winners \ud83d\udd17. Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, Erich Elsen Metadata Supplemental. Compared to dense networks, sparse neural networks are shown to be more parameter efficient, more compute efficient and have been used to decrease wall clock inference times. There ...", "dateLastCrawled": "2022-01-29T11:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Paper Digest: <b>ICML 2019 Highlights</b> \u2013 Paper Digest", "url": "https://www.paperdigest.org/2019/05/icml-2019-highlights/", "isFamilyFriendly": true, "displayUrl": "https://www.paperdigest.org/2019/05/<b>icml-2019-highlights</b>", "snippet": "Here, we characterize this trade-off for an <b>empirical</b> <b>risk</b> <b>minimization</b> setting, showing that in general there is a \u201csweet spot\u201d that depends on measurable properties of the dataset, but that there is also a concrete cost to privacy that cannot be avoided simply by collecting more data. 28: Explaining Deep Neural Networks with a Polynomial Time Algorithm for Shapley Value Approximation: Marco Ancona, Cengiz Oztireli, Markus Gross: In this work, by leveraging recent results on uncertainty ...", "dateLastCrawled": "2022-01-22T07:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) <b>Human Resource Management Theory and Practice 9780805838626</b> | \u8bd7\u6768 ...", "url": "https://www.academia.edu/8674567/Human_Resource_Management_Theory_and_Practice_9780805838626", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/8674567/<b>Human_Resource_Management_Theory_and_Practice</b>...", "snippet": "<b>Human Resource Management Theory and Practice 9780805838626</b>", "dateLastCrawled": "2022-02-03T04:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "NIPS <b>2018 Videos</b>", "url": "https://nips.cc/Conferences/2018/Videos", "isFamilyFriendly": true, "displayUrl": "https://nips.cc/Conferences/<b>2018/Videos</b>", "snippet": "The success of machine <b>learning</b> crucially relies on human machine <b>learning</b> experts, who construct appropriate features and workflows, and select appropriate machine <b>learning</b> paradigms, algorithms, neural architectures, and their hyperparameters. Automatic machine <b>learning</b> (AutoML) is an emerging research area that targets the progressive automation of machine <b>learning</b>, which uses machine <b>learning</b> and optimization to develop off-the-shelf machine <b>learning</b> methods that can be used easily and ...", "dateLastCrawled": "2022-01-26T03:48:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Paper Digest: <b>NIPS 2015 Highlights</b> \u2013 Paper Digest", "url": "https://www.paperdigest.org/2015/12/nips-2015-highlights/", "isFamilyFriendly": true, "displayUrl": "https://www.paperdigest.org/2015/12/<b>nips-2015-highlights</b>", "snippet": "We consider <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) in the context of stochastic optimization with exp-concave and smooth losses\u2014a general optimization framework that captures several important <b>learning</b> problems including linear and logistic regression, <b>learning</b> SVMs with the squared hinge-loss, portfolio selection and more. 166", "dateLastCrawled": "2021-12-23T02:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "comp.ai.neural-nets FAQ, Part 4 of 7: Books, data, etc.", "url": "http://www.faqs.org/faqs/ai-faq/neural-nets/part4/index.html", "isFamilyFriendly": true, "displayUrl": "www.faqs.org/faqs/ai-faq/neural-nets/part4/index.html", "snippet": "Conditions for Consistency of <b>Empirical</b> <b>Risk</b> <b>Minimization</b> Principle; 4. Bounds on the <b>Risk</b> for Indicator Loss Functions; Appendix: Lower Bounds on the <b>Risk</b> of the <b>ERM</b> Principle; 5. Bounds on the <b>Risk</b> for Real-Valued Loss Functions; 6. The Structural <b>Risk</b> <b>Minimization</b> Principle; Appendix: Estimating Functions on the Basis of Indirect ...", "dateLastCrawled": "2021-10-15T05:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Paper Digest: <b>ICML 2019 Highlights</b> \u2013 Paper Digest", "url": "https://www.paperdigest.org/2019/05/icml-2019-highlights/", "isFamilyFriendly": true, "displayUrl": "https://www.paperdigest.org/2019/05/<b>icml-2019-highlights</b>", "snippet": "Here, we characterize this trade-off for an <b>empirical</b> <b>risk</b> <b>minimization</b> setting, showing that in general there is a \u201csweet spot\u201d that depends on measurable properties of the dataset, but that there is also a concrete cost to privacy that cannot be avoided simply by collecting more data. 28: Explaining Deep Neural Networks with a Polynomial Time Algorithm for Shapley Value Approximation: Marco Ancona, Cengiz Oztireli, Markus Gross: In this work, by leveraging recent results on uncertainty ...", "dateLastCrawled": "2022-01-22T07:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Deep Q-<b>network (DQN) Based Path Planning Method for Mobile Robots</b> ...", "url": "https://www.researchgate.net/publication/335423316_A_Deep_Q-network_DQN_Based_Path_Planning_Method_for_Mobile_Robots", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/335423316_A_Deep_Q-network_DQN_Based_<b>Path</b>...", "snippet": "Experiments undertaken on simulated <b>maze</b> and real platforms confirm that the Q-table obtained by the proposed Q-<b>learning</b> when used for the <b>path</b>-planning application of mobile robots outperforms ...", "dateLastCrawled": "2022-01-19T21:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) <b>Statistical Analyses of Fingerprint Growth</b> | Thomas Hotz ...", "url": "https://www.academia.edu/33237043/Statistical_Analyses_of_Fingerprint_Growth", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/33237043/<b>Statistical_Analyses_of_Fingerprint_Growth</b>", "snippet": "Academia.edu is a platform for academics to share research papers.", "dateLastCrawled": "2022-01-27T17:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Human Resource Management: Theory and Practice</b> - SILO.PUB", "url": "https://silo.pub/human-resource-management-theory-and-practice.html", "isFamilyFriendly": true, "displayUrl": "https://silo.pub/<b>human-resource-management-theory-and-practice</b>.html", "snippet": "Some of these writers, for example, emphasize how &#39;cultural control&#39; <b>can</b> be reinforced <b>through</b> workplace <b>learning</b> (Legge, 1995) and how the training of &#39;competencies&#39; <b>can</b> render work more &#39;visible&#39; in order to be more manageable (Townley, 1994). Coopey (1996) challenges the academic entrepreneurs such as Peter Senge, The Fifth Discipline (1990). Coopey argues that workplace <b>learning</b> theory assumes a unitarist perspective in which goals are shared and largely ignores conflict stemming from ...", "dateLastCrawled": "2022-01-29T02:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) <b>Human Resource Management Theory and Practice 9780805838626</b> | \u8bd7\u6768 ...", "url": "https://www.academia.edu/8674567/Human_Resource_Management_Theory_and_Practice_9780805838626", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/8674567/<b>Human_Resource_Management_Theory_and_Practice</b>...", "snippet": "<b>Human Resource Management Theory and Practice 9780805838626</b>", "dateLastCrawled": "2022-02-03T04:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "The Known, the Unknown, and the Unknowable in Financial <b>Risk</b> Management ...", "url": "https://ebin.pub/the-known-the-unknown-and-the-unknowable-in-financial-risk-management-measurement-and-theory-advancing-practice-0691128839-9780691128832.html", "isFamilyFriendly": true, "displayUrl": "https://ebin.pub/the-known-the-unknown-and-the-unknowable-in-financial-<b>risk</b>-management...", "snippet": "<b>Risk</b> is associated with the tails of the distribution of returns In the discussion above and in what follows a very standard situation is considered in which some \u201casset\u201d is purchased, and over some future period the benefit or \u201cutility\u201d derived from the purchase is uncertain, and is thus <b>best</b> described in probability terms.2 The first of the two statements above indicates that <b>risk</b> <b>can</b> only be found in situations that have to be described by probabilities, so that only if there is ...", "dateLastCrawled": "2022-01-20T19:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Talks | <b>Machine Learning @ Johns Hopkins University</b>", "url": "https://ml.jhu.edu/talks/", "isFamilyFriendly": true, "displayUrl": "https://ml.jhu.edu/talks", "snippet": "A Machine <b>Learning</b> Approach to Causal Inference in the Presence of Missing Data Xiaochun Li, Indiana University ) Fri 02/15/19, 11:30am, Clark Hall 110 Machine <b>Learning</b> for Medical Decision Support Pengtao Xie, Pentuum, Inc. Tue 02/12/19, 01:00pm, Clark 110 Bayesian Estimation of Sparse Spiked Covariance Matrices in High Dimensions Yanxun Xu, Johns Hopkins University ) Mon 02/11/19, 11:30am, Clark 110 Neural data science: From recordings to theoretical models ...", "dateLastCrawled": "2022-01-30T23:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(PDF) <b>Penalty Functions for Genetic Programming Algorithms</b>", "url": "https://www.researchgate.net/publication/221435193_Penalty_Functions_for_Genetic_Programming_Algorithms", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221435193_Penalty_Functions_for_Genetic...", "snippet": "The <b>maze</b>-type shortest <b>path</b> problem is characterized by the <b>maze</b>-type network that contains many dead-ends, and the conventional genetic algorithms based on the population of feasible paths are ...", "dateLastCrawled": "2021-11-11T08:05:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine Learning</b> Glossary | <b>Google Developers</b>", "url": "https://developers.google.com/machine-learning/glossary/", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine-learning</b>/glossary", "snippet": "<b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) Choosing the function that minimizes loss on the training set. Contrast with structural <b>risk</b> <b>minimization</b>. encoder. #language . In general, any ML system that converts from a raw, sparse, or external representation into a more processed, denser, or more internal representation. Encoders are often a component of a larger model, where they are frequently paired with a decoder. Some Transformers pair encoders with decoders, though other Transformers use only ...", "dateLastCrawled": "2022-02-03T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Simulation of Learners&#39; Behaviors Based</b> on the Modified Cellular ...", "url": "https://www.researchgate.net/publication/220518156_Simulation_of_Learners'_Behaviors_Based_on_the_Modified_Cellular_Automata_Model", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/220518156_Simulation_of_Learners", "snippet": "A milestone in <b>learning</b> theory was a characterization of conditions on the hypothesis space that ensure generalization for the natural class of <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) <b>learning</b> ...", "dateLastCrawled": "2022-01-20T03:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>ICML 2018 Abstracts</b> \u00b7 GitHub", "url": "https://gist.github.com/cwhy/2db49912f1d22495d2788ea89e933045", "isFamilyFriendly": true, "displayUrl": "https://gist.github.com/cwhy/2db49912f1d22495d2788ea89e933045", "snippet": "In this paper, we derive bounds on the mutual information of the <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) procedure for both 0-1 and strongly-convex loss classes. We prove that under the Axiom of Choice, the existence of an <b>ERM</b> <b>learning</b> rule with a vanishing mutual information is equivalent to the assertion that the loss class has a finite VC dimension, thus bridging information theory with statistical <b>learning</b> theory. Similarly, an asymptotic bound on the mutual information is established for ...", "dateLastCrawled": "2022-01-19T20:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>NIPS 2018 Abstract</b> \u00b7 GitHub", "url": "https://gist.github.com/cwhy/3d9fdd54a75a6f698c929f691e9d4d83", "isFamilyFriendly": true, "displayUrl": "https://<b>gist</b>.github.com/cwhy/3d9fdd54a75a6f698c929f691e9d4d83", "snippet": "As opposed to standard <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>), distributionally robust optimization aims to minimize the worst-case <b>risk</b> over a larger ambiguity set containing the original <b>empirical</b> distribution of the training data. In this work, we describe a minimax framework for statistical <b>learning</b> with ambiguity sets given by balls in Wasserstein space. In particular, we prove a generalization bound that involves the covering number properties of the original <b>ERM</b> problem. As an illustrative ...", "dateLastCrawled": "2022-01-02T07:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Track: <b>Poster Session 1</b>", "url": "https://nips.cc/virtual/2020/session/21242", "isFamilyFriendly": true, "displayUrl": "https://nips.cc/virtual/2020/session/21242", "snippet": "Classical recovery approaches such as <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) are guaranteed to succeed when the measurement matrix is sub-Gaussian. However, when the measurement matrix and measurements are heavy tailed or have outliers, recovery may fail dramatically. In this paper we propose an algorithm inspired by the Median-of-Means (MOM). Our algorithm guarantees recovery for heavy tailed data, even in the presence of outliers. Theoretically, our results show our novel MOM-based algorithm ...", "dateLastCrawled": "2021-10-13T20:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>ICML2020</b>/<b>icml2020</b>_1.md at master \u00b7 <b>haozhangcn/ICML2020</b> \u00b7 <b>GitHub</b>", "url": "https://github.com/haozhangcn/ICML2020/blob/master/icml2020_1.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/haozhangcn/<b>ICML2020</b>/blob/master/<b>icml2020</b>_1.md", "snippet": "We show that in the PAC model for passive <b>learning</b>, any {\\em <b>empirical</b> <b>risk</b> minimizer} has a sample complexity that is optimal up to a factor of $\\widetilde{O}(k)$. 44.Rigging the Lottery: Making All Tickets Winners \ud83d\udd17. Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, Erich Elsen Metadata Supplemental. <b>Compared</b> to dense networks, sparse neural networks are shown to be more parameter efficient, more compute efficient and have been used to decrease wall clock inference times. There ...", "dateLastCrawled": "2022-01-29T11:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "comp.ai.neural-nets FAQ, Part 4 of 7: Books, data, etc.", "url": "http://www.faqs.org/faqs/ai-faq/neural-nets/part4/index.html", "isFamilyFriendly": true, "displayUrl": "www.faqs.org/faqs/ai-faq/neural-nets/part4/index.html", "snippet": "Conditions for Consistency of <b>Empirical</b> <b>Risk</b> <b>Minimization</b> Principle; 4. Bounds on the <b>Risk</b> for Indicator Loss Functions; Appendix: Lower Bounds on the <b>Risk</b> of the <b>ERM</b> Principle; 5. Bounds on the <b>Risk</b> for Real-Valued Loss Functions; 6. The Structural <b>Risk</b> <b>Minimization</b> Principle; Appendix: Estimating Functions on the Basis of Indirect ...", "dateLastCrawled": "2021-10-15T05:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Track: Poster Session 3", "url": "https://icml.cc/virtual/2021/session/12515", "isFamilyFriendly": true, "displayUrl": "https://icml.cc/virtual/2021/session/12515", "snippet": "Machine <b>learning</b> models trained with purely observational data and the principle of <b>empirical</b> <b>risk</b> <b>minimization</b> (Vapnik 1992) <b>can</b> fail to generalize to unseen domains. In this paper, we focus on the case where the problem arises <b>through</b> spurious correlation between the observed domains and the actual task labels. We find that many domain generalization methods do not explicitly take this spurious correlation into account. Instead, especially in more application-oriented research areas like ...", "dateLastCrawled": "2021-11-05T17:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Paper Digest: <b>ICML 2019 Highlights</b> \u2013 Paper Digest", "url": "https://www.paperdigest.org/2019/05/icml-2019-highlights/", "isFamilyFriendly": true, "displayUrl": "https://www.paperdigest.org/2019/05/<b>icml-2019-highlights</b>", "snippet": "Here, we characterize this trade-off for an <b>empirical</b> <b>risk</b> <b>minimization</b> setting, showing that in general there is a \u201csweet spot\u201d that depends on measurable properties of the dataset, but that there is also a concrete cost to privacy that cannot be avoided simply by collecting more data. 28: Explaining Deep Neural Networks with a Polynomial Time Algorithm for Shapley Value Approximation: Marco Ancona, Cengiz Oztireli, Markus Gross: In this work, by leveraging recent results on uncertainty ...", "dateLastCrawled": "2022-01-22T07:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(PDF) Guaranteed Functional Tensor Singular Value ... - researchgate.net", "url": "https://www.researchgate.net/publication/353790341_Guaranteed_Functional_Tensor_Singular_Value_Decomposition", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/353790341_Guaranteed_Functional_Tensor...", "snippet": "This paper proposed the functional tensor singular value decomposition for high-dimensional high-. order longitudinal tensor data. <b>Compared</b> with the classical p ower iteration sc hemes for tensors ...", "dateLastCrawled": "2022-01-02T10:33:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> and Stochastic Gradient Descent for ...", "url": "http://proceedings.mlr.press/v89/veitch19a/veitch19a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v89/veitch19a/veitch19a.pdf", "snippet": "where F(Sn) is the <b>empirical</b> distribution.2 The <b>ERM</b> dogma is to select the predictor \u03c0\u02c6\u03b8 n given by \u02c6\u03b8 n = argmin\u03b8 R\u02c6(\u03b8,Sn). That is, the objective function that de\ufb01nes <b>learning</b> is the <b>empirical</b> <b>risk</b>. <b>ERM</b> has two useful properties. (1) It provides a prin-cipled framework for de\ufb01ning new <b>machine</b> <b>learning</b> methods. In particular, when ...", "dateLastCrawled": "2021-09-18T06:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Statistical <b>Learning</b> Theory and the C-Loss cost function", "url": "http://www.cnel.ufl.edu/courses/EEL6814/closs.pdf", "isFamilyFriendly": true, "displayUrl": "www.cnel.ufl.edu/courses/EEL6814/closs.pdf", "snippet": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) principle Let us consider a <b>learning</b> <b>machine</b> x,d are real r.v. with joint distribution P(x,y). F(x) is a function of some parameters w, i.e. f(x,w). d d. <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) principle How can we find the possible best <b>learning</b> <b>machine</b> that generalizes for unseen data from the same distribution? Define the <b>Risk</b> functional as L(.) is called the Loss function, and minimize it w.r.t. w achieving the best possible loss. But we can not do this ...", "dateLastCrawled": "2022-01-28T20:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Computational and Statistical <b>Learning</b> Theory", "url": "https://home.ttic.edu/~nati/Teaching/TTIC31120/2015/Lecture17.pdf", "isFamilyFriendly": true, "displayUrl": "https://home.ttic.edu/~nati/Teaching/TTIC31120/2015/Lecture17.pdf", "snippet": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) / Sample Average Approximation (SAA): Collect sample z1UYU zm ... SGD for <b>Machine</b> <b>Learning</b> Initialize S 4 L r At iteration t: Draw T \u00e7\u00e1U \u00e71\u00de If U \u00e7 S \u00e7 \u00e1\u00f6 T \u00e7 O s\u00e1 S \u00e7 &gt; 5 Z S \u00e7 E\u00df \u00e7U \u00e7\u00f6 T \u00e7 else: S \u00e7 &gt; 5 Z S \u00e7 Return S % \u00cd L 5 \u00cd \u00c3 \u00cd S \u00e7 \u00e7 @ 5 Draw T 5\u00e1U 5 \u00e1\u00e5\u00e1 T \u00e0 \u00e1U \u00e0 1\u00de Initialize S 4 L r At iteration t: Pick E \u00d0 s\u00e5I at random If U \u00dc S \u00e7 \u00e1\u00f6 T \u00dc O s\u00e1 S \u00e7 &gt; 5 Z S \u00e7 E\u00df \u00e7U \u00dc\u00f6 T \u00dc else: S \u00e7 &gt; 5 Z S \u00e7 S \u00e7 &gt; 5 Z ...", "dateLastCrawled": "2022-01-26T03:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Basics of <b>Machine</b> <b>Learning</b>", "url": "https://courses.cs.duke.edu/spring21/compsci527/slides/s_04_deep_learning.pdf", "isFamilyFriendly": true, "displayUrl": "https://courses.cs.duke.edu/spring21/compsci527/slides/s_04_deep_<b>learning</b>.pdf", "snippet": "This is called <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) COMPSCI 527 \u2014 Computer Vision Basics of <b>Machine</b> <b>Learning</b> 15/26. Loss and <b>Risk</b> <b>Machine</b> <b>Learning</b> and the Statistical <b>Risk</b> <b>ERM</b>: w^ 2argmin w2R m L T(w) In <b>machine</b> <b>learning</b>, we go much farther: We also want h to do well on previously unseen inputs To relate past and future data, assume that all data comes from the same joint probability distribution p(x;y) p is called the generative data model or just model The goal of <b>machine</b> <b>learning</b> is to ...", "dateLastCrawled": "2021-11-06T09:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Topics in <b>Machine</b> <b>Learning</b> (TIML-09)", "url": "https://www.cse.iitb.ac.in/~saketh/teaching/cs689.html", "isFamilyFriendly": true, "displayUrl": "https://www.cse.iitb.ac.in/~saketh/teaching/cs689.html", "snippet": "Introduction to Statistical <b>Learning</b> Theory (SLT): Definitions of loss function, <b>risk</b>, <b>empirical</b> <b>risk</b>, motivation for <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) Further Reading, Supplementary: Jan 12: Consistency of <b>ERM</b>, Sufficient condition for <b>ERM</b> as one-sided uniform convergence, Analysis for finite sets of functions and extensions to general case using Symmetrization trick, Shattering Coeff. Further Reading, Supplementary: Jan 15: Shattering coeff., growth function, VC dimension, Annealed Entropy ...", "dateLastCrawled": "2022-01-11T09:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Lecture 1: Reinforcement <b>Learning</b>: What and Why?", "url": "https://imada.sdu.dk/~kandemir/rl-lecture1.pdf", "isFamilyFriendly": true, "displayUrl": "https://imada.sdu.dk/~kandemir/rl-lecture1.pdf", "snippet": "<b>machine</b> <b>learning</b> and is referred to as <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>). 3 Challenges of reinforcement <b>learning</b> Consider the cart pole balancing problem, where a cart carrying an unactuated pole \ufb02oats on a straight horizontal track. The cart is actuated by a torque applied either to the right or the left direction. Seeherefor a real cart ...", "dateLastCrawled": "2021-09-30T12:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Stratified <b>Sampling Meets Machine Learning</b>", "url": "http://proceedings.mlr.press/v48/liberty16.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v48/liberty16.pdf", "snippet": "3. <b>Empirical</b> <b>Risk</b> <b>Minimization</b> <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) is a standard ap-proach in <b>machine</b> <b>learning</b> in which the chosen model is the minimizer of the <b>empirical</b> <b>risk</b>. The <b>empirical</b> <b>risk</b> R emp(p) is de\ufb01ned as an average loss of the model over the training set Q. Here Qis a query log containing a ran-", "dateLastCrawled": "2021-10-13T10:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and-sample-codes ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/README.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "Loss function 3.1 Categories 3.2 <b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) and Structural <b>risk</b> <b>minimization</b> (SRM) 3.2.1 MLE 3.2.2 MAP 4. Evaluation metrics 4.1 Accuracy, precision, recall and F1 4.2 ROC, AUC 4.3 BLEU 5. Sample projects 6. Classical questions 6.1 Why is logistic regression a generalized linear model ?", "dateLastCrawled": "2021-09-22T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Liu Liu - GitHub Pages", "url": "https://liuliuforph.github.io/", "isFamilyFriendly": true, "displayUrl": "https://liuliuforph.github.io", "snippet": "In <b>analogy</b> to classical compressed sensing, here we assume a generative model as a prior, that is, we assume the vector is represented by a deep generative model G: R^k \u2013&gt; R^n. Classical recovery approaches such as <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) are guaranteed to succeed when the measurement matrix is sub-Gaussian. However, when the measurement matrix and measurements are heavy-tailed or have outliers, recovery may fail dramatically. In this paper we propose an algorithm inspired by the ...", "dateLastCrawled": "2022-02-02T23:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "[2006.09461] Robust <b>Compressed Sensing using Generative Models</b> - arXiv", "url": "https://arxiv.org/abs/2006.09461", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/abs/2006.09461", "snippet": "Classical recovery approaches such as <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) are guaranteed to succeed when the measurement matrix is sub-Gaussian. However, when the measurement matrix and measurements are heavy-tailed or have outliers, recovery may fail dramatically. In this paper we propose an algorithm inspired by the Median-of-Means (MOM). Our algorithm guarantees recovery for heavy-tailed data, even in the presence of outliers. Theoretically, our results show our novel MOM-based algorithm ...", "dateLastCrawled": "2021-06-27T11:19:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "ToyotaTechnologicalInstituteatChicago UniversityofTexasatAustin surbhi ...", "url": "https://arxiv.org/pdf/2005.07652", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/pdf/2005.07652", "snippet": "<b>Just as empirical risk minimization (ERM</b>) is central for non-robust PAC <b>learning</b>, a core component of adversarially robust <b>learning</b> is minimizing the robust empirical risk on a dataset S, \u02c6h \u2208 RERM U(S) ,argmin h\u2208H 1 m Xm i=1 sup z\u2208U(x) 1 [h(z) 6= y]. In this paper, we provide necessary and su\ufb03cient conditions on perturbation sets U, under which the robust empirical risk minimization (RERM) problem is e\ufb03ciently solvable in the realizable setting. We show that an e\ufb03cient ...", "dateLastCrawled": "2021-10-06T13:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Efficiently Learning Adversarially Robust Halfspaces with</b> Noise | DeepAI", "url": "https://deepai.org/publication/efficiently-learning-adversarially-robust-halfspaces-with-noise", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>efficiently-learning-adversarially-robust-halfspaces</b>...", "snippet": "<b>Just as empirical risk minimization (ERM</b>) is central for non-robust PAC <b>learning</b>, a core component of adversarially robust <b>learning</b> is minimizing the robust empirical risk on a dataset S, ^ h \u2208 R E R M U ( S ) \u225c argmin h \u2208 H 1 m m \u2211 i = 1 sup z \u2208 U ( x ) 1 [ h ( z ) \u2260 y ] .", "dateLastCrawled": "2021-12-05T23:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Efficiently <b>Learning</b> Adversarially Robust Halfspaces with Noise", "url": "http://proceedings.mlr.press/v119/montasser20a/montasser20a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v119/montasser20a/montasser20a.pdf", "snippet": "remains a major challenge in <b>machine</b> <b>learning</b>. A line of work has shown that predictors learned by deep neural networks are not robust to adversarial examples (Szegedy et al.,2014;Biggio et al.,2013;Goodfellow et al.,2015). This has led to a long line of research studying different aspects of robustness to adversarial examples. In this paper, we consider the problem of distribution-independent <b>learning</b> of halfspaces that are robust to ad-versarial examples at test time, also referred to as ...", "dateLastCrawled": "2021-11-21T12:03:00.0000000Z", "language": "en", "isNavigational": false}], [], []], "all_bing_queries": ["+(empirical risk minimization (erm))  is like +(learning the best path through a maze)", "+(empirical risk minimization (erm)) is similar to +(learning the best path through a maze)", "+(empirical risk minimization (erm)) can be thought of as +(learning the best path through a maze)", "+(empirical risk minimization (erm)) can be compared to +(learning the best path through a maze)", "machine learning +(empirical risk minimization (erm) AND analogy)", "machine learning +(\"empirical risk minimization (erm) is like\")", "machine learning +(\"empirical risk minimization (erm) is similar\")", "machine learning +(\"just as empirical risk minimization (erm)\")", "machine learning +(\"empirical risk minimization (erm) can be thought of as\")", "machine learning +(\"empirical risk minimization (erm) can be compared to\")"]}