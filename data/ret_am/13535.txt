{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "An Introductory Guide to <b>Few-Shot</b> <b>Learning</b> for Beginners", "url": "https://analyticsindiamag.com/an-introductory-guide-to-few-shot-learning-for-beginners/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/an-introductory-guide-to-<b>few-shot</b>-<b>learning</b>-for-beginners", "snippet": "Where this <b>learning</b> will aim here to make this recognition with very few amounts of data.just <b>like</b> <b>humans</b>. <b>Few-shot</b> <b>Learning</b>, Zero-shot <b>Learning</b>, and One-shot <b>Learning</b> . <b>Few-shot</b> <b>learning</b> methods basically work on the approach where we need to feed a light amount of data to model for training. where Zero-shot <b>learning</b> methods work on the approach where zero amount of data for any particular class is used by models to predict correctly. They have a similar application such as: Image ...", "dateLastCrawled": "2022-02-03T05:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "One-shot and <b>few-shot learning of word embeddings</b> | DeepAI", "url": "https://deepai.org/publication/one-shot-and-few-shot-learning-of-word-embeddings", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/one-shot-and-<b>few-shot-learning-of-word-embeddings</b>", "snippet": "By contrast, <b>humans</b> have an incredible ability to do one-shot or <b>few-shot</b> <b>learning</b>. For instance, from just hearing a word used in a sentence, <b>humans</b> can infer a great deal about it, by leveraging what the syntax and semantics of the surrounding <b>words</b> tells us. Here, we draw inspiration from this to highlight a simple technique by which deep recurrent networks can similarly exploit their prior knowledge to learn a useful representation for a <b>new</b> word from little data. This could make", "dateLastCrawled": "2021-12-02T08:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Training a Machine Learning model from a</b> few examples: <b>Few-Shot</b> ...", "url": "https://medium.datadriveninvestor.com/training-a-machine-learning-model-from-a-few-examples-few-shot-learning-part-1-50402ab8dfa5", "isFamilyFriendly": true, "displayUrl": "https://medium.datadriveninvestor.com/<b>training-a-machine-learning-model-from-a</b>-few...", "snippet": "This proves that we <b>humans</b> are good at <b>Few-shot</b> <b>learning</b>. In the above example, we were able to generalize from a limited number of examples. This property is missing in the current machine <b>learning</b> models. To bridge this gap between AI and <b>humans</b>, FSL is an important research area. This motivates the field of <b>Few-shot</b> <b>learning</b> which tries to generalize from a few examples using prior knowledge. Another scenario where FSL is relevant includes the set of applications where data collection is ...", "dateLastCrawled": "2022-01-17T18:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "GPT-3: <b>Few-Shot</b> <b>Learning</b> with a Giant Language Model", "url": "https://nlp.stanford.edu/seminar/details/melaniesubbiah.pdf", "isFamilyFriendly": true, "displayUrl": "https://nlp.stanford.edu/seminar/details/melaniesubbiah.pdf", "snippet": "<b>Few-Shot</b> <b>Learning</b> with a Giant Language Model Melanie Subbiah 1 OpenAI Columbia University. 2. What is the goal? 3 <b>Humans</b> learn <b>new</b> tasks through demonstrations and instructions. We\u2019d <b>like</b> general-purpose agents that could do the same. What is the goal? 4 <b>Humans</b> learn <b>new</b> tasks through demonstrations and instructions. We\u2019d <b>like</b> general-purpose agents that can do the same. Typical Approach 5. Disadvantages to Fine-tuning 6 \u2022 Creates a task-specific model \u2022 Requires large high-quality ...", "dateLastCrawled": "2022-01-29T07:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) One-shot and <b>few-shot</b> <b>learning</b> of word embeddings", "url": "https://www.researchgate.net/publication/320727415_One-shot_and_few-shot_learning_of_word_embeddings", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/320727415_One-shot_and_<b>few-shot</b>_<b>learning</b>_of...", "snippet": "By contrast, <b>humans</b> have an incredible ability to do one-shot or <b>few-shot</b> <b>learning</b>. For instance, from just hearing a word used in a sentence, <b>humans</b> can infer a great deal about it, by leveraging ...", "dateLastCrawled": "2022-01-03T18:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Learn from Concepts: Towards the Purified Memory for <b>Few-shot</b> <b>Learning</b>", "url": "https://www.ijcai.org/proceedings/2021/0123.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.ijcai.org/proceedings/2021/0123.pdf", "snippet": "<b>few-shot</b> <b>learning</b> is not to classify unseen samples, but fast adapts the meta-knowledge to <b>new</b> tasks, where only very few labeled data and knowledge gained from previous experience are given. Recently, signi\ufb01cant advantages [Vinyals et al., 2016; Equal Contribution yContact Author Finn et al., 2017; Snell et al., 2017; Sung et al., 2018] have been made to tackle this problem by using the idea of meta-<b>learning</b> coupled with episodic training [Vinyals et al., 2016]. The intuition is to use a ...", "dateLastCrawled": "2022-01-29T16:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Few-Shot Image Classification with Meta-Learning</b> | Sicara", "url": "https://www.sicara.ai/blog/2019-07-30-image-classification-few-shot-meta-learning", "isFamilyFriendly": true, "displayUrl": "https://www.sicara.ai/blog/2019-07-30-image-classification-<b>few-shot</b>-meta-<b>learning</b>", "snippet": "This problem of <b>learning</b> from few examples is called <b>few-shot</b> <b>learning</b>. For a few years now, the <b>few-shot</b> <b>learning</b> problem has drawn a lot of attention in the research community, and a lot of elegant solutions have been developed. The most popular solutions right now use meta-<b>learning</b>, or in three <b>words</b>: <b>learning</b> to learn. Keep reading if you ...", "dateLastCrawled": "2022-02-02T20:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Few-Shot</b> Text Classification", "url": "https://few-shot-text-classification.fastforwardlabs.com/", "isFamilyFriendly": true, "displayUrl": "https://<b>few-shot</b>-text-classification.fastforwardlabs.com", "snippet": "<b>Few-shot</b> <b>learning</b> for classification is a scenario in which there is a small amount of labeled data for all labels the model is expected to recognize. The goal is for the model to generalize to <b>new</b> unseen examples in the same categories both quickly and effectively. In traditional zero-shot <b>learning</b>, a classifier is trained on one set of labels and evaluated on a different set of labels that it has never seen before (thus, \u201czero-shot\u201d). This typically requires providing the model with ...", "dateLastCrawled": "2022-02-02T04:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Human <b>few-shot learning of compositional instructions</b>", "url": "https://cims.nyu.edu/~brenden/papers/LakeEtAl2019CogSci.pdf", "isFamilyFriendly": true, "displayUrl": "https://cims.nyu.edu/~brenden/papers/LakeEtAl2019CogSci.pdf", "snippet": "<b>like</b> instruction <b>learning</b> tasks. Our results show that people can learn and use novel functional concepts from very few examples (<b>few-shot</b> <b>learning</b>), successfully applying familiar functions to novel inputs. People can also compose concepts in complex ways that go beyond the provided demonstrations. Two additional experiments examined the assumptions and in- ductive biases that people make when solving these tasks, re-vealing three biases: mutual exclusivity, one-to-one mappings, and iconic ...", "dateLastCrawled": "2022-02-02T02:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What is <b>few-shot learning in machine learning? - Quora</b>", "url": "https://www.quora.com/What-is-few-shot-learning-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>few-shot-learning-in-machine-learning</b>", "snippet": "Answer (1 of 2): <b>Learning</b> from a few data points is called <b>few-shot</b> <b>learning</b> . Also called K-shot <b>learning</b> where k implies number of data points per class There is also zero shot <b>learning</b> , where we will not have data points , but we will have meta information about each of the class and we will...", "dateLastCrawled": "2022-01-18T07:58:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "An Introductory Guide to <b>Few-Shot</b> <b>Learning</b> for Beginners", "url": "https://analyticsindiamag.com/an-introductory-guide-to-few-shot-learning-for-beginners/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/an-introductory-guide-to-<b>few-shot</b>-<b>learning</b>-for-beginners", "snippet": "Where this <b>learning</b> will aim here to make this recognition with very few amounts of data.just like <b>humans</b>. <b>Few-shot</b> <b>Learning</b>, Zero-shot <b>Learning</b>, and One-shot <b>Learning</b> . <b>Few-shot</b> <b>learning</b> methods basically work on the approach where we need to feed a light amount of data to model for training. where Zero-shot <b>learning</b> methods work on the approach where zero amount of data for any particular class is used by models to predict correctly. They have a <b>similar</b> application such as: Image ...", "dateLastCrawled": "2022-02-03T05:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Few Shot</b> <b>Learning</b>: Models, code, and papers - CatalyzeX", "url": "https://www.catalyzex.com/s/Few%20Shot%20Learning", "isFamilyFriendly": true, "displayUrl": "https://www.catalyzex.com/s/Few Shot <b>Learning</b>", "snippet": "<b>Few-Shot</b> <b>Learning</b>(also known as <b>few-shot</b> <b>learning</b>) is a subfield of machine <b>learning</b> that aims to create such models that can learn the desired objective with less data, <b>similar</b> to how <b>humans</b> learn. In this paper, we have reviewed some of the well-known deep <b>learning</b>-based approaches towards <b>few-shot</b> <b>learning</b>. We have discussed the recent achievements, challenges, and possibilities of improvement of <b>few-shot</b> <b>learning</b> based deep <b>learning</b> architectures. Our aim for this paper is threefold: (i ...", "dateLastCrawled": "2022-01-24T22:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "One-shot and <b>few-shot learning of word embeddings</b> | DeepAI", "url": "https://deepai.org/publication/one-shot-and-few-shot-learning-of-word-embeddings", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/one-shot-and-<b>few-shot-learning-of-word-embeddings</b>", "snippet": "By contrast, <b>humans</b> have an incredible ability to do one-shot or <b>few-shot</b> <b>learning</b>. For instance, from just hearing a word used in a sentence, <b>humans</b> can infer a great deal about it, by leveraging what the syntax and semantics of the surrounding <b>words</b> tells us. Here, we draw inspiration from this to highlight a simple technique by which deep recurrent networks can similarly exploit their prior knowledge to learn a useful representation for a <b>new</b> word from little data. This could make", "dateLastCrawled": "2021-12-02T08:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Tutorial #2: <b>few-shot learning and meta-learning</b> I", "url": "https://www.borealisai.com/en/blog/tutorial-2-few-shot-learning-and-meta-learning-i/", "isFamilyFriendly": true, "displayUrl": "https://www.borealisai.com/en/blog/tutorial-2-<b>few-shot-learning-and-meta-learning</b>-i", "snippet": "<b>Humans</b> can recognize <b>new</b> object classes from very few instances. However, most machine <b>learning</b> techniques require thousands of examples to achieve <b>similar</b> performance. The goal of <b>few-shot</b> <b>learning</b> is to classify <b>new</b> data having seen only a few training examples. In the extreme, there might only be a single example of each class (one shot <b>learning</b>). In practice, <b>few-shot</b> <b>learning</b> is useful when training examples are hard to find (e.g., cases of a rare disease), or where the cost of ...", "dateLastCrawled": "2022-01-29T12:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) One-shot and <b>few-shot</b> <b>learning</b> of word embeddings", "url": "https://www.researchgate.net/publication/320727415_One-shot_and_few-shot_learning_of_word_embeddings", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/320727415_One-shot_and_<b>few-shot</b>_<b>learning</b>_of...", "snippet": "By contrast, <b>humans</b> have an incredible ability to do one-shot or <b>few-shot</b> <b>learning</b>. For instance, from just hearing a word used in a sentence, <b>humans</b> can infer a great deal about it, by leveraging ...", "dateLastCrawled": "2022-01-03T18:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Few Shot</b> Transfer <b>Learning</b> Between Word Relatedness and Similarity ...", "url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16981/16086", "isFamilyFriendly": true, "displayUrl": "https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16981/16086", "snippet": "<b>Few Shot</b> Transfer <b>Learning</b> Between Word Relatedness and Similarity Tasks Using a Gated Recurrent Siamese Network James O\u2019 Neill, Paul Buitelaar Insight Centre for Data Analytics National University of Ireland, Galway {james.oneill, paul.buitelaar}@insight-centre.org Abstract Word similarity and word relatedness are fundamental to nat-ural language processing and more generally, understanding how <b>humans</b> relate concepts in semantic memory. A growing number of datasets are being proposed as ...", "dateLastCrawled": "2022-01-01T04:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "GPT-3: <b>Few-Shot</b> <b>Learning</b> with a Giant Language Model", "url": "https://nlp.stanford.edu/seminar/details/melaniesubbiah.pdf", "isFamilyFriendly": true, "displayUrl": "https://nlp.stanford.edu/seminar/details/melaniesubbiah.pdf", "snippet": "<b>Few-Shot</b> <b>Learning</b> with a Giant Language Model Melanie Subbiah 1 OpenAI Columbia University. 2. What is the goal? 3 <b>Humans</b> learn <b>new</b> tasks through demonstrations and instructions. We\u2019d like general-purpose agents that could do the same. What is the goal? 4 <b>Humans</b> learn <b>new</b> tasks through demonstrations and instructions. We\u2019d like general-purpose agents that can do the same. Typical Approach 5. Disadvantages to Fine-tuning 6 \u2022 Creates a task-specific model \u2022 Requires large high-quality ...", "dateLastCrawled": "2022-01-29T07:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>GPT</b>-3: Language Models are <b>Few-Shot</b> Learners | by Grigory Sapunov | Intento", "url": "https://blog.inten.to/gpt-3-language-models-are-few-shot-learners-a13d1ae8b1f9", "isFamilyFriendly": true, "displayUrl": "https://blog.inten.to/<b>gpt</b>-3-language-models-are-<b>few-shot</b>-learners-a13d1ae8b1f9", "snippet": "The last but not least, <b>humans</b> learn in a different way \u2014 they do not require large supervised datasets to learn most language tasks. We can try a route called meta-<b>learning</b>. In the context of language models, it means the model develops a broad set of skills and pattern recognition abilities at training time and then uses those abilities at inference time to rapidly adapt to or recognize the desired task. <b>GPT</b>-2 used \u201cin-context <b>learning</b>\u201d, using the text input of a pretrained language ...", "dateLastCrawled": "2022-01-29T12:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Few-Shot</b> Text Classification", "url": "https://few-shot-text-classification.fastforwardlabs.com/", "isFamilyFriendly": true, "displayUrl": "https://<b>few-shot</b>-text-classification.fastforwardlabs.com", "snippet": "<b>Few-shot</b> <b>learning</b> for classification is a scenario in which there is a small amount of labeled data for all labels the model is expected to recognize. The goal is for the model to generalize to <b>new</b> unseen examples in the same categories both quickly and effectively. In traditional zero-shot <b>learning</b>, a classifier is trained on one set of labels and evaluated on a different set of labels that it has never seen before (thus, \u201czero-shot\u201d). This typically requires providing the model with ...", "dateLastCrawled": "2022-02-02T04:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Revisiting metric learning for few-shot image classification</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S092523122030607X", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S092523122030607X", "snippet": "In other <b>words</b>, the goal of <b>few-shot</b> <b>learning</b> is to classify unseen data instances (query examples) into a set of <b>new</b> categories, given just a small number of labeled instances in each class (support examples). In this work, we focus on the case of <b>few-shot</b> classification, where only a few labeled examples per class are given. Obviously, naively fine-tuning a model on the novel labeled data would easily overfit the few given data. Hence, data augmentation and regularization , are often ...", "dateLastCrawled": "2022-01-25T14:24:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "One-shot and <b>few-shot learning of word embeddings</b> | DeepAI", "url": "https://deepai.org/publication/one-shot-and-few-shot-learning-of-word-embeddings", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/one-shot-and-<b>few-shot-learning-of-word-embeddings</b>", "snippet": "By contrast, <b>humans</b> have an incredible ability to do one-shot or <b>few-shot</b> <b>learning</b>. For instance, from just hearing a word used in a sentence, <b>humans</b> <b>can</b> infer a great deal about it, by leveraging what the syntax and semantics of the surrounding <b>words</b> tells us. Here, we draw inspiration from this to highlight a simple technique by which deep recurrent networks <b>can</b> similarly exploit their prior knowledge to learn a useful representation for a <b>new</b> word from little data. This could make", "dateLastCrawled": "2021-12-02T08:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Human <b>few-shot learning of compositional instructions</b>", "url": "https://cims.nyu.edu/~brenden/papers/LakeEtAl2019CogSci.pdf", "isFamilyFriendly": true, "displayUrl": "https://cims.nyu.edu/~brenden/papers/LakeEtAl2019CogSci.pdf", "snippet": "Human <b>few-shot learning of compositional instructions</b> Brenden M. Lake1;2, Tal Linzen3, and Marco Baroni2;4 1New York University, 2Facebook AI Research, 3John Hopkins University, 4ICREA Abstract People learn in fast and \ufb02exible ways that have not been emu-lated by machines. Once a person learns a <b>new</b> verb \u201cdax,\u201d he or she <b>can</b> effortlessly understand how to \u201cdax twice,\u201d \u201cwalk and dax,\u201d or \u201cdax vigorously.\u201d There have been striking recent improvements in machine <b>learning</b> for ...", "dateLastCrawled": "2022-02-02T02:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) One-shot and <b>few-shot</b> <b>learning</b> of word embeddings", "url": "https://www.researchgate.net/publication/320727415_One-shot_and_few-shot_learning_of_word_embeddings", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/320727415_One-shot_and_<b>few-shot</b>_<b>learning</b>_of...", "snippet": "By contrast, <b>humans</b> have an incredible ability to do one-shot or <b>few-shot</b> <b>learning</b>. For instance, from just hearing a word used in a sentence, <b>humans</b> <b>can</b> infer a great deal about it, by leveraging ...", "dateLastCrawled": "2022-01-03T18:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "[PDF] Human <b>few-shot</b> <b>learning</b> of compositional instructions | Semantic ...", "url": "https://www.semanticscholar.org/paper/Human-few-shot-learning-of-compositional-Lake-Linzen/9d9b4cc02fc0ac6fe7eac649599db1a47cf99d89", "isFamilyFriendly": true, "displayUrl": "https://www.semanticscholar.org/paper/Human-<b>few-shot</b>-<b>learning</b>-of-compositional-Lake...", "snippet": "There have been striking recent improvements in machine <b>learning</b> for natural language processing, yet the best algorithms require vast amounts of experience and struggle to generalize <b>new</b> concepts in compositional ways. To better understand these distinctively human abilities, we study the compositional skills of people through language-like instruction <b>learning</b> tasks. Our results show that people <b>can</b> learn and use novel functional concepts from very few examples (<b>few-shot</b> <b>learning</b> ...", "dateLastCrawled": "2022-01-16T17:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Learning Compositional Representations for Few-Shot</b> Recognition | DeepAI", "url": "https://deepai.org/publication/learning-compositional-representations-for-few-shot-recognition", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>learning-compositional-representations-for-few-shot</b>...", "snippet": "<b>Humans</b>, on the other hand, <b>can</b> learn to recognize novel categories from just a few examples. Instrumental to this rapid <b>learning</b> ability is the compositional structure of concept representations in the human brain - something that deep <b>learning</b> models are lacking. In this work we make a step towards bridging this gap between human and machine <b>learning</b> by introducing a simple regularization technique that allows the learned representation to be decomposable into parts. We evaluate the ...", "dateLastCrawled": "2021-12-01T16:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Memory, Show the Way: Memory Based <b>Few Shot</b> Word Representation <b>Learning</b>", "url": "https://aclanthology.org/D18-1173.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/D18-1173.pdf", "snippet": "<b>Humans</b> <b>can</b> learn a <b>new</b> word quickly from min-imal exposure to its context, as in the following example: The Labrador runs happily towards me, barking and wagging its tail. Even this is the \ufb01rst time one hears about Labrador, we <b>can</b> guess it should be an animal or even further a dog easily, since it runs, barks and has a tail. Such ability to ef\ufb01ciently acquire representation from small data, namely fast map-ping, is <b>thought</b> to be the hallmark of human in-telligence that a cognitive ...", "dateLastCrawled": "2021-11-19T18:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Bayesian <b>Few Shot</b> <b>Learning</b> of Compositional Instructions", "url": "https://rogeriojr.com/files/6804-project.pdf", "isFamilyFriendly": true, "displayUrl": "https://rogeriojr.com/files/6804-project.pdf", "snippet": "positionality: <b>humans</b> learn <b>new</b> <b>words</b> from few exam-ples (<b>few shot</b> <b>learning</b>), and also combine <b>new</b> knowl-edge with prior information, such as, known <b>words</b>, to generate seemingly in\ufb01nite number of <b>new</b> sequence and patterns that <b>can</b> be easily understood. For instance, sel\ufb01e <b>can</b> be generalized to mirror sel\ufb01e or animal sel\ufb01e. Recently, there has been a breakthrough in the \ufb01eld of deep <b>learning</b> and natural language processing. How-ever, the most successful models require vast amount of ...", "dateLastCrawled": "2021-11-04T09:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "What is <b>few-shot learning in machine learning? - Quora</b>", "url": "https://www.quora.com/What-is-few-shot-learning-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>few-shot-learning-in-machine-learning</b>", "snippet": "Answer (1 of 2): <b>Learning</b> from a few data points is called <b>few-shot</b> <b>learning</b> . Also called K-shot <b>learning</b> where k implies number of data points per class There is also zero shot <b>learning</b> , where we will not have data points , but we will have meta information about each of the class and we will...", "dateLastCrawled": "2022-01-18T07:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "MetaGAN: GAN in <b>Few-shot</b> <b>Learning</b> Setting \u2013 CSCI 599", "url": "https://metagan.home.blog/2019/05/01/metagan-gan-in-few-shot-learning-setting/", "isFamilyFriendly": true, "displayUrl": "https://metagan.home.blog/2019/05/01/metagan-gan-in-<b>few-shot</b>-<b>learning</b>-setting", "snippet": "<b>Few-shot</b> <b>learning</b> aims to remedy this need, where models only have access to few (on the order of tens or hundreds) training samples. Furthermore, we employed principles of meta-<b>learning</b>, or <b>learning</b> to learn, to enhance our model. Simply put, models for meta-<b>learning</b> must be able to effectively deal with different tasks, as opposed to a specific task in traditional training. In Figure 3, represents the set of general parameters as the starting point. Different \u2019s represent the loss ...", "dateLastCrawled": "2022-01-28T15:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Sequence Models by Andrew Ng \u2014 11 Lessons Learned | by Ryan Shrott ...", "url": "https://towardsdatascience.com/sequence-models-by-andrew-ng-11-lessons-learned-c62fb1d3485b", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/sequence-models-by-andrew-ng-11-lessons-learned-c62fb1d...", "snippet": "Sequence models, in s upervised <b>learning</b>, <b>can</b> be used to address a variety of applications including financial time series prediction, speech recognition, music generation, sentiment classification, machine translation and video activity recognition. The only constraint is that either the input or the output is a sequence. In other <b>words</b>, you may use sequence models to address any type of supervised <b>learning</b> problem which contains a time series in either the input or output layers.", "dateLastCrawled": "2022-01-29T09:25:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "One-shot and <b>few-shot learning of word embeddings</b> | DeepAI", "url": "https://deepai.org/publication/one-shot-and-few-shot-learning-of-word-embeddings", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/one-shot-and-<b>few-shot-learning-of-word-embeddings</b>", "snippet": "One-shot and <b>few-shot learning of word embeddings</b>. Standard deep <b>learning</b> systems require thousands or millions of examples to learn a concept, and cannot integrate <b>new</b> concepts easily. By contrast, <b>humans</b> have an incredible ability to do one-shot or <b>few-shot</b> <b>learning</b>. For instance, from just hearing a word used in a sentence, <b>humans</b> <b>can</b> infer ...", "dateLastCrawled": "2021-12-02T08:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) One-shot and <b>few-shot</b> <b>learning</b> of word embeddings", "url": "https://www.researchgate.net/publication/320727415_One-shot_and_few-shot_learning_of_word_embeddings", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/320727415_One-shot_and_<b>few-shot</b>_<b>learning</b>_of...", "snippet": "By contrast, <b>humans</b> have an incredible ability to do one-shot or <b>few-shot</b> <b>learning</b>. For instance, from just hearing a word used in a sentence, <b>humans</b> <b>can</b> infer a great deal about it, by leveraging ...", "dateLastCrawled": "2022-01-03T18:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Few-Shot Learning on Google Landmark Challenge</b>", "url": "https://homes.cs.washington.edu/~zhihanx/data/few_shot.pdf", "isFamilyFriendly": true, "displayUrl": "https://homes.cs.washington.edu/~zhihanx/data/<b>few_shot</b>.pdf", "snippet": "In contrast, <b>humans</b> <b>can</b> easily learn to classify <b>new</b> objects with extremely few examples or even no example. <b>Few-shot</b> <b>learning</b> aims for better generalization on problems with small labeled datasets. Particularly, <b>few-shot</b> classi\ufb01cation learns to generalize to unseen classes during training, when only a small number of labeled examples are ...", "dateLastCrawled": "2022-01-30T05:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Learning</b> to Learn to Disambiguate: Meta-<b>Learning</b> for <b>Few-Shot</b> Word ...", "url": "https://aclanthology.org/2020.findings-emnlp.405.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/2020.findings-emnlp.405.pdf", "snippet": "the problem for rare <b>words</b> (Kumar et al.,2019). <b>Humans</b>, on the other hand, have a remarkable ability to learn from just a handful of examples (Lake et al.,2015). This inspired researchers to investigate techniques that would enable machine <b>learning</b> models to do the same. One such approach is transfer <b>learning</b> (Caruana,1993), which aims to improve the models\u2019 data ef\ufb01ciency by transferring features between tasks. However, it still fails to generalize to <b>new</b> tasks in the absence of a ...", "dateLastCrawled": "2022-01-23T12:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "One-shot and <b>few-shot learning of word embeddings</b> \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/1710.10280v2/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1710.10280v2", "snippet": "Standard deep <b>learning</b> systems require thousands or millions of examples to learn a concept, and cannot integrate <b>new</b> concepts easily. By contrast, <b>humans</b> have an incredible ability to do one-shot or <b>few-shot</b> <b>learning</b>. For instance, from just hearing a word used in a sentence, <b>humans</b> <b>can</b> infer a great deal about it, by leveraging what the syntax and semantics of the surrounding <b>words</b> tells us. Here, we draw inspiration from this to highlight a simple technique by which deep recurrent ...", "dateLastCrawled": "2022-01-11T19:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "MetaGAN: GAN in <b>Few-shot</b> <b>Learning</b> Setting \u2013 CSCI 599", "url": "https://metagan.home.blog/2019/05/01/metagan-gan-in-few-shot-learning-setting/", "isFamilyFriendly": true, "displayUrl": "https://metagan.home.blog/2019/05/01/metagan-gan-in-<b>few-shot</b>-<b>learning</b>-setting", "snippet": "<b>Compared</b> <b>to humans</b>\u2019 ability to quickly understanding the underlying essentials upon encountering <b>new</b> objects (e.g. think of how toddlers quickly and accurately identify cats vs. dogs after a couple of encounters with said animals), clearly GAN\u2019s need for numerous training samples seems impractical. <b>Few-shot</b> <b>learning</b> aims to remedy this need, where models only have access to few (on the order of tens or hundreds) training samples. Furthermore, we employed principles of meta-<b>learning</b>, or ...", "dateLastCrawled": "2022-01-28T15:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Few-Shot Representation Learning for Out-Of</b>-Vocabulary <b>Words</b> | DeepAI", "url": "https://deepai.org/publication/few-shot-representation-learning-for-out-of-vocabulary-words", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/.../<b>few-shot-representation-learning-for-out-of</b>-vocabulary-<b>words</b>", "snippet": "Recently, meta-<b>learning</b> is proposed and it achieves great performance on various <b>few-shot</b> <b>learning</b> tasks. The intuition of meta-<b>learning</b> is to learn generic knowledge on a variety of <b>learning</b> tasks, such that the model <b>can</b> be adapted to learn a <b>new</b> task with only a few training samples. Approaches for meta-<b>learning</b> <b>can</b> be categorized by the type of knowledge they learn. (1) Learn a metric function that embeds data in the same class closer to each other, including Matching Networks", "dateLastCrawled": "2021-12-02T14:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Summary: Language Models are <b>Few-Shot</b> Learners | by Flexudy Education ...", "url": "https://flexudyeducation.medium.com/summary-language-models-are-few-shot-learners-d17b827a107c", "isFamilyFriendly": true, "displayUrl": "https://flexudyeducation.medium.com/summary-language-models-are-<b>few-shot</b>-learners-d17b...", "snippet": "On LAMBADA, the <b>few-shot</b> capability of language models results in a strong boost to accuracy. Note zero-shot uses a different format from one-shot and <b>few-shot</b> as described in the text. We find that path is still promising and in a zero-shot setting GPT-3 achieves 76% on LAMBADA, a gain of 8% over the previous state of the art.", "dateLastCrawled": "2022-01-21T20:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Few-Shot</b> Text Classification", "url": "https://few-shot-text-classification.fastforwardlabs.com/", "isFamilyFriendly": true, "displayUrl": "https://<b>few-shot</b>-text-classification.fastforwardlabs.com", "snippet": "<b>Few-shot</b> <b>learning</b> for classification is a scenario in which there is a small amount of labeled data for all labels the model is expected to recognize. The goal is for the model to generalize to <b>new</b> unseen examples in the same categories both quickly and effectively. In traditional zero-shot <b>learning</b>, a classifier is trained on one set of labels and evaluated on a different set of labels that it has never seen before (thus, \u201czero-shot\u201d). This typically requires providing the model with ...", "dateLastCrawled": "2022-02-02T04:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What is <b>few-shot learning in machine learning? - Quora</b>", "url": "https://www.quora.com/What-is-few-shot-learning-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>few-shot-learning-in-machine-learning</b>", "snippet": "Answer (1 of 2): <b>Learning</b> from a few data points is called <b>few-shot</b> <b>learning</b> . Also called K-shot <b>learning</b> where k implies number of data points per class There is also zero shot <b>learning</b> , where we will not have data points , but we will have meta information about each of the class and we will...", "dateLastCrawled": "2022-01-18T07:58:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "What is <b>Few-Shot Learning</b> (FSL)? Methods &amp; Applications", "url": "https://research.aimultiple.com/few-shot-learning/", "isFamilyFriendly": true, "displayUrl": "https://research.aimultiple.com/<b>few-shot-learning</b>", "snippet": "<b>Few-shot learning</b> (FSL), also referred to as low-shot <b>learning</b> (LSL) in few sources, is a type of <b>machine</b> <b>learning</b> method where the training dataset contains limited information. The common practice for <b>machine</b> <b>learning</b> applications is to feed as much data as the model can take. This is because in most <b>machine</b> <b>learning</b> applications feeding more ...", "dateLastCrawled": "2022-02-03T03:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Few-shot</b> <b>Learning</b>: A Survey", "url": "http://static.tongtianta.site/paper_pdf/a3d8e744-ac83-11e9-8d81-00163e08bb86.pdf", "isFamilyFriendly": true, "displayUrl": "static.tongtianta.site/paper_pdf/a3d8e744-ac83-11e9-8d81-00163e08bb86.pdf", "snippet": "Particularly, a <b>machine</b> <b>learning</b> problem called <b>Few-Shot</b> <b>Learning</b> (FSL) targets at this case. It can rapidly generalize to new tasks of limited supervised experience by turning to prior knowledge, which mimics human\u2019s ability to acquire knowledge from few examples through generalization and <b>analogy</b>. It has been seen as a test-bed for real artificial intelligence, a way to reduce laborious data gathering and computationally costly training, and antidote for rare cases <b>learning</b>. With ...", "dateLastCrawled": "2021-12-10T21:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Few-shot Learning</b>: A Survey | DeepAI", "url": "https://deepai.org/publication/few-shot-learning-a-survey", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>few-shot-learning</b>-a-survey", "snippet": "Particularly, a <b>machine</b> <b>learning</b> problem called <b>Few-Shot Learning</b> (FSL) targets at this case. It can rapidly generalize to new tasks of limited supervised experience by turning to prior knowledge, which mimics human&#39;s ability to acquire knowledge from few examples through generalization and <b>analogy</b>. It has been seen as a test-bed for real artificial intelligence, a way to reduce laborious data gathering and computationally costly training, and antidote for rare cases <b>learning</b>. With extensive ...", "dateLastCrawled": "2022-01-19T10:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Few-shot Learning: A Survey</b> - GitHub Pages", "url": "https://amds123.github.io/2019/04/10/Few-shot-Learning-A-Survey/", "isFamilyFriendly": true, "displayUrl": "https://amds123.github.io/2019/04/10/<b>Few-shot-Learning-A-Survey</b>", "snippet": "Particularly, a <b>machine</b> <b>learning</b> problem called <b>Few-Shot</b> <b>Learning</b> (FSL) targets at this case. It can rapidly generalize to new tasks of limited supervised experience by turning to prior knowledge, which mimics human\u2019s ability to acquire knowledge from few examples through generalization and <b>analogy</b>. It has been seen as a test-bed for real artificial intelligence, a way to reduce laborious data gathering and computationally costly training, and antidote for rare cases <b>learning</b>. With ...", "dateLastCrawled": "2022-01-15T10:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Tutorial #3: <b>few-shot learning and meta-learning II</b>", "url": "https://www.borealisai.com/en/blog/tutorial-3-few-shot-learning-and-meta-learning-ii/", "isFamilyFriendly": true, "displayUrl": "https://www.borealisai.com/en/blog/tutorial-3-<b>few-shot-learning-and-meta-learning-ii</b>", "snippet": "Perhaps the most obvious approach to <b>few-shot</b> <b>learning</b> would be transfer <b>learning</b>; we first find a similar task for which there is plentiful data and train a network for this. Then we adapt this network for the <b>few-shot</b> task. We might either (i) fine-tune this network using the <b>few-shot</b> data, or (ii) use the hidden layers as input for a new classifier trained with the <b>few-shot</b> data. Unfortunately, when training data is really sparse, the resulting classifier typically fails to generalize well.", "dateLastCrawled": "2022-01-29T23:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) <b>Generalizing from a Few Examples</b>: A Survey on <b>Few-Shot</b> <b>Learning</b>", "url": "https://www.researchgate.net/publication/332342190_Generalizing_from_a_Few_Examples_A_Survey_on_Few-Shot_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/332342190_<b>Generalizing_from_a_Few_Examples</b>_A...", "snippet": "Particularly, a <b>machine</b> <b>learning</b> problem called <b>Few-Shot</b> <b>Learning</b> (FSL) targets at this case. It can rapidly generalize to new tasks of limited supervised experience by turning to prior knowledge ...", "dateLastCrawled": "2022-01-24T12:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Few-shot</b> <b>Visual Reasoning with Meta-analogical Contrastive Learning</b> ...", "url": "https://deepai.org/publication/few-shot-visual-reasoning-with-meta-analogical-contrastive-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/.../<b>few-shot</b>-<b>visual-reasoning-with-meta-analogical-contrastive-learning</b>", "snippet": "<b>Few-shot</b> <b>Visual Reasoning with Meta-analogical Contrastive Learning</b>. 07/23/2020 \u2219 by Youngsung Kim, et al. \u2219 KAIST \uc218\ub9ac\uacfc\ud559\uacfc \u2219 3 \u2219 share . While humans can solve a visual puzzle that requires logical reasoning by observing only few samples, it would require training over large amount of data for state-of-the-art deep reasoning models to obtain similar performance on the same task.", "dateLastCrawled": "2021-11-29T11:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Few-shot</b> fault diagnosis of rotating machinery with two-branch ...", "url": "https://link.springer.com/article/10.1007/s10845-021-01904-x", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10845-021-01904-x", "snippet": "To better evaluate our proposed TBPN model, we compare it with several classical <b>few-shot</b> <b>learning</b> and <b>machine</b> <b>learning</b> models for all types of <b>few-shot</b> tasks described above. The classical models includes: (1) k-nearest neighbor (KNN), (2) basic matching network with vibration signals (BMN-V), (3) basic matching network with spectrum (BMN-S), (4) basic prototypical network with vibration signals (BPN-V), and (5) basic prototypical network with spectrum (BPN-S). Metrics for multi ...", "dateLastCrawled": "2022-01-31T15:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) <b>Generalizing from a Few Examples</b>: A Survey on <b>Few-shot</b> <b>Learning</b>", "url": "https://www.researchgate.net/publication/342141918_Generalizing_from_a_Few_Examples_A_Survey_on_Few-shot_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/342141918_<b>Generalizing_from_a_Few_Examples</b>_A...", "snippet": "<b>Few-shot</b> <b>Learning</b> (FSL) is a type of <b>machine</b> <b>learning</b> problems (specied by E , T , and P ), where E contains only a limited number of examples with supervised information for the target T .", "dateLastCrawled": "2022-02-02T22:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What is <b>few-shot learning in machine learning? - Quora</b>", "url": "https://www.quora.com/What-is-few-shot-learning-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>few-shot-learning-in-machine-learning</b>", "snippet": "Answer (1 of 2): <b>Learning</b> from a few data points is called <b>few-shot</b> <b>learning</b> . Also called K-shot <b>learning</b> where k implies number of data points per class There is also zero shot <b>learning</b> , where we will not have data points , but we will have meta information about each of the class and we will...", "dateLastCrawled": "2022-01-18T07:58:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Few Shot Learning - A Case Study (1</b>) | by Maitreya Patel | Analytics ...", "url": "https://medium.com/analytics-vidhya/few-shot-learning-a-case-study-1-d71eb06a33df", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/<b>few-shot-learning-a-case-study-1</b>-d71eb06a33df", "snippet": "<b>Few Shot Learning - A Case Study (1</b>) Not long ago, ML research solely focused on data specific objectives. For example, if Bob wants to build a classifier that can detect cat or dog, then he will ...", "dateLastCrawled": "2021-10-12T12:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Maitreya Patel", "url": "https://maitreyapatel.github.io/few_shot_part_1.html", "isFamilyFriendly": true, "displayUrl": "https://maitreyapatel.github.io/few_shot_part_1.html", "snippet": "Therefore, <b>few shot learning is like</b> a new sub-field of ML research to increase the capability of deep <b>learning</b> models in terms of generalization. What are the applications of it? There are lots of applications of a few-shot <b>learning</b>. And there are lots of problems where few shot <b>learning</b> can&#39;t be applied as of now. Currently, many researchers are working on different classification and conversion problems. Let&#39;s check out a few examples of such research problems in various areas of <b>Machine</b> ...", "dateLastCrawled": "2021-10-27T09:18:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], []], "all_bing_queries": ["+(few-shot learning)  is like +(humans learning new words)", "+(few-shot learning) is similar to +(humans learning new words)", "+(few-shot learning) can be thought of as +(humans learning new words)", "+(few-shot learning) can be compared to +(humans learning new words)", "machine learning +(few-shot learning AND analogy)", "machine learning +(\"few-shot learning is like\")", "machine learning +(\"few-shot learning is similar\")", "machine learning +(\"just as few-shot learning\")", "machine learning +(\"few-shot learning can be thought of as\")", "machine learning +(\"few-shot learning can be compared to\")"]}