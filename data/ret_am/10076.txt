{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Opening The Black</b> <b>Box</b>\u2014<b>Interpretability</b> <b>In Deep Learning</b>", "url": "https://opendatascience.com/opening-the-black-box-interpretability-in-deep-learning/", "isFamilyFriendly": true, "displayUrl": "https://opendatascience.com/<b>opening-the-black</b>-<b>box</b>-<b>interpretability</b>-<b>in-deep-learning</b>", "snippet": "Editor\u2019s Note: See Joris and Matteo at their tutorial \u201c<b>Opening The Black</b> <b>Box</b> \u2014 <b>Interpretability</b> <b>in Deep Learning</b>\u201d at ODSC Europe 2019 this November 20th in London. Why <b>interpretability</b>? In the last decade, the application of deep neural networks to long-standing problems has brought a breakthrough in performance and prediction power. However, high accuracy, deriving from the increased model complexity, often comes at the price of loss of <b>interpretability</b>, i.e., many of these models ...", "dateLastCrawled": "2022-01-18T13:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Interpretability part 3: opening the</b> <b>black</b> <b>box</b> with LIME and SHAP ...", "url": "https://www.kdnuggets.com/2019/12/interpretability-part-3-lime-shap.html", "isFamilyFriendly": true, "displayUrl": "https://www.<b>kdnuggets</b>.com/2019/12/<b>interpretability</b>-part-3-lime-shap.html", "snippet": "<b>Interpretability part 3: opening the</b> <b>black</b> <b>box</b> with LIME and SHAP = Previous post. Next post =&gt; Tags: Explainability, ... Instead of looking at the model and trying to come <b>up</b> with global explanations <b>like</b> feature importance, these sets of methods look at every single prediction and then try to explain them. 5) Local Interpretable Model-agnostic Explanations (LIME) As the name suggests, this is a model agnostic technique to generate local explanations to the model. The core idea behind the ...", "dateLastCrawled": "2022-01-30T19:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Opening</b> the <b>Black</b> <b>Box</b>: Interpretable <b>Machine Learning</b> for Geneticists ...", "url": "https://www.sciencedirect.com/science/article/pii/S016895252030069X", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S016895252030069X", "snippet": "For example, to generate a surrogate model for <b>a black</b> <b>box</b> model that can predict gene upregulation using regulatory elements as features, we would first apply the <b>black</b> <b>box</b> model to a set of genes, G, and extract the <b>black</b> <b>box</b> predicted label (i.e., <b>up</b>- or downregulated) for those genes (Figure 1B).", "dateLastCrawled": "2021-12-30T12:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Opening</b> the <b>Black</b> <b>Box</b>: Machine Learning <b>Interpretability</b> and Inference ...", "url": "https://link.springer.com/chapter/10.1007/978-3-030-66891-4_3", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/chapter/10.1007/978-3-030-66891-4_3", "snippet": "To address the <b>black</b> <b>box</b> critique of machine learning models, we apply and compare two variables attribution methods: permutation importance and Shapley values. While the aggregate information derived from both approaches is broadly in line, Shapley values offer several advantages, such as the discovery of unknown functional forms in the data generating process and the ability to perform statistical inference. The latter is achieved by the Shapley regression framework, which allows for the ...", "dateLastCrawled": "2022-01-25T15:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Interpretability: Cracking open the black</b> <b>box</b> \u2013 Part III \u2013 Deep &amp; Shallow", "url": "https://deep-and-shallow.com/2019/11/24/interpretability-cracking-open-the-black-box-part-iii/", "isFamilyFriendly": true, "displayUrl": "https://deep-and-shallow.com/2019/11/24/<b>interpretability-cracking-open-the-black</b>-<b>box</b>...", "snippet": "<b>Interpretability: Cracking open the black</b> <b>box</b> \u2013 Part III. Previously, we looked at the pitfalls with the default \u201c feature importance \u201d in tree based models, talked about permutation importance, LOOC importance, and Partial Dependence Plots. Now let\u2019s switch lanes and look at a few model agnostic techniques which takes a bottom-<b>up</b> way ...", "dateLastCrawled": "2022-02-03T10:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Opening</b> the <b>black</b> <b>box</b>: an introduction to Model Explainability", "url": "https://www.jigso.com/opening-the-black-box-an-introduction-to-model-explainability/", "isFamilyFriendly": true, "displayUrl": "https://www.jigso.com/<b>opening</b>-the-<b>black</b>-<b>box</b>-an-introduction-to-model-explainability", "snippet": "Since Amazon mainly recruited male candidates for technical jobs <b>like</b> software engineering and solutions architects, the algorithm learned to penalise resumes that pointed to female candidates by detecting words <b>like</b> \u201cwomen\u201d or \u201cwomen\u2019s\u201d, and to recognise graduates from all-women\u2019s colleges. Evidently, the program was shut down after finding out that the model exhibited discriminatory behaviour towards female candidates.", "dateLastCrawled": "2022-02-01T04:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Forget the &#39;<b>Black</b> <b>Box</b>&#39; Issue or How to Make Users Trust AI", "url": "https://competera.net/resources/articles/black-box-vs-explainable-ai-interpretability", "isFamilyFriendly": true, "displayUrl": "https://competera.net/resources/articles/<b>black</b>-<b>box</b>-vs-explainable-ai-<b>interpretability</b>", "snippet": "Ideally <b>opening</b> them <b>up</b> so that people can actually see how they work. This is not easy for anyone to do\u201d ... <b>like</b> Competera. And this is why every provider is eager to overcome the &#39;<b>black</b> <b>box</b>&#39; challenge and make users trust the algorithms. At Competera, we&#39;ve managed to gain remarkable results on the way to explainable AI. Let&#39;s see how. Competera <b>interpretability</b> features. Frankly, the story of IBM Watson and other pioneering AI-driven projects is familiar to us at Competera. We&#39;ve all ...", "dateLastCrawled": "2022-01-29T01:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Opening</b> the <b>black</b> <b>box</b>: <b>Uncovering the leader trait paradigm</b> through ...", "url": "https://www.sciencedirect.com/science/article/pii/S1048984321000205", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1048984321000205", "snippet": "Next, to address the <b>interpretability</b> dilemma, we open <b>up</b> the RF <b>black</b> <b>box</b> using various analytical procedures. By demonstrating these procedures, we guide scholars on how to get a sense of the complexity modeled by machine learning models. This step has the potential to increase the understanding of the leader trait paradigm, inspire theory-building, and provide input for subsequent hypothesis-testing. Here, we try to answer the following three research questions:", "dateLastCrawled": "2021-12-22T13:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Interpretability</b>, Explainability, and Machine Learning \u2013 What Data ...", "url": "https://www.kdnuggets.com/2020/11/interpretability-explainability-machine-learning.html", "isFamilyFriendly": true, "displayUrl": "https://www.<b>kdnuggets</b>.com/2020/11/<b>interpretability</b>-explainability-machine-learning.html", "snippet": "Interpretable Machine Learning: A Guide for Making <b>Black</b> <b>Box</b> Models Explainable, free online book by Christoph Molnar &quot;Stop explaining <b>black</b> <b>box</b> machine learning models for high stakes decisions and use interpretable models instead,&quot; article by Cynthia Rudin in Nature Machine Intelligence; Original. Reposted with permission.", "dateLastCrawled": "2022-01-25T14:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Interpretability</b> 2020", "url": "https://ff06-2020.fastforwardlabs.com/", "isFamilyFriendly": true, "displayUrl": "https://ff06-2020.fastforwardlabs.com", "snippet": "The question of <b>interpretability</b> has been important in applied machine learning for many years, but as <b>black</b>-<b>box</b> techniques <b>like</b> deep learning grow in popularity, it\u2019s becoming an urgent concern. These techniques offer breakthrough capabilities in analyzing and even generating rich media and text data. These systems are so effective in part because they abstract out the need for manual feature engineering. This allows for automated systems that are able to do completely new things, but are ...", "dateLastCrawled": "2022-01-16T21:49:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Opening</b> the <b>Black</b> <b>Box</b>: Machine Learning <b>Interpretability</b> and Inference ...", "url": "https://link.springer.com/chapter/10.1007/978-3-030-66891-4_3", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/chapter/10.1007/978-3-030-66891-4_3", "snippet": "To address the <b>black</b> <b>box</b> critique of machine learning models, we apply and compare two variables attribution methods: permutation importance and Shapley values. While the aggregate information derived from both approaches is broadly in line, Shapley values offer several advantages, such as the discovery of unknown functional forms in the data generating process and the ability to perform statistical inference. The latter is achieved by the Shapley regression framework, which allows for the ...", "dateLastCrawled": "2022-01-25T15:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Opening</b> the <b>Black</b> <b>Box</b>: Interpretable <b>Machine Learning</b> for Geneticists ...", "url": "https://www.sciencedirect.com/science/article/pii/S016895252030069X", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S016895252030069X", "snippet": "For example, to generate a surrogate model for <b>a black</b> <b>box</b> model that can predict gene upregulation using regulatory elements as features, we would first apply the <b>black</b> <b>box</b> model to a set of genes, G, and extract the <b>black</b> <b>box</b> predicted label (i.e., <b>up</b>- or downregulated) for those genes (Figure 1B).", "dateLastCrawled": "2021-12-30T12:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Interpretability: Cracking open the black</b> <b>box</b> \u2013 Part III \u2013 Deep &amp; Shallow", "url": "https://deep-and-shallow.com/2019/11/24/interpretability-cracking-open-the-black-box-part-iii/", "isFamilyFriendly": true, "displayUrl": "https://deep-and-shallow.com/2019/11/24/<b>interpretability-cracking-open-the-black</b>-<b>box</b>...", "snippet": "<b>Interpretability: Cracking open the black</b> <b>box</b> \u2013 Part III. Previously, we looked at the pitfalls with the default \u201c feature importance \u201d in tree based models, talked about permutation importance, LOOC importance, and Partial Dependence Plots. Now let\u2019s switch lanes and look at a few model agnostic techniques which takes a bottom-<b>up</b> way ...", "dateLastCrawled": "2022-02-03T10:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Opening</b> the <b>Black Box: Interpretable Machine Learning for Geneticists</b> ...", "url": "https://www.cell.com/trends/genetics/fulltext/S0168-9525(20)30069-X", "isFamilyFriendly": true, "displayUrl": "https://www.cell.com/trends/genetics/fulltext/S0168-9525(20)30069-X", "snippet": "a family of interpretation strategies that involve training an inherently interpretable model (e.g., a linear model) using the same data as <b>a black</b> <b>box</b> model to approximate the predictions of the <b>black</b> <b>box</b> model. Training. the process of identifying the best parameters to make <b>up</b> a model: the learning part in ML.", "dateLastCrawled": "2021-11-23T12:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Opening</b> the <b>black</b> <b>box</b>: <b>interpretability</b> of machine learning algorithms ...", "url": "https://www.researchgate.net/publication/355577606_Opening_the_black_box_interpretability_of_machine_learning_algorithms_in_electrocardiography", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/355577606_<b>Opening</b>_the_<b>black</b>_<b>box</b>...", "snippet": "Request PDF | <b>Opening</b> the <b>black</b> <b>box</b>: <b>interpretability</b> of machine learning algorithms in electrocardiography | Recent studies have suggested that cardiac abnormalities can be detected from the ...", "dateLastCrawled": "2022-01-07T03:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Interpretability part 3: opening the</b> <b>black</b> <b>box</b> with LIME and SHAP ...", "url": "https://www.kdnuggets.com/2019/12/interpretability-part-3-lime-shap.html", "isFamilyFriendly": true, "displayUrl": "https://www.<b>kdnuggets</b>.com/2019/12/<b>interpretability</b>-part-3-lime-shap.html", "snippet": "<b>Interpretability part 3: opening the</b> <b>black</b> <b>box</b> with LIME and SHAP = Previous post. Next post =&gt; Tags: Explainability, ... Non-redundant coverage makes sure that the optimization is not picking instances with <b>similar</b> explanations. The advantages of the technique are: Both the methodology and the explanations are very intuitive to explain to a human being. The explanations generated are sparse, and thereby increasing <b>interpretability</b>. Model-agnostic; LIME works for structured as well as ...", "dateLastCrawled": "2022-01-30T19:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Forget the &#39;<b>Black</b> <b>Box</b>&#39; Issue or How to Make Users Trust AI", "url": "https://competera.net/resources/articles/black-box-vs-explainable-ai-interpretability", "isFamilyFriendly": true, "displayUrl": "https://competera.net/resources/articles/<b>black</b>-<b>box</b>-vs-explainable-ai-<b>interpretability</b>", "snippet": "Competera <b>interpretability</b> features. Frankly, the story of IBM Watson and other pioneering AI-driven projects is familiar to us at Competera. We&#39;ve all been there: end-users get an advanced AI solution, but the recommendations generated by the algorithm are questioned as long as there is <b>a &#39;black</b> <b>box</b>&#39; and lack of transparency.", "dateLastCrawled": "2022-01-29T01:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>UX defines chasm between explainable vs</b>. interpretable AI", "url": "https://www.techtarget.com/searchenterpriseai/feature/UX-defines-chasm-between-explainable-vs-interpretable-AI", "isFamilyFriendly": true, "displayUrl": "https://www.techtarget.com/searchenterpriseai/feature/UX-defines-chasm-between...", "snippet": "<b>Opening</b> the <b>black</b> <b>box</b>. Discussions about explainable vs. interpretable AI are important when data scientists are choosing among different algorithms. Transparent models are better from development, deployment and privacy perspectives, but <b>black</b> <b>box</b> models perform better, Nichols said. This is not right in practice, as there are specific problems where <b>black</b> <b>box</b> models perform better, like machine translation, speech to text/text to speech and robotics. &quot;If the performance <b>is similar</b>, then ...", "dateLastCrawled": "2022-01-24T11:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Interpretability</b> 2020", "url": "https://ff06-2020.fastforwardlabs.com/", "isFamilyFriendly": true, "displayUrl": "https://ff06-2020.fastforwardlabs.com", "snippet": "<b>Black</b>-<b>box</b> <b>Interpretability</b>. If you won\u2019t or can\u2019t change your model, or you didn\u2019t make it and don\u2019t have access to its internals, white-<b>box</b> approaches are not useful. In this extremely common situation, you need an approach that allows you to interpret <b>a black</b>-<b>box</b> model. Thanks to recent research, this is not only possible, but relatively simple. FIGURE 3.12 New techniques can make highly accurate neural network algorithms much more interpretable. Until recently, the usual way to ...", "dateLastCrawled": "2022-01-16T21:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Counterfactual explanations without <b>opening</b> the <b>black</b> <b>box</b>", "url": "https://jolt.law.harvard.edu/assets/articlePDFs/v31/Counterfactual-Explanations-without-Opening-the-Black-Box-Sandra-Wachter-et-al.pdf", "isFamilyFriendly": true, "displayUrl": "https://jolt.law.harvard.edu/assets/articlePDFs/v31/Counterfactual-Explanations...", "snippet": "though such <b>interpretability</b> is of great importance and should be pur-sued, explanations can, in principle, be offered without <b>opening</b> the \u201c<b>black</b> <b>box</b>.\u201d Looking at explanations as a means to help a data subject act rather than merely understand, one could gauge the scope and content of explanations according to the specific goal or action they are intended to support. Explanations can serve many purposes. To investigate the poten-tial scope of explanations, it seems reasonable to start ...", "dateLastCrawled": "2022-02-02T11:24:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Opening</b> the <b>black</b> <b>box</b>: <b>interpretability</b> of machine learning algorithms ...", "url": "https://www.researchgate.net/publication/355577606_Opening_the_black_box_interpretability_of_machine_learning_algorithms_in_electrocardiography", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/355577606_<b>Opening</b>_the_<b>black</b>_<b>box</b>...", "snippet": "Request PDF | <b>Opening</b> the <b>black</b> <b>box</b>: <b>interpretability</b> of machine learning algorithms in electrocardiography | Recent studies have suggested that cardiac abnormalities <b>can</b> be detected from the ...", "dateLastCrawled": "2022-01-07T03:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>ML Interpretability - What it is, and</b> how it affects us - Adatis", "url": "https://adatis.co.uk/ml-interpretability-what-it-is-and-how-it-affects-us/", "isFamilyFriendly": true, "displayUrl": "https://adatis.co.uk/<b>ml-interpretability-what-it-is-and</b>-how-it-affects-us", "snippet": "In a world where <b>black</b> <b>box</b> models, such as neural networks, are becoming more and more widespread, the need to be able to explain their operation becomes greater and greater. These models take the input details, process them, and output a prediction, without any reasoning. The <b>opening</b> example is tongue-in-cheek, but it touches on the potential moral dilemmas that <b>can</b> be created when <b>interpretability</b> is not considered in the development of machine learning based applications.", "dateLastCrawled": "2022-01-25T23:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Democratised AI \u2014 The Black Box</b> Problem - Adolfo Eliaz\u00e0t - Artificial ...", "url": "https://adolfoeliazat.com/2021/05/04/democratised-ai-the-black-box-problem/", "isFamilyFriendly": true, "displayUrl": "https://adolfoeliazat.com/2021/05/04/<b>democratised-ai-the-black-box</b>-problem", "snippet": "<b>Interpretability</b> in Machine Learning is not a well defined concept, because it is context dependent and will mean different things for different problems. But, arguably, one <b>can</b> take it to mean <b>opening</b> <b>up</b> the magic <b>black</b> <b>box</b>, rendering it transparent, and being able to interpret, explain and trust what is going on inside it. Speaking in general ...", "dateLastCrawled": "2022-01-26T10:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Interpretability</b> 2020", "url": "https://ff06-2020.fastforwardlabs.com/", "isFamilyFriendly": true, "displayUrl": "https://ff06-2020.fastforwardlabs.com", "snippet": "<b>Black</b>-<b>box</b> <b>Interpretability</b>. If you won\u2019t or <b>can</b>\u2019t change your model, or you didn\u2019t make it and don\u2019t have access to its internals, white-<b>box</b> approaches are not useful. In this extremely common situation, you need an approach that allows you to interpret <b>a black</b>-<b>box</b> model. Thanks to recent research, this is not only possible, but relatively simple. FIGURE 3.12 New techniques <b>can</b> make highly accurate neural network algorithms much more interpretable. Until recently, the usual way to ...", "dateLastCrawled": "2022-01-16T21:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Opening</b> the <b>Black</b> <b>Box</b>: Interpretable Machine Learning for Geneticists ...", "url": "https://www.researchgate.net/publication/340696607_Opening_the_Black_Box_Interpretable_Machine_Learning_for_Geneticists", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/340696607_<b>Opening</b>_the_<b>Black</b>_<b>Box</b>_Interpretable...", "snippet": "Thus, to summarize the previous statements, one <b>can</b> say that if a machine learning model has higher <b>interpretability</b>, then comprehending the behavior behind the prediction that the <b>black</b> <b>box</b> model ...", "dateLastCrawled": "2022-01-18T06:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>interpretable-machine-learning.pdf - Interpretable Machine</b> Learning A ...", "url": "https://www.coursehero.com/file/73385309/interpretable-machine-learningpdf/", "isFamilyFriendly": true, "displayUrl": "https://www.coursehero.com/file/73385309/<b>interpretable-machine</b>-learningpdf", "snippet": "Preface Machine learning has great potential for improving products, processes and research. But computers usually do not explain their predictions which is a barrier to the adoption of machine learning. This book is about making machine learning models and their decisions interpretable. After exploring the concepts of <b>interpretability</b>, you will learn about simple, in-terpretable models such as decision trees, decision rules and linear regression. Later chapters focus on general model ...", "dateLastCrawled": "2021-12-24T20:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Democratised AI \u2014 The Black Box Problem</b> | by Hanne-Torill Mevik | Tech ...", "url": "https://medium.com/tech-accounts/democratized-ai-the-black-box-problem-d70145e4a606", "isFamilyFriendly": true, "displayUrl": "https://<b>medium</b>.com/tech-accounts/democratized-ai-the-<b>black-box-problem</b>-d70145e4a606", "snippet": "But, arguably, one <b>can</b> take it to mean <b>opening</b> <b>up</b> the magic <b>black</b> <b>box</b>, rendering it transparent, and being able to interpret, explain and trust what is going on inside it. Speaking in general ...", "dateLastCrawled": "2022-01-27T10:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Opening</b> the <b>Black</b> <b>Box</b>: Attempts to Understand the Results of Machine ...", "url": "https://speakerdeck.com/datasciencela/opening-the-black-box-attempts-to-understand-the-results-of-machine-learning-models-michael-tiernay-la-data-science-meetup-may-2017", "isFamilyFriendly": true, "displayUrl": "https://speakerdeck.com/datasciencela/<b>opening</b>-the-<b>black</b>-<b>box</b>-attempts-to-understand-the...", "snippet": "<b>Opening</b> the <b>Black</b> <b>Box</b>: Attempts to Understand the Results of Machine Learning Models - Michael Tiernay - LA Data Science Meetup - May 2017. Data Science LA . May 12, 2017 Tweet Share More Decks by Data Science LA. See All by Data Science LA . datasciencela 1 240. datasciencela 4 25k. datasciencela 8 700k. datasciencela 1 120. datasciencela 8 9.8k. datasciencela 5 1.5k. datasciencela 3 860. datasciencela 2 890. datasciencela 2 660. Featured. See All Featured . productmarketing 11 1.3k ...", "dateLastCrawled": "2022-01-09T11:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Thoughts On The Two Cultures Of Statistical Modeling \u2013 Will Koehrsen ...", "url": "https://willkoehrsen.github.io/statictics/modeling/thoughts/thoughts-on-the-two-cultures-of-statistical-modeling/", "isFamilyFriendly": true, "displayUrl": "https://willkoehrsen.github.io/statictics/modeling/<b>thoughts</b>/<b>thoughts</b>-on-the-two...", "snippet": "A central idea in the algorithmic community is that nature is <b>a black</b>-<b>box</b>, and our models are also <b>a black</b> <b>box</b>, although one that <b>can</b> give us predictions on new observations. There is little use trying to explain a model that is not accurate, so concentrate primarily on building the model with the greatest performance before focusing on learning anything about nature from it. An accurate model, no matter how complex, is more useful for both prediction (clearly) and for information gathering ...", "dateLastCrawled": "2022-02-02T04:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Stop explaining <b>black</b> <b>box</b> machine learning models for high stakes ...", "url": "https://www.nature.com/articles/s42256-019-0048-x", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s42256-019-0048-x", "snippet": "<b>Black</b> <b>box</b> machine learning models are currently being used for high-stakes decision making throughout society, causing problems in healthcare, criminal justice and other domains. Some people hope ...", "dateLastCrawled": "2022-01-30T00:29:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Opening</b> the <b>Black</b> <b>Box</b>: Interpretable <b>Machine Learning</b> for Geneticists ...", "url": "https://www.sciencedirect.com/science/article/pii/S016895252030069X", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S016895252030069X", "snippet": "In such a case, one <b>can</b> train a more interpretable model to approximate the <b>black</b> <b>box</b> model. Examples of interpretable models include linear models where coefficients reflect feature importance, or decision trees where mean decrease node impurity <b>can</b> be calculated. These inherently interpretable models are referred to as surrogate models. For example, to generate a surrogate model for <b>a black</b> <b>box</b> model that <b>can</b> predict gene upregulation using regulatory elements as features, we would first ...", "dateLastCrawled": "2021-12-30T12:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Opening</b> the <b>Black</b> <b>Box</b>: Machine Learning <b>Interpretability</b> and Inference ...", "url": "https://link.springer.com/chapter/10.1007/978-3-030-66891-4_3", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/chapter/10.1007/978-3-030-66891-4_3", "snippet": "To address the <b>black</b> <b>box</b> critique of machine learning models, we apply and compare two variables attribution methods: permutation importance and Shapley values. While the aggregate information derived from both approaches is broadly in line, Shapley values offer several advantages, such as the discovery of unknown functional forms in the data generating process and the ability to perform statistical inference. The latter is achieved by the Shapley regression framework, which allows for the ...", "dateLastCrawled": "2022-01-25T15:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Opening</b> the <b>Black Box: Interpretable Machine Learning for Geneticists</b> ...", "url": "https://www.cell.com/trends/genetics/fulltext/S0168-9525(20)30069-X", "isFamilyFriendly": true, "displayUrl": "https://www.cell.com/trends/genetics/fulltext/S0168-9525(20)30069-X", "snippet": "In such a case, one <b>can</b> train a more interpretable model to approximate the <b>black</b> <b>box</b> model. Examples of interpretable models include linear models where coefficients reflect feature importance, or decision trees where mean decrease node impurity <b>can</b> be calculated. These inherently interpretable models are referred to as surrogate models. For example, to generate a surrogate model for <b>a black</b> <b>box</b> model that <b>can</b> predict gene upregulation using regulatory elements as features, we would first ...", "dateLastCrawled": "2021-11-23T12:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Opening</b> the <b>black</b> <b>box</b>: <b>Uncovering the leader trait paradigm</b> through ...", "url": "https://www.sciencedirect.com/science/article/pii/S1048984321000205", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1048984321000205", "snippet": "Next, to address the <b>interpretability</b> dilemma, we open <b>up</b> the RF <b>black</b> <b>box</b> using various analytical procedures. By demonstrating these procedures, we guide scholars on how to get a sense of the complexity modeled by machine learning models. This step has the potential to increase the understanding of the leader trait paradigm, inspire theory-building, and provide input for subsequent hypothesis-testing. Here, we try to answer the following three research questions:", "dateLastCrawled": "2021-12-22T13:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Interpretability part 3: opening the</b> <b>black</b> <b>box</b> with LIME and SHAP ...", "url": "https://www.kdnuggets.com/2019/12/interpretability-part-3-lime-shap.html", "isFamilyFriendly": true, "displayUrl": "https://www.<b>kdnuggets</b>.com/2019/12/<b>interpretability</b>-part-3-lime-shap.html", "snippet": "<b>Interpretability part 3: opening the</b> <b>black</b> <b>box</b> with LIME and SHAP = Previous post. Next post =&gt; Tags: Explainability, ... TreeSHAP solves it to some extent, but it is still slow <b>compared</b> to most of the other techniques we discussed. KernelSHAP is just slow and becomes infeasible to calculate for larger datasets. (Although there are techniques like clustering using K-means to reduce the dataset before calculating the Shapely values, they are still slow) SHAP values <b>can</b> be misinterpretedas it ...", "dateLastCrawled": "2022-01-30T19:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Explainable AI: A Review of Machine Learning <b>Interpretability</b> Methods", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7824368/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7824368", "snippet": "3.1.2. <b>Interpretability</b> Methods to Explain any <b>Black</b>-<b>Box</b> Model . This section focuses on <b>interpretability</b> techniques, which <b>can</b> be applied to any <b>black</b>-<b>box</b> model. First introduced in , the local interpretable model-agnostic explanations (LIME) method is one of the most popular <b>interpretability</b> methods for <b>black</b>-<b>box</b> models. Following a simple ...", "dateLastCrawled": "2022-01-29T03:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Interpretability: Cracking open the black</b> <b>box</b> \u2013 Part III \u2013 Deep &amp; Shallow", "url": "https://deep-and-shallow.com/2019/11/24/interpretability-cracking-open-the-black-box-part-iii/", "isFamilyFriendly": true, "displayUrl": "https://deep-and-shallow.com/2019/11/24/<b>interpretability-cracking-open-the-black</b>-<b>box</b>...", "snippet": "<b>Interpretability: Cracking open the black</b> <b>box</b> \u2013 Part III. Previously, we looked at the pitfalls with the default \u201c feature importance \u201d in tree based models, talked about permutation importance, LOOC importance, and Partial Dependence Plots. Now let\u2019s switch lanes and look at a few model agnostic techniques which takes a bottom-<b>up</b> way ...", "dateLastCrawled": "2022-02-03T10:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Forget the &#39;<b>Black</b> <b>Box</b>&#39; Issue or How to Make Users Trust AI", "url": "https://competera.net/resources/articles/black-box-vs-explainable-ai-interpretability", "isFamilyFriendly": true, "displayUrl": "https://competera.net/resources/articles/<b>black</b>-<b>box</b>-vs-explainable-ai-<b>interpretability</b>", "snippet": "As you <b>can</b> see, AI and ML open <b>up</b> enormous opportunities for retailers. But once again, the problem is the lack of trust which undermines the mass adoption of solutions, like Competera. And this is why every provider is eager to overcome the &#39;<b>black</b> <b>box</b>&#39; challenge and make users trust the algorithms. At Competera, we&#39;ve managed to gain ...", "dateLastCrawled": "2022-01-29T01:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Opening</b> the <b>black</b> <b>box</b> of <b>artificial intelligence</b> for clinical decision ...", "url": "https://journals.plos.org/plosone/article?id=10.1371%2Fjournal.pone.0231166", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0231166", "snippet": "Such methods <b>can</b> be tailored to one specific original <b>black</b> <b>box</b> algorithm, or <b>can</b> be generalized like the LIME algorithm . We would like to stress that no standardization of these terms currently exists. Thus, in the presented work, explainability is mainly examined from a clinical point-of-view, highlighting the ability of humans to understand which clinical features drive the prediction. This is important, as a major goal of clinical predictive modeling is the development of clinical ...", "dateLastCrawled": "2021-08-27T17:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Opening</b> the algorithm\u2019s <b>black</b> <b>box</b> and understand its outputs | by Amine ...", "url": "https://medium.com/@asaboni/opening-the-algorithms-black-box-and-understand-its-outputs-e2363b0a887c", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@asaboni/<b>opening</b>-the-algorithms-<b>black</b>-<b>box</b>-and-understand-its...", "snippet": "<b>Opening</b> the algorithm\u2019s <b>black</b> <b>box</b> and understand its outputs. Amine Saboni. Apr 1, 2020 \u00b7 12 min read. The use of data processing algorithms -from the simple SQL query to the Tech giants ...", "dateLastCrawled": "2021-11-04T08:19:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Interpretability</b> in <b>Machine</b> <b>Learning</b>: An Overview", "url": "https://thegradient.pub/interpretability-in-ml-a-broad-overview/", "isFamilyFriendly": true, "displayUrl": "https://thegradient.pub/<b>interpretability</b>-in-ml-a-broad-overview", "snippet": "First, <b>interpretability</b> in <b>machine</b> <b>learning</b> is useful because it can aid in trust. As humans, we may be reluctant to rely on <b>machine</b> <b>learning</b> models for certain critical tasks, e.g., medical diagnosis, unless we know &quot;how they work.&quot; There&#39;s often a fear of the unknown when trusting in something opaque, which we see when people confront new technology, and this can slow down adoption. Approaches to <b>interpretability</b> that focus on transparency could help mitigate some of these fears.", "dateLastCrawled": "2022-02-01T15:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "6 \u2013 <b>Interpretability</b> \u2013 <b>Machine</b> <b>Learning</b> Blog | ML@CMU | Carnegie Mellon ...", "url": "https://blog.ml.cmu.edu/2020/08/31/6-interpretability/", "isFamilyFriendly": true, "displayUrl": "https://blog.ml.cmu.edu/2020/08/31/6-<b>interpretability</b>", "snippet": "Figure 1: <b>Interpretability</b> for <b>machine</b> <b>learning</b> models bridges the concrete objectives models optimize for and the real-world (and less easy to define) desiderata that ML applications aim to achieve. Introduction . The objectives <b>machine</b> <b>learning</b> models optimize for do not always reflect the actual desiderata of the task at hand. <b>Interpretability</b> in models allows us to evaluate their decisions and obtain information that the objective alone cannot confer. <b>Interpretability</b> takes many forms ...", "dateLastCrawled": "2022-02-03T05:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Interpretability</b> in <b>Machine</b> <b>Learning</b>", "url": "https://www.cl.cam.ac.uk/teaching/1819/P230/IWML-Lecture-4.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cl.cam.ac.uk/teaching/1819/P230/IWML-Lecture-4.pdf", "snippet": "This provides a novel <b>analogy</b> between data compression and regularization. Qualitative and quantitative state-of-the-art results on three datasets. 20 / 33. Interpretable Lens Variable Model (ILVM) 21 / 33. Interactive <b>Interpretability</b> via Active <b>Learning</b> Interactive \u2018human-in-the-loop\u2019 <b>interpretability</b> Choose the point with index j that maximizes : ^j = argmax jI(s ; ) = H(s ) E q \u02da(z js)[H(s jz j)] = Z p(s j)log p(s j)ds + E q \u02da(z js) Z p (s jjz)log p (s jjz)ds : (5) Choose the point ...", "dateLastCrawled": "2022-01-19T17:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Towards Analogy-Based Explanations in Machine Learning</b> | DeepAI", "url": "https://deepai.org/publication/towards-analogy-based-explanations-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>towards-analogy-based-explanations-in-machine-learning</b>", "snippet": "More specifically, we take the view that an <b>analogy</b>-based approach is a viable alternative to existing approaches in the realm of explainable AI and interpretable <b>machine</b> <b>learning</b>, and that <b>analogy</b>-based explanations of the predictions produced by a <b>machine</b> <b>learning</b> algorithm can complement similarity-based explanations in a meaningful way. To corroborate these claims, we outline the basic idea of an <b>analogy</b>-based explanation and illustrate its potential usefulness by means of some examples.", "dateLastCrawled": "2022-01-10T12:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Towards <b>Analogy</b>-Based Explanations in <b>Machine</b> <b>Learning</b>", "url": "https://link.springer.com/chapter/10.1007/978-3-030-57524-3_17", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/chapter/10.1007/978-3-030-57524-3_17", "snippet": "More specifically, we take the view that an <b>analogy</b>-based approach is a viable alternative to existing approaches in the realm of explainable AI and interpretable <b>machine</b> <b>learning</b>, and that <b>analogy</b>-based explanations of the predictions produced by a <b>machine</b> <b>learning</b> algorithm can complement similarity-based explanations in a meaningful way. To corroborate these claims, we outline the basic idea of an <b>analogy</b>-based explanation and illustrate its potential usefulness by means of some examples.", "dateLastCrawled": "2022-01-16T03:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Towards <b>Analogy</b>-Based Explanations in <b>Machine</b> <b>Learning</b>", "url": "https://arxiv.org/abs/2005.12800", "isFamilyFriendly": true, "displayUrl": "https://<b>arxiv</b>.org/abs/2005.12800", "snippet": "Principles of analogical reasoning have recently been applied in the context of <b>machine</b> <b>learning</b>, for example to develop new methods for classification and preference <b>learning</b>. In this paper, we argue that, while analogical reasoning is certainly useful for constructing new <b>learning</b> algorithms with high predictive accuracy, is is arguably not less interesting from an <b>interpretability</b> and explainability point of view. More specifically, we take the view that an <b>analogy</b>-based approach is a ...", "dateLastCrawled": "2021-10-24T20:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Confusion Matrices</b> &amp; <b>Interpretable ML</b> | by andrea b | high stakes ...", "url": "https://medium.com/high-stakes-design/interpretability-techniques-explained-in-simple-terms-f5e1573674f3", "isFamilyFriendly": true, "displayUrl": "https://medium.com/high-stakes-design/<b>interpretability</b>-techniques-explained-in-simple...", "snippet": "The best [<b>analogy</b>] I can think of is an indicator light in your car \u2014 [and the] <b>machine</b> that you plug in to tell you more about the readout. ANDREA: Do you see <b>interpretability</b>, primarily, as ...", "dateLastCrawled": "2021-03-22T05:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Economic Methodology Meets Interpretable Machine Learning</b> - Part I ...", "url": "https://bcmullins.github.io/economic_methodology_interpretable_ml_blackboxes/", "isFamilyFriendly": true, "displayUrl": "https://bcmullins.github.io/economic_methodology_interpretable_ml_blackboxes", "snippet": "In this series of posts, we will develop an <b>analogy</b> between the realistic assumptions debate in economic methodology and the current discussion over <b>interpretability</b> when using <b>machine</b> <b>learning</b> models in the wild. While this connection may seem fuzzy at first, the past seventy years or so of economic methodology offers many lessons for <b>machine</b> <b>learning</b> theorists and practitioners to avoid analysis paralysis and make progress on the <b>interpretability</b> issue - one way or the other. Intro - Part ...", "dateLastCrawled": "2022-01-22T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Economic Methodology Meets Interpretable <b>Machine</b> <b>Learning</b> ...", "url": "https://bcmullins.github.io/economic_methodology_interpretable_ml_intro/", "isFamilyFriendly": true, "displayUrl": "https://bcmullins.github.io/economic_methodology_interpretable_ml_intro", "snippet": "In this series of posts, we will develop an <b>analogy</b> between the realistic assumptions debate in economic methodology and the current discussion over <b>interpretability</b> when using <b>machine</b> <b>learning</b> models in the wild. While this connection may seem fuzzy at first, the past seventy years or so of economic methodology offers many lessons for <b>machine</b> <b>learning</b> theorists and practitioners to avoid analysis paralysis and make progress on the <b>interpretability</b> issue - one way or the other. But first ...", "dateLastCrawled": "2022-01-05T13:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Analogies between Biology and Deep <b>Learning</b> [rough note] -- colah&#39;s blog", "url": "http://colah.github.io/notes/bio-analogies/", "isFamilyFriendly": true, "displayUrl": "colah.github.io/notes/bio-analogies", "snippet": "Neuroscience \u2194 <b>Interpretability</b>. <b>Analogy</b>: model=brain. Artificial neural networks are historically inspired by neuroscience, but I used to be pretty skeptical that the connection was anything more than superficial. I&#39;ve since come around: I now think this is a very deep connection. The thing that personally persuaded me was that, in my own investigations of what goes on inside neural networks, we kept finding things that were previously discovered by neuroscientists. The most recent ...", "dateLastCrawled": "2022-01-30T16:04:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Chris Olah on what the hell is going on inside neural networks - 80,000 ...", "url": "https://80000hours.org/podcast/episodes/chris-olah-interpretability-research/", "isFamilyFriendly": true, "displayUrl": "https://80000hours.org/podcast/episodes/chris-olah-interpretability-research", "snippet": "Chris is a <b>machine</b> <b>learning</b> researcher currently focused on neural network interpretability. Until last December he led OpenAI\u2019s interpretability team but along with some colleagues he recently moved on to help start a new AI lab focussed on large models and safety called Anthropic. Rob Wiblin: Before OpenAI he spent 4 years at Google Brain developing tools to visualize what\u2019s going on in neural networks. Chris was hugely impactful at Google Brain. He was second author on the launch of ...", "dateLastCrawled": "2022-02-01T04:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Marketing AI: Interpretability and Explainability</b> - Christopher S. Penn ...", "url": "https://www.christopherspenn.com/2021/03/marketing-ai-interpretability-and-explainability/", "isFamilyFriendly": true, "displayUrl": "https://www.christopherspenn.com/2021/03/<b>marketing-ai-interpretability-and-explainability</b>", "snippet": "<b>Interpretability is like</b> inspecting the baker\u2019s recipe for the cake. We look at the list of ingredients and the steps taken to bake the cake, and we verify that the recipe makes sense and the ingredients were good. This is a much more rigorous way of validating our results, but it\u2019s the most complete \u2013 and if we\u2019re in a high-stakes situation where we need to remove all doubt, this is the approach we take. Interpretability in AI is like that \u2013 we step through the code itself that ...", "dateLastCrawled": "2022-01-29T12:09:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Causal <b>Learning</b> From Predictive Modeling for Observational Data", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7931928/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7931928", "snippet": "Given the recent success of <b>machine</b> <b>learning</b>, specifically deep <b>learning</b>, in several applications (Goodfellow et al., ... This statistical <b>interpretability is similar</b> in spirit to traditional interpretability. This allows to answer questions, such as \u201cdoes BMI influence susceptibility to Covid?\u201d Moreover, it has been argued that developing an effective CBN for practical applications requires expert knowledge when data collection is cumbersome (Fenton and Neil, 2012). This applies to ...", "dateLastCrawled": "2021-12-09T23:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Optimal <b>Predictive Clustering</b> - Dimitris Bertsimas", "url": "https://dbertsim.mit.edu/pdfs/papers/2020-sobiesk-optimal-predictive-clustering.pdf", "isFamilyFriendly": true, "displayUrl": "https://dbertsim.mit.edu/pdfs/papers/2020-sobiesk-optimal-<b>predictive-clustering</b>.pdf", "snippet": "Table 1 Comparison of major <b>machine</b> <b>learning</b> methods relative to each other across the metrics of performance (out-of-sample R2), scalability and interpretability. 1 is the best, while 5 is the worst. Optimal <b>Predictive Clustering</b> 3 From Table 1, we observe all existing methods have weakness in at least one category. We therefore seek to design a method that has strong performance in all three categories at the same time. Optimal <b>Predictive Clustering</b> (OPC) is an algorithm that uses mixed ...", "dateLastCrawled": "2021-11-24T20:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Reviewing Challenges of Predicting Protein Melting Temperature Change ...", "url": "https://link.springer.com/article/10.1007/s12033-021-00349-0", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s12033-021-00349-0", "snippet": "Predicting the effects of mutations on protein stability is a key problem in fundamental and applied biology, still unsolved even for the relatively simple case of small, soluble, globular, monomeric, two-state-folder proteins. Many articles discuss the limitations of prediction methods and of the datasets used to train them, which result in low reliability for actual applications despite globally capturing trends. Here, we review these and other issues by analyzing one of the most detailed ...", "dateLastCrawled": "2022-02-03T02:54:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Interpretable Machine Learning: Advantages and Disadvantages</b> | by ...", "url": "https://towardsdatascience.com/interpretable-machine-learning-advantages-and-disadvantages-901769f48c43", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>interpretable-machine-learning-advantages-and</b>...", "snippet": "In my view, a shor t coming of interpretable <b>machine</b> <b>learning</b> is that it assumes to a degree that the data being fed into the model is always going to be suitable for human interpretation. This is not necessarily the case. For instance, let\u2019s say that a company is trying to implement interpretable <b>machine</b> <b>learning</b> to devise a credit scoring model, whereby prospective credit card applications are classified as approved or rejected based on numerous features. It is often the case that such ...", "dateLastCrawled": "2022-01-19T03:58:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Breaking the interpretability barrier - a</b> method for interpreting deep ...", "url": "http://www.di.uniba.it/~loglisci/NFMCP2019/NFMCP/nfMCP2019_paper_17.pdf", "isFamilyFriendly": true, "displayUrl": "www.di.uniba.it/~loglisci/NFMCP2019/NFMCP/nfMCP2019_paper_17.pdf", "snippet": "Last, but not least, <b>interpretability can be thought of as</b> a useful tool for understanding and correcting model errors. In general, we are faced with a trade-o between performance and inter-pretability. Graph classi cation is normally a domain which requires the ap-plication of complex <b>learning</b> models, such as deep neural networks, which are not interpretable by nature. Several relevant attempts have been made to in-terpret complex models post-hoc (brie y reviewed in section2). However, most ...", "dateLastCrawled": "2021-09-22T17:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Discovering Discriminative Nodes for Classi\ufb01cation with Deep Graph ...", "url": "http://muresanlab.tins.ro/publications/preprints/Palcu_et_al_LNAI_2020.pdf", "isFamilyFriendly": true, "displayUrl": "muresanlab.tins.ro/publications/preprints/Palcu_et_al_LNAI_2020.pdf", "snippet": "Last, but not least, <b>interpretability can be thought of as</b> a useful tool for understanding and correcting model errors. In general, we are faced with a trade-o\ufb00 between performance and inter-pretability. Graph classi\ufb01cation is normally a domain which requires the appli-cation of complex <b>learning</b> models, such as deep neural networks, which are not interpretable by nature. Several relevant attempts have been made to interpret complex models post-hoc (brie\ufb02y reviewed in Sect.2). However ...", "dateLastCrawled": "2021-09-02T02:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Confronting Abusive Language Online: A Survey from the Ethical and ...", "url": "https://www.arxiv-vanity.com/papers/2012.12305/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2012.12305", "snippet": "The pervasiveness of abusive content on the internet can lead to severe psychological and physical harm. Significant effort in Natural Language Processing (NLP) research has been devoted to addressing this problem through abusive content detection and related sub-areas, such as the detection of hate speech, toxicity, cyberbullying, etc. Although current technologies achieve high classification performance in research studies, it has been observed that the real-life application of this ...", "dateLastCrawled": "2021-10-13T19:21:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(interpretability)  is like +(opening up a black box)", "+(interpretability) is similar to +(opening up a black box)", "+(interpretability) can be thought of as +(opening up a black box)", "+(interpretability) can be compared to +(opening up a black box)", "machine learning +(interpretability AND analogy)", "machine learning +(\"interpretability is like\")", "machine learning +(\"interpretability is similar\")", "machine learning +(\"just as interpretability\")", "machine learning +(\"interpretability can be thought of as\")", "machine learning +(\"interpretability can be compared to\")"]}