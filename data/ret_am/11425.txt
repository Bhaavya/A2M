{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Bayesian Optimization</b> - Computer Science", "url": "https://www.cs.uic.edu/~hjin/files/bayesian_opt.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.uic.edu/~hjin/files/<b>bayesian</b>_opt.pdf", "snippet": "<b>Experience</b>: assign hyperparameters based on expert knowledges Grid search: search a hypercube of the hyperparameters Random search: sample the hypercubc uniformly, better than grid search, but still expensive 1 <b>Bayesian optimization</b>(BO): search the domain based on the Gaussian processes 1 Bergstra and Bengio, JMLR 2012 6. 1D BO at First Glimpse 7. animation by animate[2016/03/15] Pre-knowledge. Gaussian Process (GP) Optimizing over the function )predict the function based on \\small set&quot; of ...", "dateLastCrawled": "2022-02-03T01:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Bayesian</b> <b>optimization</b> with adaptive surrogate models for automated ...", "url": "https://www.nature.com/articles/s41524-021-00662-x", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41524-021-00662-x", "snippet": "<b>Bayesian</b> <b>optimization</b> (BO) is an indispensable tool to optimize objective functions that either do not have known functional forms or are expensive to evaluate. Currently, optimal experimental ...", "dateLastCrawled": "2022-01-30T19:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Bayesian Optimization</b>: fine-tuning black-box processes", "url": "https://www.innovating-automation.blog/bayesian-optimization/", "isFamilyFriendly": true, "displayUrl": "https://www.innovating-automation.blog/<b>bayesian-optimization</b>", "snippet": "AutoML (automated machine <b>learning</b>) is one of the recent paradigm-breaking developments in machine <b>learning</b>. New libraries such as HyperOpt allow data scientists and machine <b>learning</b> practitioners to save time spent on selecting, tuning and evaluating different algorithms, tasks that are often very time consuming. Some of these libraries \u2013 HyperOpt, for example \u2013 use a technique called <b>Bayesian Optimization</b>. Instead of randomly or exhaustively iterating through combinations of algorithms ...", "dateLastCrawled": "2022-02-03T16:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Bayesian</b> <b>Optimization</b> for a Better Dessert", "url": "https://bayesopt.github.io/papers/2017/37.pdf", "isFamilyFriendly": true, "displayUrl": "https://bayesopt.github.io/papers/2017/37.pdf", "snippet": "Our <b>experience</b> highlights the importance of incorporating domain expertise and the value of transfer <b>learning</b> approaches. 1 Introduction: The Challenge of Chocolate-Chip Cookies <b>Bayesian</b> <b>Optimization</b> and black-box <b>optimization</b> are used extensively to optimize hyperparameters in machine <b>learning</b> (e.g. [13, 11, 3]) but less so outside that area, and even less so in \ufb01elds <b>like</b> the culinary arts. We conjecture that the primary barrier to adoption is not technical, but rather cultural and ...", "dateLastCrawled": "2022-02-01T02:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>A Note on Bayesian Optimisation</b>", "url": "https://www.perceptronai.net/post/a-note-on-bayesian-optimisation", "isFamilyFriendly": true, "displayUrl": "https://www.perceptronai.net/post/<b>a-note-on-bayesian-optimisation</b>", "snippet": "<b>Bayesian</b> <b>optimization</b> incorporates prior belief about f and updates the prior with samples drawn from f to get a posterior that better approximates f. Configurations with the better approximation are tested on the system, and the process repeats. The performance of a <b>Bayesian</b> <b>optimization</b> algorithm is therefore determined by three components: the surrogate model, the acquisition function, and the methods that numerically optimize the acquisition function <b>like</b> gradient descent.", "dateLastCrawled": "2022-01-29T18:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A Conceptual Explanation of <b>Bayesian</b> Hyperparameter <b>Optimization</b> for ...", "url": "https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-conceptual-explanation-of-<b>bayesian</b>-model-based-hyper...", "snippet": "The aim of hyperparameter <b>optimization</b> in machine <b>learning</b> is to find the hyperparameters of a given machine <b>learning</b> algorithm that return the best performance as measured on a validation set. (Hyperparameters, in contrast to model parameters, are set by the machine <b>learning</b> engineer before training. The number of trees in a random forest is a hyperparameter while the weights in a neural network are model parameters learned during training. I <b>like</b> to think of hyperparameters as the model ...", "dateLastCrawled": "2022-02-02T22:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Controller Learning using Bayesian Optimization</b> | Autonomous Motion ...", "url": "https://am.is.mpg.de/research_projects/cont-learn-bayes-opt", "isFamilyFriendly": true, "displayUrl": "https://am.is.mpg.de/research_projects/cont-learn-bayes-opt", "snippet": "<b>Bayesian</b> <b>optimization</b> is proposed for automatic <b>learning</b> of optimal controller parameters from experimental data. A probabilistic description (a Gaussian process) is used to model the unknown function from controller parameters to a user-defined cost. The probabilistic model is updated with data, which is obtained by testing a set of parameters on the physical system and evaluating the cost. In order to learn fast, the <b>Bayesian</b> <b>optimization</b> algorithm selects the next parameters to evaluate ...", "dateLastCrawled": "2022-01-23T01:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "How <b>does Bayesian optimization and Active Learning differ</b>? - Quora", "url": "https://www.quora.com/How-does-Bayesian-optimization-and-Active-Learning-differ", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/How-<b>does-Bayesian-optimization-and-Active-Learning-differ</b>", "snippet": "Answer (1 of 2): <b>Bayesian</b> <b>Optimization</b> goal is to optimize a black box function. You don\u2019t know anything about that function. It could be convex or non-convex, or multimodal. The only thing you know about the function is you can query points to evaluate and get the function values. However, these...", "dateLastCrawled": "2022-01-26T12:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Are there <b>any alternatives to Bayesian optimization</b>? - Quora", "url": "https://www.quora.com/Are-there-any-alternatives-to-Bayesian-optimization", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Are-there-<b>any-alternatives-to-Bayesian-optimization</b>", "snippet": "Answer (1 of 2): Reinforcement <b>Learning</b>, Genetic Algorithms are two common methods which can almost always (Reinforcement <b>Learning</b> can work on specific types of loss functions only) be used to solve the problems you solve with <b>Bayesian</b> <b>Optimization</b>.", "dateLastCrawled": "2022-01-07T04:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Automated Machine <b>Learning</b> <b>Hyperparameter</b> Tuning in Python | by Will ...", "url": "https://towardsdatascience.com/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/automated-machine-<b>learning</b>-<b>hyperparameter</b>-tuning-in...", "snippet": "A complete walk through using <b>Bayesian</b> <b>optimization</b> for automated <b>hyperparameter</b> tuning in Python. Will Koehrsen . Jul 3, 2018 \u00b7 18 min read. Tuning machine <b>learning</b> hyperparameters is a tedious yet crucial task, as the performance of an algorithm can be highly dependent on the choice of hyperparameters. Manual tuning takes time away from important steps of the machine <b>learning</b> pipeline <b>like</b> feature engineering and interpreting results. Grid and random search are hands-off, but require long ...", "dateLastCrawled": "2022-02-02T08:04:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Bayesian</b> <b>optimization</b> A tutorial on", "url": "https://ziw.mit.edu/pub/bayesopt_tutorial.pdf", "isFamilyFriendly": true, "displayUrl": "https://ziw.mit.edu/pub/bayesopt_tutorial.pdf", "snippet": "<b>Bayesian</b> <b>Optimization</b> Idea: build a probabilistic model of the function f LOOP \u2022choose new query point(s) to evaluate \u2022update model decision criterion: acquisition function Zi Wang - BayesOpt / 5. Gaussian Processes (GPs) \u2022probability distribution over functions \u2022any finite set of function values is a multi-variate Gaussian Input t Zi Wang - BayesOpt / 6. Gaussian Processes (GPs) \u2022kernel function ; mean function \u2022function ; observe noisy output at \u2022probability distribution over ...", "dateLastCrawled": "2022-01-29T09:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Bayesian</b> <b>optimization</b> with adaptive surrogate models for automated ...", "url": "https://www.nature.com/articles/s41524-021-00662-x", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41524-021-00662-x", "snippet": "<b>Bayesian</b> <b>optimization</b> (BO) is an indispensable tool to optimize objective functions that either do not have known functional forms or are expensive to evaluate. Currently, optimal experimental ...", "dateLastCrawled": "2022-01-30T19:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Benchmarking the performance of <b>Bayesian</b> <b>optimization</b> across multiple ...", "url": "https://www.nature.com/articles/s41524-021-00656-9", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41524-021-00656-9", "snippet": "<b>Bayesian</b> <b>optimization</b> (BO) 3,4,5, one class of active <b>learning</b> methods, utilizes a surrogate model to approximate a mapping from experiment parameters to an objective criterion, and provides ...", "dateLastCrawled": "2022-01-30T11:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Are there <b>any alternatives to Bayesian optimization</b>? - Quora", "url": "https://www.quora.com/Are-there-any-alternatives-to-Bayesian-optimization", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Are-there-<b>any-alternatives-to-Bayesian-optimization</b>", "snippet": "Answer (1 of 2): Reinforcement <b>Learning</b>, Genetic Algorithms are two common methods which can almost always (Reinforcement <b>Learning</b> can work on specific types of loss functions only) be used to solve the problems you solve with <b>Bayesian</b> <b>Optimization</b>.", "dateLastCrawled": "2022-01-07T04:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Bayesian Optimization</b>: fine-tuning black-box processes", "url": "https://www.innovating-automation.blog/bayesian-optimization/", "isFamilyFriendly": true, "displayUrl": "https://www.innovating-automation.blog/<b>bayesian-optimization</b>", "snippet": "AutoML (automated machine <b>learning</b>) is one of the recent paradigm-breaking developments in machine <b>learning</b>. New libraries such as HyperOpt allow data scientists and machine <b>learning</b> practitioners to save time spent on selecting, tuning and evaluating different algorithms, tasks that are often very time consuming. Some of these libraries \u2013 HyperOpt, for example \u2013 use a technique called <b>Bayesian Optimization</b>. Instead of randomly or exhaustively iterating through combinations of algorithms ...", "dateLastCrawled": "2022-02-03T16:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Bayesian</b> <b>Experience</b> Reuse for <b>Learning</b> from Multiple Demonstrators ...", "url": "https://ui.adsabs.harvard.edu/abs/2020arXiv200605725G/abstract", "isFamilyFriendly": true, "displayUrl": "https://ui.adsabs.harvard.edu/abs/2020arXiv200605725G/abstract", "snippet": "<b>Learning</b> from demonstrations (LfD) improves the exploration efficiency of a <b>learning</b> agent by incorporating demonstrations from experts. However, demonstration data can often come from multiple experts with conflicting goals, making it difficult to incorporate safely and effectively in online settings. We address this problem in the static and dynamic <b>optimization</b> settings by modelling the uncertainty in source and target task functions using normal-inverse-gamma priors, whose corresponding ...", "dateLastCrawled": "2020-06-12T04:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Generalizing Transfer Bayesian Optimization to</b> Source-Target ...", "url": "https://ieeexplore.ieee.org/abstract/document/9180071/", "isFamilyFriendly": true, "displayUrl": "https://ieeexplore.ieee.org/abstract/document/9180071", "snippet": "For this reason, domain experts are always in high demand, as they are able to harness their <b>experience</b> of <b>similar</b> problems to come up with fast solutions in difficult situations. However, domain experts are not easy to find. Given this fact, the present paper puts forth a method for automating the process of knowledge extraction (through experiential <b>learning</b>) and transfer across problems in the domain of computationally expensive black-box <b>optimization</b>. The key novelty and motivation of ...", "dateLastCrawled": "2021-12-20T14:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Bayesian</b> <b>Optimization</b> with a Prior for the Optimum | SpringerLink", "url": "https://link.springer.com/chapter/10.1007/978-3-030-86523-8_17", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/chapter/10.1007/978-3-030-86523-8_17", "snippet": "While <b>Bayesian</b> <b>Optimization</b> (BO) is a very popular method for optimizing expensive black-box functions, it fails to leverage the <b>experience</b> of domain experts. This causes BO to waste function evaluations on bad design choices (e.g., machine <b>learning</b> hyperparameters) that the expert already knows to work poorly. To address this issue, we introduce <b>Bayesian</b> <b>Optimization</b> with a Prior for the Optimum (BOPrO). BOPrO allows users to inject their knowledge into the <b>optimization</b> process in the form ...", "dateLastCrawled": "2021-12-30T16:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "How <b>does Bayesian optimization and Active Learning differ</b>? - Quora", "url": "https://www.quora.com/How-does-Bayesian-optimization-and-Active-Learning-differ", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/How-<b>does-Bayesian-optimization-and-Active-Learning-differ</b>", "snippet": "Answer (1 of 2): <b>Bayesian</b> <b>Optimization</b> goal is to optimize a black box function. You don\u2019t know anything about that function. It could be convex or non-convex, or multimodal. The only thing you know about the function is you can query points to evaluate and get the function values. However, these...", "dateLastCrawled": "2022-01-26T12:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "[1710.06219] <b>Learning</b> to <b>Warm-Start Bayesian Hyperparameter Optimization</b>", "url": "https://arxiv.org/abs/1710.06219", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/abs/1710.06219", "snippet": "Hyperparameter <b>optimization</b> aims to find the optimal hyperparameter configuration of a machine <b>learning</b> model, which provides the best performance on a validation dataset. Manual search usually leads to get stuck in a local hyperparameter configuration, and heavily depends on human intuition and <b>experience</b>. A simple alternative of manual search is random/grid search on a space of hyperparameters, which still undergoes extensive evaluations of validation errors in order to find its best ...", "dateLastCrawled": "2019-02-26T09:05:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Bayesian</b> <b>Learning</b> - Rebellion Research", "url": "https://www.rebellionresearch.com/bayesian-learning", "isFamilyFriendly": true, "displayUrl": "https://www.rebellionresearch.com/<b>bayesian</b>-<b>learning</b>", "snippet": "First, there needs to exist an <b>experience</b> set, sometimes called a training set. This is data that the algorithm will \u201clearn\u201d from. Next, ... Least Squares Regression <b>can</b> <b>be thought</b> of as a very limited <b>learning</b> algorithm, where the training set consists of a number of x and y data pairs. The task would be trying to predict the y value, and the performance measure would be the sum of the squared differences between the predicted and actual y\u2019s. Of course, generally speaking, we wouldn ...", "dateLastCrawled": "2022-01-19T02:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Deep <b>Learning</b> with <b>Bayesian</b> Inference \u2013 The AI Researcher Blog", "url": "https://ai-researcher.com/2021/10/28/deep-learning-with-bayesian-inference/", "isFamilyFriendly": true, "displayUrl": "https://ai-researcher.com/2021/10/28/deep-<b>learning</b>-with-<b>bayesian</b>-inference", "snippet": "As we improve our decision-making processes using previous <b>experience</b>, <b>learning</b> <b>can</b> be seen as a <b>Bayesian</b> process. In the brain, we yet don\u2019t know the detailed processes leading to <b>learning</b>. However, it is <b>thought</b> that neurons learn using Hebbian <b>Learning</b> and Spike-Timing Dependent Plasticity (STDP) rules. In machine <b>learning</b>, and especially Deep <b>Learning</b>, most of the <b>learning</b> algorithms rely on supervised <b>learning</b> based on flavors of backpropagation algorithms. Recently, spiking neural ...", "dateLastCrawled": "2022-01-30T17:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Bayesian</b> reaction <b>optimization</b> as a tool for chemical synthesis | <b>Nature</b>", "url": "https://www.nature.com/articles/s41586-021-03213-y", "isFamilyFriendly": true, "displayUrl": "https://www.<b>nature</b>.com/articles/s41586-021-03213-y", "snippet": "<b>Bayesian</b> <b>optimization</b>, an iterative response surface-based global <b>optimization</b> algorithm, has demonstrated exceptional performance in the tuning of machine <b>learning</b> models3. <b>Bayesian</b> <b>optimization</b> ...", "dateLastCrawled": "2022-01-31T11:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "How <b>does Bayesian optimization and Active Learning differ</b>? - Quora", "url": "https://www.quora.com/How-does-Bayesian-optimization-and-Active-Learning-differ", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/How-<b>does-Bayesian-optimization-and-Active-Learning-differ</b>", "snippet": "Answer (1 of 2): <b>Bayesian</b> <b>Optimization</b> goal is to optimize a black box function. You don\u2019t know anything about that function. It could be convex or non-convex, or multimodal. The only thing you know about the function is you <b>can</b> query points to evaluate and get the function values. However, these...", "dateLastCrawled": "2022-01-26T12:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Maximum Likelihood vs. <b>Bayesian</b> Parameter Estimation", "url": "https://www.ccs.neu.edu/home/rjw/csg220/lectures/MLE-vs-Bayes.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.ccs.neu.edu/home/rjw/csg220/lectures/MLE-vs-Bayes.pdf", "snippet": "Kcan <b>be thought</b> of as \u201cimaginary\u201d counts from our prior <b>experience</b> Equivalent sample size = \u03b1 1+\u2026+\u03b1 K The larger the equivalent sample size the more confident we are in our prior 20 Effect of Priors Prediction of P(X=H ) after seeing data with N H= 0.25\u2022N Tfor different sample sizes 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.55 0 20 40 60 ...", "dateLastCrawled": "2022-01-30T19:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Bayesian Optimization - Part 1: Stochastic Processes</b>", "url": "https://www.surajx.in/2019/07/bayesian-optimization---part-1-stochastic-processes/", "isFamilyFriendly": true, "displayUrl": "https://www.surajx.in/2019/07/<b>bayesian-optimization---part-1-stochastic-processes</b>", "snippet": "Introduction More often than not, the difference between a crappy and powerful implementation of a Machine <b>Learning</b> (ML) algorithm is the choice of its hyperparameters. Hyperparameter Tuning was considered an artistic skill that ML practitioners acquired with <b>experience</b>. Over the past few years several advancements have been made in this area to perform hyperparameter tuning in a more informed manner. Techniques such as <b>Bayesian</b> <b>Optimization</b>, Neural Architecture Search, Probabilistic Matrix ...", "dateLastCrawled": "2022-01-24T11:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Beyond Grid Search: Hypercharge Hyperparameter Tuning for <b>XGBoost</b> | by ...", "url": "https://towardsdatascience.com/beyond-grid-search-hypercharge-hyperparameter-tuning-for-xgboost-7c78f7a2929d", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/beyond-grid-search-hypercharge-hyperparameter-tuning...", "snippet": "<b>Bayesian</b> <b>optimization</b> of machine <b>learning</b> model hyperparameters works faster and better than grid search. Here\u2019s how we <b>can</b> speed up hyperparameter tuning with 1) <b>Bayesian</b> <b>optimization</b> with Hyperopt\u2026 Get started. Open in app. Sign in. Get started. Follow. 618K Followers \u00b7 Editors&#39; Picks Features Deep Dives Grow Contribute. About. Get started. Open in app. Beyond Grid Search: Hypercharge Hyperparameter Tuning for <b>XGBoost</b>. Using Hyperopt, Optuna, and Ray Tune to Accelerate Machine ...", "dateLastCrawled": "2022-02-03T01:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "[D] <b>Bayesian</b> <b>Optimization</b>: does it work? : MachineLearning", "url": "https://www.reddit.com/r/MachineLearning/comments/fsupfu/d_bayesian_optimization_does_it_work/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/Machine<b>Learning</b>/comments/fsupfu/d_<b>bayesian</b>_<b>optimization</b>_does...", "snippet": "<b>Bayesian</b> <b>optimization</b> (with Gaussian processes) is the only <b>optimization</b> method (which I know about) that gives you an idea of how extensively you have sampled your search space. By plotting the utility (acquisition function optimum) at each iteration, you will notice that it tends to converge to some value over time. If you notice this convergence happen, you <b>can</b> be confident that you have thoroughly explored the parameter space. It is not perfect of course, but it", "dateLastCrawled": "2021-09-03T23:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Are there <b>any alternatives to Bayesian optimization</b>? - Quora", "url": "https://www.quora.com/Are-there-any-alternatives-to-Bayesian-optimization", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Are-there-<b>any-alternatives-to-Bayesian-optimization</b>", "snippet": "Answer (1 of 2): Reinforcement <b>Learning</b>, Genetic Algorithms are two common methods which <b>can</b> almost always (Reinforcement <b>Learning</b> <b>can</b> work on specific types of loss functions only) be used to solve the problems you solve with <b>Bayesian</b> <b>Optimization</b>.", "dateLastCrawled": "2022-01-07T04:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Experiences with bayesian hyperparameter optimization</b>? : MachineLearning", "url": "https://www.reddit.com/r/MachineLearning/comments/2m1cad/experiences_with_bayesian_hyperparameter/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/Machine<b>Learning</b>/comments/2m1cad/<b>experiences</b>_with_<b>bayesian</b>...", "snippet": "&quot;A Tutorial on <b>Bayesian</b> <b>Optimization</b> of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement <b>Learning</b>&quot;, 2010. A long tutorial (49 pages) which gives you a good introduction into the field, including several acquisition functions. Sversky, Snoek et.al., &quot;Freeze-Thaw <b>Bayesian</b> <b>Optimization</b>&quot;, 2014. A ...", "dateLastCrawled": "2021-01-10T13:17:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Comparing <b>Bayesian</b> and Classical <b>Learning</b> Techniques for Solving ...", "url": "https://wso2.com/blog/research/comparing-bayesian-and-classical-learning-techniques/", "isFamilyFriendly": true, "displayUrl": "https://wso2.com/blog/research/comparing-<b>bayesian</b>-and-classical-<b>learning</b>-techniques", "snippet": "However, <b>Bayesian</b> with MCMC sampling <b>can</b> be inefficient <b>compared</b> to MAP estimation or other machine <b>learning</b> in terms of the time take to learn the parameters (train the model). The <b>Bayesian</b> classifiers that use MCMC sampling failed to converge to a more optimal solution from the given starting step for Glass dataset, However, we were unable to conclude the exact reasons for that observation.", "dateLastCrawled": "2022-01-29T08:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Bayesian</b> <b>optimization</b> with adaptive surrogate models for automated ...", "url": "https://www.nature.com/articles/s41524-021-00662-x", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41524-021-00662-x", "snippet": "<b>Bayesian</b> <b>optimization</b> (BO) is an indispensable tool to optimize objective functions that either do not have known functional forms or are expensive to evaluate. Currently, optimal experimental ...", "dateLastCrawled": "2022-01-30T19:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Benchmarking the performance of <b>Bayesian</b> <b>optimization</b> across multiple ...", "url": "https://www.nature.com/articles/s41524-021-00656-9", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41524-021-00656-9", "snippet": "<b>Bayesian</b> <b>optimization</b> (BO) 3,4,5, one class of active <b>learning</b> methods, utilizes a surrogate model to approximate a mapping from experiment parameters to an objective criterion, and provides ...", "dateLastCrawled": "2022-01-30T11:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Practical Bayesian Optimization of Machine</b> <b>Learning</b> Algorithms | DeepAI", "url": "https://deepai.org/publication/practical-bayesian-optimization-of-machine-learning-algorithms", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>practical-bayesian-optimization-of-machine</b>-<b>learning</b>...", "snippet": "Unfortunately, this tuning is often a &quot;black art&quot; that requires expert <b>experience</b>, unwritten rules of thumb, or sometimes brute-force search. Much more appealing is the idea of developing automatic approaches which <b>can</b> optimize the performance of a given <b>learning</b> algorithm to the task at hand. In this work, we consider the automatic tuning problem within the framework of <b>Bayesian</b> <b>optimization</b>, in which a <b>learning</b> algorithm&#39;s generalization performance is modeled as a sample from a Gaussian ...", "dateLastCrawled": "2021-12-13T23:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Bayesian</b> <b>Optimization</b> in Robot <b>Learning</b> - Automatic Controller Tuning ...", "url": "https://ics.is.mpg.de/publications/bayesian", "isFamilyFriendly": true, "displayUrl": "https://ics.is.mpg.de/publications/<b>bayesian</b>", "snippet": "We propose \u201cfailures-aware excursion search\u201d, a novel algorithm for <b>Bayesian</b> <b>optimization</b> under black-box constraints, where failures are limited in number. Our results in numerical benchmarks indicate that by allowing a confined number of failures, better optima are revealed as <b>compared</b> with state-of-the-art methods. The first contribution of this thesis, \u201cautomatic LQR tuning\u201d, lies among the first on applying BO to real robots. While it demonstrated automatic controller <b>learning</b> ...", "dateLastCrawled": "2022-02-02T21:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A Conceptual Explanation of <b>Bayesian</b> Hyperparameter <b>Optimization</b> for ...", "url": "https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-conceptual-explanation-of-<b>bayesian</b>-model-based-hyper...", "snippet": "The aim of hyperparameter <b>optimization</b> in machine <b>learning</b> is to find the hyperparameters of a given machine <b>learning</b> algorithm that return the best performance as measured on a validation set. (Hyperparameters, in contrast to model parameters, are set by the machine <b>learning</b> engineer before training. The number of trees in a random forest is a hyperparameter while the weights in a neural network are model parameters learned during training. I like to think of hyperparameters as the model ...", "dateLastCrawled": "2022-02-02T22:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Meta-Learning Acquisition Functions for</b> <b>Bayesian</b> <b>Optimization</b> | DeepAI", "url": "https://deepai.org/publication/meta-learning-acquisition-functions-for-bayesian-optimization", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/.../<b>meta-learning-acquisition-functions-for</b>-<b>bayesian</b>-<b>optimization</b>", "snippet": "In an offline training phase on such a set of functions the algorithm <b>can</b> then be adjusted to a particular type of <b>optimization</b> problems with the goal of exhibiting superior performance <b>compared</b> with general-purpose <b>optimization</b> strategies in the subsequent application. For meta-<b>learning</b> global black-box <b>optimization</b>, such cheap training sets <b>can</b> be obtained in various ways. For example, in the context of hyperparameter <b>optimization</b> for NN training", "dateLastCrawled": "2022-01-04T09:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Bayesian</b> <b>Optimization</b> for a Better Dessert", "url": "https://bayesopt.github.io/papers/2017/37.pdf", "isFamilyFriendly": true, "displayUrl": "https://bayesopt.github.io/papers/2017/37.pdf", "snippet": "<b>Bayesian</b> <b>Optimization</b> for a Better Dessert Greg Kochanski, Daniel Golovin, John Karro, Benjamin Solnik, Subhodeep Moitra, and D. Sculley {gpk, dgg, karro, bsolnik, smoitra, dsculley}@google.com ; Google Brain Team Abstract We present a case study on applying <b>Bayesian</b> <b>Optimization</b> to a complex real-world system; our challenge was to optimize chocolate chip cookies. The process was a mixed-initiative system where both human chefs, human raters, and a machine optimizer participated in 144 ...", "dateLastCrawled": "2022-02-01T02:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Beyond Grid Search: Hypercharge Hyperparameter Tuning for <b>XGBoost</b> | by ...", "url": "https://towardsdatascience.com/beyond-grid-search-hypercharge-hyperparameter-tuning-for-xgboost-7c78f7a2929d", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/beyond-grid-search-hypercharge-hyperparameter-tuning...", "snippet": "<b>Bayesian</b> <b>optimization</b> of machine <b>learning</b> model hyperparameters works faster and better than grid search. Here\u2019s how we <b>can</b> speed up hyperparameter tuning with 1) <b>Bayesian</b> <b>optimization</b> with Hyperopt\u2026 Get started. Open in app. Sign in. Get started. Follow. 618K Followers \u00b7 Editors&#39; Picks Features Deep Dives Grow Contribute. About. Get started. Open in app. Beyond Grid Search: Hypercharge Hyperparameter Tuning for <b>XGBoost</b>. Using Hyperopt, Optuna, and Ray Tune to Accelerate Machine ...", "dateLastCrawled": "2022-02-03T01:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "[D] <b>Bayesian</b> <b>Optimization</b>: does it work? : MachineLearning", "url": "https://www.reddit.com/r/MachineLearning/comments/fsupfu/d_bayesian_optimization_does_it_work/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/Machine<b>Learning</b>/comments/fsupfu/d_<b>bayesian</b>_<b>optimization</b>_does...", "snippet": "<b>Bayesian</b> <b>optimization</b> (with Gaussian processes) is the only <b>optimization</b> method (which I know about) that gives you an idea of how extensively you have sampled your search space. By plotting the utility (acquisition function optimum) at each iteration, you will notice that it tends to converge to some value over time. If you notice this convergence happen, you <b>can</b> be confident that you have thoroughly explored the parameter space. It is not perfect of course, but it", "dateLastCrawled": "2021-09-03T23:40:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A <b>machine</b> <b>learning</b> approach to <b>Bayesian</b> parameter estimation | npj ...", "url": "https://www.nature.com/articles/s41534-021-00497-w", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41534-021-00497-w", "snippet": "<b>Bayesian</b> estimation is a powerful theoretical paradigm for the operation of the approach to parameter estimation. However, the <b>Bayesian</b> method for statistical inference generally suffers from ...", "dateLastCrawled": "2022-02-03T06:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Bayesian Optimization</b>: fine-tuning black-box processes", "url": "https://www.innovating-automation.blog/bayesian-optimization/", "isFamilyFriendly": true, "displayUrl": "https://www.innovating-automation.blog/<b>bayesian-optimization</b>", "snippet": "AutoML (automated <b>machine</b> <b>learning</b>) is one of the recent paradigm-breaking developments in <b>machine</b> <b>learning</b>. New libraries such as HyperOpt allow data scientists and <b>machine</b> <b>learning</b> practitioners to save time spent on selecting, tuning and evaluating different algorithms, tasks that are often very time consuming. Some of these libraries \u2013 HyperOpt, for example \u2013 use a technique called <b>Bayesian Optimization</b>. Instead of randomly or exhaustively iterating through combinations of algorithms ...", "dateLastCrawled": "2022-02-03T16:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "The Hitchhiker\u2019s Guide to <b>Optimization</b> in <b>Machine Learning</b> | by Aman ...", "url": "https://towardsdatascience.com/the-hitchhikers-guide-to-optimization-in-machine-learning-edcf5a104210", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/the-hitchhikers-guide-to-<b>optimization</b>-in-<b>machine</b>...", "snippet": "Gradient descent is one of the easiest to implement (and arguably one of the worst) <b>optimization</b> algorithms in <b>machine learning</b>. It is a first-order (i.e., gradient-based) <b>optimization</b> algorithm where we iteratively update the parameters of a differentiable cost function until its minimum is attained. Before we understand how gradient descent works, first let us have a look at the generalized formula of GD: Gradient descent (Image by author) The basic idea here is to update the model ...", "dateLastCrawled": "2022-02-02T19:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Machine Learning by Analogy</b> - SlideShare", "url": "https://www.slideshare.net/ColleenFarrelly/machine-learning-by-analogy-59094152", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ColleenFarrelly/<b>machine-learning-by-analogy</b>-59094152", "snippet": "<b>Machine Learning by Analogy</b> 1. Colleen M. Farrelly 2. Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics behind these ...", "dateLastCrawled": "2022-01-31T07:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PPT) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> | Colleen Farrelly - Academia.edu", "url": "https://www.academia.edu/30404581/Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/30404581/<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "<b>Machine</b> <b>Learning</b> by <b>Analogy</b> Colleen M. Farrelly Overview of Problem Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics ...", "dateLastCrawled": "2022-01-02T06:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "TensorFlow is an interface for expressing <b>machine</b> <b>learning</b> algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine</b> <b>learning</b> for high-throughput experimental exploration of metal ...", "url": "https://www.sciencedirect.com/science/article/pii/S2542435121004451", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2542435121004451", "snippet": "The synthesis process is controlled by <b>Bayesian</b> <b>optimization</b> (BO) workflow that can simultaneously optimize the optoelectronic properties by composition selection and processing parameters for thin film materials. The alternative is the microfluidic systems as e.g., developed by Abolhasani et al. Figure 3B). 52, 53, 54 Here, using a modular microfluidic platform enables continuous manufacturing of inorganic MHP QDs guided by an ensemble neural network (ENN) exploration of the colloidal ...", "dateLastCrawled": "2022-01-23T17:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Comparison of Hyperparameter Tuning algorithms: <b>Grid search</b>, Random ...", "url": "https://medium.com/analytics-vidhya/comparison-of-hyperparameter-tuning-algorithms-grid-search-random-search-bayesian-optimization-5326aaef1bd1", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/comparison-of-hyperparameter-tuning-algorithms...", "snippet": "<b>Bayesian</b> <b>optimization</b> is a sequential model-based <b>optimization</b> ... An interesting <b>analogy</b> is to compare this to Bagging Vs Boosting. If you think about it, the idea is very similar! In bagging, we ...", "dateLastCrawled": "2022-01-26T04:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Machine</b> <b>Learning</b> by <b>Analogy</b> II - slideshare.net", "url": "https://www.slideshare.net/ColleenFarrelly/machine-learning-by-analogy-ii", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ColleenFarrelly/<b>machine</b>-<b>learning</b>-by-<b>analogy</b>-ii", "snippet": "Updated <b>Machine</b> <b>Learning</b> by <b>Analogy</b> presentation that builds to more advanced methods (TensorFlow, geometry/topology-based methods...) and adds a section on ti\u2026", "dateLastCrawled": "2021-12-31T12:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Preliminary performance study of a brief review on <b>machine</b> <b>learning</b> ...", "url": "https://link.springer.com/article/10.1007/s12652-021-03427-y", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s12652-021-03427-y", "snippet": "<b>Analogy</b>-based effort estimation is the major task of software engineering which estimates the effort required for new software projects using existing histories for corresponding development and management. In general, the high accuracy of software effort estimation techniques can be a non-solvable problem we named as multi-objective problem. Recently, most of the authors have been used <b>machine</b> <b>learning</b> techniques for the same process however not possible to meet the higher performance ...", "dateLastCrawled": "2022-01-02T12:16:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Bayesian Optimization</b> Concept Explained in Layman Terms | by Wei Wang ...", "url": "https://towardsdatascience.com/bayesian-optimization-concept-explained-in-layman-terms-1d2bcdeaf12f", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>bayesian-optimization</b>-concept-explained-in-layman-terms...", "snippet": "<b>Bayesian Optimization</b> has been widely used for the hyperparameter tuning purpose in the <b>Machine</b> <b>Learning</b> world. Despite the fact that there are many terms and math formulas involved, the concept behind turns out to be very simple. The goal of this article is to share what I learned about <b>Bayesian Optimization</b> with a straight forward interpretation of textbook terminologies, and hopefully, it will help you understand what <b>Bayesian Optimization</b> is in a short period of time. The Overview of ...", "dateLastCrawled": "2022-01-29T21:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Bayesian optimization</b> or <b>gradient descent</b>? - Cross Validated", "url": "https://stats.stackexchange.com/questions/161923/bayesian-optimization-or-gradient-descent", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/161923/<b>bayesian-optimization</b>-or-<b>gradient-descent</b>", "snippet": "The most immediate difference is that <b>Bayesian optimization</b> is applicable when you don&#39;t know the gradients. If you can cheaply compute gradients of your function, you&#39;ll want to use a method that can incorporate those, since they can be extremely helpful in understanding the function. If you can&#39;t easily compute gradients and need to resort to ...", "dateLastCrawled": "2022-02-02T21:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Study Neural Architecture Search", "url": "https://www.cse.cuhk.edu.hk/lyu/_media/students/lyu2002_1st_term_report.pdf?id=students%3Afyp&cache=cache", "isFamilyFriendly": true, "displayUrl": "https://www.cse.cuhk.edu.hk/lyu/_media/students/lyu2002_1st_term_report.pdf?id=students...", "snippet": "searching for the best hyperparameters of a <b>machine</b> <b>learning</b> model to attain the best performance. Common hyperparameters of a model are <b>learning</b> rate, batch size, number of training epoch etc. While it is not the focus of our project, it is worth to mention that hyperparameter optimization overlaps a lot with NAS. We can think of the architecture of a network as one of the hyperparameters of the network. Meta-<b>learning</b> suggests using meta-data to lead the <b>learning</b> of our model. Meta-data is ...", "dateLastCrawled": "2021-12-15T21:44:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "What&#39;s <b>trending in machine learning (outside of deep learning</b>)? - Quora", "url": "https://www.quora.com/Whats-trending-in-machine-learning-outside-of-deep-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Whats-<b>trending-in-machine-learning-outside-of-deep-learning</b>", "snippet": "Answer (1 of 11): I don\u2019t know about trending, but I know of a powerful method (outside of mainstream ML) which is demonstrated to have tremendous flexibility, interpretability, and the advantage of relative ease of implementation in VLSI/FPGA hardware. Volterra Kernels The easiest way to under...", "dateLastCrawled": "2022-01-22T10:52:00.0000000Z", "language": "en", "isNavigational": false}], [], []], "all_bing_queries": ["+(bayesian optimization)  is like +(learning from experience)", "+(bayesian optimization) is similar to +(learning from experience)", "+(bayesian optimization) can be thought of as +(learning from experience)", "+(bayesian optimization) can be compared to +(learning from experience)", "machine learning +(bayesian optimization AND analogy)", "machine learning +(\"bayesian optimization is like\")", "machine learning +(\"bayesian optimization is similar\")", "machine learning +(\"just as bayesian optimization\")", "machine learning +(\"bayesian optimization can be thought of as\")", "machine learning +(\"bayesian optimization can be compared to\")"]}