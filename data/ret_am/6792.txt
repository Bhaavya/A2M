{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Bayesian</b> vs <b>Neural</b> Networks \u2013 Ehud Reiter&#39;s Blog", "url": "https://ehudreiter.com/2021/07/05/bayesian-vs-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://ehudreiter.com/2021/07/05/<b>bayesian</b>-vs-<b>neural</b>-<b>networks</b>", "snippet": "We have a small <b>group</b> that meets monthly to discuss this, and last week we ended up also talking about why <b>people</b> would use <b>Bayesian</b> models instead of <b>neural</b> networks, focusing on medical decision support (not NLP). Which I found very interesting and thought-provoking. One relatively obvious reason to avoid black-box <b>neural</b> models is justifiability (which relates to explanation). Ie, if I build a medical diagnosis system as a <b>Bayesian</b> model, I can explain to doctors (and regulators) exactly ...", "dateLastCrawled": "2022-02-01T00:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Researchers Explore <b>Bayesian</b> <b>Neural</b> Networks -- Pure AI", "url": "https://pureai.com/articles/2021/09/07/bayesian-neural-networks.aspx", "isFamilyFriendly": true, "displayUrl": "https://pureai.com/articles/2021/09/07/<b>bayesian</b>-<b>neural</b>-<b>networks</b>.aspx", "snippet": "The paper &quot;What Are <b>Bayesian</b> <b>Neural</b> <b>Network</b> Posteriors Really <b>Like</b>?&quot; by P. Izmailov, S. Vikram, M. Hoffman and A. Wilson addresses ideas to improve the predictive accuracy of <b>Bayesian</b> <b>neural</b> networks. The beginning of the abstract is: &quot;The posterior over <b>Bayesian</b> <b>neural</b> <b>network</b> (BNN) parameters is extremely high-dimensional and non-convex. For computational reasons, researchers approximate this posterior using inexpensive mini-batch methods such as mean-field variational inference or ...", "dateLastCrawled": "2022-01-30T04:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Bayesian Belief Networks</b>: An Introduction In 6 Easy Points", "url": "https://www.jigsawacademy.com/blogs/data-science/bayesian-belief-network", "isFamilyFriendly": true, "displayUrl": "https://www.<b>jigsawacademy</b>.com/blogs/data-science/<b>bayesian</b>-belief-<b>network</b>", "snippet": "The graph of a <b>Bayesian</b> <b>Network</b> is useful. It is readable to both computers and humans; both can interpret the information, unlike some networks <b>like</b> <b>neural</b> networks, which humans can\u2019t read. Disadvantages. The most significant disadvantage is that there is no universally acknowledged method for constructing networks from data. There have ...", "dateLastCrawled": "2022-02-03T06:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Adaptive behavior and different thermal experiences of real <b>people</b>: A ...", "url": "https://www.sciencedirect.com/science/article/pii/S036013232100281X", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S036013232100281X", "snippet": "In this study, we aim to extend the application of <b>Bayesian</b> <b>neural</b> networks (BNN) in predicting the thermal preference of a large <b>group</b> of individuals. Here we refer BNN to a <b>neural</b> <b>network</b> that is trained to fit observed data using <b>Bayesian</b> inference by considering that the <b>network</b>\u2019s parameters (i.e., its weights and biases) are random according to a prior probability distribution [ 39 ].", "dateLastCrawled": "2022-01-07T06:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A Comprehensive Introduction to <b>Bayesian</b> Deep Learning - Joris Baan", "url": "https://jorisbaan.nl/2021/03/02/introduction-to-bayesian-deep-learning.html", "isFamilyFriendly": true, "displayUrl": "https://jorisbaan.nl/2021/03/02/introduction-to-<b>bayesian</b>-deep-learning.html", "snippet": "A <b>Bayesian</b> <b>Neural</b> <b>Network</b> (BNN) is simply posterior inference applied to a <b>neural</b> <b>network</b> architecture. To be precise, a prior distribution is specified for each weight and bias. Because of their huge parameter space, however, inferring the posterior is even more difficult than usual. So why do <b>Bayesian</b> DL at all? The classic answer is to obtain a realistic expression of uncertainty, or calibration. A classifier is considered calibrated if the probability (confidence) of a class prediction ...", "dateLastCrawled": "2022-02-03T15:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "What&#39;s the likelihood in <b>Bayesian</b> <b>Neural</b> Networks? - Artificial ...", "url": "https://ai.stackexchange.com/questions/26864/whats-the-likelihood-in-bayesian-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://ai.stackexchange.com/.../26864/whats-the-<b>like</b>lihood-in-<b>bayesian</b>-<b>neural</b>-<b>networks</b>", "snippet": "The likelihood depends on the task that you are solving, so this is similar to traditional <b>neural</b> networks (in fact, even these <b>neural</b> networks have a probabilistic/<b>Bayesian</b> interpretation!).. For binary classification, you should probably use a Bernoulli, which, in practice, corresponds to using a sigmoid with a binary cross-entropy (you can show that the minimization of the cross-entropy is equivalent to the maximization of Bernoulli p.m.f.)", "dateLastCrawled": "2022-01-17T07:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "machine learning - Difference between Bayes <b>network</b>, <b>neural</b> <b>network</b> ...", "url": "https://stats.stackexchange.com/questions/94511/difference-between-bayes-network-neural-network-decision-tree-and-petri-nets", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/94511", "snippet": "The <b>Bayesian</b> <b>network</b> is different from the <b>Neural</b> <b>Network</b> in that it is explicit reasoning, even though probabilistic and hence could have multiple stable states based on each step being revisited and modified within legal values, just <b>like</b> an algorithm. It is a robust way to reason probabilistically, but it involves encoding of probabilities, conjecturing the points where randomized actions can happen and hence need more heuristic effort to build.", "dateLastCrawled": "2022-01-28T22:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Bayesian</b> <b>neural</b> networks and out-of-distribution data? - Cross Validated", "url": "https://stats.stackexchange.com/questions/511864/bayesian-neural-networks-and-out-of-distribution-data", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/511864/<b>bayesian</b>-<b>neural</b>-<b>networks</b>-and-out-of...", "snippet": "In a <b>Bayesian</b> <b>neural</b> <b>network</b> (for classification) the posterior predictive distribution is $$ P(y=c \\mid {\\bf x}, \\mathcal D_{train}) = \\int P(y=c \\mid {\\bf x}, \\theta) p(\\theta \\mid \\mathcal D_{train}) d\\theta $$ Let&#39;s assume that we have enough training data $\\mathcal D_{train}$ such that if $\\bf x$ is quite similar to the training data the uncertainty in the posterior (predictive) prediction is low. So, we assume that in this region we have low aleatoric uncertainty and due to the amount ...", "dateLastCrawled": "2022-01-28T06:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "GitHub - mainkoon81/<b>Study-09-MachineLearning-D</b>: **DeepLearning** (CNN ...", "url": "https://github.com/mainkoon81/Study-09-MachineLearning-D", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/mainkoon81/<b>Study-09-MachineLearning-D</b>", "snippet": "<b>Bayesian</b> <b>Neural</b> <b>Network</b>. 10 years ago, <b>people</b> used to think that <b>Bayesian</b> methods are mostly suited for small datasets because it&#39;s computationally expensive. In the era of Big data, our <b>Bayesian</b> methods met deep learning, and <b>people</b> started to make some mixture models that has <b>neural</b> networks inside of a probabilistic model.", "dateLastCrawled": "2021-10-29T10:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "[2104.14421] What Are <b>Bayesian</b> <b>Neural</b> <b>Network</b> Posteriors Really <b>Like</b> ...", "url": "https://www.reddit.com/r/MachineLearning/comments/n1w1aq/210414421_what_are_bayesian_neural_network/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/.../comments/n1w1aq/210414421_what_are_<b>bayesian</b>_<b>neural</b>_<b>network</b>", "snippet": "Abstract: The posterior over <b>Bayesian</b> <b>neural</b> <b>network</b> (BNN) parameters is extremely high-dimensional and non-convex. For computational reasons, researchers approximate this posterior using inexpensive mini-batch methods such as mean-field variational inference or stochastic-gradient Markov chain Monte Carlo (SGMCMC). To investigate foundational questions in <b>Bayesian</b> deep learning, we instead use full-batch Hamiltonian Monte Carlo (HMC) on modern architectures. We show that (1) BNNs can ...", "dateLastCrawled": "2021-06-24T13:36:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Bayesian</b> vs <b>Neural</b> Networks \u2013 Ehud Reiter&#39;s Blog", "url": "https://ehudreiter.com/2021/07/05/bayesian-vs-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://ehudreiter.com/2021/07/05/<b>bayesian</b>-vs-<b>neural</b>-<b>networks</b>", "snippet": "We have a small <b>group</b> that meets monthly to discuss this, and last week we ended up also talking about why <b>people</b> would use <b>Bayesian</b> models instead of <b>neural</b> networks, focusing on medical decision support (not NLP). Which I found very interesting and thought-provoking. One relatively obvious reason to avoid black-box <b>neural</b> models is justifiability (which relates to explanation). Ie, if I build a medical diagnosis system as a <b>Bayesian</b> model, I can explain to doctors (and regulators) exactly ...", "dateLastCrawled": "2022-02-01T00:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Bayesian</b> Fully Convolutional Networks for Brain Image Registration", "url": "https://www.hindawi.com/journals/jhe/2021/5528160/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.hindawi.com</b>/journals/jhe/2021/5528160", "snippet": "Moreover, the proposed method introduces <b>group</b> normalization, which is conducive to the <b>network</b> convergence of the <b>Bayesian</b> <b>neural</b> <b>network</b>. Some representative learning-based image registration methods are compared with the proposed method on different image datasets. Experimental results show that the registration accuracy of the proposed method is better than that of the methods, and its antifolding performance is comparable to that of fast image registration and VoxelMorph. Furthermore ...", "dateLastCrawled": "2022-01-29T04:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Bayesian</b> <b>neural</b> networks and out-of-distribution data? - Cross Validated", "url": "https://stats.stackexchange.com/questions/511864/bayesian-neural-networks-and-out-of-distribution-data", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/511864/<b>bayesian</b>-<b>neural</b>-<b>networks</b>-and-out-of...", "snippet": "In a <b>Bayesian</b> <b>neural</b> <b>network</b> (for classification) the posterior predictive distribution is $$ P(y=c \\mid {\\bf x}, \\mathcal D_{train}) = \\int P(y=c \\mid {\\bf x}, \\theta) p(\\theta \\mid \\mathcal D_{train}) d\\theta $$ Let&#39;s assume that we have enough training data $\\mathcal D_{train}$ such that if $\\bf x$ is quite <b>similar</b> to the training data the uncertainty in the posterior (predictive) prediction is low. So, we assume that in this region we have low aleatoric uncertainty and due to the amount ...", "dateLastCrawled": "2022-01-28T06:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Adaptive behavior and different thermal experiences of real <b>people</b>: A ...", "url": "https://www.sciencedirect.com/science/article/pii/S036013232100281X", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S036013232100281X", "snippet": "In this study, we aim to extend the application of <b>Bayesian</b> <b>neural</b> networks (BNN) in predicting the thermal preference of a large <b>group</b> of individuals. Here we refer BNN to a <b>neural</b> <b>network</b> that is trained to fit observed data using <b>Bayesian</b> inference by considering that the <b>network</b>\u2019s parameters (i.e., its weights and biases) are random according to a prior probability distribution [ 39 ].", "dateLastCrawled": "2022-01-07T06:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Dynamic Bayesian network modeling of</b> fMRI: A comparison of <b>group</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S1053811908001195", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1053811908001195", "snippet": "<b>Bayesian</b> <b>network</b> (BN) modeling has recently been introduced as a tool for determining the dependencies between brain regions from functional-magnetic-resonance-imaging (fMRI) data. However, studies to date have yet to explore the optimum way for meaningfully combining individually determined BN models to make <b>group</b> inferences. We contrasted the results from three broad approaches: the \u201cvirtual-typical- subject\u201d (VTS) approach which pools or averages <b>group</b> data as if they are sampled from ...", "dateLastCrawled": "2021-11-27T17:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "What&#39;s the likelihood in <b>Bayesian</b> <b>Neural</b> Networks? - Artificial ...", "url": "https://ai.stackexchange.com/questions/26864/whats-the-likelihood-in-bayesian-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://ai.stackexchange.com/.../26864/whats-the-likelihood-in-<b>bayesian</b>-<b>neural</b>-<b>networks</b>", "snippet": "The likelihood depends on the task that you are solving, so this <b>is similar</b> to traditional <b>neural</b> networks (in fact, even these <b>neural</b> networks have a probabilistic/<b>Bayesian</b> interpretation!).. For binary classification, you should probably use a Bernoulli, which, in practice, corresponds to using a sigmoid with a binary cross-entropy (you can show that the minimization of the cross-entropy is equivalent to the maximization of Bernoulli p.m.f.)", "dateLastCrawled": "2022-01-17T07:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A Comprehensive Introduction to <b>Bayesian</b> Deep Learning - Joris Baan", "url": "https://jorisbaan.nl/2021/03/02/introduction-to-bayesian-deep-learning.html", "isFamilyFriendly": true, "displayUrl": "https://jorisbaan.nl/2021/03/02/introduction-to-<b>bayesian</b>-deep-learning.html", "snippet": "A <b>Bayesian</b> <b>Neural</b> <b>Network</b> (BNN) is simply posterior inference applied to a <b>neural</b> <b>network</b> architecture. To be precise, a prior distribution is specified for each weight and bias. Because of their huge parameter space, however, inferring the posterior is even more difficult than usual. So why do <b>Bayesian</b> DL at all? The classic answer is to obtain a realistic expression of uncertainty, or calibration. A classifier is considered calibrated if the probability (confidence) of a class prediction ...", "dateLastCrawled": "2022-02-03T15:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) <b>Bayesian</b> <b>Neural</b> <b>Network</b> Classification of Head Movement Direction ...", "url": "https://www.researchgate.net/publication/224634487_Bayesian_Neural_Network_Classification_of_Head_Movement_Direction_using_Various_Advanced_Optimisation_Training_Algorithms", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/224634487_<b>Bayesian</b>_<b>Neural</b>_<b>Network</b>...", "snippet": "<b>Bayesian</b> <b>Neural</b> <b>Network</b> Classification of Head Movement <b>Direction using Various Advanced Optimisation Training Algorithms</b> March 2006 DOI: 10.1109/BIOROB.2006.1639224", "dateLastCrawled": "2021-08-09T14:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "How do <b>bayesian networks, MDPs and neural networks relate</b> to each other ...", "url": "https://www.quora.com/How-do-bayesian-networks-MDPs-and-neural-networks-relate-to-each-other-How-are-they-used", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/How-do-<b>bayesian-networks-MDPs-and-neural-networks-relate</b>-to-each...", "snippet": "Answer: - Artificial <b>neural</b> networks ANN try to estimate the answer to a problem. For example, they can answer classification problems. The ANN will return a probability for each class. The class with the highest probability is the most probable answer. In the same way, a yes/no problem is a prob...", "dateLastCrawled": "2022-01-14T18:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "[2104.14421] What Are <b>Bayesian</b> <b>Neural</b> <b>Network</b> Posteriors Really Like ...", "url": "https://www.reddit.com/r/MachineLearning/comments/n1w1aq/210414421_what_are_bayesian_neural_network/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/.../comments/n1w1aq/210414421_what_are_<b>bayesian</b>_<b>neural</b>_<b>network</b>", "snippet": "Abstract: The posterior over <b>Bayesian</b> <b>neural</b> <b>network</b> (BNN) parameters is extremely high-dimensional and non-convex. For computational reasons, researchers approximate this posterior using inexpensive mini-batch methods such as mean-field variational inference or stochastic-gradient Markov chain Monte Carlo (SGMCMC). To investigate foundational questions in <b>Bayesian</b> deep learning, we instead use full-batch Hamiltonian Monte Carlo (HMC) on modern architectures. We show that (1) BNNs can ...", "dateLastCrawled": "2021-06-24T13:36:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Researchers Explore <b>Bayesian</b> <b>Neural</b> Networks -- Pure AI", "url": "https://pureai.com/articles/2021/09/07/bayesian-neural-networks.aspx", "isFamilyFriendly": true, "displayUrl": "https://pureai.com/articles/2021/09/07/<b>bayesian</b>-<b>neural</b>-<b>networks</b>.aspx", "snippet": "At first <b>thought</b>, <b>Bayesian</b> <b>neural</b> networks don&#39;t seem to make much sense. However, BNNs have two advantages over standard <b>neural</b> networks. First, the built-in variability in BNNs makes them resistant to model overfitting. Model overfitting occurs when a <b>neural</b> <b>network</b> is trained too well. Even though the trained model predicts with high accuracy on the training data, when presented with new previously unseen data, the overfitted model predicts poorly. A second advantage of <b>Bayesian</b> <b>neural</b> ...", "dateLastCrawled": "2022-01-30T04:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Bayesian</b> vs <b>Neural</b> Networks \u2013 Ehud Reiter&#39;s Blog", "url": "https://ehudreiter.com/2021/07/05/bayesian-vs-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://ehudreiter.com/2021/07/05/<b>bayesian</b>-vs-<b>neural</b>-<b>networks</b>", "snippet": "We have a small <b>group</b> that meets monthly to discuss this, and last week we ended up also talking about why <b>people</b> would use <b>Bayesian</b> models instead of <b>neural</b> networks, focusing on medical decision support (not NLP). Which I found very interesting and <b>thought</b>-provoking. One relatively obvious reason to avoid black-box <b>neural</b> models is justifiability (which relates to explanation). Ie, if I build a medical diagnosis system as a <b>Bayesian</b> model, I <b>can</b> explain to doctors (and regulators) exactly ...", "dateLastCrawled": "2022-02-01T00:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Bayesian Network - The Decision Lab</b>", "url": "https://thedecisionlab.com/reference-guide/statistics/bayesian-network/", "isFamilyFriendly": true, "displayUrl": "https://thedecisionlab.com/reference-guide/statistics/<b>bayesian-network</b>", "snippet": "Silver collected data months prior to voting on how <b>people</b> <b>thought</b> they would vote. Of course, there <b>can</b> always be discrepancies between how <b>people</b> think they will vote and how they actually vote. Luckily, that did not pose an issue for Silver, because Bayes\u2019 theorem allows shifts in hypothesis depending on new information collected. Silver started off with a \u2018nowcast\u2019, which determined the probability of the outcome of each state if voting was to happen on any given day. Various ...", "dateLastCrawled": "2022-01-30T19:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Bayesian</b> <b>Network</b> as a Decision Tool for Predicting ALS Disease", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7912628/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7912628", "snippet": "<b>Bayesian</b> <b>network</b> has produced more successful results than other methods according to all comparison criteria for the Neurological Control <b>group</b>, as in the ALS <b>group</b>. For this <b>group</b>, the <b>Bayesian</b> <b>Network</b>\u2019s ACC value has been found as (0.902). The Kappa values of other methods indicate that the results obtained are random, while the Kappa value (0.677) was found for <b>Bayesian</b> <b>network</b>.", "dateLastCrawled": "2022-01-11T04:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A Comprehensive Introduction to <b>Bayesian</b> Deep Learning - Joris Baan", "url": "https://jorisbaan.nl/2021/03/02/introduction-to-bayesian-deep-learning.html", "isFamilyFriendly": true, "displayUrl": "https://jorisbaan.nl/2021/03/02/introduction-to-<b>bayesian</b>-deep-learning.html", "snippet": "\u201cA <b>neural</b> <b>network</b> <b>can</b> represent many models that are consistent with our observations. By selecting only one, in a classical procedure, we lose uncertainty when the models disagree for a test point.\u201d Recent Approaches To (Approximate) <b>Bayesian</b> Deep Learning . A number <b>of people</b> have recently been trying to combine the advantages of a traditional <b>neural</b> <b>network</b> (e.g. computationally efficient training using SGD &amp; back propagation) with the advantages of a <b>Bayesian</b> approach (e.g ...", "dateLastCrawled": "2022-02-03T15:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "machine learning - What are the <b>downsides of bayesian neural networks</b> ...", "url": "https://stats.stackexchange.com/questions/311008/what-are-the-downsides-of-bayesian-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/311008", "snippet": "<b>Bayesian</b> <b>neural</b> nets (BNN) are very popular topic. With development of variational approximation it became possible to train such models much faster then with Monte Carlo sampling. BNNs allow such interesting features as natural regularisation and even uncertainty estimation. So, the question is: why haven&#39;t we still completely migrated on BNNs? I <b>can</b> assume that variational inference does not provide enough accuracy. Is it the only reason? machine-learning deep-learning <b>bayesian</b>-<b>network</b> ...", "dateLastCrawled": "2022-01-07T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Why hasn&#39;t the <b>Bayesian network been as successful</b> as the Deep <b>Neural</b> ...", "url": "https://www.quora.com/Why-hasnt-the-Bayesian-network-been-as-successful-as-the-Deep-Neural-Network", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Why-hasnt-the-<b>Bayesian-network-been-as-successful</b>-as-the-Deep...", "snippet": "Answer (1 of 5): Well, the definition of \u201csuccessful\u201d is important, here. <b>Bayesian</b> networks are arguably more successful than deep <b>neural</b> networks to date. They are widely applied across industries and <b>can</b> readily be used for inference, modelling, and prediction. Presumably your question is real...", "dateLastCrawled": "2022-01-15T17:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "machine learning - <b>Bayesian</b> Perceptron: Why to marginalize over neuron ...", "url": "https://ai.stackexchange.com/questions/26950/bayesian-perceptron-why-to-marginalize-over-neurons-output-instead-of-its-wei", "isFamilyFriendly": true, "displayUrl": "https://ai.stackexchange.com/questions/26950/<b>bayesian</b>-perceptron-why-to-marginalize...", "snippet": "I found a very interesting paper on the internet that tries to apply <b>Bayesian</b> inference with a gradient-free online-learning approach: <b>Bayesian</b> Perceptron: Towards fully <b>Bayesian</b> <b>Neural</b> Networks. I would love to understand this work, but unfortunately I am reaching my limits with my <b>Bayesian</b> knowledge.", "dateLastCrawled": "2022-01-21T16:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "python - <b>Bayesian neural network in tensorflow-probability</b> - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/54841085/bayesian-neural-network-in-tensorflow-probability", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/54841085", "snippet": "Show activity on this post. I am new to tensorflow and I am trying to set up a <b>bayesian</b> <b>neural</b> <b>network</b> with dense flipout-layers. My code looks as follows: from tensorflow.keras.models import Sequential import tensorflow_probability as tfp import tensorflow as tf def train_BNN (training_data, training_labels, test_data, test_labels, layers ...", "dateLastCrawled": "2022-01-24T16:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Bayesian</b> Dark Knowledge &lt;paper by Kevin Murphy&#39;s <b>group</b> at Google ...", "url": "https://www.reddit.com/r/MachineLearning/comments/3a2crl/bayesian_dark_knowledge_paper_by_kevin_murphys/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/MachineLearning/comments/3a2crl/<b>bayesian</b>_dark_knowledge_paper...", "snippet": "e.g. if you make your <b>neural</b> <b>network</b> 1 layer then it should be able to match the performance of a linear regression baseline, if it doesn\u2019t then you have a bug! e.g. if adding a feature improves the performance of linear regression then it should probably also improve the performance of your <b>neural</b> net unless you have a bug! Hyperparameter optimisation <b>can</b> help a bit (especially for the learning rate) but in general there are default hyperparameters that <b>can</b> do quite well and so closely ...", "dateLastCrawled": "2021-03-24T12:01:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Dynamic Bayesian network modeling of</b> fMRI: A comparison of <b>group</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S1053811908001195", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1053811908001195", "snippet": "The three approaches are <b>compared</b> from the aspects of their statistical goodness-of-fit to the data, and more importantly their sensitivity in detecting the effect of the L-dopa medication on the disease. To the best of our knowledge, this is the first study specifically devoted <b>to group</b>-analysis on fMRI with BN modeling. Materials and methods fMRI data. The fMRI data were collected from ten healthy <b>people</b> and ten Parkinson&#39;s disease (PD) patients, each of whom was asked to squeeze a rubber ...", "dateLastCrawled": "2021-11-27T17:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Bayesian</b> <b>Network</b> as a Decision Tool for Predicting ALS Disease", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7912628/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7912628", "snippet": "<b>Bayesian</b> <b>network</b> has produced more successful results than other methods according to all comparison criteria for the Neurological Control <b>group</b>, as in the ALS <b>group</b>. For this <b>group</b>, the <b>Bayesian</b> <b>Network</b>\u2019s ACC value has been found as (0.902). The Kappa values of other methods indicate that the results obtained are random, while the Kappa value (0.677) was found for <b>Bayesian</b> <b>network</b>.", "dateLastCrawled": "2022-01-11T04:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Bayesian</b> Fully Convolutional Networks for Brain Image Registration", "url": "https://www.hindawi.com/journals/jhe/2021/5528160/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.hindawi.com</b>/journals/jhe/2021/5528160", "snippet": "Moreover, the proposed method introduces <b>group</b> normalization, which is conducive to the <b>network</b> convergence of the <b>Bayesian</b> <b>neural</b> <b>network</b>. Some representative learning-based image registration methods are <b>compared</b> with the proposed method on different image datasets. Experimental results show that the registration accuracy of the proposed method is better than that of the methods, and its antifolding performance is comparable to that of fast image registration and VoxelMorph. Furthermore ...", "dateLastCrawled": "2022-01-29T04:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Dynamic <b>Bayesian</b> <b>network</b> modeling of fMRI: A comparison of <b>group</b> ...", "url": "https://people.ece.ubc.ca/zjanew/journal/ZJW_NeuroImage_DBNgroup_2.pdf", "isFamilyFriendly": true, "displayUrl": "https://<b>people</b>.ece.ubc.ca/zjanew/journal/ZJW_NeuroImage_DBN<b>group</b>_2.pdf", "snippet": "<b>Bayesian</b> <b>network</b>; <b>Group</b> analysis; Parkinson&#39;s disease Introduction Effective brain connectivity, defined as the <b>neural</b> influence that one brain region exerts over another ( Friston, 1994; Harrison and Friston, 2004), is important for the assessment of normal brain function, and its impairment is associated with neurodegenerative diseases such as Alzheimer&#39;s or Parkinson&#39;s disease (PD). Various mathematical methods, such as structural equation modeling (SEM) (McIntosh and Gonzalez-Lima, 1994 ...", "dateLastCrawled": "2021-07-19T15:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "machine learning - Difference between Bayes <b>network</b>, <b>neural</b> <b>network</b> ...", "url": "https://stats.stackexchange.com/questions/94511/difference-between-bayes-network-neural-network-decision-tree-and-petri-nets", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/94511", "snippet": "<b>Bayesian</b> <b>Network</b>: The <b>Bayesian</b> <b>Network</b> is a directed acyclic graph, which more like the flowchart, only that the flow chart <b>can</b> have cyclic loops. The <b>Bayesian</b> <b>network</b> unlike the flow chart <b>can</b> have multiple start points. It basically traces the propagation of events across multiple ambiguous points, where the event diverges probabilistically between pathways. Obviously, at any given point in the <b>network</b>, the probability of that node being visited is dependent on the joint probability of the ...", "dateLastCrawled": "2022-01-28T22:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "machine learning - What are the <b>downsides of bayesian neural networks</b> ...", "url": "https://stats.stackexchange.com/questions/311008/what-are-the-downsides-of-bayesian-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/311008", "snippet": "<b>Bayesian</b> <b>neural</b> nets (BNN) are very popular topic. With development of variational approximation it became possible to train such models much faster then with Monte Carlo sampling. BNNs allow such interesting features as natural regularisation and even uncertainty estimation. So, the question is: why haven&#39;t we still completely migrated on BNNs? I <b>can</b> assume that variational inference does not provide enough accuracy. Is it the only reason? machine-learning deep-learning <b>bayesian</b>-<b>network</b> ...", "dateLastCrawled": "2022-01-07T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Bayesian Belief Networks</b>: An Introduction In 6 Easy Points", "url": "https://www.jigsawacademy.com/blogs/data-science/bayesian-belief-network", "isFamilyFriendly": true, "displayUrl": "https://www.<b>jigsawacademy</b>.com/blogs/data-science/<b>bayesian</b>-belief-<b>network</b>", "snippet": "Hence, only the person creating the <b>network</b> <b>can</b> exploit causal influences. <b>Neural</b> networks are an advantage <b>compared</b> to this, as they learn different patterns and aren\u2019t limited to only the creator. The <b>Bayesian</b> <b>network</b> fails to define cyclic relationships\u2014for example, deflection of airplane wings and fluid pressure field around it. The ...", "dateLastCrawled": "2022-02-03T06:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "applications - What are the main benefits of using <b>Bayesian</b> networks ...", "url": "https://ai.stackexchange.com/questions/10649/what-are-the-main-benefits-of-using-bayesian-networks", "isFamilyFriendly": true, "displayUrl": "https://ai.stackexchange.com/.../what-are-the-main-benefits-of-using-<b>bayesian</b>-<b>networks</b>", "snippet": "$\\begingroup$ One other thing that comes to mind is markov blankets and other conditional independences, so local information is sufficient and other nodes are conditionally independent. I am not experienced enough to say how this is applied, but you <b>can</b> search for that. Having a <b>Bayesian</b> <b>network</b> feels to me like when I&#39;m happy when I <b>can</b> use a Markov chain as a model, because of the structure and simplified dependencies.", "dateLastCrawled": "2022-01-12T06:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A <b>quick intro to Bayesian neural networks</b> - matthewmcateer.me", "url": "https://matthewmcateer.me/blog/a-quick-intro-to-bayesian-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://matthewmcateer.me/blog/a-<b>quick-intro-to-bayesian-neural-networks</b>", "snippet": "Pretty impressive! This illustrates one of the other add-ons we <b>can</b> easily make for <b>bayesian</b> <b>neural</b> networks: a probability cutoff. In this case, if none of our probabilities exceed 0.2, we <b>can</b> get our <b>network</b> to refuse to classify the images. Of course, it wasn\u2019t always like this. It took our <b>network</b> a while before it was correctly able to ...", "dateLastCrawled": "2021-05-30T11:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How do <b>bayesian networks, MDPs and neural networks relate</b> to each other ...", "url": "https://www.quora.com/How-do-bayesian-networks-MDPs-and-neural-networks-relate-to-each-other-How-are-they-used", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/How-do-<b>bayesian-networks-MDPs-and-neural-networks-relate</b>-to-each...", "snippet": "Answer: - Artificial <b>neural</b> networks ANN try to estimate the answer to a problem. For example, they <b>can</b> answer classification problems. The ANN will return a probability for each class. The class with the highest probability is the most probable answer. In the same way, a yes/no problem is a prob...", "dateLastCrawled": "2022-01-14T18:42:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A <b>machine</b> <b>learning</b> approach to <b>Bayesian</b> parameter estimation | npj ...", "url": "https://www.nature.com/articles/s41534-021-00497-w", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41534-021-00497-w", "snippet": "The parameter estimation discussed in this manuscript is divided in two parts: i) a <b>neural</b> <b>network</b> is trained and ii) <b>Bayesian</b> estimation performed on a test set, which we detail below.", "dateLastCrawled": "2022-02-03T06:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "9.7. <b>Bayesian</b> <b>neural</b> networks \u2014 <b>Learning</b> from data", "url": "https://furnstahl.github.io/Physics-8820/notebooks/Machine_learning/Bayesian_neural_networks_tif285.html", "isFamilyFriendly": true, "displayUrl": "https://furnstahl.github.io/.../<b>Machine</b>_<b>learning</b>/<b>Bayesian</b>_<b>neural</b>_<b>networks</b>_tif285.html", "snippet": "<b>Bayesian</b> <b>neural</b> networks differ from plain <b>neural</b> networks in that their weights are assigned a probability distribution instead of a single value or point estimate. These probability distributions describe the uncertainty in weights and can be used to estimate uncertainty in predictions. Training a <b>Bayesian</b> <b>neural</b> <b>network</b> via variational inference learns the parameters of these distributions instead of the weights directly.", "dateLastCrawled": "2021-12-21T06:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "34. <b>Bayesian</b> <b>neural</b> networks \u2014 <b>Learning</b> from data", "url": "https://physics-chalmers.github.io/tif285/doc/LectureNotes/_build/html/content/MachineLearning/BNN/bnn.html", "isFamilyFriendly": true, "displayUrl": "https://physics-chalmers.github.io/.../_build/html/content/<b>MachineLearning</b>/BNN/bnn.html", "snippet": "34. <b>Bayesian</b> <b>neural</b> networks\u00b6. The introduction part of this lecture is inspired by the chapter \u201c<b>Learning</b> as Inference\u201d in the excellent book Information Theory, Inference, and <b>Learning</b> Algorithms by David MacKay.. Some python libraries that are relevant for <b>Bayesian</b> <b>Neural</b> Networks (and part of the general trend towards Probabilistic Programming in <b>Machine</b> <b>Learning</b>) are:", "dateLastCrawled": "2022-01-13T02:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Researchers Explore <b>Bayesian</b> <b>Neural</b> Networks -- Pure AI", "url": "https://pureai.com/articles/2021/09/07/bayesian-neural-networks.aspx", "isFamilyFriendly": true, "displayUrl": "https://pureai.com/articles/2021/09/07/<b>bayesian</b>-<b>neural</b>-<b>networks</b>.aspx", "snippet": "<b>Bayesian</b> <b>neural</b> networks are best explained using an <b>analogy</b> example. Suppose that instead of a <b>neural</b> <b>network</b>, you have a prediction equation y = (8.5 * x1) + (9.5 * x2) + 2.5 where y is the predicted income of an employee, x1 is normalized age, and x2 is years of job tenure. The predicted income of a 30-year old who has been on the job for 4 years would be y = (8.5 * 3.0) + (9.5 * 4.0) + 2.5 = 64.5 = $64,500. If you feed the same (age, tenure) input of (3.0, 4.0) to the prediction equation ...", "dateLastCrawled": "2022-01-30T04:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Bayesian</b> <b>Neural Network</b> Series Post 2: Background Knowledge | by Kumar ...", "url": "https://medium.com/neuralspace/bayesian-neural-network-series-post-2-background-knowledge-fdec6ac62d43", "isFamilyFriendly": true, "displayUrl": "https://medium.com/<b>neural</b>space/<b>bayesian</b>-<b>neural-network</b>-series-post-2-background...", "snippet": "I will try to brief the <b>neural</b> networks <b>analogy</b> with the brain and will spend more time explaining the Probabilistic <b>Machine</b> <b>Learning</b> segments that we will work on in future. Brain Analogies. A ...", "dateLastCrawled": "2022-01-30T12:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) <b>Bayesian</b> Strategies for <b>Machine</b> <b>Learning</b>: Rule Extraction and ...", "url": "https://www.researchgate.net/publication/280112594_Bayesian_Strategies_for_Machine_Learning_Rule_Extraction_and_Concept_Detection", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/280112594_<b>Bayesian</b>_Strategies_for_<b>Machine</b>...", "snippet": "A common formalism can be used to describe two seemingly different models: the Boltzmann <b>machine</b> connectionist <b>learning</b> model and the Bayes <b>network</b> model for probabilistic reasoning. The <b>learning</b> ...", "dateLastCrawled": "2022-01-27T01:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep <b>neural</b> <b>network</b> models, and it has been used for conducting ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Machine Learning by Analogy</b> - SlideShare", "url": "https://www.slideshare.net/ColleenFarrelly/machine-learning-by-analogy-59094152", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ColleenFarrelly/<b>machine-learning-by-analogy</b>-59094152", "snippet": "<b>Machine Learning by Analogy</b> 1. Colleen M. Farrelly 2. Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics behind these ...", "dateLastCrawled": "2022-01-31T07:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Bayesian Belief Network in Artificial Intelligence</b> - Javatpoint", "url": "https://www.javatpoint.com/bayesian-belief-network-in-artificial-intelligence", "isFamilyFriendly": true, "displayUrl": "https://www.javatpoint.com/<b>bayesian-belief-network-in-artificial-intelligence</b>", "snippet": "<b>Bayesian Belief Network in artificial intelligence</b>. <b>Bayesian</b> belief <b>network</b> is key computer technology for dealing with probabilistic events and to solve a problem which has uncertainty. We can define a <b>Bayesian</b> <b>network</b> as: &quot;A <b>Bayesian</b> <b>network</b> is a probabilistic graphical model which represents a set of variables and their conditional ...", "dateLastCrawled": "2022-02-02T21:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Machine</b> <b>Learning</b> by <b>Analogy</b> II - slideshare.net", "url": "https://www.slideshare.net/ColleenFarrelly/machine-learning-by-analogy-ii", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ColleenFarrelly/<b>machine</b>-<b>learning</b>-by-<b>analogy</b>-ii", "snippet": "Updated <b>Machine</b> <b>Learning</b> by <b>Analogy</b> presentation that builds to more advanced methods (TensorFlow, geometry/topology-based methods...) and adds a section on ti\u2026", "dateLastCrawled": "2021-12-31T12:33:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(bayesian neural network)  is like +(group of people)", "+(bayesian neural network) is similar to +(group of people)", "+(bayesian neural network) can be thought of as +(group of people)", "+(bayesian neural network) can be compared to +(group of people)", "machine learning +(bayesian neural network AND analogy)", "machine learning +(\"bayesian neural network is like\")", "machine learning +(\"bayesian neural network is similar\")", "machine learning +(\"just as bayesian neural network\")", "machine learning +(\"bayesian neural network can be thought of as\")", "machine learning +(\"bayesian neural network can be compared to\")"]}