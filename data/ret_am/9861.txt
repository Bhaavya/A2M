{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Fairness in Machine Learning: How</b> Can a <b>Model Trained on Aerial Imagery</b> ...", "url": "https://opendri.org/wp-content/uploads/2020/05/CatherineInness_-Fairness_in_ML_for_DRM.pdf", "isFamilyFriendly": true, "displayUrl": "https://opendri.org/wp-content/uploads/2020/05/CatherineInness_-Fairness_in_ML_for_DRM.pdf", "snippet": "A major limitation of the <b>equalized</b> <b>odds</b> measure is that we need to know our protected groups in advance so as to code them into the <b>model</b>, which often requires prior knowledge of the domain. According to a 2018 World Bank report, Building Back Better [2], the poorest people are disproportionately impacted by disasters (the report urges us to consider the experience of a 1 dollar loss for a rich and a poor person). They are less likely to have savings or insurance and they are more likely to ...", "dateLastCrawled": "2021-12-05T09:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Discriminative but Not Discriminatory: A Comparison of Fairness ...", "url": "https://deepai.org/publication/discriminative-but-not-discriminatory-a-comparison-of-fairness-definitions-under-different-worldviews", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/discriminative-but-not-discriminatory-a-comparison-of...", "snippet": "In this work, we handle the issue of biased ground truth by adopting the framework of Friedler et al. (friedler2016impossibility), who make the distinction between the observed ground truth and the construct, which is the attribute that is truly relevant for prediction.Using this framework, we define what it means for a <b>model</b> to be discriminative, <b>that is, able</b> to accurately predict the construct, and discriminatory, that is, disproportionately favoring one protected group over another in an ...", "dateLastCrawled": "2022-01-16T07:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A review of the <b>machine learning literature on fairness</b> | FUTURIUM ...", "url": "https://ec.europa.eu/futurium/en/european-ai-alliance/review-machine-learning-literature-fairness.html", "isFamilyFriendly": true, "displayUrl": "https://<b>ec.europa.eu</b>/futurium/en/european-ai-alliance/review-machine-learning...", "snippet": "A notion that is implicit in <b>equalized</b> <b>odds</b> as <b>well</b> as predictive parity is that a <b>model</b> which exhibits perfect classification is necessarily fair. More precisely, y = 1 if and only if f(x) &gt; \u03b8 implies that f is fair. This also conforms to the intuition that higher accuracy should not contradict fairness Hardt et al., 2016), because this requirement implies as a special case that perfect classification can not be unfair. Beyond this utility-based intuition, Corbett-Davies and Goel (2018 ...", "dateLastCrawled": "2021-12-26T22:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Equalizing Financial Impact in Supervised Learning | DeepAI", "url": "https://deepai.org/publication/equalizing-financial-impact-in-supervised-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/equalizing-financial-impact-in-supervised-learning", "snippet": "Now we discuss the second notion of fairness, called <b>equalized</b> <b>odds</b>, which appears in the work of Hardt, Price, and Srebro . The motivation of <b>equalized</b> <b>odds</b> is that, unlike statistical parity, it does not prevent the ground truth from being a classifier in the case where base rates are unequal. <b>Equalized</b> <b>odds</b> enforces the conditions that the ...", "dateLastCrawled": "2022-01-13T12:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Achieving <b>Equalized</b> <b>Odds</b> by Resampling Sensitive Attributes | Request PDF", "url": "https://www.researchgate.net/publication/342027351_Achieving_Equalized_Odds_by_Resampling_Sensitive_Attributes", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/342027351_Achieving_<b>Equalized</b>_<b>Odds</b>_by_Re...", "snippet": "Both the <b>model</b> fitting and hypothesis testing leverage a resampled version of the sensitive attribute obeying <b>equalized</b> <b>odds</b>, by construction. We demonstrate the applicability and validity of the ...", "dateLastCrawled": "2022-01-07T05:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A Human-centric Approach to Fairness in AI", "url": "https://www.timlrx.com/blog/a-human-centric-approach-to-fairness-in-ai", "isFamilyFriendly": true, "displayUrl": "https://www.timlrx.com/blog/a-human-centric-approach-to-fairness-in-ai", "snippet": "This includes measures such as Demographic Parity / Statistical Parity (Dwork et al., 2012), <b>Equalized</b> <b>Odds</b> Metric (Hardt et al ., 2016) and Calibration within Groups (Chouldechova, 2017). They are all statistical measures derived from the predictions of a classification <b>model</b> and differ in terms of which element(s) of the confusion matrix they are trying to test for equivalence. In another survey of fairness definitions, Verma &amp; Rubin (2018) listed 20 definitions of fairness, 13 belonging ...", "dateLastCrawled": "2022-02-03T02:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine learning fairness notions: Bridging the</b> gap with real-world ...", "url": "https://www.sciencedirect.com/science/article/pii/S0306457321001321", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0306457321001321", "snippet": "By contrast to statistical parity, <b>equalized</b> <b>odds</b> is <b>well</b>-suited for scenarios where the ground truth exists such as: disease prediction or stop-and-frisk (Bellin, 2014). It is also suitable when the emphasis is on recall (the fraction of the total number of positive instances that are correctly predicted positive) rather than precision (making sure that a predicted positive instance is actually a positive instance).", "dateLastCrawled": "2022-02-03T07:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Taking Advantage of Multitask Learning for Fair Classi\ufb01cation</b>", "url": "https://www.aies-conference.com/2019/wp-content/papers/main/AIES-19_paper_37.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.aies-conference.com/2019/wp-content/papers/main/AIES-19_paper_37.pdf", "snippet": "learns a shared <b>model</b> between the groups as <b>well</b> as a spe-ci\ufb01c <b>model</b> per group. We show how fairness constraints, measured with <b>Equalized</b> <b>Odds</b> or Equal Opportunities in-troduced in (Hardt, Price, and Srebro 2016), can be built in MTL directly during the training phase. This is in contrast to other approaches which impose the fairness ...", "dateLastCrawled": "2021-09-18T20:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Training <b>Well</b>-Generalizing Classifiers for Fairness Metrics and Other ...", "url": "https://www.arxiv-vanity.com/papers/1807.00028/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1807.00028", "snippet": "Classifiers can be trained with data-dependent constraints to satisfy fairness goals, reduce churn, achieve a targeted false positive rate, or other policy goals. We study the generalization performance for such constrained optimization problems, in terms of how <b>well</b> the constraints are satisfied at evaluation time, given that they are satisfied at training time. To improve generalization performance, we frame the problem as a two-player game where one player optimizes the <b>model</b> parameters ...", "dateLastCrawled": "2021-09-17T15:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Research Methods and Statistical Terms Flashcards | <b>Quizlet</b>", "url": "https://quizlet.com/3176831/research-methods-and-statistical-terms-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/3176831/research-methods-and-statistical-terms-flash-cards", "snippet": "process of evaluating claims or hypotheses and making judgments about them on the basis of <b>well</b>-supported evidence. hypothesis. a prediction stated as a testable proposition, usually in the form of an if-then statement. variables . specific factors or characteristics that are manipulated and measured in research. data. numbers that represent research findings and provide the basis for research conclusions. operational definition. a statement of the specific methods used to measure a variable ...", "dateLastCrawled": "2020-10-23T22:57:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Algorithmic Fairness and Bias Mitigation for Clinical Machine Learning ...", "url": "https://www.medrxiv.org/content/10.1101/2022.01.13.22268948v1.full.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.medrxiv.org/content/10.1101/2022.01.13.22268948v1.full.pdf", "snippet": "tical definition of <b>equalized</b> <b>odds</b>. We evaluated our <b>model</b> for the task of rapidly predicting COVID-19 for patients presenting to hospital emergency departments, and aimed to mitigate regional (hospital) and ethnic biases present. We trained our framework on a large, real-world COVID-19 dataset and demonstrated that adversarial training demonstrably improves outcome fairness (with respect to <b>equalized</b> <b>odds</b>), while still achieving clinically-effective screening performances (NPV&gt;0.98). We ...", "dateLastCrawled": "2022-01-31T18:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Fairness in Machine Learning: How</b> Can a <b>Model Trained on Aerial Imagery</b> ...", "url": "https://opendri.org/wp-content/uploads/2020/05/CatherineInness_-Fairness_in_ML_for_DRM.pdf", "isFamilyFriendly": true, "displayUrl": "https://opendri.org/wp-content/uploads/2020/05/CatherineInness_-Fairness_in_ML_for_DRM.pdf", "snippet": "A major limitation of the <b>equalized</b> <b>odds</b> measure is that we need to know our protected groups in advance so as to code them into the <b>model</b>, which often requires prior knowledge of the domain. According to a 2018 World Bank report, Building Back Better [2], the poorest people are disproportionately impacted by disasters (the report urges us to consider the experience of a 1 dollar loss for a rich and a poor person). They are less likely to have savings or insurance and they are more likely to ...", "dateLastCrawled": "2021-12-05T09:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "ALGORITHMIC FAIRNESS: MEASURES, METHODS AND REPRESENTATIONS", "url": "https://www.cs.utah.edu/~suresh/static/files/tutorial.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.utah.edu/~suresh/static/files/tutorial.pdf", "snippet": "\u2022 <b>Equalized</b> <b>odds</b>: Groups may have different innate skill levels, but we should make mistakes equally. MORE HIDDEN ASSUMPTIONS. MORE HIDDEN ASSUMPTIONS \u2022 All groups are equivalent and unfair treatment of one is the same as unfair treatment of another. MORE HIDDEN ASSUMPTIONS \u2022 All groups are equivalent and unfair treatment of one is the same as unfair treatment of another. \u2022 All instances of unfairness boil down to individual decisions about people, and not structural factors that ...", "dateLastCrawled": "2022-02-03T18:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Discriminative but Not Discriminatory: A Comparison of Fairness ...", "url": "https://deepai.org/publication/discriminative-but-not-discriminatory-a-comparison-of-fairness-definitions-under-different-worldviews", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/discriminative-but-not-discriminatory-a-comparison-of...", "snippet": "In this work, we handle the issue of biased ground truth by adopting the framework of Friedler et al. (friedler2016impossibility), who make the distinction between the observed ground truth and the construct, which is the attribute that is truly relevant for prediction.Using this framework, we define what it means for a <b>model</b> to be discriminative, <b>that is, able</b> to accurately predict the construct, and discriminatory, that is, disproportionately favoring one protected group over another in an ...", "dateLastCrawled": "2022-01-16T07:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A review of the <b>machine learning literature on fairness</b> | FUTURIUM ...", "url": "https://ec.europa.eu/futurium/en/european-ai-alliance/review-machine-learning-literature-fairness.html", "isFamilyFriendly": true, "displayUrl": "https://<b>ec.europa.eu</b>/futurium/en/european-ai-alliance/review-machine-learning...", "snippet": "A notion that is implicit in <b>equalized</b> <b>odds</b> as <b>well</b> as predictive parity is that a <b>model</b> which exhibits perfect classification is necessarily fair. More precisely, y = 1 if and only if f(x) &gt; \u03b8 implies that f is fair. This also conforms to the intuition that higher accuracy should not contradict fairness", "dateLastCrawled": "2021-12-26T22:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Fairness in credit scoring: Assessment, implementation and profit ...", "url": "https://www.sciencedirect.com/science/article/pii/S0377221721005385", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0377221721005385", "snippet": "The separation condition, also known as the <b>equalized</b> <b>odds</b> condition, ... When the difference between group sizes is large, the criterion will punish models that perform <b>well</b> only on the majority group (Hardt et al., 2016). To measure the degree to which the separation condition is satisfied, we suggest using a criterion denoted as SP, which we define as: (4) SP = 1 2 | (FPR {x a = 1} \u2212 FPR {x a = 0}) + (FNR {x a = 1} \u2212 FNR {x a = 0}) | SP calculates the average absolute difference ...", "dateLastCrawled": "2022-01-26T22:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A Human-centric Approach to Fairness in AI", "url": "https://www.timlrx.com/blog/a-human-centric-approach-to-fairness-in-ai", "isFamilyFriendly": true, "displayUrl": "https://www.timlrx.com/blog/a-human-centric-approach-to-fairness-in-ai", "snippet": "This includes measures such as Demographic Parity / Statistical Parity (Dwork et al., 2012), <b>Equalized</b> <b>Odds</b> Metric (Hardt et al ., 2016) and Calibration within Groups (Chouldechova, 2017). They are all statistical measures derived from the predictions of a classification <b>model</b> and differ in terms of which element(s) of the confusion matrix they are trying to test for equivalence. In another survey of fairness definitions, Verma &amp; Rubin (2018) listed 20 definitions of fairness, 13 belonging ...", "dateLastCrawled": "2022-02-03T02:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Towards Supporting and Documenting Algorithmic Fairness in the Data ...", "url": "https://www.blaseur.com/papers/conpro19-cameraready.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.blaseur.com/papers/conpro19-cameraready.pdf", "snippet": "subgroups [10]. Zafar et al. <b>generalize</b> this approach by segregating sensitive data to only be necessary when training the <b>model</b> [11]. Heidari et al. have attempted to put various forms of group fairness into a Rawlsian notion of equality of opportunity [12]. Depending on the nature of the underlying population distributions and which speci\ufb01c <b>odds</b> are consid-ered, <b>equalized</b> <b>odds</b> may or may not be reconcilable with disparate impact or individual fairness. In addition to encoding different ...", "dateLastCrawled": "2021-10-24T07:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Fairness and Robustness in Invariant Learning: A Case Study in Toxicity ...", "url": "https://www.arxiv-vanity.com/papers/2011.06485/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2011.06485", "snippet": "These predictors can consequently <b>generalize</b> <b>well</b> to all test out-of-distribution (OOD) environments which share this same invariance. Building on work in philosophy which characterizes causation as invariance (Cartwright, 2003; Mitchell, 2000), existing invariance-based DG methods have been interpreted as a weak form of causal discovery whose returned predictors are the causal factors underlying the phenomena we wish to predict. Fairness can be often characterized by robustness to changes ...", "dateLastCrawled": "2021-12-23T18:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Training <b>Well</b>-Generalizing Classifiers for Fairness Metrics and Other ...", "url": "https://www.arxiv-vanity.com/papers/1807.00028/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1807.00028", "snippet": "Classifiers can be trained with data-dependent constraints to satisfy fairness goals, reduce churn, achieve a targeted false positive rate, or other policy goals. We study the generalization performance for such constrained optimization problems, in terms of how <b>well</b> the constraints are satisfied at evaluation time, given that they are satisfied at training time. To improve generalization performance, we frame the problem as a two-player game where one player optimizes the <b>model</b> parameters ...", "dateLastCrawled": "2021-09-17T15:42:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Fairness in Machine Learning: How</b> <b>Can</b> a <b>Model Trained on Aerial Imagery</b> ...", "url": "https://opendri.org/wp-content/uploads/2020/05/CatherineInness_-Fairness_in_ML_for_DRM.pdf", "isFamilyFriendly": true, "displayUrl": "https://opendri.org/wp-content/uploads/2020/05/CatherineInness_-Fairness_in_ML_for_DRM.pdf", "snippet": "One common definition of fairness in machine learning is the concept of <b>equalized</b> <b>odds</b> (Hardt, Price, &amp; Srebro [1]), ... population. For our disaster damage use case, false-negatives <b>can</b> <b>be thought</b> of as times where disaster damage is missed by the <b>model</b>, i.e. &#39;invisible&#39; to the humans reviewing the output of the <b>model</b> and therefore ignored in decision-making on allocation of protective measures. If the <b>model</b> misses damage more often for one subgroup of the population than for others, it ...", "dateLastCrawled": "2021-12-05T09:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "2. Understanding Fairness and the Data Science Pipeline \u2013 Practical ...", "url": "https://goois.net/2-understanding-fairness-and-the-data-science-pipeline-practical-fairness.html", "isFamilyFriendly": true, "displayUrl": "https://goois.net/2-understanding-fairness-and-the-data-science-pipeline-practical...", "snippet": "Formally, <b>equalized</b> <b>odds</b> are achieved when the predicted target variable of a <b>model</b> and the label of a protected category are statistically independent of one another conditional on the true value of the target variable. In a binary classification task, this <b>can</b> be simplified to requiring that the true-positive rates and false-positive rates are equal across groups, where groups are determined by the protected category. A slightly less demanding fairness criterion is equality of opportunity ...", "dateLastCrawled": "2022-01-13T00:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A review of the <b>machine learning literature on fairness</b> | FUTURIUM ...", "url": "https://ec.europa.eu/futurium/en/european-ai-alliance/review-machine-learning-literature-fairness.html", "isFamilyFriendly": true, "displayUrl": "https://<b>ec.europa.eu</b>/futurium/en/european-ai-alliance/review-machine-learning...", "snippet": "Furthermore, as we have established above, predictive parity and <b>equalized</b> <b>odds</b> <b>can</b> not be both fulfilled except in special circumstances (Berk et al., 2018; Corbett-Davies and Goel, 2018). Accuracy Fairness. A notion that is implicit in <b>equalized</b> <b>odds</b> as <b>well</b> as predictive parity is that a <b>model</b> which exhibits perfect classification is necessarily fair. More precisely, y = 1 if and only if f(x) &gt; \u03b8 implies that f is fair. This also conforms to the intuition that higher accuracy should not ...", "dateLastCrawled": "2021-12-26T22:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Human-centric Approach to Fairness in AI", "url": "https://www.timlrx.com/blog/a-human-centric-approach-to-fairness-in-ai", "isFamilyFriendly": true, "displayUrl": "https://www.timlrx.com/blog/a-human-centric-approach-to-fairness-in-ai", "snippet": "This includes measures such as Demographic Parity / Statistical Parity (Dwork et al., 2012), <b>Equalized</b> <b>Odds</b> Metric (Hardt et al., 2016) and Calibration within Groups (Chouldechova, 2017). They are all statistical measures derived from the predictions of a classification <b>model</b> and differ in terms of which element(s) of the confusion matrix they are trying to test for equivalence. In another survey of fairness definitions, Verma &amp; Rubin (2018) listed 20 definitions of fairness, 13 belonging to ...", "dateLastCrawled": "2022-02-03T02:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Discriminative but Not Discriminatory: A Comparison of Fairness ...", "url": "https://deepai.org/publication/discriminative-but-not-discriminatory-a-comparison-of-fairness-definitions-under-different-worldviews", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/discriminative-but-not-discriminatory-a-comparison-of...", "snippet": "In this work, we handle the issue of biased ground truth by adopting the framework of Friedler et al. (friedler2016impossibility), who make the distinction between the observed ground truth and the construct, which is the attribute that is truly relevant for prediction.Using this framework, we define what it means for a <b>model</b> to be discriminative, <b>that is, able</b> to accurately predict the construct, and discriminatory, that is, disproportionately favoring one protected group over another in an ...", "dateLastCrawled": "2022-01-16T07:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) On Fair Representation in Machine Learning", "url": "https://www.researchgate.net/publication/341736051_On_Fair_Representation_in_Machine_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/341736051_On_Fair_Representation_in_Machine...", "snippet": "demographic parity and metrics such as <b>equalized</b> <b>odds</b> are either ignored ( e.g. [21], [22]) or assumed to be automatically satis\ufb01ed by &quot;removing&quot; information about sensitive attributes", "dateLastCrawled": "2022-01-26T19:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Equalizing Financial Impact in Supervised Learning | DeepAI", "url": "https://deepai.org/publication/equalizing-financial-impact-in-supervised-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/equalizing-financial-impact-in-supervised-learning", "snippet": "Some proponents of machine learning have suggested that not only are these algorithms <b>able</b> to leverage the increasing amount of data we have access to, but also that they might be <b>able</b> to make these decisions more fairly, as they seem to not be subject to human biases. There is some truth to these claims. For example, an algorithm to judge whether criminal defendants will recidivate <b>can</b> be used nationwide (or potentially even world-wide). This <b>can</b> alleviate biases that may be local. For ...", "dateLastCrawled": "2022-01-13T12:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "BayLearn - Machine Learning Symposium - 2021: Accepted Submissions", "url": "https://baylearn-org.github.io/www/submissions", "isFamilyFriendly": true, "displayUrl": "https://baylearn-org.github.io/www/submissions", "snippet": "In this paper, we are concerned with how group fairness (e.g., equal opportunity, <b>equalized</b> <b>odds</b>) as an ML fairness concept plays out in the multi-task scenario. In multi-task learning, several tasks are learned jointly to exploit task correlations for a more efficient inductive transfer. This presents a multi-dimensional Pareto frontier on (1) the trade-off between group fairness and accuracy with respect to each task, as <b>well</b> as (2) the trade-offs across multiple tasks. We aim to provide a ...", "dateLastCrawled": "2022-01-24T17:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Exploring rule\u2010following identity at the frontline: The roles of ...", "url": "https://onlinelibrary.wiley.com/doi/10.1111/padm.12721", "isFamilyFriendly": true, "displayUrl": "https://onlinelibrary.wiley.com/doi/10.1111/padm.12721", "snippet": "This expectation <b>can</b> be at <b>odds</b> with strict rule-adherence (Merton, 1940). ... As <b>can</b> be seen in <b>Model</b> 3, the general self-efficacy\u2013gender interaction term was positive but not significant \u03b2 = 0.040, ns). Consequently, the analysis provides no support for Hypothesis 2. Building on an other-centered perspective on rule-following identity, the last hypothesis predicted street-level bureaucrats&#39; general attitude to clients to moderate the relationship between general self-efficacy and rule ...", "dateLastCrawled": "2022-01-27T00:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Research Methods and Statistical Terms Flashcards | <b>Quizlet</b>", "url": "https://quizlet.com/3176831/research-methods-and-statistical-terms-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/3176831/research-methods-and-statistical-terms-flash-cards", "snippet": "process of evaluating claims or hypotheses and making judgments about them on the basis of <b>well</b>-supported evidence. hypothesis. a prediction stated as a testable proposition, usually in the form of an if-then statement. variables . specific factors or characteristics that are manipulated and measured in research. data. numbers that represent research findings and provide the basis for research conclusions. operational definition. a statement of the specific methods used to measure a variable ...", "dateLastCrawled": "2020-10-23T22:57:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Fairness in Machine Learning: How</b> <b>Can</b> a <b>Model Trained on Aerial Imagery</b> ...", "url": "https://opendri.org/wp-content/uploads/2020/05/CatherineInness_-Fairness_in_ML_for_DRM.pdf", "isFamilyFriendly": true, "displayUrl": "https://opendri.org/wp-content/uploads/2020/05/CatherineInness_-Fairness_in_ML_for_DRM.pdf", "snippet": "One common definition of fairness in machine learning is the concept of <b>equalized</b> <b>odds</b> (Hardt, Price, &amp; Srebro [1]), ... unable <b>to generalize</b> on unseen data) is a bad outcome. This means that the <b>model</b> has to make some generalizations. For the example of damage detection, the <b>model</b> will recognise attributes that are linked to damage, for example that collapsed buildings do not cast a shadow, or that damaged buildings have uneven building boundary lines. It will then apply this knowledge to ...", "dateLastCrawled": "2021-12-05T09:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Algorithmic Fairness and Bias Mitigation for Clinical Machine Learning ...", "url": "https://www.medrxiv.org/content/10.1101/2022.01.13.22268948v1.full.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.medrxiv.org/content/10.1101/2022.01.13.22268948v1.full.pdf", "snippet": "tical definition of <b>equalized</b> <b>odds</b>. We evaluated our <b>model</b> for the task of rapidly predicting COVID-19 for patients presenting to hospital emergency departments, and aimed to mitigate regional (hospital) and ethnic biases present. We trained our framework on a large, real-world COVID-19 dataset and demonstrated that adversarial training demonstrably improves outcome fairness (with respect to <b>equalized</b> <b>odds</b>), while still achieving clinically-effective screening performances (NPV&gt;0.98). We ...", "dateLastCrawled": "2022-01-31T18:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A review of the <b>machine learning literature on fairness</b> | FUTURIUM ...", "url": "https://ec.europa.eu/futurium/en/european-ai-alliance/review-machine-learning-literature-fairness.html", "isFamilyFriendly": true, "displayUrl": "https://<b>ec.europa.eu</b>/futurium/en/european-ai-alliance/review-machine-learning...", "snippet": "Furthermore, as we have established above, predictive parity and <b>equalized</b> <b>odds</b> <b>can</b> not be both fulfilled except in special circumstances (Berk et al., 2018; Corbett-Davies and Goel, 2018). Accuracy Fairness. A notion that is implicit in <b>equalized</b> <b>odds</b> as <b>well</b> as predictive parity is that a <b>model</b> which exhibits perfect classification is necessarily fair. More precisely, y = 1 if and only if f(x) &gt; \u03b8 implies that f is fair. This also conforms to the intuition that higher accuracy should not ...", "dateLastCrawled": "2021-12-26T22:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Achieving <b>Equalized</b> <b>Odds</b> by Resampling Sensitive Attributes | Request PDF", "url": "https://www.researchgate.net/publication/342027351_Achieving_Equalized_Odds_by_Resampling_Sensitive_Attributes", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/342027351_Achieving_<b>Equalized</b>_<b>Odds</b>_by_Re...", "snippet": "Both the <b>model</b> fitting and hypothesis testing leverage a resampled version of the sensitive attribute obeying <b>equalized</b> <b>odds</b>, by construction. We demonstrate the applicability and validity of the ...", "dateLastCrawled": "2022-01-07T05:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Fairness in credit scoring: Assessment, implementation and profit ...", "url": "https://www.sciencedirect.com/science/article/pii/S0377221721005385", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0377221721005385", "snippet": "The separation condition, also known as the <b>equalized</b> <b>odds</b> condition, ... This is achieved by sacrificing more than 30 % profit <b>compared</b> to the unconstrained <b>model</b>. On the other hand, we observe the least profit decrease of less than 5 % for the prejudice remover, which also attains a similar AUC as the unconstrained <b>model</b>. At the same time, the prejudice remover provides a smaller fairness improvement than other processors. These results emphasize the trade-off between profit and fairness ...", "dateLastCrawled": "2022-01-26T22:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Machine learning fairness notions: Bridging the</b> gap with real-world ...", "url": "https://www.sciencedirect.com/science/article/pii/S0306457321001321", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0306457321001321", "snippet": "Likewise, no unresolved discrimination might be equivalent to <b>equalized</b> <b>odds</b> (Section 5.3) in a causal context if the set of resolving variables is the singleton set of actual outcomes: {Y}. <b>Compared</b> to counterfactual fairness, no unresolved discrimination is a weaker notion. That is, a counterfactually unfair scenario may be identified as fair based on no unresolved discrimination. This <b>can</b> happen in case one or several variables in the causal graph are identified as resolving.", "dateLastCrawled": "2022-02-03T07:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Cyberbullying Detection with Fairness Constraints</b> | DeepAI", "url": "https://deepai.org/publication/cyberbullying-detection-with-fairness-constraints", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>cyberbullying-detection-with-fairness-constraints</b>", "snippet": "Cyberbullying is a widespread adverse phenomenon among online social interactions in today&#39;s digital society. While numerous computational studies focus on enhancing the cyberbullying detection performance of machine learning algorithms, proposed models tend to carry and reinforce unintended social biases. In this study, we try to answer the research question of &quot;<b>Can</b> we mitigate the unintended bias of cyberbullying detection models by guiding the <b>model</b> training with fairness constraints?&quot;.", "dateLastCrawled": "2022-01-31T23:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Infant Perception of Non-Native Consonant Contrasts that Adults ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2773797/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC2773797", "snippet": "Infants under 6\u20138 months of age <b>can</b> discriminate both native and non-native ... This suggests that older infants (i.e., 10\u201312 months) should still be <b>able</b> to discriminate the lateral fricatives <b>well</b> as two nonexperienced nonprototypes, but they should show a modest age-related decline in discriminating the velar stops, which include a native-English prototype ([k h]) and a clear nonprototype. They should show a sharp decline in discriminating the bilabials, both of which are prototypical ...", "dateLastCrawled": "2022-02-02T12:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Equalizing Financial Impact in Supervised Learning | DeepAI", "url": "https://deepai.org/publication/equalizing-financial-impact-in-supervised-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/equalizing-financial-impact-in-supervised-learning", "snippet": "Some proponents of machine learning have suggested that not only are these algorithms <b>able</b> to leverage the increasing amount of data we have access to, but also that they might be <b>able</b> to make these decisions more fairly, as they seem to not be subject to human biases. There is some truth to these claims. For example, an algorithm to judge whether criminal defendants will recidivate <b>can</b> be used nationwide (or potentially even world-wide). This <b>can</b> alleviate biases that may be local. For ...", "dateLastCrawled": "2022-01-13T12:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Research Methods and Statistical Terms Flashcards | <b>Quizlet</b>", "url": "https://quizlet.com/3176831/research-methods-and-statistical-terms-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/3176831/research-methods-and-statistical-terms-flash-cards", "snippet": "process of evaluating claims or hypotheses and making judgments about them on the basis of <b>well</b>-supported evidence. hypothesis. a prediction stated as a testable proposition, usually in the form of an if-then statement. variables . specific factors or characteristics that are manipulated and measured in research. data. numbers that represent research findings and provide the basis for research conclusions. operational definition. a statement of the specific methods used to measure a variable ...", "dateLastCrawled": "2020-10-23T22:57:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine Learning</b> Glossary | <b>Google Developers</b>", "url": "https://developers.google.com/machine-learning/glossary/", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine-learning</b>/glossary", "snippet": "A <b>machine learning</b> technique that iteratively combines a set of simple and not very accurate classifiers ... Contrast with <b>equalized</b> <b>odds</b> and equality of opportunity, which permit classification results in aggregate to depend on sensitive attributes, but do not permit classification results for certain specified ground-truth labels to depend on sensitive attributes. See &quot;Attacking discrimination with smarter <b>machine learning</b>&quot; for a visualization exploring the tradeoffs when optimizing for ...", "dateLastCrawled": "2022-02-03T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Residual Unfairness in Fair <b>Machine</b> <b>Learning</b> from Prejudiced Data", "url": "http://proceedings.mlr.press/v80/kallus18a/kallus18a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v80/kallus18a/kallus18a.pdf", "snippet": "chine <b>learning</b> has considered some subcomponents of the overall problem we study of <b>learning</b> fair policies from bi-ased datasets. Hardt et al. (2016) formalize the criteria of equal opportunity and <b>equalized</b> <b>odds</b>. Lum &amp; Isaac (2016) show that a predictive policing algorithm for drug enforce-ment in Oakland, trained on police records, will ...", "dateLastCrawled": "2022-01-31T07:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A Survey on Bias and Fairness in <b>Machine</b> <b>Learning</b> \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/1908.09635/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1908.09635", "snippet": "With the widespread use of AI systems and applications in our everyday lives, it is important to take fairness issues into consideration while designing and engineering these types of systems. Such systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that the decisions do not reflect discriminatory behavior toward certain groups or populations. We have recently seen work in <b>machine</b> <b>learning</b>, natural language ...", "dateLastCrawled": "2021-11-15T17:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Human-centric Approach to Fairness in AI", "url": "https://www.timlrx.com/blog/a-human-centric-approach-to-fairness-in-ai", "isFamilyFriendly": true, "displayUrl": "https://www.timlrx.com/blog/a-human-centric-approach-to-fairness-in-ai", "snippet": "We embed the evaluation of AI fairness within the best practices of <b>machine</b> <b>learning</b> development and operations such as version control, ... This includes measures such as Demographic Parity / Statistical Parity (Dwork et al., 2012), <b>Equalized</b> <b>Odds</b> Metric (Hardt et al., 2016) and Calibration within Groups (Chouldechova, 2017). They are all statistical measures derived from the predictions of a classification model and differ in terms of which element(s) of the confusion matrix they are ...", "dateLastCrawled": "2022-02-03T02:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Mitigating Unwanted Biases with Adversarial Learning</b> | DeepAI", "url": "https://deepai.org/publication/mitigating-unwanted-biases-with-adversarial-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>mitigating-unwanted-biases-with-adversarial-learning</b>", "snippet": "<b>Machine</b> <b>learning</b> leverages data to build models capable of assessing the labels and properties of novel data. Unfortunately, the available training data frequently contains biases with respect to things that we would rather not use for decision making. <b>Machine</b> <b>learning</b> builds models faithful to training data and can lead to perpetuating these undesirable biases. For example, systems designed to predict creditworthiness and systems designed to perform <b>analogy</b> completion have been demonstrated ...", "dateLastCrawled": "2021-12-10T20:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Mitigating Unwanted Biases with Adversarial Learning</b>", "url": "https://www.aies-conference.com/2018/contents/papers/main/AIES_2018_paper_162.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.aies-conference.com/2018/contents/papers/main/AIES_2018_paper_162.pdf", "snippet": "<b>Machine</b> <b>learning</b> is a tool for building models that accurately represent input training data. When undesired biases concern- ing demographic groups are in the training data, well-trained models will re\ufb02ect those biases. We present a framework for mitigating such biases by including a variable for the group of interest and simultaneously <b>learning</b> a predictor and an ad-versary. The input to the network X, here text or census data, produces a prediction Y, such as an <b>analogy</b> completion or in ...", "dateLastCrawled": "2021-12-17T22:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "RStudio AI Blog: Starting to think about AI Fairness", "url": "https://blogs.rstudio.com/ai/posts/2021-07-15-ai-fairness/", "isFamilyFriendly": true, "displayUrl": "https://blogs.rstudio.com/ai/posts/2021-07-15-ai-fairness", "snippet": "Papers on fairness in <b>machine</b> <b>learning</b>, as is common in fields like computer science, abound with formulae. Even the papers referenced here, though selected not for their theorems and proofs but for the ideas they harbor, are no exception. But to start thinking about fairness as it might apply to an ML process at hand, common language \u2013 and common sense \u2013 will do just fine. If, after analyzing your use case, you judge that the more technical results are relevant to the process in ...", "dateLastCrawled": "2022-01-31T09:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Classification - Fairness and <b>machine</b> <b>learning</b>", "url": "https://fairmlbook.org/classification.html", "isFamilyFriendly": true, "displayUrl": "https://fairmlbook.org/classification.html", "snippet": "Simply put, the goal of classification is to determine a plausible value for an unknown variable Y given an observed variable X.For example, we might try to predict whether a loan applicant will pay back her loan by looking at various characteristics such as credit history, income, and net worth. Classification also applies in situations where the variable Y does not refer to an event that lies in the future. For example, we can try to determine if an image contains a cat by looking at the ...", "dateLastCrawled": "2022-02-03T07:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "On Predicting Recidivism: Epistemic Risk, Tradeoffs, and Values in ...", "url": "https://www.cambridge.org/core/journals/canadian-journal-of-philosophy/article/on-predicting-recidivism-epistemic-risk-tradeoffs-and-values-in-machine-learning/7E541FA03E78C3141A65EA99A0CA6E9A", "isFamilyFriendly": true, "displayUrl": "https://www.cambridge.org/core/journals/canadian-journal-of-philosophy/article/on...", "snippet": "This paper examines the role of value judgments in the design of <b>machine</b>-<b>learning</b> (ML) systems generally and in recidivism-prediction algorithms specifically. Drawing on work on inductive and epistemic risk, the paper argues that ML systems are value laden in ways similar to human decision making, because the development and design of ML systems requires human decisions that involve tradeoffs that reflect values. In many cases, these decisions have significant\u2014and, in some cases, disparate ...", "dateLastCrawled": "2022-01-26T20:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Measuring discrimination in algorithmic <b>decision making</b> | SpringerLink", "url": "https://link.springer.com/article/10.1007/s10618-017-0506-1", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10618-017-0506-1", "snippet": "A <b>machine</b> <b>learning</b> algorithm is a procedure used for producing a predictive model from historical data. A model is a collection of decision rules used for <b>decision making</b> for new incoming data. The model would take personal characteristics as inputs (for example, income, credit history, employment status), and produce a prediction (for example, credit risk level). Fig. 1. A typical <b>machine</b> <b>learning</b> setting. Full size image. <b>Learning</b> algorithms as such cannot discriminate, because they are ...", "dateLastCrawled": "2022-01-29T20:54:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(equalized odds)  is like +(model that is able to generalize well)", "+(equalized odds) is similar to +(model that is able to generalize well)", "+(equalized odds) can be thought of as +(model that is able to generalize well)", "+(equalized odds) can be compared to +(model that is able to generalize well)", "machine learning +(equalized odds AND analogy)", "machine learning +(\"equalized odds is like\")", "machine learning +(\"equalized odds is similar\")", "machine learning +(\"just as equalized odds\")", "machine learning +(\"equalized odds can be thought of as\")", "machine learning +(\"equalized odds can be compared to\")"]}