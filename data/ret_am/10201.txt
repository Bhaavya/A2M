{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Implementing <b>Logistic Regression</b> for Tweet Sentiment Analysis ...", "url": "https://necromuralist.github.io/Neurotic-Networking/posts/nlp/implementing-twitter-logistic-regression/", "isFamilyFriendly": true, "displayUrl": "https://necromuralist.github.io/Neurotic-Networking/posts/nlp/implementing-twitter...", "snippet": "To train our <b>model</b> we need a way to measure how well (or in this case poorly) it&#39;s doing. For this we&#39;ll use the <b>Log</b> <b>Loss</b> function which is <b>the negative</b> <b>logarithm</b> of our <b>probability</b> - so for each tweet, we&#39;ll calculate \\(\\sigma\\) (which is the <b>probability</b> that it&#39;s positive) and take <b>the negative</b> <b>logarithm</b> of it to get the <b>log</b>-<b>loss</b>.. The formula for <b>loss</b>: \\[ <b>Loss</b> = - \\left( y\\<b>log</b> (p) + (1-y)\\<b>log</b> (1-p) \\right) \\]", "dateLastCrawled": "2022-01-30T20:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "About <b>loss</b> functions, regularization and joint losses : multinomial ...", "url": "https://christopher5106.github.io/deep/learning/2016/09/16/about-loss-functions-multinomial-logistic-logarithm-cross-entropy-square-errors-euclidian-absolute-frobenius-hinge.html", "isFamilyFriendly": true, "displayUrl": "https://christopher5106.github.io/deep/learning/2016/09/16/about-<b>loss</b>-functions...", "snippet": "The reason for taking <b>the negative</b> of the <b>logarithm</b> of the likelihood are. it is more convenient to work with the <b>log</b>, because the <b>log</b>-likelihood of statistically independant observation will simply be the sum of the <b>log</b>-likelihood of each observation. we usually prefer to write the objective function as a cost function to minimize. Binomial probabilities - <b>log</b> <b>loss</b> / logistic <b>loss</b> / cross-entropy <b>loss</b>. Binomial means 2 classes, which are usually 0 or 1. Each class has a <b>probability</b> \\(p ...", "dateLastCrawled": "2022-02-01T18:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Loss</b> Functions in Neural Networks", "url": "https://www.theaidream.com/post/loss-functions-in-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://www.theaidream.com/post/<b>loss</b>-functions-in-neural-networks", "snippet": "Reason for <b>the Negative</b> sign: <b>log</b>(p(x))&lt;0 for all p(x) in (0,1). p(x) is a <b>probability</b> distribution and therefore the values must range between 0 and 1. A plot of <b>log</b>(x). For x values between 0 and 1, <b>log</b>(x) &lt;0 (is <b>negative</b>). Cross-Entropy <b>loss</b> is also called logarithmic <b>loss</b>, <b>log</b> <b>loss</b>, or logistic <b>loss</b>. Each predicted class <b>probability</b> is compared to the actual class desired output 0 or 1 and a score/<b>loss</b> is calculated that penalizes the <b>probability</b> based on how far it is from the actual ...", "dateLastCrawled": "2022-02-02T10:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Understanding the <b>Log</b> <b>Loss</b> Metric | by Temiloluwa Awoyele | Medium", "url": "https://awoyeletemiloluwa.medium.com/understanding-the-log-loss-metric-15b38b4dbaff", "isFamilyFriendly": true, "displayUrl": "https://awoyeletemiloluwa.medium.com/understanding-the-<b>log</b>-<b>loss</b>-metric-15b38b4dbaff", "snippet": "The logarithmic <b>loss</b>(<b>log</b> <b>loss</b>) basically penalizes our <b>model</b> for uncertainty in correct predictions and heavily penalizes our <b>model</b> for making the <b>wrong</b> prediction. In this article, we will understand how the <b>log</b> <b>loss</b> metric works and we will be coding the <b>log</b> <b>loss</b> metric from scratch ourselves. <b>Log</b> <b>Loss</b> as the name implies is a <b>loss</b> metric and <b>loss</b> is not something we want, so we make sure to cut our losses to the barest minimum. We want our <b>log</b> <b>loss</b> score to be as small as possible, so we ...", "dateLastCrawled": "2022-01-31T00:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Cross-Entropy Loss</b> and Its Applications in Deep Learning - neptune.ai", "url": "https://neptune.ai/blog/cross-entropy-loss-and-its-applications-in-deep-learning", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/b<b>log</b>/<b>cross-entropy-loss</b>-and-its-applications-in-deep-learning", "snippet": "The product <b>probability</b> for <b>model</b> B is better than that of A. ... <b>Negative</b> Logs <b>Model</b> A:-<b>log</b>(0.1) + -<b>log</b>( 0.7) + -<b>log</b>( 0.6) + -<b>log</b>( 0.2) 1 + 0.154 + 0.221 + 0.698 = 2.073. <b>Negative</b> Logs <b>Model</b> B: -<b>log</b>(0.8) +- <b>log</b>( 0.6) + -<b>log</b>( 0.7) + -<b>log</b>( 0.9) 0.09 + 0.22 + 0.15 + 0.045 = 0.505. <b>Cross-entropy loss</b> is the sum of <b>the negative</b> <b>logarithm</b> of predicted probabilities of each student. <b>Model</b> A\u2019s <b>cross-entropy loss</b> is 2.073; <b>model</b> B\u2019s is 0.505. <b>Cross-Entropy</b> gives a good measure of how effective ...", "dateLastCrawled": "2022-02-02T02:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Logarithmic <b>Loss</b>", "url": "https://emilyswebber.github.io/LogLoss/", "isFamilyFriendly": true, "displayUrl": "https://emilyswebber.github.io/<b>LogLoss</b>", "snippet": "This is good because the function is penalizing a <b>wrong</b> answer <b>that the model</b> is \u201cconfident\u201d about. Conversely, the bottom example shows a good prediction that is close to the actual <b>probability</b>. This results in a low LogLoss, which is good because the <b>model</b> is rewarding a correct answer <b>that the model</b> is \u201cconfident\u201d about. Formula. Below is the formula for LogLoss from wikipedia. It is calculated for every observation in the dataset and then averaged. Simple right? So, this all ...", "dateLastCrawled": "2022-01-26T06:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Understanding binary <b>cross-entropy</b> / <b>log</b> <b>loss</b>: a visual explanation ...", "url": "https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-binary-<b>cross-entropy</b>-<b>log</b>-<b>loss</b>-a-visual...", "snippet": "For a binary classification <b>like</b> our example, the typical <b>loss</b> function is the binary <b>cross-entropy</b> / <b>log</b> <b>loss</b>. <b>Loss</b> Function: Binary <b>Cross-Entropy</b> / <b>Log</b> <b>Loss</b>. If you look this <b>loss</b> function up, this is what you\u2019ll find: Binary <b>Cross-Entropy</b> / <b>Log</b> <b>Loss</b>. where y is the label (1 for green points and 0 for red points) and p(y) is the predicted <b>probability</b> of the point being green for all N points. Reading this formula, it tells you that, for each green point (y=1), it adds <b>log</b>(p(y)) to the ...", "dateLastCrawled": "2022-02-03T02:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Cross Entropy and Log Likelihood</b> | Andrew M. Webb", "url": "http://www.awebb.info/probability/2017/05/18/cross-entropy-and-log-likelihood.html", "isFamilyFriendly": true, "displayUrl": "www.awebb.info/<b>probability</b>/2017/05/18/<b>cross-entropy-and-log-likelihood</b>.html", "snippet": "One source of confusion for me is that I read in a few places &quot;<b>the negative</b> <b>log</b> likelihood is the same as the cross entropy&quot; without it having been specified whether they are talking about a per-example <b>loss</b> function or a batch <b>loss</b> function over a number of examples. As we saw above, the per-example <b>negative</b> <b>log</b> likelihood can indeed be interpreted as cross entropy. However, <b>the negative</b> <b>log</b> likelihood of a batch of data (which is just the sum of <b>the negative</b> <b>log</b> likelihoods of the ...", "dateLastCrawled": "2022-02-03T03:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "If you use <b>the negative</b> <b>log</b> likelihood as a <b>loss</b> function, should you ...", "url": "https://www.quora.com/If-you-use-the-negative-log-likelihood-as-a-loss-function-should-you-receive-negative-values", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/If-you-use-<b>the-negative</b>-<b>log</b>-<b>like</b>lihood-as-a-<b>loss</b>-function-should...", "snippet": "Answer: If it&#39;s a proper likelihood (i.e. between 0 and 1), then the <b>log</b> likelihood is between <b>negative</b> infinity and zero, and therefore <b>the negative</b> <b>log</b> likelihood is between zero and positive infinity. If your likelihood comes from a <b>probability</b> density, <b>the negative</b> <b>log</b> likelihood can take bo...", "dateLastCrawled": "2022-01-26T00:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "aic - Can <b>log likelihood</b> function be <b>negative</b> - Cross Validated", "url": "https://stats.stackexchange.com/questions/346720/can-log-likelihood-function-be-negative", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/346720/can-<b>log-likelihood</b>-function-be-<b>negative</b>", "snippet": "<b>Probability</b> densities are non-<b>negative</b>, while probabilities also are less or equal to one. It follows that their product cannot be <b>negative</b>. The natural <b>logarithm</b> function is <b>negative</b> for values less than one and positive for values greater than one. So yes, it is possible that you end up with a <b>negative</b> value for <b>log-likelihood</b> (for discrete variables it will always be so). Share. Cite. Improve this answer. Follow edited May 17 &#39;18 at 12:31. Nick Cox. 48k 8 8 gold badges 110 110 silver ...", "dateLastCrawled": "2022-01-28T04:30:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>What is an intuitive explanation for the log</b> <b>loss</b> function? - Quora", "url": "https://www.quora.com/What-is-an-intuitive-explanation-for-the-log-loss-function", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>What-is-an-intuitive-explanation-for-the-log</b>-<b>loss</b>-function", "snippet": "Answer (1 of 8): To me an intuitive explanation is that minimizing the <b>log</b> <b>loss</b> equals minimizing the Kullback-Leibler divergence (Kullback\u2013Leibler divergence - Wikipedia) between the function you want to optimize (for example a neural network) and the true function that generates the data (from ...", "dateLastCrawled": "2022-01-30T02:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "If you use the <b>negative</b> <b>log</b> likelihood as a <b>loss</b> function, should you ...", "url": "https://www.quora.com/If-you-use-the-negative-log-likelihood-as-a-loss-function-should-you-receive-negative-values", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/If-you-use-the-<b>negative</b>-<b>log</b>-likelihood-as-a-<b>loss</b>-function-should...", "snippet": "Answer: If it&#39;s a proper likelihood (i.e. between 0 and 1), then the <b>log</b> likelihood is between <b>negative</b> infinity and zero, and therefore the <b>negative</b> <b>log</b> likelihood is between zero and positive infinity. If your likelihood comes from a <b>probability</b> density, the <b>negative</b> <b>log</b> likelihood can take bo...", "dateLastCrawled": "2022-01-26T00:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Estimators, <b>Loss</b> Functions, Optimizers \u2014Core of ML Algorithms | by ...", "url": "https://towardsdatascience.com/estimators-loss-functions-optimizers-core-of-ml-algorithms-d603f6b0161a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/estimators-<b>loss</b>-functions-<b>optimizer</b>s-core-of-ml...", "snippet": "The graph below shows the range of possible <b>log</b> <b>loss</b> values given a true observation (y= 1). As the predicted <b>probability</b> approaches 1, <b>log</b> <b>loss</b> slowly decreases. As the predicted <b>probability</b> decreases, however, the <b>log</b> <b>loss</b> increases rapidly. <b>Log</b> <b>loss</b> penalizes both types of errors, but especially those predictions that are confident and <b>wrong</b>! Multi-class Classification. In multi-class classification (M&gt;2), we take the sum of <b>log</b> <b>loss</b> values for each class prediction in the observation ...", "dateLastCrawled": "2022-02-02T04:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A30: Logistic Regression (Part-2)&gt;&gt; Behind the Scene! | by Junaid Qazi ...", "url": "https://medium.com/mlearning-ai/a30-logistic-regression-part-2-behind-the-scene-38a98b70192a", "isFamilyFriendly": true, "displayUrl": "https://medium.com/mlearning-ai/a30-<b>log</b>istic-regression-part-2-behind-the-scene-38a98b...", "snippet": "<b>Log</b>-odds can take any positive or <b>negative</b> value, and the purpose of the logit link is to take a linear combination of the covariate (dependent, or response) values, between \u2212\u221e and \u221e, and ...", "dateLastCrawled": "2022-02-03T12:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Cross Entropy</b> for Tensorflow | Mustafa Murat ARAT", "url": "https://mmuratarat.github.io/2018-12-21/cross-entropy", "isFamilyFriendly": true, "displayUrl": "https://mmuratarat.github.io/2018-12-21/<b>cross-entropy</b>", "snippet": "The plot below shows the <b>Log</b> <b>Loss</b> contribution from a single positive instance where the predicted <b>probability</b> ranges from 0 (the completely <b>wrong</b> prediction) to 1 (the correct prediction). It\u2019s apparent from the gentle downward slope towards the right that the <b>Log</b> <b>Loss</b> gradually declines as the predicted <b>probability</b> improves. Moving in the opposite direction though, the <b>Log</b> <b>Loss</b> ramps up very rapidly as the predicted <b>probability</b> approaches 0.", "dateLastCrawled": "2022-01-27T22:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "machine learning - <b>Multi-class</b> logarithmic <b>loss</b> function per class ...", "url": "https://stats.stackexchange.com/questions/113301/multi-class-logarithmic-loss-function-per-class", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/113301", "snippet": "As you rightly pointed out, a pure classifier (with <b>probability</b> 1) will have <b>log</b> <b>loss</b> of 0, which is the preferred case. Consider a classifier that assigns labels in a completely random manner. <b>Probability</b> of assigning to the correct class will be 1/M. Therefore, the <b>log</b> <b>loss</b> for each observation will be -<b>log</b>(1/M) = <b>log</b>(M). This is label independent. <b>Log</b> <b>loss</b> for an individual observation can be compared with this value to check how well the classifier is performing with respect to random ...", "dateLastCrawled": "2022-01-25T23:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Understanding <b>Logistic Regression</b> Coefficients | by Ravi Charan ...", "url": "https://towardsdatascience.com/understanding-logistic-regression-coefficients-7a719ebebd35", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-<b>logistic-regression</b>-coefficients-7a719ebebd35", "snippet": "My goal is convince you to adopt a third: the <b>log</b>-odds, or the <b>logarithm</b> of the odds. For interpretation, ... You can check that the cross entropy <b>loss</b> (also called the <b>log</b>-<b>loss</b> or deviance) may be described as follows. Let S evidence be given by the <b>model</b> in favor of the <b>wrong</b> prediction. Then, in the limit as S is large, the <b>loss</b> is S. Conversely, if S is the evidence given in favor of the correct prediction, then, in the limit as S is large, the cross-entropy <b>loss</b> is exp(-S). Part 3 ...", "dateLastCrawled": "2022-02-03T05:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>An Introduction to Objective Functions Used</b> in Machine Learning", "url": "https://mlbhanuyerra.github.io/2019-03-07-Objective-Functions/", "isFamilyFriendly": true, "displayUrl": "https://mlbhanuyerra.github.io/2019-03-07-Objective-Functions", "snippet": "Target variable should be non-<b>negative</b>, but can be equal to zero. If the target is never zero, the addition of 1 in the <b>logarithm</b> can be dropped. Regularization: In cases where the <b>model</b> complexity or limited data points leads to over-fitting of regression models, a common technique known as regularization is employed. Regularization adds a ...", "dateLastCrawled": "2022-01-29T17:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A Note on <b>Using Log-Likelihood for Generative Models</b> | <b>Bounded Rationality</b>", "url": "https://bjlkeng.github.io/posts/a-note-on-using-log-likelihood-for-generative-models/", "isFamilyFriendly": true, "displayUrl": "https://bjlkeng.github.io/posts/a-note-on-<b>using-log-likelihood-for-generative-models</b>", "snippet": "This means that we can directly interpret our average <b>log</b>-likelihood <b>loss</b> in terms of cross entropy, which gives us the &quot;average number of bits (using base 2 <b>logarithm</b>) needed to code a sample from \\(p_{true}\\) using our <b>model</b> \\(P\\)&quot;. Dividing this by the number of pixels, gives us the &quot;bits per pixel&quot; metric that we see often in papers (e.g. [4], [5]).", "dateLastCrawled": "2022-01-28T11:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "5 Regression <b>Loss</b> Functions All Machine Learners Should Know | by ...", "url": "https://heartbeat.comet.ml/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0", "isFamilyFriendly": true, "displayUrl": "https://heartbeat.comet.ml/5-regression-<b>loss</b>-functions-all-machine-learners-should...", "snippet": "But <b>Log</b>-cosh <b>loss</b> isn\u2019t perfect. It still suffers from the problem of gradient and hessian for very large off-target predictions being constant, therefore resulting in the absence of splits for XGBoost. Python code for Huber and <b>Log</b>-cosh <b>loss</b> functions: 5. Quantile <b>Loss</b>. In most of the real-world prediction problems, we are often interested to know about the uncertainty in our predictions. Knowing about the range of predictions as opposed to only point estimates can significantly improve ...", "dateLastCrawled": "2022-02-02T07:34:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Cross-Entropy Loss</b> and Its Applications in Deep Learning - neptune.ai", "url": "https://neptune.ai/blog/cross-entropy-loss-and-its-applications-in-deep-learning", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/b<b>log</b>/<b>cross-entropy-loss</b>-and-its-applications-in-deep-learning", "snippet": "The product <b>probability</b> for <b>model</b> B is better than that of A. ... <b>Negative</b> Logs <b>Model</b> A:-<b>log</b>(0.1) + -<b>log</b>( 0.7) + -<b>log</b>( 0.6) + -<b>log</b>( 0.2) 1 + 0.154 + 0.221 + 0.698 = 2.073. <b>Negative</b> Logs <b>Model</b> B: -<b>log</b>(0.8) +- <b>log</b>( 0.6) + -<b>log</b>( 0.7) + -<b>log</b>( 0.9) 0.09 + 0.22 + 0.15 + 0.045 = 0.505. <b>Cross-entropy loss</b> is the sum of the <b>negative</b> <b>logarithm</b> of predicted probabilities of each student. <b>Model</b> A\u2019s <b>cross-entropy loss</b> is 2.073; <b>model</b> B\u2019s is 0.505. <b>Cross-Entropy</b> gives a good measure of how effective ...", "dateLastCrawled": "2022-02-02T02:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Understanding binary <b>cross-entropy</b> / <b>log</b> <b>loss</b>: a visual explanation ...", "url": "https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-binary-<b>cross-entropy</b>-<b>log</b>-<b>loss</b>-a-visual...", "snippet": "Binary <b>Cross-Entropy</b> / <b>Log</b> <b>Loss</b>. where y is the label (1 for green points and 0 for red points) and p(y) is the predicted <b>probability</b> of the point being green for all N points.. Reading this formula, it tells you that, for each green point (y=1), it adds <b>log</b>(p(y)) to the <b>loss</b>, that is, the <b>log</b> <b>probability</b> of it being green.Conversely, it adds <b>log</b>(1-p(y)), that is, the <b>log</b> <b>probability</b> of it being red, for each red point (y=0).Not necessarily difficult, sure, but no so intuitive too\u2026", "dateLastCrawled": "2022-02-03T02:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Model</b> Selection Techniques", "url": "https://people.duke.edu/~vt45/DTY-overview.pdf", "isFamilyFriendly": true, "displayUrl": "https://people.duke.edu/~vt45/DTY-overview.pdf", "snippet": "A typical data analysis <b>can</b> <b>be thought</b> of as consisting of two steps. ... (=- <b>log</b> z ), (3) the <b>negative</b> <b>logarithm</b> of the distribution of z t. Then, (2) pro-duces the MLE for a parametric <b>model</b>. For time series data, (3) is written as -logpz^h tt zz 11,,f - , and the quadratic <b>loss</b> sp(, zz tt) Ez pt zz 11,, t =- f 2&quot; ^h-, is often used, where the expectation is taken over the joint distribution p of zz 1,,f t. The best <b>model</b> Let ppt m = iu m denote the estimated distribution under <b>model</b> M m ...", "dateLastCrawled": "2022-02-02T02:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Probabilistic and Deterministic Mindsets of Logistic Regression ...", "url": "https://medium.com/analytics-vidhya/probabilistic-and-deterministic-mindsets-of-logistic-regression-4786cb126ce3?source=post_internal_links---------7-------------------------------", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/probabilistic-and-deterministic-mindsets-of...", "snippet": "This equation is known as Logistic <b>loss</b> or Cross-Entropy <b>loss</b>. For y=0, it transforms to the -<b>log</b> (1-p), and its minimum value 1 is reached at p=0; for y=1, we need to minimize -<b>log</b> p, which ...", "dateLastCrawled": "2021-12-14T02:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Cross Entropy</b> for Tensorflow | Mustafa Murat ARAT", "url": "https://mmuratarat.github.io/2018-12-21/cross-entropy", "isFamilyFriendly": true, "displayUrl": "https://mmuratarat.github.io/2018-12-21/<b>cross-entropy</b>", "snippet": "As you <b>can</b> see, maximizing the <b>log</b>-likelihood (minimizing the <b>negative</b> <b>log</b>-likelihood) is equivalent to minimizing the binary <b>cross entropy</b>. Let\u2019s take a closer look at this relationship. The plot below shows the <b>Log</b> <b>Loss</b> contribution from a single positive instance where the predicted <b>probability</b> ranges from 0 (the completely <b>wrong</b> prediction) to 1 (the correct prediction).", "dateLastCrawled": "2022-01-27T22:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 5, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Kullback\u2013Leibler divergence</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Kullback\u2013Leibler_divergence</b>", "snippet": "The self-information, also known as the information content of a signal, random variable, or event is defined as the <b>negative</b> <b>logarithm</b> <b>of the probability</b> of the given outcome occurring.. When applied to a discrete random variable, the self-information <b>can</b> be represented as [citation needed] \u2061 = ({}), is the relative entropy <b>of the probability</b> distribution () from a Kronecker delta representing certainty that = \u2014 i.e. the number of extra bits that must be transmitted to identify if only ...", "dateLastCrawled": "2022-02-07T07:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Cost Function and <b>Loss</b> Function in Machine Learning - Shishir Kant Singh", "url": "http://shishirkant.com/cost-function-and-loss-function-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "shishirkant.com/cost-function-and-<b>loss</b>-function-in-machine-learning", "snippet": "Going by this, predicting a <b>probability</b> of .011 when the actual observation label is 1 would result in a high <b>loss</b> value. In an ideal situation, a \u201cperfect\u201d <b>model</b> would have a <b>log</b> <b>loss</b> of 0. Looking at the <b>loss</b> function would make things even clearer \u2013 Where y i is the true label and h \u03b8 (x i) is the predicted value post hypothesis.", "dateLastCrawled": "2022-01-29T23:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "CNN results <b>negative</b> when using <b>log</b>_softmax and nll <b>loss</b> - PyTorch Forums", "url": "https://discuss.pytorch.org/t/cnn-results-negative-when-using-log-softmax-and-nll-loss/16839", "isFamilyFriendly": true, "displayUrl": "https://discuss.pytorch.org/t/cnn-results-<b>negative</b>-when-using-<b>log</b>-softmax-and-nll-<b>loss</b>/...", "snippet": "Hi all, I\u2019m using the nll_<b>loss</b> function in conjunction with <b>log</b>_softmax as advised in the documentation when creating a CNN. However, when I test new images, I get <b>negative</b> numbers rather than 0-1 limited results. This is really strange given the bound nature of the softmax function and I was wondering if anyone has encountered this problem or <b>can</b> see where I\u2019m going <b>wrong</b>? import torch from torch.autograd import Variable import torch.nn as nn import torch.nn.functional as F import torch ...", "dateLastCrawled": "2022-02-01T06:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Can information be negative</b>? - ResearchGate", "url": "https://www.researchgate.net/post/Can_information_be_negative", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/post/<b>Can_information_be_negative</b>", "snippet": "Because the <b>logarithm</b> of a real number between zero and one is <b>negative</b>, information (as per the <b>model</b> above) is always positive. I&#39;ve written a primer (see my profile) to explain information ...", "dateLastCrawled": "2022-01-27T13:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "On regression to minimize <b>log</b> distance rather than distance - Data ...", "url": "https://datascience.stackexchange.com/questions/66901/on-regression-to-minimize-log-distance-rather-than-distance", "isFamilyFriendly": true, "displayUrl": "https://<b>datascience.stackexchange</b>.com/questions/66901/on-regression-to-minimize-<b>log</b>...", "snippet": "Suppose I have a lot of points $ x_i \\in \\mathbb{R}^N $ with corresponding non-<b>negative</b> labels $ y_i \\in \\mathbb{R} $ and I want to do regression and make a prediction on some new datapoint $ x^* \\... Stack Exchange Network . Stack Exchange network consists of 178 Q&amp;A communities including Stack Overflow, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers. Visit Stack Exchange. Loading\u2026 0 +0; Tour Start here for a quick ...", "dateLastCrawled": "2022-01-25T03:42:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Loss</b> Functions in Neural Networks", "url": "https://www.theaidream.com/post/loss-functions-in-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://www.theaidream.com/post/<b>loss</b>-functions-in-neural-networks", "snippet": "Reason for the <b>Negative</b> sign: <b>log</b>(p(x))&lt;0 for all p(x) in (0,1). p(x) is a <b>probability</b> distribution and therefore the values must range between 0 and 1. A plot of <b>log</b>(x). For x values between 0 and 1, <b>log</b>(x) &lt;0 (is <b>negative</b>). Cross-Entropy <b>loss</b> is also called logarithmic <b>loss</b>, <b>log</b> <b>loss</b>, or logistic <b>loss</b>. Each predicted class <b>probability</b> is <b>compared</b> to the actual class desired output 0 or 1 and a score/<b>loss</b> is calculated that penalizes the <b>probability</b> based on how far it is from the actual ...", "dateLastCrawled": "2022-02-02T10:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "What is sigmoid cross entropy?", "url": "https://philosophy-question.com/library/lecture/read/37641-what-is-sigmoid-cross-entropy", "isFamilyFriendly": true, "displayUrl": "https://philosophy-question.com/library/lecture/read/37641-what-is-sigmoid-cross-entropy", "snippet": "Also called logarithmic <b>loss</b>, <b>log</b> <b>loss</b> or logistic <b>loss</b>. Each predicted class <b>probability</b> is <b>compared</b> to the actual class desired output 0 or 1 and a score/<b>loss</b> is calculated that penalizes the <b>probability</b> based on how far it is from the actual expected value. How do you calculate LogLoss? The LogLoss Formula. LogLoss=\u22121nn\u2211i=1[yi\u22c5loge(^yi)+(1\u2212yi)\u22c5loge(1\u2212^yi)] \u22121nn\u2211i=1[yi\u22c5loge(^yi)+(1\u2212yi)\u22c5loge(1\u2212^yi)] n\u2211i=1[yi\u22c5loge(^yi)+(1\u2212yi)\u22c5loge(1\u2212^yi)] <b>Can</b> cross entropy ...", "dateLastCrawled": "2022-01-25T01:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "machine learning - <b>Multi-class</b> logarithmic <b>loss</b> function per class ...", "url": "https://stats.stackexchange.com/questions/113301/multi-class-logarithmic-loss-function-per-class", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/113301", "snippet": "<b>Log</b> <b>loss</b> for an individual observation <b>can</b> <b>be compared</b> with this value to check how well the classifier is performing with respect to random classification. However, this may not make much sense. Let us take an example. Consider a powerful classifier which misclassified an observation. Let us assume that the observation actually belongs to class &#39;x&#39; and the predicted <b>probability</b> of belonging to class is 0 (nearly). Therefore, the individual and overall value of <b>log</b> <b>loss</b> will be Inf. This is ...", "dateLastCrawled": "2022-01-25T23:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A30: Logistic Regression (Part-2)&gt;&gt; Behind the Scene! | by Junaid Qazi ...", "url": "https://medium.com/mlearning-ai/a30-logistic-regression-part-2-behind-the-scene-38a98b70192a", "isFamilyFriendly": true, "displayUrl": "https://medium.com/mlearning-ai/a30-<b>log</b>istic-regression-part-2-behind-the-scene-38a98b...", "snippet": "<b>Log</b>-odds <b>can</b> take any positive or <b>negative</b> value, and the purpose of the logit link is to take a linear combination of the covariate (dependent, or response) values, between \u2212\u221e and \u221e, and ...", "dateLastCrawled": "2022-02-03T12:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Log</b> rules | <b>logarithm rules</b> - <b>RapidTables.com</b>", "url": "https://www.rapidtables.com/math/algebra/Logarithm.html", "isFamilyFriendly": true, "displayUrl": "https://<b>www.rapidtables.com</b>/math/algebra/<b>Logarithm</b>.html", "snippet": "See: <b>log</b> base change rule. <b>Logarithm</b> of <b>negative</b> number. The base b real <b>logarithm</b> of x when x&lt;=0 is undefined when x is <b>negative</b> or equal to zero: <b>log</b> b (x) is undefined when x \u2264 0. See: <b>log</b> of <b>negative</b> number. <b>Logarithm</b> of 0. The base b <b>logarithm</b> of zero is undefined: <b>log</b> b (0) is undefined. The limit of the base b <b>logarithm</b> of x, when x approaches zero, is minus infinity: See: <b>log</b> of zero. <b>Logarithm</b> of 1. The base b <b>logarithm</b> of one is zero: <b>log</b> b (1) = 0. For example, teh base two ...", "dateLastCrawled": "2022-02-02T07:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "optimization - Why to <b>optimize max log probability instead</b> of ...", "url": "https://stats.stackexchange.com/questions/174481/why-to-optimize-max-log-probability-instead-of-probability", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/174481", "snippet": "Moreover, for sufficient sample sizes the <b>probability</b> density of any simple sample from a parametric <b>model</b> will eventually be less than $2^{-127}$. In large problems (with millions of data), <b>probability</b> densities routinely are $2^{-1000000}$ or smaller. Even a sample of size $80$ from the standard Normal distribution is almost certain to have a <b>probability</b> density less than $2^{-127}$.", "dateLastCrawled": "2022-02-02T22:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "5 Regression <b>Loss</b> Functions All Machine Learners Should Know | by ...", "url": "https://heartbeat.comet.ml/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0", "isFamilyFriendly": true, "displayUrl": "https://heartbeat.comet.ml/5-regression-<b>loss</b>-functions-all-machine-learners-should...", "snippet": "But <b>Log</b>-cosh <b>loss</b> isn\u2019t perfect. It still suffers from the problem of gradient and hessian for very large off-target predictions being constant, therefore resulting in the absence of splits for XGBoost. Python code for Huber and <b>Log</b>-cosh <b>loss</b> functions: 5. Quantile <b>Loss</b>. In most of the real-world prediction problems, we are often interested to know about the uncertainty in our predictions. Knowing about the range of predictions as opposed to only point estimates <b>can</b> significantly improve ...", "dateLastCrawled": "2022-02-02T07:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A Brief Overview of <b>Loss</b> Functions in <b>Pytorch</b> | by Pratyaksha Jha ...", "url": "https://medium.com/udacity-pytorch-challengers/a-brief-overview-of-loss-functions-in-pytorch-c0ddb78068f7", "isFamilyFriendly": true, "displayUrl": "https://medium.com/udacity-<b>pytorch</b>-challengers/a-brief-overview-of-<b>loss</b>-functions-in-p...", "snippet": "If x &gt; 0 <b>loss</b> will be x itself (higher value), if 0&lt;x&lt;1 <b>loss</b> will be 1 \u2014 x (smaller value) and if x &lt; 0 <b>loss</b> will be 0 (minimum value). For y =1, the <b>loss</b> is as high as the value of x .", "dateLastCrawled": "2022-02-02T14:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>7.2.1 Evaluating</b> Predictions\u2023 7.2 Supervised Learning \u2023 Chapter 7 ...", "url": "https://artint.info/2e/html/ArtInt2e.Ch7.S2.SS1.html", "isFamilyFriendly": true, "displayUrl": "https://artint.info/2e/html/ArtInt2e.Ch7.S2.SS1.html", "snippet": "The predicted and actual values <b>can</b> <b>be compared</b> numerically. There is nothing special about {0, 1} for binary features; it is possible to use {-1, 1} or to use zero and non-zero. \u2022 In a cardinal feature the values are mapped to real numbers. This is appropriate when values in the domain of Y are totally ordered, and the differences between the values are meaningful. In this case, the predicted and actual values <b>can</b> <b>be compared</b> on this scale. Often, mapping values to the real line is not ...", "dateLastCrawled": "2021-11-10T09:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "math - Why use <b>softmax</b> as opposed to standard normalization? - Stack ...", "url": "https://stackoverflow.com/questions/17187507/why-use-softmax-as-opposed-to-standard-normalization", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/17187507", "snippet": "@KilianBatzner If a neuron&#39;s output is a <b>log</b> <b>probability</b>, then the summation of many neurons&#39; outputs is a multiplication of their probabilities. That&#39;s more commonly useful than a sum of probabilities. \u2013 alltom. Jul 27 &#39;17 at 16:05. 1 @KilianBatzner If your features come from different Gaussian clusters (on cluster per class) then you <b>can</b> derive a perfect classifier (logistic regression). There are some additional conditions, but essentially you <b>can</b> justify/derive <b>softmax</b> and logits with ...", "dateLastCrawled": "2022-01-27T12:11:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "1 Motivation: <b>log</b> <b>loss</b> in online <b>learning</b>.", "url": "https://www.cs.princeton.edu/courses/archive/spring08/cos511/scribe_notes/0421.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.princeton.edu/courses/archive/spring08/cos511/scribe_notes/0421.pdf", "snippet": "COS 511: Theoretical <b>Machine</b> <b>Learning</b> Lecturer: Rob Schapire Lecture #20 Scribe: Joe Wenjie Jiang April 21, 2008 1 Motivation: <b>log</b> <b>loss</b> in online <b>learning</b>. In the last lecture, we started to discuss the following model of online <b>learning</b>, in which our goal is to minimize the total <b>log</b> <b>loss</b>: Let X be the space of all possible outcomes in any time step. There are N experts from whom we can consult. At each time step t = 1,...,T: \u2022 Each expert i predicts pt,i, which is a distribution over all ...", "dateLastCrawled": "2021-11-29T18:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "TensorFlow is an interface for expressing <b>machine</b> <b>learning</b> algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PPT) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> | Colleen Farrelly - Academia.edu", "url": "https://www.academia.edu/30404581/Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/30404581/<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "<b>Machine</b> <b>Learning</b> by <b>Analogy</b> Colleen M. Farrelly Overview of Problem Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics ...", "dateLastCrawled": "2022-01-02T06:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Choosing and Customizing <b>Loss</b> Functions for Image Processing | by ...", "url": "https://towardsdatascience.com/choosing-and-customizing-loss-functions-for-image-processing-a0e4bf665b0a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/choosing-and-customizing-<b>loss</b>-functions-for-image...", "snippet": "This is what a <b>machine</b> <b>learning</b> (ML) algorithm does during training. More specifically, ... Cross-Entropy (aka <b>log</b> <b>loss</b>): calculates the differences between the predicted class probabilities and those from ground truth across a logarithmic scale. Useful for object detection. Weighted Cross-Entropy: improves on Cross-Entropy accuracy by adding weights to certain aspects (e.g., certain object classes) which are under-represented in the data (e.g., objects occurring in fewer data samples\u00b3 ...", "dateLastCrawled": "2022-01-31T10:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Machined Learnings: ML and OR: An <b>analogy</b> with cost-sensitive ...", "url": "http://www.machinedlearnings.com/2010/07/ml-and-or.html", "isFamilyFriendly": true, "displayUrl": "www.<b>machine</b>d<b>learning</b>s.com/2010/07/ml-and-or.html", "snippet": "Nonetheless I&#39;ve been amusing myself by thinking about it, in particular trying to think about it from a <b>machine</b> <b>learning</b> reduction standpoint. The simplest well-understood reduction that I can think of which is analogous to supplying estimates to a linear program is the reduction of cost-sensitive multiclass classification (CSMC) to regression.", "dateLastCrawled": "2021-12-25T17:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Why and How to use <b>Cross Entropy</b>. The fundamental reasons for ...", "url": "https://towardsdatascience.com/why-and-how-to-use-cross-entropy-4e983cbdd873", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/why-and-how-to-use-<b>cross-entropy</b>-4e983cbdd873", "snippet": "A convenient way to think of <b>log</b> <b>loss</b> is as follows: If the model predicts that an observation should be labeled 1 and assigns a high probability to that prediction, a high penalty will be incurred when the true label is 0. If the model had assigned a lower probability to that prediction, a lower penalty would have been incurred. The reason for taking the <b>log</b> of predicted probabilities goes back to the original formulation of entropy. Information Theory looks at entropy as a measure of ...", "dateLastCrawled": "2022-01-31T14:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Probabilistic Model Selection with AIC/BIC in <b>Python</b> | by Shachi Kaul ...", "url": "https://medium.com/analytics-vidhya/probabilistic-model-selection-with-aic-bic-in-python-f8471d6add32", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/probabilistic-model-selection-with-aic-bic-in...", "snippet": "Dear <b>learning</b> souls..sit in a comfortable posture, set your focus, and let\u2019s kick-off this dilemma of selecting your best <b>machine</b> <b>learning</b> model. Presenting a secret of how to choose the best ...", "dateLastCrawled": "2022-02-03T02:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>machine</b> <b>learning</b> - What is the relation between a <b>loss</b> function and an ...", "url": "https://stats.stackexchange.com/questions/409247/what-is-the-relation-between-a-loss-function-and-an-energy-function", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/409247/what-is-the-relation-between-a-<b>loss</b>...", "snippet": "A <b>loss</b> function is a function that measures the distance between the expected value and the actual value of a model (an example of a <b>loss</b> function is the cross entropy).. An energy function can be defined as a function that we want to minimise or maximise and it is a function of the variables of the system. It is referred to as &quot;energy function&quot; because it is often related or compared to the concept of &quot;energy&quot; in physics. These two expression seem to refer to the same concept.", "dateLastCrawled": "2022-01-17T15:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "What is the name of this <b>loss</b> function? : learnmachinelearning", "url": "https://www.reddit.com/r/learnmachinelearning/comments/s0hrxh/what_is_the_name_of_this_loss_function/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/learn<b>machinelearning</b>/comments/s0hrxh/what_is_the_name_of_this...", "snippet": "I have been studying and practicing <b>Machine</b> <b>Learning</b> and Computer Vision for 7+ years. As time has passed I have realized more and more the power of data-driven decision-making. Seeing firsthand what ML is capable of I have personally felt that it can be a great inter-disciplinary tool to automate workflows. I will bring up different topics of ML in the form of short notes which can be of interest to existing practitioners and fresh enthusiasts alike.", "dateLastCrawled": "2022-01-12T12:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>machine</b> <b>learning</b> - Why we need to pass values using feed_dict to print ...", "url": "https://stackoverflow.com/questions/51407644/why-we-need-to-pass-values-using-feed-dict-to-print-loss-value-in-tensorflow", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/51407644", "snippet": "A useful <b>analogy</b> might be to think of your TensorFlow computational graph as a physical <b>machine</b> \u2013 with inputs pipes (x and y) and output pipes (<b>loss</b>). The <b>machine</b> consumes data from the input pipes (so the data doesn&#39;t remain across multiple calls), and the <b>machine</b> also spits out stuff from the output pipes \u2013 if you didn&#39;t catch the output, you lost it. The <b>machine</b> (graph) doesn&#39;t", "dateLastCrawled": "2021-11-27T09:00:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Predicting the 2019 All-<b>NBA teams with machine learning</b> - <b>Dribble Analytics</b>", "url": "https://dribbleanalytics.blog/2019/03/ml-all-nba-predict/", "isFamilyFriendly": true, "displayUrl": "https://<b>dribbleanalytics</b>.blog/2019/03/ml-all-nba-predict", "snippet": "<b>Log loss is like</b> accuracy, but instead of analyzing the labeled predictions, it analyzes the prediction probabilities. This is particularly important given that we\u2019re more interested in the probabilities than we are in the actual labels. A \u201cperfect\u201d model will have a log loss of 0. The table below shows each model\u2019s log loss. Model Log loss; SVC: 0.416: RF: 0.416: KNN: 0.403: DNN: 0.43: The SVC and RF have the same log loss, while the KNN has the lowest. Next, let\u2019s look at the ...", "dateLastCrawled": "2022-01-04T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Predicting the 2019 All-NBA teams with <b>machine</b> <b>learning</b>", "url": "https://dribbleanalytics.blogspot.com/2019/03/ml-all-nba-predict.html", "isFamilyFriendly": true, "displayUrl": "https://dribbleanalytics.blogspot.com/2019/03/ml-all-nba-predict.html", "snippet": "Predicting the 2019 All-NBA teams with <b>machine</b> <b>learning</b> Get link; Facebook; Twitter; Pinterest; Email; Other Apps; March 01, 2019 There is a summary at the bottom if you want to skip to the results. Introduction Last year, media members unanimously selected LeBron James to the All-NBA first team, giving him a record 12 All-NBA first team selections. However, given the Lakers recent struggles and LeBron&#39;s absence earlier in the season, LeBron might miss not only the first team but also the ...", "dateLastCrawled": "2021-12-11T07:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "[OC] Predicting the 2019 All-<b>NBA teams with machine learning</b> : nba", "url": "https://www.reddit.com/r/nba/comments/aw51j6/oc_predicting_the_2019_allnba_teams_with_machine/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/.../aw51j6/oc_predicting_the_2019_allnba_teams_with_<b>machine</b>", "snippet": "[OC] Predicting the 2019 All-<b>NBA teams with machine learning</b>. Original Content. This post has a lot of graphs. If you don&#39;t want to click on each one individually, they&#39;re all in an imgur album here. There is a tl;dr and summary infographic at the very end. Introduction . Last year, media members unanimously selected LeBron James to the All-NBA first team, giving him a record 12 All-NBA first team selections. However, given the Lakers recent struggles and LeBron&#39;s absence earlier in the ...", "dateLastCrawled": "2021-10-14T12:52:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "What\u2019s considered a good Log <b>Loss</b> in <b>Machine</b> <b>Learning</b> ? | by Federico ...", "url": "https://medium.com/@fzammito/whats-considered-a-good-log-loss-in-machine-learning-a529d400632d", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@fzammito/whats-considered-a-good-log-<b>loss</b>-in-<b>machine</b>-<b>learning</b>-a529...", "snippet": "<b>Log Loss is similar</b> to the Accuracy, but it will favor models that distinguish more strongly the classes. Log <b>Loss</b> it useful to compare models not only on their output but on their probabilistic ...", "dateLastCrawled": "2022-01-30T14:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>What is an intuitive explanation for the log</b> loss function? - Quora", "url": "https://www.quora.com/What-is-an-intuitive-explanation-for-the-log-loss-function", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>What-is-an-intuitive-explanation-for-the-log</b>-loss-function", "snippet": "Answer (1 of 8): To me an intuitive explanation is that minimizing the log loss equals minimizing the Kullback-Leibler divergence (Kullback\u2013Leibler divergence - Wikipedia) between the function you want to optimize (for example a neural network) and the true function that generates the data (from ...", "dateLastCrawled": "2022-01-30T02:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Key techniques for Evaluating <b>Machine</b> <b>Learning</b> models - Data Analytics", "url": "https://vitalflux.com/key-techniques-evaluating-machine-learning-models-performance/", "isFamilyFriendly": true, "displayUrl": "https://vitalflux.com/key-techniques-evaluating-<b>machine</b>-<b>learning</b>-models-performance", "snippet": "Log loss is used to evaluate the performance of classification <b>machine</b> <b>learning</b> models that are built using classification algorithms such as logistic regression, support vector <b>machine</b> (SVM), random forest, and gradient boosting. The idea behind the use of <b>Log loss is similar</b> to taking a base-e exponential or natural logarithm in order to compare model scores from high-value functions which may indicate poor <b>machine</b> <b>learning</b> model performance. The logarithmic loss value is defined as ...", "dateLastCrawled": "2022-01-31T06:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Loss In Machine Learning</b> - 02/2021 - Course f", "url": "https://www.coursef.com/loss-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.coursef.com/<b>loss-in-machine-learning</b>", "snippet": "<b>Log Loss is similar</b> to the Accuracy, but it will favor models that ... Two of the most popular loss functions in <b>machine</b> <b>learning</b> are the 0-1 loss function and the quadratic loss function. The 0-1 loss function is an indicator function that returns 1 when the target and output are not equal and zero otherwise: 0-1 Loss: The quadratic loss is a commonly used symmetric loss \u2026 161 People Used View all course \u203a\u203a Visit Site \u2039 1; 2 \u203a FAQs. Do online classes have tests? Not all online ...", "dateLastCrawled": "2021-02-08T01:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Diagnosing malaria from some symptoms: a <b>machine</b> <b>learning</b> approach and ...", "url": "https://link.springer.com/article/10.1007/s12553-020-00488-5", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s12553-020-00488-5", "snippet": "<b>Machine</b> <b>learning</b> tools have become available in the diagnosis and prediction of diseases, thereby saving costs and improving the likelihood of survivorship, especially in some terminal diseases. In the case of infectious diseases, early diagnosis is highly needed in isolating the subjects to reduce the spread of the disease. Researchers continue to propose new data mining tools that help in the early diagnosis of diseases, reducing the mortality rate, and improving the quality of life of ...", "dateLastCrawled": "2021-12-03T05:54:00.0000000Z", "language": "en", "isNavigational": false}], [], [], []], "all_bing_queries": ["+(log loss)  is like +(the negative logarithm of the probability that the model will be wrong)", "+(log loss) is similar to +(the negative logarithm of the probability that the model will be wrong)", "+(log loss) can be thought of as +(the negative logarithm of the probability that the model will be wrong)", "+(log loss) can be compared to +(the negative logarithm of the probability that the model will be wrong)", "machine learning +(log loss AND analogy)", "machine learning +(\"log loss is like\")", "machine learning +(\"log loss is similar\")", "machine learning +(\"just as log loss\")", "machine learning +(\"log loss can be thought of as\")", "machine learning +(\"log loss can be compared to\")"]}