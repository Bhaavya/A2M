{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Towards Demystifying Subliminal Persuasiveness: Using XAI-Techniques to ...", "url": "https://europepmc.org/article/PMC/PMC7338173", "isFamilyFriendly": true, "displayUrl": "https://europepmc.org/article/PMC/PMC7338173", "snippet": "<b>L2</b>-<b>regularization</b> (<b>regularization</b> factor 0.01), on the other hand, is applied to each convolutional layer in the network. The model was trained for 100 epochs using a batch size of 32 and with the dataset split into training and validation data by a ratio of 4:1.", "dateLastCrawled": "2021-07-15T15:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Towards Demystifying Subliminal Persuasiveness: Using XAI-Techniques to ...", "url": "https://link.springer.com/chapter/10.1007/978-3-030-51924-7_7", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/chapter/10.1007/978-3-030-51924-7_7", "snippet": "To tackle overfitting, we use batch normalization as well as <b>L2</b>-<b>regularization</b>. Batch normalization is applied after the second and third convolutional layer, followed by pooling layers. <b>L2</b>-<b>regularization</b> (<b>regularization</b> factor 0.01), on the other hand, is applied to each convolutional layer in the network.", "dateLastCrawled": "2022-01-15T12:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Deep Learning applications for COVID-19", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7797891/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7797891", "snippet": "Bullock et al. [] describe the aim of their survey as \u201cnot to evaluate the impact of the described techniques, nor to recommend their use, but to show the reader the extent of existing applications and to provide an initial picture and road map of how Artificial Intelligence could help the global response to the COVID-19 pandemic\u201d.We have a similar aim in our survey, focusing solely on Deep Learning. Our survey draws heavy inspiration from Raghu and Schmidt\u2019s paper, \u201cA Survey of Deep ...", "dateLastCrawled": "2022-01-29T12:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Decoding spoken English from intracortical electrode ... - IOPscience", "url": "https://iopscience.iop.org/article/10.1088/1741-2552/abbfef", "isFamilyFriendly": true, "displayUrl": "https://iopscience.iop.org/article/10.1088/1741-2552/abbfef", "snippet": "The training cost function also included a <b>L2</b> <b>regularization</b> term to penalize large network <b>weights</b> (\u03bb = 1e-5). During testing, for each phoneme utterance a new 192 \u00d7 50 neural data matrix was input to the RNN, and a 39 \u00d7 50 output matrix was read out, which consisted of the RNN&#39;s predicted likelihoods (logits) that the input data came from each of the 39 possible phonemes, for each bin.", "dateLastCrawled": "2022-01-08T01:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Talk <b>Archive</b> - Research on Algorithms and Incentives in Networks", "url": "https://rain.stanford.edu/schedule/archive.shtml", "isFamilyFriendly": true, "displayUrl": "https://rain.stanford.edu/schedule/<b>archive</b>.shtml", "snippet": "Our main result is that synthetic control <b>weights</b> are numerically equivalent to inverse propensity score <b>weights</b> (IPW) with pre-treatment outcomes as covariates and heavy <b>regularization</b> of the propensity score model. Leveraging a primal-dual connection, we map features of the propensity score model to choices about the specification of the original SC optimization problem. In particular, the original SC method, which balances the <b>L2</b> norm of pre-treatment outcomes, is identical to IPW with an ...", "dateLastCrawled": "2022-01-20T11:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Deep neural networks for human\u2019s <b>fall-risk prediction using force-plate</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0957417421006539", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0957417421006539", "snippet": "To evaluate a <b>person\u2019s</b> balance, he/she was asked to stand still on the force-plate for 60 s with <b>arms</b> at his/her sides and repeated it three times for four different test conditions. Test conditions were defined as standing eyes-open on a firm surface, eyes-closed on a firm surface, eyes-open on a foam and finally eyes-closed on a foam. Both rigid surface and foam tests were done on a force-platform and the participants\u2019 balance characteristics of Force, Moments of forces, and Centers of ...", "dateLastCrawled": "2022-01-08T09:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) <b>Proceedings of the Second Annual Data Science Sympoisum</b> | M ...", "url": "https://www.academia.edu/39820485/Proceedings_of_the_Second_Annual_Data_Science_Sympoisum", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/39820485/<b>Proceedings_of_the_Second_Annual_Data_Science_Sympoisum</b>", "snippet": "This paper creates a framework 2 f <b>Proceedings of the Second Annual Data Science</b> Symposium, Mercyhurst University, Erie, PA, May 4, 2019 3 on how to combine social media, a news source, and 1. 2.4 Long Term Short Term Memory Neural traditional economic metrics into one model. The idea is to Networks create separate models for each source.", "dateLastCrawled": "2022-01-22T00:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) <b>Adaptive Computation and Machine Learning series- Deep learning</b> ...", "url": "https://www.academia.edu/38223830/Adaptive_Computation_and_Machine_Learning_series_Deep_learning_The_MIT_Press_2016_pdf", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/38223830/<b>Adaptive_Computation_and_Machine_Learning_series</b>...", "snippet": "<b>Adaptive Computation and Machine Learning series- Deep learning</b>-The MIT Press (2016).pdf", "dateLastCrawled": "2022-02-02T06:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Data Mining - Classification: Alternative Techniques - DocShare.tips", "url": "https://docshare.tips/data-mining-classification-alternative-techniques_61cd9e361c4f85bf188b4578.html", "isFamilyFriendly": true, "displayUrl": "https://docshare.tips/data-mining-classification-alternative-techniques_61cd9e361c4f85...", "snippet": "The justi\ufb01cation for using nearest neighbors is best exempli\ufb01ed by the following saying: \u201cIf it walks <b>like</b> a duck, quacks <b>like</b> a duck, and looks <b>like</b> a duck, then it\u2019s probably a duck.\u201d A nearestneighbor classi\ufb01er represents each example as a data point in a d-dimensional d -dimensional space, where d where d is is the number number of attributes. attributes. Given Given a test example, example, we compute compute its proximity to the rest of the data points in the training set ...", "dateLastCrawled": "2021-12-30T13:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Second Language Acquisition: An Introductory Course</b> - SILO.PUB", "url": "https://silo.pub/second-language-acquisition-an-introductory-course-f-8271081.html", "isFamilyFriendly": true, "displayUrl": "https://silo.pub/<b>second-language-acquisition-an-introductory-course</b>-f-8271081.html", "snippet": "Categorize them according to English-<b>like</b> and non-English-<b>like</b> patterns of plural usage, as in Table 3.1. Decide if the choice is clear or not, remembering that data are often ambiguous. Thus, the first step is to make a list of the sentences according to the criteria of English-<b>like</b> or non-English-<b>like</b>. In sentences 3-1, 3-5, 3-6, 3-8, 3-10, 3-12, 3-15, 3-16, and 3-18 the analysis of the phrase in boldface is clear: these sentences are English-<b>like</b> because they have an s plural marker on ...", "dateLastCrawled": "2022-02-01T18:26:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Towards Demystifying Subliminal Persuasiveness: Using XAI-Techniques to ...", "url": "https://europepmc.org/article/PMC/PMC7338173", "isFamilyFriendly": true, "displayUrl": "https://europepmc.org/article/PMC/PMC7338173", "snippet": "<b>L2</b>-<b>regularization</b> (<b>regularization</b> factor 0.01), on the other hand, is applied to each convolutional layer in the network. The model was trained for 100 epochs using a batch size of 32 and with the dataset split into training and validation data by a ratio of 4:1.", "dateLastCrawled": "2021-07-15T15:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Towards Demystifying Subliminal Persuasiveness: Using XAI-Techniques to ...", "url": "https://link.springer.com/chapter/10.1007/978-3-030-51924-7_7", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/chapter/10.1007/978-3-030-51924-7_7", "snippet": "To tackle overfitting, we use batch normalization as well as <b>L2</b>-<b>regularization</b>. Batch normalization is applied after the second and third convolutional layer, followed by pooling layers. <b>L2</b>-<b>regularization</b> (<b>regularization</b> factor 0.01), on the other hand, is applied to each convolutional layer in the network.", "dateLastCrawled": "2022-01-15T12:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Decoding spoken English from intracortical electrode ... - IOPscience", "url": "https://iopscience.iop.org/article/10.1088/1741-2552/abbfef", "isFamilyFriendly": true, "displayUrl": "https://iopscience.iop.org/article/10.1088/1741-2552/abbfef", "snippet": "The training cost function also included a <b>L2</b> <b>regularization</b> term to penalize large network <b>weights</b> (\u03bb = 1e-5). During testing, for each phoneme utterance a new 192 \u00d7 50 neural data matrix was input to the RNN, and a 39 \u00d7 50 output matrix was read out, which consisted of the RNN&#39;s predicted likelihoods (logits) that the input data came from each of the 39 possible phonemes, for each bin.", "dateLastCrawled": "2022-01-08T01:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "NLP-Project/text_val at master \u00b7 jiangxinyang227/NLP-Project \u00b7 GitHub", "url": "https://github.com/jiangxinyang227/NLP-Project/blob/master/multi_label_classifier/data/AAPD/text_val", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/jiangxinyang227/NLP-Project/blob/master/multi_label_classifier/data/...", "snippet": "in this paper , we study a fast approximation method for it large scale high dimensional sparse least squares regression problem by exploiting the johnson lindenstrauss \\( jl \\) transforms , which embed a set of high dimensional vectors into a low dimensional space in particular , we propose to apply the jl transforms to the data matrix and the target vector and then to solve a sparse least squares problem on the compressed data with a it slightly larger <b>regularization</b> parameter ...", "dateLastCrawled": "2022-01-30T23:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Pedestrian identification using motion-controlled deep neural network ...", "url": "https://link.springer.com/article/10.1007/s00500-021-05701-9", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00500-021-05701-9", "snippet": "In the last two decades, gait biometric (Wang et al. 2003) and face recognition (Huang et al. 2010; Samal and Iyengar 1992; Sghaier et al. 2018) are used for person identification in computer vision.In a <b>similar</b> way of identifying individuals, human biometrics are also used in pattern recognition (Chaki et al. 2019).Gait recognition is focused on movement of <b>person\u2019s</b> legs (Foster et al. 2003).However, some researchers focused on movement of legs as well as movement of <b>arms</b> (Tafazzoli and ...", "dateLastCrawled": "2022-01-27T00:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Data Mining - Classification: Alternative Techniques - DocShare.tips", "url": "https://docshare.tips/data-mining-classification-alternative-techniques_61cd9e361c4f85bf188b4578.html", "isFamilyFriendly": true, "displayUrl": "https://docshare.tips/data-mining-classification-alternative-techniques_61cd9e361c4f85...", "snippet": "It stops <b>adding</b> <b>adding</b> conjun conjuncts cts when the rulee starts rul starts coveri covering ng negativ negativee exampl examples. es. The new rule rule is the then n pruned pruned based on its performance on the validation set. The following metric is computed to determine whether pruning is needed: ( p ( p n)/( p p+ +n), where p where p (n) is the number of positive (negative) examples in the validation set covered by the rule. This metric is monotonically related to the rule\u2019s accuracy ...", "dateLastCrawled": "2021-12-30T13:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) <b>Adaptive Computation and Machine Learning series- Deep learning</b> ...", "url": "https://www.academia.edu/38223830/Adaptive_Computation_and_Machine_Learning_series_Deep_learning_The_MIT_Press_2016_pdf", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/38223830/<b>Adaptive_Computation_and_Machine_Learning_series</b>...", "snippet": "<b>Adaptive Computation and Machine Learning series- Deep learning</b>-The MIT Press (2016).pdf", "dateLastCrawled": "2022-02-02T06:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) <b>Deep Learning with Predictive Control for Human Motion</b> Tracking", "url": "https://www.researchgate.net/publication/326892429_Deep_Learning_with_Predictive_Control_for_Human_Motion_Tracking", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/326892429_Deep_Learning_with_Predictive...", "snippet": "The physical implications of dressing are complicated by non-rigid garments, which can result in a robot indirectly applying high forces <b>to a person&#39;s</b> body. We present a deep recurrent model that ...", "dateLastCrawled": "2021-09-15T11:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Technical Reports | Department of Computer Science, Columbia University", "url": "https://www.cs.columbia.edu/technical-reports/", "isFamilyFriendly": true, "displayUrl": "https://www.cs.columbia.edu/technical-reports", "snippet": "Realizing the useful advantages of Sensor Emulation which is about <b>adding</b> virtualized sensors support to emulated environments, the current work emulates a total of ten sensors present in the real smartphone, Samsung Nexus S, an Android device. Virtual phones run Android-x86 while real phones run Android. The real reason behind choosing Android-x86 for virtual phone is that x86-based Android devices are feature-rich over ARM based ones, for example a full-fledged x86 desktop or a tablet has ...", "dateLastCrawled": "2022-01-28T22:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "CVPR2017\u8bba\u6587\u6458\u8981\u6c47\u603b_super_chicken\u7684\u535a\u5ba2-\u7a0b\u5e8f\u5458\u5b9d\u5b9d - \u7a0b\u5e8f\u5458\u5b9d\u5b9d", "url": "https://cxybb.com/article/super_chicken/79866849", "isFamilyFriendly": true, "displayUrl": "https://cxybb.com/article/super_chicken/79866849", "snippet": "Our core idea is to identify and use a subset of training images from the original source learning task whose low-level characteristics are <b>similar</b> to those from the target learning task, and jointly fine-tune shared convolutional layers for both tasks. Specifically, we compute descriptors from linear or nonlinear filter bank responses on training images from both tasks, and use such descriptors to search for a desired subset of training samples for the source learning task. Experiments ...", "dateLastCrawled": "2022-02-03T00:36:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Towards Demystifying Subliminal Persuasiveness: Using XAI-Techniques to ...", "url": "https://europepmc.org/article/PMC/PMC7338173", "isFamilyFriendly": true, "displayUrl": "https://europepmc.org/article/PMC/PMC7338173", "snippet": "<b>L2</b>-<b>regularization</b> (<b>regularization</b> factor 0.01), on the other hand, is applied to each convolutional layer in the network. The model was trained for 100 epochs using a batch size of 32 and with the dataset split into training and validation data by a ratio of 4:1.", "dateLastCrawled": "2021-07-15T15:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Towards Demystifying Subliminal Persuasiveness: Using XAI-Techniques to ...", "url": "https://link.springer.com/chapter/10.1007/978-3-030-51924-7_7", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/chapter/10.1007/978-3-030-51924-7_7", "snippet": "To tackle overfitting, we use batch normalization as well as <b>L2</b>-<b>regularization</b>. Batch normalization is applied after the second and third convolutional layer, followed by pooling layers. <b>L2</b>-<b>regularization</b> (<b>regularization</b> factor 0.01), on the other hand, is applied to each convolutional layer in the network.", "dateLastCrawled": "2022-01-15T12:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Deep Learning applications for COVID-19", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7797891/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7797891", "snippet": "The efforts of Deep Learning research <b>can</b> <b>be thought</b> of as discovering mechanisms of prior knowledge, collecting experience, and measuring generalization difficulty. The current generation of Deep Learning is defined in our survey as sequential processing networks with many layers, updating its parameters with a global loss function, and forming distributed representations of data. We have seen an evolution from Machine Learning in representation learning. We also seek to integrate Symbolic ...", "dateLastCrawled": "2022-01-29T12:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Interpreting single trial data using groupwise regularisation</b> | Request PDF", "url": "https://www.researchgate.net/publication/24199764_Interpreting_single_trial_data_using_groupwise_regularisation", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/24199764_<b>Interpreting_single_trial_data_using</b>...", "snippet": "Here, we describe a novel method based on logistic regression using a combination of L1 and <b>L2</b> norm <b>regularization</b> that more accurately estimates discriminative brain regions across multiple ...", "dateLastCrawled": "2021-12-24T03:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) <b>Adaptive Computation and Machine Learning series- Deep learning</b> ...", "url": "https://www.academia.edu/38223830/Adaptive_Computation_and_Machine_Learning_series_Deep_learning_The_MIT_Press_2016_pdf", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/38223830/<b>Adaptive_Computation_and_Machine_Learning_series</b>...", "snippet": "<b>Adaptive Computation and Machine Learning series- Deep learning</b>-The MIT Press (2016).pdf", "dateLastCrawled": "2022-02-02T06:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Talk <b>Archive</b> - Research on Algorithms and Incentives in Networks", "url": "https://rain.stanford.edu/schedule/archive.shtml", "isFamilyFriendly": true, "displayUrl": "https://rain.stanford.edu/schedule/<b>archive</b>.shtml", "snippet": "Our main result is that synthetic control <b>weights</b> are numerically equivalent to inverse propensity score <b>weights</b> (IPW) with pre-treatment outcomes as covariates and heavy <b>regularization</b> of the propensity score model. Leveraging a primal-dual connection, we map features of the propensity score model to choices about the specification of the original SC optimization problem. In particular, the original SC method, which balances the <b>L2</b> norm of pre-treatment outcomes, is identical to IPW with an ...", "dateLastCrawled": "2022-01-20T11:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Second Language Acquisition: An Introductory Course</b> - SILO.PUB", "url": "https://silo.pub/second-language-acquisition-an-introductory-course-f-8271081.html", "isFamilyFriendly": true, "displayUrl": "https://silo.pub/<b>second-language-acquisition-an-introductory-course</b>-f-8271081.html", "snippet": "The second language is commonly referred to as the <b>L2</b>. As with the phrase \u201csecond language,\u201d <b>L2</b> <b>can</b> refer to any language learned after learning the L1, regardless of whether it is the second, third, fourth, or fifth language. By this term, we mean both the acquisition of a second language in a classroom situation, as well as in more \u201cnatural\u201d exposure situations. The word acquisition in this book is used broadly in the sense that we talk about language use (sometimes independently ...", "dateLastCrawled": "2022-02-01T18:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Computer Vision</b> - Term Paper", "url": "https://www.termpaperwarehouse.com/essay-on/Computer-Vision/133533", "isFamilyFriendly": true, "displayUrl": "https://www.termpaperwarehouse.com/essay-on/<b>Computer-Vision</b>/133533", "snippet": "For all intents and purposes, an IplImage <b>can</b> <b>be thought</b> of as being derived from CvMat. Therefore, it is best to understand the (would-be) base class before attempting to understand the added complexities of the derived class. A third class, called CvArr, <b>can</b> <b>be thought</b> of as an abstract base class from which CvMat is itself derived. You will often see CvArr (or, more accurately, CvArr*) in function prototypes. When it appears, it is acceptable to pass CvMat* or IplImage* to the routine.", "dateLastCrawled": "2022-01-17T21:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Modeling Users&#39; Exposure with Social Knowledge Influence and ...", "url": "https://www.researchgate.net/publication/328433787_Modeling_Users'_Exposure_with_Social_Knowledge_Influence_and_Consumption_Influence_for_Recommendation", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/328433787_Modeling_Users", "snippet": "In the model, we use item categories as <b>arms</b>, rather than using items as <b>arms</b> in existing related models, so that the number of <b>arms</b> <b>can</b> be fixed and the scale <b>can</b> be controlled, thereby avoiding ...", "dateLastCrawled": "2021-11-02T17:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Technicla Program \u00a9APSIPA ASC 2013", "url": "http://www.apsipa.org/proceedings_2013/guide/P3.html", "isFamilyFriendly": true, "displayUrl": "www.apsipa.org/proceedings_2013/guide/P3.html", "snippet": "Second, we applied a low-complexity zero-motion-based visible artifact propagation procedure, which emphasizes the most significant visual distortion rather than equally <b>weights</b> the propagated distortion and the initial distortion. Finally, we modeled the visibility of temporal artifacts by extracting two new features from the contextual distortions. The proposed CVD scheme outperforms or emulates the fullreference metric MSE on the five training databases of P.NBAMS, with an average ...", "dateLastCrawled": "2021-11-20T08:27:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Decoding spoken English from intracortical electrode ... - IOPscience", "url": "https://iopscience.iop.org/article/10.1088/1741-2552/abbfef", "isFamilyFriendly": true, "displayUrl": "https://iopscience.iop.org/article/10.1088/1741-2552/abbfef", "snippet": "The training cost function also included a <b>L2</b> <b>regularization</b> term to penalize large network <b>weights</b> (\u03bb = 1e-5). During testing, for each phoneme utterance a new 192 \u00d7 50 neural data matrix was input to the RNN, and a 39 \u00d7 50 output matrix was read out, which consisted of the RNN&#39;s predicted likelihoods (logits) that the input data came from each of the 39 possible phonemes, for each bin.", "dateLastCrawled": "2022-01-08T01:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Deep Learning applications for COVID-19", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7797891/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7797891", "snippet": "This approach of sequence modeling <b>can</b> be improved by <b>adding</b> more structure to the problem. This structure involves training sequence models for each county, state, country, or continent-level. These models <b>can</b> then be combined at later layers of the network such that the predictions of nearby regions <b>can</b> learned from one another. An important application of this modeling level is to decide how to resume travel. The SARS-CoV-2 pandemic has incurred a huge hit to travel and resulting economic ...", "dateLastCrawled": "2022-01-29T12:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Interpreting single trial data using groupwise regularisation</b> | Request PDF", "url": "https://www.researchgate.net/publication/24199764_Interpreting_single_trial_data_using_groupwise_regularisation", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/24199764_<b>Interpreting_single_trial_data_using</b>...", "snippet": "<b>Compared</b> to strategies aiming to ... stable relationships between the <b>person&#39;s</b> intent and the signals that convey it. This study shows that humans <b>can</b> learn over a series of training sessions to ...", "dateLastCrawled": "2021-12-24T03:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) <b>Adaptive Computation and Machine Learning series- Deep learning</b> ...", "url": "https://www.academia.edu/38223830/Adaptive_Computation_and_Machine_Learning_series_Deep_learning_The_MIT_Press_2016_pdf", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/38223830/<b>Adaptive_Computation_and_Machine_Learning_series</b>...", "snippet": "<b>Adaptive Computation and Machine Learning series- Deep learning</b>-The MIT Press (2016).pdf", "dateLastCrawled": "2022-02-02T06:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Talk <b>Archive</b> - Research on Algorithms and Incentives in Networks", "url": "https://rain.stanford.edu/schedule/archive.shtml", "isFamilyFriendly": true, "displayUrl": "https://rain.stanford.edu/schedule/<b>archive</b>.shtml", "snippet": "Our main result is that synthetic control <b>weights</b> are numerically equivalent to inverse propensity score <b>weights</b> (IPW) with pre-treatment outcomes as covariates and heavy <b>regularization</b> of the propensity score model. Leveraging a primal-dual connection, we map features of the propensity score model to choices about the specification of the original SC optimization problem. In particular, the original SC method, which balances the <b>L2</b> norm of pre-treatment outcomes, is identical to IPW with an ...", "dateLastCrawled": "2022-01-20T11:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) Estimating treatment effect for individuals with progressive ...", "url": "https://www.researchgate.net/publication/355838122_Estimating_treatment_effect_for_individuals_with_progressive_multiple_sclerosis_using_deep_learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/355838122_Estimating_treatment_effect_for...", "snippet": "PDF | Modeling treatment effect could identify a subgroup of individuals who experience greater benefit from disease modifying therapy, allowing for... | Find, read and cite all the research you ...", "dateLastCrawled": "2021-12-16T03:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Deep neural networks for human\u2019s <b>fall-risk prediction using force-plate</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0957417421006539", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0957417421006539", "snippet": "To evaluate a <b>person\u2019s</b> balance, he/she was asked to stand still on the force-plate for 60 s with <b>arms</b> at his/her sides and repeated it three times for four different test conditions. Test conditions were defined as standing eyes-open on a firm surface, eyes-closed on a firm surface, eyes-open on a foam and finally eyes-closed on a foam. Both rigid surface and foam tests were done on a force-platform and the participants\u2019 balance characteristics of Force, Moments of forces, and Centers of ...", "dateLastCrawled": "2022-01-08T09:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Second Language Acquisition: An Introductory Course</b> - SILO.PUB", "url": "https://silo.pub/second-language-acquisition-an-introductory-course-f-8271081.html", "isFamilyFriendly": true, "displayUrl": "https://silo.pub/<b>second-language-acquisition-an-introductory-course</b>-f-8271081.html", "snippet": "The second language is commonly referred to as the <b>L2</b>. As with the phrase \u201csecond language,\u201d <b>L2</b> <b>can</b> refer to any language learned after learning the L1, regardless of whether it is the second, third, fourth, or fifth language. By this term, we mean both the acquisition of a second language in a classroom situation, as well as in more \u201cnatural\u201d exposure situations. The word acquisition in this book is used broadly in the sense that we talk about language use (sometimes independently ...", "dateLastCrawled": "2022-02-01T18:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Data Mining - Classification: Alternative Techniques - DocShare.tips", "url": "https://docshare.tips/data-mining-classification-alternative-techniques_61cd9e361c4f85bf188b4578.html", "isFamilyFriendly": true, "displayUrl": "https://docshare.tips/data-mining-classification-alternative-techniques_61cd9e361c4f85...", "snippet": "It stops <b>adding</b> <b>adding</b> conjun conjuncts cts when the rulee starts rul starts coveri covering ng negativ negativee exampl examples. es. The new rule rule is the then n pruned pruned based on its performance on the validation set. The following metric is computed to determine whether pruning is needed: ( p ( p n)/( p p+ +n), where p where p (n) is the number of positive (negative) examples in the validation set covered by the rule. This metric is monotonically related to the rule\u2019s accuracy ...", "dateLastCrawled": "2021-12-30T13:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "VISAPP 2017 - International Conference on Computer Vision Theory and ...", "url": "http://www.visapp.visigrapp.org/Abstracts/2017/VISAPP_2017_Abstracts.htm", "isFamilyFriendly": true, "displayUrl": "www.visapp.visigrapp.org/Abstracts/2017/<b>VISAPP_2017_Abstracts</b>.htm", "snippet": "<b>Compared</b> with a generic pedestrian detector and the state-of-the-art methods, our proposed framework presents encouraging results. Paper Nr: 49: Title: Fast Scalable Coding based on a 3D Low Bit Rate Fractal Video Encoder: Authors: Vitor de Lima, Thierry Moreira, Helio Pedrini and William Robson Schwartz: Abstract: Video transmissions usually occur at a fixed or at a small number of predefined bit rates. This <b>can</b> lead to several problems in communication channels whose bandwidth <b>can</b> vary ...", "dateLastCrawled": "2021-08-10T12:10:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Regularization</b> \u2014 Understanding L1 and <b>L2</b> <b>regularization</b> for Deep <b>Learning</b>", "url": "https://medium.com/analytics-vidhya/regularization-understanding-l1-and-l2-regularization-for-deep-learning-a7b9e4a409bf", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/<b>regularization</b>-understanding-l1-and-<b>l2</b>...", "snippet": "Understanding what <b>regularization</b> is and why it is required for <b>machine</b> <b>learning</b> and diving deep to clarify the importance of L1 and <b>L2</b> <b>regularization</b> in Deep <b>learning</b>.", "dateLastCrawled": "2022-02-01T00:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Regularization</b> : What? Why? and How? (Part -1) | by Siddhant Rai ...", "url": "https://medium.com/mlearning-ai/regularization-what-why-and-how-part-1-ef6bdb6bafea", "isFamilyFriendly": true, "displayUrl": "https://medium.com/m<b>learning</b>-ai/<b>regularization</b>-what-why-and-how-part-1-ef6bdb6bafea", "snippet": "In general <b>machine</b> <b>learning</b> sense, it is solving an objective function to perform maximum or minimum evaluation. In reality, optimization is lot more profound in usage. Then we have two terms ...", "dateLastCrawled": "2022-01-28T00:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Deep <b>Learning</b> (BEV033DLE) Lecture 7. <b>Regularization</b>", "url": "https://cw.fel.cvut.cz/b202/_media/courses/bev033dle/regularizers.pdf", "isFamilyFriendly": true, "displayUrl": "https://cw.fel.cvut.cz/b202/_media/courses/bev033dle/regularizers.pdf", "snippet": "<b>L2</b> <b>regularization</b> (Weight Decay) Dropout Implicit <b>Regularization</b> and Other Methods. Over\ufb01tting in Deep <b>Learning</b> (Recall) Underfitting and Overfitting Classical view in ML: 3 Underfitting \u2014 capacity too low Overfitting \u2014 capacity to high Just right Control model capacity (prefer simpler models, regularize) to prevent overfitting \u2022 In this example: limit the number of parameters to avoid fitting the noise. Underfitting and Overfitting 4 Underfitting \u2014 model capacity too low ...", "dateLastCrawled": "2021-11-21T21:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "4.5. <b>Weight Decay</b> \u2014 Dive into Deep <b>Learning</b> 0.17.2 documentation", "url": "http://d2l.ai/chapter_multilayer-perceptrons/weight-decay.html", "isFamilyFriendly": true, "displayUrl": "d2l.ai/chapter_multilayer-perceptrons/<b>weight-decay</b>.html", "snippet": "<b>Weight decay</b> (commonly called \\(<b>L_2</b>\\) <b>regularization</b>), might be the most widely-used technique for regularizing parametric <b>machine</b> <b>learning</b> models. The technique is motivated by the basic intuition that among all functions \\(f\\) , the function \\(f = 0\\) (assigning the value \\(0\\) to all inputs) is in some sense the simplest , and that we can measure the complexity of a function by its distance from zero.", "dateLastCrawled": "2022-02-03T05:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Why <b>Machine Learning Is A Metaphor For</b> Life \u2013 Adit Deshpande ...", "url": "https://adeshpande3.github.io/Why-Machine-Learning-is-a-Metaphor-For-Life", "isFamilyFriendly": true, "displayUrl": "https://adeshpande3.github.io/Why-<b>Machine-Learning-is-a-Metaphor-For</b>-Life", "snippet": "Well, in <b>machine</b> <b>learning</b>, we use regularizers. The first form (and the most common) of <b>regularization</b> that I first learned about was <b>L2</b> <b>regularization</b> or weight decay. This type of <b>regularization</b> is basically imposing a soft constraint on the cost function. We\u2019re telling the network \u201cHey, we want you to minimize the loss from the training examples, but it would also be cool if you keep the weights of your network at a low value because your cost is gonna increase a lot if those values ...", "dateLastCrawled": "2022-01-31T13:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Experiments on Hyperparameter tuning in</b> deep <b>learning</b> \u2014 Rules to follow ...", "url": "https://towardsdatascience.com/experiments-on-hyperparameter-tuning-in-deep-learning-rules-to-follow-efe6a5bb60af", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>experiments-on-hyperparameter-tuning-in</b>-deep-<b>learning</b>...", "snippet": "The book Deep <b>Learning</b> provides a nice <b>analogy</b> to understand why too-large batches aren\u2019t efficient. ... Weight decay is the strength of <b>L2</b> <b>regularization</b>. It essentially penalizes large values of weights in the model. Setting the right strength can improve the model\u2019s ability to generalize and reduce overfitting. But a value too high will lead to severe underfitting. For example, I tried a normal and extremely high value of weight decay. As you can see, the <b>learning</b> capacity is almost ...", "dateLastCrawled": "2022-02-02T18:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and-sample-codes ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/README.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "Model 1: accuracy = 92%. Model 2: accuracy = 95%. Model 2 has higher accuracy than model 1, but model 2 is useless. This is called accuracy paradox, which means the model with higher accuracy may not have better generalization power. In general, when TP &lt; FP, the accuracy will always increase when we change the classifier to always output &#39;negative&#39;.Conversely, when TN &lt; FN, the same will happen when we change the classifier to always output &#39;positive&#39; [1].. Recall@k", "dateLastCrawled": "2021-09-22T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "lasso - Why do we only see $L_1$ and $<b>L_2</b>$ <b>regularization</b> but not other ...", "url": "https://stats.stackexchange.com/questions/269298/why-do-we-only-see-l-1-and-l-2-regularization-but-not-other-norms", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/269298", "snippet": "That covers the gamut. In effect, a linear combination of an L 1 and <b>L 2</b> norm approximates any norm to second order at the origin--and this is what matters most in regression without outlying residuals. (**) The l 0 -&quot;norm&quot; lacks homogeneity, which is one of the axioms for norms. Homogeneity means for \u03b1 \u2265 0 that \u2016 \u03b1 x \u2016 = \u03b1 \u2016 x \u2016.", "dateLastCrawled": "2022-02-01T12:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Why does adding a <b>dropout</b> layer improve deep/<b>machine</b> <b>learning</b> ...", "url": "https://datascience.stackexchange.com/questions/37021/why-does-adding-a-dropout-layer-improve-deep-machine-learning-performance-given", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/37021", "snippet": "Unlike L1 and <b>L2</b> <b>regularization</b>, <b>dropout</b> doesn&#39;t rely on modifying the cost function. Instead, in <b>dropout</b> we modify the network itself. ... To be more concrete with regards to your kitchen <b>analogy</b>, <b>Dropout</b> is used during training only, not during inference. Hence, the complex model is not partially utilized. $\\endgroup$ \u2013 Vaibhav Garg. Aug 25 &#39;18 at 10:53 $\\begingroup$ i wsa typing this reply wiwth my eyes close. gyes more training neede. nwws moew seopour. $\\endgroup$ \u2013 VHanded. Nov 30 ...", "dateLastCrawled": "2022-01-21T21:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What is <b>elastic net regularization in machine learning? - Quora</b>", "url": "https://www.quora.com/What-is-elastic-net-regularization-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>elastic-net-regularization-in-machine-learning</b>", "snippet": "Answer (1 of 3): Elastic net <b>regularization</b> method includes both LASSO (L1) and Ridge (<b>L2</b>) <b>regularization</b> methods. Overfitting : The core idea behind <b>machine</b> <b>learning</b> algorithms is to build models that can find the generalised trends within the data. However, if no measures are taken, sometimes ...", "dateLastCrawled": "2022-01-18T14:32:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Intro to <b>Machine</b> <b>Learning</b> with TensorFlow Nanodegree Program - <b>GitHub</b>", "url": "https://github.com/danielmapar/IntroductionToMachineLearningWithTensorFlow", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/danielmapar/IntroductionTo<b>MachineLearning</b>WithTensorFlow", "snippet": "<b>L2 regularization is similar</b>, but here we add the squares of the coefficients. In order to determine how impactful model complexity is over the error, we introduce a new parameter lambda. Small lambda = ok with more complex models; Big lambda = sensitive to complex models", "dateLastCrawled": "2022-01-16T23:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Regularization techniques in <b>machine</b> <b>learning</b>", "url": "https://www.linkedin.com/pulse/regularization-techniques-machine-learning-nedhir-ben-hammouda", "isFamilyFriendly": true, "displayUrl": "https://www.linkedin.com/pulse/regularization-techniques-<b>machine</b>-<b>learning</b>-nedhir-ben...", "snippet": "in <b>machine</b> <b>learning</b> regularization is the process of adding information in order to prevent overfitting and in general ... The <b>L2 Regularization is similar</b> to the L1 but we make a change to the ...", "dateLastCrawled": "2021-10-08T20:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "An Overview On Regularization. In this article we will discuss about ...", "url": "https://arunm8489.medium.com/an-overview-on-regularization-f2a878507eae", "isFamilyFriendly": true, "displayUrl": "https://arunm8489.medium.com/an-overview-on-regularization-f2a878507eae", "snippet": "Goal of our <b>machine</b> <b>learning</b> algorithm is to learn the data patterns and ignore the noise in the data set. Now, there are few ways we can avoid overfitting our model on training data like cross-validation , feature reduction, regularization etc.In this article we will discuss about regularization. Regularization basically adds the penalty as model complexity increases. Regularization parameter (lambda) penalizes all the parameters except intercept so that model generalizes the data and won ...", "dateLastCrawled": "2022-01-29T08:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A comparative study of <b>machine</b> <b>learning</b> methods for predicting the ...", "url": "https://www.sciencedirect.com/science/article/pii/S0165027022000024", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0165027022000024", "snippet": "The tailored <b>machine</b> <b>learning</b> pipelines are composed of different blocks such as data processing and dimensionality reduction. Here we grouped the blocks into three categories: (1) pre-processing methods, (2) dimensionality reduction methods and (3) <b>learning</b> models. In this section, we present and analyze the pipelines of the top 20 teams in each of the 3 categories. Table 1 provides an overview on the used components by each team where the teams are sorted based on their final ranks. Table ...", "dateLastCrawled": "2022-01-08T15:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A Comparative Study of <b>Machine</b> <b>Learning</b> Methods for Predicting the ...", "url": "https://deepai.org/publication/a-comparative-study-of-machine-learning-methods-for-predicting-the-evolution-of-brain-connectivity-from-a-baseline-timepoint", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/a-comparative-study-of-<b>machine</b>-<b>learning</b>-methods-for...", "snippet": "A Comparative Study of <b>Machine</b> <b>Learning</b> Methods for Predicting the Evolution of Brain Connectivity from a Baseline Timepoint. 09/16/2021 \u2219 by \u015eeymanur Akt\u0131, et al. \u2219 8 \u2219 share . Predicting the evolution of the brain network, also called connectome, by foreseeing changes in the connectivity weights linking pairs of anatomical regions makes it possible to spot connectivity-related neurological disorders in earlier stages and detect the development of potential connectomic anomalies.", "dateLastCrawled": "2021-11-30T13:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Linear Regression</b>", "url": "https://ryanwingate.com/intro-to-machine-learning/supervised/linear-regression/", "isFamilyFriendly": true, "displayUrl": "https://ryanwingate.com/intro-to-<b>machine</b>-<b>learning</b>/supervised/<b>linear-regression</b>", "snippet": "In <b>machine</b> <b>learning</b>, there is often a tradeoff between accuracy and generalizability. In the image below, the linear model is not as accurate as the polynomial. Specifically, the linear model makes two misclassifications. But, the linear model is simpler and may generalize better to other datasets better than the polynomial model, which is more complex and accurate but may be overfit to this particular dataset. Regularization is a way to take the complexity of the model into account when ...", "dateLastCrawled": "2022-02-01T08:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Evaluation of <b>machine</b> <b>learning</b> algorithms to predict the hydrodynamic ...", "url": "https://europepmc.org/article/PMC/PMC7775344", "isFamilyFriendly": true, "displayUrl": "https://europepmc.org/article/PMC/PMC7775344", "snippet": "<b>Machine</b> <b>learning</b> algorithm implementation was performed using Scikit-Learn (v.0.21.3) in a Jupyter Notebook (v.6.0.1) running Python (v.3.7.4). The data was randomly split into two groups using the Numpy (v.1.16.5) train_test_split function. The function allocated 80% of the data for model development, and 20% of the data for testing the final model. Data importation and manipulation were handled using Pandas (v.0.25.1). The algorithms tested in this study include linear regression, elastic ...", "dateLastCrawled": "2022-01-06T07:09:00.0000000Z", "language": "en", "isNavigational": false}], [], [], []], "all_bing_queries": ["+(l2 regularization)  is like +(adding weights to a person's arms)", "+(l2 regularization) is similar to +(adding weights to a person's arms)", "+(l2 regularization) can be thought of as +(adding weights to a person's arms)", "+(l2 regularization) can be compared to +(adding weights to a person's arms)", "machine learning +(l2 regularization AND analogy)", "machine learning +(\"l2 regularization is like\")", "machine learning +(\"l2 regularization is similar\")", "machine learning +(\"just as l2 regularization\")", "machine learning +(\"l2 regularization can be thought of as\")", "machine learning +(\"l2 regularization can be compared to\")"]}