{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A Brief Overview of <b>Loss</b> Functions in <b>Pytorch</b> | by Pratyaksha Jha ...", "url": "https://medium.com/udacity-pytorch-challengers/a-brief-overview-of-loss-functions-in-pytorch-c0ddb78068f7", "isFamilyFriendly": true, "displayUrl": "https://medium.com/udacity-<b>pytorch</b>-challengers/a-brief-overview-of-<b>loss</b>-functions-in-p...", "snippet": "While other <b>loss</b> functions <b>like</b> <b>squared</b> <b>loss</b> penalize wrong predictions, cross entropy gives a greater <b>penalty</b> when incorrect predictions are predicted with high confidence. What differentiates it ...", "dateLastCrawled": "2022-02-02T14:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "python - Meaning of `<b>penalty</b>` and `<b>loss</b>` in LinearSVC - Stack Overflow", "url": "https://stackoverflow.com/questions/68819288/meaning-of-penalty-and-loss-in-linearsvc", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/68819288/meaning-of-<b>penalty</b>-and-<b>loss</b>-in-linearsvc", "snippet": "L2-regularized, L2-<b>loss</b> (<b>penalty</b>=&#39;l2&#39;, <b>loss</b>=&#39;<b>squared</b>_hinge&#39;), default in LinearSVC: L1-regularized, L2-<b>loss</b> (<b>penalty</b>=&#39;l1&#39;, <b>loss</b>=&#39;<b>squared</b>_hinge&#39;): Instead, as stated within the documentation, LinearSVC does not support the combination of <b>penalty</b>=&#39;l1&#39; and <b>loss</b>=&#39;hinge&#39;. As far as I see the paper does not specify why, but I found a possible answer here (within the answer by Arun Iyer). Eventually, effectively the combination of <b>penalty</b>=&#39;l2&#39;, <b>loss</b>=&#39;hinge&#39;, dual=False is not supported as specified ...", "dateLastCrawled": "2022-01-26T17:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Most Used <b>Loss</b> <b>Functions To Optimize Machine Learning Algorithms</b>", "url": "https://analyticsindiamag.com/most-used-loss-functions-to-optimize-machine-learning-algorithms/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/most-used-<b>loss</b>-<b>functions-to-optimize-machine-learning</b>...", "snippet": "There are plenty of regression algorithms <b>like</b> linear regression, ... the function will impose a higher <b>penalty</b> at this point because it can sense the difference in the sign. The below given formula would work if the class labels are -1 and +1. <b>Squared</b> Hinge <b>Loss</b>: <b>Squared</b> hinge <b>loss</b> is nothing but an extension to hinge <b>loss</b>. At times we don\u2019t need to know the probabilities of how certain the classifier is about the output. Suppose the problem requires an output which will either be yes or ...", "dateLastCrawled": "2022-02-01T05:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Scikit Learn - Stochastic Gradient Descent</b>", "url": "https://www.tutorialspoint.com/scikit_learn/scikit_learn_stochastic_gradient_descent.htm", "isFamilyFriendly": true, "displayUrl": "https://<b>www.tutorialspoint.com</b>/scikit_learn/<b>scikit_learn_stochastic_gradient_descent</b>.htm", "snippet": "modified_huber \u2212 a smooth <b>loss</b> that brings tolerance to outliers along with probability estimates. <b>squared</b>_hinge \u2212 similar to \u2018hinge\u2019 <b>loss</b> but it is quadratically penalized. perceptron \u2212 as the name suggests, it is a linear <b>loss</b> which is used by the perceptron algorithm. 2: <b>penalty</b> \u2212 str, \u2018none\u2019, \u2018l2\u2019, \u2018l1\u2019, \u2018elasticnet\u2019", "dateLastCrawled": "2022-02-03T02:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A Comprehensive Guide To <b>Loss</b> Functions \u2014 Part 1 : Regression | by ...", "url": "https://medium.com/analytics-vidhya/a-comprehensive-guide-to-loss-functions-part-1-regression-ff8b847675d6", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/a-comprehensive-guide-to-<b>loss</b>-functions-part-1...", "snippet": "Image by author. These are the most common <b>loss</b> functions used for regression. There are other <b>loss</b> functions <b>like</b> quantile <b>loss</b> and Poisson <b>loss</b>, but in my opinion, these should be enough to get ...", "dateLastCrawled": "2022-01-30T11:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>sklearn.linear_model.SGDRegressor</b> \u2014 scikit-learn 1.0.2 documentation", "url": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html", "isFamilyFriendly": true, "displayUrl": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html", "snippet": "The regularizer is a <b>penalty</b> added to the <b>loss</b> function that shrinks model parameters towards the zero vector using either the <b>squared</b> euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.", "dateLastCrawled": "2022-02-02T11:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "5 Regression <b>Loss</b> Functions All Machine Learners Should Know | by ...", "url": "https://heartbeat.comet.ml/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0", "isFamilyFriendly": true, "displayUrl": "https://heartbeat.comet.ml/5-regression-<b>loss</b>-functions-all-machine-learners-should...", "snippet": "A most commonly used method of finding the minimum point of function is \u201cgradient descent\u201d. Think of <b>loss</b> function <b>like</b> undulating mountain and gradient descent <b>is like</b> sliding down the mountain to reach the bottommost point. There is not a single <b>loss</b> function that works for all kind of data. It depends on a number of factors including the presence of outliers, choice of machine learning algorithm, time efficiency of gradient descent, ease of finding the derivatives and confidence of ...", "dateLastCrawled": "2022-02-02T07:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>sklearn.linear_model.SGDClassifier</b> \u2014 scikit-learn 1.0.2 documentation", "url": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html", "isFamilyFriendly": true, "displayUrl": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html", "snippet": "The regularizer is a <b>penalty</b> added to the <b>loss</b> function that shrinks model parameters towards the zero vector using either the <b>squared</b> euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection. Read more in the User Guide. Parameters <b>loss</b> str, default=\u2019hinge\u2019 The <b>loss</b> function to be used ...", "dateLastCrawled": "2022-02-03T06:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "probability - <b>Standard Error Loss vs. Absolute</b> <b>Loss</b> - Mathematics Stack ...", "url": "https://math.stackexchange.com/questions/1592151/standard-error-loss-vs-absolute-loss", "isFamilyFriendly": true, "displayUrl": "https://math.stackexchange.com/questions/1592151/<b>standard-error-loss-vs-absolute</b>-<b>loss</b>", "snippet": "Thanks for contributing an answer to <b>Mathematics Stack Exchange</b>! Please be sure to answer the question.Provide details and share your research! But avoid \u2026. Asking for help, clarification, or responding to other answers.", "dateLastCrawled": "2022-01-19T07:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "python - value <b>error happens when using GridSearchCV</b> - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/29902190/value-error-happens-when-using-gridsearchcv", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/29902190", "snippet": "ValueError: Unsupported set of arguments: <b>penalty</b>=&#39;l1&#39; is only supported when dual=&#39;false&#39;., Parameters: <b>penalty</b>=&#39;l1&#39;, <b>loss</b>=&#39;<b>squared</b>_hinge&#39;, dual=False. Anyone has idea what I should do to deal with that?", "dateLastCrawled": "2022-01-24T02:20:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Mean <b>Squared</b> Error vs Cross entropy <b>loss</b> function - Data Analytics", "url": "https://vitalflux.com/mean-squared-error-vs-cross-entropy-loss-function/", "isFamilyFriendly": true, "displayUrl": "https://vitalflux.com/mean-<b>squared</b>-error-vs-cross-entropy-<b>loss</b>-function", "snippet": "The idea of this <b>loss</b> function is to give a high <b>penalty</b> for wrong predictions and a low <b>penalty</b> for correct classifications. It calculates a probability that each sample belongs to one of the classes, then it uses cross-entropy between these probabilities as its cost function. The more confident model is about prediction, the less <b>penalty</b> it incurs. Cross-entropy <b>loss</b> is very <b>similar</b> to cross entropy. They both measure the difference between an actual probability and predicted probability ...", "dateLastCrawled": "2022-02-03T10:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A Brief Overview of <b>Loss</b> Functions in <b>Pytorch</b> | by Pratyaksha Jha ...", "url": "https://medium.com/udacity-pytorch-challengers/a-brief-overview-of-loss-functions-in-pytorch-c0ddb78068f7", "isFamilyFriendly": true, "displayUrl": "https://medium.com/udacity-<b>pytorch</b>-challengers/a-brief-overview-of-<b>loss</b>-functions-in-p...", "snippet": "While other <b>loss</b> functions like <b>squared</b> <b>loss</b> penalize wrong predictions, cross entropy gives a greater <b>penalty</b> when incorrect predictions are predicted with high confidence. What differentiates it ...", "dateLastCrawled": "2022-02-02T14:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "optimization - square <b>loss</b> function in classification - Mathematics ...", "url": "https://math.stackexchange.com/questions/2370977/square-loss-function-in-classification", "isFamilyFriendly": true, "displayUrl": "https://math.stackexchange.com/questions/2370977/square-<b>loss</b>-function-in-classification", "snippet": "I know the the square <b>loss</b> function in the regression context as follows: $(y-f(x))^2$ for y the real, and f(x) the predicted value. This formulation is quite easy to understand: We have a convex <b>loss</b> function where the <b>loss</b> is based on the difference between real and predicted values, and outliers are penalized heavier by squaring this difference.", "dateLastCrawled": "2022-01-16T22:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Robust Variable Selection With Exponential <b>Squared</b> <b>Loss</b>", "url": "https://www.jstor.org/stable/24246469", "isFamilyFriendly": true, "displayUrl": "https://www.jstor.org/stable/24246469", "snippet": "classification problem with <b>similar</b> success (Friedman, Hastie, . . , , and Tibshirani 2000) objective function of penalized robust regression consists... , . , , &#39; .. . . .. of a <b>loss</b> function and a <b>penalty</b> term. In this article, we propose We begin with the following <b>loss</b> function ... maximizing 4&gt;y(t)= 1 - exp(-i2/}/), \u00ab 2 , A in(fi) = \u03a3 exp {- (Yi - xj \u00dfj/\u03b3\u03b7 }-nJ2 pKj(\\\u00dfj\\) (2.2) which is an exponential <b>squared</b> <b>loss</b> with a tuning parameter 1=1 i=l \u03b3. The tuning parameter \u03b3 ...", "dateLastCrawled": "2022-01-24T10:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Picking Loss Functions - A comparison between MSE, Cross Entropy</b>, and ...", "url": "https://rohanvarma.me/Loss-Functions/", "isFamilyFriendly": true, "displayUrl": "https://rohanvarma.me/<b>Loss</b>-Functions", "snippet": "For example, the cross-entropy <b>loss</b> would invoke a much higher <b>loss</b> than the hinge <b>loss</b> if our (un-normalized) scores were \\([10, 8, 8]\\) versus \\([10, -10, -10]\\), where the first class is correct. In fact, the (multi-class) hinge <b>loss</b> would recognize that the correct class score already exceeds the other scores by more than the margin, so it will invoke zero <b>loss</b> on both scores. Once the margins are satisfied, the SVM will no longer optimize the weights in an attempt to \u201cdo better ...", "dateLastCrawled": "2021-12-04T02:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Squared Error Loss</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/computer-science/squared-error-loss", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/computer-science/<b>squared-error-loss</b>", "snippet": "<b>Similar</b> to the MLE, a point estimator can also be found by directly maximizing the posterior distribution. Definition 18 (Maximum a posteriori estimator) The estimator found by . \u03b8 \u02c6 M A P (x) = arg max \u03b8 \u2208 \u0398 \u03c0 (\u03b8 | x) = arg max \u03b8 \u2208 \u0398 \u03c0 (x | \u03b8) \u03c0 (\u03b8) is called a maximum a posteriori (MAP) estimator. The MAP estimator coincides with the MLE if the prior \u03c0(\u03b8) is a uniform distribution. Example 50. For the binomial example (Example 43), the posterior is a beta(y + \u03b1, n \u2212 y ...", "dateLastCrawled": "2022-01-27T13:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "python - difference between <b>penalty</b> and <b>loss</b> parameters in Sklearn ...", "url": "https://stackoverflow.com/questions/25042909/difference-between-penalty-and-loss-parameters-in-sklearn-linearsvc-library", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/25042909", "snippet": "In machine learning, <b>loss</b> function measures the quality of your solution, while <b>penalty</b> function imposes some constraints on your solution. Specifically, Let X be your data, and y be labels of your data. Then <b>loss</b> function V(f(X),y) measures how well your model f maps your data to the labels. Here, f(X) is a vector of predicted labels. L1 and L2 norms are commonly used and intuitively understood <b>loss</b> functions (see *", "dateLastCrawled": "2022-01-15T22:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "5 Regression <b>Loss</b> Functions All Machine Learners Should Know | by ...", "url": "https://heartbeat.comet.ml/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0", "isFamilyFriendly": true, "displayUrl": "https://heartbeat.comet.ml/5-regression-<b>loss</b>-functions-all-machine-learners-should...", "snippet": "<b>Loss</b> function tries to give different penalties to overestimation and underestimation based on the value of the chosen quantile (\u03b3). For example, a quantile <b>loss</b> function of \u03b3 = 0.25 gives more <b>penalty</b> to overestimation and tries to keep prediction values a little below median", "dateLastCrawled": "2022-02-02T07:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Why is it that weight decay involves adding the same <b>penalty</b> term to ...", "url": "https://www.quora.com/Why-is-it-that-weight-decay-involves-adding-the-same-penalty-term-to-the-sum-squared-error-as-in-Ridge-Regression", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Why-is-it-that-weight-decay-involves-adding-the-same-<b>penalty</b>...", "snippet": "Answer: Suppose your network has weights \\theta and you add an L_2 regularization term to your <b>loss</b> function, i.e. L(\\theta) becomes L(\\theta)+\\lambda \\|\\theta\\|_2^2, where \\lambda &gt; 0. This is the idea of ridge regression. Then your gradient changes as well, from \\nabla L(\\theta) to \\nabla L(\\t...", "dateLastCrawled": "2021-12-20T14:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "A Comprehensive Guide To <b>Loss</b> Functions \u2014 Part 1 : Regression | by ...", "url": "https://medium.com/analytics-vidhya/a-comprehensive-guide-to-loss-functions-part-1-regression-ff8b847675d6", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/a-comprehensive-guide-to-<b>loss</b>-functions-part-1...", "snippet": "Graphically, Log-cosh is quite <b>similar</b> to Huber <b>loss</b> as it is also a combination of linear and quadratic scorings. One difference that sets this apart is that it is double differentiable .", "dateLastCrawled": "2022-01-30T11:01:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Picking Loss Functions - A comparison between MSE, Cross Entropy</b>, and ...", "url": "https://rohanvarma.me/Loss-Functions/", "isFamilyFriendly": true, "displayUrl": "https://rohanvarma.me/<b>Loss</b>-Functions", "snippet": "In this post, I\u2019ll discuss three common <b>loss</b> functions: the mean-<b>squared</b> (MSE) <b>loss</b>, cross-entropy <b>loss</b>, and the hinge <b>loss</b>. These are the most commonly used functions I\u2019ve seen used in traditional machine learning and deep learning models, so I <b>thought</b> it would be a good idea to figure out the underlying theory behind each one, and when to prefer one over the others.", "dateLastCrawled": "2021-12-04T02:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "How <b>can</b> I <b>penalize a regression loss function to account</b> for ... - Quora", "url": "https://www.quora.com/How-can-I-penalize-a-regression-loss-function-to-account-for-correctness-on-the-sign-of-the-prediction", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/How-<b>can</b>-I-<b>penalize-a-regression-loss-function-to-account</b>-for...", "snippet": "Answer (1 of 4): There\u2019s been good suggestions made already, but unfortunately not all of them are scale invariant. For example, imagine if your <b>loss</b> function is: L = \\displaystyle \\sum_{i = 1}^N \\displaystyle (\\hat{Y}_i - Y_i)^2 + k*I(\\text{sign}(\\hat{Y}_i) \\neq \\text{sign}(Y_i) for some const...", "dateLastCrawled": "2022-01-19T22:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Cost Function and <b>Loss</b> Function in Machine Learning - Shishir Kant Singh", "url": "http://shishirkant.com/cost-function-and-loss-function-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "shishirkant.com/cost-function-and-<b>loss</b>-function-in-machine-learning", "snippet": "A mail <b>can</b> be classified as a spam or not a spam and a person\u2019s dietary preferences <b>can</b> be put in one of three categories \u2013 vegetarian, non-vegetarian and vegan. Let\u2019s take a look at <b>loss</b> functions that <b>can</b> be used for classification problems. Binary Cross Entropy <b>Loss</b>", "dateLastCrawled": "2022-01-29T23:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Regularization: Ridge, Lasso &amp; Elastic Net Regression - DataCamp", "url": "https://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net", "isFamilyFriendly": true, "displayUrl": "https://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net", "snippet": "Ridge Regression, which penalizes sum of <b>squared</b> coefficients (L2 <b>penalty</b>). Lasso Regression, which penalizes the sum of absolute values of the coefficients (L1 <b>penalty</b>). Elastic Net, a convex combination of Ridge and Lasso. The size of the respective <b>penalty</b> terms <b>can</b> be tuned via cross-validation to find the model&#39;s best fit.", "dateLastCrawled": "2022-02-02T03:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Concepts</b> | Machine Learning - Michael Clark", "url": "https://m-clark.github.io/introduction-to-machine-learning/concepts.html", "isFamilyFriendly": true, "displayUrl": "https://m-clark.github.io/introduction-to-machine-learning/<b>concepts</b>.html", "snippet": "The above is in deviance form. If you\u2019re not familiar, deviance <b>can</b> conceptually <b>be thought</b> of as the GLM version of residual variance. This <b>loss</b> is equivalent to binomial log likelihood when \\(y\\) is on the 0-1 scale. Exponential. Exponential <b>loss</b> is yet another <b>loss</b> function at our disposal. \\[L(Y, f(X)) = \\sum e^{-yf}\\] Hinge <b>Loss</b>. A final <b>loss</b> function to consider, typically used with support vector machines, is the hinge <b>loss</b> function. \\[L(Y, f(X)) = \\max(1-yf, 0)\\] Here negative ...", "dateLastCrawled": "2022-02-03T07:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "regression - Why is using <b>squared error</b> the standard when absolute ...", "url": "https://stats.stackexchange.com/questions/470626/why-is-using-squared-error-the-standard-when-absolute-error-is-more-relevant-to", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/470626/why-is-using-<b>squared-error</b>-the...", "snippet": "Also, although symmetric, the <b>squared</b> <b>loss</b> is at least non linear. Yet the differences between absolute and <b>squared</b> <b>loss</b> functions don&#39;t end here. For instance, it <b>can</b> be shown that the optimal point forecast in absolute <b>loss</b> is the median while for the <b>squared</b> <b>loss</b> it is mean.", "dateLastCrawled": "2022-02-02T17:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "machine learning - Linear regression with non-symmetric cost function ...", "url": "https://datascience.stackexchange.com/questions/10471/linear-regression-with-non-symmetric-cost-function", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/10471", "snippet": "One simple candidate is to tweak the <b>squared</b> <b>loss</b>: L: ( x, \u03b1) \u2192 x 2 ( s g n x + \u03b1) 2. where \u2212 1 &lt; \u03b1 &lt; 1 is a parameter you <b>can</b> use to trade off the <b>penalty</b> of underestimation against overestimation. Positive values of \u03b1 penalize overestimation, so you will want to set \u03b1 negative. In python this looks like def <b>loss</b> (x, a): return x**2 ...", "dateLastCrawled": "2022-01-27T08:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "regression - Why do the terms in L2 Regularized likelihood represent ...", "url": "https://stats.stackexchange.com/questions/355386/why-do-the-terms-in-l2-regularized-likelihood-represent-gaussian-distributions", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/355386/why-do-the-terms-in-l2-regularized...", "snippet": "We began with a simple ordinary least squares <b>loss</b> function, and added a <b>penalty</b> term proportional to the <b>squared</b> weights of the coefficients, as seen below: Because minimizing the least <b>squared</b> <b>loss</b> function is equal minimizing the negative log-likelihood, we flipped the signs so that maximizing the negative <b>loss</b> function is equal to maximizing the log likelihood.", "dateLastCrawled": "2022-02-03T02:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Introduction \u2014 py-earth 0.1.0 documentation", "url": "https://contrib.scikit-learn.org/py-earth/content.html", "isFamilyFriendly": true, "displayUrl": "https://contrib.scikit-learn.org/py-earth/content.html", "snippet": "Earth models <b>can</b> <b>be thought</b> of as linear models in a higher dimensional basis space. Each term in an Earth model is a product of so called \u201chinge functions\u201d. A hinge function is a function that\u2019s equal to its argument where that argument is greater than zero and is zero everywhere else. \\[\\begin{split}\\text{h}\\left(x-t\\right)=\\left[x-t\\right]_{+}=\\begin{cases} x-t, &amp; x&gt;t\\\\ 0, &amp; x\\leq t \\end{cases}\\end{split}\\] An Earth model is a linear combination of basis functions, each of which is ...", "dateLastCrawled": "2022-01-28T17:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What will happen to 18 shares sold at 400 of BPCL but was not able to ...", "url": "https://www.quora.com/What-will-happen-to-18-shares-sold-at-400-of-BPCL-but-was-not-able-to-squared-off-How-much-loss-will-occur", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-will-happen-to-18-shares-sold-at-400-of-BPCL-but-was-not...", "snippet": "Answer (1 of 2): Hi, If you are short and unable to settle your position then your shares will be auctioned on t+2 days after the completion of payout. In case of short delivery, an additional 0.5% <b>penalty</b> shall be levied towards the auction <b>penalty</b>. The upper limit of the range: 20% higher tha...", "dateLastCrawled": "2022-01-08T23:28:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Cost Function &amp; <b>Loss</b> Function. In this article, I wanted to put\u2026 | by ...", "url": "https://nadeemm.medium.com/cost-function-loss-function-c3cab1ddffa4", "isFamilyFriendly": true, "displayUrl": "https://nadeemm.medium.com/cost-function-<b>loss</b>-function-c3cab1ddffa4", "snippet": "<b>Loss</b> function is usually a function defined on a data point, prediction, and label, and measures the <b>penalty</b>. Cost function is usually more general. It might be a sum of <b>loss</b> functions over your training set plus some model complexity <b>penalty</b> (regularization).", "dateLastCrawled": "2022-02-02T15:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Loss</b> functions (<b>Incorrect predictions</b> <b>penalty</b>) | Data Mining ...", "url": "https://datacadamia.com/data_mining/loss_function", "isFamilyFriendly": true, "displayUrl": "https://datacadamia.com/data_mining/<b>loss</b>_function", "snippet": "<b>Squared</b> <b>loss</b> = &lt;math&gt;(y-\\hat{y})^2&lt;/math&gt; Classification. 0-1. 0-1 <b>loss</b>: <b>Penalty</b> is 0 for correct prediction, and 1 otherwise As 0-1 <b>loss</b> is not convex, the standard approach is to transform the categorical features into numerical features: (See Statistics - Dummy (Coding|Variable) - One-hot-encoding (OHE)) and to use a regression <b>loss</b>. Log. Log <b>loss</b> is defined as: &lt;MATH&gt; \\begin{align} \\ell_{log}(p, y) = \\begin{cases} -\\log (p) &amp; \\text{if } y = 1\\\\\\ -\\log(1-p) &amp; \\text{if } y = 0 \\end{cases ...", "dateLastCrawled": "2022-01-21T17:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Loss</b> Functions in Neural Networks - theaidream.com", "url": "https://www.theaidream.com/post/loss-functions-in-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://www.theaidream.com/post/<b>loss</b>-functions-in-neural-networks", "snippet": "Cross-Entropy <b>loss</b> is also called logarithmic <b>loss</b>, log <b>loss</b>, or logistic <b>loss</b>. Each predicted class probability is <b>compared</b> to the actual class desired output 0 or 1 and a score/<b>loss</b> is calculated that penalizes the probability based on how far it is from the actual expected value. The <b>penalty</b> is logarithmic in nature yielding a large score for large differences close to 1 and small score for small differences tending to 0.", "dateLastCrawled": "2022-02-02T10:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Picking Loss Functions - A comparison between MSE, Cross Entropy</b>, and ...", "url": "https://rohanvarma.me/Loss-Functions/", "isFamilyFriendly": true, "displayUrl": "https://rohanvarma.me/<b>Loss</b>-Functions", "snippet": "It turns out we <b>can</b> derive the mean-<b>squared</b> <b>loss</b> by considering a typical linear regression problem. With linear regression, we seek to model our real-valued labels \\(Y\\) as being a linear function of our inputs \\(X\\), corrupted by some noise. Let\u2019s write out this assumption: \\[Y = \\theta_0 + \\theta_1x + \\eta\\] And to solidify our assumption, we\u2019ll say that \\(\\eta\\) is Gaussian noise with 0 mean and unit variance, that is \\(\\eta \\sim N(0, 1)\\). This means that \\(E[Y] = E[\\theta_0 ...", "dateLastCrawled": "2021-12-04T02:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "5 Regression <b>Loss</b> Functions All Machine Learners Should Know | by ...", "url": "https://heartbeat.comet.ml/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0", "isFamilyFriendly": true, "displayUrl": "https://heartbeat.comet.ml/5-regression-<b>loss</b>-functions-all-machine-learners-should...", "snippet": "<b>Loss</b> functions <b>can</b> be broadly categorized into 2 types: Classification and Regression <b>Loss</b>. In this post, I\u2019m focussing on regression <b>loss</b>. In future posts I cover <b>loss</b> functions in other categories. Please let me know in comments if I miss something. Also, all the codes and plots shown in this blog <b>can</b> be found in this notebook. Regression functions predict a quantity, and classification functions predict a label. Regression <b>loss</b> 1. Mean Square Error, Quadratic <b>loss</b>, L2 <b>Loss</b>. Mean Square ...", "dateLastCrawled": "2022-02-02T07:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>sklearn.linear_model.SGDRegressor</b> \u2014 scikit-learn 1.0.2 documentation", "url": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html", "isFamilyFriendly": true, "displayUrl": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html", "snippet": "The regularizer is a <b>penalty</b> added to the <b>loss</b> function that shrinks model parameters towards the zero vector using either the <b>squared</b> euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection. This implementation works with data represented as dense numpy arrays of floating point values for ...", "dateLastCrawled": "2022-02-02T11:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Squared Error Loss</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/computer-science/squared-error-loss", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/computer-science/<b>squared-error-loss</b>", "snippet": "Note that the function that minimizes the mean of the log-<b>loss</b>, with respect to y, is the same as the one given in (7.100) (try it). However, if one employs the log-<b>loss</b> instead of the exponential, the optimization task gets more involved, and one has to resort to gradient descent or Newton-type schemes for optimization (see [16]).", "dateLastCrawled": "2022-01-27T13:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Loss Functions in Machine Learning</b> | Working | Different Types", "url": "https://www.educba.com/loss-functions-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.educba.com/<b>loss-functions-in-machine-learning</b>", "snippet": "The word \u2018<b>Loss</b>\u2019 states the <b>penalty</b> for failing to achieve the expected output. If the deviation in the predicted value than the expected value by our model is large, then the <b>loss</b> function gives the higher number as output, and if the deviation is small &amp; much closer to the expected value, it outputs a smaller number. Start Your Free Data Science Course. Hadoop, Data Science, Statistics &amp; others. Here\u2019s an example of when we are trying to predict house sales price in metro cities ...", "dateLastCrawled": "2022-02-03T02:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "L1 vs L2 <b>Regularization</b>: The intuitive difference | by Dhaval Taunk ...", "url": "https://medium.com/analytics-vidhya/l1-vs-l2-regularization-which-is-better-d01068e6658c", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/l1-vs-l2-<b>regularization</b>-which-is-better-d01068e6658c", "snippet": "As we <b>can</b> see from the formula of L1 and L2 <b>regularization</b>, L1 <b>regularization</b> adds the <b>penalty</b> term in cost function by adding the absolute value of weight (Wj) parameters, while L2 <b>regularization</b> ...", "dateLastCrawled": "2022-01-29T09:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Regularization: Ridge, Lasso &amp; Elastic Net Regression - DataCamp", "url": "https://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net", "isFamilyFriendly": true, "displayUrl": "https://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net", "snippet": "The only difference in ridge and lasso <b>loss</b> functions is in the <b>penalty</b> terms. Under lasso, the <b>loss</b> is defined as: Lasso: R example. To run Lasso Regression you <b>can</b> re-use the glmnet() function, but with the alpha parameter set to 1. # Perform 10-fold cross-validation to select lambda ----- lambdas_to_try &lt;- 10^seq(-3, 5, length.out = 100) # Setting alpha = 1 implements lasso regression lasso_cv &lt;- cv.glmnet(X, y, alpha = 1, lambda = lambdas_to_try, standardize = TRUE, nfolds = 10) # Plot ...", "dateLastCrawled": "2022-02-02T03:52:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine</b> <b>Learning</b> fundamentals I: An <b>analogy</b> | Finn Rietz.dev", "url": "http://www.finnrietz.dev/machine%20learning/part-1-analogy/", "isFamilyFriendly": true, "displayUrl": "www.finnrietz.dev/<b>machine</b> <b>learning</b>/part-1-<b>analogy</b>", "snippet": "And this is what the <b>loss</b> function does, so the <b>loss</b> function for a <b>Machine</b> <b>learning</b> algorithm is like the teacher for the real-world dermatologist in-training. In mathematical terms, the <b>loss</b> function could look something like this: \\(L = (y_i - \\hat{y_i})^2\\), where \\(y_i\\) is the actual output value (the one that the teacher has written down) and \\(\\hat{y_i}\\) is the one our <b>learning</b> algorithm produced.", "dateLastCrawled": "2022-01-16T09:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Bias-Variance Decomposition</b> - mlxtend", "url": "http://rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp/", "isFamilyFriendly": true, "displayUrl": "rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp", "snippet": "We can decompose a <b>loss</b> function such as the <b>squared</b> <b>loss</b> into three terms, a variance, bias, and a noise term (and the same is true for the decomposition of the 0-1 <b>loss</b> later). However, for simplicity, we will ignore the noise term. Before we introduce the <b>bias-variance decomposition</b> of the 0-1 <b>loss</b> for classification, let us start with the decomposition of the <b>squared</b> <b>loss</b> as an easy warm-up exercise to get familiar with the overall concept. The previous section already listed the common ...", "dateLastCrawled": "2022-01-31T12:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Machined Learnings: ML and OR: An <b>analogy</b> with cost-sensitive ...", "url": "http://www.machinedlearnings.com/2010/07/ml-and-or.html", "isFamilyFriendly": true, "displayUrl": "www.<b>machine</b>d<b>learning</b>s.com/2010/07/ml-and-or.html", "snippet": "Nonetheless I&#39;ve been amusing myself by thinking about it, in particular trying to think about it from a <b>machine</b> <b>learning</b> reduction standpoint. The simplest well-understood reduction that I can think of which is analogous to supplying estimates to a linear program is the reduction of cost-sensitive multiclass classification (CSMC) to regression.", "dateLastCrawled": "2021-12-25T17:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Understanding the 3 most common <b>loss</b> functions for <b>Machine</b> <b>Learning</b> ...", "url": "https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-the-3-most-common-<b>loss</b>-functions-for...", "snippet": "A <b>loss function</b> in <b>Machine</b> <b>Learning</b> is a measure of how accurately your ML model is able to predict the expected outcome i.e the ground truth. The <b>loss function</b> will take two items as input: the output value of our model and the ground truth expected value. The output of the <b>loss function</b> is called the <b>loss</b> which is a measure of how well our model did at predicting the outcome. A high value for the <b>loss</b> means our model performed very poorly. A low value for the <b>loss</b> means our model performed ...", "dateLastCrawled": "2022-02-02T13:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "TensorFlow is an interface for expressing <b>machine</b> <b>learning</b> algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Introduction to Machine Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/24/introduction-to-machine-learning-2/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/24/<b>introduction-to-machine-learning</b>-2", "snippet": "Optimization methods are applied to minimize the <b>loss</b> function by changing the parameter values, which is the central theme of <b>machine</b> <b>learning</b>.Zero-one <b>loss</b> is L0-1 = 1 (m &lt;= 0); in zero-one <b>loss</b>, value of <b>loss</b> is 0 for m &gt;= 0 whereas 1 for m &lt; 0. The difficult part with this <b>loss</b> is it is not differentiable, non-convex, and also NP-hard. Hence, in order to make optimization feasible and solvable, these losses are replaced by different surrogate losses for different problems.", "dateLastCrawled": "2022-01-17T05:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Mean squared error</b> in <b>machine</b> <b>learning</b> | by Aaron Li | MLearning.ai ...", "url": "https://medium.com/mlearning-ai/gradient-descent-4fda4e3fbdc0", "isFamilyFriendly": true, "displayUrl": "https://medium.com/m<b>learning</b>-ai/gradient-descent-4fda4e3fbdc0", "snippet": "For the ones <b>learning</b> <b>machine</b> <b>learning</b> in a bottom up approach, I suggest you try to train some models in Google Co-lab and get an idea of how <b>machine</b> <b>learning</b> works.", "dateLastCrawled": "2022-01-30T03:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A Big Problem with Linear Regression and How to Solve It | Towards Data ...", "url": "https://towardsdatascience.com/robust-regression-23b633e5d6a5", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/robust-regression-23b633e5d6a5", "snippet": "Huber regression in action. Red dashed lines are <b>squared</b> while orange dotted lines are not. (Animation by author) Nice! Now the outliers are not affecting the algorithm as much as before, because their corresponding <b>loss</b> is attenuated. Using the spring <b>analogy</b>, the orange springs are now weaker than the red ones and not pulling as much.", "dateLastCrawled": "2022-02-01T17:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Evaluating a <b>Machine</b> <b>Learning</b> Model: Regression and Classification ...", "url": "https://arnavbansal-8232.medium.com/evaluating-a-machine-learning-model-regression-and-classification-metrics-4f2316e180b4", "isFamilyFriendly": true, "displayUrl": "https://arnavbansal-8232.medium.com/evaluating-a-<b>machine</b>-<b>learning</b>-model-regression-and...", "snippet": "<b>Machine</b> <b>Learning</b> and Deep <b>Learning</b> plays a vital role in all this. The most important part of anything which is done in the field of <b>Machine</b> <b>Learning</b> and Deep <b>Learning</b> is its application. Applications are driven by performance and performance is achieved by better and improved results. Now, there are many automated tools, libraries and scripts which allows you to directly run a ML model without knowing anything about it. This results in fast development, and especially beginners are ...", "dateLastCrawled": "2022-01-05T00:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Solutions to the exercises for <b>Machine</b> <b>Learning</b>", "url": "http://www.perfmath.com/ml/ml_liu_text_solutions.pdf", "isFamilyFriendly": true, "displayUrl": "www.perfmath.com/ml/ml_liu_text_solutions.pdf", "snippet": "<b>Machine</b> <b>Learning</b> A Quantitative Approach Henry H. Liu P PerfMath. ... Bayesian, (4) <b>Analogy</b>, and (5) Unsupervised <b>learning</b>. Pedro Domingos proposed these five ML paradigms, and \u00a71.3 explains briefly what each of these five ML paradigms is about. <b>MACHINE</b> <b>LEARNING</b>: A QUANTITATIVE APPROACH 5 2 <b>Machine</b> <b>Learning</b> Fundamentals Illustrated with Regression 2.1 Try to find a publicly available <b>machine</b> <b>learning</b> dataset and apply an end-to-end procedure similar to the one we used with the fuel economy ...", "dateLastCrawled": "2022-01-18T07:21:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A novel <b>semi-supervised support vector machine with asymmetric</b> squared ...", "url": "https://link.springer.com/article/10.1007/s11634-020-00390-y", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11634-020-00390-y", "snippet": "In the field of <b>machine</b> <b>learning</b>, loss function is usually one of the key issues in designing <b>learning</b> algorithms since most problems require it to describe the cost of the discrepancy between the prediction and the observation. In fact, the use of the loss function can be traced back to a long time ago. For example, the least-square loss function for regression was already employed by Legendre, Gauss, and Adrain in the early 19th century (Steinwart and Christmann 2008). At present, various ...", "dateLastCrawled": "2021-11-13T10:19:00.0000000Z", "language": "en", "isNavigational": false}], [], [], []], "all_bing_queries": ["+(squared loss)  is like +(penalty)", "+(squared loss) is similar to +(penalty)", "+(squared loss) can be thought of as +(penalty)", "+(squared loss) can be compared to +(penalty)", "machine learning +(squared loss AND analogy)", "machine learning +(\"squared loss is like\")", "machine learning +(\"squared loss is similar\")", "machine learning +(\"just as squared loss\")", "machine learning +(\"squared loss can be thought of as\")", "machine learning +(\"squared loss can be compared to\")"]}