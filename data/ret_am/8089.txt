{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "End-<b>to-end Masked Language Modeling with BERT</b> - Keras", "url": "https://keras.io/examples/nlp/masked_language_modeling/", "isFamilyFriendly": true, "displayUrl": "https://keras.io/examples/nlp/<b>masked</b>_<b>language</b>_<b>model</b>ing", "snippet": "<b>Masked</b> <b>language</b> modeling is a great way to train a <b>language</b> <b>model</b> in a self-supervised setting (without <b>human</b>-annotated labels). Such a <b>model</b> can then be fine-tuned to accomplish various supervised NLP tasks. This example teaches you how to build a BERT <b>model</b> from scratch, train it with the <b>masked</b> <b>language</b> modeling task, and then fine-tune this ...", "dateLastCrawled": "2022-02-02T22:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Easy <b>Masked</b> <b>Language Modeling with Machine Learning</b> and HuggingFace ...", "url": "https://www.machinecurve.com/index.php/2021/03/02/easy-masked-language-modeling-with-machine-learning-and-huggingface-transformers/", "isFamilyFriendly": true, "displayUrl": "https://www.machinecurve.com/index.php/2021/03/02/easy-<b>masked</b>-<b>language</b>-<b>model</b>ing-with...", "snippet": "<b>Masked</b> <b>Language</b> Modeling works slightly differently. In this case, a <b>model</b> does not have access to the full input. Rather, it has access to a ... The task adds complexity on top of a regular <b>language</b> <b>model</b> task, and some works argue that it can help boost performance. <b>Masked</b> <b>Language</b> Modeling. MLM is primarily used for pretraining a <b>model</b>, after which it can be finetuned to a particular downstream task. As you can see in the image below, no text needs to be labeled by <b>human</b> labelers in order ...", "dateLastCrawled": "2022-01-31T01:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "10 <b>Leading Language Models For NLP</b> In 2021 - TOPBOTS", "url": "https://www.topbots.com/leading-nlp-language-models-2020/", "isFamilyFriendly": true, "displayUrl": "https://www.topbots.com/leading-nlp-<b>language</b>-<b>models</b>-2020", "snippet": "<b>Masked</b> <b>language</b> modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK] and then train a <b>model</b> to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some ...", "dateLastCrawled": "2022-02-02T10:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "On the Inductive Bias of <b>Masked</b> <b>Language</b> Modeling: From Statistical to ...", "url": "https://aclanthology.org/2021.naacl-main.404.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/2021.naacl-main.404.pdf", "snippet": "<b>masked</b> <b>language</b> <b>model</b> (MLM) objective and existing methods for learning statistical depen-dencies in graphical models. Using this, we derive a method for extracting these learned statistical dependencies in MLMs and show that these dependencies encode useful induc-tive biases in the form of syntactic structures.", "dateLastCrawled": "2021-12-27T07:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "What is <b>a masked language model, and how</b> is it related to BERT? - Quora", "url": "https://www.quora.com/What-is-a-masked-language-model-and-how-is-it-related-to-BERT", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>a-masked-language-model-and-how</b>-is-it-related-to-BERT", "snippet": "Answer (1 of 2): <b>Masked</b> <b>language</b> modeling is an example of autoencoding <b>language</b> modeling (the output is reconstructed from corrupted input) - we typically mask one or more of words in a sentence and have the <b>model</b> predict those <b>masked</b> words given the other words in sentence. By training the mode...", "dateLastCrawled": "2022-02-03T01:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "What is <b>BERT (Language Model</b>) and How Does It Work?", "url": "https://www.techtarget.com/searchenterpriseai/definition/BERT-language-model", "isFamilyFriendly": true, "displayUrl": "https://www.techtarget.com/searchenterpriseai/definition/<b>BERT-language-model</b>", "snippet": "The objective of <b>Masked</b> <b>Language</b> <b>Model</b> (MLM) training is to hide a word in a sentence and then have the program predict what word has been hidden (<b>masked</b>) based on the hidden word&#39;s context. The objective of Next Sentence Prediction training is to have the program predict whether two given sentences have a logical, sequential connection or whether their relationship is simply random. Background. Transformers were first introduced by Google in 2017. At the time of their introduction, <b>language</b> ...", "dateLastCrawled": "2022-02-02T23:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "The <b>language</b> of <b>proteins: NLP, machine learning &amp; protein sequences</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S2001037021000945", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2001037021000945", "snippet": "<b>Like</b> <b>human</b> <b>language</b>, protein sequences can be naturally represented as strings of letters ... an English <b>language</b> <b>model</b> might be given a <b>masked</b> sentence such as \u201cThe ____ sat on the mat\u201d and be tasked to predict what English words are plausible candidates for the mask token (e.g. \u201ccat\u201d or \u201cdog\u201d). While <b>language</b> modeling problems may not have unique solutions (e.g. both cats and dogs are plausible mat-sitting entities), it serves as an excellent generalizable proxy for ...", "dateLastCrawled": "2022-02-03T02:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Guide <b>to XLNet for Language Understanding</b>", "url": "https://analyticsindiamag.com/guide-to-xlnet-for-language-understanding/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/guide-<b>to-xlnet-for-language-understanding</b>", "snippet": "The <b>Masked</b> <b>Language</b> modelling objective of BERT can be represented mathematically as . Drawbacks of BERT. There are two problems with this objective . Independence Assumption. We make an assumption that each missing token is dependent on all the input tokens but independent of other <b>masked</b> tokens. Let\u2019s see an example of why this is problematic. Dogs love _____ _____ <b>Model</b> ranks [\u2018eating\u2019,\u2019playing\u2019] as the most probable words in position 2. It also ranks [\u2018fetch\u2019,\u2019meat\u2019] as ...", "dateLastCrawled": "2022-02-03T04:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>BERT</b> Explained: State of the art <b>language</b> <b>model</b> for NLP | by Rani Horev ...", "url": "https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>bert</b>-explained-state-of-the-art-<b>language</b>-<b>model</b>-for-nlp...", "snippet": "The paper\u2019s results show that a <b>language</b> <b>model</b> which is bidirectionally trained can have a deeper sense of <b>language</b> context and flow than single-direction <b>language</b> models. In the paper, the researchers detail a novel technique named <b>Masked</b> LM (MLM) which allows bidirectional training in models in which it was previously impossible. Background. In the field of computer vision, researchers have repeatedly shown the value of transfer learning \u2014 pre-training a neural network <b>model</b> on a known ...", "dateLastCrawled": "2022-02-03T02:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "A deep dive into <b>multilingual</b> NLP models - Peltarion", "url": "https://peltarion.com/blog/data-science/a-deep-dive-into-multilingual-nlp-models", "isFamilyFriendly": true, "displayUrl": "https://peltarion.com/blog/data-science/a-deep-dive-into-<b>multilingual</b>-nlp-<b>models</b>", "snippet": "XLM. XLM (Lample and Conneau, 2019) is a Transformer-based <b>model</b> that, <b>like</b> BERT, is trained with the <b>masked</b> <b>language</b> modeling (MLM) objective. Additionally, XLM is trained with a Translation <b>Language</b> Modeling (TLM) objective in an attempt to force the <b>model</b> to learn similar representations for different languages. TLM is quite simple: input the same sentence in two different languages and mask tokens as usual.", "dateLastCrawled": "2022-01-26T03:08:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Mask and Infill: <b>Applying Masked Language Model</b> for Sentiment Transfer", "url": "https://www.ijcai.org/Proceedings/2019/0732.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.ijcai.org/Proceedings/2019/0732.pdf", "snippet": "a process is very <b>similar</b> to the task of text in\ufb01lling[Zhuet al., 2019] or Cloze[Taylor, 1953]. Given the template \ufb01 scenery and service\ufb02, we <b>human</b> feel easy to \ufb01ll the right words because we have enough prior linguistic knowledge to predict the missing words from their contexts. We transform the problem of sentiment transfer to the task of text in\ufb01lling by a pre-trained <b>Masked</b> <b>Language</b> <b>Model</b> (MLM). We utilize a pre-trained deep bidirectional <b>language</b> <b>model</b> (corresponding <b>to human</b> ...", "dateLastCrawled": "2022-02-03T03:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Euphemistic Phrase Detection by <b>Masked</b> <b>Language</b> <b>Model</b>", "url": "https://aclanthology.org/2021.findings-emnlp.16.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/2021.findings-emnlp.16.pdf", "snippet": "<b>human</b> effort for the \ufb01rst time, as far as we are aware. We \ufb01rst perform phrase mining on a raw text corpus (e.g., social media posts) to extract quality phrases. Then, we utilize word embedding similarities to select a set of euphemistic phrase candidates. Finally, we rank those candidates by a <b>masked</b> <b>language</b> <b>model</b>\u2014SpanBERT. Compared to strong baselines, we report 20-50% higher detection accuracies using our algorithm for detecting euphemistic phrases. 1 Introduction Euphemisms ...", "dateLastCrawled": "2022-01-12T23:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "10 <b>Leading Language Models For NLP</b> In 2021 - TOPBOTS", "url": "https://www.topbots.com/leading-nlp-language-models-2020/", "isFamilyFriendly": true, "displayUrl": "https://www.topbots.com/leading-nlp-<b>language</b>-<b>models</b>-2020", "snippet": "<b>Masked</b> <b>language</b> modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK] and then train a <b>model</b> to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some ...", "dateLastCrawled": "2022-02-02T10:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) A Closer Look at Linguistic Knowledge in <b>Masked</b> <b>Language</b> Models ...", "url": "https://www.academia.edu/66081605/A_Closer_Look_at_Linguistic_Knowledge_in_Masked_Language_Models_The_Case_of_Relative_Clauses_in_American_English", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/66081605/A_Closer_Look_at_Linguistic_Knowledge_in_<b>Masked</b>...", "snippet": "Our <b>masked</b> <b>language</b> modeling evaluation provided deeper insights into <b>model</b>-specific differences. We evaluated relativizer as well as antecedent prediction. Overall, all models show better performance on grammatical than semantic knowledge (animacy and plausibility). Regarding relativizer prediction, all models perform worst on the target word which (plausible, as it is the most versatile of the relativizers). Comparing models, BERT is best in predicting the actual targets, RoBERTa ...", "dateLastCrawled": "2022-02-05T03:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "What is <b>a masked language model, and how</b> is it related to BERT? - Quora", "url": "https://www.quora.com/What-is-a-masked-language-model-and-how-is-it-related-to-BERT", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>a-masked-language-model-and-how</b>-is-it-related-to-BERT", "snippet": "Answer (1 of 2): <b>Masked</b> <b>language</b> modeling is an example of autoencoding <b>language</b> modeling (the output is reconstructed from corrupted input) - we typically mask one or more of words in a sentence and have the <b>model</b> predict those <b>masked</b> words given the other words in sentence. By training the mode...", "dateLastCrawled": "2022-02-03T01:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>PEGASUS</b>: Google\u2019s State of the Art Abstractive Summarization <b>Model</b> | by ...", "url": "https://towardsdatascience.com/pegasus-google-state-of-the-art-abstractive-summarization-model-627b1bbbc5ce", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>pegasus</b>-google-state-of-the-art-abstractive...", "snippet": "<b>Masked</b> <b>Language</b> <b>Model</b> (MLM) Although the main contribution of <b>PEGASUS</b> is the GSG (discussed in the previous section), its base architecture consists of an encoder and a decoder; hence, it makes sense to pre-train the encoder as a <b>masked</b> <b>language</b> <b>model</b>. <b>Language</b> Modeling v/s <b>Masked</b> <b>Language</b> Modeling by Google AI Blog. In this task, we randomly mask words from the sequences and use other words from the sequence to predict these <b>masked</b> words. The GSG task can be construed as a document-level ...", "dateLastCrawled": "2022-02-03T02:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) <b>Masked Language Model Scoring</b>", "url": "https://www.researchgate.net/publication/336936226_Masked_Language_Model_Scoring", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/336936226_<b>Masked_Language_Model_Scoring</b>", "snippet": "<b>similar</b> size but trained on more data. Gains scale . with dataset and <b>model</b> size: RoBERT a large (Liu. et al., 2019) improves an end-to-end ASR <b>model</b>. with relative WER reductions of 30%, 18% on ...", "dateLastCrawled": "2021-11-13T20:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "[1908.08039] &quot;Mask <b>and Infill&quot; : Applying Masked Language Model</b> to ...", "url": "https://arxiv.org/abs/1908.08039", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/abs/1908.08039", "snippet": "Such a process is very <b>similar</b> to the task of Text Infilling or Cloze, which could be handled by a deep bidirectional <b>Masked</b> <b>Language</b> <b>Model</b> (e.g. BERT). So we propose a two step approach &quot;Mask and Infill&quot;. In the mask step, we separate style from content by masking the positions of sentimental tokens. In the infill step, we retrofit MLM to Attribute Conditional MLM, to infill the <b>masked</b> positions by predicting words or phrases conditioned on the context1 and target sentiment. We evaluate our ...", "dateLastCrawled": "2021-06-09T10:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "LAnoBERT : System Log Anomaly Detection based on BERT <b>Masked</b> <b>Language</b> <b>Model</b>", "url": "https://deepai.org/publication/lanobert-system-log-anomaly-detection-based-on-bert-masked-language-model", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/lanobert-system-log-anomaly-detection-based-on-bert...", "snippet": "Train Phase Training is performed using the BERT <b>Masked</b> <b>language</b> <b>model</b> for a log sequence that has been tokenized through a tokenizer trained with normal data. Training is initiated from scratch using the initialized BERT, and the same parameters as the BERT-base-uncased are used for the <b>model</b>. The training parameters are almost identical to those of the original BERT (devlin-etal-2019-bert), and only the masking probability differs. As an unsupervised learning-based anomaly detection <b>model</b> ...", "dateLastCrawled": "2022-02-03T04:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Evaluation Metrics for <b>Language</b> Modeling", "url": "https://thegradient.pub/understanding-evaluation-metrics-for-language-models/", "isFamilyFriendly": true, "displayUrl": "https://thegradient.pub/understanding-evaluation-metrics-for-<b>language</b>-<b>models</b>", "snippet": "If a <b>human</b> was a <b>language</b> <b>model</b> with statistically low cross entropy. Source: xkcd Bits-per-character and bits-per-word. Bits-per-character (BPC) is another metric often reported for recent <b>language</b> models. It measures exactly the quantity that it is named after: the average number of bits needed to encode on character. This leads to revisiting Shannon\u2019s explanation of entropy of a <b>language</b>: \u201cif the <b>language</b> is translated into binary digits (0 or 1) in the most efficient way, the entropy ...", "dateLastCrawled": "2022-02-03T11:56:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Understanding <b>BERT</b> Part 2: <b>BERT</b> Specifics | by Francisco Ingham ...", "url": "https://medium.com/dissecting-bert/dissecting-bert-part2-335ff2ed9c73", "isFamilyFriendly": true, "displayUrl": "https://medium.com/dissecting-<b>bert</b>/dissecting-<b>bert</b>-part2-335ff2ed9c73", "snippet": "Task 1: <b>Masked</b> <b>Language</b> <b>Model</b>. The <b>Masked</b> <b>Language</b> <b>Model</b> asks the <b>model</b> to predict, not the next word for a sequence of words, but rather random words from within the sequence. How were tokens <b>masked</b>?", "dateLastCrawled": "2022-01-25T23:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Cognitive and Linguistic Contributions to <b>Masked</b> Speech Recognition in ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8060059/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8060059", "snippet": "The overall goal of this study was to examine <b>masked</b> sentence recognition with three different maskers to determine whether the masker characteristics influence the relationships between <b>masked</b> speech recognition and <b>language</b>, working memory, and selective attention. Based on the ELU <b>model</b> and our previous research, we predicted that children with advanced <b>language</b> and working memory abilities would have better performance in an SSN masker than children with poorer abilities. Groups of ...", "dateLastCrawled": "2021-12-09T11:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "AI models from Microsoft and Google already surpass <b>human</b> performance ...", "url": "https://venturebeat.com/2021/01/06/ai-models-from-microsoft-and-google-already-surpass-human-performance-on-the-superglue-language-benchmark/", "isFamilyFriendly": true, "displayUrl": "https://<b>venturebeat.com</b>/2021/01/06/ai-<b>models</b>-from-microsoft-and-google-already-surpass...", "snippet": "DeBERTa is pretrained through <b>masked</b> <b>language</b> modeling (MLM), a fill-in-the-blank task where a <b>model</b> is taught to use the words surrounding a <b>masked</b> \u201ctoken\u201d to predict what the <b>masked</b> word ...", "dateLastCrawled": "2022-01-17T06:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Vokenization: Multimodel Learning for Vision</b> and <b>Language</b> - ML@B Blog", "url": "https://ml.berkeley.edu/blog/posts/vokens/", "isFamilyFriendly": true, "displayUrl": "https://ml.berkeley.edu/blog/posts/vokens", "snippet": "The name vokenization name stems from the combination of vision and tokens, forming vokens. A voken is an image that corresponds with a given <b>language</b> token and <b>can</b> <b>be thought</b> of as a visualization of a token. Figure 2: A visualization of <b>model</b>-generated vokens in which the <b>model</b> classifies which visual voken corresponds with the <b>language</b> token.", "dateLastCrawled": "2022-01-21T00:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "GitHub - Sylar257/<b>Transformers-in-NLP</b>: Understand concept in the paper ...", "url": "https://github.com/Sylar257/Transformers-in-NLP", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/Sylar257/<b>Transformers-in-NLP</b>", "snippet": "Its performance improvement is largely attributed to its bidirectional transformer using <b>Masked</b> <b>Language</b> <b>Model</b>. RoBERTa, ... To <b>human</b>, this is such a simple question but not as simple to an algorithm. At the high-level, Self-attention allows the algorithm to associate &#39;it&#39; to &#39;cat&#39;. As the <b>model</b> processes each word (each position in the input sequence), self attention allows it to look at other positions in the input sequence for clues that <b>can</b> help lead to a better encoding for this word ...", "dateLastCrawled": "2021-12-14T12:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "The Form of Morphemes: MEG Evidence From <b>Masked</b> Priming of Two Hebrew ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6240614/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC6240614", "snippet": "On some level of representation, all <b>masked</b> priming <b>can</b> <b>be thought</b> of as identity priming: the visual form is recognized, making it easier to recognize once the form appears as part of the target. In English, stem priming has been reported in a number of studies: a stimulus consisting of a stem and one suffix primes a target consisting of the same stem and a different suffix ( Rastle et al., 2000 , 2004 ).", "dateLastCrawled": "2021-10-02T06:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Using lexical <b>language</b> models to detect borrowings in monolingual wordlists", "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0242709", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0242709", "snippet": "Although <b>masked</b> with time, <b>language</b>-internal evidence for borrowing <b>can</b> be observed in many languages from different families. In many Hmong-Mien languages, for example, some Chinese words are borrowed with a very specific tone that only occurs in Chinese words . Similarly, it is easy for German speakers to identify job as a loan from English, since only in borrowed words the grapheme j is pronounced as [dZ] in German. In the same line, but in a radically different context, speakers of ...", "dateLastCrawled": "2020-12-10T17:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 7, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "BERT (<b>language</b> <b>model</b>) - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/BERT_(Language_model)", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/BERT_(<b>Language</b>_<b>model</b>)", "snippet": "Architecture. BERT is at its core a transformer <b>language</b> <b>model</b> with a variable number of encoder layers and self-attention heads. The architecture is &quot;almost identical&quot; to the original transformer implementation in Vaswani et al. (2017). BERT was pretrained on two tasks: <b>language</b> modelling (15% of tokens were <b>masked</b> and BERT was trained to predict them from context) and next sentence prediction (BERT was trained to predict if a chosen next sentence was probable or not given the first ...", "dateLastCrawled": "2022-02-02T23:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Face masks reduce emotion-recognition accuracy and perceived ... - PLOS", "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0249792", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0249792", "snippet": "Face masks and emotion recognition. Effective <b>human</b> communication relies on the accurate perception of emotions [].For instance, which emotions people perceive influences how they respond to their interaction partners [].According to the emotion-as-social-information <b>model</b> [], an emotional expression <b>can</b> instigate inferential processes, which inform cognitions and actions in turn.For example, if a target person expresses sadness when telling an observer about not being able to reach the top ...", "dateLastCrawled": "2021-12-07T05:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How to <b>Develop Word-Based Neural Language Models in Python</b> with Keras", "url": "https://machinelearningmastery.com/develop-word-based-neural-language-models-python-keras/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>develop-word-based-neural-language-models</b>-python-keras", "snippet": "<b>Language</b> modeling involves predicting the next word in a sequence given the sequence of words already present. A <b>language</b> <b>model</b> is a key element in many natural <b>language</b> processing models such as machine translation and speech recognition. The choice of how the <b>language</b> <b>model</b> is framed must match how the <b>language</b> <b>model</b> is intended to be used. In this tutorial, you will", "dateLastCrawled": "2022-02-02T23:34:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Robustness of <b>Masked</b> <b>Language</b> Models to Damage | by Megan Nguyen | Medium", "url": "https://megantnguyen.medium.com/robustness-of-bert-vs-mbert-observing-multilingual-advantage-in-nlp-44e60db560c4", "isFamilyFriendly": true, "displayUrl": "https://megantnguyen.medium.com/robustness-of-bert-vs-mbert-observing-multilingual...", "snippet": "Bilingual <b>masked</b> <b>language</b> models that fall under XLM have 6 layers of transformer blocks and 8 attention heads. However, the 17-<b>language</b> XLM <b>model</b> used has 16 layers of transformer blocks and 16 attention heads. The two bilingual models we focus on are xlm-mlm-ende-1024 (trained on English and German wikipedia words) and xlm-mlm-enfr-1024 (trained on English and French wikipedia words). Although all of the XLM models were fine tuned for each of the three downstream tasks mentioned below, we ...", "dateLastCrawled": "2021-12-12T18:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "BioBERT: a pre-trained biomedical <b>language</b> representation <b>model</b> for ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7703786/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7703786", "snippet": "BERT (Devlin et al., 2019) is a contextualized word representation <b>model</b> that is based on a <b>masked</b> <b>language</b> <b>model</b> and pre-trained using bidirectional transformers (Vaswani et al., 2017). Due to the nature of <b>language</b> modeling where future words cannot be seen, previous <b>language</b> models were limited to a combination of two unidirectional <b>language</b> models (i.e. left-to-right and right-to-left). BERT uses a <b>masked</b> <b>language</b> <b>model</b> that predicts randomly <b>masked</b> words in a sequence, and hence <b>can</b> be ...", "dateLastCrawled": "2022-02-02T22:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Fine-<b>Tuning Language Models for Sentiment Analysis</b> | by Scott Duda | Medium", "url": "https://scottmduda.medium.com/fine-tuning-language-models-for-sentiment-analysis-91db72396549", "isFamilyFriendly": true, "displayUrl": "https://scottmduda.medium.com/fine-<b>tuning-language-models-for-sentiment-analysis</b>-91db...", "snippet": "The <b>masked</b> <b>language</b> <b>model</b> task was described in the previous paragraph. The next sentence prediction task was incorporated to provide the <b>model</b> with more information about the relationships between sentences. While training on this task, the <b>model</b> sees an even mix of examples where the correct sentence from the training data follows the first sentence and where a random sentence has been inserted following the first sentence. Training on this task increases <b>model</b> performance on Question ...", "dateLastCrawled": "2022-01-24T19:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Understanding <b>BERT</b> Part 2: <b>BERT</b> Specifics | by Francisco Ingham ...", "url": "https://medium.com/dissecting-bert/dissecting-bert-part2-335ff2ed9c73", "isFamilyFriendly": true, "displayUrl": "https://medium.com/dissecting-<b>bert</b>/dissecting-<b>bert</b>-part2-335ff2ed9c73", "snippet": "Task 1: <b>Masked</b> <b>Language</b> <b>Model</b>. The <b>Masked</b> <b>Language</b> <b>Model</b> asks the <b>model</b> to predict, not the next word for a sequence of words, but rather random words from within the sequence. How were tokens <b>masked</b>?", "dateLastCrawled": "2022-01-25T23:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "An Illustrative Guide to <b>Masked</b> Image Modelling", "url": "https://analyticsindiamag.com/an-illustrative-guide-to-masked-image-modelling/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/an-illustrative-guide-to-<b>masked</b>-image-<b>model</b>ling", "snippet": "In machine learning, nowadays, we <b>can</b> see that the models and techniques of one domain <b>can</b> perform tasks of other domains. For example, models focused on natural <b>language</b> processing <b>can</b> also perform a few tasks related to computer vision.In this article, we will discuss such a technique that is transferable from NLP to computer vision. When applying it to the computer vision tasks, we <b>can</b> call it <b>Masked</b> Image Modelling.", "dateLastCrawled": "2022-02-02T11:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Step by <b>Step Guide: Abstractive Text Summarization Using RoBERTa</b> - Medium", "url": "https://anubhav20057.medium.com/step-by-step-guide-abstractive-text-summarization-using-roberta-e93978234a90", "isFamilyFriendly": true, "displayUrl": "https://anubhav20057.medium.com/step-by-<b>step-guide-abstractive-text-summarization</b>...", "snippet": "This allows RoBERTa to improve on the <b>masked</b> <b>language</b> modeling objective <b>compared</b> with BERT and leads to better downstream task performance. RoBERTa is a transformers <b>model</b> pretrained on a la r ge corpus of English data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labeling them in any way (which is why it <b>can</b> use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it was ...", "dateLastCrawled": "2022-02-02T18:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>GitHub</b> - <b>sunilchomal/GECwBERT</b>: Use <b>Language</b> <b>Model</b> (LM) for Grammar ...", "url": "https://github.com/sunilchomal/GECwBERT", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/sunilchomal/GECwBERT", "snippet": "The <b>masked</b> <b>language</b> <b>model</b> randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the <b>masked</b> word based only on its context. Unlike left-to right <b>language</b> <b>model</b> pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pretrain a deep bidirectional Transformer. [2]", "dateLastCrawled": "2022-02-01T06:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Vokenization: Multimodel Learning for Vision</b> and <b>Language</b> - ML@B Blog", "url": "https://ml.berkeley.edu/blog/posts/vokens/", "isFamilyFriendly": true, "displayUrl": "https://ml.berkeley.edu/blog/posts/vokens", "snippet": "How <b>can</b> <b>language</b> models better <b>model</b> <b>human</b> learning? A new technique known as Vokenization addresses this by grounding information from the external visual world. All Articles. <b>Vokenization: Multimodel Learning for Vision</b> and <b>Language</b>. By Aryia Dattamajumdar. \ud83d\udca1 Computer Vision meets Natural <b>Language</b> Processing. Vokenization is the bridge between visually supervised <b>language</b> models and their related images. In this blog post we explore the vokenization procedure and the inner works of the ...", "dateLastCrawled": "2022-01-21T00:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Google AI Blog: SimVLM: Simple Visual <b>Language</b> <b>Model</b> Pre-training with ...", "url": "https://ai.googleblog.com/2021/10/simvlm-simple-visual-language-model-pre.html", "isFamilyFriendly": true, "displayUrl": "https://ai.googleblog.com/2021/10/simvlm-simple-visual-<b>language</b>-<b>model</b>-pre.html", "snippet": "<b>Model</b> and Pre-training Procedure Unlike existing VLP methods that adopt pre-training procedures similar to <b>masked</b> <b>language</b> modeling (like in BERT), SimVLM adopts the sequence-to-sequence framework and is trained with a one prefix <b>language</b> <b>model</b> (PrefixLM) objective, which receives the leading part of a sequence (the prefix) as inputs, then predicts its continuation. For example, given the sequence \u201cA dog is chasing after a yellow ball\u201d, the sequence is randomly truncated to \u201cA dog is ...", "dateLastCrawled": "2022-02-02T13:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Domain-specific language model pretraining</b> for biomedical natural ...", "url": "https://www.microsoft.com/en-us/research/blog/domain-specific-language-model-pretraining-for-biomedical-natural-language-processing/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.microsoft.com</b>/en-us/research/blog/domain-specific-<b>language</b>-<b>model</b>-p...", "snippet": "Pretrained neural <b>language</b> models are the underpinning of state-of-the-art NLP methods. Pretraining works by masking some words from text and training a <b>language</b> <b>model</b> to predict them from the rest. Then, the pre-trained <b>model</b> <b>can</b> be fine-tuned for various downstream tasks using task-specific training data. As in mainstream NLP, prior work on ...", "dateLastCrawled": "2022-01-28T03:48:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine</b> <b>Learning</b> Glossary: <b>Language</b> Evaluation | Google Developers", "url": "https://developers.google.com/machine-learning/glossary/language", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine</b>-<b>learning</b>/glossary/<b>language</b>", "snippet": "For instance, a <b>masked</b> <b>language</b> <b>model</b> can calculate probabilities for candidate word(s) to replace the underline in the following sentence: The ____ in the hat came back. The literature typically uses the string &quot;MASK&quot; instead of an underline. For example: The &quot;MASK&quot; in the hat came back. Most modern <b>masked</b> <b>language</b> models are bidirectional.", "dateLastCrawled": "2022-01-29T02:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "The Illustrated GPT-2 (Visualizing Transformer <b>Language</b> Models) \u2013 Jay ...", "url": "http://jalammar.github.io/illustrated-gpt2/", "isFamilyFriendly": true, "displayUrl": "jalammar.github.io/illustrated-gpt2", "snippet": "GPT-2 <b>Masked</b> Self-Attention; Beyond <b>Language</b> modeling; You\u2019ve Made it! Part 3: Beyond <b>Language</b> Modeling. <b>Machine</b> Translation; Summarization ; Transfer <b>Learning</b>; Music Generation; Part #1: GPT2 And <b>Language</b> Modeling # So what exactly is a <b>language</b> <b>model</b>? What is a <b>Language</b> <b>Model</b>. In The Illustrated Word2vec, we\u2019ve looked at what a <b>language</b> <b>model</b> is \u2013 basically a <b>machine</b> <b>learning</b> <b>model</b> that is able to look at part of a sentence and predict the next word. The most famous <b>language</b> models ...", "dateLastCrawled": "2022-02-01T04:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "The <b>language</b> of <b>proteins: NLP, machine learning &amp; protein sequences</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S2001037021000945", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2001037021000945", "snippet": "For example, in the <b>masked</b> <b>language</b> task, some fraction of the tokens in the original text are <b>masked</b> at random, and the <b>language</b> <b>model</b> attempts to predict the original text. (B) (Pre-)trained <b>language</b> models are commonly fine-tuned on downstream tasks over labeled text, through a standard supervised-<b>learning</b> approach. Fine-tuning is typically much faster and provides superior performance than training a <b>model</b> from scratch, especially when labeled data is scarce.", "dateLastCrawled": "2022-02-03T02:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Natrual <b>language</b> processing basic concepts - <b>language</b> <b>model</b> - word ...", "url": "https://shuffleai.blog/blog/nlp_concepts_part_1.html", "isFamilyFriendly": true, "displayUrl": "https://shuffleai.blog/blog/nlp_concepts_part_1.html", "snippet": "Before deep <b>learning</b>&#39;s domination in natural <b>language</b> processing, a <b>language</b> <b>model</b> is basically a large lookup table, recording frequencies of different combinations of words&#39; occurrences in a large corpus. Now it&#39;s a neural network trained on a corpus or dataset. In addition, a causal <b>language</b> <b>model</b>(e.g., GPT) predicts the next word, and a <b>masked</b> <b>language</b> <b>model</b>(e.g., BERT) fills the blank given the rest of a sentence. If you input &quot;The man ____ to the store&quot; to BERT, it will predict the ...", "dateLastCrawled": "2021-12-24T07:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "An introduction to Deep <b>Learning</b> in Natural <b>Language</b> Processing: Models ...", "url": "https://www.sciencedirect.com/science/article/pii/S0925231221010997", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0925231221010997", "snippet": "The pre-training was driven by two <b>language</b> <b>model</b> objectives, i.e. <b>Masked</b> <b>Language</b> <b>Model</b> (MLM) and Next Sentence Prediction (NSP). In MLM, showed in Fig. 8 , the network masks a small number of words of the input sequence and it tries to predict them in output, whereas in NSP the network tries to understand the relations between sentences by means of a binary loss.", "dateLastCrawled": "2022-01-04T17:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>GitHub</b> - <b>rosinality/ml-papers</b>: My collection of <b>machine</b> <b>learning</b> papers", "url": "https://github.com/rosinality/ml-papers", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/rosinality/ml-papers", "snippet": "210413 <b>Masked</b> <b>Language</b> Modeling and the Distributional Hypothesis #<b>language</b>_<b>model</b> #mlm; 210417 mT6 #<b>language</b>_<b>model</b>; 210418 Data-Efficient <b>Language</b>-Supervised Zero-Shot <b>Learning</b> with #multimodal; 210422 ImageNet-21K Pretraining for the Masses #backbone; 210510 Are Pre-trained Convolutions Better than Pre-trained Transformers #nlp #convolution # ...", "dateLastCrawled": "2022-01-31T15:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Word Embeddings, WordPiece and Language-Agnostic BERT</b> (LaBSE) | by ...", "url": "https://medium.com/mlearning-ai/word-embeddings-wordpiece-and-language-agnostic-bert-labse-98c7626878c7", "isFamilyFriendly": true, "displayUrl": "https://medium.com/m<b>learning</b>-ai/<b>word-embeddings-wordpiece-and-language-agnostic-bert</b>...", "snippet": "Word embeddings are the representation of words in a numeric format, which can be understood by a computer. Simplest example would be (Yes, No) represented as (1, 0). But when we are dealing with\u2026", "dateLastCrawled": "2022-02-03T17:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "The 5 <b>Components Towards Building Production-Ready Machine Learning Systems</b>", "url": "https://www.topbots.com/building-production-ready-machine-learning-systems/", "isFamilyFriendly": true, "displayUrl": "https://www.topbots.com/<b>building-production-ready-machine-learning-systems</b>", "snippet": "A well-known recent case study of applying knowledge distillation in practice is Hugging Face\u2019s DistilBERT, which is a smaller <b>language</b> <b>model</b> derived from the supervision of the popular BERT <b>language</b> <b>model</b>. DistilBERT removed the toke-type embeddings and the pooler (used for the next sentence classification task) from BERT while keeping the rest of the architecture identical and reducing the number of layers by a factor of two. Overall, DistilBERT has about half the total number of ...", "dateLastCrawled": "2022-01-25T15:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "SpringerLink - International Journal of <b>Machine</b> <b>Learning</b> and Cybernetics", "url": "https://link.springer.com/article/10.1007/s13042-020-01069-8", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s13042-020-01069-8", "snippet": "The Neural Network <b>Language</b> <b>Model</b> (NNLM) is a pioneering work which introduces the idea of deep <b>learning</b> into <b>language</b> modeling and successfully mitigates the curse of dimensionality (i.e. Sequences in the test set is likely to have not been observed in the training data) by <b>learning</b> a distributed representation of words. The goal of <b>language</b> modeling is to learn a <b>model</b> that predicts the next word given previous ones. Practically, we assume the", "dateLastCrawled": "2022-01-29T10:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Intuitive Introduction to BERT \u2013 MachineCurve</b>", "url": "https://www.machinecurve.com/index.php/2021/01/04/intuitive-introduction-to-bert/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/index.php/2021/01/04/<b>intuitive-introduction-to-bert</b>", "snippet": "We do however often want to create a <b>machine</b> <b>learning</b> <b>model</b> that can perform one task really well. This is where finetuning comes in: using a labeled corpus, which is often smaller, we can then train the pretrained <b>model</b> further, with an additional or replacing NLP task. The end result is a <b>model</b> that has been pretrained on the large unlabeled corpus and which is finetuned to a specific <b>language</b> task, such as summarization, text generation in a particular domain, or translation.", "dateLastCrawled": "2022-01-30T22:30:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Improving Text Generation with Dynamic Masking and Recovering", "url": "https://www.ijcai.org/proceedings/2021/0534.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.ijcai.org/proceedings/2021/0534.pdf", "snippet": "tokens, <b>just as masked language model</b> does. Therefore, our approach jointly maximizes both the likelihoods of both sen-tence generation and prediction of masked tokens. We verify the effectiveness and generality of our ap-proach on three types of text generation tasks which use var-ious forms of input data including text, graph, and image. For sequence-to-sequence (seq2seq) generation task (specif-ically, <b>machine</b> translation), our model obtains signi\ufb01cant improvement of 1.01 and 0.90 BLEU ...", "dateLastCrawled": "2022-01-29T07:50:00.0000000Z", "language": "en", "isNavigational": false}], [], []], "all_bing_queries": ["+(masked language model)  is like +(human)", "+(masked language model) is similar to +(human)", "+(masked language model) can be thought of as +(human)", "+(masked language model) can be compared to +(human)", "machine learning +(masked language model AND analogy)", "machine learning +(\"masked language model is like\")", "machine learning +(\"masked language model is similar\")", "machine learning +(\"just as masked language model\")", "machine learning +(\"masked language model can be thought of as\")", "machine learning +(\"masked language model can be compared to\")"]}