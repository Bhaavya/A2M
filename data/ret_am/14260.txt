{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "An <b>introduction to sequence-to-sequence learning</b> - Loren Lugosch", "url": "https://lorenlugosch.github.io/posts/2019/02/seq2seq/", "isFamilyFriendly": true, "displayUrl": "https://lorenlugosch.github.io/posts/2019/02/seq2seq", "snippet": "This toy <b>task</b> is a bit easier to work with than a <b>task</b> <b>like</b> speech recognition or translation, in which you need lots of labelled data and lots of tricks to get something to work. A tutorial which covers the actual useful <b>task</b> of translating from French to English using PyTorch can be found here. Running the experiment. We can easily generate a dataset for the <b>task</b> of inferring missing vowels using existing text. I used the text of \u201cWar and Peace\u201d 14: the input sequences are lines from ...", "dateLastCrawled": "2022-02-02T00:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "How <b>to deal with a vocabulary</b>? - <b>Sequence to sequence</b> tasks | <b>Coursera</b>", "url": "https://www.coursera.org/lecture/language-processing/how-to-deal-with-a-vocabulary-mvV6t", "isFamilyFriendly": true, "displayUrl": "https://<b>www.coursera.org</b>/lecture/<b>language</b>-processing/how-<b>to-deal-with-a-vocabulary</b>-mvV6t", "snippet": "Nearly any <b>task</b> in NLP can be formulates as a <b>sequence to sequence</b> <b>task</b>: machine translation, summarization, question answering, and many more. In this module we will learn a general encoder-decoder-attention architecture that can be used to solve them. We will cover machine translation in more details and you will see how attention technique ...", "dateLastCrawled": "2022-01-18T13:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Introduction to Transformers in Machine Learning</b> \u2013 MachineCurve", "url": "https://www.machinecurve.com/index.php/2020/12/28/introduction-to-transformers-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.machinecurve.com/index.php/2020/12/28/introduction-to-transformers-in...", "snippet": "Suppose that our goal is to build a <b>language</b> model capable of translating German text <b>into</b> English. In the classic scenario, with more classic approaches, we would learn a model which is capable of making the translation directly. In other words, we are <b>teaching</b> <b>one</b> translator to <b>translate</b> German <b>into</b> English. In other words, the translator ...", "dateLastCrawled": "2022-02-03T03:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "The 7 <b>NLP</b> Techniques That Will Change How You Communicate in the Future ...", "url": "https://heartbeat.comet.ml/the-7-nlp-techniques-that-will-change-how-you-communicate-in-the-future-part-i-f0114b2f0497", "isFamilyFriendly": true, "displayUrl": "https://heartbeat.comet.ml/the-7-<b>nlp</b>-techniques-that-will-change-how-you-communicate...", "snippet": "The goal is for computers to process or \u201cunderstand\u201d natural <b>language</b> in order to perform tasks <b>like</b> <b>Language</b> Translation and Question Answering. With the rise of voice interfaces and chatbots, <b>NLP</b> is <b>one</b> of the most important technologies of the information age a crucial part of artificial intelligence. Fully understanding and representing the meaning of <b>language</b> is an extremely difficult goal. Why? Because human <b>language</b> is quite special. What\u2019s special about human <b>language</b>? A few ...", "dateLastCrawled": "2022-01-23T13:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "How to perform Text Summarization with Python, HuggingFace Transformers ...", "url": "https://www.machinecurve.com/index.php/2020/12/21/easy-text-summarization-with-huggingface-transformers-and-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.machinecurve.com/index.php/2020/12/21/easy-text-summarization-with-hugging...", "snippet": "The first translator can <b>translate</b> French <b>into</b> the imaginary <b>language</b>; the second then has learned to <b>translate</b> the intermediate <b>language</b> back <b>into</b> German. Without an understanding of both human languages, <b>one</b> translator (the encoder) and <b>another</b> (the decoder) can still perform the translation job. They have become the primary choice for ML driven <b>language</b> tasks these days because they can apply self-attention and are parallel in nature. As we have seen, previous approaches couldn\u2019t do ...", "dateLastCrawled": "2022-02-02T23:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "How to <b>train GPT-2 to Create JSOn</b>? : LanguageTechnology", "url": "https://www.reddit.com/r/LanguageTechnology/comments/j1iaxq/how_to_train_gpt2_to_create_json/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/<b>Language</b>Technology/comments/j1iaxq/how_to_train_gpt2_to...", "snippet": "So you&#39;re describing a <b>sequence to sequence</b> (seq2seq) <b>task</b>, which is a bit tricky with GPT-2 in huggingface. Most seq2seq tasks use a encoder-decoder setup where the encoder takes your inputs and encodes it <b>into</b> an intermediate representation and a decoder decodes <b>into</b> a target <b>language</b>. I don&#39;t think there&#39;s a straightforward way (e.g. using existing training scripts) in huggingface but I could be wrong. The combiners api", "dateLastCrawled": "2022-01-22T07:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "How to <b>build autocorrect with neural networks? How</b> should I start - Quora", "url": "https://www.quora.com/How-can-I-build-autocorrect-with-neural-networks-How-should-I-start", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/How-can-I-<b>build-autocorrect-with-neural-networks-How</b>-should-I-start", "snippet": "Answer (1 of 3): This is an interesting problem. If you opt for neural networks, then the approach would likely be a <b>sequence-to-sequence</b> model such as the ones used for <b>language</b> translation and such. However you need to take <b>into</b> account a dictionary of words and find which of these words would ...", "dateLastCrawled": "2022-01-13T16:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) <b>Artificial Intelligence A Modern Approach</b> (4th Edition) | Fahim ...", "url": "https://www.academia.edu/45126798/Artificial_Intelligence_A_Modern_Approach_4th_Edition_", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/45126798/<b>Artificial_Intelligence_A_Modern_Approach</b>_4th_Edition_", "snippet": "Artificial Intelligence (AI) is a big field, and this is a big book. We have tried to explore the full breadth of the field, which encompasses logic, probability, and continuous mathematics; perception, reasoning, learning, and action; fairness,", "dateLastCrawled": "2022-02-02T22:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Technological disruption in foreign <b>language</b> <b>teaching</b>: The rise of ...", "url": "https://www.cambridge.org/core/journals/language-teaching/article/technological-disruption-in-foreign-language-teaching-the-rise-of-simultaneous-machine-translation/A31BA5AB690B370B01535EF2D1AFAE42", "isFamilyFriendly": true, "displayUrl": "https://www.cambridge.org/core/journals/<b>language</b>-<b>teaching</b>/article/technological...", "snippet": "Generally, systems <b>like</b> Google <b>Translate</b> rely on n-gram approaches (i.e., continuous sequences of words) and do not rely on grammatical or syntactic structures. With large enough corpora, the machine learning algorithms can distinguish patterns between the two corpora and develop rules to produce <b>one</b> <b>language</b> given input from <b>another</b> <b>language</b>. More importantly, because simultaneous translation relies on deep learning algorithms that allow for feedback within the system, mistranslations can ...", "dateLastCrawled": "2021-08-08T08:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "An Introduction to Attention", "url": "https://wandb.ai/authors/under-attention/reports/An-Introduction-to-Attention--Vmlldzo1MzQwMTU", "isFamilyFriendly": true, "displayUrl": "https://wandb.ai/authors/under-attention/reports/An-Introduction-to-Attention--Vmlldzo...", "snippet": "The point being made here is that the sequence of words in <b>one</b> <b>language</b> has an impact in the formation of the sentence in <b>another</b> <b>language</b>. Things <b>like</b> tenses, parts of speech and several other grammatical concepts come <b>in to</b> the fray. Making <b>a computer</b> understand all of this is both very difficult and a main objective of this work.", "dateLastCrawled": "2021-11-29T17:20:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Chapter 8: Building a Chatbot Using Attention-Based Neural Networks ...", "url": "http://devguis.com/chapter-8-building-a-chatbot-using-attention-based-neural-networks-hands-on-natural-language-processing-with-pytorch-1-x.html", "isFamilyFriendly": true, "displayUrl": "devguis.com/chapter-8-building-a-chatbot-using-attention-based-neural-networks-hands...", "snippet": "In the previous chapter, we looked at how to construct <b>sequence-to-sequence</b> models to <b>translate</b> sentences from <b>one</b> <b>language</b> <b>into</b> <b>another</b>. A conversational chatbot that is capable of basic interactions works in much the same way. When we talk to a chatbot, our sentence becomes the input to the model. The output is whatever the chatbot chooses to reply with. Therefore, rather than training our chatbot to learn how to interpret our input sentence, we are <b>teaching</b> it how to respond.", "dateLastCrawled": "2021-12-28T04:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "How <b>to deal with a vocabulary</b>? - <b>Sequence to sequence</b> tasks | <b>Coursera</b>", "url": "https://www.coursera.org/lecture/language-processing/how-to-deal-with-a-vocabulary-mvV6t", "isFamilyFriendly": true, "displayUrl": "https://<b>www.coursera.org</b>/lecture/<b>language</b>-processing/how-<b>to-deal-with-a-vocabulary</b>-mvV6t", "snippet": "Nearly any <b>task</b> in NLP can be formulates as a <b>sequence to sequence</b> <b>task</b>: machine translation, summarization, question answering, and many more. In this module we will learn a general encoder-decoder-attention architecture that can be used to solve them. We will cover machine translation in more details and you will see how attention technique ...", "dateLastCrawled": "2022-01-18T13:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. TBD ... - <b>Computer</b> Science", "url": "https://www.cs.wm.edu/~denys/pubs/seq2seq4repair_TSE_cameraready.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.wm.edu/~denys/pubs/seq2seq4repair_TSE_cameraready.pdf", "snippet": "TION WITH <b>SEQUENCE-TO-SEQUENCE</b> LEARNING SEQUENCER is based on the idea of receiving buggy code as input and producing \ufb01xed code as output. The concept <b>is similar</b> to neural machine translation where the input is a sequence of words in <b>one</b> <b>language</b> and the output is a sequence in <b>another</b> <b>language</b>. In this section, we provide a", "dateLastCrawled": "2021-10-13T01:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "The 7 <b>NLP</b> Techniques That Will Change How You Communicate in the Future ...", "url": "https://heartbeat.comet.ml/the-7-nlp-techniques-that-will-change-how-you-communicate-in-the-future-part-i-f0114b2f0497", "isFamilyFriendly": true, "displayUrl": "https://heartbeat.comet.ml/the-7-<b>nlp</b>-techniques-that-will-change-how-you-communicate...", "snippet": "<b>Sequence to Sequence</b> Learning with Neural Networks proved the effectiveness of LSTM for Neural Machine Translation. It presents a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. The method uses a multilayered LSTM to map the input sequence to a vector of a fixed dimensionality, and then <b>another</b> deep LSTM to decode the target sequence from the vector.", "dateLastCrawled": "2022-01-23T13:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Intent classifier and slot tagger (NLU</b>) - Dialog systems | <b>Coursera</b>", "url": "https://www.coursera.org/lecture/language-processing/intent-classifier-and-slot-tagger-nlu-RmVnE", "isFamilyFriendly": true, "displayUrl": "https://<b>www.coursera.org</b>/lecture/<b>language</b>-processing/<b>intent-classifier-and-slot-tagger</b>...", "snippet": "In the next slide, I want to overview convolutional <b>sequence-to-sequence</b> model because that is- that gains popularity because it works faster and sometimes it even beats RNN in some tasks. Okay, let&#39;s see how convolutional networks can be used to model sequences. Let&#39;s say we have an input sequence which is bedding-bedding, then start of sequence and three German watts. And what we actually want to do, let&#39;s say, where we want to solve the <b>task</b> of <b>language</b> modeling. When we see each new ...", "dateLastCrawled": "2022-01-21T13:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Speech Recognition System</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/computer-science/speech-recognition-system", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/<b>computer</b>-science/<b>speech-recognition-system</b>", "snippet": "Li and others [14] trained a single <b>sequence to sequence</b> model for multi-dialect speech recognition and achieved a <b>similar</b> performance as <b>another</b> <b>sequence-to-sequence</b> model for single dialect tasks. <b>Another</b> side, the CNN-based acoustic model is proposed by Palaz et al. [15\u201317] which processes the raw speech directly as input. This model ...", "dateLastCrawled": "2022-01-20T22:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "7 Applications of Deep Learning for Natural <b>Language</b> Processing", "url": "https://machinelearningmastery.com/applications-of-deep-learning-for-natural-language-processing/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/applications-of-deep-learning-for-natural-<b>language</b>...", "snippet": "<b>Sequence to Sequence</b> \u2013 Video to Text, 2015. 5. Machine Translation . Machine translation is the problem of converting a source text in <b>one</b> <b>language</b> to <b>another</b> <b>language</b>. Machine translation, the automatic translation of text or speech from <b>one</b> <b>language</b> to <b>another</b>, is <b>one</b> [of] the most important applications of NLP. \u2014 Page 463, Foundations of Statistical Natural <b>Language</b> Processing, 1999. Given that deep neural networks are used, the field is referred to as neural machine translation. In a ...", "dateLastCrawled": "2022-01-31T10:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Is there a way of <b>teaching</b> a neural network using a string as input, a ...", "url": "https://www.quora.com/Is-there-a-way-of-teaching-a-neural-network-using-a-string-as-input-a-string-as-expected-output-and-to-teach-it-to-output-strings-when-given-a-string-as-input", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Is-there-a-way-of-<b>teaching</b>-a-neural-network-using-a-string-as...", "snippet": "Answer (1 of 3): Yes. Such networks are called <b>Sequence to Sequence</b> models. You already know that neural networks or any other model only understands numbers. So we have to find a way to represent these strings by numbers ( vectors to be more precise). For this, there are lot of approaches follow...", "dateLastCrawled": "2022-01-23T04:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "How to <b>build autocorrect with neural networks? How</b> should I start - Quora", "url": "https://www.quora.com/How-can-I-build-autocorrect-with-neural-networks-How-should-I-start", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/How-can-I-<b>build-autocorrect-with-neural-networks-How</b>-should-I-start", "snippet": "Answer (1 of 3): This is an interesting problem. If you opt for neural networks, then the approach would likely be a <b>sequence-to-sequence</b> model such as the ones used for <b>language</b> translation and such. However you need to take <b>into</b> account a dictionary of words and find which of these words would ...", "dateLastCrawled": "2022-01-13T16:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How to <b>train GPT-2 to Create JSOn</b>? : LanguageTechnology", "url": "https://www.reddit.com/r/LanguageTechnology/comments/j1iaxq/how_to_train_gpt2_to_create_json/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/<b>Language</b>Technology/comments/j1iaxq/how_to_train_gpt2_to...", "snippet": "So you&#39;re describing a <b>sequence to sequence</b> (seq2seq) <b>task</b>, which is a bit tricky with GPT-2 in huggingface. Most seq2seq tasks use a encoder-decoder setup where the encoder takes your inputs and encodes it <b>into</b> an intermediate representation and a decoder decodes <b>into</b> a target <b>language</b>. I don&#39;t think there&#39;s a straightforward way (e.g. using existing training scripts) in huggingface but I could be wrong. The combiners api", "dateLastCrawled": "2022-01-22T07:20:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "An <b>introduction to sequence-to-sequence learning</b> - Loren Lugosch", "url": "https://lorenlugosch.github.io/posts/2019/02/seq2seq/", "isFamilyFriendly": true, "displayUrl": "https://lorenlugosch.github.io/posts/2019/02/seq2seq", "snippet": "An <b>introduction to sequence-to-sequence learning</b>. Published: February 19, 2019. Many interesting problems in artificial intelligence <b>can</b> be described in the following way: Map a sequence of inputs $\\mathbf{x}$ to the correct sequence of outputs $\\mathbf{y}$. Speech recognition is <b>one</b> example: the goal is to map an audio signal $\\mathbf{x}$ (a sequence of real-valued audio samples) to the correct text transcript $\\mathbf{y}$ (a sequence of letters). Other examples are machine translation ...", "dateLastCrawled": "2022-02-02T00:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Generating Natural-Language Text with Neural Networks</b> \u2013 DeUmbra", "url": "https://deumbra.com/2018/07/generating-natural-language-text-with-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://deumbra.com/2018/07/<b>generating-natural-language-text-with-neural-networks</b>", "snippet": "A <b>sequence-to-sequence</b> (seq2seq) model consists of an encoder and a decoder, and it converts <b>one</b> sequence of words <b>into</b> <b>another</b>. The encoder converts the source sequence of words <b>into</b> a vector in meaning space, and the decoder converts that vector <b>into</b> the target sequence of words. Both the encoder and decoder are often recurrent neural networks (RNNs). The initial big application for seq2seq models was machine translation. For example, a model <b>can</b> be trained to convert source sentences in ...", "dateLastCrawled": "2022-01-16T17:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "BART: Denoising <b>Sequence-to-Sequence</b> Pre-training for Natural <b>Language</b> ...", "url": "https://www.researchgate.net/publication/343301801_BART_Denoising_Sequence-to-Sequence_Pre-training_for_Natural_Language_Generation_Translation_and_Comprehension", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/343301801_BART_Denoising_<b>Sequence-to-Sequence</b>...", "snippet": "Experimental results on SegNews demonstrate that our model <b>can</b> outperform several state-of-the-art <b>sequence-to-sequence</b> generation models for this new <b>task</b>. View Show abstract", "dateLastCrawled": "2021-11-30T17:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) <b>Sequence to Sequence</b> Model Performance for Education Chatbot", "url": "https://www.researchgate.net/publication/338056666_Sequence_to_Sequence_Model_Performance_for_Education_Chatbot", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/338056666_<b>Sequence_to_Sequence</b>_Model...", "snippet": "It is <b>thought</b> that the performance of the smart virtual assistant, which was revealed within the scope of the study, <b>can</b> be increased by enriching the data set in the future and it <b>can</b> provide <b>one</b> ...", "dateLastCrawled": "2022-01-28T01:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "The 7 <b>NLP</b> Techniques That Will Change How You Communicate in the Future ...", "url": "https://heartbeat.comet.ml/the-7-nlp-techniques-that-will-change-how-you-communicate-in-the-future-part-i-f0114b2f0497", "isFamilyFriendly": true, "displayUrl": "https://heartbeat.comet.ml/the-7-<b>nlp</b>-techniques-that-will-change-how-you-communicate...", "snippet": "The categorical symbols of a <b>language</b> <b>can</b> be encoded as a signal for communication in several ways: sound, gesture, writing, images, etc. human <b>language</b> is capable of being any of those. Human languages are ambiguous (unlike programming and other formal languages); thus there is a high level of complexity in representing, learning, and using linguistic / situational / contextual / word / visual knowledge towards the human <b>language</b>. Why study <b>NLP</b>? There\u2019 s a fast-growing collection of ...", "dateLastCrawled": "2022-01-23T13:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Sequence Classification with LSTM Recurrent Neural Networks in Python ...", "url": "https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/sequence-classification-", "snippet": "Sequence classification is a predictive modeling problem where you have some sequence of inputs over space or time and the <b>task</b> is to predict a category for the sequence. What makes this problem difficult is that the sequences <b>can</b> vary in length, be comprised of a very large vocabulary of input symbols and may require the model to learn the long-term", "dateLastCrawled": "2022-02-02T22:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Using <b>NLP to Summarize Human Thoughts &amp; Feelings</b> | by Tiana Cornelius ...", "url": "https://medium.com/maslo/using-nlp-to-summarize-human-thoughts-feelings-b64079030104", "isFamilyFriendly": true, "displayUrl": "https://medium.com/maslo/using-<b>nlp-to-summarize-human-thoughts-feelings</b>-b64079030104", "snippet": "<b>One</b> of the most successful and commonly-used types of NLP is machine translation, wherein machines <b>translate</b> text or speech from <b>one</b> <b>language</b> to <b>another</b>. Text summarization uses the same methods ...", "dateLastCrawled": "2021-07-16T17:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Is there a way of <b>teaching</b> a neural network using a string as input, a ...", "url": "https://www.quora.com/Is-there-a-way-of-teaching-a-neural-network-using-a-string-as-input-a-string-as-expected-output-and-to-teach-it-to-output-strings-when-given-a-string-as-input", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Is-there-a-way-of-<b>teaching</b>-a-neural-network-using-a-string-as...", "snippet": "Answer (1 of 3): Yes. Such networks are called <b>Sequence to Sequence</b> models. You already know that neural networks or any other model only understands numbers. So we have to find a way to represent these strings by numbers ( vectors to be more precise). For this, there are lot of approaches follow...", "dateLastCrawled": "2022-01-23T04:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>GitHub</b> - mayli10/<b>deep-learning</b>-<b>chatbot</b>: A deep-dive beginner&#39;s walk ...", "url": "https://github.com/mayli10/deep-learning-chatbot", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/mayli10/<b>deep-learning</b>-<b>chatbot</b>", "snippet": "A deep-dive beginner&#39;s walk-through of sentdex&#39;s tutorial for how to build a <b>chatbot</b> with <b>deep learning</b>, Tensorflow, and an NMT <b>sequence-to-sequence</b> model - <b>GitHub</b> - mayli10/<b>deep-learning</b>-<b>chatbot</b>: A deep-dive beginner&#39;s walk-through of sentdex&#39;s tutorial for how to build a <b>chatbot</b> with <b>deep learning</b>, Tensorflow, and an NMT <b>sequence-to-sequence</b> model", "dateLastCrawled": "2022-01-31T20:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How to <b>train GPT-2 to Create JSOn</b>? : LanguageTechnology", "url": "https://www.reddit.com/r/LanguageTechnology/comments/j1iaxq/how_to_train_gpt2_to_create_json/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/<b>Language</b>Technology/comments/j1iaxq/how_to_train_gpt2_to...", "snippet": "So you&#39;re describing a <b>sequence to sequence</b> (seq2seq) <b>task</b>, which is a bit tricky with GPT-2 in huggingface. Most seq2seq tasks use a encoder-decoder setup where the encoder takes your inputs and encodes it <b>into</b> an intermediate representation and a decoder decodes <b>into</b> a target <b>language</b>. I don&#39;t think there&#39;s a straightforward way (e.g. using existing training scripts) in huggingface but I could be wrong. The combiners api", "dateLastCrawled": "2022-01-22T07:20:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Google</b> AI Blog: Introducing Translatotron: An End-to-End Speech-to ...", "url": "https://ai.googleblog.com/2019/05/introducing-translatotron-end-to-end.html", "isFamilyFriendly": true, "displayUrl": "https://ai.<b>google</b>blog.com/2019/05/introducing-translatotron-end-to-end.html", "snippet": "Translatotron goes a step further by demonstrating that a single <b>sequence-to-sequence</b> model <b>can</b> directly <b>translate</b> speech from <b>one</b> <b>language</b> <b>into</b> speech in <b>another</b> <b>language</b>, without relying on an intermediate text representation in either <b>language</b>, as is required in cascaded systems. Translatotron is based on a <b>sequence-to-sequence</b> network which takes source spectrograms as input and generates spectrograms of the translated content in the target <b>language</b>. It also makes use of two other ...", "dateLastCrawled": "2022-01-29T23:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Sequence to Sequence</b> Model Performance for Education Chatbot", "url": "https://www.researchgate.net/publication/338056666_Sequence_to_Sequence_Model_Performance_for_Education_Chatbot", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/338056666_<b>Sequence_to_Sequence</b>_Model...", "snippet": "Paper \u2014 <b>S equence to Sequence</b> Model Performance for Education Cha t bot. For the models with word embedding, the input and output vocabulary size are 177. and 245 tokens respectively. As for the ...", "dateLastCrawled": "2022-01-28T01:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Seq-SG2SL: Inferring Semantic Layout from Scene Graph Through Sequence ...", "url": "https://www.arxiv-vanity.com/papers/1908.06592/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1908.06592", "snippet": "Generating semantic layout from scene graph is a crucial intermediate <b>task</b> connecting text to image. We present a conceptually simple, flexible and general framework using <b>sequence to sequence</b> (seq-to-seq) learning for this <b>task</b>. The framework, called Seq-SG2SL, derives sequence proxies for the two modality and a Transformer-based seq-to-seq model learns to transduce <b>one</b> <b>into</b> the other. A scene graph is decomposed <b>into</b> a sequence of semantic fragments (SF), <b>one</b> for each relationship. A ...", "dateLastCrawled": "2022-01-13T00:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Google AI</b> Blog: April 2017", "url": "https://ai.googleblog.com/2017/04/", "isFamilyFriendly": true, "displayUrl": "https://ai.googleblog.com/2017/04", "snippet": "In addition to machine translation, tf-seq2seq <b>can</b> also be applied to any other <b>sequence-to-sequence</b> <b>task</b> (i.e. learning to produce an output sequence given an input sequence), including machine summarization, image captioning, speech recognition, and conversational modeling.", "dateLastCrawled": "2022-01-29T07:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "The 7 <b>NLP</b> Techniques That Will Change How You Communicate in the Future ...", "url": "https://heartbeat.comet.ml/the-7-nlp-techniques-that-will-change-how-you-communicate-in-the-future-part-i-f0114b2f0497", "isFamilyFriendly": true, "displayUrl": "https://heartbeat.comet.ml/the-7-<b>nlp</b>-techniques-that-will-change-how-you-communicate...", "snippet": "<b>Sequence to Sequence</b> Learning with Neural Networks proved the effectiveness of LSTM for Neural Machine Translation. It presents a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. The method uses a multilayered LSTM to map the input sequence to a vector of a fixed dimensionality, and then <b>another</b> deep LSTM to decode the target sequence from the vector.", "dateLastCrawled": "2022-01-23T13:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "How to easily do Handwriting Recognition using Deep Learning", "url": "https://nanonets.com/blog/handwritten-character-recognition/", "isFamilyFriendly": true, "displayUrl": "https://nan<b>one</b>ts.com/blog/handwritten-character-recognition", "snippet": "This paper proposes an attention based <b>sequence-to-sequence</b> model for handwritten word recognition. The proposed architecture has three main parts: an encoder, consisting of a CNN and a bi-directional GRU, an attention mechanism devoted to focus on the pertinent features and a decoder formed by a <b>one</b>-directional GRU, able to spell the corresponding word, character by character. The encoder uses a CNN to extract visual features. A pre-trained VGG-19-BN architecture is used as a feature ...", "dateLastCrawled": "2022-02-03T01:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Sequence Classification with LSTM Recurrent Neural Networks in Python ...", "url": "https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/sequence-classification-", "snippet": "Sequence classification is a predictive modeling problem where you have some sequence of inputs over space or time and the <b>task</b> is to predict a category for the sequence. What makes this problem difficult is that the sequences <b>can</b> vary in length, be comprised of a very large vocabulary of input symbols and may require the model to learn the long-term", "dateLastCrawled": "2022-02-02T22:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Is there a way of <b>teaching</b> a neural network using a string as input, a ...", "url": "https://www.quora.com/Is-there-a-way-of-teaching-a-neural-network-using-a-string-as-input-a-string-as-expected-output-and-to-teach-it-to-output-strings-when-given-a-string-as-input", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Is-there-a-way-of-<b>teaching</b>-a-neural-network-using-a-string-as...", "snippet": "Answer (1 of 3): Yes. Such networks are called <b>Sequence to Sequence</b> models. You already know that neural networks or any other model only understands numbers. So we have to find a way to represent these strings by numbers ( vectors to be more precise). For this, there are lot of approaches follow...", "dateLastCrawled": "2022-01-23T04:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) <b>Artificial Intelligence A Modern Approach</b> (4th Edition) | Fahim ...", "url": "https://www.academia.edu/45126798/Artificial_Intelligence_A_Modern_Approach_4th_Edition_", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/45126798/<b>Artificial_Intelligence_A_Modern_Approach</b>_4th_Edition_", "snippet": "Artificial Intelligence (AI) is a big field, and this is a big book. We have tried to explore the full breadth of the field, which encompasses logic, probability, and continuous mathematics; perception, reasoning, learning, and action; fairness,", "dateLastCrawled": "2022-02-02T22:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Evolution of Language Modelling in Modern Life</b> | upGrad blog", "url": "https://www.upgrad.com/blog/nlp-natural-language-processing-real-life-applications/", "isFamilyFriendly": true, "displayUrl": "https://www.upgrad.com/blog/nlp-natural-<b>language</b>-processing-real-life-applications", "snippet": "The 1970s was the decade of creating structured real-world information <b>into</b> <b>computer</b>-understandable data, and a number of programs improved on the available technology. Notable ones included PARRY (a 1972 chatbot with emotional responses), and later, Racter (a tongue-in-cheek chatbot created in 1984) and Jabberwacky (a chatbot conceived in 1988 that aimed to simulate a human conversation in an entertaining way). The 1980s were revolutionary in natural <b>language</b> processing, when machine ...", "dateLastCrawled": "2022-01-05T01:06:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "The language of <b>proteins: NLP, machine learning &amp; protein sequences</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S2001037021000945", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2001037021000945", "snippet": "Popular deep-<b>learning</b> architectures are long short-term memory (LSTM) , <b>sequence-to-sequence</b> (seq2seq) and attention . In seq2seq models, a text is transformed using an encoder component, then a separate decoder uses the encoded representation to solve some <b>task</b> (e.g. translating between English and French). Attention models use attention layers (also called attention heads) that allow the network to concentrate on specific tokens in the text", "dateLastCrawled": "2022-02-03T02:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Is the race over for <b>Seq2Seq</b> models? | by Thushan Ganegedara | Towards ...", "url": "https://towardsdatascience.com/is-the-race-over-for-seq2seq-models-adef2b24841c", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/is-the-race-over-for-<b>seq2seq</b>-models-adef2b24841c", "snippet": "This goes for any <b>machine</b> <b>learning</b> <b>task</b>, be it <b>machine</b> translation, dependency parsing or language modelling. Self-attention layer enables to transformer to exactly do that. While processing the word \u201cits\u201d, the model can look at all the other words and decide for itself which words are important to \u201c mix \u201d into the output, so that the transformer can solve the <b>task</b> effectively.", "dateLastCrawled": "2022-02-02T02:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Introduction to Transformers in Machine Learning</b>", "url": "https://www.machinecurve.com/index.php/2020/12/28/introduction-to-transformers-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/.../12/28/<b>introduction-to-transformers-in-machine-learning</b>", "snippet": "When you talk about <b>Machine</b> <b>Learning</b> in Natural Language Processing these days, all you hear is one thing \u2013 Transformers. Models based on this Deep <b>Learning</b> architecture have taken the NLP world by storm since 2017. In fact, they are the go-to approach today, and many of the approaches build on top of the original Transformer, one way or another. Transformers are however not simple. The original Transformer architecture is quite complex and the same is true for many of the spin-off ...", "dateLastCrawled": "2022-02-03T03:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "9.7. <b>Sequence to Sequence</b> <b>Learning</b> \u2014 Dive into Deep <b>Learning</b> 0.17.2 ...", "url": "https://d2l.ai/chapter_recurrent-modern/seq2seq.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_recurrent-modern/seq2seq.html", "snippet": "As we have seen in Section 9.5, in <b>machine</b> translation both the input and output are a variable-length <b>sequence</b>.To address this type of problem, we have designed a general encoder-decoder architecture in Section 9.6.In this section, we will use two RNNs to design the encoder and the decoder of this architecture and apply it to <b>sequence to sequence</b> <b>learning</b> for <b>machine</b> translation [Sutskever et al., 2014] [Cho et al., 2014b].. Following the design principle of the encoder-decoder architecture ...", "dateLastCrawled": "2022-01-26T00:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Geometric deep <b>learning</b> on molecular representations | Nature <b>Machine</b> ...", "url": "https://www.nature.com/articles/s42256-021-00418-8", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s42256-021-00418-8", "snippet": "In <b>analogy</b> to some popular pre-deep <b>learning</b> ... which can be cast as a <b>sequence-to-sequence</b> translation <b>task</b> in which the string representations of the reactants are mapped to those of the ...", "dateLastCrawled": "2022-01-29T04:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Machine Learning Gist</b> \u00b7 GitHub", "url": "https://gist.github.com/sgoyal1012/b30d70d12b6efad88bb285e8e709b161", "isFamilyFriendly": true, "displayUrl": "https://gist.github.com/sgoyal1012/b30d70d12b6efad88bb285e8e709b161", "snippet": "Week 3: <b>Sequence to sequence</b> architectures. <b>Sequence to sequence</b> models Language translation for example; Image captioning, caption an image; Picking the most likely model <b>Machine</b> Transation Model Split into a model encoding the sentence; and then a language model. Calculate the probability of an English sentence conditioned on a French sentence.", "dateLastCrawled": "2022-01-29T03:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Benefits of AI and Deep <b>Learning</b> - <b>Machine</b> <b>Learning</b> Company ...", "url": "https://www.folio3.ai/blog/advantages-of-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://www.folio3.ai/blog/<b>advantages-of-neural-networks</b>", "snippet": "<b>Sequence-To-Sequence</b> models are mainly applied in question answering, <b>machine</b> translations systems, and chatbots. What Are The <b>Advantages of Neural Networks</b> . There are various <b>advantages of neural networks</b>, some of which are discussed below: 1) Store information on the entire network. Just like it happens in traditional programming where information is stored on the network and not on a database. If a few pieces of information disappear from one place, it does not stop the whole network ...", "dateLastCrawled": "2022-02-02T09:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A Gentle Introduction to <b>Long Short-Term Memory</b> Networks by the Experts", "url": "https://machinelearningmastery.com/gentle-introduction-long-short-term-memory-networks-experts/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/gentle-introduction-<b>long-short-term-memory</b>-networks...", "snippet": "<b>Long Short-Term Memory</b> (LSTM) networks are a type of recurrent neural network capable of <b>learning</b> order dependence in sequence prediction problems. This is a behavior required in complex problem domains like <b>machine</b> translation, speech recognition, and more. LSTMs are a complex area of deep <b>learning</b>. It can be hard to get your hands around what LSTMs are, and how terms like bidirectional", "dateLastCrawled": "2022-01-31T03:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Sequence Classification with LSTM Recurrent Neural Networks in Python ...", "url": "https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/sequence-classification-", "snippet": "Sequence Classification with LSTM Recurrent Neural Networks in Python with Keras. Sequence classification is a predictive modeling problem where you have some sequence of inputs over space or time and the <b>task</b> is to predict a category for the sequence. What makes this problem difficult is that the sequences can vary in length, be comprised of a ...", "dateLastCrawled": "2022-02-02T22:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "deep-<b>learning</b>-coursera/Week 1 Quiz - Introduction to deep <b>learning</b>.md ...", "url": "https://github.com/Kulbear/deep-learning-coursera/blob/master/Neural%20Networks%20and%20Deep%20Learning/Week%201%20Quiz%20-%20Introduction%20to%20deep%20learning.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/Kulbear/deep-<b>learning</b>-coursera/blob/master/Neural Networks and Deep...", "snippet": "Week 1 Quiz - Introduction to deep <b>learning</b>. What does the <b>analogy</b> \u201cAI is the new electricity\u201d refer to? AI is powering personal devices in our homes and offices, similar to electricity. Through the \u201csmart grid\u201d, AI is delivering a new wave of electricity. AI runs on computers and is thus powered by electricity, but it is letting computers do things not possible before. Similar to electricity starting about 100 years ago, AI is transforming multiple industries. Note: Andrew ...", "dateLastCrawled": "2022-02-03T05:16:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(sequence-to-sequence task)  is like +(teaching a computer how to translate one language into another)", "+(sequence-to-sequence task) is similar to +(teaching a computer how to translate one language into another)", "+(sequence-to-sequence task) can be thought of as +(teaching a computer how to translate one language into another)", "+(sequence-to-sequence task) can be compared to +(teaching a computer how to translate one language into another)", "machine learning +(sequence-to-sequence task AND analogy)", "machine learning +(\"sequence-to-sequence task is like\")", "machine learning +(\"sequence-to-sequence task is similar\")", "machine learning +(\"just as sequence-to-sequence task\")", "machine learning +(\"sequence-to-sequence task can be thought of as\")", "machine learning +(\"sequence-to-sequence task can be compared to\")"]}