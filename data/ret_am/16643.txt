{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "How to Implement <b>Wasserstein</b> <b>Loss</b> for Generative Adversarial Networks", "url": "https://machinelearningmastery.com/how-to-implement-wasserstein-loss-for-generative-adversarial-networks/", "isFamilyFriendly": true, "displayUrl": "https://machine<b>learning</b>mastery.com/how-to-implement-<b>wasserstein</b>-<b>loss</b>-for-generative...", "snippet": "The <b>Wasserstein</b> Generative Adversarial Network, or <b>Wasserstein</b> GAN, is an extension to the generative adversarial network that both improves the stability when training the model and provides a <b>loss</b> function that correlates with the quality of generated images. It is an important extension to the GAN model and requires a conceptual shift away from a discriminator that predicts the probability of", "dateLastCrawled": "2022-02-02T22:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Wasserstein Distance</b> and Textual Similarity - neptune.ai", "url": "https://neptune.ai/blog/wasserstein-distance-and-textual-similarity", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/<b>wasserstein-distance</b>-and-textual-similarity", "snippet": "<b>Wasserstein Distance</b> and Textual Similarity. In many machine <b>learning</b> (ML) projects, there comes a point when we have to decide the level of similarity between different objects of interest. We might be trying to understand the similarity between different images, weather patterns, or probability distributions.", "dateLastCrawled": "2022-01-19T11:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>Learning with a Wasserstein Loss</b>", "url": "https://www.researchgate.net/publication/278733698_Learning_with_a_Wasserstein_Loss", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/278733698_<b>Learning_with_a_Wasserstein_Loss</b>", "snippet": "Decomposable <b>loss</b> functions <b>like</b> KL Divergence and ` p ... described a statistical <b>learning</b> bound for the <b>loss</b>. The <b>Wasserstein</b> <b>loss</b> can encourage smoothness . of the predictions with respect to a ...", "dateLastCrawled": "2021-09-30T06:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "ALLWAS: Active <b>Learning</b> on <b>Language</b> models in <b>WASserstein</b> space | DeepAI", "url": "https://deepai.org/publication/allwas-active-learning-on-language-models-in-wasserstein-space", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/.../allwas-active-<b>learning</b>-on-<b>language</b>-models-in-<b>wasserstein</b>-space", "snippet": "Active <b>learning</b> has emerged as a standard paradigm in areas with scarcity of labeled training data, such as in the medical domain. <b>Language</b> models have emerged as the prevalent choice of several natural <b>language</b> tasks due to the performance boost offered by these models. However, in several domains, such as medicine, the scarcity of labeled training data is a common issue. Also, these models may not work well in cases where class imbalance is prevalent.", "dateLastCrawled": "2022-01-26T19:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Wasserstein</b> <b>loss</b> layer/criterion - PyTorch Forums", "url": "https://discuss.pytorch.org/t/wasserstein-loss-layer-criterion/1275?page=2", "isFamilyFriendly": true, "displayUrl": "https://discuss.pytorch.org/t/<b>wasserstein</b>-<b>loss</b>-layer-criterion/1275?page=2", "snippet": "Hi @AjayTalati,. thanks you for the kind words. I do use some things at work, but mainly I do Machine <b>Learning</b> as a hobby. Regarding Improved Training of <b>Wasserstein</b> GAN, I have implemented the toy examples of the article with pytorch in a Jupyter notebook. I also included a novel (to me) method that I call Semi-Improved Training of <b>Wasserstein</b> GAN that checks the Lipschitz term directly for a pair (or a pair and an interpolate) of points instead of referring to the gradient. In my limited ...", "dateLastCrawled": "2022-01-19T13:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "How to Develop a <b>Wasserstein Generative Adversarial Network</b> (WGAN) From ...", "url": "https://machinelearningmastery.com/how-to-code-a-wasserstein-generative-adversarial-network-wgan-from-scratch/", "isFamilyFriendly": true, "displayUrl": "https://machine<b>learning</b>mastery.com/how-to-code-a-<b>wasserstein</b>-generative-adversarial...", "snippet": "The <b>Wasserstein Generative Adversarial Network</b>, or <b>Wasserstein</b> GAN, is an extension to the generative adversarial network that both improves the stability when training the model and provides a <b>loss</b> function that correlates with the quality of generated images. The development of the WGAN has a dense mathematical motivation, although in practice requires only a few minor modifications to the established standard", "dateLastCrawled": "2022-01-31T23:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "What is <b>Wasserstein</b> Gan? - Quora", "url": "https://www.quora.com/What-is-Wasserstein-Gan", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>Wasserstein</b>-Gan", "snippet": "Answer: <b>Wasserstein</b> GAN is intended to improve GANs\u2019 training by adopting a smooth metric for measuring the distance between two probability distributions. It is an important extension to the GAN model and requires a conceptual shift away from a discriminator that predicts the probability of a g...", "dateLastCrawled": "2022-01-05T18:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "GANs in computer vision - Improved training with <b>Wasserstein</b> distance ...", "url": "https://theaisummer.com/gan-computer-vision-incremental-training/", "isFamilyFriendly": true, "displayUrl": "https://the<b>aisummer</b>.com/gan-computer-vision-incremental-training", "snippet": "\ud83d\udcd6 You can now grab a copy of our <b>new</b> Deep <b>Learning</b> in Production Book \ud83d\udcd6 . Learn more. GANs in computer vision - Improved training with <b>Wasserstein</b> distance, game theory control and progressively growing schemes. Nikolas Adaloglou on 2020-04-22 \u00b7 12 mins. Generative Adversarial Networks Generative <b>Learning</b> Computer Vision. SIMILAR ARTICLES. Generative Adversarial Networks. Decrypt Generative Adversarial Networks (GAN) GANs in computer vision - Introduction to generative <b>learning</b>. GANs ...", "dateLastCrawled": "2022-02-03T19:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) Speech Enhancement Based on <b>A New</b> Architecture of <b>Wasserstein</b> ...", "url": "https://www.researchgate.net/publication/333228504_Speech_Enhancement_Based_on_A_New_Architecture_of_Wasserstein_Generative_Adversarial_Networks", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/333228504_Speech_Enhancement_Based_on_<b>A_New</b>...", "snippet": "In the paper, we propose a speech enhancement method. based on <b>a new</b> <b>architecture of Wasserstein generative. adversarial</b> network, whose generator uses the FCNNs [13] and. discriminator is ...", "dateLastCrawled": "2022-02-03T08:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How <b>is the Wasserstein metric used in machine learning</b>? - Quora", "url": "https://www.quora.com/How-is-the-Wasserstein-metric-used-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/How-<b>is-the-Wasserstein-metric-used-in-machine-learning</b>", "snippet": "Answer: Great opportunity for me to reuse bits from a recent course report of mine ;) Optimal transport and the associated <b>Wasserstein</b> metric have been applied in formulating solutions to numerous machine <b>learning</b> problems such as <b>learning</b> document distances [12,13], MCMC-free sampling from Baye...", "dateLastCrawled": "2022-01-22T15:58:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "ALLWAS: Active <b>Learning</b> on <b>Language</b> models in <b>WASserstein</b> space | DeepAI", "url": "https://deepai.org/publication/allwas-active-learning-on-language-models-in-wasserstein-space", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/.../allwas-active-<b>learning</b>-on-<b>language</b>-models-in-<b>wasserstein</b>-space", "snippet": "Active <b>learning</b> has emerged as a standard paradigm in areas with scarcity of labeled training data, such as in the medical domain. <b>Language</b> models have emerged as the prevalent choice of several natural <b>language</b> tasks due to the performance boost offered by these models. However, in several domains, such as medicine, the scarcity of labeled training data is a common issue. Also, these models may not work well in cases where class imbalance is prevalent.", "dateLastCrawled": "2022-01-26T19:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Wasserstein of Wasserstein Loss for Learning Generative Models</b>", "url": "http://proceedings.mlr.press/v97/dukler19a/dukler19a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v97/dukler19a/dukler19a.pdf", "snippet": "<b>Wasserstein of Wasserstein Loss for Learning Generative Models</b> ... <b>similar</b>. In contrast, the Euclidean distance is highly sensitive and oftentimes the nearest neighbors are predominantly white images. metric exhibits a metric tensor structure (Otto,2001;Villani, 2009). This introduces a Lipschitz condition based on the <b>Wasserstein</b> norm, rather than the L2 norm of the standard WGAN setting. In this work we focus on generative models for images and speci\ufb01cally the WGAN formulation, but the ...", "dateLastCrawled": "2022-01-24T10:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>Learning with a Wasserstein Loss</b> - ResearchGate", "url": "https://www.researchgate.net/publication/278733698_Learning_with_a_Wasserstein_Loss", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/278733698_<b>Learning_with_a_Wasserstein_Loss</b>", "snippet": "7 The <b>Wasserstein</b> <b>loss</b> can achie ve a <b>similar</b> trade-off by choosing the metric parameter p, as discussed in Section 6.1. However, the relationship between p and the smoothing behavior is complex ...", "dateLastCrawled": "2021-09-30T06:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Wasserstein Distance</b> and Textual Similarity - neptune.ai", "url": "https://neptune.ai/blog/wasserstein-distance-and-textual-similarity", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/<b>wasserstein-distance</b>-and-textual-<b>similar</b>ity", "snippet": "The <b>Wasserstein distance</b> and moving dirt! We have two distributions, one representing a series of fair coin tosses, and the other a series of tosses with a bias coin. We want to understand how <b>similar</b> they are to each other. Now, we need a method to measure the distance necessary to move all the points on one graph to the other. You can also ...", "dateLastCrawled": "2022-01-19T11:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) <b>Wasserstein</b> of <b>Wasserstein</b> <b>Loss</b> for <b>Learning</b> Generative Models", "url": "https://www.researchgate.net/publication/330699954_Wasserstein_of_Wasserstein_Loss_for_Learning_Generative_Models", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/330699954_<b>Wasserstein</b>_of_<b>Wasserstein</b>_<b>Loss</b>_for...", "snippet": "<b>W asserstein</b> of <b>Wasserstein</b> <b>loss</b> function can be applied <b>to. learning</b> with other types of models or other types of data for. which a natural distance between features can be introduced. This paper ...", "dateLastCrawled": "2022-01-28T02:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Learning</b> with <b>minibatch</b> <b>Wasserstein</b> | by Kilian Fatras | Towards Data ...", "url": "https://towardsdatascience.com/learning-with-minibatch-wasserstein-d87dcf52efb5", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>learning</b>-with-<b>minibatch</b>-<b>wasserstein</b>-d87dcf52efb5", "snippet": "We see a <b>similar</b> effect between the <b>minibatch</b> <b>Wasserstein</b> distance and the regularized <b>Wasserstein</b> variants. We get non optimal connections between samples. For the <b>minibatch</b> <b>Wasserstein</b> distance, the number of connections increases when m decreases. It <b>is similar</b> to the entropic OT variant when the regularization coefficient gets bigger. One can also note that the highest intensity of connections decreases when the batch size decrease, which is due to the constraints. Now that we saw the ...", "dateLastCrawled": "2022-02-02T06:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "GANs in computer vision - Improved training with <b>Wasserstein</b> distance ...", "url": "https://theaisummer.com/gan-computer-vision-incremental-training/", "isFamilyFriendly": true, "displayUrl": "https://the<b>aisummer</b>.com/gan-computer-vision-incremental-training", "snippet": "\ud83d\udcd6 You can now grab a copy of our <b>new</b> Deep <b>Learning</b> in Production Book \ud83d\udcd6 . Learn more. GANs in computer vision - Improved training with <b>Wasserstein</b> distance, game theory control and progressively growing schemes. Nikolas Adaloglou on 2020-04-22 \u00b7 12 mins. Generative Adversarial Networks Generative <b>Learning</b> Computer Vision. <b>SIMILAR</b> ARTICLES. Generative Adversarial Networks. Decrypt Generative Adversarial Networks (GAN) GANs in computer vision - Introduction to generative <b>learning</b>. GANs ...", "dateLastCrawled": "2022-02-03T19:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Semi-Supervised Pairing via Basis-Sharing <b>Wasserstein</b> Matching Auto-Encoder", "url": "http://bayesiandeeplearning.org/2018/papers/123.pdf", "isFamilyFriendly": true, "displayUrl": "bayesiandeep<b>learning</b>.org/2018/papers/123.pdf", "snippet": "A <b>similar</b> idea that uses a different method has been shown to work on some <b>language</b> tasks [3]; our work explores <b>a new</b> approach based on distribution matching. Through preliminary experiments, we show that the proposed algorithm can successfully incorporate the unlabeled data for improving the classi\ufb01cation accuracy on MNIST and CIFAR10 datasets. 2 Proposed Method 2.1 <b>Wasserstein</b> Training of Cross-Domain Auto-encoders We denote unlabeled training set as D UL ={{x i}n i=1;{y j} m j=1}, on ...", "dateLastCrawled": "2021-10-16T23:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "[D] <b>Is &quot;Wasserstein metric&quot; the right name</b> to use? : MachineLearning", "url": "https://www.reddit.com/r/MachineLearning/comments/cwrly5/d_is_wasserstein_metric_the_right_name_to_use/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/Machine<b>Learning</b>/comments/cwrly5/d_is_<b>wasserstein</b>_metric_the...", "snippet": "According to wiki: &quot; The name &quot;<b>Wasserstein</b> distance&quot; was coined by R. L. Dobrushin in 1970, after the Russian mathematician Leonid Vaser\u0161te\u012dn who introduced the concept in 1969. And indeed, I found the paper written in Russian by Dobrushin, which mentioned in reference:&quot; \u041b..\u041d. \u0412\u0430\u0441\u0435\u0440\u0448\u0442\u0435\u0439\u043d, \u041c\u0430\u0440\u043a\u043e\u0432\u0441\u043a\u0438\u0435 \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u044b \u043d\u0430 \u0441\u0447\u0435\u0442\u043d\u043e\u043c \u043f\u0440\u043e\u0438\u0437\u0432\u0435\u0434\u0435\u043d\u0438\u0438 \u043f\u0440\u043e\u0441\u0442\u00ad\u0440\u0430\u043d\u0441\u0442\u0432, \u043e\u043f\u0438\u0441\u044b\u0432\u0430\u044e\u0449\u0438\u0435 \u0431\u043e\u043b\u044c\u0448\u0438\u0435 \u0441\u0438\u0441\u0442\u0435\u043c\u044b \u0430\u0432\u0442\u043e\u043c\u0430\u0442\u043e\u0432.", "dateLastCrawled": "2021-08-12T05:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "A <b>Principled Approach for Learning Task Similarity in Multitask Learning</b>", "url": "https://www.ijcai.org/Proceedings/2019/0478.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.ijcai.org/Proceedings/2019/0478.pdf", "snippet": "A <b>Principled Approach for Learning Task Similarity in Multitask Learning</b> ... imize a weighted sum of empirical <b>loss</b> in which <b>similar</b> tasks are assigned higher weights. These approaches explic-itly estimate the task similarities through a linear model. Since these approaches are estimated in the original input space, it is difcult to handle thecovariate shiftproblem. Therefore, many neural network based approaches started to explore tasks similarities implicitly:[Liu et al., 2017; Li et al ...", "dateLastCrawled": "2022-01-20T23:59:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "How to Implement <b>Wasserstein</b> <b>Loss</b> for Generative Adversarial Networks", "url": "https://machinelearningmastery.com/how-to-implement-wasserstein-loss-for-generative-adversarial-networks/", "isFamilyFriendly": true, "displayUrl": "https://machine<b>learning</b>mastery.com/how-to-implement-<b>wasserstein</b>-<b>loss</b>-for-generative...", "snippet": "The <b>Wasserstein</b> Generative Adversarial Network, or <b>Wasserstein</b> GAN, is an extension to the generative adversarial network that both improves the stability when training the model and provides a <b>loss</b> function that correlates with the quality of generated images. It is an important extension to the GAN model and requires a conceptual shift away from a discriminator that predicts the probability of", "dateLastCrawled": "2022-02-02T22:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "How to Develop a <b>Wasserstein Generative Adversarial Network</b> (WGAN) From ...", "url": "https://machinelearningmastery.com/how-to-code-a-wasserstein-generative-adversarial-network-wgan-from-scratch/", "isFamilyFriendly": true, "displayUrl": "https://machine<b>learning</b>mastery.com/how-to-code-a-<b>wasserstein</b>-generative-adversarial...", "snippet": "The <b>Wasserstein Generative Adversarial Network</b>, or <b>Wasserstein</b> GAN, is an extension to the generative adversarial network that both improves the stability when training the model and provides a <b>loss</b> function that correlates with the quality of generated images. The development of the WGAN has a dense mathematical motivation, although in practice requires only a few minor modifications to the established standard", "dateLastCrawled": "2022-01-31T23:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "On a Novel Application of <b>Wasserstein</b>-Procrustes for UnsupervisedCross ...", "url": "https://www.researchgate.net/profile/Preslav-Nakov/publication/343095709_On_a_Novel_Application_of_Wasserstein-Procrustes_for_Unsupervised_Cross-Lingual_Learning/links/5f2d4f28299bf13404ac00f1/On-a-Novel-Application-of-Wasserstein-Procrustes-for-Unsupervised-Cross-Lingual-Learning.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/profile/Preslav-Nakov/publication/343095709_On_a_Novel...", "snippet": "arXiv:2007.09456v1 [cs.CL] 18 Jul 2020 On a Novel Application of <b>Wasserstein</b>-Procrustes for UnsupervisedCross-Lingual <b>Learning</b> Guillem Ram\u00edrez\u2217 MIT", "dateLastCrawled": "2021-09-19T17:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Wasserstein</b> <b>loss</b> layer/criterion - PyTorch Forums", "url": "https://discuss.pytorch.org/t/wasserstein-loss-layer-criterion/1275?page=2", "isFamilyFriendly": true, "displayUrl": "https://discuss.pytorch.org/t/<b>wasserstein</b>-<b>loss</b>-layer-criterion/1275?page=2", "snippet": "Hi @AjayTalati,. thanks you for the kind words. I do use some things at work, but mainly I do Machine <b>Learning</b> as a hobby. Regarding Improved Training of <b>Wasserstein</b> GAN, I have implemented the toy examples of the article with pytorch in a Jupyter notebook. I also included a novel (to me) method that I call Semi-Improved Training of <b>Wasserstein</b> GAN that checks the Lipschitz term directly for a pair (or a pair and an interpolate) of points instead of referring to the gradient. In my limited ...", "dateLastCrawled": "2022-01-19T13:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Probabilistic Natural <b>Language</b> Generation with <b>Wasserstein</b> ... - DeepAI", "url": "https://deepai.org/publication/probabilistic-natural-language-generation-with-wasserstein-autoencoders", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/probabilistic-natural-<b>language</b>-generation-with...", "snippet": "Probabilistic Natural <b>Language</b> Generation with <b>Wasserstein</b> Autoencoders. Probabilistic generation of natural <b>language</b> sentences is an important task in NLP. Existing models such as variational autoencoders (VAE) for sequence generation are extremely difficult to train due to the issues associated with the Kullback-Leibler (KL) <b>loss</b> collapsing ...", "dateLastCrawled": "2021-12-14T09:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Set Representation <b>Learning</b> with Generalized Sliced-<b>Wasserstein</b> ...", "url": "https://deepai.org/publication/set-representation-learning-with-generalized-sliced-wasserstein-embeddings", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/set-representation-<b>learning</b>-with-generalized-sliced...", "snippet": "An increasing number of machine <b>learning</b> tasks deal with <b>learning</b> representations from set-structured data. Solutions to these problems involve the composition of permutation-equivariant modules (e.g., self-attention, or individual processing via feed-forward neural networks) and permutation-invariant modules (e.g., global average pooling, or pooling by multi-head attention).In this paper, we propose a geometrically-interpretable framework for <b>learning</b> representations from set-structured ...", "dateLastCrawled": "2022-01-29T12:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) Speech Enhancement Based on <b>A New</b> Architecture of <b>Wasserstein</b> ...", "url": "https://www.researchgate.net/publication/333228504_Speech_Enhancement_Based_on_A_New_Architecture_of_Wasserstein_Generative_Adversarial_Networks", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/333228504_Speech_Enhancement_Based_on_<b>A_New</b>...", "snippet": "In the paper, we propose a speech enhancement method. based on <b>a new</b> <b>architecture of Wasserstein generative. adversarial</b> network, whose generator uses the FCNNs [13] and. discriminator is ...", "dateLastCrawled": "2022-02-03T08:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Calculate Value-at-<b>Risk Using Wasserstein Generative Adversarial</b> ...", "url": "https://chatbotslife.com/calculate-value-at-risk-using-wasserstein-generative-adversarial-networks-wgan-gp-for-risk-2b1d320fde59", "isFamilyFriendly": true, "displayUrl": "https://chatbotslife.com/calculate-value-at-risk-using-<b>wasserstein</b>-generative...", "snippet": "It turns out that this <b>can</b> be done with the <b>Wasserstein</b> distance between the generator distribution and the data distribution. This is the WGAN discriminator\u2019s <b>loss</b> function: disc_<b>loss</b>_base = -tf.reduce_mean(x_out) + tf.reduce_mean(z_out) As the discriminator learns to correctly identify the training data as real, x_out should increase. This ...", "dateLastCrawled": "2022-01-31T02:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Learning semantic similarity in a continuous space</b>", "url": "https://papers.nips.cc/paper/2018/file/97e8527feaf77a97fc38f34216141515-Paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://papers.nips.cc/paper/2018/file/97e8527feaf77a97fc38f34216141515-Paper.pdf", "snippet": "The model <b>can</b> then be used to encode <b>new</b> sentences into vector representations. SDAEs are the top performer on paraphrase identi\ufb01cation, among unsupervised model [30]. Another approach is Skip <b>Thought</b> (ST) vectors [32] which adapt the skip-gram model for words to the sentence level, by encoding a sentence to predict the sentences around it. Consequently, ST vectors require a consequent training corpus of ordered sentences, with a coherent narrative. On the SICK sentence relatedness ...", "dateLastCrawled": "2021-09-19T16:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>GitHub</b> - <b>ivallesp/scriptGAN</b>: A natural <b>language</b> generation system using ...", "url": "https://github.com/ivallesp/scriptGAN", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/ivallesp/scriptGAN", "snippet": "The repository is totally open for contribution. As this is a log of a set of experiments, the contributions should be done so that they do not destroy the current branches. <b>New</b> trials should be performed in <b>new</b> branches, although bug fixes in the current branches are also totally accepted. License. This project has been licensed under MIT ...", "dateLastCrawled": "2022-02-03T07:44:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "ALLWAS: Active <b>Learning</b> on <b>Language</b> models in <b>WASserstein</b> space | DeepAI", "url": "https://deepai.org/publication/allwas-active-learning-on-language-models-in-wasserstein-space", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/.../allwas-active-<b>learning</b>-on-<b>language</b>-models-in-<b>wasserstein</b>-space", "snippet": "Active <b>learning</b> has emerged as a standard paradigm in areas with scarcity of labeled training data, such as in the medical domain. <b>Language</b> models have emerged as the prevalent choice of several natural <b>language</b> tasks due to the performance boost offered by these models. However, in several domains, such as medicine, the scarcity of labeled training data is a common issue. Also, these models may not work well in cases where class imbalance is prevalent.", "dateLastCrawled": "2022-01-26T19:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Learning with a Wasserstein Loss</b>", "url": "https://www.researchgate.net/publication/278733698_Learning_with_a_Wasserstein_Loss", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/278733698_<b>Learning_with_a_Wasserstein_Loss</b>", "snippet": "<b>Learning</b> to predict multi-label outputs is challenging, but in many problems there is a natural metric on the outputs that <b>can</b> be used to improve predictions. In this paper we develop a <b>loss</b> ...", "dateLastCrawled": "2021-09-30T06:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Wasserstein Distance</b> and Textual Similarity - neptune.ai", "url": "https://neptune.ai/blog/wasserstein-distance-and-textual-similarity", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/<b>wasserstein-distance</b>-and-textual-similarity", "snippet": "Blog \u00bb Natural <b>Language</b> Processing \u00bb <b>Wasserstein Distance</b> and Textual Similarity. Working on an NLP project? You may be spending too much time documenting it. Adding a metadata store to your workflow <b>can</b> change this. See example dashboard <b>Wasserstein Distance</b> and Textual Similarity. mins read; Author Cathal Horan ; Updated November 29th, 2021; In many machine <b>learning</b> (ML) projects, there comes a point when we have to decide the level of similarity between different objects of interest. We ...", "dateLastCrawled": "2022-01-19T11:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Gaussian Word Embedding with a <b>Wasserstein</b> Distance <b>Loss</b> | DeepAI", "url": "https://deepai.org/publication/gaussian-word-embedding-with-a-wasserstein-distance-loss", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/gaussian-word-embedding-with-a-<b>wasserstein</b>-distance-<b>loss</b>", "snippet": "For example, <b>compared</b> to cosine similarity and Euclidean distance represented by the points, using the distribution-based representation <b>can</b> extend the definition of indicators that measure the similarity of two words, such as KL divergence, <b>Wasserstein</b> distance, etc. However, vilnis2014word vilnis2014word use a <b>loss</b> function based on KL divergence. KL divergence is ill-defined for two very similar distributions, and it is not sensitive to the distance between two distant distributions. It ...", "dateLastCrawled": "2021-12-21T19:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A Simulated Annealing Based Inexact Oracle for <b>Wasserstein</b> <b>Loss</b> ...", "url": "http://proceedings.mlr.press/v70/ye17b/ye17b.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v70/ye17b/ye17b.pdf", "snippet": "a sequence of <b>Wasserstein</b> losses. Our <b>new</b> ap-proach has the advantages of numerical stability and readiness for warm starts. These character- istics are valuable for WLM problems that of-ten require multiple levels of iterations in which the oracle for computing the value and gradient of a <b>loss</b> function is embedded. We applied the method to optimal transport with Coulomb cost and the <b>Wasserstein</b> non-negative matrix factor-ization problem, and made comparisons with the existing method of ...", "dateLastCrawled": "2022-01-29T17:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Learning</b> with <b>minibatch</b> <b>Wasserstein</b> | by Kilian Fatras | Towards Data ...", "url": "https://towardsdatascience.com/learning-with-minibatch-wasserstein-d87dcf52efb5", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>learning</b>-with-<b>minibatch</b>-<b>wasserstein</b>-d87dcf52efb5", "snippet": "As we have <b>a new</b> <b>loss</b> function, it is necessary to review its strengths and weaknesses to compare probability distributions. It has the following properties: For iid data, U and \u0168 are unbiased estimator of Eq.(2) U and \u0168 are symmetric in their arguments; U and \u0168 are strictly positive; U(\u03b1,\u03b1) and \u0168(\u03b1,\u03b1) are strictly positive; The interesting property here is the last one. For non trivial measures, we break the separability distance axiom. Hence, the <b>minibatch</b> <b>Wasserstein</b> distance IS ...", "dateLastCrawled": "2022-02-02T06:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Two-Sided <b>Wasserstein</b> Procrustes Analysis", "url": "https://www.ijcai.org/proceedings/2021/0484.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.ijcai.org/proceedings/2021/0484.pdf", "snippet": "Two-sided <b>Wasserstein</b> Procrustes Analysis Kun Jin 1, Chaoyue Liu , Cathy Xia2 1Department of Computer Science and Engineering, The Ohio State University 2Department of Integrated Systems Engineering, The Ohio State University Abstract <b>Learning</b> correspondence between sets of objects is a key component in many machine <b>learning</b> tasks. Recently, optimal Transport (OT) has been success-fully applied to such correspondence problems and it is appealing as a fully unsupervised approach. However, OT ...", "dateLastCrawled": "2022-01-22T13:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Automatic Text Evaluation through the Lens of <b>Wasserstein</b> Barycenters", "url": "https://aclanthology.org/2021.emnlp-main.817.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/2021.emnlp-main.817.pdf", "snippet": "<b>learning</b> phase of models by deriving losses that are better surrogate of human judgment than the widely used cross-entropy <b>loss</b> (Clark et al.,2019). A plethora of automatic metrics has been intro-duced these last few years and may be grouped into two general classes: trained (Ma et al.,2017;Shi- manaka et al.,2018;Lowe et al.,2016;Lita et al., 2005) and untrained metrics (Doddington,2002; Popovic\u00b4,2015). In this paper, we mainly focus on untrained metrics that <b>can</b> be further split into ...", "dateLastCrawled": "2022-02-03T12:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Wasserstein</b> <b>loss</b> layer/criterion - PyTorch Forums", "url": "https://discuss.pytorch.org/t/wasserstein-loss-layer-criterion/1275?page=2", "isFamilyFriendly": true, "displayUrl": "https://discuss.pytorch.org/t/<b>wasserstein</b>-<b>loss</b>-layer-criterion/1275?page=2", "snippet": "Hi @AjayTalati,. thanks you for the kind words. I do use some things at work, but mainly I do Machine <b>Learning</b> as a hobby. Regarding Improved Training of <b>Wasserstein</b> GAN, I have implemented the toy examples of the article with pytorch in a Jupyter notebook. I also included a novel (to me) method that I call Semi-Improved Training of <b>Wasserstein</b> GAN that checks the Lipschitz term directly for a pair (or a pair and an interpolate) of points instead of referring to the gradient. In my limited ...", "dateLastCrawled": "2022-01-19T13:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What is <b>Wasserstein</b> Gan? - Quora", "url": "https://www.quora.com/What-is-Wasserstein-Gan", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>Wasserstein</b>-Gan", "snippet": "Answer: <b>Wasserstein</b> GAN is intended to improve GANs\u2019 training by adopting a smooth metric for measuring the distance between two probability distributions. It is an important extension to the GAN model and requires a conceptual shift away from a discriminator that predicts the probability of a g...", "dateLastCrawled": "2022-01-05T18:17:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "How to stabilize GAN training. Understand <b>Wasserstein</b> distance and ...", "url": "https://towardsdatascience.com/wasserstein-distance-gan-began-and-progressively-growing-gan-7e099f38da96", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>wasserstein</b>-distance-gan-began-and-progressively...", "snippet": "<b>Wasserstein</b> <b>loss</b> leads to a higher quality of the gradients to train G. ... Finally, one intuitive way to understand this paper is to make an <b>analogy</b> with the gradients on the history of in-layer activation functions. Specifically, the gradients of sigmoid and tanh activations that disappeared in favor of ReLUs, because of the improved gradients in the whole range of values. BEGAN (Boundary Equilibrium Generative Adversarial Networks 2017) We often see that the discriminator progresses too ...", "dateLastCrawled": "2022-01-25T18:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Learning Wasserstein Embeddings</b> | DeepAI", "url": "https://deepai.org/publication/learning-wasserstein-embeddings", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>learning-wasserstein-embeddings</b>", "snippet": "The <b>Wasserstein</b> distance received a lot of attention recently in the community of <b>machine</b> <b>learning</b>, especially for its principled way of comparing distributions. It has found numerous applications in several hard problems, such as domain adaptation, dimensionality reduction or generative models. However, its use is still limited by a heavy ...", "dateLastCrawled": "2022-01-05T07:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>Learning</b> <b>Wasserstein</b> Embeddings - ResearchGate", "url": "https://www.researchgate.net/publication/320564581_Learning_Wasserstein_Embeddings", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/320564581_<b>Learning</b>_<b>Wasserstein</b>_Embeddings", "snippet": "Designed through an <b>analogy</b> with ... Fast dictionary <b>learning</b> with a smoothed <b>wasserstein</b> <b>loss</b>. In AISTA TS, pages 630\u2013638, 2016. [32] F. Santambrogio. Introduction to optimal transport theory ...", "dateLastCrawled": "2021-12-13T04:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Wasserstein</b> GANs \u2013 Emma Benjaminson \u2013 Mechanical Engineering Graduate ...", "url": "https://sassafras13.github.io/Wasserstein/", "isFamilyFriendly": true, "displayUrl": "https://sassafras13.github.io/<b>Wasserstein</b>", "snippet": "Welcome back to the blog. Today we are (still) talking about MolGAN, this time with a focus on the <b>loss</b> function used to train the entire architecture. De Cao and Kipf use a <b>Wasserstein</b> GAN (WGAN) to operate on graphs, and today we are going to understand what that means [1]. The WGAN was developed by another team of researchers, Arjovsky et al., in 2017, and it uses the <b>Wasserstein</b> distance to compute the <b>loss</b> function for training the GAN [2]. In this post, we will provide some motivation ...", "dateLastCrawled": "2022-01-29T02:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Wasserstein</b> Distributionally Robust Optimization: Theory and ...", "url": "https://www.researchgate.net/publication/335395361_Wasserstein_Distributionally_Robust_Optimization_Theory_and_Applications_in_Machine_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/335395361_<b>Wasserstein</b>_Distributionally_Robust...", "snippet": "The <b>Wasserstein</b> distance between two probability measures on a metric space is a measure of closeness with applications in statistics, probability, and <b>machine</b> <b>learning</b>. In this work, we consider ...", "dateLastCrawled": "2022-01-26T23:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "deep <b>learning</b> - How can both generator and discriminator losses ...", "url": "https://datascience.stackexchange.com/questions/32699/how-can-both-generator-and-discriminator-losses-decrease", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/32699", "snippet": "In the widely used <b>analogy</b>: ... despite the WGAN having a different <b>loss</b> function, namely the <b>Wasserstein</b> distance, one should still not expect that the discriminator and generator simultaneously monotonically increase -- generally one of them &quot;wins&quot; the round and receives a lower portion of the <b>loss</b>. $\\endgroup$ \u2013 PSub. Mar 13 &#39;21 at 6:07 $\\begingroup$ @PSub You are completely misunderstanding the question. It&#39;s not a question about the small scale changes of the <b>loss</b> values. OP is asking ...", "dateLastCrawled": "2022-01-28T20:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "[D] Is the <b>Wasserstein</b> distance really what we optimize in WGAN ...", "url": "https://www.reddit.com/r/MachineLearning/comments/ew2lzs/d_is_the_wasserstein_distance_really_what_we/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/<b>MachineLearning</b>/comments/ew2lzs/d_is_the_<b>wasserstein</b>_distance...", "snippet": "The &quot;genuine&quot; <b>Wasserstein</b> <b>loss</b> relies on optimal transport, a generalization of sorting to high-dimensional feature spaces. In a nutshell: OT relies on the matrix of distances between samples to define a &quot;least action&quot; matching between any two distributions. Now, unfortunately, in spaces of images, the L2 distance is (essentially) meaningless: natural images should not be compared with each other pixel-wise. As a consequence, the baseline <b>Wasserstein</b> distance between two batches of images is ...", "dateLastCrawled": "2021-09-30T11:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Advanced <b>Machine</b> <b>Learning</b> - Computer Vision", "url": "https://www.vision.rwth-aachen.de/media/course/SS/2019/advanced-machine-learning/ml2_19-part17-gans-6on1.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.vision.rwth-aachen.de/media/course/SS/2019/advanced-<b>machine</b>-<b>learning</b>/ml2...", "snippet": "<b>Analogy</b>: police investigator \u2022Both generator and discriminator are deep networks We can train them with backprop. Image sources: www.bundesbank.de, weclipart.com, Kevin McGuiness 15 Advanced <b>Machine</b> <b>Learning</b> Part 17 \u2013Generative Adversarial Networks Training the Discriminator \u2022Procedure Fix generator weights Train discriminator to distinguish between real and generated images Image credit: Kevin McGuiness 16 Visual Computing Institute | Prof. Dr . Bastian Leibe Advanced <b>Machine</b> <b>Learning</b> ...", "dateLastCrawled": "2021-10-25T15:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A <b>Tour of Generative Adversarial Network Models</b> - <b>Machine</b> <b>Learning</b> Mastery", "url": "https://machinelearningmastery.com/tour-of-generative-adversarial-network-models/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>tour-of-generative-adversarial-network-models</b>", "snippet": "By <b>analogy</b> with auto-encoders, we propose Context Encoders \u2013 a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. \u2014 Context Encoders: Feature <b>Learning</b> by Inpainting, 2016. Example of the Context Encoders Encoder-Decoder Model Architecture. Taken from: Context Encoders: Feature <b>Learning</b> by Inpainting. The model is trained with a joint-<b>loss</b> that combines both the adversarial <b>loss</b> of generator and discriminator models ...", "dateLastCrawled": "2022-02-03T03:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "A <b>Gentle Introduction to Pix2Pix Generative</b> Adversarial Network", "url": "https://machinelearningmastery.com/a-gentle-introduction-to-pix2pix-generative-adversarial-network/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/a-<b>gentle-introduction-to-pix2pix-generative</b>...", "snippet": "Image-to-image translation is the controlled conversion of a given source image to a target image. An example might be the conversion of black and white photographs to color photographs. Image-to-image translation is a challenging problem and often requires specialized models and <b>loss</b> functions for a given translation task or dataset. The Pix2Pix GAN is a general approach for image-to-image translation. It is based", "dateLastCrawled": "2022-02-02T13:26:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(wasserstein loss)  is like +(learning a new language)", "+(wasserstein loss) is similar to +(learning a new language)", "+(wasserstein loss) can be thought of as +(learning a new language)", "+(wasserstein loss) can be compared to +(learning a new language)", "machine learning +(wasserstein loss AND analogy)", "machine learning +(\"wasserstein loss is like\")", "machine learning +(\"wasserstein loss is similar\")", "machine learning +(\"just as wasserstein loss\")", "machine learning +(\"wasserstein loss can be thought of as\")", "machine learning +(\"wasserstein loss can be compared to\")"]}