{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Dynamic decision making and value computations in medial frontal cortex", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8162729/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8162729", "snippet": "Variable ratio schedules are intuitive \u2014 reward is simply delivered with a fixed probability, much <b>like</b> <b>flipping</b> a (biased) <b>coin</b>. If given the choice between two variable ratio schedules, the optimal policy is to choose the higher probability option exclusively. Note that exclusively choosing one option is trivially consistent with matching behavior since all choices are allocated to one option, and all rewards are received from that option. Variableratio-schedule tasks are typically ...", "dateLastCrawled": "2021-07-21T15:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Compensatory variability in network parameters enhances memory ...", "url": "https://europepmc.org/article/PMC/PMC8670477", "isFamilyFriendly": true, "displayUrl": "https://europepmc.org/article/PMC/PMC8670477", "snippet": "The fly\u2019s behavior then depended probabilistically (via a <b>softmax</b> function; ... [Fig. 3E]) for the same reason that <b>flipping</b> <b>a coin</b> 5 times is more likely to give all heads than <b>flipping</b> <b>a coin</b> 50 times; a KC active for only a few odors is more likely to be active only for rewarded (or punished) odors, compared with a KC active for many odors. Under dense coding, KCs also have more variable lifetime sparseness in the random model (dashed lines in Fig. 3C and SI Appendix, Fig. S2). However ...", "dateLastCrawled": "2022-01-06T05:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Logistic Regression Cross Entropy Loss</b> - Logistic Regression for ...", "url": "https://www.coursera.org/lecture/deep-neural-networks-with-pytorch/logistic-regression-cross-entropy-loss-3LjrE", "isFamilyFriendly": true, "displayUrl": "https://<b>www.coursera.org</b>/lecture/deep-neural-networks-with-pytorch/logistic-regression...", "snippet": "So just <b>like</b> <b>flipping</b> <b>a coin</b>, you can come up with an expression for the likelihood, and here&#39;s an idealized plot of just one parameter, we want to find the maximum point. We can take the log, as you can see it didn&#39;t affect the position of the maximum just the shape of the function. And, if we just want to minimize something, we simply multiply it by a negative number. If you notice carefully, the location of the minimum is in the same place as the position of the maximum and we can simply ...", "dateLastCrawled": "2022-02-02T11:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "What is <b>Bernoulli Distribution</b>? <b>Bernoulli Distribution</b> Explained!", "url": "https://www.mygreatlearning.com/blog/bernoulli-distribution-explained/", "isFamilyFriendly": true, "displayUrl": "https://www.mygreatlearning.com/blog/<b>bernoulli-distribution</b>-explained", "snippet": "In the case of <b>flipping</b> an unbiased or fair <b>coin</b>, the value of p would be 0.5, giving a 50% probability of each outcome. However we must note that the probabilities of success and failure need not be equal all the time, <b>like</b> <b>Bernoulli distribution</b> in the case of a biased <b>coin</b> flip where probability of heads (success) is 0.6 while probability of tails (failure) is 0.4.", "dateLastCrawled": "2022-02-02T15:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Reinforcement Learning | Duke Institute for Brain Sciences Methods Meetings", "url": "https://dibsmethodsmeetings.github.io/RL-Guide/", "isFamilyFriendly": true, "displayUrl": "https://dibsmethodsmeetings.github.io/RL-Guide", "snippet": "Now that we have our probability, lets choose one of the arms based on those probabilities. Here, it <b>is like</b> <b>flipping</b> <b>a coin</b>. Sometimes you\u2019ll pick Arm 1, sometimes you\u2019ll pick Arm 2, depending on the outcome of that <b>coin</b> flip. Try it a few times.", "dateLastCrawled": "2021-12-04T07:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>8.2 Dropout</b> - Deep Networks | <b>Coursera</b>", "url": "https://www.coursera.org/lecture/deep-neural-networks-with-pytorch/8-2-dropout-6Lkrw", "isFamilyFriendly": true, "displayUrl": "https://<b>www.coursera.org</b>/lecture/deep-neural-networks-with-pytorch/<b>8-2-dropout</b>-6Lkrw", "snippet": "So if p is 0.5 it&#39;s <b>like</b> <b>flipping</b> a fair <b>coin</b>, r=0 with probability p or probability of heads, r=1 with the same probability as tails. Let&#39;s see how we implement the dropout method. In the l th layer of the neural network, we multiply. The activation function of each element in the layer with a Bernoulli distributed random variable r. R take on the value 0 with probability p and r takes on the value 1 with probability 1-p. In this case if r subscript 1 equals 0, we are shutting off the first ...", "dateLastCrawled": "2022-01-15T19:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Reinforcement Learning Demo</b> with Keras", "url": "https://maxcandocia.com/article/2017/Nov/05/reinforcement-learning-demo-keras/", "isFamilyFriendly": true, "displayUrl": "https://maxcandocia.com/article/2017/Nov/05/<b>reinforcement-learning-demo</b>-keras", "snippet": "The other night, I was given a problem: Two people are playing a game. Player 1 flips <b>a coin</b>, and gets a point if it&#39;s heads. Then Player 2 decides to flip any number of coins, and gets two to the power of that number of coins minus one (2 (n_coins-1)) points.The players take turns performing these actions, and the game ends when either player has at least 100 points.", "dateLastCrawled": "2022-01-13T14:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Utilising Uncertainty</b> for Efficient Learning of Likely-Admissible ...", "url": "https://www.raillab.org/posts/utilising-uncertainty-for-efficient-learning-of-likely-admissible-heuristics", "isFamilyFriendly": true, "displayUrl": "https://www.raillab.org/posts/<b>utilising-uncertainty</b>-for-efficient-learning-of-<b>like</b>ly...", "snippet": "These epistemic uncertainties are converted to a <b>softmax</b> distribution (bottom right of the tiles) and a particular next state is sampled from this distribution. These steps repeat until we observe a next state with epistemic uncertainty above some threshold (in this example the threshold is 1). At that point we generate a task with start state equal to that next state (last image).", "dateLastCrawled": "2021-12-03T04:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Machine Learning: Overfitting Is Your Friend, Not Your Foe", "url": "https://stackabuse.com/machine-learning-overfitting-is-your-friend-not-your-foe/", "isFamilyFriendly": true, "displayUrl": "https://stackabuse.com/machine-learning-overfitting-is-your-friend-not-your-foe", "snippet": "The overall accuracy gets up to ~50% and the network gets here pretty quickly and starts plateauing. 5/10 images being correctly classified sounds <b>like</b> tossing <b>a coin</b>, but remember that there are 10 classes here, so if it were randomly guessing, it&#39;d on average guess a single image out of ten. Let&#39;s switch to the CIFAR100 dataset, which also necessitates a network with at least a tiny bit more power, as there are less training instances per class, as well as a vastly higher number of classes:", "dateLastCrawled": "2022-02-03T15:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Reinforcement-Learning-Resource-Guide</b>/RLResourceGuide.md at master ...", "url": "https://github.com/rmgeddert/Reinforcement-Learning-Resource-Guide/blob/master/RLResourceGuide.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/rmgeddert/<b>Reinforcement-Learning-Resource-Guide</b>/blob/master/RL...", "snippet": "Sure, the probability of choosing Arm 2 was about 10% during that time and doing so corresponds to <b>flipping</b> heads on <b>a coin</b> 1000 times in a row, but it is still possible. Critically, though, it isn&#39;t very likely , and in fact the data makes much more sense (i.e., the data is much more likely ) if beta instead equals 0.5 .", "dateLastCrawled": "2021-10-26T00:48:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Compensatory variability in network parameters enhances memory ...", "url": "https://europepmc.org/article/PMC/PMC8670477", "isFamilyFriendly": true, "displayUrl": "https://europepmc.org/article/PMC/PMC8670477", "snippet": "The fly\u2019s behavior then depended probabilistically (via a <b>softmax</b> function; ... [Fig. 3E]) for the same reason that <b>flipping</b> <b>a coin</b> 5 times is more likely to give all heads than <b>flipping</b> <b>a coin</b> 50 times; a KC active for only a few odors is more likely to be active only for rewarded (or punished) odors, compared with a KC active for many odors. Under dense coding, KCs also have more variable lifetime sparseness in the random model (dashed lines in Fig. 3C and SI Appendix, Fig. S2). However ...", "dateLastCrawled": "2022-01-06T05:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Dynamic decision making and value computations in medial frontal cortex", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8162729/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8162729", "snippet": "Variable ratio schedules are intuitive \u2014 reward is simply delivered with a fixed probability, much like <b>flipping</b> a (biased) <b>coin</b>. If given the choice between two variable ratio schedules, the optimal policy is to choose the higher probability option exclusively. Note that exclusively choosing one option is trivially consistent with matching behavior since all choices are allocated to one option, and all rewards are received from that option. Variableratio-schedule tasks are typically ...", "dateLastCrawled": "2021-07-21T15:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Reinforcement-Learning-Resource-Guide</b>/RLResourceGuide.md at master ...", "url": "https://github.com/rmgeddert/Reinforcement-Learning-Resource-Guide/blob/master/RLResourceGuide.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/rmgeddert/<b>Reinforcement-Learning-Resource-Guide</b>/blob/master/RL...", "snippet": "<b>Softmax</b> function: the probability of choosing a given Q-value is e^(Q-value) divided by the sum of e^ ... the probability of choosing Arm 2 was about 10% during that time and doing so corresponds <b>to flipping</b> heads on <b>a coin</b> 1000 times in a row, but it is still possible. Critically, though, it isn&#39;t very likely, and in fact the data makes much more sense (i .e., the data is much more likely) if beta instead equals 0.5. It turns out that we can quantify just how likely the data is in each case ...", "dateLastCrawled": "2021-10-26T00:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "What is <b>Bernoulli Distribution</b>? <b>Bernoulli Distribution</b> Explained!", "url": "https://www.mygreatlearning.com/blog/bernoulli-distribution-explained/", "isFamilyFriendly": true, "displayUrl": "https://www.mygreatlearning.com/blog/<b>bernoulli-distribution</b>-explained", "snippet": "In the case of <b>flipping</b> an unbiased or fair <b>coin</b>, the value of p would be 0.5, giving a 50% probability of each outcome. However we must note that the probabilities of success and failure need not be equal all the time, like <b>Bernoulli distribution</b> in the case of a biased <b>coin</b> flip where probability of heads (success) is 0.6 while probability of tails (failure) is 0.4.", "dateLastCrawled": "2022-02-02T15:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Reinforcement Learning Demo</b> with Keras", "url": "https://maxcandocia.com/article/2017/Nov/05/reinforcement-learning-demo-keras/", "isFamilyFriendly": true, "displayUrl": "https://maxcandocia.com/article/2017/Nov/05/<b>reinforcement-learning-demo</b>-keras", "snippet": "The other night, I was given a problem: Two people are playing a game. Player 1 flips <b>a coin</b>, and gets a point if it&#39;s heads. Then Player 2 decides to flip any number of coins, and gets two to the power of that number of coins minus one (2 (n_coins-1)) points.The players take turns performing these actions, and the game ends when either player has at least 100 points.", "dateLastCrawled": "2022-01-13T14:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "10 <b>Model Comparison and Hierarchical Modeling</b> | Doing Bayesian Data ...", "url": "https://bookdown.org/ajkurz/DBDA_recoded/model-comparison-and-hierarchical-modeling.html", "isFamilyFriendly": true, "displayUrl": "https://bookdown.org/ajkurz/DBDA_recoded/<b>model-comparison-and-hierarchical-modeling</b>.html", "snippet": "The results are <b>similar</b>, for sure. But they\u2019re not the same. The stacking method via the brms default weights = &quot;loo2&quot; is the current preferred method by the folks on the Stan team (e.g., the authors of the above linked paper). For more on stacking and other weighting schemes, see Vehtari and Gabry\u2019s vignette Bayesian Stacking and Pseudo-BMA weights using the loo package or Vehtari\u2019s modelselection_tutorial GitHub repository. But don\u2019t worry. We will have more opportunities to ...", "dateLastCrawled": "2022-01-30T00:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 6, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Likelihood function</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Likelihood_function", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Likelihood_function</b>", "snippet": "Imagine <b>flipping</b> a fair <b>coin</b> twice, and observing the following data: two heads in two tosses (&quot;HH&quot;). Assuming that each successive <b>coin</b> flip is i.i.d., then the probability of observing HH is (=) = = Hence, given the observed data HH, the likelihood that the model parameter equals 0.5 is 0.25. Mathematically, this is written as (=) = This is not the same as saying that the probability that =, given the observation HH, is 0.25. (For that, we could apply Bayes&#39; theorem, which implies that the ...", "dateLastCrawled": "2022-02-07T13:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Machine Learning: Overfitting Is Your Friend, Not Your Foe", "url": "https://stackabuse.com/machine-learning-overfitting-is-your-friend-not-your-foe/", "isFamilyFriendly": true, "displayUrl": "https://stackabuse.com/machine-learning-overfitting-is-your-friend-not-your-foe", "snippet": "A cup is a cylinder, <b>similar</b> to a soda can, and some bottles may be too. Since these low-level features are relatively <b>similar</b>, it&#39;s easy to chuck them all into the &quot;food container&quot; category, but higher-level abstraction is required to properly guess whether something is a &quot;cup&quot; or a &quot;can&quot;.. What makes this job even harder is that CIFAR10 has 6000 images per class, while CIFAR100 has 600 images per class, giving the network less images to learn the ever so subtle differences from.", "dateLastCrawled": "2022-02-03T15:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "What is the <b>triplet loss in the context of Machine Learning</b> ... - Quora", "url": "https://www.quora.com/What-is-the-triplet-loss-in-the-context-of-Machine-Learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-the-<b>triplet-loss-in-the-context-of-Machine-Learning</b>", "snippet": "Answer: It\u2019s a loss function that is used when training a NN for face recognition/verification. Each training sample is actually composed of a \u201ctriplet\u201d of ...", "dateLastCrawled": "2022-01-24T16:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Making a neural network say \u201cI Don\u2019t Know\u201d: Bayesian NNs using Pyro and ...", "url": "https://news.ycombinator.com/item?id=18550910", "isFamilyFriendly": true, "displayUrl": "https://news.ycombinator.com/item?id=18550910", "snippet": "Let&#39;s take <b>a coin</b> toss as an example. It does not make sense to say I think that the <b>coin</b> has a 0.05 chance of <b>flipping</b> heads with a 0.9 credible interval of (0.01, 0.1). It does not make sense even to have a probability distribution on coming up heads (strictly speaking). Coming up heads is an event and events do not distributions associated with them, but rather single probabilities. On the other hand, we can model the outcomes of <b>a coin</b> with a Bernoulli distribution with parameter p where ...", "dateLastCrawled": "2019-11-01T00:09:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Supervised Learning - frnsys.com", "url": "http://frnsys.com/notes/ai/machine_learning/supervised_learning.html", "isFamilyFriendly": true, "displayUrl": "frnsys.com/notes/ai/machine_learning/supervised_learning.html", "snippet": "The hypothesis <b>can</b> <b>thought</b> of as the model that you try to learn for a particular task. You then use this model on new inputs, e.g. to make predictions - generalization is how the model performs on new examples; this is most important in machine learning. Basic concepts. Capacity: the flexibility of a model - that is, the variety of functions it <b>can</b> fit. Representational capacity - the functions which the model <b>can</b> learn; Effective capacity - in practice, a learning algorithm is not likely ...", "dateLastCrawled": "2021-12-13T03:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Implicit Value Updating Explains Transitive Inference Performance: The ...", "url": "https://europepmc.org/article/MED/26407227", "isFamilyFriendly": true, "displayUrl": "https://europepmc.org/article/MED/26407227", "snippet": "If H and T are <b>thought</b> of as the accumulated number of Heads and Tails resulting from <b>flipping</b> <b>a coin</b>, then Beta(p;H, T) ... and Q in the case of Q/<b>softmax</b>; <b>full</b> density functions are omitted for clarity). Fig 7. Visualization of the contents of memory for the three algorithms under simulated conditions. Three phases were included for each algorithm: 200 trials of adjacent pairs only, followed by 200 trials of all pairs, and then followed by 200 massed trials of only the pair FG. (A ...", "dateLastCrawled": "2021-11-26T01:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Dynamic decision making and value computations in medial frontal cortex", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8162729/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8162729", "snippet": "Variable ratio schedules are intuitive \u2014 reward is simply delivered with a fixed probability, much like <b>flipping</b> a (biased) <b>coin</b>. If given the choice between two variable ratio schedules, the optimal policy is to choose the higher probability option exclusively. Note that exclusively choosing one option is trivially consistent with matching behavior since all choices are allocated to one option, and all rewards are received from that option. Variableratio-schedule tasks are typically ...", "dateLastCrawled": "2021-07-21T15:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Reinforcement Learning Demo</b> with Keras", "url": "https://maxcandocia.com/article/2017/Nov/05/reinforcement-learning-demo-keras/", "isFamilyFriendly": true, "displayUrl": "https://max<b>can</b>docia.com/article/2017/Nov/05/<b>reinforcement-learning-demo</b>-keras", "snippet": "The other night, I was given a problem: Two people are playing a game. Player 1 flips <b>a coin</b>, and gets a point if it&#39;s heads. Then Player 2 decides to flip any number of coins, and gets two to the power of that number of coins minus one (2 (n_coins-1)) points.The players take turns performing these actions, and the game ends when either player has at least 100 points.", "dateLastCrawled": "2022-01-13T14:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 4, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Likelihood function</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Likelihood_function", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Likelihood_function</b>", "snippet": "The parameter is the probability that <b>a coin</b> lands heads up (&quot;H&quot;) when tossed. <b>can</b> take on any value within the range 0.0 to 1.0. For a perfectly fair <b>coin</b>, =. Imagine <b>flipping</b> a fair <b>coin</b> twice, and observing the following data: two heads in two tosses (&quot;HH&quot;).", "dateLastCrawled": "2022-02-02T22:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Aleatoric and <b>epistemic uncertainty</b> in machine learning: an ...", "url": "https://link.springer.com/article/10.1007/s10994-021-05946-3", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10994-021-05946-3", "snippet": "The prototypical example of aleatoric uncertainty is <b>coin</b> <b>flipping</b>: The data-generating process in this type of experiment has a stochastic component that cannot be reduced by any additional source of information (except Laplace\u2019s demon). Consequently, even the best model of this process will only be able to provide probabilities for the two possible outcomes, heads and tails, but no definite answer. As opposed to this, epistemic aka systematic) uncertainty refers to uncertainty caused by ...", "dateLastCrawled": "2022-01-29T09:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Learning Task Specifications from Demonstrations via the</b> ... - DeepAI", "url": "https://deepai.org/publication/learning-task-specifications-from-demonstrations-via-the-principle-of-maximum-causal-entropy", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>learning-task-specifications-from-demonstrations-via</b>...", "snippet": "07/26/19 - In many settings (e.g., robotics) demonstrations provide a natural way to specify sub-tasks; however, most methods for learning fr...", "dateLastCrawled": "2021-11-27T07:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "What <b>is posterior probability in machine learning</b>? - Quora", "url": "https://www.quora.com/What-is-posterior-probability-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-<b>is-posterior-probability-in-machine-learning</b>", "snippet": "Answer (1 of 2): That is a mathematical concept. It was Thomas Bayes [1763] who derived a way of inferring probabilities based on past observations. What it basically states, is that one <b>can</b> determine the probability of an event, based upon other knowledge, real or estimated; i.e. the probabilit...", "dateLastCrawled": "2021-12-22T09:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>CS486</b> | Anthony Zhang", "url": "https://anthony-zhang.me/University-Notes/CS486/CS486.html", "isFamilyFriendly": true, "displayUrl": "https://anthony-zhang.me/University-Notes/<b>CS486</b>/<b>CS486</b>.html", "snippet": "Search problems <b>can</b> <b>be thought</b> of as minimum-cost pathfinding on a graph of world states, so we <b>can</b> apply algorithms like breadth-first search and A*. The search tree is the tree made of possible paths within the graph of world states. The factors we care about in search algorithms are things like completeness (will it always find a solution?), optimality (does it always find the minimum cost solution?), time complexity, and space complexity. While DFS is light on memory and goes deep ...", "dateLastCrawled": "2021-04-30T01:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Making a neural network say \u201cI Don\u2019t Know\u201d: Bayesian NNs using Pyro and ...", "url": "https://news.ycombinator.com/item?id=18550910", "isFamilyFriendly": true, "displayUrl": "https://news.ycombinator.com/item?id=18550910", "snippet": "It does not make sense to say I think that the <b>coin</b> has a 0.05 chance of <b>flipping</b> heads with a 0.9 credible interval of (0.01, 0.1). It does not make sense even to have a probability distribution on coming up heads (strictly speaking). Coming up heads is an event and events do not distributions associated with them, but rather single probabilities. On the other hand, we <b>can</b> model the outcomes of <b>a coin</b> with a Bernoulli distribution with parameter p where the probability of heads is p and ...", "dateLastCrawled": "2019-11-01T00:09:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Compensatory variability in network parameters enhances memory ...", "url": "https://europepmc.org/article/PMC/PMC8670477", "isFamilyFriendly": true, "displayUrl": "https://europepmc.org/article/PMC/PMC8670477", "snippet": "The fly\u2019s behavior then depended probabilistically (via a <b>softmax</b> function; ... [Fig. 3E]) for the same reason that <b>flipping</b> <b>a coin</b> 5 times is more likely to give all heads than <b>flipping</b> <b>a coin</b> 50 times; a KC active for only a few odors is more likely to be active only for rewarded (or punished) odors, <b>compared</b> with a KC active for many odors. Under dense coding, KCs also have more variable lifetime sparseness in the random model (dashed lines in Fig. 3C and SI Appendix, Fig. S2). However ...", "dateLastCrawled": "2022-01-06T05:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Logistic Regression Cross Entropy Loss</b> - Logistic Regression for ...", "url": "https://www.coursera.org/lecture/deep-neural-networks-with-pytorch/logistic-regression-cross-entropy-loss-3LjrE", "isFamilyFriendly": true, "displayUrl": "https://<b>www.coursera.org</b>/lecture/deep-neural-networks-with-pytorch/logistic-regression...", "snippet": "So just like <b>flipping</b> <b>a coin</b>, you <b>can</b> come up with an expression for the likelihood, and here&#39;s an idealized plot of just one parameter, we want to find the maximum point. We <b>can</b> take the log, as you <b>can</b> see it didn&#39;t affect the position of the maximum just the shape of the function. And, if we just want to minimize something, we simply multiply it by a negative number. If you notice carefully, the location of the minimum is in the same place as the position of the maximum and we <b>can</b> simply ...", "dateLastCrawled": "2022-02-02T11:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Dynamic decision making and value computations in medial frontal cortex", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8162729/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8162729", "snippet": "Variable ratio schedules are intuitive \u2014 reward is simply delivered with a fixed probability, much like <b>flipping</b> a (biased) <b>coin</b>. If given the choice between two variable ratio schedules, the optimal policy is to choose the higher probability option exclusively. Note that exclusively choosing one option is trivially consistent with matching behavior since all choices are allocated to one option, and all rewards are received from that option. Variableratio-schedule tasks are typically ...", "dateLastCrawled": "2021-07-21T15:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Reinforcement Learning Demo</b> with Keras", "url": "https://maxcandocia.com/article/2017/Nov/05/reinforcement-learning-demo-keras/", "isFamilyFriendly": true, "displayUrl": "https://max<b>can</b>docia.com/article/2017/Nov/05/<b>reinforcement-learning-demo</b>-keras", "snippet": "The other night, I was given a problem: Two people are playing a game. Player 1 flips <b>a coin</b>, and gets a point if it&#39;s heads. Then Player 2 decides to flip any number of coins, and gets two to the power of that number of coins minus one (2 (n_coins-1)) points.The players take turns performing these actions, and the game ends when either player has at least 100 points.", "dateLastCrawled": "2022-01-13T14:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Logistic Regression Cross Entropy Loss - Logistic Regression for ...", "url": "https://pt.coursera.org/lecture/deep-neural-networks-with-pytorch/logistic-regression-cross-entropy-loss-3LjrE", "isFamilyFriendly": true, "displayUrl": "https://pt.coursera.org/lecture/deep-neural-networks-with-pytorch/logistic-regression...", "snippet": "So just like <b>flipping</b> <b>a coin</b>, you <b>can</b> come up with an expression for the likelihood, and here&#39;s an idealized plot of just one parameter, we want to find the maximum point. We <b>can</b> take the log, as you <b>can</b> see it didn&#39;t affect the position of the maximum just the shape of the function. And, if we just want to minimize something, we simply multiply it by a negative number. If you notice carefully, the location of the minimum is in the same place as the position of the maximum and we <b>can</b> simply ...", "dateLastCrawled": "2022-01-01T18:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "10 <b>Model Comparison and Hierarchical Modeling</b> | Doing Bayesian Data ...", "url": "https://bookdown.org/ajkurz/DBDA_recoded/model-comparison-and-hierarchical-modeling.html", "isFamilyFriendly": true, "displayUrl": "https://bookdown.org/ajkurz/DBDA_recoded/<b>model-comparison-and-hierarchical-modeling</b>.html", "snippet": "Kruschke considered the <b>coin</b> bias of two factories, each described by the beta distribution. We <b>can</b> organize how to derive the \\(\\alpha\\) and \\ ... But the <b>full</b> model of the data is actually the complete hierarchical structure that spans all the models being <b>compared</b>, as indicated in Figure 10.1 (p. 267). Therefore, if the hierarchical structure really expresses our prior beliefs, then the most complete prediction of future data takes into account all the models, weighted by their posterior ...", "dateLastCrawled": "2022-01-30T00:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Reinforcement-Learning-Resource-Guide</b>/RLResourceGuide.md at master ...", "url": "https://github.com/rmgeddert/Reinforcement-Learning-Resource-Guide/blob/master/RLResourceGuide.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/rmgeddert/<b>Reinforcement-Learning-Resource-Guide</b>/blob/master/RL...", "snippet": "Sure, the probability of choosing Arm 2 was about 10% during that time and doing so corresponds <b>to flipping</b> heads on <b>a coin</b> 1000 times in a row, but it is still possible. Critically, though, it isn&#39;t very likely , and in fact the data makes much more sense (i.e., the data is much more likely ) if beta instead equals 0.5 .", "dateLastCrawled": "2021-10-26T00:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Deep <b>ancient Roman Republican coin classification via feature</b> fusion ...", "url": "https://www.sciencedirect.com/science/article/pii/S0031320321000583", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0031320321000583", "snippet": "The object represented on <b>a coin</b> <b>can</b> have many forms, such as a person, instrument, animal, and building, to give a few examples. The object is the main element for <b>coin</b> classification in addition to the <b>coin</b> legend and smaller auxiliary symbols. We call these visible marks as motifs. Since there is a huge variation among the positions of these motifs on the Roman Republican coins, the task of image-based <b>coin</b> classification is very challenging and non-trivial. Exemplar images are shown in ...", "dateLastCrawled": "2021-11-06T12:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "What is the best definition <b>of pdata (data generating distribution</b>) in ...", "url": "https://www.quora.com/What-is-the-best-definition-of-pdata-data-generating-distribution-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-the-best-definition-of-pdata-data-generating...", "snippet": "Answer (1 of 2): I am adding myself something from How do I understand &#39;neural networks are trained to learn the distribution of data&#39;? that may clarify the question. \u201c\u2026 To answer your next question of how to understand \u201cdistribution\u201d in terms of images, we <b>can</b> consider each image as an n-dimens...", "dateLastCrawled": "2022-01-19T04:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What are <b>perceptual loss functions (machine learning</b>)? - Quora", "url": "https://www.quora.com/What-are-perceptual-loss-functions-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-are-<b>perceptual-loss-functions-machine-learning</b>", "snippet": "Answer: In instances where we want to know if two images look like each-other, we could use a mathematical equation to compare the images but this is unlikely to produce good results. Two images <b>can</b> look the same to humans but be very different mathematically (i.e. if there is a picture of a man ...", "dateLastCrawled": "2022-01-22T22:18:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "The <b>Softmax</b> and the Hierarchical <b>Softmax</b> - Anil Keshwani / paeninsula", "url": "https://anilkeshwani.github.io/hierarchical-softmax/", "isFamilyFriendly": true, "displayUrl": "https://anilkeshwani.github.io/hierarchical-<b>softmax</b>", "snippet": "The <b>Softmax</b> and the Hierarchical <b>Softmax</b>. 6 minute read. Published: January 27, 2022 Small additions or modifications to the end of the article will be made when time allows in order to include remarks on use of Huffman coding with the Hierarchical <b>Softmax</b> and to draw links between the concepts introduced here and, for example, Noise Contrastive Estimation (NCE).", "dateLastCrawled": "2022-01-28T11:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Softmax Regression</b>. Build a <b>Softmax Regression</b> Model from\u2026 | by Looi ...", "url": "https://medium.datadriveninvestor.com/softmax-regression-bda793e2bfc8", "isFamilyFriendly": true, "displayUrl": "https://medium.datadriveninvestor.com/<b>softmax-regression</b>-bda793e2bfc8", "snippet": "The derived equation above is known as <b>Softmax</b> function. From the derivation, we can see that the probability of y=i given x can be estimated by the <b>softmax</b> function. Summary of the model: weight vector associated with class g. weight matrix where each element corresponds to a feature of a class. Figure: illustration of the <b>softmax regression</b> ...", "dateLastCrawled": "2022-01-25T19:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "<b>Machine</b> <b>Learning</b> by <b>Analogy</b>. November 2017; Authors: Colleen Farrelly. Jenzabar; Download file PDF Read file. Download file PDF. Read file. Download citation. Copy link Link copied. Read file ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Relaxed Softmax</b> for <b>learning</b> from Positive and Unlabeled data - DeepAI", "url": "https://deepai.org/publication/relaxed-softmax-for-learning-from-positive-and-unlabeled-data", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>relaxed-softmax</b>-for-<b>learning</b>-from-positive-and...", "snippet": "In recent years, the <b>softmax</b> model and its fast approximations have become the de-facto loss functions for deep neural networks when dealing with multi-class prediction. This loss has been extended to language modeling and recommendation, two fields that fall into the framework of <b>learning</b> from Positive and Unlabeled data.", "dateLastCrawled": "2022-01-01T09:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Softmax</b> and Cross-<b>entropy for multi-class classification</b>. - AppliedAICourse", "url": "https://www.appliedaicourse.com/lecture/11/applied-machine-learning-online-course/3384/softmax-and-cross-entropy-for-multi-class-classification/8/module-8-neural-networks-computer-vision-and-deep-learning", "isFamilyFriendly": true, "displayUrl": "https://www.appliedaicourse.com/lecture/11/applied-<b>machine</b>-<b>learning</b>-online-course/3384/...", "snippet": "Home Courses Applied <b>Machine</b> <b>Learning</b> Online Course <b>Softmax</b> and Cross-<b>entropy for multi-class classification</b>. <b>Softmax</b> and Cross-<b>entropy for multi-class classification</b>. Instructor: Applied AI Course Duration: 25 mins . Close. This content is restricted. Please Login. Prev. Next . Gradient Checking and clipping. How to train a Deep MLP? Deep <b>Learning</b>:Neural Networks. 1.1 History of Neural networks and Deep <b>Learning</b>. 25 min. 1.2 How Biological Neurons work? 8 min. 1.3 Growth of biological ...", "dateLastCrawled": "2022-02-02T02:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Machine</b> <b>Learning</b> Concepts for Revision | by Raunak Sarada | Medium", "url": "https://raunaksarada-cse21.medium.com/machine-learning-concepts-for-revision-491384952d27", "isFamilyFriendly": true, "displayUrl": "https://raunaksarada-cse21.medium.com/<b>machine</b>-<b>learning</b>-concepts-for-revision-491384952d27", "snippet": "ML Concepts. A.I \u2014 Intelligence showed by machines which is common for humans <b>Machine</b> <b>Learning</b>- Recognize the pattern in data and automatically learn and improve through experience without explicitly being programmed Deep <b>Learning</b>- branch of <b>machine</b> <b>learning</b>.We have to deal with lots of data so in that case problems can\u2019t be solved with simple ML algorithms.", "dateLastCrawled": "2022-01-25T20:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Semantic trees for <b>training word embeddings with hierarchical softmax</b> ...", "url": "https://www.lateral.io/resources-blog/semantic-trees-hierarchical-softmax", "isFamilyFriendly": true, "displayUrl": "https://www.lateral.io/resources-blog/semantic-trees-hierarchical-<b>softmax</b>", "snippet": "<b>Machine</b> <b>Learning</b>. Semantic trees for <b>training word embeddings with hierarchical softmax</b>. September 7, 2017. Matthias Leimeister. Introduction. Word vector models represent each word in a vocabulary as a vector in a continuous space such that words that share the same context are \u201cclose\u201d together. Being close is measured using a distance metric or similarity measure such as the Euclidean distance or cosine similarity. Once word vectors have been trained on a large corpus, one can form ...", "dateLastCrawled": "2022-02-01T10:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Machine</b> <b>Learning</b>: Overfitting Is Your Friend, Not Your Foe", "url": "https://stackabuse.com/machine-learning-overfitting-is-your-friend-not-your-foe/", "isFamilyFriendly": true, "displayUrl": "https://stackabuse.com/<b>machine</b>-<b>learning</b>-overfitting-is-your-friend-not-your-foe", "snippet": "In cooking - a reverse <b>analogy</b> can be created. It&#39;s better to undersalt the stew early on, as you can always add salt later to taste, but it&#39;s hard to take it away once already put in. In <b>Machine</b> <b>Learning</b> - it&#39;s the opposite. It&#39;s better to have a model overfit, then simplify it, change hyperparameters, augment the data, etc. to make it ...", "dateLastCrawled": "2022-02-03T15:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Machine</b> <b>Learning</b> \u2014 Multiclass <b>Classification</b> with Imbalanced Dataset ...", "url": "https://towardsdatascience.com/machine-learning-multiclass-classification-with-imbalanced-data-set-29f6a177c1a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>machine</b>-<b>learning</b>-multiclass-<b>classification</b>-with...", "snippet": "The skewed distribution makes many conventional <b>machine</b> <b>learning</b> algorithms less effective, especially in predicting minority class examples. In order to do so, let us first understand the problem at hand and then discuss the ways to overcome those. Multiclass <b>Classification</b>: A <b>classification</b> task with more than two classes; e.g., classify a set of images of fruits which may be oranges, apples, or pears. Multi-class <b>classification</b> makes the assumption that each sample is assigned to one and ...", "dateLastCrawled": "2022-02-02T22:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Python implementation of Softmax Regression</b> : MachineLearning", "url": "https://www.reddit.com/r/MachineLearning/comments/1u983x/python_implementation_of_softmax_regression/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/<b>MachineLearning</b>/comments/1u983x/python_implementation_of...", "snippet": "So his \u201ckernel <b>machine</b>\u201d actually allows feature <b>learning</b> in the sense that his path kernel can change over the course of training. This really doesn&#39;t jibe with his comment that &quot; Perhaps the most significant implication of our result for deep <b>learning</b> is that it casts doubt on the common view that it works by automatically discovering new representations of the data, in contrast with other <b>machine</b> <b>learning</b> methods, which rely on predefined features (Bengio et al., 2013).&quot;", "dateLastCrawled": "2020-12-14T04:17:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(full softmax)  is like +(flipping a coin)", "+(full softmax) is similar to +(flipping a coin)", "+(full softmax) can be thought of as +(flipping a coin)", "+(full softmax) can be compared to +(flipping a coin)", "machine learning +(full softmax AND analogy)", "machine learning +(\"full softmax is like\")", "machine learning +(\"full softmax is similar\")", "machine learning +(\"just as full softmax\")", "machine learning +(\"full softmax can be thought of as\")", "machine learning +(\"full softmax can be compared to\")"]}