{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Universal</b> <b>Approximation</b> with Quadratic Deep Networks", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7076904/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7076904", "snippet": "The first result is that a network with a single hidden layer can <b>approximate</b> a <b>continuous</b> <b>function</b> at any accuracy <b>given</b> an infinite number of neurons . This means that the network can be extremely wide. Then, a number of papers were published investigating the complexity of <b>approximation</b>. For example, Barron et al. studied one-hidden-layer feedforward networks using sigmoidal activations, which can <b>approximate</b> a <b>function</b> family, wherein the first moment of the magnitude distribution of the ...", "dateLastCrawled": "2021-12-16T12:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Does the <b>universal</b> <b>approximation</b> <b>theorem</b> for neural networks hold for ...", "url": "https://stats.stackexchange.com/questions/325776/does-the-universal-approximation-theorem-for-neural-networks-hold-for-any-activa", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/325776/does-the-<b>universal</b>-<b>approximation</b>...", "snippet": "Does the <b>universal</b> <b>approximation</b> <b>theorem</b> for neural networks hold for any activation <b>function</b> (sigmoid, ReLU, Softmax, etc...) or is it limited to sigmoid functions? Update: As shimao points out in the comments, it doesn&#39;t hold for absolutely any <b>function</b>. So for which class of activation functions does it hold? neural-networks <b>approximation</b>. Share. Cite. Improve this question. Follow edited Jan 30 &#39;18 at 5:11. Skander H. asked Jan 30 &#39;18 at 4:44. Skander H. Skander H. 10.5k 2 2 gold badges ...", "dateLastCrawled": "2022-01-19T03:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Where can I find the proof of the <b>universal approximation theorem</b>?", "url": "https://ai.stackexchange.com/questions/13317/where-can-i-find-the-proof-of-the-universal-approximation-theorem", "isFamilyFriendly": true, "displayUrl": "https://ai.stackexchange.com/questions/13317/where-can-i-find-the-proof-of-the...", "snippet": "Part 1: <b>Universal</b> <b>Approximation</b>. Here I&#39;ve listed a few recent <b>universal</b> <b>approximation</b> results that come to mind. Remember, <b>universal</b> <b>approximation</b> asks if feed-forward networks (or some other architecture type) can <b>approximate</b> any (in this case <b>continuous</b>) <b>function</b> to arbitrary accuracy (I&#39;ll focus on the : uniformly on compacts sense). Let me mention, that <b>there</b> are two types of guarantees: quantitative ones and qualitative ones.The latter are akin to Hornik&#39;s results (Neural Networks ...", "dateLastCrawled": "2022-02-03T01:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Why Neural Nets Can <b>Approximate</b> Any <b>Function</b> | by Thomas Hikaru Clark ...", "url": "https://towardsdatascience.com/why-neural-nets-can-approximate-any-function-a878768502f0", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/why-neural-nets-can-<b>approximate</b>-any-<b>function</b>-a878768502f0", "snippet": "The central claim of the <b>Universal</b> <b>Approximation</b> <b>Theorem</b> is that with enough hidden neurons, <b>there</b> exists some set of connection weights that can <b>approximate</b> any <b>function</b> \u2014 even if that <b>function</b> is not something that you could possible write down neatly <b>like</b> f(x)=x\u00b2. Even a crazy, complicated <b>function</b>, <b>like</b> the one that takes as input a 100x100 pixel image and outputs either \u201cdog\u201d or \u201ccat\u201d is covered by this <b>Theorem</b>.", "dateLastCrawled": "2022-01-31T18:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "machine learning - <b>Neural nets as universal approximators</b> - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/8160183/neural-nets-as-universal-approximators", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/8160183", "snippet": "In fact, the <b>function</b> only needs to be measurable since, by Lusin&#39;s <b>theorem</b>, any measurable <b>function</b> is <b>continuous</b> on nearly all of its domain. This is good enough for the <b>universal</b> <b>approximation</b> <b>theorem</b>. Note, however, that the <b>theorem</b> only says that a <b>function</b> can be represented by a neural net. It does not say whether this representation can ...", "dateLastCrawled": "2022-01-02T10:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Universal Approximations of Invariant Maps</b> by Neural Networks ...", "url": "https://link.springer.com/article/10.1007/s00365-021-09546-1", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00365-021-09546-1", "snippet": "We describe generalizations of the <b>universal</b> <b>approximation</b> <b>theorem</b> for neural networks to maps invariant or equivariant with respect to linear representations of groups. Our goal is to establish network-<b>like</b> computational models that are both invariant/equivariant and provably complete in the sense of their ability <b>to approximate</b> any <b>continuous</b> invariant/equivariant map. Our contribution is three-fold. First, in the general case of compact groups we propose a construction of a complete ...", "dateLastCrawled": "2022-01-23T15:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Neural Networks are Function Approximation</b> Algorithms", "url": "https://machinelearningmastery.com/neural-networks-are-function-approximators/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>neural-networks-are-function</b>-approximators", "snippet": "\u2026 the <b>universal</b> <b>approximation</b> <b>theorem</b> states that a feedforward network with a linear output layer and at least one hidden layer with any \u201csquashing\u201d activation <b>function</b> (such as the logistic sigmoid activation <b>function</b>) can <b>approximate</b> any [\u2026] <b>function</b> from one finite-dimensional space to another with any desired non-zero amount of error, provided that the network is <b>given</b> enough hidden units", "dateLastCrawled": "2022-01-30T03:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "machine learning - Can neural networks <b>approximate</b> any <b>function</b> <b>given</b> ...", "url": "https://stackoverflow.com/questions/25609347/can-neural-networks-approximate-any-function-given-enough-hidden-neurons", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/25609347", "snippet": "The only rigorous <b>theorem</b> that exists about the ability of neural networks <b>to approximate</b> different kinds of functions is the <b>Universal</b> <b>Approximation</b> <b>Theorem</b>. The UAT states that any <b>continuous</b> <b>function</b> on a compact domain can be approximated by a <b>neural network</b> with only one hidden layer provided the activation functions used are BOUNDED, <b>continuous</b> and monotonically increasing.", "dateLastCrawled": "2022-01-16T04:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "[<b>R] Universal Approximation - Transposed</b>! : MachineLearning", "url": "https://www.reddit.com/r/MachineLearning/comments/hqcf2y/r_universal_approximation_transposed/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/MachineLearning/comments/hqcf2y/r_<b>universal</b>_<b>approximation</b>...", "snippet": "The original <b>Universal</b> <b>Approximation</b> <b>Theorem</b> is a classical <b>theorem</b> (from 1999-ish) that states that shallow neural networks can <b>approximate</b> any <b>function</b>. This is one of the foundational results on the topic of &quot;why neural networks work&quot;! Here: We establish a new version of the <b>theorem</b> that applies to arbitrarily deep neural networks. In doing so, we demonstrate a qualitative difference between shallow neural networks and deep neural networks (with respect to allowable activation functions ...", "dateLastCrawled": "2021-08-30T07:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "A visual proof that neural nets can <b>approximate</b> any <b>function</b> | Hacker News", "url": "https://news.ycombinator.com/item?id=19708620", "isFamilyFriendly": true, "displayUrl": "https://news.ycombinator.com/item?id=19708620", "snippet": "The <b>universal</b> <b>approximation</b> <b>theorem</b> guarantees that a finite-width neural network that approximates the <b>function</b> to within some epsilon exists. But, regardless of the <b>approximation</b> method, <b>there</b> is no way to certify that <b>a given</b> <b>approximation</b> method is sufficient for an arbitrary <b>continuous</b> <b>function</b> <b>given</b> only a finite number of samples (i.e., without oracle knowledge of the underlying <b>function</b>), which is the typical situation where neural networks are applied. I can construct a <b>continuous</b> ...", "dateLastCrawled": "2021-09-02T11:49:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Universal</b> <b>Approximation</b> with Quadratic Deep Networks", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7076904/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7076904", "snippet": "Since a ReLU quadratic network can represent any univariate <b>polynomial</b> in a unique and global manner, and by the Weierstrass <b>theorem</b> and the Kolmogorov <b>theorem</b> that multivariate functions can be represented through summation and composition of univariate functions, we can <b>approximate</b> any multivariate <b>function</b> with a well-structured ReLU quadratic neural network, justifying the <b>universal</b> <b>approximation</b> power of the quadratic network. To our <b>best</b> knowledge, our quadratic network is the first-of ...", "dateLastCrawled": "2021-12-16T12:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Fundamental Belief: <b>Universal</b> <b>Approximation</b> Theorems", "url": "https://sunju.org/teach/DL-Fall-2020/sep-21-A.pdf", "isFamilyFriendly": true, "displayUrl": "https://sunju.org/teach/DL-Fall-2020/sep-21-A.pdf", "snippet": "[A] <b>universal</b> <b>approximation</b> <b>theorem</b> (UAT) <b>Theorem</b> (UAT, [Cybenko, 1989,Hornik, 1991]) Let \u02d9: R !R be anonconstant, bounded, and continuousfunction. Let I m denote the m-dimensionalunit hypercube [0;1]m. The space ofreal-valued <b>continuous</b> functions on I m is denoted by C(I m). Then, <b>given</b> any &quot;&gt;0 and any <b>function</b> f2C(I", "dateLastCrawled": "2022-01-31T08:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "regression - <b>Universal Approximation Theorem \u2014 Neural Networks</b> - Cross ...", "url": "https://stats.stackexchange.com/questions/58799/universal-approximation-theorem-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/58799/<b>universal</b>-<b>approximation</b>-<b>theorem</b>-neural...", "snippet": "I have posted this question elsewhere--MSE-Meta, MSE, TCS, MetaOptimize.Previously, no one had <b>given</b> a solution. But now, here is a really excellent and comprehensive answer. <b>Universal</b> <b>approximation</b> <b>theorem</b> states that &quot;the standard multilayer feed-forward network with a single hidden layer, which contains finite number of hidden neurons, is a <b>universal</b> approximator among <b>continuous</b> functions on compact subsets of Rn, under mild assumptions on the activation <b>function</b>.&quot;. I understand what ...", "dateLastCrawled": "2022-01-20T11:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Why Neural Nets Can <b>Approximate</b> Any <b>Function</b> | by Thomas Hikaru Clark ...", "url": "https://towardsdatascience.com/why-neural-nets-can-approximate-any-function-a878768502f0", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/why-neural-nets-can-<b>approximate</b>-any-<b>function</b>-a878768502f0", "snippet": "The central claim of the <b>Universal</b> <b>Approximation</b> <b>Theorem</b> is that with enough hidden neurons, <b>there</b> exists some set of connection weights that can <b>approximate</b> any <b>function</b> \u2014 even if that <b>function</b> is not something that you could possible write down neatly like f(x)=x\u00b2. Even a crazy, complicated <b>function</b>, like the one that takes as input a 100x100 pixel image and outputs either \u201cdog\u201d or \u201ccat\u201d is covered by this <b>Theorem</b>.", "dateLastCrawled": "2022-01-31T18:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Neural Networks are Function Approximation</b> Algorithms", "url": "https://machinelearningmastery.com/neural-networks-are-function-approximators/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>neural-networks-are-function</b>-approximators", "snippet": "\u2026 the <b>universal</b> <b>approximation</b> <b>theorem</b> states that a feedforward network with a linear output layer and at least one hidden layer with any \u201csquashing\u201d activation <b>function</b> (such as the logistic sigmoid activation <b>function</b>) can <b>approximate</b> any [\u2026] <b>function</b> from one finite-dimensional space to another with any desired non-zero amount of error, provided that the network is <b>given</b> enough hidden units", "dateLastCrawled": "2022-01-30T03:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A <b>Universal Approximation Theorem</b> of Deep Neural Networks for ...", "url": "https://deepai.org/publication/a-universal-approximation-theorem-of-deep-neural-networks-for-expressing-distributions", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/a-<b>universal-approximation-theorem</b>-of-deep-neural...", "snippet": "<b>Given</b> a fairly general source distribution and a target distribution defined on R d which satisfies certain integrability assumptions, we show that <b>there</b> is a ReLU DNN with d inputs and one output such that the push-forward of the source distribution via the gradient of the output <b>function</b> defined by the DNN is arbitrarily close to the target. We measure the closeness between probability distributions by three integral probability metrics (IPMs): 1-Wasserstein metric, maximum mean ...", "dateLastCrawled": "2022-02-01T11:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "machine learning - <b>Universal Function approximation</b> - Theoretical ...", "url": "https://cstheory.stackexchange.com/questions/7894/universal-function-approximation", "isFamilyFriendly": true, "displayUrl": "https://cstheory.stackexchange.com/questions/7894/<b>universal-function-approximation</b>", "snippet": "It is known via the <b>universal</b> <b>approximation</b> <b>theorem</b> that a neural network with even a single hidden layer and an arbitrary activation <b>function</b> can <b>approximate</b> any <b>continuous</b> <b>function</b>. What other models are <b>there</b> that are also <b>universal</b> <b>function</b> approximators. machine-learning <b>function</b> <b>approximation</b>. Share.", "dateLastCrawled": "2022-01-30T16:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>While the universal approximation theorem states a</b> one layer NN can ...", "url": "https://www.quora.com/While-the-universal-approximation-theorem-states-a-one-layer-NN-can-approximate-any-function-wouldnt-most-such-NNs-be-impractical", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>While-the-universal-approximation-theorem-states-a</b>-one-layer-NN...", "snippet": "Answer: Yes. the value of it is purely theoretical: it can be done, but it probably shouldn\u2019t be used in practice. The <b>theorem</b> was mostly to show that the use of a hidden layer solved the theoretical problems with NNs without hidden layers: not only could they solve the XOR problem, they can act...", "dateLastCrawled": "2022-01-09T16:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "How to prove that <b>a single-layer neural network is</b> a <b>universal</b> ...", "url": "https://www.quora.com/How-do-you-prove-that-a-single-layer-neural-network-is-a-universal-approximator", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/How-do-you-prove-that-<b>a-single-layer-neural-network-is</b>-a...", "snippet": "Answer (1 of 4): This is a handy-wavy answer to a mathematical notion. You should check the reference for more details. I guess you are referring to this <b>theorem</b> [1]. The <b>Universal</b> <b>approximation</b> <b>theorem</b> roughly states that \u201csimple\u201d neural networks can <b>approximate</b> \u201cmany\u201d functions. Notice that ...", "dateLastCrawled": "2022-01-17T13:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "machine learning - Artificial neural networks EQUIVALENT to linear ...", "url": "https://stats.stackexchange.com/questions/305619/artificial-neural-networks-equivalent-to-linear-regression-with-polynomial-featu", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/305619/artificial-neural-networks-equivalent...", "snippet": "A line is too simple, but the <b>best</b> <b>approximation</b> is not on the right, it&#39;s in the middle, allthough the <b>function</b> on the right fits <b>best</b>. The <b>function</b> on the right would make some pretty weird (and probably suboptimal) predictions for new data points, especially if they fall near the wiggly bits on the left.", "dateLastCrawled": "2022-01-21T22:45:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Why are <b>neural networks</b> so powerful? | by Tivadar Danka | Towards Data ...", "url": "https://towardsdatascience.com/why-are-neural-networks-so-powerful-bc308906696c", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/why-are-<b>neural-networks</b>-so-powerful-bc308906696c", "snippet": "The <b>universal</b> <b>approximation</b> <b>theorem</b> in its full glory :) Source: Cybenko, G. (1989) \u201cApproximations by superpositions of sigmoidal functions\u201d, Mathematics of Control, Signals, and Systems, 2(4), 303\u2013314. A famous result from 1989, called <b>universal</b> <b>approximation</b> <b>theorem</b>, states that as long as the activation <b>function</b> is sigmoid-like and the <b>function</b> to be approximated is <b>continuous</b>, a neural network with a single hidden layer <b>can</b> <b>approximate</b> it as precisely as you want.(Or learn it in ...", "dateLastCrawled": "2022-01-26T04:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Universal Approximations of Invariant Maps</b> by Neural Networks ...", "url": "https://link.springer.com/article/10.1007/s00365-021-09546-1", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00365-021-09546-1", "snippet": "We describe generalizations of the <b>universal</b> <b>approximation</b> <b>theorem</b> for neural networks to maps invariant or equivariant with respect to linear representations of groups. Our goal is to establish network-like computational models that are both invariant/equivariant and provably complete in the sense of their ability <b>to approximate</b> any <b>continuous</b> invariant/equivariant map. Our contribution is three-fold. First, in the general case of compact groups we propose a construction of a complete ...", "dateLastCrawled": "2022-01-23T15:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Newest &#39;universal-approximation-theorems&#39; Questions</b> - Artificial ...", "url": "https://ai.stackexchange.com/questions/tagged/universal-approximation-theorems", "isFamilyFriendly": true, "displayUrl": "https://ai.stackexchange.com/questions/tagged/<b>universal-approximation-theorems</b>", "snippet": "The <b>universal</b> <b>approximation</b> <b>theorem</b> says that MLP with a single hidden layer and enough number of neurons <b>can</b> able <b>to approximate</b> any bounded <b>continuous</b> <b>function</b>. You <b>can</b> validate it from the ... comparison multilayer-perceptrons symbolic-ai <b>universal-approximation-theorems</b>. asked Dec 14 &#39;21 at 13:26. hanugm. 2,783 2 2 gold badges 8 8 silver badges 24 24 bronze badges. 4. votes. 1answer 68 views. Why does the activation <b>function</b> for a hidden layer in a MLP have to be non-<b>polynomial</b>? Across ...", "dateLastCrawled": "2022-01-23T13:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "How to prove that <b>a single-layer neural network is</b> a <b>universal</b> ...", "url": "https://www.quora.com/How-do-you-prove-that-a-single-layer-neural-network-is-a-universal-approximator", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/How-do-you-prove-that-<b>a-single-layer-neural-network-is</b>-a...", "snippet": "Answer (1 of 4): This is a handy-wavy answer to a mathematical notion. You should check the reference for more details. I guess you are referring to this <b>theorem</b> [1]. The <b>Universal</b> <b>approximation</b> <b>theorem</b> roughly states that \u201csimple\u201d neural networks <b>can</b> <b>approximate</b> \u201cmany\u201d functions. Notice that ...", "dateLastCrawled": "2022-01-17T13:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "What graph neural networks <b>can and cannot learn</b> \u2013 andreas loukas", "url": "https://andreasloukas.blog/2019/12/27/what-gnn-can-and-cannot-learn/", "isFamilyFriendly": true, "displayUrl": "https://andreasloukas.blog/2019/12/27/what-gnn-<b>can-and-cannot-learn</b>", "snippet": "Thus, by the <b>universal</b> <b>approximation</b> <b>theorem</b> and its variants, <b>given</b> sufficient depth and/or width, these networks <b>can</b> <b>approximate</b> any <b>continuous</b> <b>function</b> to arbitrary precision. MLPs with appropriate activations are also Turing <b>universal</b>. 1.2 Sufficient depth and width . Let us recall two basic definitions: The depth of a GNN equals the number of layers it contains. The width of a GNN equals the size of its node representations (i.e., the maximum length of any vector across all layers). It ...", "dateLastCrawled": "2022-01-30T15:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Do Gaussian process (<b>regression) have the universal approximation</b> ...", "url": "https://stats.stackexchange.com/questions/268429/do-gaussian-process-regression-have-the-universal-approximation-property", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/268429/do-gaussian-process-regression-have...", "snippet": "Dustin Tran et al. also proved an <b>universal</b> <b>approximation</b> <b>theorem</b> in the Bayesian framework for the Variational Gaussian Process, which is a more complex model because of the warping functions, but it&#39;s very closely related. I&#39;ll write an answer if the question gets reopened. PS note that <b>universal</b> <b>approximation</b>, as for Neural Networks, only holds over a compact set, not over all of $\\mathbb{R}^p$.", "dateLastCrawled": "2022-01-07T17:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) <b>Universal</b> <b>Approximation</b> by Ridge Computational Models and Neural ...", "url": "https://www.researchgate.net/publication/228696065_Universal_Approximation_by_Ridge_Computational_Models_and_Neural_Networks_A_Survey", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/228696065_<b>Universal</b>_<b>Approximation</b>_by_Ridge...", "snippet": "In [35], Hecht-Nielsen called attention to the fact that this <b>theorem</b> <b>can</b> be read as an existence <b>theorem</b> for neural networks with t wo hidden layers, used to represent any <b>continuous</b> <b>function</b> ...", "dateLastCrawled": "2022-01-06T04:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "[<b>R] Universal Approximation - Transposed</b>! : MachineLearning", "url": "https://www.reddit.com/r/MachineLearning/comments/hqcf2y/r_universal_approximation_transposed/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/MachineLearning/comments/hqcf2y/r_<b>universal</b>_<b>approximation</b>...", "snippet": "The original <b>Universal</b> <b>Approximation</b> <b>Theorem</b> is a classical <b>theorem</b> (from 1999-ish) that states that shallow neural networks <b>can</b> <b>approximate</b> any <b>function</b>. This is one of the foundational results on the topic of &quot;why neural networks work&quot;! Here: We establish a new version of the <b>theorem</b> that applies to arbitrarily deep neural networks. In doing so, we demonstrate a qualitative difference between shallow neural networks and deep neural networks (with respect to allowable activation functions ...", "dateLastCrawled": "2021-08-30T07:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "complexity theory - Why <b>can</b>&#39;t we say that a Neural Network is a NP ...", "url": "https://cs.stackexchange.com/questions/130270/why-cant-we-say-that-a-neural-network-is-a-np-problem-solver", "isFamilyFriendly": true, "displayUrl": "https://cs.stackexchange.com/questions/130270/why-<b>can</b>t-we-say-that-a-neural-network-is...", "snippet": "A precise definition of a &quot;Borel measurable <b>function</b>&quot; is <b>given</b> here. A more thorough discussion will be <b>given</b> in any text on Measure Theory. This type of result is an known as a <b>Universal</b> <b>Approximation</b> <b>Theorem</b>. For another example, see here.", "dateLastCrawled": "2022-01-24T13:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "machine learning - <b>Can</b> a neural network with linear activation ...", "url": "https://ai.stackexchange.com/questions/3753/can-a-neural-network-with-linear-activation-functions-produce-a-connection-of-li", "isFamilyFriendly": true, "displayUrl": "https://ai.stackexchange.com/questions/3753/<b>can</b>-a-neural-network-with-linear...", "snippet": "No matter how many layers and neurons <b>there</b> are, if all are linear in nature, the final activation <b>function</b> of the last layer is also a linear <b>function</b> of the input of the first layer. That means that any or all of these layers <b>can</b> be replaced by one layer. This completely loses the advantage of stacking layers because any multilayer network is equivalent to a single layer with linear activation, <b>given</b> that a combination of linear functions in a linear manner is still another linear <b>function</b> ...", "dateLastCrawled": "2022-01-08T03:44:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Universal</b> <b>Approximation</b> with Quadratic Deep Networks", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7076904/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7076904", "snippet": "Since a ReLU quadratic network <b>can</b> represent any univariate <b>polynomial</b> in a unique and global manner, and by the Weierstrass <b>theorem</b> and the Kolmogorov <b>theorem</b> that multivariate functions <b>can</b> be represented through summation and composition of univariate functions, we <b>can</b> <b>approximate</b> any multivariate <b>function</b> with a well-structured ReLU quadratic neural network, justifying the <b>universal</b> <b>approximation</b> power of the quadratic network. To our <b>best</b> knowledge, our quadratic network is the first-of ...", "dateLastCrawled": "2021-12-16T12:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A <b>Universal Approximation Theorem</b> of Deep Neural Networks for ...", "url": "https://deepai.org/publication/a-universal-approximation-theorem-of-deep-neural-networks-for-expressing-distributions", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/a-<b>universal-approximation-theorem</b>-of-deep-neural...", "snippet": "<b>Given</b> a fairly general source distribution and a target distribution defined on R d which satisfies certain integrability assumptions, we show that <b>there</b> is a ReLU DNN with d inputs and one output such that the push-forward of the source distribution via the gradient of the output <b>function</b> defined by the DNN is arbitrarily close to the target. We measure the closeness between probability distributions by three integral probability metrics (IPMs): 1-Wasserstein metric, maximum mean ...", "dateLastCrawled": "2022-02-01T11:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Why are <b>neural networks</b> so powerful? | by Tivadar Danka | Towards Data ...", "url": "https://towardsdatascience.com/why-are-neural-networks-so-powerful-bc308906696c", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/why-are-<b>neural-networks</b>-so-powerful-bc308906696c", "snippet": "The <b>universal</b> <b>approximation</b> <b>theorem</b> in its full glory :) Source: Cybenko, G. (1989) \u201cApproximations by superpositions of sigmoidal functions\u201d, Mathematics of Control, Signals, and Systems, 2(4), 303\u2013314. A famous result from 1989, called <b>universal</b> <b>approximation</b> <b>theorem</b>, states that as long as the activation <b>function</b> is sigmoid-like and the <b>function</b> to be approximated is <b>continuous</b>, a neural network with a single hidden layer <b>can</b> <b>approximate</b> it as precisely as you want.(Or learn it in ...", "dateLastCrawled": "2022-01-26T04:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Does the <b>universal</b> <b>approximation</b> <b>theorem</b> for neural networks hold for ...", "url": "https://stats.stackexchange.com/questions/325776/does-the-universal-approximation-theorem-for-neural-networks-hold-for-any-activa", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/325776/does-the-<b>universal</b>-<b>approximation</b>...", "snippet": "In other words, the hypothesis that the activation <b>function</b> is bounded and nonconstant is sufficient <b>to approximate</b> nearly any <b>function</b> <b>given</b> we <b>can</b> use as many hidden units as we like in the neural network.", "dateLastCrawled": "2022-01-19T03:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Why Neural Nets <b>Can</b> <b>Approximate</b> Any <b>Function</b> | by Thomas Hikaru Clark ...", "url": "https://towardsdatascience.com/why-neural-nets-can-approximate-any-function-a878768502f0", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/why-neural-nets-<b>can</b>-<b>approximate</b>-any-<b>function</b>-a878768502f0", "snippet": "The central claim of the <b>Universal</b> <b>Approximation</b> <b>Theorem</b> is that with enough hidden neurons, <b>there</b> exists some set of connection weights that <b>can</b> <b>approximate</b> any <b>function</b> \u2014 even if that <b>function</b> is not something that you could possible write down neatly like f(x)=x\u00b2. Even a crazy, complicated <b>function</b>, like the one that takes as input a 100x100 pixel image and outputs either \u201cdog\u201d or \u201ccat\u201d is covered by this <b>Theorem</b>. Non-Linearity. The key to neural networks\u2019 ability to ...", "dateLastCrawled": "2022-01-31T18:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "machine learning - Artificial neural networks EQUIVALENT to linear ...", "url": "https://stats.stackexchange.com/questions/305619/artificial-neural-networks-equivalent-to-linear-regression-with-polynomial-featu", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/305619/artificial-neural-networks-equivalent...", "snippet": "If the <b>universal</b> <b>approximation</b> basically states that <b>given</b> infinite amount of neurons we <b>can</b> <b>approximate</b> any <b>function</b> (thank you very much?), what PAC says in practical terms is, <b>given</b> (practically!) infinite amount of labelled examples we <b>can</b> get as close as we want to the <b>best</b> hypothesis within our model. It was absolutely hilarious when I ...", "dateLastCrawled": "2022-01-21T22:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Universal Approximations of Invariant Maps</b> by Neural Networks ...", "url": "https://link.springer.com/article/10.1007/s00365-021-09546-1", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00365-021-09546-1", "snippet": "We describe generalizations of the <b>universal</b> <b>approximation</b> <b>theorem</b> for neural networks to maps invariant or equivariant with respect to linear representations of groups. Our goal is to establish network-like computational models that are both invariant/equivariant and provably complete in the sense of their ability <b>to approximate</b> any <b>continuous</b> invariant/equivariant map. Our contribution is three-fold. First, in the general case of compact groups we propose a construction of a complete ...", "dateLastCrawled": "2022-01-23T15:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>SVM Vs Neural Network</b> | <b>Baeldung on Computer Science</b>", "url": "https://www.baeldung.com/cs/svm-vs-neural-network", "isFamilyFriendly": true, "displayUrl": "https://www.baeldung.com/cs/<b>svm-vs-neural-network</b>", "snippet": "Let\u2019s start by considering neural networks and, in particular, single-layer networks. The <b>universal</b> <b>approximation</b> <b>theorem</b> tells us that a neural network with a single hidden layer and a non-linear activation <b>can</b> <b>approximate</b>, with an appropriate choice of weights, any <b>continuous</b> <b>function</b>.. If the decision boundary of a classification problem <b>can</b> be defined as a <b>continuous</b> <b>function</b>, which is always the case, then it <b>can</b> also be defined as a <b>continuous</b> mapping of the feature space.This, in ...", "dateLastCrawled": "2022-02-02T17:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>While the universal approximation theorem states a</b> one layer NN <b>can</b> ...", "url": "https://www.quora.com/While-the-universal-approximation-theorem-states-a-one-layer-NN-can-approximate-any-function-wouldnt-most-such-NNs-be-impractical", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>While-the-universal-approximation-theorem-states-a</b>-one-layer-NN...", "snippet": "Answer: Yes. the value of it is purely theoretical: it <b>can</b> be done, but it probably shouldn\u2019t be used in practice. The <b>theorem</b> was mostly to show that the use of a hidden layer solved the theoretical problems with NNs without hidden layers: not only could they solve the XOR problem, they <b>can</b> act...", "dateLastCrawled": "2022-01-09T16:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Numerical solution and bifurcation analysis of nonlinear partial ...", "url": "https://link.springer.com/article/10.1007%2Fs10915-021-01650-5", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10915-021-01650-5", "snippet": "<b>Theorem</b> 1 (<b>Universal</b> <b>approximation</b>) Let the coefficients \\(\\varvec{\\alpha } ... (\\varvec{x}) \\Vert \\), where f is a <b>continuous</b> <b>function</b>. Then, one has with probability one that \\(\\lim _{N\\rightarrow \\infty } \\Vert f-{\\tilde{v}}^N\\Vert = 0 \\). We remark that in the ANN framework, the classical way is to optimize the parameters of the network (internal and external weights and biases) iteratively, e.g. by stochastic gradient descent algorithms that have a high computational cost and don\u2019t ...", "dateLastCrawled": "2022-01-30T06:21:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Universal Approximation Theorem</b>. The power of Neural Networks | by ...", "url": "https://medium.com/swlh/universal-approximation-theorem-d1a1a67c1b5b", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/<b>universal-approximation-theorem</b>-d1a1a67c1b5b", "snippet": "<b>Universal Approximation Theorem</b>, in its lose form, states that a feed-forward network with a single hidden layer containing a finite number of neurons can approximate any continuous function. Whoa ...", "dateLastCrawled": "2022-01-28T02:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Universal Approximation Theorem</b>, Neural Nets &amp; Lego Blocks | by ...", "url": "https://medium.com/analytics-vidhya/universal-approximation-theorem-neural-nets-lego-blocks-1f5a7d93542a", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/<b>universal-approximation-theorem</b>-neural-nets-lego...", "snippet": "In this post, we will look at the <b>Universal Approximation Theorem</b> \u2014 one of the fundamental theorems on which the entire concept of Deep <b>Learning</b> is based upon. We will make use of lego blocks ...", "dateLastCrawled": "2022-01-28T22:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Learning</b> Complex Functions using <b>Universal</b> Approximate <b>Theorem</b> - Ai Nxt", "url": "https://ainxt.co.in/learning-complex-functions-using-universal-approximate-theorem/", "isFamilyFriendly": true, "displayUrl": "https://ainxt.co.in/<b>learning</b>-complex-functions-using-<b>universal</b>-approximate-<b>theorem</b>", "snippet": "<b>Universal</b> <b>Approximation</b> <b>Theorem</b>. No matter how complex our output logic is, we can use collection of neurons and form Dense Neural Network to approximate our function. This is known as \u201c<b>UNIVERSAL</b> <b>APPROXIMATION</b> <b>THEOREM</b>\u201c. Lets take an example of Two Dimensional data where y = f(x) i.e. y is some function of x. Now, we need to find that ...", "dateLastCrawled": "2022-01-21T05:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) <b>Universal Approximation Theorems</b> - ResearchGate", "url": "https://www.researchgate.net/publication/336361517_Universal_Approximation_Theorems", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/336361517_<b>Universal_Approximation_Theorems</b>", "snippet": "In the <b>machine</b> <b>learning</b> literature, <b>universal</b> <b>approximation</b> refers to a model class\u2019 ability. to generically approximate any member of a large topological space whose elements are. functions, or ...", "dateLastCrawled": "2022-01-25T23:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Illustrative Proof of <b>Universal Approximation Theorem</b> | HackerNoon", "url": "https://hackernoon.com/illustrative-proof-of-universal-approximation-theorem-5845c02822f6", "isFamilyFriendly": true, "displayUrl": "https://hackernoon.com/illustrative-proof-of-<b>universal-approximation-theorem</b>-5845c02822f6", "snippet": "We will talk about the <b>Universal approximation theorem</b> and we will also prove the <b>theorem</b> graphically. The most commonly used sigmoid function is the logistic function, which has a characteristic of an \u201cS\u201d shaped curve. In real life, we deal with complex functions where the relationship between input and output might be complex. To solve this problem, let&#39;s take an <b>analogy</b> of building a house. The way we are going to create complex functions is that we will combine the sigmoids neurons ...", "dateLastCrawled": "2022-02-01T02:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "neural networks - <b>Universal Approximation Theorem and high dimension</b> ...", "url": "https://stats.stackexchange.com/questions/298622/universal-approximation-theorem-and-high-dimension-linear-regression", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/298622/<b>universal</b>-<b>approximation</b>-<b>theorem</b>-and...", "snippet": "Cross Validated is a question and answer site for people interested in statistics, <b>machine</b> <b>learning</b>, data analysis, data mining, and data visualization. It only takes a minute to sign up. Sign up to join this community", "dateLastCrawled": "2022-01-17T22:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine Learning by Analogy</b> - SlideShare", "url": "https://www.slideshare.net/ColleenFarrelly/machine-learning-by-analogy-59094152", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ColleenFarrelly/<b>machine-learning-by-analogy</b>-59094152", "snippet": "<b>Machine Learning by Analogy</b> 1. Colleen M. Farrelly 2. Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics behind these ...", "dateLastCrawled": "2022-01-31T07:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "TensorFlow is an interface for expressing <b>machine</b> <b>learning</b> algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Approximation</b> of Continuous Functions by Artificial Neural Networks", "url": "https://digitalworks.union.edu/cgi/viewcontent.cgi?article=3335&context=theses", "isFamilyFriendly": true, "displayUrl": "https://digitalworks.union.edu/cgi/viewcontent.cgi?article=3335&amp;context=theses", "snippet": "tations. Recently, techniques from <b>machine</b> <b>learning</b> have trained neural networks to perform a variety of tasks. It can be shown that any continuous function can be approximated by an arti cial neural network with arbitrary precision. This is known as the <b>universal</b> <b>approximation</b> <b>theorem</b>. In this thesis, we will introduce neural networks and one of the rst versions of this <b>theorem</b>, due to Cybenko. He modeled arti cial neural networks using sigmoidal functions and used tools from measure theory ...", "dateLastCrawled": "2022-01-24T09:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Neural Networks and Learning Machines</b> - etsmtl.ca", "url": "https://cours.etsmtl.ca/sys843/REFS/Books/ebook_Haykin09.pdf", "isFamilyFriendly": true, "displayUrl": "https://cours.etsmtl.ca/sys843/REFS/Books/ebook_Haykin09.pdf", "snippet": "15.3 <b>Universal</b> <b>Approximation</b> <b>Theorem</b> 797 15.4 Controllability and Observability 799 15.5 Computational Power of Recurrent Networks 804 15.6 <b>Learning</b> Algorithms 806 15.7 Back Propagation Through Time 808 15.8 Real-Time Recurrent <b>Learning</b> 812 15.9 Vanishing Gradients in Recurrent Networks 818", "dateLastCrawled": "2022-01-31T06:23:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(universal approximation theorem)  is like +(there is a \"best\" polynomial function to approximate a given continuous function)", "+(universal approximation theorem) is similar to +(there is a \"best\" polynomial function to approximate a given continuous function)", "+(universal approximation theorem) can be thought of as +(there is a \"best\" polynomial function to approximate a given continuous function)", "+(universal approximation theorem) can be compared to +(there is a \"best\" polynomial function to approximate a given continuous function)", "machine learning +(universal approximation theorem AND analogy)", "machine learning +(\"universal approximation theorem is like\")", "machine learning +(\"universal approximation theorem is similar\")", "machine learning +(\"just as universal approximation theorem\")", "machine learning +(\"universal approximation theorem can be thought of as\")", "machine learning +(\"universal approximation theorem can be compared to\")"]}