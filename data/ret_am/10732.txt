{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Why <b>is squared hinge loss differentiable? - Quora</b>", "url": "https://www.quora.com/Why-is-squared-hinge-loss-differentiable", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Why-<b>is-squared-hinge-loss-differentiable</b>", "snippet": "Answer (1 of 4): Let\u2019s start by defining the <b>hinge</b> <b>loss</b> function h(x) = max(1-x,0). Now let\u2019s think about the derivative h\u2019(x). This does not exist at x = 1 because the left and right limits do not converge to the same <b>number</b> (ie: the derivative is undefined at x=1, but it is -1 for x&lt;1 and 0 fo...", "dateLastCrawled": "2022-02-03T02:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Understanding Loss Functions the Smart</b> Way | by Paras Varshney ...", "url": "https://towardsdatascience.com/understanding-loss-functions-the-smart-way-904266e9393", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>understanding-loss-functions-the-smart</b>-way-904266e9393", "snippet": "<b>Squared</b> <b>hinge</b> <b>loss</b> is simply the square of <b>hinge</b> <b>loss</b>. I f you want to punish the large errors, <b>squared</b> <b>hinge</b> <b>loss</b> comes into the picture . Max function will be the same except the output will be <b>squared</b> .", "dateLastCrawled": "2022-02-01T01:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Ultimate Guide To Loss functions</b> In Tensorflow Keras API With Python ...", "url": "https://analyticsindiamag.com/ultimate-guide-to-loss-functions-in-tensorflow-keras-api-with-python-implementation/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/ultimate-guide-to-lo", "snippet": "<b>Squared</b> <b>Hinge</b>; Categorical <b>Hinge</b>; Implementation. You can use the <b>loss</b> function by simply calling tf.keras.<b>loss</b> as shown in the below command, and we are also importing NumPy additionally for our upcoming sample usage of <b>loss</b> functions: import tensorflow as tf import numpy as np bce_<b>loss</b> = tf.keras.losses.BinaryCrossentropy() 1. Binary Cross-Entropy(BCE) <b>loss</b>. BCE is used to compute the cross-entropy between the true labels and predicted outputs, it is majorly used when there are only two ...", "dateLastCrawled": "2022-02-03T09:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Loss</b> Functions. <b>Loss</b> functions explanations and\u2026 | by Tomer Kordonsky ...", "url": "https://medium.com/artificialis/loss-functions-361b2ad439a0", "isFamilyFriendly": true, "displayUrl": "https://medium.com/artificialis/<b>loss</b>-functions-361b2ad439a0", "snippet": "The <b>Hinge</b> <b>Loss</b> Equation def <b>Hinge</b>(yhat, y): return np.max(0,1 - yhat * y) Where y is the actual label (-1 or 1) and \u0177 is the prediction; The <b>loss</b> is 0 when the signs of the labels and prediction ...", "dateLastCrawled": "2022-02-03T02:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Loss</b> Functions in Machine Learning | G. Wu", "url": "https://guangyuwu.wordpress.com/2021/02/02/loss-functions-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://guangyuwu.wordpress.com/2021/02/02/<b>loss</b>-functions-in-machine-learning", "snippet": "<b>Hinge</b> <b>Loss</b>. <b>Hinge</b> <b>loss</b> is primarily used with Support Vector Machine (SVM) Classifiers with class labels -1 and 1. So make sure you change the label of the \u2018Malignant\u2019 class in the dataset from 0 to -1. <b>Hinge</b> <b>Loss</b> not only penalizes the wrong predictions but also the right predictions that are not confident.", "dateLastCrawled": "2022-01-12T02:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Probabilistic Soft Logic</b> | <b>Probabilistic soft logic</b> (PSL) is a machine ...", "url": "https://psl.linqs.org/wiki/master/Rule-Specification.html", "isFamilyFriendly": true, "displayUrl": "https://psl.linqs.org/wiki/master/Rule-Specification.html", "snippet": "<b>Squaring</b>. Any weighted rule can choose to square their <b>hinge</b>-<b>loss</b> functions. <b>Squaring</b> the <b>hinge</b>-<b>loss</b> (or \u201c<b>squared</b> potentials\u201d) may result in better performance. Non-<b>squared</b> potentials tend to encourage a \u201cwinner take all\u201d optimization, while <b>squared</b> potentials encourage more trading off. To square a rule, just suffix a ^2 to it:", "dateLastCrawled": "2022-01-31T18:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Common Loss functions and their uses - quick</b> note - Petamind", "url": "https://petamind.com/common-loss-functions-and-their-use-quick-note/", "isFamilyFriendly": true, "displayUrl": "https://petamind.com/<b>common-loss-functions-and-their</b>-use-quick-note", "snippet": "<b>Hinge</b> <b>Loss</b>/Multi-class SVM <b>Loss</b>. In simple terms, the score of the correct category should be greater than the sum of scores of all incorrect categories by some safety margin (usually one). And hence <b>hinge</b> <b>loss</b> is used for maximum-margin classification, most notably for support vector machines. Although not differentiable, it\u2019s a convex function which makes it easy to work with usual convex optimizers used in the machine learning domain. Mathematical formulation: Consider an example where ...", "dateLastCrawled": "2022-01-21T14:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "How to Choose <b>Loss</b> Functions When Training Deep Learning Neural Networks", "url": "https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/how-to-choose-<b>loss</b>-functions-when-training-deep...", "snippet": "<b>Squared</b> <b>Hinge</b> <b>Loss</b>; Multi-Class Classification <b>Loss</b> Functions Multi-Class Cross-Entropy <b>Loss</b>; Sparse Multiclass Cross-Entropy <b>Loss</b>; Kullback Leibler Divergence <b>Loss</b> ; We will focus on how to choose and implement different <b>loss</b> functions. For more theory on <b>loss</b> functions, see the post: <b>Loss</b> and <b>Loss</b> Functions for Training Deep Learning Neural Networks; Regression <b>Loss</b> Functions. A regression predictive modeling problem involves predicting a real-valued quantity. In this section, we will ...", "dateLastCrawled": "2022-02-02T15:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Loss</b> functions.pptx - <b>Loss</b> functions Model Selection and Evaluation ...", "url": "https://www.coursehero.com/file/84993906/Loss-functionspptx/", "isFamilyFriendly": true, "displayUrl": "https://www.coursehero.com/file/84993906/<b>Loss</b>-functionspptx", "snippet": "<b>Hinge</b> <b>Loss</b> not only penalizes wrong predictions but also right predictions that are not confident. <b>Hinge</b> <b>loss</b> for an input-output pair (x, y) is : After running the update function for 2000 iterations with three different values of alpha, we obtain this plot used when we want to make real-time decisions with not a laser-sharp focus on accuracy", "dateLastCrawled": "2021-12-30T22:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "10.2. <b>Loss</b> Functions \u2014 Principles and Techniques of Data Science", "url": "https://textbook.ds100.org/ch/10/modeling_loss_functions.html", "isFamilyFriendly": true, "displayUrl": "https://textbook.ds100.org/ch/10/modeling_<b>loss</b>_functions.html", "snippet": "10.2. <b>Loss</b> Functions\u00b6. Recall our assumptions thus far: we assume that there is a single population tip percentage \\( \\theta^* \\).Our model estimates this parameter; we use the variable \\( \\theta \\) to denote our estimate. We would <b>like</b> to use the collected data on tips to determine the value that \\( \\theta \\) should have,. To precisely decide which value of \\( \\theta \\) is best, we define a <b>loss</b> function.A <b>loss</b> function is a mathematical function that takes in an estimate \\( \\theta \\) and ...", "dateLastCrawled": "2021-12-23T18:51:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Why <b>is squared hinge loss differentiable? - Quora</b>", "url": "https://www.quora.com/Why-is-squared-hinge-loss-differentiable", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Why-<b>is-squared-hinge-loss-differentiable</b>", "snippet": "Answer (1 of 4): Let\u2019s start by defining the <b>hinge</b> <b>loss</b> function h(x) = max(1-x,0). Now let\u2019s think about the derivative h\u2019(x). This does not exist at x = 1 because the left and right limits do not converge to the same <b>number</b> (ie: the derivative is undefined at x=1, but it is -1 for x&lt;1 and 0 fo...", "dateLastCrawled": "2022-02-03T02:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Loss</b> Functions. <b>Loss</b> functions explanations and\u2026 | by Tomer Kordonsky ...", "url": "https://medium.com/artificialis/loss-functions-361b2ad439a0", "isFamilyFriendly": true, "displayUrl": "https://medium.com/artificialis/<b>loss</b>-functions-361b2ad439a0", "snippet": "The <b>Hinge</b> <b>Loss</b> Equation def <b>Hinge</b>(yhat, y): return np.max(0,1 - yhat * y) Where y is the actual label (-1 or 1) and \u0177 is the prediction; The <b>loss</b> is 0 when the signs of the labels and prediction ...", "dateLastCrawled": "2022-02-03T02:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Probabilistic Soft Logic</b> | <b>Probabilistic soft logic</b> (PSL) is a machine ...", "url": "https://psl.linqs.org/wiki/master/Rule-Specification.html", "isFamilyFriendly": true, "displayUrl": "https://psl.linqs.org/wiki/master/Rule-Specification.html", "snippet": "<b>Squaring</b>. Any weighted rule can choose to square their <b>hinge</b>-<b>loss</b> functions. <b>Squaring</b> the <b>hinge</b>-<b>loss</b> (or \u201c<b>squared</b> potentials\u201d) may result in better performance. Non-<b>squared</b> potentials tend to encourage a \u201cwinner take all\u201d optimization, while <b>squared</b> potentials encourage more trading off. To square a rule, just suffix a ^2 to it:", "dateLastCrawled": "2022-01-31T18:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Loss</b> Functions in Machine Learning | G. Wu", "url": "https://guangyuwu.wordpress.com/2021/02/02/loss-functions-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://guangyuwu.wordpress.com/2021/02/02/<b>loss</b>-functions-in-machine-learning", "snippet": "<b>Hinge</b> <b>Loss</b>. <b>Hinge</b> <b>loss</b> is primarily used with Support Vector Machine (SVM) Classifiers with class labels -1 and 1. So make sure you change the label of the \u2018Malignant\u2019 class in the dataset from 0 to -1. <b>Hinge</b> <b>Loss</b> not only penalizes the wrong predictions but also the right predictions that are not confident.", "dateLastCrawled": "2022-01-12T02:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Common Loss functions and their uses - quick</b> note - Petamind", "url": "https://petamind.com/common-loss-functions-and-their-use-quick-note/", "isFamilyFriendly": true, "displayUrl": "https://petamind.com/<b>common-loss-functions-and-their</b>-use-quick-note", "snippet": "<b>Hinge</b> <b>Loss</b>/Multi-class SVM <b>Loss</b>. In simple terms, the score of the correct category should be greater than the sum of scores of all incorrect categories by some safety margin (usually one). And hence <b>hinge</b> <b>loss</b> is used for maximum-margin classification, most notably for support vector machines. Although not differentiable, it\u2019s a convex function which makes it easy to work with usual convex optimizers used in the machine learning domain. Mathematical formulation: Consider an example where ...", "dateLastCrawled": "2022-01-21T14:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "How to <b>Choose Loss Functions When Training Deep Learning</b> Neural ...", "url": "https://www.aiproblog.com/index.php/2019/01/29/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://www.aiproblog.com/index.php/2019/01/29/how-to-choose-<b>loss</b>-functions-when...", "snippet": "If using a <b>hinge</b> <b>loss</b> does result in better performance on a given binary classification problem, is likely that a <b>squared</b> <b>hinge</b> <b>loss</b> may be appropriate. As with using the <b>hinge</b> <b>loss</b> function, the target variable must be modified to have values in the set {-1, 1}. # change y from {0,1} to {-1,1} y[where(y == 0)] = -1", "dateLastCrawled": "2022-01-04T21:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "An overview of the <b>Gradient Descent</b> algorithm | by Nishit Jain ...", "url": "https://towardsdatascience.com/an-overview-of-the-gradient-descent-algorithm-8645c9e4de1e", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/an-overview-of-the-<b>gradient-descent</b>-algorithm-8645c9e4de1e", "snippet": "SVM <b>Loss</b> (<b>Hinge</b> <b>Loss</b>) Learning Rate: ... So, in order to keep the value of cost function &gt;=0, we are <b>squaring</b> it up. We could have done the same using absolute but there are two main reasons why we are not doing so. Putting absolutes instead of <b>squaring</b> it would penalize the model equally for high and low residuals, whereas we need a weighted penalizing rule where the data points with higher residuals are penalized more and lower ones less. That can be simply achieved by <b>squaring</b> the ...", "dateLastCrawled": "2022-02-01T20:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "How to Choose <b>Loss</b> Functions When Training Deep Learning Neural Networks", "url": "https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/how-to-choose-<b>loss</b>-functions-when-training-deep...", "snippet": "<b>Squared</b> <b>Hinge</b> <b>Loss</b>; Multi-Class Classification <b>Loss</b> Functions Multi-Class Cross-Entropy <b>Loss</b>; Sparse Multiclass Cross-Entropy <b>Loss</b> ; Kullback Leibler Divergence <b>Loss</b>; We will focus on how to choose and implement different <b>loss</b> functions. For more theory on <b>loss</b> functions, see the post: <b>Loss</b> and <b>Loss</b> Functions for Training Deep Learning Neural Networks; Regression <b>Loss</b> Functions. A regression predictive modeling problem involves predicting a real-valued quantity. In this section, we will ...", "dateLastCrawled": "2022-02-02T15:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A Brief Overview of <b>Loss</b> Functions in <b>Pytorch</b> | by Pratyaksha Jha ...", "url": "https://medium.com/udacity-pytorch-challengers/a-brief-overview-of-loss-functions-in-pytorch-c0ddb78068f7", "isFamilyFriendly": true, "displayUrl": "https://medium.com/udacity-<b>pytorch</b>-challengers/a-brief-overview-of-<b>loss</b>-functions-in-p...", "snippet": "<b>Hinge</b> Embedding <b>Loss</b>. torch.nn.HingeEmbeddingLoss. Measures the <b>loss</b> given an input tensor x and a labels tensor y containing values (1 or -1). It is used for measuring whether two inputs are ...", "dateLastCrawled": "2022-02-02T14:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Understanding the 3 most common <b>loss</b> functions for Machine Learning ...", "url": "https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-the-3-most-common-<b>loss</b>-functions-for...", "snippet": "A <b>loss function</b> in Machine Learning is a measure of how accurately your ML model is able to predict the expected outcome i.e the ground truth. The <b>loss function</b> will take two items as input: the\u2026 Get started. Open in app. Sign in. Get started. Follow. 618K Followers \u00b7 Editors&#39; Picks Features Deep Dives Grow Contribute. About. Get started. Open in app. Understanding the 3 most common <b>loss</b> functions for Machine Learning Regression. George Seif. May 20, 2019 \u00b7 5 min read. \u2b50\ufe0f If you love ...", "dateLastCrawled": "2022-02-02T13:52:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Loss</b> Functions. <b>Loss</b> functions explanations and\u2026 | by Tomer Kordonsky ...", "url": "https://medium.com/artificialis/loss-functions-361b2ad439a0", "isFamilyFriendly": true, "displayUrl": "https://medium.com/artificialis/<b>loss</b>-functions-361b2ad439a0", "snippet": "The <b>Hinge</b> <b>Loss</b> Equation def <b>Hinge</b>(yhat, y): return np.max(0,1 - yhat * y) Where y is the actual label (-1 or 1) and \u0177 is the prediction; The <b>loss</b> is 0 when the signs of the labels and prediction ...", "dateLastCrawled": "2022-02-03T02:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Hinge</b>-<b>Loss</b> <b>Markov Random Fields and Probabilistic Soft Logic</b> | DeepAI", "url": "https://deepai.org/publication/hinge-loss-markov-random-fields-and-probabilistic-soft-logic", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>hinge</b>-<b>loss</b>-<b>markov-random-fields-and-probabilistic-soft</b>...", "snippet": "By using <b>hinge</b>-<b>loss</b> functions to model the dependencies among the variables, which admit highly scalable inference without restrictions on their connectivity structure, they <b>can</b> capture a very wide range of useful relationships. One reason they are so expressive is that <b>hinge</b>-<b>loss</b> dependencies are at the core of <b>a number</b> of scalable techniques for modeling both discrete and continuous structured data.", "dateLastCrawled": "2022-01-03T22:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "cs231n/Lecture3_en.srt at master \u00b7 aikorea/cs231n \u00b7 GitHub", "url": "https://github.com/aikorea/cs231n/blob/master/captions/En/Lecture3_en.srt", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/aikorea/cs231n/blob/master/captions/En/Lecture3_en.srt", "snippet": "<b>squared</b> <b>hinge</b> <b>loss</b> instead of the one on: top which recall <b>hinge</b> <b>loss</b> and you <b>can</b>: 177: 00:12:57,529--&gt; 00:13:01,480: use two different kind of hyper: primarily 20 use most often you see the: 178: 00:13:01,480--&gt; 00:13:04,750: first formulation that&#39;s what we use: most of the time but sometimes you <b>can</b>: 179: 00:13:04,750--&gt; 00:13:07,950: see these assets with the square inch: <b>loss</b> and better so that&#39;s something you: 180: 00:13:07,950--&gt; 00:13:12,550: play with that&#39;s really hyper primer but ...", "dateLastCrawled": "2021-09-11T14:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "MITOCW | 33. <b>Neural Nets and the Learning Function</b>", "url": "https://ocw.mit.edu/courses/mathematics/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/video-lectures/lecture-33-neural-nets-and-the-learning-function/L3-WFKCW-tY.pdf", "isFamilyFriendly": true, "displayUrl": "https://<b>ocw.mit.edu</b>/courses/mathematics/18-065-matrix-methods-in-data-analysis-signal...", "snippet": "One would be the one we know best, square <b>loss</b>, and <b>number</b> two, I&#39;ve never seen it used quite this directly, would be the l1 <b>loss</b>, maybe the sum of L1 norms. This is sum of these errors <b>squared</b> in the L2 norm. The L1 <b>loss</b> could be the sum over i of the L1 losses. Well, this comes into specific other problems like Lasso and other important problems you&#39;re minimizing an L1 norm but not in deep learning. Now, and three would be <b>Hinge</b> <b>loss</b>. Probably some of you know better than I the formula and ...", "dateLastCrawled": "2021-09-15T22:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "econometrics - Why do we usually choose to <b>minimize</b> the sum of square ...", "url": "https://stats.stackexchange.com/questions/135103/why-do-we-usually-choose-to-minimize-the-sum-of-square-errors-sse-when-fitting", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/135103", "snippet": "$\\begingroup$ This answer fit into my mindset. But I still have a question, what do you mean by &#39;they don&#39;t determine the criterion&#39;? Does this mean that e.g., in econometric 101 in linear regression, under the functional (no distributional) assumption, in order to get the consistent estimator, you got to use ols, you <b>can</b>&#39;t use some arbitary objective function to <b>minimize</b>, since no guarantee for deriving consistent estimator from there? $\\endgroup$ \u2013 KevinKim", "dateLastCrawled": "2022-01-26T06:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "important inputs and become nearly invariant to the noisy ... - Course Hero", "url": "https://www.coursehero.com/file/p6v99tc/important-inputs-and-become-nearly-invariant-to-the-noisy-inputs-In-comparison/", "isFamilyFriendly": true, "displayUrl": "https://www.coursehero.com/file/p6v99tc/important-inputs-and-become-nearly-invariant...", "snippet": "<b>Loss</b> functions We have discussed the regularization <b>loss</b> part of the objective, which <b>can</b> be seen as penalizing some measure of complexity of the model. The second part of an objective is the data <b>loss</b> , which in a supervised learning problem measures the compatibility between a prediction (e.g. the class scores in classification) and the ground truth label.", "dateLastCrawled": "2022-01-17T18:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A leak is a situation where a variable collected in historical data ...", "url": "https://www.coursehero.com/file/phqgnfa/A-leak-is-a-situation-where-a-variable-collected-in-historical-data-gives/", "isFamilyFriendly": true, "displayUrl": "https://www.coursehero.com/file/phqgnfa/A-leak-is-a-situation-where-a-variable...", "snippet": "Frequency based estimate of a class probability: assign the same class probability to every member of the segment corresponding to a tree leaf, we <b>can</b> use instance counts at each leaf to compute a class probability estimate. Laplace correction: moderates the influence of leaves with only a few instances p(c) = n + 1 / n + m + 2 Where n is the <b>number</b> of examples in the leave belonging to class c, and m is the <b>number</b> of examples not belonging to class c As the <b>number</b> of instances increases ...", "dateLastCrawled": "2021-12-04T04:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Wrist unhinge/<b>arm rotation - my biggest confusion about the golf</b> swing ...", "url": "https://forums.golfwrx.com/topic/493554-wrist-unhingearm-rotation-my-biggest-confusion-about-the-golf-swing/", "isFamilyFriendly": true, "displayUrl": "https://forums.golfwrx.com/topic/493554-wrist-un<b>hingearm-rotation-my-biggest-confusion</b>...", "snippet": "if only <b>hinge</b> and unhinge, will have a few problem.. 1 its difficult to be on plane,need a great pivot , flexible atheletic body. 2nd <b>loss</b> of power cause you lose power for centrifugal + kinetic+ wrist speed 3rdly Low point may be tougher to control for some type of swings. especially short games.", "dateLastCrawled": "2022-02-01T19:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Perceptron Algorithm - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/computer-science/perceptron-algorithm", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/computer-science/perceptron-algorithm", "snippet": "This measure <b>can</b> be used as the <b>number</b> of votes given to the weight vector, ... The first <b>thought</b> that comes into mind is to consider the generalization of the gradient descent method, which was introduced in Chapter 5, and replace the gradient by the subgradient operation. The resulting scheme is known as the subgradient algorithm [74,75]. Starting from an arbitrary estimate, \u03b8 (0) \u2208 R l, the update recursions become (8.51) where J \u2032 denotes any subgradient of the cost function, and \u03bc ...", "dateLastCrawled": "2022-01-09T06:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "JAIC 2003, Volume 42, <b>Number</b> 3, Article 7 (pp. 463 to 477)", "url": "https://cool.culturalheritage.org/jaic/articles/jaic42-03-007.html", "isFamilyFriendly": true, "displayUrl": "https://cool.culturalheritage.org/jaic/articles/jaic42-03-007.html", "snippet": "The first layer was prepared by <b>squaring</b> three sheets of mian liao and cutting them 10 cm wider than the scroll. The three sheets were placed on the table and misted with water. Thin consistency paste was evenly applied all over one sheet with the water brush, carefully avoiding creasing the paper. A second sheet was misted with water and joined 3 mm onto the first one. The join", "dateLastCrawled": "2021-12-13T09:16:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Why <b>is squared hinge loss differentiable? - Quora</b>", "url": "https://www.quora.com/Why-is-squared-hinge-loss-differentiable", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Why-<b>is-squared-hinge-loss-differentiable</b>", "snippet": "Answer (1 of 4): Let\u2019s start by defining the <b>hinge</b> <b>loss</b> function h(x) = max(1-x,0). Now let\u2019s think about the derivative h\u2019(x). This does not exist at x = 1 because the left and right limits do not converge to the same <b>number</b> (ie: the derivative is undefined at x=1, but it is -1 for x&lt;1 and 0 fo...", "dateLastCrawled": "2022-02-03T02:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "How to Choose <b>Loss</b> Functions When Training Deep Learning Neural Networks", "url": "https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/how-to-choose-<b>loss</b>-functions-when-training-deep...", "snippet": "<b>Squared</b> <b>Hinge</b> <b>Loss</b>; Multi-Class Classification <b>Loss</b> Functions Multi-Class Cross-Entropy <b>Loss</b>; Sparse Multiclass Cross-Entropy <b>Loss</b>; Kullback Leibler Divergence <b>Loss</b> ; We will focus on how to choose and implement different <b>loss</b> functions. For more theory on <b>loss</b> functions, see the post: <b>Loss</b> and <b>Loss</b> Functions for Training Deep Learning Neural Networks; Regression <b>Loss</b> Functions. A regression predictive modeling problem involves predicting a real-valued quantity. In this section, we will ...", "dateLastCrawled": "2022-02-02T15:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Common <b>Loss</b> functions in machine learning | by Ravindra Parmar ...", "url": "https://towardsdatascience.com/common-loss-functions-in-machine-learning-46af0ffc4d23", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/common-<b>loss</b>-functions-in-machine-learning-46af0ffc4d23", "snippet": "<b>Hinge</b> <b>Loss</b>/Multi class SVM <b>Loss</b>. In simple terms, the score of correct category should be greater than sum of scores of all incorrect categories by some safety margin (usually one). And hence <b>hinge</b> <b>loss</b> is used for maximum-margin classification, most notably for support vector machines.", "dateLastCrawled": "2022-02-03T02:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Loss</b> Functions in Machine Learning | G. Wu", "url": "https://guangyuwu.wordpress.com/2021/02/02/loss-functions-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://guangyuwu.wordpress.com/2021/02/02/<b>loss</b>-functions-in-machine-learning", "snippet": "<b>Hinge</b> <b>Loss</b>. <b>Hinge</b> <b>loss</b> is primarily used with Support Vector Machine (SVM) Classifiers with class labels -1 and 1. So make sure you change the label of the \u2018Malignant\u2019 class in the dataset from 0 to -1. <b>Hinge</b> <b>Loss</b> not only penalizes the wrong predictions but also the right predictions that are not confident.", "dateLastCrawled": "2022-01-12T02:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Common Loss functions and their uses - quick</b> note - Petamind", "url": "https://petamind.com/common-loss-functions-and-their-use-quick-note/", "isFamilyFriendly": true, "displayUrl": "https://petamind.com/<b>common-loss-functions-and-their</b>-use-quick-note", "snippet": "<b>Hinge</b> <b>Loss</b>/Multi-class SVM <b>Loss</b>. In simple terms, the score of the correct category should be greater than the sum of scores of all incorrect categories by some safety margin (usually one). And hence <b>hinge</b> <b>loss</b> is used for maximum-margin classification, most notably for support vector machines. Although not differentiable, it\u2019s a convex function which makes it easy to work with usual convex optimizers used in the machine learning domain. Mathematical formulation: Consider an example where ...", "dateLastCrawled": "2022-01-21T14:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Most Common <b>Loss</b> Functions in Machine Learning | by Sparsh Gupta ...", "url": "https://towardsdatascience.com/most-common-loss-functions-in-machine-learning-c7212a99dae0", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/most-common-<b>loss</b>-functions-in-machine-learning-c7212a99dae0", "snippet": "The Cross-Entropy <b>Loss</b> formula is derived from the regular likelihood function, but with logarithms added in. 2. <b>Hinge</b> <b>Loss</b>. The second most common <b>loss function</b> used for Classification problems and an alternative to Cross-Entropy <b>loss function</b> is <b>Hinge</b> <b>Loss</b>, primarily developed for Support Vector Machine (SVM) model evaluation.", "dateLastCrawled": "2022-02-03T02:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Multi-class SVM Loss - PyImageSearch</b>", "url": "https://www.pyimagesearch.com/2016/09/05/multi-class-svm-loss/", "isFamilyFriendly": true, "displayUrl": "https://www.pyimagesearch.com/2016/09/05/<b>multi-class-svm-loss</b>", "snippet": "The <b>squared</b> term penalizes our <b>loss</b> more heavily by <b>squaring</b> the output. This leads to a quadratic growth in <b>loss</b> rather than a linear one. As for which <b>loss</b> function you should use, that is entirely dependent on your dataset. It\u2019s typical to see the standard <b>hinge</b> <b>loss</b> function used more often, but on some datasets the <b>squared</b> variation might obtain better accuracy \u2014 overall, this is a hyperparameter that you should cross-validate. A <b>Multi-class SVM loss</b> example. Now that we\u2019ve taken ...", "dateLastCrawled": "2022-02-02T22:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A Brief Overview of <b>Loss</b> Functions in <b>Pytorch</b> | by Pratyaksha Jha ...", "url": "https://medium.com/udacity-pytorch-challengers/a-brief-overview-of-loss-functions-in-pytorch-c0ddb78068f7", "isFamilyFriendly": true, "displayUrl": "https://medium.com/udacity-<b>pytorch</b>-challengers/a-brief-overview-of-<b>loss</b>-functions-in-p...", "snippet": "<b>Hinge</b> Embedding <b>Loss</b>. torch.nn.HingeEmbeddingLoss. Measures the <b>loss</b> given an input tensor x and a labels tensor y containing values (1 or -1). It is used for measuring whether two inputs are ...", "dateLastCrawled": "2022-02-02T14:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>What Are Different Loss Functions Used as Optimizers</b> in Neural Networks ...", "url": "https://www.analyticssteps.com/blogs/what-are-different-loss-functions-used-optimizers-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://www.analyticssteps.com/blogs/<b>what-are-different-loss-functions-used</b>-optimizers...", "snippet": "<b>Loss</b> functions are mainly classified into two different categories that are Classification <b>loss</b> and Regression <b>Loss</b>. Classification <b>loss</b> is the case where the aim is to predict the output from the different categorical values for example, if we have a dataset of handwritten images and the digit is to be predicted that lies between (0-9), in these kinds of scenarios classification <b>loss</b> is used.", "dateLastCrawled": "2022-02-02T14:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Loss</b> functions.pptx - <b>Loss</b> functions Model Selection and Evaluation ...", "url": "https://www.coursehero.com/file/84993906/Loss-functionspptx/", "isFamilyFriendly": true, "displayUrl": "https://www.coursehero.com/file/84993906/<b>Loss</b>-functionspptx", "snippet": "<b>Hinge</b> <b>Loss</b> not only penalizes wrong predictions but also right predictions that are not confident. <b>Hinge</b> <b>loss</b> for an input-output pair (x, y) is : After running the update function for 2000 iterations with three different values of alpha, we obtain this plot used when we want to make real-time decisions with not a laser-sharp focus on accuracy", "dateLastCrawled": "2021-12-30T22:40:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Introduction to Machine Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/24/introduction-to-machine-learning-2/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/24/<b>introduction-to-machine-learning</b>-2", "snippet": "<b>Squared</b> <b>loss</b> (for regression) <b>Hinge</b> <b>loss</b> (SVM) Logistic/log <b>loss</b> (logistic regression) Some <b>loss</b> functions are as follows: When to stop tuning <b>machine</b> <b>learning</b> models. When to stop tuning the hyperparameters in a <b>machine</b> <b>learning</b> model is a million-dollar question. This problem can be mostly solved by keeping tabs on training and testing errors. While increasing the complexity of a model, the following stages occur: Stage 1: Underfitting stage \u2013 high train and high test errors (or low ...", "dateLastCrawled": "2022-01-17T05:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Data Science: <b>Support Vector Machines (SVM</b>)", "url": "https://www.datasciencesmachinelearning.com/2019/01/support-vector-machines-svm.html", "isFamilyFriendly": true, "displayUrl": "https://www.datasciences<b>machinelearning</b>.com/2019/01/<b>support-vector-machines-svm</b>.html", "snippet": "In this case, <b>squared</b> <b>hinge</b> <b>loss</b> function (as against <b>hinge</b> <b>loss</b> function) and l2 penalty are the major changes compared to the earlier three methods. This method is useful for when sample size is larger.", "dateLastCrawled": "2022-01-28T08:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Introduction to <b>Machine</b> <b>Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/30/introduction-to-machine-learning-3/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/30/introduction-to-<b>machine</b>-<b>learning</b>-3", "snippet": "However, in <b>machine</b> <b>learning</b> methodology, <b>squared</b> <b>loss</b> will be minimized with respect to ... <b>Squared</b> <b>loss</b> (for regression) <b>Hinge</b> <b>loss</b> (SVM) Logistic/log <b>loss</b> (logistic regression) Some <b>loss</b> functions are as follows: When to stop tuning <b>machine</b> <b>learning</b> models. When to stop tuning the hyperparameters in a <b>machine</b> <b>learning</b> model is a million-dollar question. This problem can be mostly solved by keeping tabs on training and testing errors. While increasing the complexity of a model, the ...", "dateLastCrawled": "2022-01-28T18:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A study on L2-<b>loss (Squared Hinge-Loss) multiclass SVM</b> | Request PDF", "url": "https://www.researchgate.net/publication/235884495_A_study_on_L2-loss_Squared_Hinge-Loss_multiclass_SVM", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/235884495_A_study_on_L2-<b>loss</b>_<b>Squared</b>_<b>Hinge</b>...", "snippet": "Taking the <b>analogy</b> to classification task, it has been previously studied [13] that using the <b>squared</b> <b>hinge</b> <b>loss</b> in SVM would yield better accuracy when \u03bb is large. In this case, underfitting ...", "dateLastCrawled": "2021-12-14T01:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Metrics to Evaluate Classification and Regression Algorithms | by ...", "url": "https://medium.com/@poojitha.penmethsa/metrics-to-evaluate-classification-and-regression-algorithms-1554f1e00a75", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@poojitha.penmethsa/metrics-to-evaluate-classification-and...", "snippet": "<b>Hinge</b> <b>loss</b>. cross-entropy <b>loss</b> / log <b>loss</b>. likelihood <b>loss</b>. MSE / Quadratic <b>loss</b> / L2 <b>loss</b>: Mean <b>Squared</b> Error, or MSE <b>loss</b> is the default <b>loss</b> to use for regression problems. Mathematically, it ...", "dateLastCrawled": "2022-01-17T01:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Understanding the 3 most common <b>loss</b> functions for <b>Machine</b> <b>Learning</b> ...", "url": "https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-the-3-most-common-<b>loss</b>-functions-for...", "snippet": "A <b>loss function</b> in <b>Machine</b> <b>Learning</b> is a measure of how accurately your ML model is able to predict the expected outcome i.e the ground truth. The <b>loss function</b> will take two items as input: the output value of our model and the ground truth expected value. The output of the <b>loss function</b> is called the <b>loss</b> which is a measure of how well our model did at predicting the outcome. A high value for the <b>loss</b> means our model performed very poorly. A low value for the <b>loss</b> means our model performed ...", "dateLastCrawled": "2022-02-02T13:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and-sample-codes ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/README.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "Models 1.1 Support vector <b>machine</b> 1.1.1 Principle 1.1.2 Kernel 1.1.3 Soft margin SVM 1.1.4 <b>Hinge</b> <b>loss</b> view 1.1.5 Multi-class SVM 1.1.6 Extensions 1.2 Tree-based models 1.2.1 Decision tree 1.2.2 Random forest 1.2.3 Gradient boosted decision trees 1.2.4 Tools 1.3 EM Principle 1.4 MaxEnt 1.4.1 Entropy 1.5 Model selection 1.5.1 Under-fitting / Over-fitting 1.5.2 Model ensemble, sklearn 2.", "dateLastCrawled": "2021-09-22T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Fast generalization rates for distance metric</b> <b>learning</b> - <b>Machine</b> <b>Learning</b>", "url": "https://link.springer.com/article/10.1007/s10994-018-5734-0", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10994-018-5734-0", "snippet": "For example, the <b>squared</b> <b>hinge</b> <b>loss</b> \\(\\ell _s^1(x) = \\max (1-x, 0) ... <b>Analogy</b>-preserving semantic embedding for visual object categorization. In Proceedings of the 30th international conference on <b>machine</b> <b>learning</b>, Atlanta, GA (pp. 639\u2013647). Jin, R., Wang, S., &amp; Zhou, Y. (2010). Regularized distance metric <b>learning</b>: Theory and algorithm. Advances in neural information processing systems (Vol. 23, pp. 862\u2013870). Cambridge, MA: MIT Press. Google Scholar Kulis, B. (2012). Metric <b>learning</b>: A ...", "dateLastCrawled": "2021-12-28T14:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>MLP for regression with TensorFlow 2 and</b> Keras \u2013 MachineCurve", "url": "https://www.machinecurve.com/index.php/2019/07/30/creating-an-mlp-for-regression-with-keras/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/index.php/2019/07/30/creating-an-mlp-for-regression-with...", "snippet": "Last Updated on 30 March 2021. <b>Machine</b> <b>learning</b> is a wide field and <b>machine</b> <b>learning</b> problems come in many flavors. If, say, you wish to group data based on similarities, you would choose an unsupervised approach called clustering.If you have a fixed number of classes which you wish to assign new data to, you\u2019ll choose a supervised approach named classification.If, however, you don\u2019t have a fixed number, but wish to estimate a real value \u2013 your approach will still be supervised, but ...", "dateLastCrawled": "2022-02-03T02:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "CPSC 340: Data Mining <b>Machine</b> <b>Learning</b>", "url": "https://www.cs.ubc.ca/~schmidtm/Courses/340-F16/L35.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.ubc.ca/~schmidtm/Courses/340-F16/L35.pdf", "snippet": "<b>Machine</b> <b>Learning</b> and Data Mining Course Review/Preview Fall 2016 Some images from this lecture are taken from Google Image Search. Admin \u2022Assignment 6: \u20131 late day to hand in next Monday, 2 for Wednesday, 3 for Friday. \u2022Final: \u2013December 12 (8:30am \u2013HEBB 100) \u2013Covers Assignments 1-6. \u2013List of topics posted. \u2013Final from last year will be posted after class. \u2013Closed-book, cheat sheet: 4-pages each double-sided. Last Time: Semi-Supervised <b>Learning</b> \u2022In semi-supervised <b>learning</b> ...", "dateLastCrawled": "2021-11-22T01:45:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>About loss and loss functions</b> \u2013 MachineCurve", "url": "https://www.machinecurve.com/index.php/2019/10/04/about-loss-and-loss-functions/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/index.php/2019/10/04/<b>about-loss-and-loss-functions</b>", "snippet": "We\u2019re then using <b>machine</b> <b>learning</b> for ... The <b>squared hinge loss is like</b> the hinge formula displayed above, but then the \\(max()\\) function output is squared. This helps achieving two things: Firstly, it makes the loss value more sensitive to outliers, just as we saw with MSE vs MAE. Large errors will add to the loss more significantly than smaller errors. Note that simiarly, this may also mean that you\u2019ll need to inspect your dataset for the presence of such outliers first. Secondly ...", "dateLastCrawled": "2022-01-25T13:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "\u635f\u5931\u51fd\u6570 - \u7b97\u6cd5\u6742\u8d27\u94fa - bjmsong.github.io", "url": "https://bjmsong.github.io/2020/02/21/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/", "isFamilyFriendly": true, "displayUrl": "https://bjmsong.github.io/2020/02/21/\u635f\u5931\u51fd\u6570", "snippet": "the training data is fed into the <b>machine</b> <b>learning</b> model; Loss : compare between some actual targets and predicted targets; the lower the loss, the more the set of targets and the set of predictions resemble each other; the more they resemble each other, the better the <b>machine</b> <b>learning</b> model performs. Backward pass", "dateLastCrawled": "2021-12-27T11:43:00.0000000Z", "language": "zh_chs", "isNavigational": false}], [], [], [], []], "all_bing_queries": ["+(squared hinge loss)  is like +(squaring a number)", "+(squared hinge loss) is similar to +(squaring a number)", "+(squared hinge loss) can be thought of as +(squaring a number)", "+(squared hinge loss) can be compared to +(squaring a number)", "machine learning +(squared hinge loss AND analogy)", "machine learning +(\"squared hinge loss is like\")", "machine learning +(\"squared hinge loss is similar\")", "machine learning +(\"just as squared hinge loss\")", "machine learning +(\"squared hinge loss can be thought of as\")", "machine learning +(\"squared hinge loss can be compared to\")"]}