{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Short Term Gains, Long Term Pains: How Cues About State Aid Learning in ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2783717/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC2783717", "snippet": "As the <b>value</b> of the weight connecting C to the output units is increased, it allows the model to predict that the estimated <b>value</b> of an <b>action</b> (calculated at the output node) will increase as a <b>function</b> of the current state (i.e., states with larger <b>value</b> on input C result in higher estimated values of <b>reward</b>). The linear input weight allows the model to \u201cextrapolate\u201d learned values from one region of the state space to (as-of-yet) unexperienced states, rapidly improving performance in ...", "dateLastCrawled": "2021-12-02T23:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Beyond Winning and Losing: Modeling Human Motivations and ... - DeepAI", "url": "https://deepai.org/publication/beyond-winning-and-losing-modeling-human-motivations-and-behaviors-using-inverse-reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/beyond-winning-and-losing-modeling-human-motivations...", "snippet": "Such approximation should be a fair estimation of the cumulative <b>reward</b> the user <b>would receive</b> if the user chooses an <b>action</b> a at the state s and maximizes the i -th <b>reward</b> thereafter. DQN uses the recursive property (i.e. Bellman equation) that the <b>action</b>-<b>value</b> estimator should have, that is, the cumulative <b>reward</b> since the current step onwards should be the immediate <b>reward</b> plus the cumulative <b>reward</b> since the next step onwards. Using the property, DQN updates the <b>action</b>-<b>value</b> <b>function</b> ...", "dateLastCrawled": "2021-12-23T20:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Reinforcement learning for cyber-physical systems with cybersecurity ...", "url": "https://dokumen.pub/reinforcement-learning-for-cyber-physical-systems-with-cybersecurity-case-studies-9781351006590-1351006592-9781351006606-1351006606-9781351006613-1351006614-9781351006620-1351006622-9781138543539.html", "isFamilyFriendly": true, "displayUrl": "https://dokumen.pub/reinforcement-learning-for-cyber-physical-systems-with-cyber...", "snippet": "<b>Value</b> functions: In a reinforcement learning problem, each state or <b>state-action</b> pair is associated with a <b>value</b> <b>function</b> that serves as the input argument for finding policy for each state. A <b>value</b> <b>function</b> is the accumulated rewards in a long run. Two kinds of <b>value</b> functions are usually used in reinforcement learning: statevalue <b>function</b> V (s) and <b>action</b>-<b>value</b> <b>function</b> Q(s, a). V (s) is defined as the total amount of rewards an agent will accumulate over the future if starting from that ...", "dateLastCrawled": "2022-01-23T02:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "ML Intro 6: Reinforcement Learning for <b>non-Differentiable</b> Functions ...", "url": "https://towardsdatascience.com/ml-intro-6-reinforcement-learning-for-non-differentiable-functions-c75e1464c6b9", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/ml-intro-6-reinforcement-learning-for-non...", "snippet": "So the last <b>action</b> before <b>the reward</b> should <b>receive</b> <b>the reward</b>. And the <b>action</b> n steps before <b>the reward</b> was given should <b>receive</b> less <b>reward</b>, \u0194^n where every previous step receives only (0 &lt; \u0194 &lt; 1) as much <b>reward</b> as the following. In this way, if you make a horrible mistake that leads you to a sudden loss, that gets a more negative signal than playing, and 20 moves later losing. Alternative Approach: <b>Value</b> Functions. What we discussed above is called policy <b>function</b> optimization. We have ...", "dateLastCrawled": "2022-01-31T16:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Recent advances in leveraging human guidance for sequential decision ...", "url": "https://link.springer.com/article/10.1007/s10458-021-09514-w", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10458-021-09514-w", "snippet": "The state <b>value</b> <b>function</b> \\(V^\\pi (s)\\) measures the expected cumulative <b>reward</b> to be in <b>a particular</b> state s and following policy \\(\\pi\\) afterward. The <b>action</b>-<b>value</b> <b>function</b> \\(Q^\\pi (s,a)\\) defines the same quantity but <b>for taking</b> <b>a particular</b> <b>action</b> a when in state s and following policy \\(\\pi\\) afterward. The advantage <b>function</b> tells us the relative gain (\u201cadvantage&quot;) that could be obtained by <b>taking</b> a certain <b>action</b> compared to the average <b>action</b> taken at that state [].Several ...", "dateLastCrawled": "2022-01-31T18:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Hands-on Reinforcement Learning with Python. Master ... - <b>DOKUMEN.PUB</b>", "url": "https://dokumen.pub/hands-on-reinforcement-learning-with-python-master-reinforcement-and-deep-reinforcement-learning-using-openai-gym-and-tensorflow-978-1-78883-652-4.html", "isFamilyFriendly": true, "displayUrl": "https://<b>dokumen.pub</b>/hands-on-reinforcement-learning-with-python-master-reinforcement...", "snippet": "<b>State-action</b> <b>value</b> <b>function</b> (Q <b>function</b>) A <b>state-action</b> <b>value</b> <b>function</b> is also called the Q <b>function</b>. It specifies how good it is for an agent to perform <b>a particular</b> <b>action</b> in a state with a policy \u03c0. The Q <b>function</b> is denoted by Q(s). It denotes the <b>value</b> of <b>taking</b> an <b>action</b> in a state following a policy \u03c0. We can define Q <b>function</b> as follows: [ 46 ] The Markov Decision Process and Dynamic Programming Chapter 3 This specifies the expected return starting from state s with the <b>action</b> a ...", "dateLastCrawled": "2022-02-02T22:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "5 Things You Need to Know about <b>Reinforcement Learning</b>", "url": "https://www.kdnuggets.com/2018/03/5-things-reinforcement-learning.html", "isFamilyFriendly": true, "displayUrl": "https://www.<b>kdnuggets</b>.com/2018/03/5-things-<b>reinforcement-learning</b>.html", "snippet": "<b>Value</b>: Future <b>reward</b> that an agent <b>would receive</b> by <b>taking</b> an <b>action</b> in <b>a particular</b> state. A <b>Reinforcement Learning</b> problem can be best explained through games. Let\u2019s take the <b>game</b> of PacMan where the goal of the agent (PacMan) is to eat the food in the grid while avoiding the ghosts on its way. The grid world is the interactive environment for the agent. PacMan receives a <b>reward</b> for eating food and punishment if it gets killed by the ghost (loses the <b>game</b>). The states are the location of ...", "dateLastCrawled": "2022-02-02T16:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "114 questions with answers in <b>REWARD</b> | Science topic", "url": "https://www.researchgate.net/topic/Reward", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/topic/<b>Reward</b>", "snippet": "<b>The reward</b> <b>function</b> ( in this case the <b>Value</b> V of a specific policy &quot;Pi&quot; is the weighted sum of all the rewards starting from <b>the reward</b> in time t (rt), the weight &quot;Gamma&quot; defines how important ...", "dateLastCrawled": "2022-02-02T00:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "NEUROSTUDIO: <b>Deep Reinforcement Learning with</b> Neural Networks \u2013 Machine ...", "url": "https://unrealai.wordpress.com/2018/05/08/deep-rl-with-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://unrealai.wordpress.com/2018/05/08/deep-rl-with-neural-networks", "snippet": "Initially the Neural Networks prediction about the <b>value</b> of these <b>action</b> given a certain state the agent is in will be random, but after the Agent receives a <b>reward</b>, it can form a more accurate estimate of the values of the previous <b>state action</b> pairs. The difference between its initial <b>value</b> estimates for each <b>action</b> and the new ones obtained after <b>taking</b> an <b>action</b> are used as a loss <b>function</b> to train the Neural network. After many <b>game</b> episodes and many epochs training the neural network ...", "dateLastCrawled": "2022-01-24T22:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(PDF) Policy Gradient With <b>Value Function Approximation For Collective</b> ...", "url": "https://www.researchgate.net/publication/324387640_Policy_Gradient_With_Value_Function_Approximation_For_Collective_Multiagent_Planning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/324387640_Policy_Gradient_With_<b>Value</b>_<b>Function</b>...", "snippet": "Compatible <b>value</b> <b>function</b> for C Dec-POMDPs can be factorized as: f w ( s t, a t) = X. m. f m. w ( s m. t, o ( s m. t, n s t), a m) W e can directly replace Q \u03c0 ( \u00b7) in policy gradient (29) by ...", "dateLastCrawled": "2021-08-27T20:39:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A Beginner&#39;s Guide to Deep <b>Reinforcement Learning</b> | Pathmind", "url": "https://wiki.pathmind.com/deep-reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://wiki.pathmind.com/deep-<b>reinforcement-learning</b>", "snippet": "Q-<b>value</b> or <b>action</b>-<b>value</b> (Q): Q-<b>value</b> <b>is similar</b> to <b>Value</b>, except that it takes an extra parameter, the current <b>action</b> a. Q\u03c0(s, a) refers to the long-term return of an <b>action</b> <b>taking</b> <b>action</b> a under policy \u03c0 from the current state s. Q maps <b>state-action</b> pairs to rewards. Note the difference between Q and policy. Trajectory: A sequence of states and actions that influence those states. From the Latin \u201cto throw across.\u201d The life of an agent is but a ball tossed high and arching through ...", "dateLastCrawled": "2022-02-03T00:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Reinforcement learning for cyber-physical systems with cybersecurity ...", "url": "https://dokumen.pub/reinforcement-learning-for-cyber-physical-systems-with-cybersecurity-case-studies-9781351006590-1351006592-9781351006606-1351006606-9781351006613-1351006614-9781351006620-1351006622-9781138543539.html", "isFamilyFriendly": true, "displayUrl": "https://dokumen.pub/reinforcement-learning-for-cyber-physical-systems-with-cyber...", "snippet": "<b>Value</b> functions: In a reinforcement learning problem, each state or <b>state-action</b> pair is associated with a <b>value</b> <b>function</b> that serves as the input argument for finding policy for each state. A <b>value</b> <b>function</b> is the accumulated rewards in a long run. Two kinds of <b>value</b> functions are usually used in reinforcement learning: statevalue <b>function</b> V (s) and <b>action</b>-<b>value</b> <b>function</b> Q(s, a). V (s) is defined as the total amount of rewards an agent will accumulate over the future if starting from that ...", "dateLastCrawled": "2022-01-23T02:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Hands-on Reinforcement Learning with Python. Master ... - <b>DOKUMEN.PUB</b>", "url": "https://dokumen.pub/hands-on-reinforcement-learning-with-python-master-reinforcement-and-deep-reinforcement-learning-using-openai-gym-and-tensorflow-978-1-78883-652-4.html", "isFamilyFriendly": true, "displayUrl": "https://<b>dokumen.pub</b>/hands-on-reinforcement-learning-with-python-master-reinforcement...", "snippet": "<b>State-action</b> <b>value</b> <b>function</b> (Q <b>function</b>) A <b>state-action</b> <b>value</b> <b>function</b> is also called the Q <b>function</b>. It specifies how good it is for an agent to perform <b>a particular</b> <b>action</b> in a state with a policy \u03c0. The Q <b>function</b> is denoted by Q(s). It denotes the <b>value</b> of <b>taking</b> an <b>action</b> in a state following a policy \u03c0. We can define Q <b>function</b> as follows: [ 46 ] The Markov Decision Process and Dynamic Programming Chapter 3 This specifies the expected return starting from state s with the <b>action</b> a ...", "dateLastCrawled": "2022-02-02T22:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) <b>Reward</b> <b>Function</b> and Initial Values: Better Choices for ...", "url": "https://www.researchgate.net/publication/40619138_Reward_Function_and_Initial_Values_Better_Choices_for_Accelerated_Goal-Directed_Reinforcement_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/40619138_<b>Reward</b>_<b>Function</b>_and_Initial_<b>Values</b>...", "snippet": "The correct <b>action</b> is, in general, unknown, and the <b>reward</b> assesses the agent&#39;s progress in achieving its goal. Designing the <b>reward</b> <b>function</b> is crucial for the success of RL algorithms, and ...", "dateLastCrawled": "2021-11-11T08:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "NEUROSTUDIO: <b>Deep Reinforcement Learning with</b> Neural Networks \u2013 Machine ...", "url": "https://unrealai.wordpress.com/2018/05/08/deep-rl-with-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://unrealai.wordpress.com/2018/05/08/deep-rl-with-neural-networks", "snippet": "Initially the Neural Networks prediction about the <b>value</b> of these <b>action</b> given a certain state the agent is in will be random, but after the Agent receives a <b>reward</b>, it can form a more accurate estimate of the values of the previous <b>state action</b> pairs. The difference between its initial <b>value</b> estimates for each <b>action</b> and the new ones obtained after <b>taking</b> an <b>action</b> are used as a loss <b>function</b> to train the Neural network. After many <b>game</b> episodes and many epochs training the neural network ...", "dateLastCrawled": "2022-01-24T22:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "5 Things You Need to Know about <b>Reinforcement Learning</b>", "url": "https://www.kdnuggets.com/2018/03/5-things-reinforcement-learning.html", "isFamilyFriendly": true, "displayUrl": "https://www.<b>kdnuggets</b>.com/2018/03/5-things-<b>reinforcement-learning</b>.html", "snippet": "<b>Value</b>: Future <b>reward</b> that an agent <b>would receive</b> by <b>taking</b> an <b>action</b> in <b>a particular</b> state. A <b>Reinforcement Learning</b> problem can be best explained through games. Let\u2019s take the <b>game</b> of PacMan where the goal of the agent (PacMan) is to eat the food in the grid while avoiding the ghosts on its way. The grid world is the interactive environment for the agent. PacMan receives a <b>reward</b> for eating food and punishment if it gets killed by the ghost (loses the <b>game</b>). The states are the location of ...", "dateLastCrawled": "2022-02-02T16:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "ML Intro 6: Reinforcement Learning for <b>non-Differentiable</b> Functions ...", "url": "https://towardsdatascience.com/ml-intro-6-reinforcement-learning-for-non-differentiable-functions-c75e1464c6b9", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/ml-intro-6-reinforcement-learning-for-non...", "snippet": "So the last <b>action</b> before the <b>reward</b> should <b>receive</b> the <b>reward</b>. And the <b>action</b> n steps before the <b>reward</b> was given should <b>receive</b> less <b>reward</b>, \u0194^n where every previous step receives only (0 &lt; \u0194 &lt; 1) as much <b>reward</b> as the following. In this way, if you make a horrible mistake that leads you to a sudden loss, that gets a more negative signal than playing, and 20 moves later losing. Alternative Approach: <b>Value</b> Functions. What we discussed above is called policy <b>function</b> optimization. We have ...", "dateLastCrawled": "2022-01-31T16:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "learning.pdf based using reinforcement Check <b>Game</b> artificial intelligence", "url": "https://core.ac.uk/download/pdf/187726168.pdf", "isFamilyFriendly": true, "displayUrl": "https://core.ac.uk/download/pdf/187726168.pdf", "snippet": "<b>action</b>.<b>State-action</b> pair functions that create estimation about the preferable <b>action</b> for agent, is called <b>Value</b> Functions. In this research, we used this following tion[31: \u2014 the <b>value</b> of a state s under policy z. The expected return when starting in s and following zthereafter. Q&quot; (S, a) S, at = a} = Ed a), \u2014the <b>value</b> of <b>taking</b> <b>action</b> a ...", "dateLastCrawled": "2021-08-04T20:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Affordance as general <b>value</b> <b>function</b>: A computational model \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/2010.14289/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2010.14289", "snippet": "The <b>value</b> of an <b>action</b> possibility of a task defined by a specific <b>reward</b> <b>function</b> is the cumulative sum of rewards collected when following \u03c4 (a | s) until termination according to \u03b2 (s), ignoring any actions the agent may take after terminating the option. In order to learn the <b>value</b> of an option, we propose a more general form of the <b>value</b> <b>function</b> that takes into account the termination probability:", "dateLastCrawled": "2021-12-25T14:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "114 questions with answers in <b>REWARD</b> | Science topic", "url": "https://www.researchgate.net/topic/Reward", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/topic/<b>Reward</b>", "snippet": "The <b>reward</b> <b>function</b> ( in this case the <b>Value</b> V of a specific policy &quot;Pi&quot; is the weighted sum of all the rewards starting from the <b>reward</b> in time t (rt), the weight &quot;Gamma&quot; defines how important ...", "dateLastCrawled": "2022-02-02T00:43:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Ensuring Effective Public Health Communication: Insights and Modeling ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8548420/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8548420", "snippet": "V \u03c0 is called the state-<b>value</b> <b>function</b> for a given policy \u03c0. In a similar way, the <b>action</b>-<b>value</b> <b>function</b> for policy \u03c0 defines the <b>value</b> of <b>taking</b> <b>action</b> (a decision) a in state s, under a given policy \u03c0, and <b>can</b> be denoted by Q \u03c0 (s, a), as the expected reinforcing return when starting from s, and <b>taking</b> <b>action</b> a, whilst following a given ...", "dateLastCrawled": "2022-01-24T12:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A survey and critique of multiagent deep <b>reinforcement learning</b> ...", "url": "https://link.springer.com/article/10.1007/s10458-019-09421-1", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10458-019-09421-1", "snippet": "In the case when the critic learns a <b>state-action</b> <b>function</b> (Q <b>function</b>) and a state <b>value</b> <b>function</b> (V <b>function</b>), an advantage <b>function</b> <b>can</b> be computed by subtracting state values from the <b>state-action</b> values [283, 315]. The advantage <b>function</b> indicates the relative quality of an <b>action</b> compared to other available actions computed from the baseline, i.e., state <b>value</b> <b>function</b>. An example of an actor-critic algorithm is Deterministic Policy Gradient (DPG)", "dateLastCrawled": "2022-01-29T11:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Reinforcement learning <b>in the</b> brain - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S0022249608001181", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0022249608001181", "snippet": "1.3.2.. <b>State-action</b> valuesAn alternative to Actor/Critic methods for model-free RL, is to explicitly learn the predictive <b>value</b> (in terms of future expected rewards) of <b>taking</b> a specific <b>action</b> at a certain state, that is, learning the <b>value</b> of the <b>state-action</b> pair, denoted Q (S, a).In his Ph.D. thesis, Watkins (1989) suggested Q-learning as a modification of TD learning that allows one to learn such Q-values (and brings TD learning closer to dynamic programming methods of \u2018policy ...", "dateLastCrawled": "2022-01-06T01:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Beginner&#39;s Guide to Deep <b>Reinforcement Learning</b> | Pathmind", "url": "https://wiki.pathmind.com/deep-reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://wiki.pathmind.com/deep-<b>reinforcement-learning</b>", "snippet": "So you <b>can</b> have states where <b>value</b> and <b>reward</b> diverge: you might <b>receive</b> a low, immediate <b>reward</b> (spinach) even as you move to position with great potential for long-term <b>value</b>; or you might <b>receive</b> a high immediate <b>reward</b> (cocaine) that leads to diminishing prospects over time. This is why the <b>value</b> <b>function</b>, rather than immediate rewards, is what <b>reinforcement learning</b> seeks to predict and control. So environments are functions that transform an <b>action</b> taken in the current state into the ...", "dateLastCrawled": "2022-02-03T00:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Reinforcement learning for cyber-physical systems with cybersecurity ...", "url": "https://dokumen.pub/reinforcement-learning-for-cyber-physical-systems-with-cybersecurity-case-studies-9781351006590-1351006592-9781351006606-1351006606-9781351006613-1351006614-9781351006620-1351006622-9781138543539.html", "isFamilyFriendly": true, "displayUrl": "https://dokumen.pub/reinforcement-learning-for-cyber-physical-systems-with-cyber...", "snippet": "<b>Value</b> functions: In a reinforcement learning problem, each state or <b>state-action</b> pair is associated with a <b>value</b> <b>function</b> that serves as the input argument for finding policy for each state. A <b>value</b> <b>function</b> is the accumulated rewards in a long run. Two kinds of <b>value</b> functions are usually used in reinforcement learning: statevalue <b>function</b> V (s) and <b>action</b>-<b>value</b> <b>function</b> Q(s, a). V (s) is defined as the total amount of rewards an agent will accumulate over the future if starting from that ...", "dateLastCrawled": "2022-01-23T02:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Recent advances in leveraging human guidance for sequential decision ...", "url": "https://link.springer.com/article/10.1007/s10458-021-09514-w", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10458-021-09514-w", "snippet": "The state <b>value</b> <b>function</b> \\(V^\\pi (s)\\) measures the expected cumulative <b>reward</b> to be in <b>a particular</b> state s and following policy \\(\\pi\\) afterward. The <b>action</b>-<b>value</b> <b>function</b> \\(Q^\\pi (s,a)\\) defines the same quantity but <b>for taking</b> <b>a particular</b> <b>action</b> a when in state s and following policy \\(\\pi\\) afterward. The advantage <b>function</b> tells us the relative gain (\u201cadvantage&quot;) that could be obtained by <b>taking</b> a certain <b>action</b> compared to the average <b>action</b> taken at that state [].Several ...", "dateLastCrawled": "2022-01-31T18:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Explainable Reinforcement Learning for Broad-XAI: A Conceptual ...", "url": "https://deepai.org/publication/explainable-reinforcement-learning-for-broad-xai-a-conceptual-framework-and-survey", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/explainable-reinforcement-learning-for-broad-xai-a...", "snippet": "Broad Explainable Artificial Intelligence moves away from interpreting individual decisions based on a single datum and aims to provide integrated explanations from multiple machine learning algorithms into a coherent explanation of an agent&#39;s behaviour that is aligned to the communication needs of the explainee. Reinforcement Learning", "dateLastCrawled": "2022-01-13T14:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "114 questions with answers in <b>REWARD</b> | Science topic", "url": "https://www.researchgate.net/topic/Reward", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/topic/<b>Reward</b>", "snippet": "3 answers. Sep 4, 2019. Assuming that, there is a maze with 9X9 grids. The robot moves in the maze, and <b>can</b> only get <b>reward</b> in one grid. A robot in the maze <b>can</b> pick <b>action</b> from up, down, left and ...", "dateLastCrawled": "2022-02-02T00:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Lecture 16: Reinforcement Learning, Part 1 | Lecture Videos | Machine ...", "url": "https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-s897-machine-learning-for-healthcare-spring-2019/lecture-videos/lecture-16-reinforcement-learning-part-1/", "isFamilyFriendly": true, "displayUrl": "https://<b>ocw.mit.edu</b>/courses/electrical-engineering-and-computer-science/6-s897-machine...", "snippet": "And in <b>particular</b>, we&#39;ll be interested in coming up with a policy for making decisions repeatedly that optimizes a given outcome--something that we care about. It could be minimize the risk of death. It could be a <b>reward</b> that says that the vitals of a patients are in the right range. We might want to optimize that. But essentially, think about it now as having this choice of administering a medication or an intervention at any time, t--and having the best policy for doing so. OK, I&#39;m going ...", "dateLastCrawled": "2022-02-02T22:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(PDF) Regret-based <b>Reward</b> Elicitation for Markov Decision Processes", "url": "https://www.researchgate.net/publication/224943865_Regret-based_Reward_Elicitation_for_Markov_Decision_Processes", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/224943865_Regret-based_<b>Reward</b>_Elicitation_for...", "snippet": "ward <b>function</b> and f \u2208 F be a vector of valid visitation. frequenices (corresponding to a policy chosen by the. decision maker). The re gret of f w.r.t. r is: Regret ( f,r) = max. g \u2208F r \u00b7 g ...", "dateLastCrawled": "2022-02-02T04:16:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Recent advances in leveraging human guidance for sequential decision ...", "url": "https://link.springer.com/article/10.1007/s10458-021-09514-w", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10458-021-09514-w", "snippet": "The state <b>value</b> <b>function</b> \\(V^\\pi (s)\\) measures the expected cumulative <b>reward</b> to be in <b>a particular</b> state s and following policy \\(\\pi\\) afterward. The <b>action</b>-<b>value</b> <b>function</b> \\(Q^\\pi (s,a)\\) defines the same quantity but <b>for taking</b> <b>a particular</b> <b>action</b> a when in state s and following policy \\(\\pi\\) afterward. The advantage <b>function</b> tells us the relative gain (\u201cadvantage&quot;) that could be obtained by <b>taking</b> a certain <b>action</b> <b>compared</b> to the average <b>action</b> taken at that state [].Several ...", "dateLastCrawled": "2022-01-31T18:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A survey and critique of multiagent deep <b>reinforcement learning</b> ...", "url": "https://link.springer.com/article/10.1007/s10458-019-09421-1", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10458-019-09421-1", "snippet": "In the case when the critic learns a <b>state-action</b> <b>function</b> (Q <b>function</b>) and a state <b>value</b> <b>function</b> (V <b>function</b>), an advantage <b>function</b> <b>can</b> be computed by subtracting state values from the <b>state-action</b> values [283, 315]. The advantage <b>function</b> indicates the relative quality of an <b>action</b> <b>compared</b> to other available actions computed from the baseline, i.e., state <b>value</b> <b>function</b>. An example of an actor-critic algorithm is Deterministic Policy Gradient (DPG) . In DPG the critic follows the ...", "dateLastCrawled": "2022-01-29T11:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Reinforcement learning for cyber-physical systems with cybersecurity ...", "url": "https://dokumen.pub/reinforcement-learning-for-cyber-physical-systems-with-cybersecurity-case-studies-9781351006590-1351006592-9781351006606-1351006606-9781351006613-1351006614-9781351006620-1351006622-9781138543539.html", "isFamilyFriendly": true, "displayUrl": "https://dokumen.pub/reinforcement-learning-for-cyber-physical-systems-with-cyber...", "snippet": "<b>Value</b> functions: In a reinforcement learning problem, each state or <b>state-action</b> pair is associated with a <b>value</b> <b>function</b> that serves as the input argument for finding policy for each state. A <b>value</b> <b>function</b> is the accumulated rewards in a long run. Two kinds of <b>value</b> functions are usually used in reinforcement learning: statevalue <b>function</b> V (s) and <b>action</b>-<b>value</b> <b>function</b> Q(s, a). V (s) is defined as the total amount of rewards an agent will accumulate over the future if starting from that ...", "dateLastCrawled": "2022-01-23T02:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) <b>Reward</b> <b>Function</b> and Initial Values: Better Choices for ...", "url": "https://www.researchgate.net/publication/40619138_Reward_Function_and_Initial_Values_Better_Choices_for_Accelerated_Goal-Directed_Reinforcement_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/40619138_<b>Reward</b>_<b>Function</b>_and_Initial_<b>Values</b>...", "snippet": "The correct <b>action</b> is, in general, unknown, and the <b>reward</b> assesses the agent&#39;s progress in achieving its goal. Designing the <b>reward</b> <b>function</b> is crucial for the success of RL algorithms, and ...", "dateLastCrawled": "2021-11-11T08:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "NEUROSTUDIO: <b>Deep Reinforcement Learning with</b> Neural Networks \u2013 Machine ...", "url": "https://unrealai.wordpress.com/2018/05/08/deep-rl-with-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://unrealai.wordpress.com/2018/05/08/deep-rl-with-neural-networks", "snippet": "Initially the Neural Networks prediction about the <b>value</b> of these <b>action</b> given a certain state the agent is in will be random, but after the Agent receives a <b>reward</b>, it <b>can</b> form a more accurate estimate of the values of the previous <b>state action</b> pairs. The difference between its initial <b>value</b> estimates for each <b>action</b> and the new ones obtained after <b>taking</b> an <b>action</b> are used as a loss <b>function</b> to train the Neural network. After many <b>game</b> episodes and many epochs training the neural network ...", "dateLastCrawled": "2022-01-24T22:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Beyond Winning and Losing: Modeling Human Motivations and ... - DeepAI", "url": "https://deepai.org/publication/beyond-winning-and-losing-modeling-human-motivations-and-behaviors-using-inverse-reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/beyond-winning-and-losing-modeling-human-motivations...", "snippet": "Such approximation should be a fair estimation of the cumulative <b>reward</b> the user <b>would receive</b> if the user chooses an <b>action</b> a at the state s and maximizes the i -th <b>reward</b> thereafter. DQN uses the recursive property (i.e. Bellman equation) that the <b>action</b>-<b>value</b> estimator should have, that is, the cumulative <b>reward</b> since the current step onwards should be the immediate <b>reward</b> plus the cumulative <b>reward</b> since the next step onwards. Using the property, DQN updates the <b>action</b>-<b>value</b> <b>function</b> ...", "dateLastCrawled": "2021-12-23T20:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "5 Things You Need to Know about <b>Reinforcement Learning</b>", "url": "https://www.kdnuggets.com/2018/03/5-things-reinforcement-learning.html", "isFamilyFriendly": true, "displayUrl": "https://www.<b>kdnuggets</b>.com/2018/03/5-things-<b>reinforcement-learning</b>.html", "snippet": "<b>Value</b>: Future <b>reward</b> that an agent <b>would receive</b> by <b>taking</b> an <b>action</b> in <b>a particular</b> state. A <b>Reinforcement Learning</b> problem <b>can</b> be best explained through games. Let\u2019s take the <b>game</b> of PacMan where the goal of the agent (PacMan) is to eat the food in the grid while avoiding the ghosts on its way. The grid world is the interactive environment for the agent. PacMan receives a <b>reward</b> for eating food and punishment if it gets killed by the ghost (loses the <b>game</b>). The states are the location of ...", "dateLastCrawled": "2022-02-02T16:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "MIT 6.S094: Deep Learning for Self-Driving Cars 2018 Lecture 3 Notes ...", "url": "https://hackernoon.com/mit-6-s094-deep-learning-for-self-driving-cars-2018-lecture-3-notes-deep-reinforcement-learning-fe9a8592e14a", "isFamilyFriendly": true, "displayUrl": "https://hackernoon.com/mit-6-s094-deep-learning-for-self-driving-cars-2018-lecture-3...", "snippet": "<b>Value</b> of the Environment state is the <b>reward</b> we are likely to <b>receive</b> in the future. This is determined by discounting the future discount. Gamma: Reduces the importance of future goal. A good strategy is maximising the sum of discounted future goals. Q-Learning: We use any policy to estimate state that maximises the future <b>reward</b>. This allows us to consider much larger state space and <b>action</b> space. We move about <b>simulation</b> <b>taking</b> actions and updating our estimate of how good actions are ...", "dateLastCrawled": "2022-02-02T22:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Hands-on Reinforcement Learning with Python. Master ... - <b>DOKUMEN.PUB</b>", "url": "https://dokumen.pub/hands-on-reinforcement-learning-with-python-master-reinforcement-and-deep-reinforcement-learning-using-openai-gym-and-tensorflow-978-1-78883-652-4.html", "isFamilyFriendly": true, "displayUrl": "https://<b>dokumen.pub</b>/hands-on-reinforcement-learning-with-python-master-reinforcement...", "snippet": "<b>State-action</b> <b>value</b> <b>function</b> (Q <b>function</b>) A <b>state-action</b> <b>value</b> <b>function</b> is also called the Q <b>function</b>. It specifies how good it is for an agent to perform <b>a particular</b> <b>action</b> in a state with a policy \u03c0. The Q <b>function</b> is denoted by Q(s). It denotes the <b>value</b> of <b>taking</b> an <b>action</b> in a state following a policy \u03c0. We <b>can</b> define Q <b>function</b> as follows: [ 46 ] The Markov Decision Process and Dynamic Programming Chapter 3 This specifies the expected return starting from state s with the <b>action</b> a ...", "dateLastCrawled": "2022-02-02T22:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Deep reinforcement learning in medical imaging: A literature review ...", "url": "https://www.sciencedirect.com/science/article/pii/S1361841521002395", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1361841521002395", "snippet": "To represent the optimal <b>value</b> of each <b>state-action</b>, Q-<b>value</b> is defined as (3) Q \u03c0 * (s t, a t) = \u2211 s t + 1 T (s t + 1 | s t, a t) [R (s t, s t + 1) + \u03b3 V \u03c0 * (s t + 1)]. 2.2. MDP Solutions. Many solution techniques are available to compute an optimal policy for a given MDP. In general, these techniques <b>can</b> be divided into model-free and model-based methods, depending on whether an explicit model is constructed or not. Here, \u201cmodel\u201d refers to the environment itself that is defined ...", "dateLastCrawled": "2022-01-26T16:53:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Relationship between state (V) and action(Q) <b>value</b> <b>function</b> in ...", "url": "https://medium.com/intro-to-artificial-intelligence/relationship-between-state-v-and-action-q-value-function-in-reinforcement-learning-bb9a988c0127", "isFamilyFriendly": true, "displayUrl": "https://medium.com/intro-to-artificial-intelligence/relationship-between-state-v-and...", "snippet": "<b>Value</b> <b>function</b> can be defined as the expected <b>value</b> of an agent in a certain state. There are two types of <b>value</b> functions in RL: State-<b>value</b> and action-<b>value</b>. It is important to understand the\u2026", "dateLastCrawled": "2022-02-03T03:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and-sample-codes ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/README.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "<b>Value</b>. State-<b>value</b> <b>function</b> v_\u03c0: gives us the <b>value</b> of a state under \u03c0; Action-<b>value</b> <b>function</b> q_\u03c0: gives us the <b>value</b> of an action under \u03c0. q_\u03c0 is referred to as the Q-<b>function</b>, and the output from the <b>function</b> for any given <b>state-action</b> pair is called a Q-<b>value</b>.", "dateLastCrawled": "2021-09-22T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>GitHub</b> - gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "<b>Value</b>. State-<b>value</b> <b>function</b> v_\u03c0: gives us the <b>value</b> of a state under \u03c0; Action-<b>value</b> <b>function</b> q_\u03c0: gives us the <b>value</b> of an action under \u03c0. q_\u03c0 is referred to as the Q-<b>function</b>, and the output from the <b>function</b> for any given <b>state-action</b> pair is called a Q-<b>value</b>.", "dateLastCrawled": "2021-09-12T01:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) <b>Value</b>-<b>Function</b>-<b>Based Transfer for Reinforcement Learning</b> Using ...", "url": "https://www.researchgate.net/publication/221604435_Value-Function-Based_Transfer_for_Reinforcement_Learning_Using_Structure_Mapping", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221604435_<b>Value</b>-<b>Function</b>-Based_Transfer_for...", "snippet": "chological and computational theory about <b>analogy</b> making, ... the form of a <b>state-action</b> <b>value</b> <b>function</b>, or a q-<b>functio n</b>. A. q-<b>function</b> q: S \u00d7 A 7\u2192 R maps from <b>state-action</b> pairs to. real ...", "dateLastCrawled": "2021-10-16T01:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Hands-on Reinforcement <b>Learning</b> with Python. Master Reinforcement and ...", "url": "https://dokumen.pub/hands-on-reinforcement-learning-with-python-master-reinforcement-and-deep-reinforcement-learning-using-openai-gym-and-tensorflow-978-1-78883-652-4.html", "isFamilyFriendly": true, "displayUrl": "https://<b>dokumen.pub</b>/hands-on-reinforcement-<b>learning</b>-with-python-master-reinforcement...", "snippet": "<b>State-action</b> <b>value</b> <b>function</b> (Q <b>function</b>) A <b>state-action</b> <b>value</b> <b>function</b> is also called the Q <b>function</b>. It specifies how good it is for an agent to perform a particular action in a state with a policy \u03c0. The Q <b>function</b> is denoted by Q(s). It denotes the <b>value</b> of taking an action in a state following a policy \u03c0. We can define Q <b>function</b> as follows:", "dateLastCrawled": "2022-02-02T22:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "AI and Reinforcement <b>Learning</b> \u2014 Machines that Learn through Experience ...", "url": "https://www.cantorsparadise.com/ai-and-reinforcement-learning-machines-that-learn-through-experience-e7eea7bb6765", "isFamilyFriendly": true, "displayUrl": "https://www.cantorsparadise.com/ai-and-reinforcement-<b>learning</b>-<b>machines</b>-that-learn...", "snippet": "To align the policy with the updated <b>value</b> <b>function</b>, the algorithm modifies the policy so it would greedily follow the <b>value</b> <b>function</b> (meaning, choosing to perform actions that has the highest <b>value</b>). The algorithm continues by generating a new episode, now under the improved policy, which, in turn, derives a more accurate <b>value</b> estimation and so on. In this process, both the policy and the <b>value</b> <b>function</b> converge to their optimal values, until sufficient accuracy is reached, or when no more ...", "dateLastCrawled": "2022-01-25T17:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>SARSA</b> vs Q - <b>learning</b>", "url": "https://tcnguyen.github.io/reinforcement_learning/sarsa_vs_q_learning.html", "isFamilyFriendly": true, "displayUrl": "https://tcnguyen.github.io/reinforcement_<b>learning</b>/<b>sarsa</b>_vs_q_<b>learning</b>.html", "snippet": "<b>SARSA</b> will learn the optimal $\\epsilon$-greedy policy, i.e, the Q-<b>value</b> <b>function</b> will converge to a optimal Q-<b>value</b> <b>function</b> but in the space of $\\epsilon$-greedy policy only (as long as each <b>state action</b> pair will be visited infinitely). We expect that in the limit of $\\epsilon$ decaying to $0$, <b>SARSA</b> will converge to the overall optimal policy. I quote here a paragraph from", "dateLastCrawled": "2022-01-30T04:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Reinforcement Learning: Prediction, Control and</b> <b>Value</b> <b>Function</b> ...", "url": "https://deepai.org/publication/reinforcement-learning-prediction-control-and-value-function-approximation", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>reinforcement-learning-prediction-control-and</b>-<b>value</b>...", "snippet": "<b>Reinforcement Learning: Prediction, Control and Value Function Approximation</b>. With the increasing power of computers and the rapid development of self-<b>learning</b> methodologies such as <b>machine</b> <b>learning</b> and artificial intelligence, the problem of constructing an automatic Financial Trading Systems (FTFs) becomes an increasingly attractive research ...", "dateLastCrawled": "2022-01-16T03:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Machine</b> <b>learning</b> and AI <b>in marketing \u2013 Connecting computing power to</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0167811620300410", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0167811620300410", "snippet": "<b>State-Action</b>-Reward-<b>State-Action</b>: 2.2.3: SVD: Singular <b>Value</b> Decomposition: 2.2.2: SVM: Support Vector <b>Machine</b> : 2.2.1: TD: Temporal-Difference: 2.2.3: UGC: User-Generated Content: 3.1: Table 3. Strengths and weaknesses of <b>machine</b> <b>learning</b> methods. Strength \u2022 Ability to handle unstructured data and data of hybrid formats \u2022 Ability to handle large data volume \u2022 Flexible model structure \u2022 Strong predictive performance. Weakness \u2022 Not easy to interpret \u2022 Relationship typically ...", "dateLastCrawled": "2022-01-12T18:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Reinforcement Learning with Factored States</b> and Actions.", "url": "https://www.researchgate.net/publication/220320206_Reinforcement_Learning_with_Factored_States_and_Actions", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/220320206_Reinforcement_<b>Learning</b>_with...", "snippet": "Restricted In [25], the authors use Restricted Bolzman <b>Machine</b> to deal with MDPs of large state and action spaces, by modeling the <b>state-action</b> <b>value</b> <b>function</b> with the negative free energy of the ...", "dateLastCrawled": "2022-01-15T11:37:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(state-action value function)  is like +(the reward a person would receive for taking a particular action in a game or simulation)", "+(state-action value function) is similar to +(the reward a person would receive for taking a particular action in a game or simulation)", "+(state-action value function) can be thought of as +(the reward a person would receive for taking a particular action in a game or simulation)", "+(state-action value function) can be compared to +(the reward a person would receive for taking a particular action in a game or simulation)", "machine learning +(state-action value function AND analogy)", "machine learning +(\"state-action value function is like\")", "machine learning +(\"state-action value function is similar\")", "machine learning +(\"just as state-action value function\")", "machine learning +(\"state-action value function can be thought of as\")", "machine learning +(\"state-action value function can be compared to\")"]}