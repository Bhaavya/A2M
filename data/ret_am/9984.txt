{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Bias</b> in A.I. &amp; <b>Machine</b> <b>Learning</b> Examples \u2013 Tanner Abraham", "url": "https://tannerabraham.com/bias-in-machine-learning-and-ai-examples/", "isFamilyFriendly": true, "displayUrl": "https://tannerabraham.com/<b>bias</b>-in-<b>machine</b>-<b>learning</b>-and-ai-examples", "snippet": "These biases include sample <b>bias</b>, reporting <b>bias</b>, prejudice <b>bias</b>, confirmation <b>bias</b>, <b>group</b> <b>attribution</b> <b>bias</b>, <b>algorithm</b> <b>bias</b>, measurement <b>bias</b>, recall <b>bias</b>, exclusion <b>bias</b>, and automation <b>bias</b>. <b>Machine</b> <b>learning</b> is highly susceptible to many forms of <b>bias</b> that can undermine model performance. After all, AI is assembled by humans, and humans are ...", "dateLastCrawled": "2022-01-28T07:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "AI <b>Bias</b> Example, Meaning And How To Avoid Them - Kotai Electronics Pvt ...", "url": "https://kotaielectronics.com/ai-bias-example-meaning-and-how-to-avoid-them/", "isFamilyFriendly": true, "displayUrl": "https://kotaielectronics.com/ai-<b>bias</b>-example-meaning-and-how-to-avoid-them", "snippet": "<b>Group</b> <b>attribution</b> AI <b>bias</b> meaning, The <b>Bias</b> that takes place when the <b>algorithm</b> puts more weight onto an individual it happens because the <b>algorithm</b> classifies the data and extrapolates a certain set of data from the rest of the data set. Here the AI <b>bias</b> example can be a tool that is used for admission and recruiting people, Here the system can put more importance on students who graduate from certain universities over others.", "dateLastCrawled": "2022-01-18T15:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "HUMAN <b>BIAS</b> THAT CAN RESULT INTO ML BIASES | by CoffeeBeans Consulting ...", "url": "https://coffeebeansconsulting.medium.com/human-bias-that-can-result-into-ml-biases-b1ea9f6d2767?source=post_internal_links---------6----------------------------", "isFamilyFriendly": true, "displayUrl": "https://coffeebeansconsulting.medium.com/human-<b>bias</b>-that-can-result-into-ml-<b>bias</b>es-b1...", "snippet": "<b>Group</b> <b>attribution</b> <b>Bias</b>: This <b>bias</b> assumes a particular attribute to the entire <b>group</b>. For example, if the majority of women are in the designing industry and a majority of men in the hardware, it tends to assume the respective professions for both. 6. <b>Algorithm</b> <b>Bias</b>: In <b>machine</b> <b>learning</b>, <b>bias</b> is a mathematical property of an <b>algorithm</b>. The counterpart to <b>bias</b> in this context is variance. ML algorithms with a high value of variance can easily fit into training data and welcome complexity but ...", "dateLastCrawled": "2022-01-25T05:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "AI <b>Bias</b>: Definition, Types, Examples, and Debiasing Strategies \u2014 ITRex", "url": "https://itrexgroup.com/blog/ai-bias-definition-types-examples-debiasing-strategies/", "isFamilyFriendly": true, "displayUrl": "https://itrex<b>group</b>.com/blog/ai-<b>bias</b>-definition-types-examples-de<b>bias</b>ing-strategies", "snippet": "A simple definition of AI <b>bias</b> could sound <b>like</b> that: a phenomenon that occurs when an AI <b>algorithm</b> produces results that are systemically prejudiced due to erroneous assumptions in the <b>machine</b> <b>learning</b> process. <b>Bias</b> in artificial intelligence can take many forms \u2014 from racial <b>bias</b> and gender prejudice to recruiting inequity and age discrimination. &quot;The underlying reason for AI <b>bias</b> lies in human prejudice - conscious or unconscious - lurking in AI algorithms throughout their development ...", "dateLastCrawled": "2022-01-31T20:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Welcome To The <b>Machine</b> <b>Learning</b> Biases That Still Exist In 2019", "url": "https://analyticsindiamag.com/welcome-to-the-machine-learning-biases-that-still-exist-in-2019/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/welcome-to-the-<b>machine</b>-<b>learning</b>-<b>bias</b>es-that-still-exist...", "snippet": "5.<b>Group</b> <b>attribution</b> <b>Bias</b>: This <b>bias</b> assumes a particular attribute to the entire <b>group</b>. For example, if the majority of women are in the designing industry and a majority of men in the hardware, it tends to assume the respective professions for both. 4.<b>Algorithm</b> <b>Bias</b>: In <b>machine</b> <b>learning</b>, <b>bias</b> is a mathematical property of an <b>algorithm</b>. The ...", "dateLastCrawled": "2022-01-08T19:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Biases in <b>machine</b> <b>learning</b> models and big data analytics: The ...", "url": "https://www.cambridge.org/core/journals/international-review-of-the-red-cross/article/biases-in-machine-learning-models-and-big-data-analytics-the-international-criminal-and-humanitarian-law-implications/86BEAC9ADD165C90B2931AB2B665FFDF", "isFamilyFriendly": true, "displayUrl": "https://www.cambridge.org/core/journals/international-review-of-the-red-cross/article/...", "snippet": "<b>Group</b> <b>attribution</b> <b>bias</b> is a tendency to impute what is true of a few individuals to an entire <b>group</b> to which they belong. For instance, imagine that an ML model is created to identify the most suitable candidates for a position with the OTP. In creating this model, the designers assume that the \u201cbest\u201d candidates are individuals with a doctorate degree from a Western European university and internship experience with the ICC, purely because some successful employees possess those traits ...", "dateLastCrawled": "2021-12-21T11:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine Learning</b> Glossary | <b>Google Developers</b>", "url": "https://developers.google.com/machine-learning/glossary/", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine-learning</b>/glossary", "snippet": "Out-<b>group</b> homogeneity <b>bias</b> is a form of <b>group</b> <b>attribution</b> <b>bias</b>. See also in-<b>group</b> <b>bias</b>. outlier detection. The process of identifying outliers in a training set. Contrast with novelty detection. outliers. Values distant from most other values. In <b>machine learning</b>, any of the following are outliers: Weights with high absolute values.", "dateLastCrawled": "2022-02-03T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Bias</b> in AI: What it is, Types, Examples &amp; 6 Ways to Fix it in 2022", "url": "https://research.aimultiple.com/ai-bias/", "isFamilyFriendly": true, "displayUrl": "https://research.aimultiple.com/ai-<b>bias</b>", "snippet": "AI <b>bias</b> is an anomaly in the output of <b>machine</b> <b>learning</b> algorithms, due to the prejudiced assumptions made during the <b>algorithm</b> development process or prejudices in the training data. What are the types of AI <b>bias</b>? AI systems contain biases due to two reasons: Cognitive biases: These are unconscious errors in thinking that affects individuals\u2019 judgements and decisions. These biases arise from the brain\u2019s attempt to simplify processing information about the world. More than 180 human ...", "dateLastCrawled": "2022-02-02T07:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Machine</b> <b>Learning</b> Fairness: Types of <b>Bias</b> | by Svs Nagesh | Medium", "url": "https://nageshsomayajula.medium.com/machine-learning-fairness-types-of-bias-82bcf3df2d47", "isFamilyFriendly": true, "displayUrl": "https://nageshsomayajula.medium.com/<b>machine</b>-<b>learning</b>-fairness-types-of-<b>bias</b>-82bcf3df2d47", "snippet": "AI a p plications are <b>like</b> a small kid, we must train with the right data otherwise they can be misguided, and correcting machines or AI applications will be big challenging, for kids also (pun intended). The AI systems themselves will construct models that will explain how it works and follow anti-<b>bias</b> rules. In the <b>machine</b>, <b>learning</b> <b>bias</b> is one of the most common problems and every <b>algorithm</b> falls trap on this various kind of <b>bias</b>, let\u2019s discuss in detail various types of <b>bias</b> and how to ...", "dateLastCrawled": "2022-01-30T12:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Bias</b>, noise, <b>and interpretability in machine learning: from</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/B9780128157398000171", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/B9780128157398000171", "snippet": "The <b>machine</b> <b>learning</b> <b>algorithm</b> cannot find a valid weight vector to separate the groups. (D) Ideally, the aggregation combines features having the same relationship with the target (i.e., the correct labels); the model is comparable to that of (B) (i.e., effect size d y is the same), but the <b>algorithm</b> can work with a feature vector with lower dimensionality and less noise.", "dateLastCrawled": "2021-11-09T13:06:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Bias</b> in A.I. &amp; <b>Machine</b> <b>Learning</b> Examples \u2013 Tanner Abraham", "url": "https://tannerabraham.com/bias-in-machine-learning-and-ai-examples/", "isFamilyFriendly": true, "displayUrl": "https://tannerabraham.com/<b>bias</b>-in-<b>machine</b>-<b>learning</b>-and-ai-examples", "snippet": "These biases include sample <b>bias</b>, reporting <b>bias</b>, prejudice <b>bias</b>, confirmation <b>bias</b>, <b>group</b> <b>attribution</b> <b>bias</b>, <b>algorithm</b> <b>bias</b>, measurement <b>bias</b>, recall <b>bias</b>, exclusion <b>bias</b>, and automation <b>bias</b>. <b>Machine</b> <b>learning</b> is highly susceptible to many forms of <b>bias</b> that can undermine model performance. After all, AI is assembled by humans, and humans are ...", "dateLastCrawled": "2022-01-28T07:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Biases in <b>machine</b> <b>learning</b> models and big data analytics: The ...", "url": "https://www.cambridge.org/core/journals/international-review-of-the-red-cross/article/biases-in-machine-learning-models-and-big-data-analytics-the-international-criminal-and-humanitarian-law-implications/86BEAC9ADD165C90B2931AB2B665FFDF", "isFamilyFriendly": true, "displayUrl": "https://www.cambridge.org/core/journals/international-review-of-the-red-cross/article/...", "snippet": "<b>Group</b> <b>attribution</b> <b>bias</b> is a tendency to impute what is true of a few individuals to an entire <b>group</b> to which they belong. For instance, imagine that an ML model is created to identify the most suitable candidates for a position with the OTP. In creating this model, the designers assume that the \u201cbest\u201d candidates are individuals with a doctorate degree from a Western European university and internship experience with the ICC, purely because some successful employees possess those traits ...", "dateLastCrawled": "2021-12-21T11:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Bias</b>, <b>Fairness</b> and Explainability \u2014 steps towards building Responsible ...", "url": "https://medium.com/walmartglobaltech/bias-fairness-and-explainability-steps-towards-building-responsible-ai-dc735b06279", "isFamilyFriendly": true, "displayUrl": "https://medium.com/walmartglobaltech/<b>bias</b>-<b>fairness</b>-and-explainability-steps-towards...", "snippet": "<b>Group</b> <b>Attribution</b> <b>bias</b>: ... As mentioned in Wikipedia: \u201cIn <b>machine</b> <b>learning</b>, a given <b>algorithm</b> is said to be fair, or to have <b>fairness</b>, if its results are independent of given variables ...", "dateLastCrawled": "2022-01-23T22:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Comparing different supervised <b>machine</b> <b>learning</b> algorithms for disease ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6925840/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC6925840", "snippet": "To avoid the risk of selection <b>bias</b>, from the literature we extracted those articles that used more than one supervised <b>machine</b> <b>learning</b> <b>algorithm</b>. The same supervised <b>learning</b> <b>algorithm</b> can generate different results across various study settings. There is a chance that a performance comparison between two supervised <b>learning</b> algorithms can generate imprecise results if they were employed in different studies separately. On the other side, the results of this study could suffer a variable ...", "dateLastCrawled": "2022-01-28T14:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine Learning</b> Glossary | <b>Google Developers</b>", "url": "https://developers.google.com/machine-learning/glossary/", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine-learning</b>/glossary", "snippet": "Out-<b>group</b> homogeneity <b>bias</b> is a form of <b>group</b> <b>attribution</b> <b>bias</b>. See also in-<b>group</b> <b>bias</b>. outlier detection. The process of identifying outliers in a training set. Contrast with novelty detection. outliers. Values distant from most other values. In <b>machine learning</b>, any of the following are outliers: Weights with high absolute values.", "dateLastCrawled": "2022-02-03T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Introduction <b>to Machine</b> <b>Learning</b>, Neural Networks, and Deep <b>Learning</b>", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7347027/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7347027", "snippet": "The performance of the <b>algorithm</b> is evaluated on the test dataset, data that the <b>algorithm</b> has never seen before.8, 9 The basic steps of supervised <b>machine</b> <b>learning</b> are (1) acquire a dataset and split it into separate training, validation, and test datasets; (2) use the training and validation datasets to inform a model of the relationship between features and target; and (3) evaluate the model via the test dataset to determine how well it predicts housing prices for unseen instances. In ...", "dateLastCrawled": "2022-02-02T05:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Evolution and impact of <b>bias</b> in human and <b>machine learning</b> <b>algorithm</b> ...", "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0235502", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0235502", "snippet": "Traditionally, <b>machine learning</b> algorithms relied on reliable labels from experts to build predictions. More recently however, algorithms have been receiving data from the general population in the form of labeling, annotations, etc. The result is that algorithms are subject to <b>bias</b> that is born from ingesting unchecked information, such as biased samples and biased labels. Furthermore, people and algorithms are increasingly engaged in interactive processes wherein neither the human nor the ...", "dateLastCrawled": "2021-11-15T00:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Frontiers | Addressing Fairness, <b>Bias</b>, and Appropriate Use of ...", "url": "https://www.frontiersin.org/articles/10.3389/frai.2020.561802/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/frai.2020.561802", "snippet": "A well-known example of <b>machine</b> <b>learning</b> <b>bias</b>, publicized by Joy Boulamwini in 2017 ... to enable the proper tuning of a <b>machine</b> <b>learning</b> <b>algorithm</b>. Individual vs. <b>Group</b> Fairness. Given the dictionary definition of fairness (impartial and just treatment), we can consider fairness at the level of an individual or a <b>group</b>. We can ask whether a computer <b>algorithm</b> disproportionately helps or harms specific individuals or specific groups of people. Ideally, an <b>algorithm</b> would be customized to an ...", "dateLastCrawled": "2022-01-29T07:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Review into bias in algorithmic decision-making</b> - <b>GOV.UK</b>", "url": "https://www.gov.uk/government/publications/cdei-publishes-review-into-bias-in-algorithmic-decision-making/main-report-cdei-review-into-bias-in-algorithmic-decision-making", "isFamilyFriendly": true, "displayUrl": "https://<b>www.gov.uk</b>/government/publications/cdei-publishes-review-into-<b>bias</b>-in...", "snippet": "Second, A <b>machine</b> <b>learning</b> <b>algorithm</b> is chosen, and uses historical data (e.g. a set of past input data (e.g. a set of past input data, the decisions reached) to build a model, optimising against ...", "dateLastCrawled": "2022-02-03T02:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Underdiagnosis <b>bias</b> of artificial intelligence algorithms applied to ...", "url": "https://www.nature.com/articles/s41591-021-01595-0", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41591-021-01595-0", "snippet": "As artificial intelligence (AI) algorithms increasingly affect decision-making in society 1, researchers have raised concerns about algorithms creating or amplifying biases 2,3,4,5,6,7,8,9,10,11 ...", "dateLastCrawled": "2022-02-02T21:08:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Frontiers | Addressing Fairness, <b>Bias</b>, and Appropriate Use of ...", "url": "https://www.frontiersin.org/articles/10.3389/frai.2020.561802/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/frai.2020.561802", "snippet": "A well-known example of <b>machine</b> <b>learning</b> <b>bias</b>, publicized by Joy Boulamwini in 2017 ... to enable the proper tuning of a <b>machine</b> <b>learning</b> <b>algorithm</b>. Individual vs. <b>Group</b> Fairness. Given the dictionary definition of fairness (impartial and just treatment), we <b>can</b> consider fairness at the level of an individual or a <b>group</b>. We <b>can</b> ask whether a computer <b>algorithm</b> disproportionately helps or harms specific individuals or specific groups of people. Ideally, an <b>algorithm</b> would be customized to an ...", "dateLastCrawled": "2022-01-29T07:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Introduction to <b>Machine</b> <b>Learning</b>, Neural Networks, and Deep <b>Learning</b>", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7347027/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7347027", "snippet": "The performance of the <b>algorithm</b> is evaluated on the test dataset, data that the <b>algorithm</b> has never seen before.8, 9 The basic steps of supervised <b>machine</b> <b>learning</b> are (1) acquire a dataset and split it into separate training, validation, and test datasets; (2) use the training and validation datasets to inform a model of the relationship between features and target; and (3) evaluate the model via the test dataset to determine how well it predicts housing prices for unseen instances. In ...", "dateLastCrawled": "2022-02-02T05:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Review into bias in algorithmic decision-making</b> - <b>GOV.UK</b>", "url": "https://www.gov.uk/government/publications/cdei-publishes-review-into-bias-in-algorithmic-decision-making/main-report-cdei-review-into-bias-in-algorithmic-decision-making", "isFamilyFriendly": true, "displayUrl": "https://<b>www.gov.uk</b>/government/publications/cdei-publishes-review-into-<b>bias</b>-in...", "snippet": "A <b>machine</b> <b>learning</b> <b>algorithm</b> takes data as ... not available to the <b>algorithm</b>) but <b>can</b> also introduce human <b>bias</b> into the system. Humans \u2018over the loop\u2019 monitoring the fairness of the whole ...", "dateLastCrawled": "2022-02-03T02:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Chapter 11 Bias and Fairness</b> | Big Data and Social Science", "url": "https://textbook.coleridgeinitiative.org/chap-bias.html", "isFamilyFriendly": true, "displayUrl": "https://textbook.coleridgeinitiative.org/chap-<b>bias</b>.html", "snippet": "Unfortunately, just as there is no single <b>machine</b> <b>learning</b> <b>algorithm</b> that is best suited to every application, no one fairness metric will fit every situation. However, we hope this chapter will provide you with a grounding in the available ways of measuring algorithmic fairness that will help you navigate the trade-offs involved putting these into practice in your own applications. 11.2 Sources of <b>Bias</b>. <b>Bias</b> may be introduced into a <b>machine</b> <b>learning</b> project at any step along the way and it ...", "dateLastCrawled": "2022-01-30T01:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Evolution and impact of <b>bias</b> in human and <b>machine learning</b> <b>algorithm</b> ...", "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0235502", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0235502", "snippet": "Traditionally, <b>machine learning</b> algorithms relied on reliable labels from experts to build predictions. More recently however, algorithms have been receiving data from the general population in the form of labeling, annotations, etc. The result is that algorithms are subject to <b>bias</b> that is born from ingesting unchecked information, such as biased samples and biased labels. Furthermore, people and algorithms are increasingly engaged in interactive processes wherein neither the human nor the ...", "dateLastCrawled": "2021-11-15T00:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Subjective and Objective in <b>the Development of Artificial Intelligence</b> ...", "url": "https://towardsdatascience.com/subjective-and-objective-in-the-development-of-artificial-intelligence-c5627a522f47", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/subjective-and-objective-in-the-development-of...", "snippet": "An <b>attribution</b> <b>bias</b> <b>can</b> happen when individuals assess or attempt to discover explanations behind their own and others\u2019 behaviors. People make attributions about the causes of their own and others\u2019 behaviors; but these attributions don\u2019t necessarily precisely reflect reality. Rather than operating as objective perceivers, individuals are inclined to perceptual slips that prompt biased understandings of their social world. Confirmation <b>bias</b> is the tendency to search for, interpret ...", "dateLastCrawled": "2022-01-26T07:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Bias</b>, awareness, and ignorance in deep-<b>learning</b>-based face recognition ...", "url": "https://link.springer.com/article/10.1007%2Fs43681-021-00108-6", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s43681-021-00108-6", "snippet": "We investigate how well <b>machine</b> <b>learning</b> models <b>can</b> predict the sensitive features, such as ethnicity and gender, based on the face embedding. The intuition is that an FR model is \u201caware\u201d of a sensitive feature if it <b>can</b> be predicted from the embedding vectors produced by the FR model. This inference is a classification task and the performance depends on the classification model at hand. If simple models, more precisely models with a low number of parameters, <b>can</b> properly infer the ...", "dateLastCrawled": "2022-02-03T10:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A <b>machine</b> <b>learning</b> toolkit for genetic engineering <b>attribution</b> to ...", "url": "https://www.nature.com/articles/s41467-020-19612-0", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41467-020-19612-0", "snippet": "With a framework for calibration of <b>attribution</b> models that <b>can</b> in principle be applied to any deep-<b>learning</b> classification <b>algorithm</b>, we next sought to expand the toolkit of genetic engineering ...", "dateLastCrawled": "2022-01-13T13:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "The ethics of <b>algorithms</b>: key problems and solutions", "url": "https://link.springer.com/article/10.1007/s00146-021-01154-8", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00146-021-01154-8", "snippet": "Actions (1) and (2) may be performed by (semi-)autonomous <b>algorithms</b>\u2014such as <b>machine</b> <b>learning</b> (ML) <b>algorithms</b>\u2014and this complicates, (3) the <b>attribution</b> of responsibility for the effects of actions that an <b>algorithm</b> may trigger. Here, ML is of particular interest, as a field which includes deep <b>learning</b> architectures. Computer systems deploying ML <b>algorithms</b> may be described as \u201cautonomous\u201d or \u201csemi-autonomous\u201d, to the extent that their outputs are induced from data and thus, non ...", "dateLastCrawled": "2022-01-30T20:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "300+ TOP <b>Neural Networks Multiple Choice Questions and Answers</b>", "url": "https://engineeringinterviewquestions.com/neural-networks-multiple-choice-questions-and-answers/", "isFamilyFriendly": true, "displayUrl": "https://engineeringinterviewquestions.com/<b>neural-networks-multiple-choice-questions</b>...", "snippet": "34. Decision Trees <b>can</b> be used for Classification Tasks. a) True b) False. Answer: a. 35. How many types of <b>learning</b> are available in <b>machine</b> <b>learning</b>? a) 1 b) 2 c) 3 d) 4. Answer: c Explanation: The three types of <b>machine</b> <b>learning</b> are supervised, unsupervised and reinforcement. 36. Choose from the following that are Decision Tree nodes. a ...", "dateLastCrawled": "2022-02-02T15:24:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Biases in <b>machine</b> <b>learning</b> models and big data analytics: The ...", "url": "https://www.cambridge.org/core/journals/international-review-of-the-red-cross/article/biases-in-machine-learning-models-and-big-data-analytics-the-international-criminal-and-humanitarian-law-implications/86BEAC9ADD165C90B2931AB2B665FFDF", "isFamilyFriendly": true, "displayUrl": "https://www.cambridge.org/core/journals/international-review-of-the-red-cross/article/...", "snippet": "<b>Group</b> <b>attribution</b> <b>bias</b> is a tendency to impute what is true of a few individuals to an entire <b>group</b> to which they belong. For instance, imagine that an ML model is created to identify the most suitable candidates for a position with the OTP. In creating this model, the designers assume that the \u201cbest\u201d candidates are individuals with a doctorate degree from a Western European university and internship experience with the ICC, purely because some successful employees possess those traits ...", "dateLastCrawled": "2021-12-21T11:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "What Is Model <b>Bias</b> In <b>Machine</b> <b>Learning</b>? \u2013 charmestrength.com", "url": "https://charmestrength.com/what-is-model-bias-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://charmestrength.com/what-is-model-<b>bias</b>-in-<b>machine</b>-<b>learning</b>", "snippet": "<b>Machine</b> <b>learning</b> <b>bias</b>, also sometimes called <b>algorithm</b> <b>bias</b> or AI <b>bias</b>, ... <b>Group</b> <b>attribution</b> <b>Bias</b>. What to do if model is Overfitting? Reduce the network&#39;s capacity by removing layers or reducing the number of elements in the hidden layers. Apply regularization , which comes down to adding a cost to the loss function for large weights. Use Dropout layers, which will randomly remove certain features by setting them to zero. How <b>can</b> <b>machine</b> <b>learning</b> models reduce <b>bias</b>? Choose the correct ...", "dateLastCrawled": "2022-01-15T06:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "AI <b>Bias</b> Example, Meaning And How To Avoid Them - Kotai Electronics Pvt ...", "url": "https://kotaielectronics.com/ai-bias-example-meaning-and-how-to-avoid-them/", "isFamilyFriendly": true, "displayUrl": "https://kotaielectronics.com/ai-<b>bias</b>-example-meaning-and-how-to-avoid-them", "snippet": "<b>Group</b> <b>attribution</b> AI <b>bias</b> meaning, The <b>Bias</b> that takes place when the <b>algorithm</b> puts more weight onto an individual it happens because the <b>algorithm</b> classifies the data and extrapolates a certain set of data from the rest of the data set. Here the AI <b>bias</b> example <b>can</b> be a tool that is used for admission and recruiting people, Here the system <b>can</b> put more importance on students who graduate from certain universities over others.", "dateLastCrawled": "2022-01-18T15:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Bias</b>, <b>Fairness</b> and Explainability \u2014 steps towards building Responsible ...", "url": "https://medium.com/walmartglobaltech/bias-fairness-and-explainability-steps-towards-building-responsible-ai-dc735b06279", "isFamilyFriendly": true, "displayUrl": "https://medium.com/walmartglobaltech/<b>bias</b>-<b>fairness</b>-and-explainability-steps-towards...", "snippet": "<b>Group</b> <b>Attribution</b> <b>bias</b>: ... in Wikipedia: \u201cIn <b>machine</b> <b>learning</b>, a given <b>algorithm</b> is said to be fair, or to have <b>fairness</b>, if its results are independent of given variables, especially those ...", "dateLastCrawled": "2022-01-23T22:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Can</b> <b>bias</b> be eliminated from algorithms?", "url": "https://www.weforum.org/agenda/2021/12/bias-eliminated-algorithms-software-code-systems/", "isFamilyFriendly": true, "displayUrl": "https://www.weforum.org/agenda/2021/12/<b>bias</b>-eliminated-<b>algorithms</b>-software-code-systems", "snippet": "<b>Algorithm</b> <b>bias</b>, also called <b>machine</b> <b>learning</b> <b>bias</b>, is a phenomenon in which algorithms <b>can</b> act in a discriminatory or prejudiced manner due to misplaced assumptions during the <b>learning</b> phase of their development. Unconscious biases regarding gender, race and social class <b>can</b> make their way into the training data fed by programmers into \u201c<b>machine</b>-<b>learning</b> algorithms\u201d, systems which constantly improve their own performance by including new data into an existing model. These biases <b>can</b> be ...", "dateLastCrawled": "2022-02-03T05:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "how to reduce <b>bias</b> in <b>machine</b> <b>learning</b> - Publicaffairsworld.com", "url": "https://publicaffairsworld.com/how-to-reduce-bias-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://publicaffairsworld.com/how-to-reduce-<b>bias</b>-in-<b>machine</b>-<b>learning</b>", "snippet": "If you have ever developed or worked on any type of <b>machine</b> <b>learning</b> <b>algorithm</b>, ... <b>Group</b> <b>attribution</b> <b>Bias</b>. What are the necessary steps to avoid biases in gathering and interpreting data? There are ways, however, to try to maintain objectivity and avoid <b>bias</b> with qualitative data analysis: Use multiple people to code the data. \u2026 Have participants review your results. \u2026 Verify with more data sources. \u2026 Check for alternative explanations. \u2026 Review findings with peers. What <b>can</b> a data ...", "dateLastCrawled": "2022-01-22T04:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Comparing different supervised <b>machine</b> <b>learning</b> algorithms for disease ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6925840/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC6925840", "snippet": "To avoid the risk of selection <b>bias</b>, from the literature we extracted those articles that used more than one supervised <b>machine</b> <b>learning</b> <b>algorithm</b>. The same supervised <b>learning</b> <b>algorithm</b> <b>can</b> generate different results across various study settings. There is a chance that a performance comparison between two supervised <b>learning</b> algorithms <b>can</b> generate imprecise results if they were employed in different studies separately. On the other side, the results of this study could suffer a variable ...", "dateLastCrawled": "2022-01-28T14:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Frontiers | Addressing Fairness, <b>Bias</b>, and Appropriate Use of ...", "url": "https://www.frontiersin.org/articles/10.3389/frai.2020.561802/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/frai.2020.561802", "snippet": "A well-known example of <b>machine</b> <b>learning</b> <b>bias</b>, publicized by Joy Boulamwini in 2017 ... to enable the proper tuning of a <b>machine</b> <b>learning</b> <b>algorithm</b>. Individual vs. <b>Group</b> Fairness. Given the dictionary definition of fairness (impartial and just treatment), we <b>can</b> consider fairness at the level of an individual or a <b>group</b>. We <b>can</b> ask whether a computer <b>algorithm</b> disproportionately helps or harms specific individuals or specific groups of people. Ideally, an <b>algorithm</b> would be customized to an ...", "dateLastCrawled": "2022-01-29T07:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A <b>machine</b> <b>learning</b> toolkit for genetic engineering <b>attribution</b> to ...", "url": "https://www.nature.com/articles/s41467-020-19612-0", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41467-020-19612-0", "snippet": "With a framework for calibration of <b>attribution</b> models that <b>can</b> in principle be applied to any deep-<b>learning</b> classification <b>algorithm</b>, we next sought to expand the toolkit of genetic engineering ...", "dateLastCrawled": "2022-01-13T13:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Underdiagnosis <b>bias</b> of artificial intelligence algorithms applied to ...", "url": "https://www.nature.com/articles/s41591-021-01595-0", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41591-021-01595-0", "snippet": "This is an especially dangerous outcome for <b>machine</b> <b>learning</b> models in healthcare, given that existing biases in health practice risk being magnified, rather than ameliorated, by algorithmic ...", "dateLastCrawled": "2022-02-02T21:08:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Attribution</b> Part 3: Applying <b>Machine</b> <b>Learning</b> to <b>Attribution</b> - Trust ...", "url": "https://www.trustinsights.ai/blog/2019/11/attribution-part-3-applying-machine-learning-to-attribution/", "isFamilyFriendly": true, "displayUrl": "https://www.trustinsights.ai/blog/2019/11/<b>attribution</b>-part-3-applying-<b>machine</b>-<b>learning</b>...", "snippet": "The <b>machine</b> <b>learning</b> approach to <b>attribution</b> analysis starts with no <b>bias</b>. With the inexpensive computational power now available to us you can run all of the data from every transaction, and all the data from transactions that did not complete. <b>Machine</b> <b>learning</b> will examine every single transition from one page to the next and from one goal (milestones along the customer journey) to the next and determine the probability of a customer moving from one point to the next.", "dateLastCrawled": "2022-02-02T04:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) Mitigating <b>bias</b> in <b>machine</b> <b>learning</b> for medicine", "url": "https://www.researchgate.net/publication/354073359_Mitigating_bias_in_machine_learning_for_medicine", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/354073359_Mitigating_<b>bias</b>_in_<b>machine</b>_<b>learning</b>...", "snippet": "Strategies for mitigating <b>bias</b> across the different steps in <b>machine</b> <b>learning</b> systems development. Diagram outlining proposed solutions on how to mitigate <b>bias</b> across the different development ...", "dateLastCrawled": "2022-01-21T21:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Bias</b> in <b>AI Increasingly Recognized; Progress Being Made</b> \u2013 Jay van Zyl", "url": "https://jayvanzyl.me/bias-in-ai-increasingly-recognized-progress-being-made/", "isFamilyFriendly": true, "displayUrl": "https://jayvanzyl.me/<b>bias</b>-in-<b>ai-increasingly-recognized-progress-being-made</b>", "snippet": "<b>Bias</b> in AI decision-making and in the algorithms of <b>machine</b> <b>learning</b> has been outed as a real issue in the march of AI progress. Here is an update on where we are and efforts being made to recognize <b>bias</b> and counteract it, including a discussion of selected AI startups. AI reflects the <b>bias</b> of its creators, notes Will Bryne, CEO of Groundswell in a recent article in Fast Company. Societal <b>bias</b> \u2013 the <b>attribution</b> of individuals or groups with distinct traits without any data to back it up ...", "dateLastCrawled": "2021-03-25T19:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Comparison of <b>machine</b>-<b>learning</b> methodologies for accurate diagnosis of ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8128240/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8128240", "snippet": "The residual 3,571 genes (approximately 60%) are processed with all investigated <b>machine</b>-<b>learning</b>-based methods independently in <b>analogy</b> to the second experiment. Classification is again repeated for the native testing data and all levels of data degeneration for 100 iterations while performance is reported. In a detailed comparison of experiments 2 and 3 (Tables", "dateLastCrawled": "2022-01-26T11:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "6 \u2013 Interpretability \u2013 <b>Machine</b> <b>Learning</b> Blog | ML@CMU | Carnegie Mellon ...", "url": "https://blog.ml.cmu.edu/2020/08/31/6-interpretability/", "isFamilyFriendly": true, "displayUrl": "https://blog.ml.cmu.edu/2020/08/31/6-interpretability", "snippet": "For a person inexperienced in <b>machine</b> <b>learning</b>, it would be difficult to apply the Olah method across many images, especially if you wanted to explain why the model chose a labrador retriever instead of a beagle. In such a case, abstract dog visualizations would be uninformative. Moreover, there is a potential observer <b>bias</b>: if a human expects visualizations of dogs and cats, they might miss more abstract but important visualizations, such as the snow in the husky and wolf classification ...", "dateLastCrawled": "2022-02-03T05:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "RStudio AI Blog: Starting to think about AI Fairness", "url": "https://blogs.rstudio.com/ai/posts/2021-07-15-ai-fairness/", "isFamilyFriendly": true, "displayUrl": "https://blogs.rstudio.com/ai/posts/2021-07-15-ai-fairness", "snippet": "From a <b>machine</b> <b>learning</b> perspective, the interesting point is the classification of metrics into <b>bias</b>-preserving and <b>bias</b>-transforming. The terms speak for themselves: Metrics in the first <b>group</b> reflect biases in the dataset used for training; ones in the second do not. In that way, the distinction parallels", "dateLastCrawled": "2022-01-31T09:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "300+ TOP <b>Neural Networks Multiple Choice Questions and Answers</b>", "url": "https://engineeringinterviewquestions.com/neural-networks-multiple-choice-questions-and-answers/", "isFamilyFriendly": true, "displayUrl": "https://engineeringinterviewquestions.com/<b>neural-networks-multiple-choice-questions</b>...", "snippet": "How many types of <b>learning</b> are available in <b>machine</b> <b>learning</b>? a) 1 b) 2 c) 3 d) 4. Answer: c Explanation: The three types of <b>machine</b> <b>learning</b> are supervised, unsupervised and reinforcement. 36. Choose from the following that are Decision Tree nodes. a) Decision Nodes b) Weighted Nodes c) Chance Nodes d) End Nodes. Answer: a, c, d. 37. Decision Nodes are represented by, a) Disks b) Squares c) Circles d) Triangles. Answer: b. 38. Chance Nodes are represented by, a) Disks b) Squares c) Circles ...", "dateLastCrawled": "2022-02-02T15:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Beneficial and harmful explanatory <b>machine learning</b> | SpringerLink", "url": "https://link.springer.com/article/10.1007/s10994-020-05941-0", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10994-020-05941-0", "snippet": "(<b>Machine</b>-explained human comprehension of examples, \\(C_{ex}(D, H, M(E))\\)): Given a logic program D representing the definition of a target predicate, a <b>group</b> of humans H, a theory M(E) learned using <b>machine learning</b> algorithm M and examples E, the <b>machine</b>-explained human comprehension of examples E is the mean accuracy with which a human \\(h \\in H\\) after brief study of an explanation based on M(E) can classify new material selected from the domain of D.", "dateLastCrawled": "2022-01-21T19:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Two Dimensions of Opacity and the Deep <b>Learning</b> Predicament | SpringerLink", "url": "https://link.springer.com/article/10.1007/s11023-021-09569-4", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11023-021-09569-4", "snippet": "But the <b>analogy</b> between human and <b>machine</b> <b>learning</b> can only be taken so far; for instance, it remains an open question \u201cwhether current or future DNN architectures can implement compositional recursive grammar\u201d (ibid, pp. 4\u20135). Thus, what happens during the training of DNNs can also serve as an abstract model of aspects of human <b>learning</b>, independently of the brain-<b>machine</b> (or a close mind-<b>machine</b>) <b>analogy</b>. In sum, at least three distinct senses of \u2018model\u2019 should be distinguished ...", "dateLastCrawled": "2021-11-23T13:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "About Us \u2013 <b>Toronto Machine Learning</b>", "url": "https://www.torontomachinelearning.com/about-us/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>torontomachinelearning</b>.com/about-us", "snippet": "The <b>Toronto Machine Learning</b> Society ... at Portland State University. Her current research focuses on conceptual abstraction, <b>analogy</b>-making, and visual recognition in artificial intelligence systems. Melanie is the author or editor of six books and numerous scholarly papers in the fields of artificial intelligence, cognitive science, and complex systems. Her book Complexity: A Guided Tour (Oxford University Press) won the 2010 Phi Beta Kappa Science Book Award and was named by Amazon.com ...", "dateLastCrawled": "2022-02-03T03:50:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Bias in A.I. &amp; <b>Machine</b> <b>Learning</b> Examples \u2013 Tanner Abraham", "url": "https://tannerabraham.com/bias-in-machine-learning-and-ai-examples/", "isFamilyFriendly": true, "displayUrl": "https://tannerabraham.com/bias-in-<b>machine</b>-<b>learning</b>-and-ai-examples", "snippet": "<b>Machine</b> <b>learning</b> algorithms are often mistaken as objective analytics and decision-making solutions to human inefficiencies. Paradoxically, humans often make <b>machine</b> <b>learning</b> algorithms inefficient by way of biases. These biases include sample bias, reporting bias, prejudice bias, confirmation bias, group attribution bias, algorithm bias, measurement bias, recall bias, exclusion bias, and automation bias. <b>Machine</b> <b>learning</b> is highly susceptible to many forms of bias that can undermine model ...", "dateLastCrawled": "2022-01-28T07:55:00.0000000Z", "language": "en", "isNavigational": false}], [], [], []], "all_bing_queries": ["+(group attribution bias)  is like +(machine learning algorithm)", "+(group attribution bias) is similar to +(machine learning algorithm)", "+(group attribution bias) can be thought of as +(machine learning algorithm)", "+(group attribution bias) can be compared to +(machine learning algorithm)", "machine learning +(group attribution bias AND analogy)", "machine learning +(\"group attribution bias is like\")", "machine learning +(\"group attribution bias is similar\")", "machine learning +(\"just as group attribution bias\")", "machine learning +(\"group attribution bias can be thought of as\")", "machine learning +(\"group attribution bias can be compared to\")"]}