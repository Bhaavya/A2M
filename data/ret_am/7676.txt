{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "What is &quot;<b>Disparate</b> <b>Impact</b>?&quot;", "url": "https://www.wellworkforce.com/what-is-disparate-impact", "isFamilyFriendly": true, "displayUrl": "https://www.wellworkforce.com/what-is-<b>disparate</b>-<b>impact</b>", "snippet": "<b>Accuracy</b> of testing After the PDA is complete and job-specific tests have been developed, a representative from your organization is asked to review and approve all testing criteria to ensure <b>accuracy</b> and efficacy. <b>Accuracy</b> of testing criteria as compared to essential job functions is critical to a valid and legally-defensible testing process. The ADA/EEOC require post-offer testing protocols to be \u201cjob-related and consistent with business necessity,\u201d meaning they accurately reflect the ...", "dateLastCrawled": "2022-01-10T09:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Disparate</b> <b>Impact</b> Analysis", "url": "https://h2oai.github.io/tutorials/disparate-impact-analysis/", "isFamilyFriendly": true, "displayUrl": "https://h2oai.github.io/tutorials/<b>disparate</b>-<b>impact</b>-analysis", "snippet": "<b>Disparate</b> <b>Impact</b> Analysis or DIA, which is sometimes called Adverse <b>Impact</b> Analysis, is a way to measure quantitatively the adverse treatment of protected classes, which leads to discrimination in hiring, housing, etc., or in general, any public policy decisions. The regulatory agencies will generally regard a selection rate for any group with less than four-fifths (4\u20445) or eighty percent of the rate for the group with the highest selection rate as constituting evidence of adverse <b>impact</b>.", "dateLastCrawled": "2022-02-03T09:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Chapter 7 <b>People-Based ML Models: Algorithmic Fairness</b> | Public Policy ...", "url": "https://urbanspatial.github.io/PublicPolicyAnalytics/people-based-ml-models-algorithmic-fairness.html", "isFamilyFriendly": true, "displayUrl": "https://urbanspatial.github.io/PublicPolicyAnalytics/people-based-ml-models...", "snippet": "7.1.3 <b>Accuracy</b> and generalizability in recidivism algorithms. <b>Accuracy</b> and generalizability continue to be the two yardsticks we use to measure the utility of our algorithms. The goal of a recidivism classifier is to predict two binary outcomes - Recidivate and notRecidivate.While the \u201cpercent of correct predictions\u201d is a simple measure of <b>accuracy</b>, it lacks the nuance needed to detect <b>disparate</b> <b>impact</b>.", "dateLastCrawled": "2022-02-01T05:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "The <b>Problem of Bias in Facial Recognition</b> | Center for Strategic and ...", "url": "https://www.csis.org/blogs/technology-policy-blog/problem-bias-facial-recognition", "isFamilyFriendly": true, "displayUrl": "https://www.csis.org/blogs/technology-policy-blog/problem-bias-facial-recognition", "snippet": "Even if an algorithm shows no <b>difference</b> in its <b>accuracy</b> between demographics, its use could still result in a <b>disparate</b> <b>impact</b> if certain groups are over-represented in databases. African American males, for example, are disproportionately represented in the mugshot databases many law enforcement facial recognition systems use for matching. This is the result of larger social trends, but if facial recognition becomes a common policing tool, this could mean that African American males will ...", "dateLastCrawled": "2022-02-02T12:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>GDI Deep Read: Algorithmic Fairness</b> | by Good Data Initiative | Good ...", "url": "https://medium.com/good-data-initiative/gdi-deep-read-algorithmic-fairness-6ccb885f9399", "isFamilyFriendly": true, "displayUrl": "https://medium.com/good-data-initiative/<b>gdi-deep-read-algorithmic-fairness</b>-6ccb885f9399", "snippet": "<b>Disparate</b> <b>impact</b>: negatively ... this metric is similar to <b>disparate</b> <b>impact</b> but takes into account the <b>difference</b> in positive prediction <b>rates</b> across subgroups rather than the ratio. Once again ...", "dateLastCrawled": "2021-02-27T10:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Standards of Fairness for Disparate Impact Assessment</b> of Big Data ...", "url": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3154788", "isFamilyFriendly": true, "displayUrl": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3154788", "snippet": "The goal of individual fairness is satisfied with equal <b>accuracy</b> in classification; while the goal of group fairness allows for some sacrifice of equal <b>accuracy</b> to protect vulnerable groups. To bring this normative dimension into sharper focus, this paper explores the extent to which the choice between the statistical concepts of individual and group fairness is related to a fundamental <b>difference</b> toward the principle that people are entitled to reap the rewards of their own talents and ...", "dateLastCrawled": "2021-12-29T02:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Resolving the <b>Disparate</b> <b>Impact</b> of Uncertainty: Affirmative Action vs ...", "url": "https://www.readkong.com/page/resolving-the-disparate-impact-of-uncertainty-affirmative-7138868", "isFamilyFriendly": true, "displayUrl": "https://www.readkong.com/page/resolving-the-<b>disparate</b>-<b>impact</b>-of-uncertainty...", "snippet": "3 Theory 3.1 Characterizing the <b>disparate</b> <b>impact</b> of uncertainty In this section we describe conditions that are necessary for classifiers to yield equal true and false positive <b>rates</b> to groups with different mean outcomes. We start by briefly introducing our notation and basic definitions. Let Y \u2208 {0, 1} be a binary outcome variable and G \u2208 {L, H} be a protected attribute differentiating two groups. Each group has a unique fixed base rate \u00b5G \u2261 E[Y |G] , where 0 &lt; \u00b5L &lt; \u00b5H &lt; 1. We ...", "dateLastCrawled": "2022-01-20T00:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "fairMLHealth/Evaluating_Fairness.md at integration \u00b7 KenSciResearch ...", "url": "https://github.com/KenSciResearch/fairMLHealth/blob/integration/docs/resources/Evaluating_Fairness.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/KenSciResearch/fairMLHealth/blob/integration/docs/resources/...", "snippet": "Returning to the example: the <b>Disparate</b> <b>Impact</b> Ratio and Statistical Parity <b>Difference</b> are two related measures that compare the selection <b>rates</b> between the protected and unprotected groups. Although the <b>Disparate</b> <b>Impact</b> Ratio in our example is outside of the &quot;fair&quot; range for ratios (it&#39;s above 1.2), the Statistical Parity <b>Difference</b> is well within range for differences. We can see why more clearly by examining the Stratified Performance Table (also above). Here we see that the selection ...", "dateLastCrawled": "2022-02-02T03:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Removing biased data to improve fairness and <b>accuracy</b>", "url": "https://homes.cs.washington.edu/~rjust/publ/improve_fairness_accuracy_tr_2021.pdf", "isFamilyFriendly": true, "displayUrl": "https://homes.cs.washington.edu/~rjust/publ/improve_fairness_<b>accuracy</b>_tr_2021.pdf", "snippet": "Bias in models can have far-reaching societal consequences, <b>like</b> worsening wealth inequality [46], <b>difference</b> in employment rate across genders [24, 80], and <b>difference</b> in incarceration <b>rates</b> across races [63]. Discrimination has been reported in machine-produced decisions for real-life scenarios <b>like</b> parole [53], credit cards [69], hiring [24], and predictive policing [41]. These problems are exac-erbated by humans\u2019 unwarranted confidence in machine-produced decisions: people generally ...", "dateLastCrawled": "2021-12-05T01:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "A Tutorial on <b>Fairness</b> in Machine Learning | by Ziyuan Zhong | Towards ...", "url": "https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-tutorial-on-<b>fairness</b>-in-machine-learning-3ff8ba1040cb", "snippet": "A decision making process suffers from <b>disparate</b> treatment if its decisions are (partly) based on the subject\u2019s sensitive attribute, and it has <b>disparate</b> <b>impact</b> if its outcomes disproportionately hurt (or benefit) people with certain sensitive attribute values (e.g., females, blacks). These two definitions, however, are too abstract for the purpose of computation. As a result, there is no consensus on the mathematical formulations of <b>fairness</b>.", "dateLastCrawled": "2022-02-01T18:57:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Measure <b>Posttraining Data and Model Bias</b> - Amazon SageMaker", "url": "https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-measure-post-training-bias.html", "isFamilyFriendly": true, "displayUrl": "https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-measure-post-training-bias.html", "snippet": "<b>Disparate</b> <b>Impact</b> (DI) Measures the ratio of ... <b>Difference</b> in Acceptance <b>Rates</b> (DAR) Measures the <b>difference</b> in the ratios of the observed positive outcomes (TP) to the predicted positives (TP + FP) between the favored and disfavored facets. Does the model have equal precision when predicting loan acceptances for qualified applicants across all age groups? The range for binary, multicategory facet, and continuous labels is [-1, +1]. Positive values indicate a possible bias against facet d ...", "dateLastCrawled": "2022-01-29T06:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Differential prediction and <b>disparate</b> <b>impact</b> of pretrial risk ...", "url": "https://link.springer.com/article/10.1007/s11292-021-09492-9", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11292-021-09492-9", "snippet": "Instead of a <b>disparate</b> <b>impact</b> of pretrial risk assessments, changes in pretrial release decisions were either negligible for both groups or moved in <b>similar</b> directions. There was little <b>impact</b> on risk assessment on overall <b>rates</b> of pretrial release. However, both Black and White defendants spent fewer days in pretrial detention in the risk assessment condition relative to comparison conditions. For Black defendants, this translated into between four to eight fewer days in pretrial detention ...", "dateLastCrawled": "2022-02-04T12:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Disparate</b> <b>Impact</b> Analysis", "url": "https://h2oai.github.io/tutorials/disparate-impact-analysis/", "isFamilyFriendly": true, "displayUrl": "https://h2oai.github.io/tutorials/<b>disparate</b>-<b>impact</b>-analysis", "snippet": "<b>Disparate</b> <b>Impact</b> Analysis or DIA, which is sometimes called Adverse <b>Impact</b> Analysis, is a way to measure quantitatively the adverse treatment of protected classes, which leads to discrimination in hiring, housing, etc., or in general, any public policy decisions. The regulatory agencies will generally regard a selection rate for any group with less than four-fifths (4\u20445) or eighty percent of the rate for the group with the highest selection rate as constituting evidence of adverse <b>impact</b>.", "dateLastCrawled": "2022-02-03T09:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Tutorial #1: <b>bias and fairness in AI</b> - Borealis AI", "url": "https://www.borealisai.com/en/blog/tutorial1-bias-and-fairness-ai/", "isFamilyFriendly": true, "displayUrl": "https://www.borealisai.com/en/blog/tutorial1-bias-and-fairness-ai", "snippet": "The unintentional unfairness that occurs when a decision has widely different outcomes for different groups is known as <b>disparate</b> <b>impact</b>. As machine learning algorithms are increasingly used to determine important real-world outcomes such as loan approval, pay <b>rates</b>, and parole decisions, it is incumbent on the AI community to minimize unintentional discrimination. This tutorial discusses how bias can be introduced into the machine learning pipeline, what it means for a decision to be fair ...", "dateLastCrawled": "2022-02-02T02:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Bias and Fairness in Machine Learning, Part 1: introducing our dataset ...", "url": "https://freecontent.manning.com/bias-and-fairness-in-machine-learning-part-1-introducing-our-dataset-and-the-problem/", "isFamilyFriendly": true, "displayUrl": "https://freecontent.manning.com/bias-and-fairness-in-machine-learning-part-1...", "snippet": "<b>Disparate</b> Treatment vs <b>Disparate</b> <b>Impact</b> In general, a model\u2014or really any predictive/decision-making process\u2014can suffer from two forms of bias: <b>Disparate</b> Treatment and <b>Disparate</b> <b>Impact</b>. A model is considered to suffer from <b>Disparate</b> Treatment if predictions are in some way based on a sensitive attribute (such as sex or race).", "dateLastCrawled": "2022-01-29T20:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Understanding and Reducing Bias in <b>Machine Learning</b> | by Jaspreet ...", "url": "https://towardsdatascience.com/understanding-and-reducing-bias-in-machine-learning-6565e23900ac", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-and-reducing-bias-in-<b>machine-learning</b>...", "snippet": "<b>Disparate</b> <b>Impact</b>: This type of relative discrimination can be detected if there is a <b>difference</b> in the fraction of positive (negative) outcomes for the different sensitive groups. In the above case this would happen if more percentage of black people were classified as defaulters as compared to white people. We formalise this requirement as: i.e. the probability of a positive label (returning loans) for z=1(white clients) is the same as the probability of positive label for z=0 (black ...", "dateLastCrawled": "2022-01-28T17:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A Tutorial on <b>Fairness</b> in Machine Learning | by Ziyuan Zhong | Towards ...", "url": "https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-tutorial-on-<b>fairness</b>-in-machine-learning-3ff8ba1040cb", "snippet": "These laws typically evaluate the <b>fairness</b> of a decision making process using two distinct notions (Barocas and Selbst, 2016): <b>disparate</b> treatment and <b>disparate</b> <b>impact</b>. A decision making process suffers from <b>disparate</b> treatment if its decisions are (partly) based on the subject\u2019s sensitive attribute, and it has <b>disparate</b> <b>impact</b> if its outcomes disproportionately hurt (or benefit) people with certain sensitive attribute values (e.g., females, blacks). These two definitions, however, are too ...", "dateLastCrawled": "2022-02-01T18:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Standards of Fairness for Disparate Impact Assessment</b> of Big Data ...", "url": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3154788", "isFamilyFriendly": true, "displayUrl": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3154788", "snippet": "The goal of individual fairness is satisfied with equal <b>accuracy</b> in classification; while the goal of group fairness allows for some sacrifice of equal <b>accuracy</b> to protect vulnerable groups. To bring this normative dimension into sharper focus, this paper explores the extent to which the choice between the statistical concepts of individual and group fairness is related to a fundamental <b>difference</b> toward the principle that people are entitled to reap the rewards of their own talents and ...", "dateLastCrawled": "2021-12-29T02:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "fairMLHealth/Evaluating_Fairness.md at integration \u00b7 KenSciResearch ...", "url": "https://github.com/KenSciResearch/fairMLHealth/blob/integration/docs/resources/Evaluating_Fairness.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/KenSciResearch/fairMLHealth/blob/integration/docs/resources/...", "snippet": "Returning to the example: the <b>Disparate</b> <b>Impact</b> Ratio and Statistical Parity <b>Difference</b> are two related measures that compare the selection <b>rates</b> between the protected and unprotected groups. Although the <b>Disparate</b> <b>Impact</b> Ratio in our example is outside of the &quot;fair&quot; range for ratios (it&#39;s above 1.2), the Statistical Parity <b>Difference</b> is well within range for differences. We can see why more clearly by examining the Stratified Performance Table (also above). Here we see that the selection ...", "dateLastCrawled": "2022-02-02T03:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>aif360.metrics.ClassificationMetric</b> - aif360 0.4.0 documentation", "url": "https://aif360.readthedocs.io/en/latest/modules/generated/aif360.metrics.ClassificationMetric.html", "isFamilyFriendly": true, "displayUrl": "https://aif360.readthedocs.io/en/latest/modules/generated/aif360.metrics...", "snippet": "Individual fairness metric from [1]_ that measures how <b>similar</b> the labels are for <b>similar</b> instances. <b>difference</b>: Compute <b>difference</b> of the metric for unprivileged and privileged groups. differential_fairness_bias_amplification: Bias amplification is the <b>difference</b> in smoothed EDF between the classifier and the original dataset. <b>disparate</b>_<b>impact</b>", "dateLastCrawled": "2022-02-03T02:00:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "What is &quot;<b>Disparate</b> <b>Impact</b>?&quot;", "url": "https://www.wellworkforce.com/what-is-disparate-impact", "isFamilyFriendly": true, "displayUrl": "https://www.wellworkforce.com/what-is-<b>disparate</b>-<b>impact</b>", "snippet": "<b>Accuracy</b> of testing After the PDA is complete and job-specific tests have been developed, a representative from your organization is asked to review and approve all testing criteria to ensure <b>accuracy</b> and efficacy. <b>Accuracy</b> of testing criteria as compared to essential job functions is critical to a valid and legally-defensible testing process. The ADA/EEOC require post-offer testing protocols to be \u201cjob-related and consistent with business necessity,\u201d meaning they accurately reflect the ...", "dateLastCrawled": "2022-01-10T09:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Fair prediction with <b>disparate</b> <b>impact</b>: A study of bias in recidivism ...", "url": "https://www.arxiv-vanity.com/papers/1703.00056/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1703.00056", "snippet": "In this section we show how differences in false positive and false negative <b>rates</b> <b>can</b> result in <b>disparate</b> <b>impact</b> under policies where a high-risk assessment results in a stricter penalty for the defendant. Such situations may arise when risk assessments are used to inform bail, parole, or sentencing decisions. In Pennsylvania and Virginia, for instance, statutes permit the use of RPI\u2019s in sentencing, provided that the sentence ultimately falls within accepted guidelines", "dateLastCrawled": "2022-01-11T20:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Certifying and removing disparate impact</b> | DeepAI", "url": "https://deepai.org/publication/certifying-and-removing-disparate-impact", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>certifying-and-removing-disparate-impact</b>", "snippet": "We note that <b>disparate</b> <b>impact</b> itself is not illegal; in hiring decisions, business necessity arguments <b>can</b> be made to excuse <b>disparate</b> <b>impact</b>. Definition 1.1 ( <b>Disparate</b> <b>Impact</b> (\u201c80% rule\u201d)) . Given data set D = ( X , Y , C ) , with protected attribute X (e.g., race, sex, religion, etc.), remaining attributes Y , and binary class to be predicted C (e.g., \u201cwill hire\u201d), we will say that D has <b>disparate</b> <b>impact</b> if", "dateLastCrawled": "2022-01-22T13:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Fairness metrics and bias mitigation strategies for rating predictions ...", "url": "https://www.sciencedirect.com/science/article/pii/S0306457321001369", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0306457321001369", "snippet": "Formally, <b>Disparate</b> <b>Impact</b> is defined as: (2) D ... This <b>difference</b> \u03b4 <b>can</b> represent different types of unfairness, e.g., if \u03b4 is different for subgroups of users (thus indicating potential unfairness in predicted ratings). Assuming that the data used for training the recommender system is comparable to the test data/new data the recommender system is applied on, the ratings <b>can</b> be actively adjusted using the learned differences \u03b4. Here, we consider several versions of the bias mitigation ...", "dateLastCrawled": "2022-02-03T07:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "HUD&#39;s <b>Implementation of the Fair Housing</b> Act&#39;s <b>Disparate</b> <b>Impact</b> Standard", "url": "https://www.federalregister.gov/documents/2020/09/24/2020-19887/huds-implementation-of-the-fair-housing-acts-disparate-impact-standard", "isFamilyFriendly": true, "displayUrl": "https://<b>www.federalregister.gov</b>/documents/2020/09/24/2020-19887/huds-implementation-of...", "snippet": "One commenter <b>thought</b> that, if implemented, the Proposed Rule would take HUD one step closer to making communities a better place. Commenters also stated the Proposed Rule is effective in uncovering discrimination and ensuring <b>disparate</b> <b>impact</b> cases <b>can</b> be brought forward, while still being consistent with the Act. Commenters stated that the Proposed Rule would specifically incentivize Start Printed Page 60291 parties to work together and may reduce frivolous and arbitrary claims without ...", "dateLastCrawled": "2022-01-30T03:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Handling Discriminatory Biases in Data for <b>Machine Learning</b> | by ...", "url": "https://towardsdatascience.com/machine-learning-and-discrimination-2ed1a8b01038", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>machine-learning</b>-and-<b>discrimination</b>-2ed1a8b01038", "snippet": "There are two forms of <b>discrimination</b> that we will refer to as <b>disparate</b> <b>impact</b> and <b>disparate</b> treatment. ... Even when we optimize for <b>accuracy</b>, <b>machine learning</b> algorithms may perpetuate <b>discrimination</b> even when we work from an unbiased data set and have a performance task that has social goods in mind. What else could we do? Sequential learning; More theory; Causal modeling; Optimizing for fairness; Of all of these, optimizing for fairness seems like the easiest and the best course of ...", "dateLastCrawled": "2022-01-29T20:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Big data, <b>disparate impact, and the neoliberal mindset</b> | mathbabe", "url": "https://mathbabe.org/2015/09/07/big-data-disparate-impact-and-the-neoliberal-mindset/", "isFamilyFriendly": true, "displayUrl": "https://mathbabe.org/2015/09/07/big-data-<b>disparate-impact-and-the-neoliberal-mindset</b>", "snippet": "Big data, <b>disparate impact, and the neoliberal mindset</b>. September 7, 2015 Cathy O&#39;Neil, mathbabe. When you\u2019re writing a book for the general public\u2019s consumption, you have to keep things pretty simple. You <b>can</b>\u2019t spend a lot of time theorizing about why some stuff is going on, you have to focus on what\u2019s happening, and how bad it is, and ...", "dateLastCrawled": "2022-01-28T06:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Does mitigating ML&#39;s <b>impact</b> disparity require treatment ... - NeurIPS", "url": "https://proceedings.neurips.cc/paper/2018/file/8e0384779e58ce2af40eb365b318cc32-Paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/2018/file/8e0384779e58ce2af40eb365b318cc32-Paper.pdf", "snippet": "based on irrelevant features, and <b>can</b> harm some members of the protected group. 3.DLPs provide a suboptimal trade-off between <b>accuracy</b> and <b>impact</b> parity. 4. While <b>disparate</b> treatment is by de\ufb01nition illegal, the status of treatment disparity is debated [9]. 2 <b>Disparate</b> Learning Processes", "dateLastCrawled": "2021-12-18T14:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Mirror Mirror", "url": "https://shiraamitchell.github.io/fairness/", "isFamilyFriendly": true, "displayUrl": "https://shiraamitchell.github.io/fairness", "snippet": "<b>Disparate</b> <b>impact</b>: facially neutral ... Equal False Negative <b>Rates</b>: the fraction of positives which are marked negative in each group agree. Equal False Positive <b>Rates</b>: the fraction of negatives which are marked positive in each group agree. Equal Positive Predictive Values: the fraction of those marked positive which are actually positive in each group agree. It is useful to take a moment to consider that these definitions are genuinely different, both mathematically and as principles of ...", "dateLastCrawled": "2022-01-06T19:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Angela Zhou | publications", "url": "https://angelamzhou.github.io/publications/", "isFamilyFriendly": true, "displayUrl": "https://angelamzhou.github.io/publications", "snippet": "The increasing <b>impact</b> of algorithmic decisions on people\u2019s lives compels us to scrutinize their fairness and, in particular, the <b>disparate</b> impacts that ostensibly-color-blind algorithms <b>can</b> have on different groups. Examples include credit decisioning, hiring, advertising, criminal justice, personalized medicine, and targeted policymaking, where in some cases legislative or regulatory frameworks for fairness exist and define specific protected classes. In this paper we study a fundamental ...", "dateLastCrawled": "2022-02-01T18:50:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Differential prediction and <b>disparate</b> <b>impact</b> of pretrial risk ...", "url": "https://link.springer.com/article/10.1007/s11292-021-09492-9", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11292-021-09492-9", "snippet": "Instead of a <b>disparate</b> <b>impact</b> of pretrial risk assessments, changes in pretrial release decisions were either negligible for both groups or moved in similar directions. There was little <b>impact</b> on risk assessment on overall <b>rates</b> of pretrial release. However, both Black and White defendants spent fewer days in pretrial detention in the risk assessment condition relative to comparison conditions. For Black defendants, this translated into between four to eight fewer days in pretrial detention ...", "dateLastCrawled": "2022-02-04T12:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Disparate</b> <b>Impact</b> Analysis", "url": "https://h2oai.github.io/tutorials/disparate-impact-analysis/", "isFamilyFriendly": true, "displayUrl": "https://h2oai.github.io/tutorials/<b>disparate</b>-<b>impact</b>-analysis", "snippet": "If we had to break this down, the <b>Disparate</b> <b>Impact</b> Analysis basically took model prediction results which was from 11K males, 18k females and looked at various measures such as <b>Accuracy</b>, Adverse <b>Impact</b>, True Positive Rate, Precision, Recall, etc., across both binary classes and then found the ratios are not comparable across the two groups over the cutoff value. The results for the Female was not more than four-fifths of the reference class Male \u2014 which means adverse <b>impact</b> by its basic ...", "dateLastCrawled": "2022-02-03T09:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Measure <b>Posttraining Data and Model Bias</b> - Amazon SageMaker", "url": "https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-measure-post-training-bias.html", "isFamilyFriendly": true, "displayUrl": "https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-measure-post-training-bias.html", "snippet": "<b>Disparate</b> <b>Impact</b> (DI) Measures the ratio of ... <b>Difference</b> in Acceptance <b>Rates</b> (DAR) Measures the <b>difference</b> in the ratios of the observed positive outcomes (TP) to the predicted positives (TP + FP) between the favored and disfavored facets. Does the model have equal precision when predicting loan acceptances for qualified applicants across all age groups? The range for binary, multicategory facet, and continuous labels is [-1, +1]. Positive values indicate a possible bias against facet d ...", "dateLastCrawled": "2022-01-29T06:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Understanding and Reducing Bias in <b>Machine Learning</b> | by Jaspreet ...", "url": "https://towardsdatascience.com/understanding-and-reducing-bias-in-machine-learning-6565e23900ac", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-and-reducing-bias-in-<b>machine-learning</b>...", "snippet": "<b>Disparate</b> <b>Impact</b>: This type of relative discrimination <b>can</b> be detected if there is a <b>difference</b> in the fraction of positive (negative) outcomes for the different sensitive groups. In the above case this would happen if more percentage of black people were classified as defaulters as <b>compared</b> to white people. We formalise this requirement as: i.e. the probability of a positive label (returning loans) for z=1(white clients) is the same as the probability of positive label for z=0 (black ...", "dateLastCrawled": "2022-01-28T17:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Questions and Answers on EEOC Final Rule on <b>Disparate</b> <b>Impact</b> and ...", "url": "https://www.eeoc.gov/regulations/questions-and-answers-eeoc-final-rule-disparate-impact-and-reasonable-factors-other-age", "isFamilyFriendly": true, "displayUrl": "https://<b>www.eeoc.gov</b>/regulations/questions-and-answers-eeoc-final-rule-<b>disparate</b>...", "snippet": "If <b>disparate</b> <b>impact</b> is established, the employer <b>can</b> support an RFOA defense with evidence that would be admissible in court, including testimony. The rule does not change existing recordkeeping requirements under the ADEA ( see 29 C.F.R. Part 1627); it does not require, and should not prompt, documentation other than that which an employer would make as part of its normal business operations.", "dateLastCrawled": "2022-01-30T06:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Chapter 7 <b>People-Based ML Models: Algorithmic Fairness</b> | Public Policy ...", "url": "https://urbanspatial.github.io/PublicPolicyAnalytics/people-based-ml-models-algorithmic-fairness.html", "isFamilyFriendly": true, "displayUrl": "https://urbanspatial.github.io/PublicPolicyAnalytics/people-based-ml-models...", "snippet": "7.1.3 <b>Accuracy</b> and generalizability in recidivism algorithms. <b>Accuracy</b> and generalizability continue to be the two yardsticks we use to measure the utility of our algorithms. The goal of a recidivism classifier is to predict two binary outcomes - Recidivate and notRecidivate.While the \u201cpercent of correct predictions\u201d is a simple measure of <b>accuracy</b>, it lacks the nuance needed to detect <b>disparate</b> <b>impact</b>.", "dateLastCrawled": "2022-02-01T05:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "How Data Scientists Help Regulators and Banks Ensure Fairness when ...", "url": "https://facctconference.org/static/tutorials/schmidt_banking18.pdf", "isFamilyFriendly": true, "displayUrl": "https://facctconference.org/static/tutorials/schmidt_banking18.pdf", "snippet": "an equally business justified model with less <b>disparate</b> <b>impact</b> <b>can</b> be found. In traditional GLMs, the process of finding less discriminatory models <b>can</b> be done relatively easily, as one <b>can</b> readily calculate each variable\u2019s discriminatory <b>impact</b> on the outcome, and then make changes to the model to lessen that <b>impact</b>. But no such simple process exists with ML and AI because of the high degree of nonlinearity and the collinearity of a model\u2019s features. While one predictor in combination ...", "dateLastCrawled": "2021-12-02T14:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "fairMLHealth/Evaluating_Fairness.md at integration \u00b7 KenSciResearch ...", "url": "https://github.com/KenSciResearch/fairMLHealth/blob/integration/docs/resources/Evaluating_Fairness.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/KenSciResearch/fairMLHealth/blob/integration/docs/resources/...", "snippet": "The Balanced <b>Accuracy</b> <b>Difference</b> (or Ratio) compares the Balanced <b>Accuracy</b> between groups, ... And unawareness does not always reduce <b>Disparate</b> <b>Impact</b>; in fact it <b>can</b> increase it, as we showed in the KDD 2020 Tutorial which <b>compared</b> fairness relative to gender using this same general setup. This goes to say that the field has not yet found a panacea which <b>can</b> correct all fairness issues for every model, so it&#39;s important to test different approaches. Also remember to consider the effects of ...", "dateLastCrawled": "2022-02-02T03:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Model Fairness &amp; Transparency</b>. A Project on Detecting, Understanding ...", "url": "https://medium.com/sfu-cspmp/model-transparency-fairness-552a747b444", "isFamilyFriendly": true, "displayUrl": "https://<b>medium</b>.com/sfu-cspmp/model-transparency-fairness-552a747b444", "snippet": "The metric lied between 0.8\u20131.2 in all other combinations but for African-American race as the unprivileged and Caucasian as the privileged group, the <b>Disparate</b> <b>Impact</b> was found to be 0.7 ( less ...", "dateLastCrawled": "2022-01-19T03:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Removing biased data to improve fairness and <b>accuracy</b>", "url": "https://homes.cs.washington.edu/~rjust/publ/improve_fairness_accuracy_tr_2021.pdf", "isFamilyFriendly": true, "displayUrl": "https://homes.cs.washington.edu/~rjust/publ/improve_fairness_<b>accuracy</b>_tr_2021.pdf", "snippet": "lawsuits and widespread criticism [69, 93]. Biased decisions <b>can</b> be challenged on the basis of <b>disparate</b> treatment and <b>disparate</b> <b>impact</b> laws in the US [6, 52, 99, 102, 118]. Similar laws exist in other countries [5]. Laws definesensitive attributes (e.g., race, sex, religion) that are illegal to use as the basis of any decision.", "dateLastCrawled": "2021-12-05T01:43:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A New Metric for Quantifying <b>Machine</b> <b>Learning</b> Fairness in Healthcare ...", "url": "https://towardsdatascience.com/a-new-metric-for-quantifying-machine-learning-fairness-in-healthcare-closedloop-ai-fc07b9c83487", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-new-metric-for-quantifying-<b>machine</b>-<b>learning</b>-fairness...", "snippet": "<b>Disparate</b> <b>impact</b> occurs when the predicted outcomes are different for different groups. Some examples of when this metric is used are recidivism[4], hiring[5][6], and loan applications[2]. This standard metric accounts for only one factor: the rate at which the algorithm predicts a person should benefit from a particular classification. In the context of healthcare, the standard of <b>disparate</b> <b>impact</b> is entirely inappropriate. The above examples have a common characteristic; every individual ...", "dateLastCrawled": "2022-01-17T14:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Assessing <b>Disparate</b> <b>Impact</b> of Personalized Interventions ...", "url": "https://par.nsf.gov/servlets/purl/10168517", "isFamilyFriendly": true, "displayUrl": "https://par.nsf.gov/servlets/purl/10168517", "snippet": "result in <b>disparate</b> <b>impact</b> (with regards to social welfare) for the same reasons that these disparities occur in <b>machine</b> <b>learning</b> classi\ufb01cation models [21]. (See Appendix C for an expanded discussion on our use of the term \u201c<b>disparate</b> <b>impact</b>.\u201d) However, in the problem of personalized interventions, the \u201cfundamental problem of causal inference,\u201d that outcomes are not observed for interventions not administered, poses a fundamental challenge for evaluating the fairness of any ...", "dateLastCrawled": "2022-01-20T02:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Removing <b>Disparate</b> <b>Impact</b> on Model Accuracy in Differentially Private ...", "url": "https://www.researchgate.net/publication/353907927_Removing_Disparate_Impact_on_Model_Accuracy_in_Differentially_Private_Stochastic_Gradient_Descent", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/353907927_Removing_<b>Disparate</b>_<b>Impact</b>_on_Model...", "snippet": "<b>Machine</b> <b>learning</b> is a tool for building models that accurately represent input training data. When undesired biases concerning demographic groups are in the training data, well-trained models will ...", "dateLastCrawled": "2022-01-28T09:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A New Metric for Quantifying <b>Machine</b> <b>Learning</b> Fairness in Healthcare", "url": "https://www.closedloop.ai/post/a-new-metric-for-quantifying-machine-learning-fairness-in-healthcare", "isFamilyFriendly": true, "displayUrl": "https://www.closedloop.ai/post/a-new-metric-for-quantifying-<b>machine</b>-<b>learning</b>-fairness...", "snippet": "Often, the approach to fairness draws upon the legal standard of <b>disparate</b> <b>impact</b>[2][3]. <b>Disparate</b> <b>impact</b> occurs when the predicted outcomes are different for different groups. Some examples of when this metric is used are recidivism[4], hiring[5][6], and loan applications[2]. This standard metric accounts for only one factor: the rate at which the algorithm predicts a person should benefit from a particular classification.", "dateLastCrawled": "2022-02-01T00:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Feature Engineering for Machine Learning</b>: Why and How | by ...", "url": "https://medium.com/analytics-vidhya/feature-engineering-for-machine-learning-stem-to-shtem-submission-76903112e437", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/<b>feature-engineering-for-machine-learning</b>-stem-to...", "snippet": "Here\u2019s a simple <b>analogy</b>: a student named Timmy, analogous to a supervised <b>machine</b> <b>learning</b> model, has spent the last few weeks studying for a math test so that he can answer questions correctly ...", "dateLastCrawled": "2021-09-13T20:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Machine learning</b>, artificial neural networks and social research ...", "url": "https://link.springer.com/article/10.1007/s11135-020-01037-y", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11135-020-01037-y", "snippet": "<b>Machine learning</b> (ML), and particularly algorithms based on artificial neural networks (ANNs), constitute a field of research lying at the intersection of different disciplines such as mathematics, statistics, computer science and neuroscience. This approach is characterized by the use of algorithms to extract knowledge from large and heterogeneous data sets. In addition to offering a brief introduction to ANN algorithms-based ML, in this paper we will focus our attention on its possible ...", "dateLastCrawled": "2022-01-27T04:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Responsible machine learning</b> protects intellectual property | World ...", "url": "https://www.weforum.org/agenda/2021/03/responsible-machine-learning-that-protects-intellectual-property/", "isFamilyFriendly": true, "displayUrl": "https://www.weforum.org/agenda/2021/03/<b>responsible-machine-learning</b>-that-protects...", "snippet": "Given <b>machine</b> <b>learning</b>\u2019s complexity and interdisciplinary nature, executives should employ a wide variety of approaches to manage the associated risks, which include building risk management into model development and applying holistic risk frameworks that leverage and adapt principles used in managing other types of enterprise risk. Executives must also simply step back regularly to consider the broad implications for employees and society when using ML.", "dateLastCrawled": "2022-01-29T18:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Machine Learning is Popular</b> Right Now", "url": "https://machinelearningmastery.com/machine-learning-is-popular/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>machine-learning-is-popular</b>", "snippet": "Abundant and cheap computation has driven the abundance of data we are collecting and the increase in capability of <b>machine learning</b> methods. In this post you learned that <b>machine learning is popular</b> now for three reasons: The field has matured both in terms of identity and in terms of methods and tools. There is an abundance of data to learn from.", "dateLastCrawled": "2022-02-03T09:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Structural disconnects between algorithmic decision-making</b> and the law ...", "url": "https://blogs.icrc.org/law-and-policy/2019/04/25/structural-disconnects-algorithmic-decision-making-law/", "isFamilyFriendly": true, "displayUrl": "https://blogs.icrc.org/law-and-policy/2019/04/25/structural-disconnects-algorithmic...", "snippet": "And the definition of \u2018works\u2019 is based on (in the case of <b>machine</b> <b>learning</b>) compliance with some prespecified examples of scenarios that \u2018work\u2019 and scenarios that \u2018don\u2019t\u2019. To use a legal <b>analogy</b>, this would be analogous to defining a fair decision by coming up with a rule based on past decisions that someone decided were \u2018right\u2019 or \u2018wrong\u2019 based on past outcomes. In one sense this is entirely circular: we are deciding what is \u2018right\u2019 based on someone deciding what ...", "dateLastCrawled": "2022-01-25T19:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Data lineage: Making artificial intelligence smarter</b> | <b>SAS</b> India", "url": "https://www.sas.com/en_in/insights/articles/data-management/data-lineage--making-artificial-intelligence-smarter.html", "isFamilyFriendly": true, "displayUrl": "https://<b>www.sas.com</b>/en_in/insights/articles/data-management/data-lineage--making...", "snippet": "Data lineage enables you to trace data quality issues and other errors back to their root cause and perform <b>impact</b> analysis on proposed changes. As it links data in <b>disparate</b> systems at a logical level by showing how metadata is connected, data lineage helps identify business rule discrepancies and data incompleteness. Data lineage also helps data stewards react to issues before they become a problem, define strategies for data quality improvement and promote effective reuse of existing ...", "dateLastCrawled": "2022-01-30T19:31:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(disparate impact)  is like +(difference in accuracy rates)", "+(disparate impact) is similar to +(difference in accuracy rates)", "+(disparate impact) can be thought of as +(difference in accuracy rates)", "+(disparate impact) can be compared to +(difference in accuracy rates)", "machine learning +(disparate impact AND analogy)", "machine learning +(\"disparate impact is like\")", "machine learning +(\"disparate impact is similar\")", "machine learning +(\"just as disparate impact\")", "machine learning +(\"disparate impact can be thought of as\")", "machine learning +(\"disparate impact can be compared to\")"]}