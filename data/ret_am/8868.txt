{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 0, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Empirical risk minimization</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Empirical_risk_minimization", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Empirical_risk_minimization</b>", "snippet": "<b>Empirical risk minimization</b> (<b>ERM</b>) is a principle in statistical <b>learning</b> theory which defines a family of <b>learning</b> algorithms and is used to give theoretical bounds on their performance. The core idea is that we cannot know exactly how well an <b>algorithm</b> will work in practice (the true &quot;<b>risk</b>&quot;) because we don&#39;t know the true distribution of data that the <b>algorithm</b> will work on, but we can instead measure its performance on a known set of training data (the &quot;<b>empirical</b>&quot; <b>risk</b>).", "dateLastCrawled": "2022-02-03T06:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>)", "url": "https://mdav.ece.gatech.edu/ece-6254-spring2022/notes/03-bayes-nearest-neighbors-marked.pdf", "isFamilyFriendly": true, "displayUrl": "https://mdav.ece.gatech.edu/ece-6254-spring2022/notes/03-bayes-nearest-neighbors...", "snippet": "<b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) Recall the definitions of <b>risk</b>/<b>empirical</b> <b>risk</b> Ideally, we would <b>like</b> to choose Since we do not actually know , instead we choose", "dateLastCrawled": "2022-01-21T05:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Learning</b> Theory: <b>Empirical Risk</b> <b>Minimization</b> | by Marin Vlastelica ...", "url": "https://towardsdatascience.com/learning-theory-empirical-risk-minimization-d3573f90ff77", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>learning</b>-theory-<b>empirical-risk</b>-<b>minimization</b>-d3573f90ff77", "snippet": "<b>Empirical Risk</b> <b>Minimization</b> is a fundamental concept in <b>machine</b> <b>learning</b>, yet surprisingly many prac t itioners are not familiar with it. Understanding <b>ERM</b> is essential to understanding the limits of <b>machine</b> <b>learning</b> algorithms and to form a good basis for practical problem-solving skills. The theory behind <b>ERM</b> is the theory that explains the VC-dimension, Probably Approximately Correct (PAC) <b>Learning</b> and other fundamental concepts. In my opinion, anybody that is serious about <b>machine</b> ...", "dateLastCrawled": "2022-02-03T00:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "11.1 <b>Empirical</b> <b>Risk</b> <b>Minimization</b> - Carnegie Mellon University", "url": "https://www.cs.cmu.edu/~aarti/Class/10704_Fall16/lec11.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.cmu.edu/~aarti/Class/10704_Fall16/lec11.pdf", "snippet": "10-704: Information Processing and <b>Learning</b> Fall 2016 Lecture 11: Oct 5 Lecturer: Aarti Singh Note: These notes are based on scribed notes from Spring15 o ering of this course. LaTeX template courtesy of UC Berkeley EECS dept. Disclaimer: These notes have not been subjected to the usual scrutiny reserved for formal publications. They may be distributed outside this class only with the permission of the Instructor. 11.1 <b>Empirical</b> <b>Risk</b> <b>Minimization</b> In many <b>machine</b> <b>learning</b> task, we have data ...", "dateLastCrawled": "2022-01-29T15:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Tilted <b>Empirical</b> <b>Risk</b> <b>Minimization</b> \u2013 <b>Machine</b> <b>Learning</b> Blog | ML@CMU ...", "url": "https://blog.ml.cmu.edu/2021/04/02/term/", "isFamilyFriendly": true, "displayUrl": "https://blog.ml.cmu.edu/2021/04/02/t<b>erm</b>", "snippet": "Our work explores tilted <b>empirical</b> <b>risk</b> <b>minimization</b> (TERM), a simple and general alternative to <b>ERM</b>, which is ubiquitous throughout <b>machine</b> <b>learning</b>. Our hope is that the TERM framework will allow <b>machine</b> <b>learning</b> practitioners to easily modify the <b>ERM</b> objective to handle practical concerns such as enforcing fairness amongst subgroups, mitigating the effect of outliers, and ensuring robust performance on new, unseen data. Critical to the success of such a framework is understanding the ...", "dateLastCrawled": "2022-01-30T13:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "In-depth analysis of the regularized least-squares <b>algorithm</b> over the ...", "url": "https://towardsdatascience.com/in-depth-analysis-of-the-regularized-least-squares-algorithm-over-the-empirical-risk-minimization-729a1433447f", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/in-depth-analysis-of-the-regularized-least-squares...", "snippet": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) <b>ERM</b> is a widely known concept in <b>machine</b> <b>learning</b>, and I recommend going over this explanation about <b>ERM</b> before proceeding to the actual implementation. <b>ERM</b> is used to classify the performance of <b>learning</b> algorithms, and we can solve the <b>ERM</b> optimization problem by finding a vector w that minimizes the formula ...", "dateLastCrawled": "2022-01-26T02:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Risk</b> <b>Minimization</b> from Adaptively Collected Data: Guarantees for ...", "url": "https://proceedings.neurips.cc/paper/2021/file/a0ae15571eb4a97ac1c34a114f1bb179-Paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/2021/file/a0ae15571eb4a97ac1c34a114f1bb179-Paper.pdf", "snippet": "<b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) is the workhorse of <b>machine</b> <b>learning</b>, whether for classi\ufb01cation and regression or for off-policy policy <b>learning</b>, but its model- agnostic guarantees can fail when we use adaptively collected data, such as the result of running a contextual bandit <b>algorithm</b>. We study a generic importance sampling weighted <b>ERM</b> <b>algorithm</b> for using adaptively collected data to minimize the average of a loss function over a hypothesis class and provide \ufb01rst-of-their-kind ...", "dateLastCrawled": "2022-01-18T19:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Tilted <b>Empirical Risk</b> <b>Minimization</b> | DeepAI", "url": "https://deepai.org/publication/tilted-empirical-risk-minimization", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/tilted-<b>empirical-risk</b>-<b>minimization</b>", "snippet": "<b>Empirical risk</b> <b>minimization</b> (<b>ERM</b>) is typically designed to perform well on the average loss, which can result in estimators that are sensitive to outliers, generalize poorly, or treat subgroups unfairly.While many methods aim to address these problems individually, in this work, we explore them through a unified framework\u2014tilted <b>empirical risk</b> <b>minimization</b> (TERM).", "dateLastCrawled": "2022-01-11T23:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "PRIVACY PRESERVING <b>MACHINE</b> <b>LEARNING</b>", "url": "http://researchers.lille.inria.fr/abellet/teaching/ppml_lectures/lec4.pdf", "isFamilyFriendly": true, "displayUrl": "researchers.lille.inria.fr/abellet/teaching/ppml_lectures/lec4.pdf", "snippet": "\u2022 <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) consists in choosing the parameters \u03b8\u02c6\u2208 argmin \u03b8\u2208\u0398 [F(\u03b8;D) := \u02c6R(\u03b8;D)+\u03bb\u03c8(\u03b8)] \u2022 \u03c8is a regularizer and \u03bb\u2265 0 a trade-off parameter 7. USEFUL PROPERTIES \u2022 We typically work with loss functions that are differentiable in \u03b8: for (x,y) \u2208 X \u00d7Y, we denote the gradient of L at \u03b8by \u2207L(\u03b8;x,y) \u2208 Rp \u2022 We also <b>like</b> the loss function, its gradient and/or the regularizer to be Lipschitz Definition (Lipschitz function) Let l &gt;0. A ...", "dateLastCrawled": "2022-01-30T21:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Privacy in <b>Statistics and Machine Learning Spring 2021 Lecture</b> 11 ...", "url": "https://dpcourse.github.io/lecnotes-web/lec-11-ERM.pdf", "isFamilyFriendly": true, "displayUrl": "https://dpcourse.github.io/lecnotes-web/lec-11-<b>ERM</b>.pdf", "snippet": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) for Decomposable Losses These examples \u02d9t a general framework: there is a loss function !\u201eF;x\u201dwhich takes a parameter vector Fand a dataset x 2U = . Many loss functions arising in statistics and ML are decomposable, meaning they can be written as a", "dateLastCrawled": "2021-11-18T11:43:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>)", "url": "https://mdav.ece.gatech.edu/ece-6254-spring2022/notes/03-bayes-nearest-neighbors-marked.pdf", "isFamilyFriendly": true, "displayUrl": "https://mdav.ece.gatech.edu/ece-6254-spring2022/notes/03-bayes-nearest-neighbors...", "snippet": "Using a <b>similar</b> argument as before, one can show that Thus, by letting and as , we can (asymptotically) expect to perform arbitrarily close to the Bayes <b>risk</b> This is known as universal consistency: given enough data, the <b>algorithm</b> will eventually converge to a classifier that matches the Bayes <b>risk</b>", "dateLastCrawled": "2022-01-21T05:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "11.1 <b>Empirical</b> <b>Risk</b> <b>Minimization</b> - Carnegie Mellon University", "url": "https://www.cs.cmu.edu/~aarti/Class/10704_Fall16/lec11.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.cmu.edu/~aarti/Class/10704_Fall16/lec11.pdf", "snippet": "Hence, we choose the f^ that minimizes the <b>empirical</b> <b>risk</b> over some class F, such as parametric models, histogram classi ers, decision trees or linear/polynomial functions, etc. f^<b>ERM</b> = argmin f2F R^(f) (11.3) To justify this <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) method, we need to know how <b>similar</b> the R(f) and R^(f) are. For bounded loss function ...", "dateLastCrawled": "2022-01-29T15:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Tilted <b>Empirical</b> <b>Risk</b> <b>Minimization</b> \u2013 <b>Machine</b> <b>Learning</b> Blog | ML@CMU ...", "url": "https://blog.ml.cmu.edu/2021/04/02/term/", "isFamilyFriendly": true, "displayUrl": "https://blog.ml.cmu.edu/2021/04/02/t<b>erm</b>", "snippet": "Our work explores tilted <b>empirical</b> <b>risk</b> <b>minimization</b> (TERM), a simple and general alternative to <b>ERM</b>, which is ubiquitous throughout <b>machine</b> <b>learning</b>. Our hope is that the TERM framework will allow <b>machine</b> <b>learning</b> practitioners to easily modify the <b>ERM</b> objective to handle practical concerns such as enforcing fairness amongst subgroups, mitigating the effect of outliers, and ensuring robust performance on new, unseen data. Critical to the success of such a framework is understanding the ...", "dateLastCrawled": "2022-01-30T13:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Private <b>Empirical Risk</b> <b>Minimization</b>: Efficient Algorithms and Tight ...", "url": "https://par.nsf.gov/servlets/purl/10092778", "isFamilyFriendly": true, "displayUrl": "https://par.nsf.gov/servlets/purl/10092778", "snippet": "in <b>machine</b> <b>learning</b> and statistics. We provide new algorithms and matching lower bounds for differentially private convex <b>empirical risk</b> <b>minimization</b> assuming only that each data point\u2019s contribution to the loss function is Lipschitz and that the domain of optimization is bounded. We provide a separate set of algorithms and matching lower bounds for the setting in which the loss functions are known to also be strongly convex. Our algorithms run in polynomial time, and in some cases even ...", "dateLastCrawled": "2022-01-28T12:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "In-depth analysis of the regularized least-squares <b>algorithm</b> over the ...", "url": "https://towardsdatascience.com/in-depth-analysis-of-the-regularized-least-squares-algorithm-over-the-empirical-risk-minimization-729a1433447f", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/in-depth-analysis-of-the-regularized-least-squares...", "snippet": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) <b>ERM</b> is a widely known concept in <b>machine</b> <b>learning</b>, and I recommend going over this explanation about <b>ERM</b> before proceeding to the actual implementation. <b>ERM</b> is used to classify the performance of <b>learning</b> algorithms, and we can solve the <b>ERM</b> optimization problem by finding a vector w that minimizes the formula ...", "dateLastCrawled": "2022-01-26T02:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Stability Properties of Empirical Risk Minimization over Donsker</b> Classes", "url": "https://www.mit.edu/~rakhlin/papers/erm.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.mit.edu/~rakhlin/papers/<b>erm</b>.pdf", "snippet": "Keywords: <b>empirical</b> <b>risk</b> <b>minimization</b>, <b>empirical</b> processes, stability, Donsker classes 1. Introduction The <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) <b>algorithm</b> has been studied in <b>learning</b> theory to a great extent. Vapnik and Chervonenkis (1971, 1991) showed necessary and suf\ufb01cient conditions for its consistency. In recent developments, Bartlett and ...", "dateLastCrawled": "2022-01-10T01:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine</b> <b>Learning</b> and <b>Optimization</b> - NYU Courant", "url": "https://cims.nyu.edu/~munoz/files/ml_optimization.pdf", "isFamilyFriendly": true, "displayUrl": "https://cims.nyu.edu/~munoz/files/ml_<b>optimization</b>.pdf", "snippet": "This is known as <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) and in a sense is the raw <b>optimization</b> part of <b>machine</b> <b>learning</b>, as we will see we will require something more than that. 3 <b>Learning</b> Guarantees De nition 3. Given a set of functions G= fg: Z!Rgand a sample S= (z i)n =1 the <b>empirical</b> Rademacher complexity of Gis de ned by: &lt; S(G) = E \u02d9 1 n sup ...", "dateLastCrawled": "2022-01-29T21:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Tilted <b>Empirical Risk</b> <b>Minimization</b> | DeepAI", "url": "https://deepai.org/publication/tilted-empirical-risk-minimization", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/tilted-<b>empirical-risk</b>-<b>minimization</b>", "snippet": "<b>Empirical risk</b> <b>minimization</b> (<b>ERM</b>) is typically designed to perform well on the average loss, which can result in estimators that are sensitive to outliers, generalize poorly, or treat subgroups unfairly.While many methods aim to address these problems individually, in this work, we explore them through a unified framework\u2014tilted <b>empirical risk</b> <b>minimization</b> (TERM).", "dateLastCrawled": "2022-01-11T23:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "[1411.5417] <b>Private Empirical Risk Minimization</b> Beyond the Worst Case ...", "url": "https://arxiv.org/abs/1411.5417", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/abs/1411.5417", "snippet": "Download PDF Abstract: <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) is a standard technique in <b>machine</b> <b>learning</b>, where a model is selected by minimizing a loss function over constraint set. When the training dataset consists of private information, it is natural to use a differentially private <b>ERM</b> <b>algorithm</b>, and this problem has been the subject of a long line of work started with Chaudhuri and Monteleoni 2008.", "dateLastCrawled": "2021-09-20T11:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Machine</b> <b>Learning</b>: What is the general idea of why minimizing <b>Empirical</b> ...", "url": "https://www.quora.com/Machine-Learning-What-is-the-general-idea-of-why-minimizing-Empirical-Risk-Minimization-is-NP-Complete", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>Machine</b>-<b>Learning</b>-What-is-the-general-idea-of-why-minimizing...", "snippet": "Answer: The computational complexity of <b>Empirical</b> <b>Risk</b> <b>Minimization</b> depends on the hypothesis class and the loss function that defines the <b>risk</b>. For example, <b>ERM</b> for linear regression with the square loss (ordinary least squares) can be solved in polynomial time. But <b>ERM</b> for a standard 3-layer ne...", "dateLastCrawled": "2022-01-19T10:26:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Part VI <b>Learning</b> Theory", "url": "https://elektrotehnika.github.io/ml/notes/ML-notes4.pdf", "isFamilyFriendly": true, "displayUrl": "https://elektrotehnika.github.io/ml/notes/ML-notes4.pdf", "snippet": "<b>Empirical</b> <b>risk</b> <b>minimization</b> <b>can</b> now <b>be thought</b> of as a <b>minimization</b> over the class of functions H, in which the <b>learning</b> <b>algorithm</b> picks the hypothesis: \u02c6h = argmin h\u2208H \u03b5\u02c6(h) 2PAC stands for \u201cprobably approximately correct,\u201d which is a framework and set of assumptions under which numerous results on <b>learning</b> theory were proved. Of ...", "dateLastCrawled": "2022-01-18T04:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Empirical Risk Minimization for Probabilistic Grammars: Sample</b> ...", "url": "https://direct.mit.edu/coli/article/38/3/479/2169/Empirical-Risk-Minimization-for-Probabilistic", "isFamilyFriendly": true, "displayUrl": "https://direct.mit.edu/coli/article/38/3/479/2169/<b>Empirical</b>-<b>Risk</b>-<b>Minimization</b>-for...", "snippet": "The more data we have, the more complex our <b>can</b> be for <b>empirical</b> <b>risk</b> <b>minimization</b>. Structural <b>risk</b> <b>minimization</b> (Vapnik 1998) and the method of sieves (Grenander 1981) are examples of methods that adopt such an approach. Structural <b>risk</b> <b>minimization</b>, for example, <b>can</b> be represented in many cases as a penalization of the <b>empirical</b> <b>risk</b> method ...", "dateLastCrawled": "2021-12-08T04:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Establishing connection between <b>ERM</b> (<b>Empirical</b> <b>Risk</b> <b>Minimization</b>) and MLE", "url": "https://stats.stackexchange.com/questions/464622/establishing-connection-between-erm-empirical-risk-minimization-and-mle", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/464622/establishing-connection-between-<b>erm</b>...", "snippet": "$\\begingroup$ He shows MLE, that is a way to infer an unknown parameter with an estimator which is a function of the observed data, that is $\\hat{\\theta} \\in \\Theta^{\\mathcal{X}}$, function from $\\mathcal{X} \\rightarrow \\Theta$. In section 2.3.4 he states &quot;Most model classes will have some parameters $\\theta \\in \\Theta$ that the <b>learning</b> <b>algorithm</b> will adjust to fit the data.", "dateLastCrawled": "2022-01-09T10:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "svm - difference between <b>empirical</b> <b>risk</b> <b>minimization</b> and structural ...", "url": "https://datascience.stackexchange.com/questions/66729/difference-between-empirical-risk-minimization-and-structural-risk-minimization", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/66729", "snippet": "<b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) is a principle in statistical <b>learning</b> theory that defines a family of <b>learning</b> algorithms and is used to give theoretical bounds on their performance. The core idea is that we cannot know exactly how well an <b>algorithm</b> will work in practice (the true &quot;<b>risk</b>&quot;) because we don&#39;t know the true distribution of data that the <b>algorithm</b> will work on, but we <b>can</b> instead measure its performance on a known set of training data (the &quot;<b>empirical</b>&quot; <b>risk</b>).", "dateLastCrawled": "2022-01-24T04:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Beyond Empirical Risk Minimization</b>: the lessons of deep <b>learning</b> | The ...", "url": "https://cbmm.mit.edu/video/beyond-empirical-risk-minimization-lessons-deep-learning", "isFamilyFriendly": true, "displayUrl": "https://cbmm.mit.edu/video/<b>beyond-empirical-risk-minimization</b>-lessons-deep-<b>learning</b>", "snippet": "This apparent contradiction points to troubling cracks in the conceptual foundations of <b>machine</b> <b>learning</b>. While classical analyses of <b>Empirical</b> <b>Risk</b> <b>Minimization</b> rely on balancing the complexity of predictors with training error, modern models are best described by interpolation. In that paradigm a predictor is chosen by minimizing (explicitly ...", "dateLastCrawled": "2022-01-30T21:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Part VI <b>Learning</b> Theory - CS229: <b>Machine</b> <b>Learning</b>", "url": "http://cs229.stanford.edu/notes_archive/cs229-notes4.pdf", "isFamilyFriendly": true, "displayUrl": "cs229.stanford.edu/notes_archive/cs229-notes4.pdf", "snippet": "Wecall this process <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>), andthe resulting hypothesis output by the <b>learning</b> <b>algorithm</b> is \u02c6h = h \u02c6\u03b8. We think of <b>ERM</b> as the most \u201cbasic\u201d <b>learning</b> <b>algorithm</b>, and it will be this <b>algorithm</b> that we focus on in these notes. (Algorithms such as logistic regression <b>can</b> also be viewed as approximations to <b>empirical</b> <b>risk</b> <b>minimization</b>.) In our study of <b>learning</b> theory, it will be useful to abstract away from the speci\ufb01c parameterization of hypotheses and from ...", "dateLastCrawled": "2022-01-26T13:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Adaptive Newton Method for <b>Empirical</b> <b>Risk</b> <b>Minimization</b> to Statistical ...", "url": "https://slideblast.com/adaptive-newton-method-for-empirical-risk-minimization-to-statistical-_59b73a4a1723dd731a2c7a55.html", "isFamilyFriendly": true, "displayUrl": "https://slideblast.com/adaptive-newton-method-for-<b>empirical</b>-<b>risk</b>-<b>minimization</b>-to...", "snippet": "A hallmark of <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) on large datasets is that evaluating descent directions requires a complete pass over the dataset. Since this is undesirable due to the large number of training samples, stochastic optimization algorithms with descent directions estimated from a subset of samples are the method of choice. First order stochastic optimization has a long history [20, 19] but the last decade has seen fundamental progress in developing alternatives with faster ...", "dateLastCrawled": "2021-10-28T06:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "10-716: Advanced <b>Machine</b> <b>Learning</b> Spring 2019 Lecture 5: January 29", "url": "https://www.cs.cmu.edu/~pradeepr/courses/716/2019-spring/notes/lec5.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.cmu.edu/~pradeepr/courses/716/2019-spring/notes/lec5.pdf", "snippet": "But unfortunately, we don\u2019t know the true distribution Pof the data to perform the <b>minimization</b>. Here is where <b>Empirical</b> <b>Risk</b> <b>Minimization</b> is useful. We <b>can</b> use the <b>ERM</b> estimate fb n to obtain an approximately good classi er given by, fb n2arginf f 1 n Xn i=1 I(f(X i) 6= Y i) This gives us a very actionable estimator. However, there seems to ...", "dateLastCrawled": "2020-12-08T14:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "The use of vicinal-<b>risk</b> <b>minimization</b> for training decision trees", "url": "https://eprints.whiterose.ac.uk/90870/1/CaoRockett-ASC_Final.pdf", "isFamilyFriendly": true, "displayUrl": "https://eprints.whiterose.ac.uk/90870/1/CaoRockett-ASC_Final.pdf", "snippet": "ports the application of soft computing to an important problem in <b>machine</b> <b>learning</b>. In general, training in <b>machine</b> <b>learning</b> is ill-posed [6] and <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) does not necessarily produce best generalization over an unseen test set, a problem which is exacerbated by small datasets; it is this 2 \u2018small data\u2019 scenario we explicitly address in this paper. The de\ufb01ciencies of <b>ERM</b> are illustrated in Fig. 1 for the trivial case of classifying linearly separable patterns ...", "dateLastCrawled": "2022-01-30T20:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Chapter 10 Supervised Learning</b> | Introduction to Data Science", "url": "https://bookdown.org/ronsarafian/IntrotoDS/supervised.html", "isFamilyFriendly": true, "displayUrl": "https://bookdown.org/ronsarafian/IntrotoDS/supervised.html", "snippet": "We now present the <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) approach to supervised <b>learning</b>, a.k.a. M-estimation in the statistical literature. Remark. We do not discuss purely algorithmic approaches such as K-nearest neighbour and kernel smoothing due to space constraints. For a broader review of supervised <b>learning</b>, see the Bibliographic Notes. Example 10.1 (Spam classification) Consider the problem of predicting if a mail is spam or not based on its attributes: length, number of exclamation ...", "dateLastCrawled": "2022-02-03T00:57:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "In-depth analysis of the regularized least-squares <b>algorithm</b> over the ...", "url": "https://towardsdatascience.com/in-depth-analysis-of-the-regularized-least-squares-algorithm-over-the-empirical-risk-minimization-729a1433447f", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/in-depth-analysis-of-the-regularized-least-squares...", "snippet": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) <b>ERM</b> is a widely known concept in <b>machine</b> <b>learning</b>, and I recommend going over this explanation about <b>ERM</b> before proceeding to the actual implementation. <b>ERM</b> is used to classify the performance of <b>learning</b> algorithms, and we <b>can</b> solve the <b>ERM</b> optimization problem by finding a vector w that minimizes the formula ...", "dateLastCrawled": "2022-01-26T02:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Empirical Risk Minimization</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/mathematics/empirical-risk-minimization", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/mathematics/<b>empirical-risk-minimization</b>", "snippet": "Now we <b>can</b> define the <b>ERM</b> <b>learning</b> <b>algorithm</b> for the regression problem associated with a general regressing loss function ... It turns out the conditions required to render <b>empirical risk minimization</b> consistent involve restricting the set of admissible functions. The main insight of VC (Vapnik-Chervonenkis) theory is that the consistency of <b>empirical risk minimization</b> is determined by the worst case behavior over all functions f \u2208 F that the <b>learning</b> <b>machine</b> could choose. We will see ...", "dateLastCrawled": "2022-01-16T03:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Risk</b> <b>Minimization</b> from Adaptively Collected Data: Guarantees for ...", "url": "https://proceedings.neurips.cc/paper/2021/file/a0ae15571eb4a97ac1c34a114f1bb179-Paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/2021/file/a0ae15571eb4a97ac1c34a114f1bb179-Paper.pdf", "snippet": "<b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) is the workhorse of <b>machine</b> <b>learning</b>, whether for classi\ufb01cation and regression or for off-policy policy <b>learning</b>, but its model- agnostic guarantees <b>can</b> fail when we use adaptively collected data, such as the result of running a contextual bandit <b>algorithm</b>. We study a generic importance sampling weighted <b>ERM</b> <b>algorithm</b> for using adaptively collected data to minimize the average of a loss function over a hypothesis class and provide \ufb01rst-of-their-kind ...", "dateLastCrawled": "2022-01-18T19:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>machine</b> <b>learning</b> - <b>Empirical</b> <b>Risk</b> <b>Minimization</b>: <b>empirical</b> vs expected ...", "url": "https://stats.stackexchange.com/questions/265551/empirical-risk-minimization-empirical-vs-expected-and-true-vs-surrogate", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/265551/<b>empirical</b>-<b>risk</b>-<b>minimization</b>-<b>empirical</b>...", "snippet": "In Tie-Yan Liu&#39;s book, he says that in a statistical <b>learning</b> theory for <b>empirical</b> <b>risk</b> <b>minimization</b> has to observe four <b>risk</b> functions: We also need to de\ufb01ne the true loss of the <b>learning</b> problem, which serves as a reference to study the properties of different surrogate loss functions used by various <b>learning</b> algorithms.", "dateLastCrawled": "2022-02-03T02:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Diametrical <b>Risk</b> <b>Minimization</b>: theory and computations - <b>Machine</b> <b>Learning</b>", "url": "https://link.springer.com/article/10.1007/s10994-021-06036-0", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10994-021-06036-0", "snippet": "The theoretical and <b>empirical</b> performance of <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) often suffers when loss functions are poorly behaved with large Lipschitz moduli and spurious sharp minimizers. We propose and analyze a counterpart to <b>ERM</b> called Diametrical <b>Risk</b> <b>Minimization</b> (DRM), which accounts for worst-case <b>empirical</b> risks within neighborhoods in parameter space. DRM has generalization bounds that are independent of Lipschitz moduli for convex as well as nonconvex problems and it <b>can</b> be ...", "dateLastCrawled": "2021-12-24T06:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Tilted <b>Empirical Risk</b> <b>Minimization</b> | DeepAI", "url": "https://deepai.org/publication/tilted-empirical-risk-minimization", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/tilted-<b>empirical-risk</b>-<b>minimization</b>", "snippet": "<b>Empirical risk</b> <b>minimization</b> (<b>ERM</b>) is typically designed to perform well on the average loss, which <b>can</b> result in estimators that are sensitive to outliers, generalize poorly, or treat subgroups unfairly.While many methods aim to address these problems individually, in this work, we explore them through a unified framework\u2014tilted <b>empirical risk</b> <b>minimization</b> (TERM).", "dateLastCrawled": "2022-01-11T23:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "An <b>Empirical</b> Study of <b>Invariant Risk Minimization</b> | DeepAI", "url": "https://deepai.org/publication/an-empirical-study-of-invariant-risk-minimization", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/an-<b>empirical</b>-study-of-<b>invariant-risk-minimization</b>", "snippet": "<b>Invariant risk minimization</b> (IRM) is a recently proposed <b>machine</b> <b>learning</b> framework where the goal is to learn invariances across multiple training environments . <b>Compared</b> to the widely used framework of <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>), IRM does not assume that training samples are identically distributed. Rather, IRM assumes that training ...", "dateLastCrawled": "2022-01-24T09:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "The importance of k-<b>fold cross-validation</b> for model prediction in ...", "url": "https://towardsdatascience.com/the-importance-of-k-fold-cross-validation-for-model-prediction-in-machine-learning-4709d3fed2ef", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/the-importance-of-k-<b>fold-cross-validation</b>-for-model...", "snippet": "This article will discuss and analyze the importance of k-<b>fold cross-validation</b> for model prediction in <b>machine</b> <b>learning</b> using the least-squares <b>algorithm</b> for <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>). We\u2019ll use a polynomial curve-fitting problem to predict the best polynomial for the sample dataset. Also, we\u2019ll go over the implementation step-by ...", "dateLastCrawled": "2022-02-02T10:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Lecture 1 - University of Texas at Austin", "url": "https://users.ece.utexas.edu/~dimakis/DataScience/Lecture1.pdf", "isFamilyFriendly": true, "displayUrl": "https://users.ece.utexas.edu/~dimakis/DataScience/Lecture1.pdf", "snippet": "This will lead to the main <b>algorithm</b> used in <b>learning</b>: <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>). We need a mathematical model of how data is generated and labeled. We assume we are given a distribution D over the feature space. Each sample x i (weight and height of a nanochip) is assumed to be randomly and independently sampled from this distribution. We use bold for x because it is a vector of p numbers (the features), i.e. x 2Rp. We assume a true labeling function h T. The universe samples a ...", "dateLastCrawled": "2021-11-21T16:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "3. <b>Robust algorithms for Regression, Classification and</b> Clustering ...", "url": "https://scikit-learn-extra.readthedocs.io/en/stable/modules/robust.html", "isFamilyFriendly": true, "displayUrl": "https://scikit-learn-extra.readthedocs.io/en/stable/modules/robust.html", "snippet": "3.2. Robust estimation with robust weighting\u00b6. A lot of <b>learning</b> algorithms are based on a paradigm known as <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) which consists in finding the estimator \\(\\widehat{f}\\) that minimizes an estimation of the <b>risk</b>.", "dateLastCrawled": "2022-01-26T20:37:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Computational and Statistical <b>Learning</b> Theory", "url": "https://home.ttic.edu/~nati/Teaching/TTIC31120/2015/Lecture17.pdf", "isFamilyFriendly": true, "displayUrl": "https://home.ttic.edu/~nati/Teaching/TTIC31120/2015/Lecture17.pdf", "snippet": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) / Sample Average Approximation (SAA): Collect sample z1UYU zm ... SGD for <b>Machine</b> <b>Learning</b> Initialize S 4 L r At iteration t: Draw T \u00e7\u00e1U \u00e71\u00de If U \u00e7 S \u00e7 \u00e1\u00f6 T \u00e7 O s\u00e1 S \u00e7 &gt; 5 Z S \u00e7 E\u00df \u00e7U \u00e7\u00f6 T \u00e7 else: S \u00e7 &gt; 5 Z S \u00e7 Return S % \u00cd L 5 \u00cd \u00c3 \u00cd S \u00e7 \u00e7 @ 5 Draw T 5\u00e1U 5 \u00e1\u00e5\u00e1 T \u00e0 \u00e1U \u00e0 1\u00de Initialize S 4 L r At iteration t: Pick E \u00d0 s\u00e5I at random If U \u00dc S \u00e7 \u00e1\u00f6 T \u00dc O s\u00e1 S \u00e7 &gt; 5 Z S \u00e7 E\u00df \u00e7U \u00dc\u00f6 T \u00dc else: S \u00e7 &gt; 5 Z S \u00e7 S \u00e7 &gt; 5 Z ...", "dateLastCrawled": "2022-01-26T03:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> and Stochastic Gradient Descent for ...", "url": "http://proceedings.mlr.press/v89/veitch19a/veitch19a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v89/veitch19a/veitch19a.pdf", "snippet": "where F(Sn) is the <b>empirical</b> distribution.2 The <b>ERM</b> dogma is to select the predictor \u03c0\u02c6\u03b8 n given by \u02c6\u03b8 n = argmin\u03b8 R\u02c6(\u03b8,Sn). That is, the objective function that de\ufb01nes <b>learning</b> is the <b>empirical</b> <b>risk</b>. <b>ERM</b> has two useful properties. (1) It provides a prin-cipled framework for de\ufb01ning new <b>machine</b> <b>learning</b> methods. In particular, when ...", "dateLastCrawled": "2021-09-18T06:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Basics of <b>Machine</b> <b>Learning</b>", "url": "https://courses.cs.duke.edu/spring21/compsci527/slides/s_04_deep_learning.pdf", "isFamilyFriendly": true, "displayUrl": "https://courses.cs.duke.edu/spring21/compsci527/slides/s_04_deep_<b>learning</b>.pdf", "snippet": "This is called <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) COMPSCI 527 \u2014 Computer Vision Basics of <b>Machine</b> <b>Learning</b> 15/26. Loss and <b>Risk</b> <b>Machine</b> <b>Learning</b> and the Statistical <b>Risk</b> <b>ERM</b>: w^ 2argmin w2R m L T(w) In <b>machine</b> <b>learning</b>, we go much farther: We also want h to do well on previously unseen inputs To relate past and future data, assume that all data comes from the same joint probability distribution p(x;y) p is called the generative data model or just model The goal of <b>machine</b> <b>learning</b> is to ...", "dateLastCrawled": "2021-11-06T09:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Statistical <b>Learning</b> Theory and the C-Loss cost function", "url": "http://www.cnel.ufl.edu/courses/EEL6814/closs.pdf", "isFamilyFriendly": true, "displayUrl": "www.cnel.ufl.edu/courses/EEL6814/closs.pdf", "snippet": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) principle Let us consider a <b>learning</b> <b>machine</b> x,d are real r.v. with joint distribution P(x,y). F(x) is a function of some parameters w, i.e. f(x,w). d d. <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) principle How can we find the possible best <b>learning</b> <b>machine</b> that generalizes for unseen data from the same distribution? Define the <b>Risk</b> functional as L(.) is called the Loss function, and minimize it w.r.t. w achieving the best possible loss. But we can not do this ...", "dateLastCrawled": "2022-01-28T20:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Lecture 1: Reinforcement <b>Learning</b>: What and Why?", "url": "https://imada.sdu.dk/~kandemir/rl-lecture1.pdf", "isFamilyFriendly": true, "displayUrl": "https://imada.sdu.dk/~kandemir/rl-lecture1.pdf", "snippet": "<b>machine</b> <b>learning</b> and is referred to as <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>). 3 Challenges of reinforcement <b>learning</b> Consider the cart pole balancing problem, where a cart carrying an unactuated pole \ufb02oats on a straight horizontal track. The cart is actuated by a torque applied either to the right or the left direction. Seeherefor a real cart ...", "dateLastCrawled": "2021-09-30T12:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Topics in <b>Machine</b> <b>Learning</b> (TIML-09)", "url": "https://www.cse.iitb.ac.in/~saketh/teaching/cs689.html", "isFamilyFriendly": true, "displayUrl": "https://www.cse.iitb.ac.in/~saketh/teaching/cs689.html", "snippet": "Introduction to Statistical <b>Learning</b> Theory (SLT): Definitions of loss function, <b>risk</b>, <b>empirical</b> <b>risk</b>, motivation for <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) Further Reading, Supplementary: Jan 12: Consistency of <b>ERM</b>, Sufficient condition for <b>ERM</b> as one-sided uniform convergence, Analysis for finite sets of functions and extensions to general case using Symmetrization trick, Shattering Coeff. Further Reading, Supplementary: Jan 15: Shattering coeff., growth function, VC dimension, Annealed Entropy ...", "dateLastCrawled": "2022-01-11T09:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and-sample-codes ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/README.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "Loss function 3.1 Categories 3.2 <b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) and Structural <b>risk</b> <b>minimization</b> (SRM) 3.2.1 MLE 3.2.2 MAP 4. Evaluation metrics 4.1 Accuracy, precision, recall and F1 4.2 ROC, AUC 4.3 BLEU 5. Sample projects 6. Classical questions 6.1 Why is logistic regression a generalized linear model ?", "dateLastCrawled": "2021-09-22T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>GitHub</b> - gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "Loss function 3.1 Categories 3.2 <b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) and Structural <b>risk</b> <b>minimization</b> (SRM) 3.2.1 MLE 3.2.2 MAP 4. Evaluation metrics 4.1 Accuracy, precision, recall and F1 4.2 ROC, AUC 4.3 BLEU 5. Sample projects 6. Classical questions 6.1 Why is logistic regression a generalized linear model ?", "dateLastCrawled": "2021-09-12T01:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "[2006.09461] Robust <b>Compressed Sensing using Generative Models</b> - arXiv", "url": "https://arxiv.org/abs/2006.09461", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/abs/2006.09461", "snippet": "Classical recovery approaches such as <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) are guaranteed to succeed when the measurement matrix is sub-Gaussian. However, when the measurement matrix and measurements are heavy-tailed or have outliers, recovery may fail dramatically. In this paper we propose an algorithm inspired by the Median-of-Means (MOM). Our algorithm guarantees recovery for heavy-tailed data, even in the presence of outliers. Theoretically, our results show our novel MOM-based algorithm ...", "dateLastCrawled": "2021-06-27T11:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(PDF) <b>Support Vector Machines: Theory and Applications</b>", "url": "https://www.researchgate.net/publication/221621494_Support_Vector_Machines_Theory_and_Applications", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221621494_Support_Vector_<b>Machine</b>s", "snippet": "The principle used is <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) over a set of possible functions, called hypothesis space. Formally this can be written as minimizing the <b>empirical</b> . error: \u2211 = l. 1 i. x ...", "dateLastCrawled": "2022-02-02T02:47:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "ToyotaTechnologicalInstituteatChicago UniversityofTexasatAustin surbhi ...", "url": "https://arxiv.org/pdf/2005.07652.pdf", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/pdf/2005.07652.pdf", "snippet": "<b>Just as empirical risk minimization (ERM</b>) is central for non-robust PAC <b>learning</b>, a core component of adversarially robust <b>learning</b> is minimizing the robust empirical risk on a dataset S, \u02c6h \u2208 RERM U(S) ,argmin h\u2208H 1 m Xm i=1 sup z\u2208U(x) 1 [h(z) 6= y]. In this paper, we provide necessary and su\ufb03cient conditions on perturbation sets U ...", "dateLastCrawled": "2021-07-27T03:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Efficiently Learning Adversarially Robust Halfspaces with</b> Noise | DeepAI", "url": "https://deepai.org/publication/efficiently-learning-adversarially-robust-halfspaces-with-noise", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>efficiently-learning-adversarially-robust-halfspaces</b>...", "snippet": "<b>Just as empirical risk minimization (ERM</b>) is central for non-robust PAC <b>learning</b>, a core component of adversarially robust <b>learning</b> is minimizing the robust empirical risk on a dataset S, ^ h \u2208 R E R M U ( S ) \u225c argmin h \u2208 H 1 m m \u2211 i = 1 sup z \u2208 U ( x ) 1 [ h ( z ) \u2260 y ] .", "dateLastCrawled": "2021-12-05T23:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Efficiently <b>Learning</b> Adversarially Robust Halfspaces with Noise", "url": "http://proceedings.mlr.press/v119/montasser20a/montasser20a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v119/montasser20a/montasser20a.pdf", "snippet": "remains a major challenge in <b>machine</b> <b>learning</b>. A line of work has shown that predictors learned by deep neural networks are not robust to adversarial examples (Szegedy et al.,2014;Biggio et al.,2013;Goodfellow et al.,2015). This has led to a long line of research studying different aspects of robustness to adversarial examples. In this paper, we consider the problem of distribution-independent <b>learning</b> of halfspaces that are robust to ad-versarial examples at test time, also referred to as ...", "dateLastCrawled": "2021-11-21T12:03:00.0000000Z", "language": "en", "isNavigational": false}], [], []], "all_bing_queries": ["+(empirical risk minimization (erm))  is like +(machine learning algorithm)", "+(empirical risk minimization (erm)) is similar to +(machine learning algorithm)", "+(empirical risk minimization (erm)) can be thought of as +(machine learning algorithm)", "+(empirical risk minimization (erm)) can be compared to +(machine learning algorithm)", "machine learning +(empirical risk minimization (erm) AND analogy)", "machine learning +(\"empirical risk minimization (erm) is like\")", "machine learning +(\"empirical risk minimization (erm) is similar\")", "machine learning +(\"just as empirical risk minimization (erm)\")", "machine learning +(\"empirical risk minimization (erm) can be thought of as\")", "machine learning +(\"empirical risk minimization (erm) can be compared to\")"]}