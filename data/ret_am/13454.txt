{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 0, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Empirical risk minimization</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Empirical_risk_minimization", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Empirical_risk_minimization</b>", "snippet": "<b>Empirical risk minimization</b> (<b>ERM</b>) is a principle in statistical learning theory which defines a family of learning algorithms and is used to give theoretical bounds on their performance. The core idea is that we cannot know exactly how well an algorithm will work in practice (the true &quot;<b>risk</b>&quot;) because we don&#39;t know the true distribution of data that the algorithm will work on, but we can instead measure its performance on a known set of training data (the &quot;<b>empirical</b>&quot; <b>risk</b>).", "dateLastCrawled": "2022-02-03T06:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "International Journal of Finance &amp; Banking Studies IJFBS Vol.2 No.3 ...", "url": "https://core.ac.uk/download/pdf/230938243.pdf", "isFamilyFriendly": true, "displayUrl": "https://core.ac.uk/download/pdf/230938243.pdf", "snippet": "Learning Theory which uses Structural <b>Risk</b> <b>Minimization</b> (SRM) inductive principle instead of traditional <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>). Unlike <b>ERM</b> which focuses on <b>minimizing</b> the training error, SRM tries to minimize the", "dateLastCrawled": "2021-06-12T20:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Regression Using Support Vector Machines: Basic Foundations</b> | Request PDF", "url": "https://www.researchgate.net/publication/238087923_Regression_Using_Support_Vector_Machines_Basic_Foundations", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/238087923_Regression_Using_Support_Vector...", "snippet": "SVM is a new technique built on structural <b>risk</b> minimisation (SRM) instead of <b>empirical</b> <b>risk</b> minimisation (<b>ERM</b>) <b>like</b> ANN, hence SVMs are robust and accurate (Yoon et al. 2011). ...", "dateLastCrawled": "2022-01-20T07:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Conservative Online Convex Optimization", "url": "https://2021.ecmlpkdd.org/wp-content/uploads/2021/07/sub_271.pdf", "isFamilyFriendly": true, "displayUrl": "https://2021.ecmlpkdd.org/wp-content/uploads/2021/07/sub_271.pdf", "snippet": "In the classic <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) framework [38], the objective is to solve a stochastic optimization problem by <b>minimizing</b> the <b>empirical</b> loss function over a given set of training examples drawn from the unknown distribu-tion. However, using the <b>ERM</b> approach in production exposes the learner to the issue of concept drift [37], i.e., the <b>risk</b> that the distribution producing a training dataset may di er from the one observed during the operational life of the model. A solution ...", "dateLastCrawled": "2021-12-31T17:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Learning</b> from uniformly ergodic Markov chains - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S0885064X09000028", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0885064X09000028", "snippet": "The <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) principle then advocates that instead of <b>minimizing</b> the expected <b>risk</b>, an ... We define f \u02c6 to be a function <b>minimizing</b> the <b>empirical</b> <b>risk</b> E n (f) over F, i.e., (1) f \u02c6 = arg min f \u2208 F E n (f) = arg min f \u2208 F 1 n \u2211 i = 0 n \u2212 1 \u2113 (f, z i). According to the <b>ERM</b> principle, we then consider the function f \u02c6 as an approximation of the target function f \u0303. Thus a central question of the <b>ERM</b> <b>learning</b> is how well f \u02c6 really approximate f \u0303. If ...", "dateLastCrawled": "2022-01-14T12:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "IBISML, PRMU &amp; CVIM joint workshop September 16, 2017 Recent Advances ...", "url": "https://niug1984.github.io/paper/niu_ibisml030.pdf", "isFamilyFriendly": true, "displayUrl": "https://niug1984.github.io/paper/niu_ibisml030.pdf", "snippet": "<b>Empirical</b> <b>risk</b> <b>minimization</b> (Vapnik, Statistical Learning Theory, 1998) A \u201ccookbook\u201d procedure of <b>ERM</b> 1. Choose a loss, so that the (expected) learning objective (which is known as the <b>risk</b>) can be defined 2. Choose a model, so that the <b>risk</b> can be minimized over this model family (rather than all measurable functions) 3.", "dateLastCrawled": "2021-09-03T00:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Publications - MIT", "url": "https://www.mit.edu/~rakhlin/publications.html", "isFamilyFriendly": true, "displayUrl": "https://www.mit.edu/~rakhlin/publications.html", "snippet": "Our algorithm BISTRO requires d calls to the <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) oracle per round, where d is the number of actions. The method uses unlabeled data to make the problem computationally simple. When the <b>ERM</b> problem itself is computationally hard, we extend the approach by employing multiplicative approximation algorithms for the <b>ERM</b> ...", "dateLastCrawled": "2022-02-02T01:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "maximum likelihood - <b>Model fitting vs minimizing expected risk</b> - Cross ...", "url": "https://stats.stackexchange.com/questions/371171/model-fitting-vs-minimizing-expected-risk", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/371171", "snippet": "Maximum likelihood as <b>minimizing</b> the dissimilarity between the <b>empirical</b> distriution and the model distribution 4 Establishing connection between <b>ERM</b> (<b>Empirical</b> <b>Risk</b> <b>Minimization</b>) and MLE", "dateLastCrawled": "2022-01-19T11:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 8, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Support-vector machine</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Support-vector_machine", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Support-vector_machine</b>", "snippet": "The soft-margin <b>support vector machine</b> described above is an example of an <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) algorithm for the hinge loss. Seen this way, support vector machines belong to a natural class of algorithms for statistical inference, and many of its unique features are due to the behavior of the hinge loss. This perspective can provide further insight into how and why SVMs work, and allow us to better analyze their statistical properties.", "dateLastCrawled": "2022-02-02T07:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "2021 Mathematical and Scientific Foundations of Deep Learning Annual ...", "url": "https://www.simonsfoundation.org/event/mathematical-and-scientific-foundations-of-deep-learning-annual-meeting-2021/", "isFamilyFriendly": true, "displayUrl": "https://www.simonsfoundation.org/event/mathematical-and-scientific-foundations-of-deep...", "snippet": "Statistical learning theory relates the problem of <b>minimizing</b> a statistical <b>risk</b> to the problem of <b>minimizing</b> an <b>empirical</b> <b>risk</b>. In this talk, we consider the approximation of a constrained statistical program with a constrained <b>empirical</b> program. To do that, we develop a theory of probably approximately correct constrained (PAC-C) learning that leverages and extends the standard theory of probably approximately correct (PAC) learning. Our results show that PAC and PAC-C learning problems ...", "dateLastCrawled": "2022-01-19T22:40:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Applying Empirical Risk Minimization</b>", "url": "https://donskerclass.github.io/Forecasting/ApplyingERM.html", "isFamilyFriendly": true, "displayUrl": "https://donskerclass.github.io/Forecasting/Applying<b>ERM</b>.html", "snippet": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b>. <b>Empirical</b> <b>Risk</b> Minimizer \\[\\widehat{f}^{<b>ERM</b>}(\\mathcal{Y}_T)=\\underset{f\\in\\mathcal{F}}{\\arg\\min}\\frac{1}{T-h}\\sum_{t=1}^{T}\\ell(y_{t+h},f(\\mathcal{Y}_t))\\]; Saw in previous classes that <b>empirical</b> <b>risk</b> <b>minimization</b> can do a good job Most of the time, loss is small on average with respect to \u201ctrue\u201d distribution, relative to <b>empirical</b> <b>risk</b> or oracle <b>risk</b>; If true distribution \\(p\\in\\mathcal{P}\\) is stationary and weak dependent and hypothesis class \\(\\mathcal{F ...", "dateLastCrawled": "2021-12-27T22:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "International Journal of Finance &amp; Banking Studies IJFBS Vol.2 No.3 ...", "url": "https://core.ac.uk/download/pdf/230938243.pdf", "isFamilyFriendly": true, "displayUrl": "https://core.ac.uk/download/pdf/230938243.pdf", "snippet": "Learning Theory which uses Structural <b>Risk</b> <b>Minimization</b> (SRM) inductive principle instead of traditional <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>). Unlike <b>ERM</b> which focuses on <b>minimizing</b> the training error, SRM tries to minimize the", "dateLastCrawled": "2021-06-12T20:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Conservative Online Convex Optimization", "url": "https://2021.ecmlpkdd.org/wp-content/uploads/2021/07/sub_271.pdf", "isFamilyFriendly": true, "displayUrl": "https://2021.ecmlpkdd.org/wp-content/uploads/2021/07/sub_271.pdf", "snippet": "In the classic <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) framework [38], the objective is to solve a stochastic optimization problem by <b>minimizing</b> the <b>empirical</b> loss function over a given set of training examples drawn from the unknown distribu-tion. However, using the <b>ERM</b> approach in production exposes the learner to the issue of concept drift [37], i.e., the <b>risk</b> that the distribution producing a training dataset may di er from the one observed during the operational life of the model. A solution ...", "dateLastCrawled": "2021-12-31T17:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Learning</b> from uniformly ergodic Markov chains - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S0885064X09000028", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0885064X09000028", "snippet": "The <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) principle then advocates that instead of <b>minimizing</b> the expected <b>risk</b>, an ... We define f \u02c6 to be a function <b>minimizing</b> the <b>empirical</b> <b>risk</b> E n (f) over F, i.e., (1) f \u02c6 = arg min f \u2208 F E n (f) = arg min f \u2208 F 1 n \u2211 i = 0 n \u2212 1 \u2113 (f, z i). According to the <b>ERM</b> principle, we then consider the function f \u02c6 as an approximation of the target function f \u0303. Thus a central question of the <b>ERM</b> <b>learning</b> is how well f \u02c6 really approximate f \u0303. If ...", "dateLastCrawled": "2022-01-14T12:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Machine <b>Learning: A Statistics and Optimization Perspective</b>", "url": "https://nan-ye.com/teach/ml.pdf", "isFamilyFriendly": true, "displayUrl": "https://nan-ye.com/teach/ml.pdf", "snippet": "Expected <b>risk</b> The expected <b>risk</b> of his E (L Z; )). We want to nd the hypothesis with minimum expected <b>risk</b>, arg min h2H E(L(Z;h)): <b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) Minimize <b>empirical</b> <b>risk</b> R n(h) def= 1 n P i L(z i;h) over h 2H. e.g. choose to minimize 70ln 30ln(1 ). 23 / 109", "dateLastCrawled": "2022-01-31T22:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Prescriptive Trees for Integrated Forecasting and Optimization Applied ...", "url": "https://hal.archives-ouvertes.fr/hal-03330017/document", "isFamilyFriendly": true, "displayUrl": "https://hal.archives-ouvertes.fr/hal-03330017/document", "snippet": "third approach suggests applying <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) to directly forecast the decisions, a form of policy selection. This is appropriate when the cost function can be employed as a loss function in a learning algorithm. For example, [16] describes data-driven solutions to the newsven-", "dateLastCrawled": "2022-01-18T11:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Publications - MIT", "url": "https://www.mit.edu/~rakhlin/publications.html", "isFamilyFriendly": true, "displayUrl": "https://www.mit.edu/~rakhlin/publications.html", "snippet": "Our algorithm BISTRO requires d calls to the <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) oracle per round, where d is the number of actions. The method uses unlabeled data to make the problem computationally simple. When the <b>ERM</b> problem itself is computationally hard, we extend the approach by employing multiplicative approximation algorithms for the <b>ERM</b> ...", "dateLastCrawled": "2022-02-02T01:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Learning from Positive and Unlabeled Data with Arbitrary Positive Shift</b>", "url": "https://www.cs.uoregon.edu/Reports/DRP-202003-Hammoudeh.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.uoregon.edu/Reports/DRP-202003-Hammoudeh.pdf", "snippet": "aPU <b>risk</b> estimator that works within an <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) framework. 3. We evaluate our methods on a wide range of benchmark datasets, demonstrating our algorithms\u2019 effectiveness over the state-of-the-art in PU and bPU learning. Additional experiments and all proofs are in the supplemen-tal materials. 2. Standard Positive-Unlabeled Learning Before exploring aPU learning, it is helpful to be famil-iar with PU learning without distributional shifts including standard ...", "dateLastCrawled": "2021-11-11T09:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Entropy | Free Full-Text | Towards a Unified Theory <b>of Learning</b> and ...", "url": "https://www.mdpi.com/1099-4300/22/4/438/htm", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/1099-4300/22/4/438/htm", "snippet": "<b>Minimizing</b> the <b>empirical</b> <b>risk</b> can be achieved using tractable approximations to the <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) procedure, such as stochastic convex optimization [1,2]. However, the generalization <b>risk</b> is often difficult to deal with directly because the underlying distribution is often unknown. Instead, it is a common practice to bound it", "dateLastCrawled": "2022-01-04T00:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "2021 Mathematical and Scientific Foundations of Deep Learning Annual ...", "url": "https://www.simonsfoundation.org/event/mathematical-and-scientific-foundations-of-deep-learning-annual-meeting-2021/", "isFamilyFriendly": true, "displayUrl": "https://www.simonsfoundation.org/event/mathematical-and-scientific-foundations-of-deep...", "snippet": "Statistical learning theory relates the problem of <b>minimizing</b> a statistical <b>risk</b> to the problem of <b>minimizing</b> an <b>empirical</b> <b>risk</b>. In this talk, we consider the approximation of a constrained statistical program with a constrained <b>empirical</b> program. To do that, we develop a theory of probably approximately correct constrained (PAC-C) learning that leverages and extends the standard theory of probably approximately correct (PAC) learning. Our results show that PAC and PAC-C learning problems ...", "dateLastCrawled": "2022-01-19T22:40:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Publications - MIT", "url": "https://www.mit.edu/~rakhlin/publications.html", "isFamilyFriendly": true, "displayUrl": "https://www.mit.edu/~rakhlin/publications.html", "snippet": "Our algorithm BISTRO requires d calls to the <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) oracle per round, where d is the number of actions. The method uses unlabeled data to make the problem computationally simple. When the <b>ERM</b> problem itself is computationally hard, we extend the approach by employing multiplicative approximation algorithms for the <b>ERM</b> ...", "dateLastCrawled": "2022-02-02T01:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) Regularization and statistical learning theory for data analysis ...", "url": "https://www.academia.edu/1107482/Regularization_and_statistical_learning_theory_for_data_analysis", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/1107482/Regularization_and_statistical_learning_theory_for...", "snippet": "The <b>ERM</b> method consists in using the data set D\u2018 to build a stochastic approximation of the expected <b>risk</b>, which is usually called the <b>empirical</b> <b>risk</b>, de/ned as 1 \u2018 Iemp [f; \u2018] = V (yi ; f(xi )): \u2018 i=1 Straightforward <b>minimization</b> of the <b>empirical</b> <b>risk</b> in H <b>can</b> be problematic. First, as we have already discussed in the previous section, it is an ill-posed problem. Second, it <b>can</b> lead to over&lt;tting, meaning that although the minimum of the <b>empirical</b> <b>risk</b> <b>can</b> be very close to zero, the ...", "dateLastCrawled": "2022-01-26T08:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "IBISML, PRMU &amp; CVIM joint workshop September 16, 2017 Recent Advances ...", "url": "https://niug1984.github.io/paper/niu_ibisml030.pdf", "isFamilyFriendly": true, "displayUrl": "https://niug1984.github.io/paper/niu_ibisml030.pdf", "snippet": "<b>Empirical</b> <b>risk</b> <b>minimization</b> (Vapnik, Statistical Learning Theory, 1998) A \u201ccookbook\u201d procedure of <b>ERM</b> 1. Choose a loss, so that the (expected) learning objective (which is known as the <b>risk</b>) <b>can</b> be defined 2. Choose a model, so that the <b>risk</b> <b>can</b> be minimized over this model family (rather than all measurable functions) 3.", "dateLastCrawled": "2021-09-03T00:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) The Role of <b>Enterprise Risk Management</b> (<b>ERM</b>) Using ISO 31000 for ...", "url": "https://www.academia.edu/60161005/The_Role_of_Enterprise_Risk_Management_ERM_Using_ISO_31000_for_the_Competitiveness_of_a_Company_That_Adopts_the_Value_Chain_VC_Model_and_Life_Cycle_Cost_LCC_Approach", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/60161005/The_Role_of_<b>Enterprise_Risk_Management</b>_<b>ERM</b>_Using_ISO...", "snippet": "<b>Enterprise risk management</b> (<b>ERM</b>) <b>ERM</b> is the leading approach to managing and optimizing risks, enabling a company to determine how much uncertainty and <b>risk</b> are acceptable to an organization. With a company-wide scope, <b>ERM</b> serves as a strategic analysis of <b>risk</b> throughout an organization, cutting across business units and department, and considering end-to-end processes. In adopting an <b>ERM</b> approach, companies gain the ability to align their <b>risk</b> appetite and tolerance with business strategy ...", "dateLastCrawled": "2022-01-24T04:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Avrim Blum&#39;s publications - TTIC", "url": "https://home.ttic.edu/~avrim/Papers/pubs.html", "isFamilyFriendly": true, "displayUrl": "https://home.ttic.edu/~avrim/Papers/pubs.html", "snippet": "Given such biased training data, <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) may produce a classifier that not only is biased but also has suboptimal accuracy on the true data distribution. We examine the ability of fairness-constrained <b>ERM</b> to correct this problem. In particular, we find that the Equal Opportunity fairness constraint (Hardt, Price, and Srebro 2016) combined with <b>ERM</b> will provably recover the Bayes Optimal Classifier under a range of bias models. In contrast, requiring calibration will ...", "dateLastCrawled": "2022-01-29T11:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Understanding Machine Learning 9781107057135, 1107057132</b> - DOKUMEN.PUB", "url": "https://dokumen.pub/understanding-machine-learning-9781107057135-1107057132-w-5259393.html", "isFamilyFriendly": true, "displayUrl": "https://dokumen.pub/<b>understanding-machine-learning-9781107057135-1107057132</b>-w-5259393.html", "snippet": "We describe the <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>), Structural <b>Risk</b> <b>Minimization</b> (SRM), and Minimum Description Length (MDL) learning rules, which show \u201chow a machine <b>can</b> learn.\u201d We quantify the amount of data needed for learning using the <b>ERM</b>, SRM, and MDL rules and show how learning might fail by deriving a \u201cno-free-lunch\u201d theorem. We also discuss how much computation time is required for learning. In the second part of the book we describe various learning algorithms. For some of ...", "dateLastCrawled": "2022-01-19T12:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Remote Sensing | Free Full-Text | Deep Transfer Learning for Few-Shot ...", "url": "https://www.mdpi.com/2072-4292/11/11/1374/htm", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/2072-4292/11/11/1374/htm", "snippet": "We <b>can</b> solve for an optimal parameter by <b>minimizing</b> the average <b>empirical</b> <b>risk</b> on the training labeled data points, i.e., <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>): \u03b8 ^ = arg min \u03b8 e ^ \u03b8 = arg min \u03b8 1 N \u2211 i = 1 N L ( f \u03b8 ( x i s ) , y i s ) ,", "dateLastCrawled": "2022-01-08T17:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Notes from MIT Deep Learning Theory and Non-Convex Optimization ...", "url": "https://kiranvodrahalli.github.io/notes/liveTeX/MITdeep2019/", "isFamilyFriendly": true, "displayUrl": "https://kiranvodrahalli.github.io/notes/liveTeX/MITdeep2019", "snippet": "The first part is just the reduction to <b>empirical</b> <b>risk</b> \u2013 it follows a \\(1/T\\) rate, but there is no dependence on distance to the optimum. You <b>can</b> also prove a parameter convergence guarantee. Superficially it does not have a unique optimum. There always exists a unique ray such that gradient descent follows this path. The purpose of this part of the talk is to say that even in the case of linear logistic regression, there were interesting things to uncover in the case where the optimum is ...", "dateLastCrawled": "2022-01-20T20:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Latest Papers \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/?page=1066", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/?page=1066", "snippet": "We analyze the local Rademacher complexity of <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>)-based multi-label learning algorithms, and in doing so propose a new algorithm for multi-label learning. Rather than \u2026 machine learning learning. Fully Automated Myocardial Infarction Classification using Ordinary Differential Equations. 26 October 2014. Portable, Wearable and Wireless electrocardiogram (ECG) Systems have the potential to be used as point-of-care for cardiovascular disease diagnostic systems ...", "dateLastCrawled": "2022-01-28T06:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Risk</b> Management Question Paper And Memo", "url": "https://archive.forstenet.com/download-risk-management-question-paper-and-memo-pdf", "isFamilyFriendly": true, "displayUrl": "https://archive.forstenet.com/download-<b>risk</b>-management-question-paper-and-memo-pdf", "snippet": "comprehensive <b>empirical</b> assessmentRisk Management Assignment: Airline Industry - Total Compliance and Ethics in <b>Risk</b> ManagementISO 14971 <b>Risk</b> Management for Medical Devices: The <b>Risk</b> management process in banking industryFountain Essays - Your grades could look better!Working with scenarios, <b>risk</b> assessment and capabilitiesGovernment finances - Province of British ColumbiaHow to Write a Legal Disclaimer for Your Business: 12 StepsRisk Management Paper - Engineers CanadaAlgorithmic and ...", "dateLastCrawled": "2022-02-01T22:24:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Empirical</b> <b>risk</b> <b>minimization</b> for heavy-tailed losses", "url": "https://www.researchgate.net/publication/263012371_Empirical_risk_minimization_for_heavy-tailed_losses", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/263012371_<b>Empirical</b>_<b>risk</b>_<b>minimization</b>_for...", "snippet": "We study an <b>empirical</b> <b>risk</b> <b>minimization</b> problem P n f \u2192 min, f \u2208 F. Given a solution fn of this problem, the goal is to obtain very general upper bounds on its excess <b>risk</b> ep( f n ):= P f n ...", "dateLastCrawled": "2021-12-15T11:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>The generalization performance of ERM algorithm</b> with strongly mixing ...", "url": "https://www.researchgate.net/publication/225177173_The_generalization_performance_of_ERM_algorithm_with_strongly_mixing_observations", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/225177173_The_generalization_performance_of...", "snippet": "A common algorithm that has been analyzed is the <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) algorithm, which tries to find the hypothesis <b>minimizing</b> the <b>empirical</b> loss on the training data.", "dateLastCrawled": "2022-01-12T08:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Heuristic Approach for Demand Forecasting under the Impact of ... - IRJET", "url": "https://www.irjet.net/archives/V4/i7/IRJET-V4I7525.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.irjet.net/archives/V4/i7/IRJET-V4I7525.pdf", "snippet": "Vapnik in 1995 and gaining popularity due to high <b>empirical</b> performance. The formulation uses <b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) principle, which has proven to be more superior than Structural <b>risk</b> <b>minimization</b> (SRM) principle that is used by conventional Neural networks [5]. Unlike the neural network where training is a search that", "dateLastCrawled": "2022-01-22T02:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "International Journal of Finance &amp; Banking Studies IJFBS Vol.2 No.3 ...", "url": "https://core.ac.uk/download/pdf/230938243.pdf", "isFamilyFriendly": true, "displayUrl": "https://core.ac.uk/download/pdf/230938243.pdf", "snippet": "Learning Theory which uses Structural <b>Risk</b> <b>Minimization</b> (SRM) inductive principle instead of traditional <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>). Unlike <b>ERM</b> which focuses on <b>minimizing</b> the training error, SRM tries to minimize the", "dateLastCrawled": "2021-06-12T20:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Extending the coverage area of regional ionosphere maps using a support ...", "url": "https://angeo.copernicus.org/articles/37/77/2019/angeo-37-77-2019.pdf", "isFamilyFriendly": true, "displayUrl": "https://angeo.copernicus.org/articles/37/77/2019/angeo-37-77-2019.pdf", "snippet": "NN is based on <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>), which is a method of <b>minimizing</b> learning errors during the learning process. On the other hand, a SVM is based on structural <b>risk</b> <b>minimization</b> (SRM), so it has excellent generalization per-formance (Gunn, 1998). SVMs have been widely used as pre-dictive models in various \ufb01elds. Huang et al. (2015) success-fully performed stock <b>market</b> movement predictions using a SVM. Mohandes et al. (2014) performed wind speed predic-tions using a SVM and ...", "dateLastCrawled": "2022-01-25T00:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Conservative Online Convex Optimization", "url": "https://2021.ecmlpkdd.org/wp-content/uploads/2021/07/sub_271.pdf", "isFamilyFriendly": true, "displayUrl": "https://2021.ecmlpkdd.org/wp-content/uploads/2021/07/sub_271.pdf", "snippet": "In the classic <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) framework [38], the objective is to solve a stochastic optimization problem by <b>minimizing</b> the <b>empirical</b> loss function over a given set of training examples drawn from the unknown distribu-tion. However, using the <b>ERM</b> approach in production exposes the learner to the issue of concept drift [37], i.e., the <b>risk</b> that the distribution producing a training dataset may di er from the one observed during the operational life of the model. A solution ...", "dateLastCrawled": "2021-12-31T17:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Available works - Emmanuel Bacry - \u00c9cole Polytechnique", "url": "http://www.cmap.polytechnique.fr/~bacry/ftpPapers.html", "isFamilyFriendly": true, "displayUrl": "www.cmap.polytechnique.fr/~bacry/ftpPapers.html", "snippet": "This is true in the <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) setting, but <b>can</b> be false when each subfunction depends on a sequence of examples. Our main motivation is acceleration of the training-time of the penalized Cox partial-likelihood, which is the core model used in survival analysis for the study of clinical data, but our algorithm <b>can</b> be used in different settings as well. The proposed algorithm is doubly stochastic in the sense that gradient steps are done using stochastic gradient ...", "dateLastCrawled": "2021-11-10T13:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Publications - MIT", "url": "https://www.mit.edu/~rakhlin/publications.html", "isFamilyFriendly": true, "displayUrl": "https://www.mit.edu/~rakhlin/publications.html", "snippet": "Our algorithm BISTRO requires d calls to the <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) oracle per round, where d is the number of actions. The method uses unlabeled data to make the problem computationally simple. When the <b>ERM</b> problem itself is computationally hard, we extend the approach by employing multiplicative approximation algorithms for the <b>ERM</b> ...", "dateLastCrawled": "2022-02-02T01:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Support <b>vector regression for porosity prediction</b> in a heterogeneous ...", "url": "https://www.sciencedirect.com/science/article/pii/S0098300410002645", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0098300410002645", "snippet": "The underlying formulation of support vector machines embodies the structural <b>risk</b> <b>minimization</b> (SRM) principle which has been shown to be superior to the traditional <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) principle employed by conventional neural networks and classical statistical methods. This new formulation uses margin-based loss functions to control model complexity independently of the dimensionality of the input space, and kernel functions to project the estimation problem to a higher ...", "dateLastCrawled": "2021-10-13T19:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "CFA Level3(2) Flashcards | <b>Quizlet</b>", "url": "https://quizlet.com/249458074/cfa-level32-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/249458074/cfa-level32-flash-cards", "snippet": "An <b>enterprise risk management</b> (<b>ERM</b>) system includes the following steps: ... <b>Empirical</b> duration is the historical duration found from regressing price sensitivity for one type of risky bond against a <b>risk</b>-free reference security across time. For all credit ratings, <b>empirical</b> duration is less than effective (i.e., theoretical) duration. The difference between <b>empirical</b> duration and effective duration across credit categories increases as the credit spread widens. Ba- and B-rated bonds have ...", "dateLastCrawled": "2021-09-24T00:43:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Basics of <b>Machine</b> <b>Learning</b>", "url": "https://courses.cs.duke.edu/spring21/compsci527/slides/s_04_deep_learning.pdf", "isFamilyFriendly": true, "displayUrl": "https://courses.cs.duke.edu/spring21/compsci527/slides/s_04_deep_<b>learning</b>.pdf", "snippet": "This is called <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) COMPSCI 527 \u2014 Computer Vision Basics of <b>Machine</b> <b>Learning</b> 15/26. Loss and <b>Risk</b> <b>Machine</b> <b>Learning</b> and the Statistical <b>Risk</b> <b>ERM</b>: w^ 2argmin w2R m L T(w) In <b>machine</b> <b>learning</b>, we go much farther: We also want h to do well on previously unseen inputs To relate past and future data, assume that all data comes from the same joint probability distribution p(x;y) p is called the generative data model or just model The goal of <b>machine</b> <b>learning</b> is to ...", "dateLastCrawled": "2021-11-06T09:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Topics in <b>Machine</b> <b>Learning</b> (TIML-09)", "url": "https://www.cse.iitb.ac.in/~saketh/teaching/cs689.html", "isFamilyFriendly": true, "displayUrl": "https://www.cse.iitb.ac.in/~saketh/teaching/cs689.html", "snippet": "Introduction to Statistical <b>Learning</b> Theory (SLT): Definitions of loss function, <b>risk</b>, <b>empirical</b> <b>risk</b>, motivation for <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) Further Reading, Supplementary: Jan 12: Consistency of <b>ERM</b>, Sufficient condition for <b>ERM</b> as one-sided uniform convergence, Analysis for finite sets of functions and extensions to general case using Symmetrization trick, Shattering Coeff. Further Reading, Supplementary: Jan 15: Shattering coeff., growth function, VC dimension, Annealed Entropy ...", "dateLastCrawled": "2022-01-11T09:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Computational and Statistical <b>Learning</b> Theory", "url": "https://home.ttic.edu/~nati/Teaching/TTIC31120/2015/Lecture17.pdf", "isFamilyFriendly": true, "displayUrl": "https://home.ttic.edu/~nati/Teaching/TTIC31120/2015/Lecture17.pdf", "snippet": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) / Sample Average Approximation (SAA): Collect sample z1UYU zm ... SGD for <b>Machine</b> <b>Learning</b> Initialize S 4 L r At iteration t: Draw T \u00e7\u00e1U \u00e71\u00de If U \u00e7 S \u00e7 \u00e1\u00f6 T \u00e7 O s\u00e1 S \u00e7 &gt; 5 Z S \u00e7 E\u00df \u00e7U \u00e7\u00f6 T \u00e7 else: S \u00e7 &gt; 5 Z S \u00e7 Return S % \u00cd L 5 \u00cd \u00c3 \u00cd S \u00e7 \u00e7 @ 5 Draw T 5\u00e1U 5 \u00e1\u00e5\u00e1 T \u00e0 \u00e1U \u00e0 1\u00de Initialize S 4 L r At iteration t: Pick E \u00d0 s\u00e5I at random If U \u00dc S \u00e7 \u00e1\u00f6 T \u00dc O s\u00e1 S \u00e7 &gt; 5 Z S \u00e7 E\u00df \u00e7U \u00dc\u00f6 T \u00dc else: S \u00e7 &gt; 5 Z S \u00e7 S \u00e7 &gt; 5 Z ...", "dateLastCrawled": "2022-01-26T03:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Statistical <b>Learning</b> Theory and the C-Loss cost function", "url": "http://www.cnel.ufl.edu/courses/EEL6814/closs.pdf", "isFamilyFriendly": true, "displayUrl": "www.cnel.ufl.edu/courses/EEL6814/closs.pdf", "snippet": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) principle Let us consider a <b>learning</b> <b>machine</b> x,d are real r.v. with joint distribution P(x,y). F(x) is a function of some parameters w, i.e. f(x,w). d d. <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) principle How can we find the possible best <b>learning</b> <b>machine</b> that generalizes for unseen data from the same distribution? Define the <b>Risk</b> functional as L(.) is called the Loss function, and minimize it w.r.t. w achieving the best possible loss. But we can not do this ...", "dateLastCrawled": "2022-01-28T20:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Lecture 1: Reinforcement <b>Learning</b>: What and Why?", "url": "https://imada.sdu.dk/~kandemir/rl-lecture1.pdf", "isFamilyFriendly": true, "displayUrl": "https://imada.sdu.dk/~kandemir/rl-lecture1.pdf", "snippet": "<b>machine</b> <b>learning</b> and is referred to as <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>). 3 Challenges of reinforcement <b>learning</b> Consider the cart pole balancing problem, where a cart carrying an unactuated pole \ufb02oats on a straight horizontal track. The cart is actuated by a torque applied either to the right or the left direction. Seeherefor a real cart ...", "dateLastCrawled": "2021-09-30T12:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Artificial Intelligence</b> #5 : A taxonomy of <b>machine</b> <b>learning</b> and deep ...", "url": "https://www.linkedin.com/pulse/artificial-intelligence-5-taxonomy-machine-learning-deep-ajit-jaokar", "isFamilyFriendly": true, "displayUrl": "https://www.linkedin.com/pulse/<b>artificial-intelligence</b>-5-taxonomy-<b>machine</b>-<b>learning</b>...", "snippet": "Parameter estimation Introduction - Maximum likelihood estimation (MLE) - <b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) - Regularization - The method of moments - Online (recursive) estimation - Parameter ...", "dateLastCrawled": "2021-05-26T21:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Strati\ufb01ed Sampling meets <b>Machine</b> <b>Learning</b>", "url": "https://cs.yale.edu/homes/el327/papers/lls15.pdf", "isFamilyFriendly": true, "displayUrl": "https://cs.yale.edu/homes/el327/papers/lls15.pdf", "snippet": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) approach. Their so-lution of the optimization problem however does not carry over. Chaudhuri et al. [10] also use the query log to iden- tify outlier records. Those are indexed separately and not sampled. While their approach mainly focuses on query ex-ecution speed, one can distill a sampling scheme from it. The work of Joshi and Jermaine [11] is closely related to 2Neyman allocation is also known as Neyman optimal allo-cation. ours. They generate a large ...", "dateLastCrawled": "2021-08-06T00:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and-sample-codes ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/README.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "Loss function 3.1 Categories 3.2 <b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) and Structural <b>risk</b> <b>minimization</b> (SRM) 3.2.1 MLE 3.2.2 MAP 4. Evaluation metrics 4.1 Accuracy, precision, recall and F1 4.2 ROC, AUC 4.3 BLEU 5. Sample projects 6. Classical questions 6.1 Why is logistic regression a generalized linear model ?", "dateLastCrawled": "2021-09-22T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "[2006.09461] Robust <b>Compressed Sensing using Generative Models</b> - arXiv", "url": "https://arxiv.org/abs/2006.09461", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/abs/2006.09461", "snippet": "Classical recovery approaches such as <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) are guaranteed to succeed when the measurement matrix is sub-Gaussian. However, when the measurement matrix and measurements are heavy-tailed or have outliers, recovery may fail dramatically. In this paper we propose an algorithm inspired by the Median-of-Means (MOM). Our algorithm guarantees recovery for heavy-tailed data, even in the presence of outliers. Theoretically, our results show our novel MOM-based algorithm ...", "dateLastCrawled": "2021-06-27T11:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>GitHub</b> - gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "Loss function 3.1 Categories 3.2 <b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) and Structural <b>risk</b> <b>minimization</b> (SRM) 3.2.1 MLE 3.2.2 MAP 4. Evaluation metrics 4.1 Accuracy, precision, recall and F1 4.2 ROC, AUC 4.3 BLEU 5. Sample projects 6. Classical questions 6.1 Why is logistic regression a generalized linear model ?", "dateLastCrawled": "2021-09-12T01:53:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "ToyotaTechnologicalInstituteatChicago UniversityofTexasatAustin surbhi ...", "url": "https://arxiv.org/pdf/2005.07652", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/pdf/2005.07652", "snippet": "<b>Just as empirical risk minimization (ERM</b>) is central for non-robust PAC <b>learning</b>, a core component of adversarially robust <b>learning</b> is minimizing the robust empirical risk on a dataset S, \u02c6h \u2208 RERM U(S) ,argmin h\u2208H 1 m Xm i=1 sup z\u2208U(x) 1 [h(z) 6= y]. In this paper, we provide necessary and su\ufb03cient conditions on perturbation sets U, under which the robust empirical risk minimization (RERM) problem is e\ufb03ciently solvable in the realizable setting. We show that an e\ufb03cient ...", "dateLastCrawled": "2021-10-06T13:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Efficiently Learning Adversarially Robust Halfspaces with</b> Noise | DeepAI", "url": "https://deepai.org/publication/efficiently-learning-adversarially-robust-halfspaces-with-noise", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>efficiently-learning-adversarially-robust-halfspaces</b>...", "snippet": "<b>Just as empirical risk minimization (ERM</b>) is central for non-robust PAC <b>learning</b>, a core component of adversarially robust <b>learning</b> is minimizing the robust empirical risk on a dataset S, ^ h \u2208 R E R M U ( S ) \u225c argmin h \u2208 H 1 m m \u2211 i = 1 sup z \u2208 U ( x ) 1 [ h ( z ) \u2260 y ] .", "dateLastCrawled": "2021-12-05T23:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Efficiently <b>Learning</b> Adversarially Robust Halfspaces with Noise", "url": "http://proceedings.mlr.press/v119/montasser20a/montasser20a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v119/montasser20a/montasser20a.pdf", "snippet": "remains a major challenge in <b>machine</b> <b>learning</b>. A line of work has shown that predictors learned by deep neural networks are not robust to adversarial examples (Szegedy et al.,2014;Biggio et al.,2013;Goodfellow et al.,2015). This has led to a long line of research studying different aspects of robustness to adversarial examples. In this paper, we consider the problem of distribution-independent <b>learning</b> of halfspaces that are robust to ad-versarial examples at test time, also referred to as ...", "dateLastCrawled": "2021-11-21T12:03:00.0000000Z", "language": "en", "isNavigational": false}], [], []], "all_bing_queries": ["+(empirical risk minimization (erm))  is like +(minimizing risk in the market)", "+(empirical risk minimization (erm)) is similar to +(minimizing risk in the market)", "+(empirical risk minimization (erm)) can be thought of as +(minimizing risk in the market)", "+(empirical risk minimization (erm)) can be compared to +(minimizing risk in the market)", "machine learning +(empirical risk minimization (erm) AND analogy)", "machine learning +(\"empirical risk minimization (erm) is like\")", "machine learning +(\"empirical risk minimization (erm) is similar\")", "machine learning +(\"just as empirical risk minimization (erm)\")", "machine learning +(\"empirical risk minimization (erm) can be thought of as\")", "machine learning +(\"empirical risk minimization (erm) can be compared to\")"]}